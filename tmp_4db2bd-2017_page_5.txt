                                                    Published as a conference paper at ICLR 2017
                                                    individual functions. We employ neural networks to model and learn the mapping from input-output
                                                    examples to attributes. We can think of these networks as consisting of two parts:
                                                               1. an encoder: a differentiable mapping from a set of M input-output examples generated by
                                                                     a single program to a latent real-valued vector, and
                                                               2. a decoder: a differentiable mapping from the latent vector representing a set of M input-
                                                                     output examples to predictions of the ground truth program’s attributes.
                                                    For the encoder we use a simple feed-forward architecture. First, we represent the input and output
                                                    types (singleton or array) by a one-hot-encoding, and we pad the inputs and outputs to a maximum
                                                    length L with a special NULL value. Second, each integer in the inputs and in the output is mapped
                                                    to a learned embedding vector of size E = 20. (The range of integers is restricted to a ﬁnite range
                                                    and each embedding is parametrized individually.) Third, for each input-output example separately,
                                                    weconcatenate the embeddings of the input types, the inputs, the output type, and the output into a
                                                    single (ﬁxed-length) vector, and pass this vector through H = 3 hidden layers containing K = 256
                                                    sigmoid units each. The third hidden layer thus provides an encoding of each individual input-output
                                                    example. Finally, for input-output examples in a set generated from the same program, we pool these
                                                    representations together by simple arithmetic averaging. See Appendix C for more details.
                                                    The advantage of this encoder lies in its simplicity, and we found it reasonably easy to train. A
                                                    disadvantage is that it requires an upper bound L on the length of arrays appearing in the input and
                                                    output. We conﬁrmed that the chosen encoder architecture is sensible in that it performs empirically
                                                    at least as well as an RNN encoder, a natural baseline, which may however be more difﬁcult to train.
                                                    DeepCoder learns to predict presence or absence of individual functions of the DSL. We shall see
                                                    this can already be exploited by various search techniques to large computational gains. We use a
                                                    decoderthat pre-multiplies the encoding of input-output examples by a learned C ×K matrix, where
                                                    C = 34 is the number of functions in our DSL (higher-order functions and lambdas are predicted
                                                    independently), and treats the resulting C numbers as log-unnormalized probabilities (logits) of each
                                                    function appearing in the source code. Fig. 2 shows the predictions a trained neural network made
                                                    from 5 input-output examples for the program shown in Fig. 1.
                                                         (+1)  (-1) (*2)  (/2) (*-1)(**2) (*3) (/3)  (*4) (/4) (>0)  (>0) (%2==1)(%2==0)HEADLASTMAP  FILTERSORT REVERSETAKEDROP ACCESSZIPWITHSCANL1+  -    *     MIN  MAX   COUNTMINIMUMMAXIMUMSUM
                                                        .0   .0    .1   .0   .0    .0   .0    .0  1.0   .0    .0  1.0    .0   .2   .0    .0  1.0 1.0 1.0 .7         .0   .1    .0   .4    .0   .0   .1    .0   .2    .1   .0   .0    .0   .0
                                                       Figure 2: Neural network predicts the probability of each function appearing in the source code.
                                                    4.4        SEARCH
                                                    One of the central ideas of this work is to use a neural network to guide the search for a program
                                                    consistent with a set of input-output examples instead of directly predicting the entire source code.
                                                    This section brieﬂy describes the search techniques and how they integrate the predicted attributes.
                                                    Depth-ﬁrst search (DFS).                                 Weuseanoptimized version of DFS to search over programs with a
                                                    given maximum length T (see Appendix D for details). When the search procedure extends a partial
                                                    program by a new function, it has to try the functions in the DSL in some order. At this point DFS
                                                    can opt to consider the functions as ordered by their predicted probabilities from the neural network.
                                                    “Sortandadd”enumeration.                                         Astrongerwayofutilizingthepredictedprobabilities of functions
                                                    in an enumerative search procedure is to use a Sort and add scheme, which maintains a set of active
                                                    functions and performs DFS with the active function set only. Whenever the search fails, the next
                                                    most probable function (or several) are added to the active set and the search restarts with this larger
                                                    active set. Note that this scheme has the deﬁciency of potentially re-exploring some parts of the
                                                    search space several times, which could be avoided by a more sophisticated search procedure.
                                                    Sketch.              Sketch (Solar-Lezama, 2008) is a successful SMT-based program synthesis tool from the
                                                    programming languages research community. While its main use case is to synthesize programs
                                                                                                                                                 5
