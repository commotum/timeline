                         ModelName                   n          n         d        n         d        Batch Size   Learning Rate
                                                       params     layers   model     heads    head
                                                                                                                              −4
                         GPT-3Small                   125M        12       768       12        64       0.5M         6.0 ×10
                                                                                                                              −4
                         GPT-3Medium                  350M        24       1024      16        64       0.5M         3.0 ×10
                                                                                                                              −4
                         GPT-3Large                   760M        24       1536      16        96       0.5M         2.5 ×10
                                                                                                                              −4
                         GPT-3XL                       1.3B       24       2048      24       128        1M          2.0 ×10
                                                                                                                              −4
                         GPT-32.7B                     2.7B       32       2560      32        80        1M          1.6 ×10
                                                                                                                              −4
                         GPT-36.7B                     6.7B       32       4096      32       128        2M          1.2 ×10
                                                                                                                              −4
                         GPT-313B                     13.0B       40       5140      40       128        2M          1.0 ×10
                                                                                                                              −4
                         GPT-3175Bor“GPT-3”           175.0B      96      12288      96       128       3.2M         0.6 ×10
                   Table 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models
                   which we trained. All models were trained for a total of 300 billion tokens.
                   2.1  ModelandArchitectures
                  Weusethesamemodelandarchitecture as GPT-2 [RWC+19], including the modiﬁed initialization, pre-normalization,
                   andreversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse
                   attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence
                   of MLperformance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125
                                                                                                                                   +
                   million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH 20]
                   suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a
                   function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for
                   downstream language tasks.
                   Table 2.1 shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters,
                   n      is the total number of layers, d       is the number of units in each bottleneck layer (we always have the
                    layers                                 model
                   feedforward layer four times the size of the bottleneck layer, d  =4∗d         ), and d     is the dimension of each
                                                                                   ﬀ         model        head
                   attention head. All models use a context window of nctx = 2048 tokens. We partition the model across GPUs along
                   both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural
                   parameters for each model are chosen based on computational efﬁciency and load-balancing in the layout of models
                                                        +
                   across GPU’s. Previous work [KMH 20] suggests that validation loss is not strongly sensitive to these parameters
                   within a reasonably broad range.
                   2.2  Training Dataset
                                                                                                                2      +
                   Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset [RSR 19]constituting
                   nearly a trillion words. This size of dataset is sufﬁcient to train our largest models without ever updating on the same
                   sequence twice. However, we have found that unﬁltered or lightly ﬁltered versions of Common Crawl tend to have
                   lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets:
                   (1) we downloaded and ﬁltered a version of CommonCrawl based on similarity to a range of high-quality reference
                   corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy
                   and preserve the integrity of our held-out validation set as an accurate measure of overﬁtting, and (3) we also added
                   knownhigh-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.
                   Details of the ﬁrst two points (processing of Common Crawl) are described in Appendix A. For the third, we added
                                                                                                                         +
                   several curated high-quality datasets, including an expanded version of the WebText dataset [RWC 19], collected
                                                                                                +
                   by scraping links over a longer period of time, and ﬁrst described in [KMH 20], two internet-based books corpora
                   (Books1 and Books2) and English-language Wikipedia.
                   Table 2.2 shows the ﬁnal mixture of datasets that we used in training. The CommonCrawl data was downloaded from
                   41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before ﬁltering
                   and570GBafterﬁltering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets
                   are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently,
                   such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are
                   sampled 2-3 times. This essentially accepts a small amount of overﬁtting in exchange for higher quality training data.
                      2https://commoncrawl.org/the-data/
                                                                             8
