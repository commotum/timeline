                  Article
                                                     a                               Minecraft Diamond                               b            Iron ore               Iron pickaxe              Diamond
                                                        12                                                                              100                      100                      100
                                                                                                                                         50                       50                       50
                                                         8                                                                            Agents (%)
                                                                                                                                          0                        0                        0
                                                                                                                                         80                       40                      1.0
                                                       Return
                                                         4                                                                               40                       20                      0.5
                                                                                                                                       Episodes (%)0               0                        0
                                                                                                                     Maximum                 0       50     100       0      50      100       0      50      100
                                                         0                                                           Mean
                                                                                                                                                Environment              Environment             Environment
                                                                                                                                                          6               steps (106                       6
                                                                      5                   6                  7                   8               steps (10 )                       )              steps (10 )
                                                                    10                 10                  10                 10
                                                                                     Environment steps                                                PPO         Rainbow          IMPALA          Dreamer
                  Fig. 5 | Performance on the Minecraft Diamond challenge. a, Applied out of                                          latest items in the diamond task, and the fraction of episodes during which they 
                  the box, Dreamer is, to our knowledge, the first algorithm to accomplish all 12                                     obtain the item. Although previous algorithms progress up to the iron pickaxe, 
                  milestones leading up to the diamond, from sparse rewards without human                                             Dreamer is the only compared algorithm that discovers diamonds, and does so 
                  data or curricula. b, Fraction of trained agents that discover each of the three                                    in every training run. Shaded areas indicate one standard deviation.
                  The results show that Dreamer learns robustly across model sizes and                                                bot scripting layer that was specifically engineered to the game and 
                                                                                                                                                                                 47
                  replay ratios, providing a predictable way of increasing its performance                                            exposes high-level actions . Dreamer uses the MineRL competition 
                  by scaling computational resources.                                                                                 action space that includes abstract crafting actions to autonomously 
                                                                                                                                      learn to collect diamonds from sparse rewards using 1 GPU for 9 days, 
                  Previous work                                                                                                       without human data.
                                                                                                                                         Learning dynamics models of unknown environments and using 
                                                                                                                                                                                             15
                  Developing general-purpose algorithms has long been a goal of                                                       them for reinforcement learning  has been explored in early algo-
                                                                                7                                                                                                                                  16
                  reinforcement-learning research. PPO  is widely used and robust but                                                 rithms, such as PILCO, E2C and Visual Foresight . PlaNet introduced 
                                                                                                                                                                                                                                          25
                  requires large amounts of experience and often underperforms spe-                                                   a latent dynamics model accurate enough to plan from pixels . IRIS 
                                                                10                                                                                    41
                  cialized alternatives. MuZero  plans over discrete actions using a value                                            and TWM  integrate transformers, whereas R2I employs structured 
                                                                                                                                                                                                          50                 43
                  prediction model, but the authors did not release an implementation                                                 state-space models for long-term memory . TD-MPC2  learns deter-
                  and the algorithm contains several complex components, making it                                                    ministic dynamics to combine a policy network with classical planning 
                                                                    49
                  challenging to reproduce. Gato  fits one model to expert demonstra-                                                 for continuous actions and employs robustness techniques of Dreamer, 
                  tions of multiple tasks but cannot improve autonomously. In com-                                                    such as percentile return normalization.
                  parison, Dreamer masters a diverse range of environments with fixed 
                  hyperparameters, does not require expert data and its implementation                                                Conclusion
                  is open source.
                     Minecraft has been a focus of recent research. MineRL offers several                                             We present the third generation of the Dreamer algorithm, a general 
                  competition environments and a diverse human dataset to support                                                     reinforcement-learning algorithm that masters a wide range of domains 
                                                                 19        21
                  exploring and learning skills . VPT  recorded contractor gameplay                                                   with fixed hyperparameters. Dreamer not only excels across over 150 
                  with keyboard and mouse actions for behaviour cloning followed by                                                   tasks but also learns robustly across varying data and compute budg-
                  reinforcement learning, obtaining diamonds using 720 GPUs for 9 days.                                               ets, moving reinforcement learning towards a wide range of practical 
                  Voyager uses a language model to call the commands of the MineFlayer                                                applications. Applied out of the box, Dreamer is, to our knowledge, the 
                                                   a Robustness techniques                                                      c Model size scaling
                                                                 14 task mean               Dreamer                                              Crafter                       DMLab goals
                                                      100                                   No observation symlog                   20                              500                                  400 million
                                                                                            No return normalization (advnorm)                                                                            200 million
                                                       50                                   No symexp two-hot (Huber)               10                              250                                  100 million
                                                                                            No Kullback–Leibler balance           Return                                                                 50 million
                                                    Return (%)                              and free bits                                                                                                25 million
                                                         0                                  Without all                              0                                 0                                 12 million
                                                            0          50         100                                                   0          20         40         0          100         200
                                                             Environment steps (%)                                                      Environment steps (106                                  6
                                                                                                                                                               )          Environment steps (10 )
                                                   b Learning signals                                                           d Replay scaling
                                                                 14 task mean                                                                    Crafter                       DMLab goals               64
                                                      100                                                                           18                              450                                  32
                                                                                            Dreamer                                                                                                      16
                                                                                            No value gradients                                                      300                                  8
                                                        50                                  No reward or value gradients             9                                                                   4
                                                     Return (%)                             No reconstruction gradients           Return                            150
                                                                                                                                                                                                         2
                                                         0                                                                           0                                 0                                 1
                                                            0          50         100                                                   0          10         20         0          100         200
                                                             Environment steps (%)                                                                            6                                 6
                                                                                                                                        Environment steps (10 )           Environment steps (10 )
                  Fig. 6 | Ablations and robust scaling of Dreamer. a, All individual robustness                                                   7,9,10. c, The performance of Dreamer increases monotonically  
                                                                                                                                      gradients
                  techniques contribute to the performance of Dreamer on average, although                                            with larger model sizes, ranging from 12 million to 400 million parameters. 
                  each individual technique may affect only some tasks. Training curves of                                            Notably, larger models not only increase task performance but also require  
                  individual tasks are included in Supplementary Information. advnorm,                                                less environment interaction. d, Higher replay ratios predictably increase the 
                  advantage normalization. b, The performance of Dreamer predominantly                                                performance of Dreamer. Together with model size, this allows practitioners  
                  rests on the unsupervised reconstruction loss of its world model, unlike most                                       to improve task performance and data efficiency by employing more 
                  previous algorithms that rely predominantly on reward and value prediction                                          computational resources.
                  652 | Nature | Vol 640 | 17 April 2025
