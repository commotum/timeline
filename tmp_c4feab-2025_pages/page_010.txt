                   Article
                   25 actions listed in Extended Data Table 7. These map onto keyboard                                                           Code availability
                   keys, mouse buttons, camera movement and abstract inventory actions. 
                   The jump action presses the jump and forward keys, because the cat-                                                           Source code for Dreamer and all presented experiments is available 
                   egorical action space allows only one action at a time and the jump key                                                       under an open-source license at https://github.com/danijar/dreamerv3.
                   alone would only allow jumping in place rather than onto something.                                                            
                   A similar but more complex version of this action space was used for                                                          51.    Hoffman, M. et al. Acme: a research framework for distributed reinforcement learning. 
                                                                                                        20                                              Preprint at https://arxiv.org/abs/2006.00979 (2020).
                   curriculum learning in Minecraft in the literature .                                                                          52.  Espeholt, L. et al. IMPALA: scalable distributed deep-RL with importance weighted 
                                                                                                                                                        actor-learner architectures. In Proc. 35th International Conference on Machine Learning 
                   Break speed. In Minecraft, breaking blocks requires keeping the left                                                                 1407–1416 (PMLR, 2018).
                   mouse button pressed continuously for a few seconds, corresponding                                                            53.  Robine, J., Höftmann, M., Uelwer, T. & Harmeling, S. Transformer-based world models are 
                                                                                                                                                        happy with 100k interactions. In Proc. International Conference on Learning Representations 
                   to hundreds of time steps at 20 Hz. For an initially uniform categorical                                                             (ICLR, 2023).
                   policy with 25 actions, the chance of breaking a wood block that is                                                           54.  Osband, I., Blundell, C., Pritzel, A. & Van Roy, B. Deep exploration via bootstrapped DQN. 
                                                                                                1 400           −560                                    In Proc. Advances in Neural Information Processing Systems 29 (NeurIPS, 2016).
                   already in front of the player would thus be 25                                       ≈10          . This makes               55.    Machado, M. C. et al. Revisiting the arcade learning environment: evaluation protocols 
                   the behaviour impossible to discover from scratch without priors of                                                                  and open problems for general agents. J. Artif. Intell. Res. 61, 523–562 (2018).
                   how humans use computers. Although this challenge could be over-                                                              56.  Hessel, M. et al. Multi-task deep reinforcement learning with PopArt. In Proc. AAAI 
                                                                                                                                        65              Conference on Artificial Intelligence Vol. 33, 3796–3803 (Association for the Advancement 
                   come with specific inductive biases, such as learned action repeat ,                                                                 of Artificial Intelligence, 2019).
                   we argue that learning to keep the same button pressed for hundreds                                                           57.    Cho, K. et al. Learning phrase representations using RNN encoder–decoder for statistical 
                   of steps does not lie at the core of what makes Minecraft an interesting                                                             machine translation. In Proc. 2014 Conference on Empirical Methods in Natural Language 
                   challenge for artificial intelligence. To allow agents to learn to break                                                             Processing (EMNLP) (eds Moschitti, A. et al.) 1724–1734 (Association for Computational 
                                                                                                                                                        Linguistics, 2014).
                   blocks, we therefore follow previous work and increase the  58.  Van Keirsbilck, M., Keller, A. & Yang, X. Rethinking full connectivity in recurrent neural 
                                                          20                                                                                            networks. Preprint at https://arxiv.org/abs/1905.12340 (2019).
                   block-breaking speed , so that blocks break within a few time steps                                                           59.  Gruslys, A. et al. The reactor: a fast and sample-efficient actor–critic agent for 
                   depending on the material. As can be seen from the tuned baselines,                                                                  reinforcement learning. In Proc. International Conference on Learning Representations 
                   the resulting environment still poses a significant challenge to current                                                             (ICLR, 2018).
                   learning algorithms.                                                                                                          60.  Brock, A., De, S., Smith, S. L. & Simonyan, K. High-performance large-scale image 
                                                                                                                                                        recognition without normalization. In Proc. 38th International Conference on Machine 
                                                                                                                                                        Learning 1059–1071 (PMLR, 2021).
                   Other environments. Voyager uses the substantially more abstract                                                              61.    Ziyin, L., Wang, Z. T. & Ueda, M. LaProp: Separating momentum and adaptivity in Adam. 
                   actions provided by the high-level MineFlayer bot scripting library,                                                                 Preprint at https://arxiv.org/abs/2002.04839 (2020).
                   such as predefined behaviours for exploring the world until a resource                                                        62.  Kingma, D. P. & Ba, J. Adam: a method for stochastic optimization. In Proc. International 
                                                                                                                                                        Conference on Learning Representations (ICLR, 2017).
                   is found and for automatically mining specified materials within a 32-m                                                       63.  Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor–critic with shared experience 
                                   47                                                                                                                   replay. In Proc. 37th International Conference on Machine Learning 8545–8554  
                   distance . It also uses high-level semantic observations instead of                                                                  (PMLR, 2020).
                   images. Unlike the Voyager environment, the MineRL competition                                                                64.  Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In Proc. 
                   environment requires visual perception and low-level actions for move-                                                               International Conference on Learning Representations (ICLR, 2018).
                   ment and the camera, such as having to jump to climb onto a block or                                                          65.  Sharma, S., Srinivas, A. & Ravindran, B. Learning to repeat: fine grained action repetition 
                                                                                                         21                                             for deep reinforcement learning. In Proc. International Conference on Learning 
                   rotate the camera to face a block for mining. VPT  uses mouse move-                                                                  Representations (ICLR, 2017).
                   ment for crafting and does not speed up block breaking, making it 
                   more challenging than the MineRL competition action space but easier                                                          Acknowledgements We thank M. Norouzi, J. Lin, A. Abdolmaleki, J. Schulman, M. Rowland,  
                   to source corresponding human data. To learn under this more chal-                                                            A. Kosiorek and O. Rybkin for discussions; B. Shahriari, D. Yarats, K. Cobbe and H. Soyer for 
                   lenging set-up, its authors leverage significant domain knowledge to                                                          sharing training curves of baseline algorithms; and D. Furrer, A. Chen and D. Garambha for 
                   design a hierarchical action space composed of 121 actions for different                                                      support with Google Cloud infrastructure.
                   foveated mouse movements and 4,230 meaningful key combinations.                                                               Author contributions D.H. designed and implemented the algorithm, conducted the majority 
                   In summary, we recommend the MineRL competition environment with                                                              of experiments, created the figures and wrote the paper. J.P. contributed to algorithm design 
                   our categorical action space when a simple set-up is preferred, the Voy-                                                      and conducted baseline experiments. J.B. contributed ideas and feedback. T.L. contributed to 
                                                                                                                                                 algorithm design, suggested experiments and provided extensive feedback.
                   ager action space for prompting language models without perception 
                   or low-level control, and the VPT action space when using human data.                                                         Competing interests The authors declare no competing interests.
                                                                                                                                                 Additional information
                   Data availability                                                                                                             Supplementary information The online version contains supplementary material available at 
                                                                                                                                                 https://doi.org/10.1038/s41586-025-08744-2.
                   The algorithm generates its own experience data by interacting with                                                           Correspondence and requests for materials should be addressed to Danijar Hafner.
                   the simulated environments at run time, thus no external datasets are                                                         Peer review information Nature thanks Hao Su and the other, anonymous, reviewer(s) for their 
                                                                                                                                                 contribution to the peer review of this work.
                   used. Datapoints for training curves are available in the code repository.                                                    Reprints and permissions information is available at http://www.nature.com/reprints.
