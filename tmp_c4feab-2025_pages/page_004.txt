                  Article
                                         Context input                Open-loop prediction
                                        True
                                        Model
                                        True
                                        Model
                                              T = 0          5              10           15            20            25           30            35            40           45            50            55            60
                  Fig. 3 | Video predictions of the world model. A procedural maze and                                              without access to intermediate images. From pixel observations, the world 
                  a quadrupedal robot are shown. Given 5 context images and the full action                                         model learns an understanding of the underlying structure of each environment.
                  sequence of an unseen video, Dreamer predicts 45 frames into the future 
                                                                                                                                                                                                                             10,28
                  cause instabilities. Constrained optimization targets a fixed entropy                                                 The network is trained on two-hot encoded targets                                          , a gener-
                                                        30,31
                  on average across states                    regardless of achievable returns, which is                            alization of one-hot encoding to continuous values. The two-hot 
                  robust but explores slowly under sparse rewards and converges lower                                               encoding of a scalar is a vector with ∣B∣ entries that are 0 except at the 
                  under dense rewards. We did not find stable hyperparameters across                                                indices k and k + 1 of the 2 bins closest to the encoded scalar. The 2 
                  domains for these approaches. Return normalization with a denomina-                                               entries sum up to 1, with linearly higher weight given to the bin that 
                  tor limit overcomes these challenges, exploring rapidly under sparse                                              is closer to the encoded continuous target. The network is trained 
                  rewards and converging to high performance across diverse domains.                                                to minimize the categorical cross entropy loss for classification with 
                                                                                                                                    soft targets. The loss depends on only the probabilities assigned 
                  Robust predictions                                                                                                to the bins and not on the continuous values associated with the 
                  Reconstructing inputs and predicting rewards and returns can be chal-                                             bin locations, fully decoupling the gradient magnitude from the  
                  lenging because the scales of these signals vary across domains. Pre-                                             signal scale:
                  dicting large targets using a squared loss can diverge, whereas L1 and                                                                                                 T
                                      9                                                                                                                    L()θy≐−twohot( )logsoftmax(fx(,θ))
                  Huber losses  stagnate learning and normalization based on running 
                                7
                  statistics  introduces non-stationarity. We suggest the symlog squared 
                  error as a simple solution to this dilemma. For this, a neural network                                                Applying these principles, Dreamer transforms vector observa-
                  f(x, θ) with inputs x and parameters θ learns to predict a transformed                                            tions using the symlog functions, both for the encoder inputs and the 
                                                                                                 ̂
                  version of its targets y. To read out predictions y of the network, we                                            decoder targets and employs the synexp two-hot loss for the reward 
                  apply the inverse transformation:                                                                                 predictor and critic. We find that these techniques enable robust and 
                                        1                                                                                           fast learning across many diverse domains.
                                                                           2          ̂
                             L()θf≐       ((xθ,)−symlog(yy))                           ≐symexp((fx,)θ )
                                        2
                     Using the logarithm as transformation would not allow us to predict                                            Evaluation
                  targets that take on negative values. Therefore, we choose a function                                             We evaluate the generality of Dreamer across 8 domains—with over 150 
                                                                                  32
                  from the bi-symmetric logarithmic family  that we name symlog as the                                              tasks—under fixed hyperparameters. We designed the experiments to 
                  transformation with the symexp function as its inverse:                                                           compare Dreamer with the best methods in the literature, which are 
                                                                                                                                    often specifically designed and tuned for the benchmark at hand. We 
                                              symlog()xx≐sign()log(||x +1)                                                                                                                                                     7
                                                                                                                                    further compare with a high-quality implementation of PPO , a standard 
                                              symexp()xx≐sign( )(exp(|x|)−1)                                                        reinforcement-learning algorithm that is known for its robustness. We 
                                                                                                                                    run PPO with fixed hyperparameters chosen to maximize performance 
                     The symlog function compresses the magnitudes of both large posi-                                              across domains, which reproduce strong published results of PPO on 
                                                                                                                                                  34
                  tive and negative values. This allows the optimization process to quickly                                         ProcGen . To push the boundaries of reinforcement learning, we apply 
                  move the network predictions to large values when needed. The symlog                                              Dreamer to the challenging video game Minecraft, comparing it to 
                  function approximates identity around the origin so it does not affect                                            strong previous algorithms. All Dreamer agents are trained on a single 
                  learning of small enough targets. An alternative asymmetric transfor-                                             Nvidia A100 graphics processing unit (GPU) each, making it reproduc-
                  mation has previously been proposed for critic learning33, which we                                               ible for many research labs. A public implementation of Dreamer that 
                  found less effective on average across domains.                                                                   reproduces all results is available.
                     For potentially noisy targets, such as rewards or returns, we addition-
                  ally introduce the symexp two-hot loss. Here, the network outputs the                                             Benchmarks
                  logits for a softmax distribution over exponentially spaced bins b ∈ B.                                           We perform an extensive empirical study across eight domains that 
                                                                                                                        i
                  Predictions are read out as the weighted average of the bin locations                                             include continuous and discrete actions, visual and low-dimensional 
                  weighted by their predicted probabilities. Importantly, the network                                               inputs, dense and sparse rewards, different reward scales, two- 
                  can output any continuous value in the supported interval because                                                 dimensional and three-dimensional worlds, and procedural generation. 
                  this weighted average can fall between the buckets:                                                               Figure 4 summarizes the benchmark results, comparing Dreamer and 
                                ̂                         T                                                                         a wide range of previous algorithms across diverse domains. Dreamer 
                                 ≐≐
                              yfsoftmax( ()xB)sB                             ymexp([                             ])
                                                                                           −20…+20                                  matches or exceeds the best experts—whether they are model based 
                  650 | Nature | Vol 640 | 17 April 2025
