                  first algorithm to collect diamonds in Minecraft from scratch, achiev-                                             25.   Hafner, D. et al. Learning latent dynamics for planning from pixels. In Proc. 36th 
                  ing a significant milestone in the field of artificial intelligence. As a                                                International Conference on Machine Learning 2555–2565 (PMLR, 2019).
                  high-performing algorithm that is based on a learned world model,                                                  26.  Kingma, D. P. et al. Improved variational inference with inverse autoregressive flow.  
                                                                                                                                           In Proc. Advances in Neural Information Processing Systems 29 (NeurIPS, 2016).
                  Dreamer paves the way for future research directions, including teach-                                             27.   Child, R. Very deep VAEs generalize autoregressive models and can outperform them on 
                  ing agents world knowledge from internet videos and learning a sin-                                                      images. In Proc. International Conference on Learning Representations (ICLR, 2021).
                  gle world model across domains to allow artificial agents to build up                                              28.   Bellemare, M. G., Dabney, W. & Munos, R. A distributional perspective on reinforcement 
                                                                                                                                           learning. In Proc. 34th International Conference on Machine Learning 449–458  
                  increasingly general knowledge and competency.                                                                           (PMLR, 2017).
                                                                                                                                     29.   Williams, R. J. Simple statistical gradient-following algorithms for connectionist 
                                                                                                                                           reinforcement learning. Mach. Learn. 8, 229–256 (1992).
                                                                                                                                     30.  Haarnoja, T., Zhou, A., Abbeel, P. & Levine, S. Soft actor–critic: off-policy maximum 
                  Online content                                                                                                           entropy deep reinforcement learning with a stochastic actor. In Proc. 35th International 
                  Any methods, additional references, Nature Portfolio reporting summa-                                                    Conference on Machine Learning (PMLR, 2018).
                                                                                                                                     31.   Abdolmaleki, A. et al. Maximum a posteriori policy optimisation. In Proc. International 
                  ries, source data, extended data, supplementary information, acknowl-                                                    Conference on Learning Representations (ICLR, 2018).
                  edgements, peer review information; details of author contributions                                                32.  Webber, J. B. W. A bi-symmetric log transformation for wide-range data. Meas. Sci. 
                  and competing interests; and statements of data and code availability                                                    Technol. 24, 027001 (2012).
                                                                                                                                     33.  Kapturowski, S., Ostrovski, G., Quan, J., Munos, R. & Dabney, W. Recurrent experience 
                  are available at https://doi.org/10.1038/s41586-025-08744-2.                                                             replay in distributed reinforcement learning. In Proc. International Conference on 
                                                                                                                                           Learning Representations (ICLR, 2018).
                                                                                                                                     34.  Cobbe, K. W., Hilton, J., Klimov, O. & Schulman, J. Phasic policy gradient. In Proc. 38th 
                  1.    Andrychowicz, M. et al. What matters for on-policy deep actor–critic methods?                                      International Conference on Machine Learning 2020–2027 (PMLR, 2021).
                        A large-scale study. In Proc. International Conference on Learning Representations                           35.  Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning environment: 
                        (ICLR, 2021).                                                                                                      an evaluation platform for general agents. J. Artif. Intell. Res. 47, 253–279 (2013).
                  2.    Huang, S., Dossa, R. F. J., Raffin, A., Kanervisto, A. & Wang, W. The 37 implementation                      36.  Hessel, M. et al. Rainbow: combining improvements in deep reinforcement learning.  
                        details of proximal policy optimization. The ICLR Blog Track https://iclr-blog-track.github.                       In Proc. Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative 
                        io/2022/03/25/ppo-implementation-details/ (2023).                                                                  Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on 
                  3.    Kanervisto, A. et al. MineRL Diamond 2021 competition: overview, results, and lessons                              Educational Advances in Artificial Intelligence 3215–3222 (Association for Computing 
                        learned. In Proc. NeurIPS Competitions and Demonstrations Track 13–28 (PMLR, 2021).                                Machinery, 2018).
                  4.    Silver, D. et al. Mastering the game of Go with deep neural networks and tree search.                        37.   Dabney, W., Ostrovski, G., Silver, D. & Munos, R. Implicit quantile networks for 
                        Nature 529, 484–489 (2016).                                                                                        distributional reinforcement learning. In Proc. 35th International Conference on Machine 
                  5.    Brockman, G. OpenAI Five intro. https://blog.gregbrockman.com/openai-five-                                         Learning 1096–1105 (PMLR, 2018).
                        benchmark-intro (2018).                                                                                      38.  Cobbe, K., Hesse, C., Hilton, J. & Schulman, J. Leveraging procedural generation to 
                  6.    Ouyang, L. et al. Training language models to follow instructions with human feedback.                             benchmark reinforcement learning. In Proc. 37th International Conference on Machine 
                        In Proc. 36th International Conference on Neural Information Processing Systems                                    Learning 2048–2056 (Association for Computing Machinery, 2020).
                        27730–27744 (Association for Computing Machinery, 2022).                                                     39.  Beattie, C. et al. DeepMind Lab. Preprint at https://arxiv.org/abs/1612.03801 (2016).
                  7.    Schulman, J., Wolski, F., Dhariwal, P., Radford, A. & Klimov, O. Proximal policy optimization                40.  Ye, W., Liu, S., Kurutach, T., Abbeel, P. & Gao, Y. Mastering Atari games with limited data. 
                        algorithms. Preprint at https://arxiv.org/abs/1707.06347 (2017).                                                   Adv. Neural Inf. Process. Syst. 34, 25476–25488 (2021).
                  8.    Lillicrap, T. P. et al. Continuous control with deep reinforcement learning. In Proc.                        41.   Micheli, V., Alonso, E. & Fleuret, F. Transformers are sample efficient world models.  
                        International Conference on Learning Representations (ICLR, 2016).                                                 In Proc. International Conference on Learning Representations (ICLR, 2023).
                  9.    Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518,                         42.  Tassa, Y. et al. DeepMind Control Suite. Preprint at https://arxiv.org/abs/1801.00690 
                        529–533 (2015).                                                                                                    (2018).
                  10.   Schrittwieser, J. et al. Mastering Atari, Go, Chess and Shogi by planning with a learned                     43.  Hansen, N., Su, H. & Wang, X. TD-MPC2: scalable, robust world models for continuous 
                        model. Nature 588, 604–609 (2020).                                                                                 control. In Proc. International Conference on Learning Representations (ICLR, 2024).
                  11.   Jaderberg, M. et al. Reinforcement learning with unsupervised auxiliary tasks. In Proc.                      44.  Yarats, D., Fergus, R., Lazaric, A. & Pinto, L. Mastering visual continuous control: Improved 
                        International Conference on Learning Representations (ICLR, 2017).                                                 data-augmented reinforcement learning. In Proc. International Conference on Learning 
                  12.   Anand, A. et al. Unsupervised state representation learning in Atari. In Proc. 33rd                                Representations (ICLR, 2022).
                        International Conference on Neural Information Processing Systems 8769–8782                                  45.  Osband, I. et al. Behaviour suite for reinforcement learning. In Proc. International 
                        (Association for Computing Machinery, 2019).                                                                       Conference on Learning Representations (ICLR, 2020).
                  13.   Driess, D., Schubert, I., Florence, P., Li, Y. & Toussaint, M. Reinforcement learning with                   46.  Dizon-Paradis, O., Wormald, S., Capecci, D., Bhandarkar, A. & Woodard, D. Investigating 
                        neural radiance fields. In Proc. 36th International Conference on Neural Information                               the practicality of existing reinforcement learning algorithms: a performance comparison. 
                        Processing Systems 16931–16945 (Association for Computing Machinery, 2022).                                        Preprint at https://www.techrxiv.org/doi/full/10.36227/techrxiv.23739099.v1 (2023).
                  14.   Silver, D. et al. Mastering the game of Go without human knowledge. Nature 550, 354–359                      47.   Wang, G. et al. Voyager: an open-ended embodied agent with large language models. 
                        (2017).                                                                                                            Preprint at https://arxiv.org/abs/2305.16291 (2023).
                  15.   Sutton, R. S. Dyna, an integrated architecture for learning, planning, and reacting. ACM                     48.  Hafner, D. Benchmarking the spectrum of agent capabilities. In Proc. International 
                        SIGART Bull. 2, 160–163 (1991).                                                                                    Conference on Learning Representations (ICLR, 2022).
                  16.   Finn, C. & Levine, S. Deep visual foresight for planning robot motion. In Proc. International                49.  Reed, S. et al. A generalist agent. Preprint at https://arxiv.org/abs/2205.06175 (2022).
                        Conference on Robotics and Automation 2786–2793 (IEEE, 2017).                                                50.  Samsami, M. R., Zholus, A., Rajendran, J. & Chandar, S. Mastering memory tasks with world 
                  17.   Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In Proc. 32nd                         models. In Proc. International Conference on Learning Representations (ICLR, 2024).
                        International Conference on Neural Information Processing Systems 2455–2467 
                        (Association for Computing Machinery, 2018).                                                                 Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in 
                  18.   Kaiser, L. et al. Model-based reinforcement learning for Atari. In Proc. International                       published maps and institutional affiliations.
                        Conference on Learning Representations (ICLR, 2020).
                  19.   Guss, W. H. et al. The MineRL competition on sample efficient reinforcement learning                                            Open Access This article is licensed under a Creative Commons Attribution 
                        using human priors. Preprint at https://arxiv.org/abs/1904.10079 (2019).                                                        4.0 International License, which permits use, sharing, adaptation, distribution 
                  20.  Kanitscheider, I. et al. Multi-task curriculum learning in a complex, visual,                                                    and reproduction in any medium or format, as long as you give appropriate 
                        hard-exploration domain: Minecraft. Preprint at https://arxiv.org/abs/2106.14876 (2021).                     credit to the original author(s) and the source, provide a link to the Creative Commons licence, 
                  21.   Baker, B. et al. Video pretraining (VPT): learning to act by watching unlabeled online                       and indicate if changes were made. The images or other third party material in this article are 
                        videos. In Proc. 36th International Conference on Neural Information Processing Systems                      included in the article’s Creative Commons licence, unless indicated otherwise in a credit line 
                        24639–24654 (Association for Computing Machinery, 2022).                                                     to the material. If material is not included in the article’s Creative Commons licence and your 
                  22.   Hafner, D., Lillicrap, T., Ba, J. & Norouzi, M. Dream to control: learning behaviors by latent               intended use is not permitted by statutory regulation or exceeds the permitted use, you will 
                        imagination. In Proc. International Conference on Learning Representations (ICLR, 2020).                     need to obtain permission directly from the copyright holder. To view a copy of this licence, 
                  23.  Hafner, D., Lillicrap, T., Norouzi, M. & Ba, J. Mastering Atari with discrete world models. In                visit http://creativecommons.org/licenses/by/4.0/.
                        Proc. International Conference on Learning Representations (ICLR, 2021).
                  24.   Kingma, D. P. & Welling, M. Auto-encoding variational Bayes. In Proc. International 
                        Conference on Learning Representations (ICLR, 2014).                                                         © The Author(s) 2025
                                                                                                                                                                                     Nature | Vol 640 | 17 April 2025 | 653
