year,title,url,duplicate_count
2014,Neural Turing Machines (NTM),https://arxiv.org/pdf/1410.5401.pdf,3
2016,Adaptive Computation Time (ACT),https://arxiv.org/pdf/1603.08983.pdf,3
2018,Universal Transformers (UT),https://arxiv.org/pdf/1807.03819.pdf,2
2019,SATNet: Bridging Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver,https://arxiv.org/pdf/1905.12149.pdf,2
2019,Deep Equilibrium Models (DEQ),https://arxiv.org/pdf/1909.01377.pdf,3
2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5),https://arxiv.org/pdf/1910.10683.pdf,3
2019,Differentiable Convex Optimization Layers (DPP),https://arxiv.org/pdf/1910.12430.pdf,2
2019,On the Measure of Intelligence (ARC),https://arxiv.org/pdf/1911.01547.pdf,3
2020,End-to-End Object Detection with Transformers (DETR),https://arxiv.org/pdf/2005.12872.pdf,4
2020,An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale (ViT),https://arxiv.org/pdf/2010.11929.pdf,3
2021,Is Space-Time Attention All You Need for Video Understanding? (TimeSformer),https://arxiv.org/pdf/2102.05095.pdf,2
2021,Zero-Shot Text-to-Image Generation (DALL-E),https://arxiv.org/pdf/2102.12092.pdf,3
2021,Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions,https://arxiv.org/pdf/2102.12122.pdf,4
2021,Measuring Mathematical Problem Solving With the MATH Dataset,https://arxiv.org/pdf/2103.03874.pdf,2
2021,Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM,https://arxiv.org/pdf/2104.04473.pdf,2
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding (RoPE),https://arxiv.org/pdf/2104.09864.pdf,5
2021,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,https://arxiv.org/pdf/2105.15203.pdf,4
2021,Training Verifiers to Solve Math Word Problems (GSM8K),https://arxiv.org/pdf/2110.14168.pdf,2
2022,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering (ScienceQA),https://arxiv.org/pdf/2209.09513.pdf,3
2023,Tree of Thoughts (ToT): Deliberate Problem Solving with Large Language Models,https://arxiv.org/pdf/2305.10601.pdf,3
2023,MMBench: Is Your Multi-modal Model an All-around Player?,https://arxiv.org/pdf/2307.06281.pdf,3
2023,MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,https://arxiv.org/pdf/2311.16502.pdf,3
2024,Rotary Position Embedding for Vision Transformer (RoPE‑Mixed),https://arxiv.org/pdf/2403.13298.pdf,2
