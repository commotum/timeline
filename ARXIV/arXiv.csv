year,title,url
2021,3DETR: An End-to-End Transformer Model for 3D Object Detection,https://arxiv.org/pdf/2109.08141.pdf
2023,4D-Former: Multimodal 4D Panoptic Segmentation,https://arxiv.org/pdf/2311.01520.pdf
2025,Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models,https://arxiv.org/pdf/2502.11075.pdf
2017,A Distributional Perspective on Reinforcement Learning,https://arxiv.org/pdf/1707.06887.pdf
2025,A Fully First-Order Layer for Differentiable Optimization,https://arxiv.org/pdf/2512.02494.pdf
2022,A-OKVQA: A Benchmark for VQA Using World Knowledge,https://arxiv.org/pdf/2206.01718.pdf
2014,Adam: A Method for Stochastic Optimization,https://arxiv.org/pdf/1412.6980.pdf
2019,Adaptive Attention Span in Transformers,https://arxiv.org/pdf/1905.07799.pdf
2016,Adaptive Computation Time (ACT),https://arxiv.org/pdf/1603.08983.pdf
2023,AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models,https://arxiv.org/pdf/2304.06364.pdf
2021,ALBEF: Align Before Fuse,https://arxiv.org/pdf/2107.07651.pdf
2020,An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale (ViT),https://arxiv.org/pdf/2010.11929.pdf
2019,Analysing Mathematical Reasoning Abilities of Neural Models,https://arxiv.org/pdf/1904.01557.pdf
2025,ARC Is a Vision Problem! (Vision ARC / VARC),https://arxiv.org/pdf/2511.14761.pdf
2021,AST: Audio Spectrogram Transformer,https://arxiv.org/pdf/2104.01778.pdf
2016,Asynchronous Methods for Deep Reinforcement Learning (A3C),https://arxiv.org/pdf/1602.01783.pdf
2017,Attention Is All You Need,https://arxiv.org/pdf/1706.03762.pdf
2023,Average-Hard Attention Transformers Are Threshold Circuits,https://arxiv.org/pdf/2308.03212.pdf
2024,Base of RoPE Bounds Context Length,https://arxiv.org/pdf/2405.14591.pdf
2015,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://arxiv.org/pdf/1502.03167.pdf
2021,BEiT: BERT Pre-Training of Image Transformers,https://arxiv.org/pdf/2106.08254.pdf
2022,BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers,https://arxiv.org/pdf/2203.17270.pdf
2024,Beyond A*: Planning with Transformers,https://arxiv.org/pdf/2402.14083.pdf
2024,Beyond Position: The Emergence of Wavelet-like Properties in Transformers,https://arxiv.org/pdf/2410.18067.pdf
2022,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models (BIG-bench),https://arxiv.org/pdf/2206.04615.pdf
2022,BIG-Bench Hard (BBH): Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,https://arxiv.org/pdf/2210.09261.pdf
2023,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Models,https://arxiv.org/pdf/2301.12597.pdf
2022,BLIP: Bootstrapping Language-Image Pre-training,https://arxiv.org/pdf/2201.12086.pdf
2019,CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain,https://arxiv.org/pdf/1911.08962.pdf
2022,ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning,https://arxiv.org/pdf/2203.10244.pdf
2024,Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference,https://arxiv.org/pdf/2403.04132.pdf
2025,Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models,https://arxiv.org/pdf/2505.16416.pdf
2021,CoAtNet: Marrying Convolution and Attention for All Data Sizes,https://arxiv.org/pdf/2106.04803.pdf
2022,CoCa: Contrastive Captioners,https://arxiv.org/pdf/2205.01917.pdf
2024,Combining Induction and Transduction for Abstract Reasoning,https://arxiv.org/pdf/2411.02272.pdf
2025,ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices,https://arxiv.org/pdf/2506.03737.pdf
2023,The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain,https://arxiv.org/pdf/2305.07141.pdf
2018,"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset",https://arxiv.org/pdf/1803.10137.pdf
2017,Constructing Datasets for Multi-hop Reading Comprehension,https://arxiv.org/pdf/1710.06481.pdf
2025,Context-aware Rotary Position Embedding (CARoPE),https://arxiv.org/pdf/2507.23083.pdf
2015,Continuous Control with Deep Reinforcement Learning,https://arxiv.org/pdf/1509.02971.pdf
2021,ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases,https://arxiv.org/pdf/2103.10697.pdf
2017,Convolutional Sequence to Sequence Learning,https://arxiv.org/pdf/1705.03122.pdf
2025,CoPE: A Lightweight Complex Positional Encoding,https://arxiv.org/pdf/2508.18308.pdf
2021,CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification,https://arxiv.org/pdf/2103.14899.pdf
2021,CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows,https://arxiv.org/pdf/2107.00652.pdf
2021,CvT: Introducing Convolutions to Vision Transformers,https://arxiv.org/pdf/2103.15808.pdf
2020,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,https://arxiv.org/pdf/2006.03654.pdf
2025,"Decoupling the ""What"" and ""Where"" With Polar Coordinate Positional Embeddings (PoPE)",https://arxiv.org/pdf/2509.10534.pdf
2019,Deep Equilibrium Models,https://arxiv.org/pdf/1909.01377.pdf
2016,Deep Reinforcement Learning with Double Q-learning (Double DQN),https://arxiv.org/pdf/1509.06461.pdf
2015,Deep Residual Learning (ResNet),https://arxiv.org/pdf/1512.03385.pdf
2016,DeepCoder: Learning to Write Programs,https://arxiv.org/pdf/1611.01989.pdf
2018,DeepProbLog: Neural Probabilistic Logic Programming,https://arxiv.org/pdf/1805.10872.pdf
2025,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://arxiv.org/pdf/2501.12948.pdf
2020,Deformable DETR: Deformable Transformers for End-to-End Object Detection,https://arxiv.org/pdf/2010.04159.pdf
2021,DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries,https://arxiv.org/pdf/2110.06922.pdf
2021,diff-SAT - Sampling and Probabilistic Reasoning for SAT and Answer Set Programming,https://arxiv.org/pdf/2101.00589.pdf
2019,Differentiable Convex Optimization Layers,https://arxiv.org/pdf/1910.12430.pdf
2023,Direct Preference Optimization: Your Language Model is Secretly a Reward Model,https://arxiv.org/pdf/2305.18290.pdf
2018,Distributed Prioritized Experience Replay,https://arxiv.org/pdf/1803.00933.pdf
2021,Do Transformer Modifications Transfer Across Implementations and Applications?,https://arxiv.org/pdf/2102.11972.pdf
2020,DocVQA: A Dataset for VQA on Document Images,https://arxiv.org/pdf/2007.00398.pdf
2025,DoPE: Denoising Rotary Position Embedding,https://arxiv.org/pdf/2511.09146.pdf
2019,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,https://arxiv.org/pdf/1903.00161.pdf
2025,DynamicCity,https://arxiv.org/pdf/2410.18084.pdf
2013,Efficient Estimation of Word Representations in Vector Space,https://arxiv.org/pdf/1301.3781.pdf
2021,Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM,https://arxiv.org/pdf/2104.04473.pdf
2025,Efficiently Allocating Test-Time Compute for LLM Agents,https://arxiv.org/pdf/2509.03581.pdf
2019,Encoding Word Order in Complex Embeddings,https://arxiv.org/pdf/1912.12333.pdf
2017,End-to-End Differentiable Proving,https://arxiv.org/pdf/1705.11040.pdf
2020,End-to-End Object Detection with Transformers (DETR),https://arxiv.org/pdf/2005.12872.pdf
2025,EVA02-AT: Egocentric Video-Language with Spatial-Temporal RoPE,https://arxiv.org/pdf/2506.14356.pdf
2021,Evaluating Large Language Models Trained on Code (HumanEval),https://arxiv.org/pdf/2107.03374.pdf
2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5),https://arxiv.org/pdf/1910.10683.pdf
2023,Extending Context Window of Large Language Models via Positional Interpolation (PI),https://arxiv.org/pdf/2306.15595.pdf
2022,Flamingo: a Visual Language Model for Few-Shot Learning,https://arxiv.org/pdf/2204.14198.pdf
2022,FOLIO: Natural Language Reasoning with First-Order Logic,https://arxiv.org/pdf/2209.00840.pdf
2023,GAIA: a benchmark for General AI Assistants,https://arxiv.org/pdf/2311.12983.pdf
2025,"Gated Attention for LLMs: Non-linearity, Sparsity, Sink-Free",https://arxiv.org/pdf/2505.06708.pdf
2024,Density Adaptive Attention is All You Need: Robust Parameter-Efficient Fine-Tuning Across Multiple Modalities,https://arxiv.org/pdf/2401.11143.pdf
2023,Gemini: A Family of Highly Capable Multimodal Models,https://arxiv.org/pdf/2312.11805.pdf
2024,Generalized Planning for the Abstraction and Reasoning Corpus (GPAR),https://arxiv.org/pdf/2401.07426.pdf
2019,Generating Long Sequences with Sparse Transformers,https://arxiv.org/pdf/1904.10509.pdf
2018,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,https://arxiv.org/pdf/1804.07461.pdf
2021,Goal-Aware Neural SAT Solver (QuerySAT / goal-aware guidance),https://arxiv.org/pdf/2106.07162.pdf
2023,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,https://arxiv.org/pdf/2311.12022.pdf
2023,GPT-4 Technical Report,https://arxiv.org/pdf/2303.08774.pdf
2019,GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering,https://arxiv.org/pdf/1902.09506.pdf
2023,Graph of Thoughts (GoT),https://arxiv.org/pdf/2308.09687.pdf
2021,Graphormer: Do Transformers Really Perform Bad for Graph Representation?,https://arxiv.org/pdf/2106.05234.pdf
2023,Grokking Modular Arithmetic,https://arxiv.org/pdf/2301.02679.pdf
2024,Grokking Modular Polynomials,https://arxiv.org/pdf/2406.03495.pdf
2022,Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets,https://arxiv.org/pdf/2201.02177.pdf
2019,Guiding High-Performance SAT Solvers with Unsat-Core Predictions,https://arxiv.org/pdf/1903.04671.pdf
2024,H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark,https://arxiv.org/pdf/2409.01374.pdf
2024,HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion,https://arxiv.org/pdf/2310.14566.pdf
2025,Head-Wise Adaptive Rotary Positional Encoding (HARoPE),https://arxiv.org/pdf/2510.10489.pdf
2019,HellaSwag: Can a Machine Really Finish Your Sentence?,https://arxiv.org/pdf/1905.07830.pdf
2015,High-Dimensional Continuous Control Using Generalized Advantage Estimation (GAE),https://arxiv.org/pdf/1506.02438.pdf
2022,Holistic Evaluation of Language Models (HELM),https://arxiv.org/pdf/2211.09110.pdf
2018,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",https://arxiv.org/pdf/1809.09600.pdf
2019,HowTo100M: Learning a Text-Video Embedding by Watching Narrated Videos,https://arxiv.org/pdf/1906.03327.pdf
2022,Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks,https://arxiv.org/pdf/2208.10442.pdf
2018,Image Transformer,https://arxiv.org/pdf/1802.05751.pdf
2018,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://arxiv.org/pdf/1802.01561.pdf
2015,Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets,https://arxiv.org/pdf/1503.01007.pdf
2023,Instruction-Following Evaluation for Large Language Models (IFEval),https://arxiv.org/pdf/2311.07911.pdf
2024,Interactive4D: Interactive 4D LiDAR Segmentation,https://arxiv.org/pdf/2410.08206.pdf
2021,Is Space-Time Attention All You Need for Video Understanding? (TimeSformer),https://arxiv.org/pdf/2102.05095.pdf
2023,Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,https://arxiv.org/pdf/2306.05685.pdf
2022,KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,https://arxiv.org/pdf/2205.09921.pdf
2023,Kosmos-1: Language Is Not All You Need,https://arxiv.org/pdf/2302.14045.pdf
2021,LAION-400M: Open Dataset for CLIP Training,https://arxiv.org/pdf/2111.02114.pdf
2022,LAION-5B: An Open Large-Scale Dataset for Training Next Generation Image-Text Models,https://arxiv.org/pdf/2210.08402.pdf
2023,Language Agent Tree Search (LATS),https://arxiv.org/pdf/2310.04406.pdf
2016,Layer Normalization,https://arxiv.org/pdf/1607.06450.pdf
2022,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering (ScienceQA),https://arxiv.org/pdf/2209.09513.pdf
2018,Learning a SAT Solver from Single-Bit Supervision (NeuroSAT),https://arxiv.org/pdf/1802.03685.pdf
2024,Learning Iterative Reasoning through Energy Diffusion,https://arxiv.org/pdf/2406.11179.pdf
2020,Learning to Encode Position for Transformer with Continuous Dynamical Model (FLOATER),https://arxiv.org/pdf/2003.09229.pdf
2014,Learning to Execute,https://arxiv.org/pdf/1410.4615.pdf
2021,Learning Transferable Visual Models From Natural Language Supervision (CLIP),https://arxiv.org/pdf/2103.00020.pdf
2024,Length Generalization of Causal Transformers without Position Encoding,https://arxiv.org/pdf/2404.12224.pdf
2024,Length-Controlled AlpacaEval,https://arxiv.org/pdf/2404.04475.pdf
2025,Less is More: Recursive Reasoning with Tiny Networks,https://arxiv.org/pdf/2510.04871.pdf
2018,Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis,https://arxiv.org/pdf/1805.04276.pdf
2024,LieRE: Lie Rotational Positional Encodings,https://arxiv.org/pdf/2406.10322.pdf
2018,ListOps: A Diagnostic Dataset for Latent Tree Learning,https://arxiv.org/pdf/1804.06028.pdf
2024,"LiveBench: A Challenging, Contamination-Free LLM Benchmark",https://arxiv.org/pdf/2406.19314.pdf
2025,LLaVA-4D: Embedding Spatiotemporal Prompt into LMMs,https://arxiv.org/pdf/2505.12253.pdf
2023,Visual Instruction Tuning,https://arxiv.org/pdf/2304.08485.pdf
2016,Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge,https://arxiv.org/pdf/1606.04422.pdf
2020,Long Range Arena (LRA): A Benchmark for Efficient Transformers,https://arxiv.org/pdf/2011.04006.pdf
2023,"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",https://arxiv.org/pdf/2308.14508.pdf
2024,LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,https://arxiv.org/pdf/2402.13753.pdf
2024,LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate,https://arxiv.org/pdf/2405.13985.pdf
2025,LOOPE: Learnable Optimal Patch Order in Vision Transformers,https://arxiv.org/pdf/2504.14386.pdf
2021,LoRA: Low-Rank Adaptation of Large Language Models,https://arxiv.org/pdf/2106.09685.pdf
2019,LXMERT: Learning Cross-Modality Encoder Representations,https://arxiv.org/pdf/1908.07490.pdf
2022,Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation,https://arxiv.org/pdf/2112.01527.pdf
2023,Mask4Former: Mask Transformer for 4D Panoptic Segmentation,https://arxiv.org/pdf/2309.16133.pdf
2021,Masked Autoencoders Are Scalable Vision Learners (MAE),https://arxiv.org/pdf/2111.06377.pdf
2022,Masked Autoencoders for Point Cloud Self-supervised Learning (Point-MAE),https://arxiv.org/pdf/2203.06604.pdf
2021,MaskFormer: Per-Pixel Classification is Not All You Need for Semantic Segmentation,https://arxiv.org/pdf/2107.06278.pdf
2022,MaskGIT: Masked Generative Image Transformer,https://arxiv.org/pdf/2202.04200.pdf
2017,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (AlphaZero),https://arxiv.org/pdf/1712.01815.pdf
2023,MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts,https://arxiv.org/pdf/2310.02255.pdf
2018,Maximum a Posteriori Policy Optimisation (MPO),https://arxiv.org/pdf/1806.06920.pdf
2022,MaxViT: Multi-Axis Vision Transformer,https://arxiv.org/pdf/2204.01697.pdf
2020,Measuring Massive Multitask Language Understanding (MMLU),https://arxiv.org/pdf/2009.03300.pdf
2021,Measuring Mathematical Problem Solving With the MATH Dataset,https://arxiv.org/pdf/2103.03874.pdf
2024,Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs,https://arxiv.org/pdf/2410.15859.pdf
2014,Microsoft COCO: Common Objects in Context,https://arxiv.org/pdf/1405.0312.pdf
2021,miniF2F: a cross-system benchmark for formal Olympiad-level mathematics,https://arxiv.org/pdf/2109.00110.pdf
2023,MiniGPT-4,https://arxiv.org/pdf/2304.10592.pdf
2024,MM-Vet v2,https://arxiv.org/pdf/2408.00765.pdf
2023,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,https://arxiv.org/pdf/2308.02490.pdf
2023,MMBench: Is Your Multi-modal Model an All-around Player?,https://arxiv.org/pdf/2307.06281.pdf
2023,MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models,https://arxiv.org/pdf/2306.13394.pdf
2024,MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark,https://arxiv.org/pdf/2406.01574.pdf
2023,MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,https://arxiv.org/pdf/2311.16502.pdf
2021,Multiscale Vision Transformers (MViT),https://arxiv.org/pdf/2104.11227.pdf
2018,Music Transformer,https://arxiv.org/pdf/1809.04281.pdf
2022,MuSiQue: Multihop Questions via Single-hop Question Composition,https://arxiv.org/pdf/2108.00573.pdf
2018,NAPS: Natural Program Synthesis Dataset,https://arxiv.org/pdf/1807.03168.pdf
2015,Neural GPUs Learn Algorithms,https://arxiv.org/pdf/1511.08228.pdf
2019,Neural Logic Machines (NLM),https://arxiv.org/pdf/1904.11694.pdf
2018,Neural Ordinary Differential Equations (Neural ODEs),https://arxiv.org/pdf/1806.07366.pdf
2015,Neural Programmer: Inducing Latent Programs with Gradient Descent,https://arxiv.org/pdf/1511.04834.pdf
2014,Neural Turing Machines (NTM),https://arxiv.org/pdf/1410.5401.pdf
2019,NEZHA: Neural Contextualized Representation for Chinese Language Understanding,https://arxiv.org/pdf/1909.00204.pdf
2021,Nystromformer: A Nystrom-Based Algorithm for Approximating Self-Attention,https://arxiv.org/pdf/2102.03902.pdf
2025,ONERULER: Benchmarking multilingual long-context language models,https://arxiv.org/pdf/2503.01996.pdf
2018,OpenBookQA: Can a Suit of Armor Conduct Electricity?,https://arxiv.org/pdf/1809.02789.pdf
2017,OptNet: Differentiable Optimization as a Layer in Neural Networks,https://arxiv.org/pdf/1703.00443.pdf
2020,OSCAR: Object-Semantics Aligned Pre-training for Vision-Language Tasks,https://arxiv.org/pdf/2004.06165.pdf
2023,PaLM-E: An Embodied Multimodal Language Model,https://arxiv.org/pdf/2303.03378.pdf
2020,PCT: Point Cloud Transformer,https://arxiv.org/pdf/2012.09688.pdf
2021,Perceiver / Perceiver IO,https://arxiv.org/pdf/2103.03206.pdf
2023,Plan-and-Solve Prompting,https://arxiv.org/pdf/2305.04091.pdf
2021,Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling,https://arxiv.org/pdf/2111.14819.pdf
2021,PonderNet: Learning to Ponder,https://arxiv.org/pdf/2107.05407.pdf
2023,"POPE (Polling-based Object Probing Evaluation) / ""Evaluating Object Hallucination in Large Vision-Language Models""",https://arxiv.org/pdf/2305.10355.pdf
2015,Prioritized Experience Replay,https://arxiv.org/pdf/1511.05952.pdf
2017,Proximal Policy Optimization (PPO),https://arxiv.org/pdf/1707.06347.pdf
2021,PSB2: The Second Program Synthesis Benchmark Suite,https://arxiv.org/pdf/2106.06086.pdf
2021,Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions,https://arxiv.org/pdf/2102.12122.pdf
2017,Rainbow: Combining Improvements in Deep Reinforcement Learning,https://arxiv.org/pdf/1710.02298.pdf
2022,ReAct: Synergizing Reasoning and Acting in Language Models,https://arxiv.org/pdf/2210.03629.pdf
2020,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,https://arxiv.org/pdf/2009.11462.pdf
2023,Reasoning with Language Model is Planning with World Model (RAP),https://arxiv.org/pdf/2305.14992.pdf
2020,ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning,https://arxiv.org/pdf/2002.04326.pdf
2018,Recurrent Relational Networks (RRN),https://arxiv.org/pdf/1711.08028.pdf
2023,Reflexion: Language Agents with Verbal Reinforcement Learning,https://arxiv.org/pdf/2303.11366.pdf
2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,https://arxiv.org/pdf/2403.00071.pdf
2021,Rethinking and Improving Relative Position Encoding for Vision Transformer (iRPE),https://arxiv.org/pdf/2107.14222.pdf
2020,Rethinking Attention with Performers,https://arxiv.org/pdf/2009.14794.pdf
2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://arxiv.org/pdf/2006.15595.pdf
2024,RETRO-style retrieval-augmented pretraining and variants,https://arxiv.org/pdf/2112.04426.pdf
2024,"Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning (PAV / ""progress rewards"")",https://arxiv.org/pdf/2410.08146.pdf
2017,RobustFill: Neural Program Learning under Noisy I/O,https://arxiv.org/pdf/1703.07469.pdf
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding (RoPE),https://arxiv.org/pdf/2104.09864.pdf
2025,Rotary Masked Autoencoders Are Versatile Learners,https://arxiv.org/pdf/2505.20535.pdf
2024,Rotary Position Embedding for Vision Transformer (RoPE‑Mixed),https://arxiv.org/pdf/2403.13298.pdf
2024,RoTHP: Rotary Position Embedding-based Transformer Hawkes Process,https://arxiv.org/pdf/2405.06985.pdf
2024,RULER: What's the Real Context Size of Your Long-Context Language Models?,https://arxiv.org/pdf/2404.06654.pdf
2016,Safe and Efficient Off-Policy Reinforcement Learning (Retrace(lambda)),https://arxiv.org/pdf/1606.02647.pdf
2019,SATNet: Bridging Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver,https://arxiv.org/pdf/1905.12149.pdf
2022,Scalable Diffusion Models with Transformers (DiT),https://arxiv.org/pdf/2212.09748.pdf
2020,Scaling Laws for Neural Language Models,https://arxiv.org/pdf/2001.08361.pdf
2024,Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters,https://arxiv.org/pdf/2408.03314.pdf
2021,Scaling Up Vision-Language Learning With Noisy Text Supervision (ALIGN),https://arxiv.org/pdf/2102.05918.pdf
2024,Searching Latent Program Spaces,https://arxiv.org/pdf/2411.08706.pdf
2021,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,https://arxiv.org/pdf/2105.15203.pdf
2023,Segment Anything,https://arxiv.org/pdf/2304.02643.pdf
2025,Selective Rotary Position Embedding,https://arxiv.org/pdf/2511.17388.pdf
2018,Self-Attention with Relative Position Representations,https://arxiv.org/pdf/1803.02155.pdf
2022,Self-Consistency Improves Chain-of-Thought Reasoning,https://arxiv.org/pdf/2203.11171.pdf
2023,Self-Refine: Iterative Refinement with Self-Feedback,https://arxiv.org/pdf/2303.17651.pdf
2020,SETR: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,https://arxiv.org/pdf/2012.15840.pdf
2020,Shortformer: Better Language Modeling using Shorter Inputs,https://arxiv.org/pdf/2012.15832.pdf
2025,SmolVLM: Redefining small and efficient multimodal models,https://arxiv.org/pdf/2504.05299.pdf
2018,Soft Actor-Critic (SAC),https://arxiv.org/pdf/1801.01290.pdf
2018,Solving Programming Tasks from Description and Examples,https://arxiv.org/pdf/1802.04335.pdf
2016,Sparse Differentiable Neural Computer (SDNC),https://arxiv.org/pdf/1610.09027.pdf
2022,Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion,https://arxiv.org/pdf/2211.10581.pdf
2023,Spherical Position Encoding for Transformers,https://arxiv.org/pdf/2310.04454.pdf
2016,"SQuAD: 100,000+ Questions for Machine Comprehension of Text",https://arxiv.org/pdf/1606.05250.pdf
2022,STaR: Self-Taught Reasoner (Bootstrapping Reasoning With Reasoning),https://arxiv.org/pdf/2203.14465.pdf
2019,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,https://arxiv.org/pdf/1905.00537.pdf
2023,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,https://arxiv.org/pdf/2310.06770.pdf
2021,Swin Transformer,https://arxiv.org/pdf/2103.14030.pdf
2021,Swin Transformer V2: Scaling Up Capacity and Resolution,https://arxiv.org/pdf/2111.09883.pdf
2016,SyGuS-Comp 2016 Results/Benchmarks (PBE track),https://arxiv.org/pdf/1611.07627.pdf
2017,SyGuS-Comp 2017: Results and Analysis,https://arxiv.org/pdf/1711.11438.pdf
2020,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/pdf/2012.09841.pdf
2025,TAPA: Positional Encoding via Token-Aware Phase Attention,https://arxiv.org/pdf/2509.12635.pdf
2024,Teaching Transformers Modular Arithmetic at Scale,https://www.arxiv.org/pdf/2410.03569v1.pdf
2024,Test-Time Training on Nearest Neighbors for Large Language Models,https://arxiv.org/pdf/2305.18466.pdf
2019,TextVQA: Towards VQA Models That Can Read,https://arxiv.org/pdf/1904.08920.pdf
2025,The Lessons of Developing Process Reward Models...,https://arxiv.org/pdf/2501.07301.pdf
2021,The Pile: An 800GB Dataset of Diverse Text for Language Modeling,https://arxiv.org/pdf/2101.00027.pdf
2025,The Rotary Position Embedding May Cause Dimension Inefficiency,https://arxiv.org/pdf/2502.11276.pdf
2018,"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",https://arxiv.org/pdf/1803.05457.pdf
2021,Tokens-to-Token ViT (T2T-ViT),https://arxiv.org/pdf/2101.11986.pdf
2023,Toolformer: Language Models Can Teach Themselves to Use Tools,https://arxiv.org/pdf/2302.04761.pdf
2015,Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks (bAbI),https://arxiv.org/pdf/1502.05698.pdf
2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://arxiv.org/pdf/2411.17708.pdf
2021,"Train Short, Test Long: Attention with Linear Biases (ALiBi)",https://arxiv.org/pdf/2108.12409.pdf
2022,Training Compute-Optimal Large Language Models (Chinchilla),https://arxiv.org/pdf/2203.15556.pdf
2020,Training data-efficient image transformers & distillation through attention,https://arxiv.org/pdf/2012.12877.pdf
2022,Training language models to follow instructions with human feedback (InstructGPT / RLHF pipeline),https://arxiv.org/pdf/2203.02155.pdf
2021,GSM8K: Training Verifiers to Solve Math Word Problems,https://arxiv.org/pdf/2110.14168.pdf
2021,Transformer in Transformer,https://arxiv.org/pdf/2103.00112.pdf
2022,Transformer Language Models without Positional Encodings Still Learn Positional Information,https://arxiv.org/pdf/2203.16634.pdf
2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://arxiv.org/pdf/1901.02860.pdf
2022,TransNeRF: Generalizable Neural Radiance Fields for Novel View Synthesis with Transformer,https://arxiv.org/pdf/2206.05375.pdf
2025,TransXSSM: Hybrid Transformer–SSM with Unified RoPE,https://arxiv.org/pdf/2506.09507.pdf
2023,Tree of Thoughts (ToT): Deliberate Problem Solving with Large Language Models,https://arxiv.org/pdf/2305.10601.pdf
2015,Trust Region Policy Optimization (TRPO),https://arxiv.org/pdf/1502.05477.pdf
2021,TruthfulQA: Measuring How Models Mimic Human Falsehoods,https://arxiv.org/pdf/2109.07958.pdf
2018,Twin Delayed DDPG (TD3),https://arxiv.org/pdf/1802.09477.pdf
2021,Twins: Revisiting the Design of Spatial Attention in Vision Transformers,https://arxiv.org/pdf/2104.13840.pdf
2023,Uni3DL: Unified Model for 3D and Language Understanding,https://arxiv.org/pdf/2312.03026.pdf
2020,UNITER: Universal Image-Text Representation Learning,https://arxiv.org/pdf/1909.11740.pdf
2021,VATT: Transformers for Multimodal Self-Supervised Learning,https://arxiv.org/pdf/2104.11178.pdf
2019,"VCR: Visual Commonsense Reasoning (""From Recognition to Cognition..."")",https://arxiv.org/pdf/1811.10830.pdf
2025,Vector Symbolic Algebras for the Abstraction and Reasoning Corpus,https://arxiv.org/pdf/2511.08747.pdf
2024,VG4D: Vision-Language Model Goes 4D Video Recognition,https://arxiv.org/pdf/2404.11605.pdf
2021,Video Swin Transformer,https://arxiv.org/pdf/2106.13230.pdf
2024,Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis,https://arxiv.org/pdf/2405.21075.pdf
2019,VideoBERT: A Joint Model for Video and Language Representation Learning,https://arxiv.org/pdf/1904.01766.pdf
2021,VideoCLIP: Contrastive Pretraining for Zero-Shot Video-Text Understanding,https://arxiv.org/pdf/2109.14084.pdf
2022,ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers,https://arxiv.org/pdf/2203.10157.pdf
2019,ViLBERT: Pretraining Task-Agnostic Vision-and-Language Representations,https://arxiv.org/pdf/1908.02265.pdf
2021,ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision,https://arxiv.org/pdf/2102.03334.pdf
2025,VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs,https://arxiv.org/pdf/2506.06727.pdf
2019,VisualBERT: A Simple and Performant Baseline for Vision and Language,https://arxiv.org/pdf/1908.03557.pdf
2022,ViTDet: Exploring Plain ViT Backbones for Object Detection,https://arxiv.org/pdf/2203.16527.pdf
2018,VizWiz Grand Challenge: Answering Visual Questions from Blind People,https://arxiv.org/pdf/1802.08218.pdf
2020,"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",https://arxiv.org/pdf/2010.06775.pdf
2021,Voxel Transformer for 3D Object Detection (VoTr),https://arxiv.org/pdf/2109.02497.pdf
2023,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,https://arxiv.org/pdf/2302.12251.pdf
2017,"VQA v2.0 (""Making the V in VQA Matter"")",https://arxiv.org/pdf/1612.00837.pdf
2015,VQA: Visual Question Answering,https://arxiv.org/pdf/1505.00468.pdf
2025,VRoPE: Rotary Position Embedding for Video Large Language Models,https://arxiv.org/pdf/2502.11664.pdf
2025,WALRUS: A Cross-Domain Foundation Model for Continuum Dynamics,https://arxiv.org/pdf/2511.15684.pdf
2024,What matters when building vision-language models? (Idefics2),https://arxiv.org/pdf/2405.02246.pdf
2022,Winoground: Probing Vision-Language Models for Compositionality,https://arxiv.org/pdf/2204.03162.pdf
2019,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://arxiv.org/pdf/1906.08237.pdf
2022,XPos / Length-Extrapolatable Transformer,https://arxiv.org/pdf/2212.10554.pdf
2023,YaRN: Efficient Context Window Extension of Large Language Models,https://arxiv.org/pdf/2309.00071.pdf
2021,Zero-Shot Text-to-Image Generation (DALL-E),https://arxiv.org/pdf/2102.12092.pdf
2020,ZeRO: Memory Optimizations Toward Training Trillion Parameter Models,https://arxiv.org/pdf/1910.02054.pdf
