year,title,url,duplicate_count
2017,Combining Improvements in Deep Reinforcement Learning (Rainbow),https://arxiv.org/pdf/1710.02298.pdf,1
2018,Rainbow: Combining Improvements in Deep Reinforcement Learning,https://arxiv.org/pdf/1710.02298.pdf,1
2021,Deformable DETR,https://arxiv.org/pdf/2010.04159.pdf,1
2020,Deformable DETR: Deformable Transformers for End-to-End Object Detection,https://arxiv.org/pdf/2010.04159.pdf,2
2020,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/pdf/2012.09841.pdf,2
2021,Taming Transformers for High-Resolution Image Synthesis (VQGAN + Transformer),https://arxiv.org/pdf/2012.09841.pdf,1
2021,SETR: Rethinking Semantic Segmentation as Seq2Seq with Transformers,https://arxiv.org/pdf/2012.15840.pdf,1
2020,SETR: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,https://arxiv.org/pdf/2012.15840.pdf,1
2020,Stand-Alone Axial-Attention (Axial-DeepLab),https://arxiv.org/pdf/2105.15203.pdf,1
2021,BEiT: BERT Pre-Training of Image Transformers,https://arxiv.org/pdf/2106.08254.pdf,2
2022,BEiT: BERT Pre-Training of Image Transformers,https://arxiv.org/pdf/2106.08254.pdf,1
2021,Masked Autoencoders Are Scalable Vision Learners (MAE),https://arxiv.org/pdf/2111.06377.pdf,1
2022,Masked Autoencoders Are Scalable Vision Learners (MAE),https://arxiv.org/pdf/2111.06377.pdf,1
2021,Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling,https://arxiv.org/pdf/2111.14819.pdf,1
2022,Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling,https://arxiv.org/pdf/2111.14819.pdf,1
2019,Generating Long Sequences with Sparse Transformers,https://arxiv.org/pdf/2203.16527.pdf,1
2022,ViTDet: Exploring Plain ViT Backbones for Object Detection,https://arxiv.org/pdf/2203.16527.pdf,1
2024,ARC Prize 2024: Technical Report,https://arxiv.org/pdf/2412.04604.pdf,1
2019,The Abstraction and Reasoning Corpus (ARC),https://arxiv.org/pdf/2412.04604.pdf,1
