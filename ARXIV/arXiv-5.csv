year,title,url,duplicate_count
2014,Neural Turing Machines,https://arxiv.org/pdf/1410.5401.pdf,1
2014,Neural Turing Machines (NTM),https://arxiv.org/pdf/1410.5401.pdf,2
2016,Adaptive Computation Time,https://arxiv.org/pdf/1603.08983.pdf,1
2016,Adaptive Computation Time (ACT),https://arxiv.org/pdf/1603.08983.pdf,2
2018,Universal Transformers,https://arxiv.org/pdf/1807.03819.pdf,1
2018,Universal Transformers (UT),https://arxiv.org/pdf/1807.03819.pdf,1
2019,SATNet: Bridging Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver,https://arxiv.org/pdf/1905.12149.pdf,1
2019,SATNet: Differentiable Satisfiability Solver,https://arxiv.org/pdf/1905.12149.pdf,1
2019,Deep Equilibrium Models,https://arxiv.org/pdf/1909.01377.pdf,1
2019,Deep Equilibrium Models (DEQ),https://arxiv.org/pdf/1909.01377.pdf,2
2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/pdf/1910.10683.pdf,1
2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5),https://arxiv.org/pdf/1910.10683.pdf,2
2019,Differentiable Convex Optimization Layers,https://arxiv.org/pdf/1910.12430.pdf,1
2019,Differentiable Convex Optimization Layers (CVXPYLayers / DPP),https://arxiv.org/pdf/1910.12430.pdf,1
2019,On the Measure of Intelligence,https://arxiv.org/pdf/1911.01547.pdf,2
2019,On the Measure of Intelligence (ARC),https://arxiv.org/pdf/1911.01547.pdf,1
2020,End-to-End Object Detection with Transformers (DETR),https://arxiv.org/pdf/2005.12872.pdf,3
2020,Generative Pretraining from Pixels (iGPT),https://arxiv.org/pdf/2005.12872.pdf,1
2020,An Image Is Worth 16x16 Words (ViT),https://arxiv.org/pdf/2010.11929.pdf,2
2020,An Image is Worth 16Ã—16 Words: Vision Transformer (ViT),https://arxiv.org/pdf/2010.11929.pdf,1
2021,Is Space-Time Attention All You Need for Video Understanding? (TimeSformer),https://arxiv.org/pdf/2102.05095.pdf,1
2021,TimeSformer: Is Space-Time Attention All You Need for Video Understanding?,https://arxiv.org/pdf/2102.05095.pdf,1
2021,Zero-Shot Text-to-Image Generation,https://arxiv.org/pdf/2102.12092.pdf,2
2021,Zero-Shot Text-to-Image Generation (DALL-E),https://arxiv.org/pdf/2102.12092.pdf,1
2021,Pyramid Vision Transformer (PVT),https://arxiv.org/pdf/2102.12122.pdf,2
2021,Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions,https://arxiv.org/pdf/2102.12122.pdf,2
2021,MATH: Measuring Mathematical Problem Solving,https://arxiv.org/pdf/2103.03874.pdf,1
2021,Measuring Mathematical Problem Solving With the MATH Dataset,https://arxiv.org/pdf/2103.03874.pdf,1
2021,Efficient Large-Scale Language Model Training on GPU Clusters,https://arxiv.org/pdf/2104.04473.pdf,1
2021,Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM,https://arxiv.org/pdf/2104.04473.pdf,1
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding,https://arxiv.org/pdf/2104.09864.pdf,3
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding (RoPE),https://arxiv.org/pdf/2104.09864.pdf,2
2021,SegFormer,https://arxiv.org/pdf/2105.15203.pdf,2
2021,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,https://arxiv.org/pdf/2105.15203.pdf,2
2021,GSM8K: Training Verifiers to Solve Math Word Problems,https://arxiv.org/pdf/2110.14168.pdf,1
2021,Training Verifiers to Solve Math Word Problems (GSM8K),https://arxiv.org/pdf/2110.14168.pdf,1
2022,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering (ScienceQA),https://arxiv.org/pdf/2209.09513.pdf,1
2022,"ScienceQA (""Learn to Explain..."")",https://arxiv.org/pdf/2209.09513.pdf,1
2022,ScienceQA: Benchmark for Multimodal Reasoning,https://arxiv.org/pdf/2209.09513.pdf,1
2023,Tree of Thoughts (ToT),https://arxiv.org/pdf/2305.10601.pdf,2
2023,Tree of Thoughts (ToT): Deliberate Problem Solving with Large Language Models,https://arxiv.org/pdf/2305.10601.pdf,1
2023,MMBench: Evaluating Multimodal LLMs,https://arxiv.org/pdf/2307.06281.pdf,1
2023,MMBench: Is Your Multi-modal Model an All-around Player?,https://arxiv.org/pdf/2307.06281.pdf,2
2023,MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,https://arxiv.org/pdf/2311.16502.pdf,2
2023,MMMU: A Massive Multidiscipline Multimodal Benchmark,https://arxiv.org/pdf/2311.16502.pdf,1
2024,Rotary Position Embedding for Vision Transformer,https://arxiv.org/pdf/2403.13298.pdf,1
2024,Rotary Position Embedding for Vision Transformer (RoPE-Mixed / 2D RoPE study),https://arxiv.org/pdf/2403.13298.pdf,1
