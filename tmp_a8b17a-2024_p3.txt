                           each absolute position i into position vector p and adds word embeddings to their corresponding
                                                                        i
                           pi, before feeding them to the LLMs Vaswani et al. (2017). However, the extrapolation ability is
                           limited because it cannot generalize to unseen positions. RoPE Su et al. (2023) rotates the query and
                           key vectors with an angle proportional to their absolute positions, so the attention dot production
                           only depends on relative distance between tokens, providing a relative positional encoding. T5’s
                           Relative Bias first maps the relative distance (i − j) between tokens at positions i and j to a scalar
                           bias value b = f(i − j), where f is a lookup table. ALiBi is similar to T5’s Relative Bias but
                           instead subtracts a scalar bias from the attention score Press et al. (2021)(refer to Appendix D.2
                           for more details). Recent works such as ReRoPE and Leaky-ReRoPE Su (2023b) achieve effective
                           extrapolation without fine-tuning by meticulously weaving relative positions of RoPE. We refer to
                           this class of methods that achieve extrapolation by weaving the relative positions of PE without
                           fine-tuning as Weave PE.
                           Kazemnejadetal. (2023) indicates that the decoder-only transformer with position encoding removed
                           (NoPE)demonstrates stronger extrapolation capabilities. Furthermore, it theoretically shows that a
                           specific transformer model can get relative and absolute positional information, even in the absence
                           of PE. Haviv et al. (2022) also demonstrates NoPE achieves comparative performance to standard
                           Transformer models. These new studies pose a key challenge regarding the choice of whether
                           using PE or not in Transformer architecture (refer to Appendix A for more related works).
                           3   ModelExtrapolation: NoPEvs. WeavePE
                           3.1  ProblemDefinition
                           Wemainly consider relative PE methods and formally define their self-attention dot product as a
                           function fPE, which takes the query qt located on position t, the key ki located on position i, and
                           their relative positions t − i as input parameters, as follows:
                                                             ⟨q ,k ⟩ := f  (q ,k ,t −i),                                (1)
                                                               t  i      PE t i
                           where f    denotes a relative PE method such as RoPE or ALiBi.
                                  PE
                           For ALiBi,
                                              ⟨q ,k ⟩ := f      (q ,k ,t −i) = qTk −(t−i)·Cm+1,
                                                t   i     ALiBi   t  i           t  i
                                                           −2−log2(#heads+3)
                           where mis head index and C = 2                   .
                           For NoPE,
                                                                  ⟨q ,k ⟩ := qTk .
                                                                    t   i      t  i
                           Based on Equ.1 we formally define weave PE as follows:
                                               ⟨q ,k ⟩ := f       (q ,k ,t −i) = f   (q ,k ,W(t−i)),                    (2)
                                                  t  i     weavePE  t  i          PE t i
                           where W is a weave function which takes the relative position t − i as input parameter.
                           For example, ReRoPE Su (2023b) can be considered as an example of weave PE, with its W function
                           defined as follows:                       
                                                        W(t−i):=        t −i   ,  t −i ≤ N
                                                                           N , t−i>N
                           where N is a constant. ReRoPE’s dot-product attention is:
                                   ⟨q ,k ⟩ := f        (q ,k ,t −i) = f       (q ,k ,W(t−i)) = qTRW(t−i)θk ,
                                      t  i      ReRoPE t     i           RoPE t i                   t           i
                           where R is a rotation matrix that rotates W(t − i)θ radians. This is based on RoPE’s dot-product
                           attention:
                                                   ⟨q ,k ⟩ := f     (q ,k ,t −i)) = qTR(t−i)θk .
                                                     t  i      RoPE t i                t         i
                           3.2  Motivation
                           Chenetal. (2023) explores the evolution of hidden state values and reveals a noticable phenomenon:
                           as the position increases, the hidden state values will explode. This finding appears consistent with
                           observed failures in extrapolation. Through probe experiments (refer to Appendix F), we investigate
                                                                          3
