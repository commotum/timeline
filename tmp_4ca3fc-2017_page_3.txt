                                                    Figure 1: The Transformer - model architecture.
                           wise fully connected feed-forward network. We employ a residual connection [10] around each of
                           the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
                           LayerNorm(x+Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer
                           itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
                           layers, produce outputs of dimension d     =512.
                                                                 model
                           Decoder:    ThedecoderisalsocomposedofastackofN = 6identicallayers. Inadditiontothetwo
                           sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
                           attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
                           around each of the sub-layers, followed by layer normalization. We also modify the self-attention
                           sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
                           masking, combined with fact that the output embeddings are offset by one position, ensures that the
                           predictions for position i can depend only on the known outputs at positions less than i.
                           3.2  Attention
                           Anattention function can be described as mapping a query and a set of key-value pairs to an output,
                           where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
                           of the values, where the weight assigned to each value is computed by a compatibility function of the
                           query with the corresponding key.
                           3.2.1  Scaled Dot-Product Attention
                           Wecall our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of
                           queries and keys of dimension d , and values of dimension d . We compute the dot products of the
                                                           k                           v
                                                                           3
