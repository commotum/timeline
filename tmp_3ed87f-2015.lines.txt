

=== Page 1 ===

LETTER
doi:10.1038/nature14236
Human-levelcontrolthroughdeepreinforcement
learning
1111111
VolodymyrMnih*,KorayKavukcuoglu*,DavidSilver*,AndreiA.Rusu,JoelVeness,MarcG.Bellemare,AlexGraves,
1111111
MartinRiedmiller,AndreasK.Fidjeland,GeorgOstrovski,StigPetersen,CharlesBeattie,AmirSadik,IoannisAntonoglou,
11111
HelenKing,DharshanKumaran,DaanWierstra,ShaneLegg&DemisHassabis
1
Thetheoryofreinforcementlearningprovidesanormativeaccount,agentistoselectactionsinafashionthatmaximizescumulativefuture
23
deeplyrootedinpsychologicalandneuroscientificperspectivesonreward.Moreformally,weuseadeepconvolutionalneuralnetworkto
animalbehaviour,ofhowagentsmayoptimizetheircontrolofanapproximatetheoptimalaction-valuefunction
!"
environment.Tousereinforcementlearningsuccessfullyinsituations
!2
Q"s,a#~maxrzcrzcrz...js~s,a~a,p,
ttz1tz2tt
approachingreal-worldcomplexity,however,agentsareconfrontedp
withadifficulttask:theymustderiveefficientrepresentationsofthe
whichisthemaximumsumofrewardsrdiscountedbycateachtime-
t
environmentfromhigh-dimensionalsensoryinputs,andusethese
stept,achievablebyabehaviourpolicyp5P\(ajs,aftermakingan
togeneralizepastexperiencetonewsituations.Remarkably,humans9
observation\(sandtakinganaction\(a\(seeMethods1.
andotheranimalsseemtosolvethisproblemthroughaharmonious
Reinforcementlearningisknowntobeunstableoreventodiverge
combinationofreinforcementlearningandhierarchicalsensorypro-
whenanonlinearfunctionapproximatorsuchasaneuralnetworkis
4,5
cessingsystems,theformerevidencedbyawealthofneuraldata20
usedtorepresenttheaction-value\(alsoknownasQfunction.This
revealingnotableparallelsbetweenthephasicsignalsemittedbydopa-
instabilityhasseveralcauses:thecorrelationspresentinthesequence
minergicneuronsandtemporaldifferencereinforcementlearning
ofobservations,thefactthatsmallupdatestoQmaysignificantlychange
3
algorithms.Whilereinforcementlearningagentshaveachievedsome
thepolicyandthereforechangethedatadistribution,andthecorrelations
6Ð8
00
successesinavarietyofdomains,theirapplicabilityhaspreviously
betweentheaction-values\(QandthetargetvaluesrzcmaxQ"s,a#.
0
a
beenlimitedtodomainsinwhichusefulfeaturescanbehandcrafted,
WeaddresstheseinstabilitieswithanovelvariantofQ-learning,which
ortodomainswithfullyobserved,low-dimensionalstatespaces.
usestwokeyideas.First,weusedabiologicallyinspiredmechanism
9Ð11
Hereweuserecentadvancesintrainingdeepneuralnetworksto
21Ð23
termedexperiencereplaythatrandomizesoverthedata,thereby
developanovelartificialagent,termedadeepQ-network,thatcan
removingcorrelationsintheobservationsequenceandsmoothingover
learnsuccessfulpoliciesdirectlyfromhigh-dimensionalsensoryinputs
changesinthedatadistribution\(seebelowfordetails.Second,weused
usingend-to-endreinforcementlearning.Wetestedthisagenton
aniterativeupdatethatadjuststheaction-values\(Qtowardstarget
12
thechallengingdomainofclassicAtari2600games.Wedemon-
valuesthatareonlyperiodicallyupdated,therebyreducingcorrelations
stratethatthedeepQ-networkagent,receivingonlythepixelsand
withthetarget.
thegamescoreasinputs,wasabletosurpasstheperformanceofall
Whileotherstablemethodsexistfortrainingneuralnetworksinthe
previousalgorithmsandachievealevelcomparabletothatofapro-
24
reinforcementlearningsetting,suchasneuralfittedQ-iteration,these
fessionalhumangamestesteracrossasetof49games,usingthesame
methodsinvolvetherepeatedtrainingofnetworksdenovoonhundreds
algorithm,networkarchitectureandhyperparameters.Thiswork
ofiterations.Consequently,thesemethods,unlikeouralgorithm,are
bridgesthedividebetweenhigh-dimensionalsensoryinputsand
tooinefficienttobeusedsuccessfullywithlargeneuralnetworks.We
actions,resultinginthefirstartificialagentthatiscapableoflearn-
parameterizeanapproximatevaluefunctionQ\(s,a;husingthedeep
i
ingtoexcelatadiversearrayofchallengingtasks.
convolutionalneuralnetworkshowninFig.1,inwhichharetheparam-
i
Wesetouttocreateasinglealgorithmthatwouldbeabletodevelop
eters\(thatis,weightsoftheQ-networkatiterationi.Toperform
awiderangeofcompetenciesonavariedrangeofchallengingtasksÑa
experiencereplaywestoretheagentÕsexperiencese5\(s,a,r,sa
ttttt11
13
centralgoalofgeneralartificialintelligencethathaseludedprevious
teachtime-steptinadatasetD5{e,É,e}.Duringlearning,we
t1t
8,14,15
efforts.Toachievethis,wedevelopedanovelagent,adeepQ-network
applyQ-learningupdates,onsamples\(orminibatchesofexperience
\(DQN,whichisabletocombinereinforcementlearningwithaclass
\(s,a,r,s9,U\(D,drawnuniformlyatrandomfromthepoolofstored
16
ofartificialneuralnetworkknownasdeepneuralnetworks.Notably,
samples.TheQ-learningupdateatiterationiusesthefollowingloss
9Ð11
recentadvancesindeepneuralnetworks,inwhichseverallayersof
function:
nodesareusedtobuildupprogressivelymoreabstractrepresentations
"#
#$
2
ofthedata,havemadeitpossibleforartificialneuralnetworkstolearn
00{
L"h#~0rzcmaxQ\(s,a;h{Q"s,a;h#
ii"s,a,r,s#*U"D#i
i
conceptssuchasobjectcategoriesdirectlyfromrawsensorydata.We
0
a
useoneparticularlysuccessfularchitecture,thedeepconvolutional
17
network,whichuseshierarchicallayersoftiledconvolutionalfiltersinwhichcisthediscountfactordeterminingtheagentÕshorizon,hare
i
{
tomimictheeffectsofreceptivefieldsÑinspiredbyHubelandWieselÕstheparametersoftheQ-networkatiterationiandharethenetwork
i
18
seminalworkonfeedforwardprocessinginearlyvisualcortexÑtherebyparametersusedtocomputethetargetatiterationi.Thetargetnet-
{
exploitingthelocalspatialcorrelationspresentinimages,andbuildingworkparametershareonlyupdatedwiththeQ-networkparameters
i
inrobustnesstonaturaltransformationssuchaschangesofviewpoint\(heveryCstepsandareheldfixedbetweenindividualupdates\(see
i
orscale.Methods.
WeconsidertasksinwhichtheagentinteractswithanenvironmentToevaluateourDQNagent,wetookadvantageoftheAtari2600
throughasequenceofobservations,actionsandrewards.Thegoaloftheplatform,whichoffersadiversearrayoftasks\(n549designedtobe
1
GoogleDeepMind,5NewStreetSquare,LondonEC4A3TW,UK.
*Theseauthorscontributedequallytothiswork.
26FEBRUARY2015|VOL518|NATURE|529
©2015Macmillan Publishers Limited. All rights reserved


=== Page 2 ===

RESEARCHLETTER
ConvolutionConvolutionFully connectedFully connected
No input
Figure1|Schematicillustrationoftheconvolutionalneuralnetwork.Thesymbolizesslidingofeachfilteracrossinputimageandtwofullyconnected
detailsofthearchitectureareexplainedintheMethods.Theinputtotheneurallayerswithasingleoutputforeachvalidaction.Eachhiddenlayerisfollowed
networkconsistsofan8438434imageproducedbythepreprocessingbyarectifiernonlinearity\(thatis,max"0,x#.
mapw,followedbythreeconvolutionallayers\(note:snakingblueline
difficultandengagingforhumanplayers.WeusedthesamenetworkWecomparedDQNwiththebestperformingmethodsfromthe
architecture,hyperparametervalues\(seeExtendedDataTable1andreinforcementlearningliteratureonthe49gameswhereresultswere
12,15
learningprocedurethroughoutÑtakinghigh-dimensionaldata\(210|160available.Inadditiontothelearnedagents,wealsoreportscoresfor
colourvideoat60HzasinputÑtodemonstratethatourapproachaprofessionalhumangamestesterplayingundercontrolledconditions
robustlylearnssuccessfulpoliciesoveravarietyofgamesbasedsolelyandapolicythatselectsactionsuniformlyatrandom\(ExtendedData
onsensoryinputswithonlyveryminimalpriorknowledge\(thatis,merelyTable2andFig.3,denotedby100%\(humanand0%\(randomony
theinputdatawerevisualimages,andthenumberofactionsavailableaxis;seeMethods.OurDQNmethodoutperformsthebestexisting
ineachgame,butnottheircorrespondences;seeMethods.Notably,reinforcementlearningmethodson43ofthegameswithoutincorpo-
ourmethodwasabletotrainlargeneuralnetworksusingareinforce-ratinganyoftheadditionalpriorknowledgeaboutAtari2600games
mentlearningsignalandstochasticgradientdescentinastablemannerÑusedbyotherapproaches\(forexample,refs12,15.Furthermore,our
illustratedbythetemporalevolutionoftwoindicesoflearning\(theDQNagentperformedatalevelthatwascomparabletothatofapro-
agentÕsaveragescore-per-episodeandaveragepredictedQ-values;seefessionalhumangamestesteracrossthesetof49games,achievingmore
Fig.2andSupplementaryDiscussionfordetails.than75%ofthehumanscoreonmorethanhalfofthegames\(29games;
b
a 2,200 6,000
e 2,000e
dd
oo 5,000
 1,800
ss
ii
pp
 1,600
ee
  
 4,000
rr
 1,400
ee
pp
  
 1,200
ee
rr 3,000
oo
 1,000
cc
ss
  
 800
ee 2,000
gg
 600
aa
rr
ee
 400
 1,000
vv
AA
 200
 0
 0
 0 20 40 60 80 100 120 140 160 180 200 0 20 40 60 80 100 120 140 160 180 200
Training epochsTraining epochs
c 10d 11
 10
 9
T
T
Q
\(Q 9
 \(
 8
 
e
e
u 8
lu
 7l
a
a
v 7
 v
 6 
n
n
 6
o
io
i
t 5
t
c
c 5
a
 a
 4 
e
e 4
g
g
a 3
a
r 3
r
e
e
v 2
v 2
A
A
 1
 1
 0 0
 0 20 40 60 80 100 120 140 160 180 200 0 20 40 60 80 100 120 140 160 180 200
raining epochsraining epochs
Figure2|TrainingcurvestrackingtheagentÕsaveragescoreandaverageonthecurveistheaverageoftheaction-valueQcomputedovertheheld-out
predictedaction-value.a,Eachpointistheaveragescoreachievedperepisodesetofstates.NotethatQ-valuesarescaledduetoclippingofrewards\(see
aftertheagentisrunwithe-greedypolicy\(e50.05for520kframesonSpaceMethods.d,Averagepredictedaction-valueonSeaquest.SeeSupplementary
Invaders.b,AveragescoreachievedperepisodeforSeaquest.c,AverageDiscussionfordetails.
predictedaction-valueonaheld-outsetofstatesonSpaceInvaders.Eachpoint
530|NATURE|VOL518|26FEBRUARY2015
©2015Macmillan Publishers Limited. All rights reserved


=== Page 3 ===

LETTERRESEARCH
Video Pinball
Boxing
Breakout
Star Gunner
Robotank
Atlantis
Crazy Climber
Gopher
Demon Attack
Name This Game
Krull
Assault
Road Runner
Kangaroo
James Bond
Tennis
Pong
Space Invaders
Beam Rider
Tutankham
Kung-Fu Master
Freeway
Time Pilot
Enduro
Fishing Derby
Up and Down
Ice Hockey
Q*bert
At human-level or above
H.E.R.O.
Asterix
Below human-level
Battle Zone
Wizard of Wor
Chopper Command
Centipede
Bank Heist
River Raid
Zaxxon
Amidar
Alien
Venture
Seaquest
Double Dunk
Bowling
Ms. Pac-Man
Asteroids
Frostbite
Gravitar
DQN
Private Eye
Best linear learner
Montezuma's Revenge
01002003004005006001,0004,500%
Figure3|ComparisonoftheDQNagentwiththebestreinforcementoutperformscompetingmethods\(alsoseeExtendedDataTable2inalmostall
15
learningmethodsintheliterature.TheperformanceofDQNisnormalizedthegames,andperformsatalevelthatisbroadlycomparablewithorsuperior
withrespecttoaprofessionalhumangamestester\(thatis,100%levelandtoaprofessionalhumangamestester\(thatis,operationalizedasalevelof
randomplay\(thatis,0%level.NotethatthenormalizedperformanceofDQN,75%oraboveinthemajorityofgames.Audiooutputwasdisabledforboth
expressedasapercentage,iscalculatedas:1003\(DQNscore2randomplayhumanplayersandagents.Errorbarsindicates.d.acrossthe30evaluation
score/\(humanscore2randomplayscore.ItcanbeseenthatDQNepisodes,startingwithdifferentinitialconditions.
seeFig.3,SupplementaryDiscussionandExtendedDataTable2.Inperceptuallydissimilar\(Fig.4,bottomright,topleftandmiddle,con-
additionalsimulations\(seeSupplementaryDiscussionandExtendedsistentwiththenotionthatthenetworkisabletolearnrepresentations
DataTables3and4,wedemonstratetheimportanceoftheindividualthatsupportadaptivebehaviourfromhigh-dimensionalsensoryinputs.
corecomponentsoftheDQNagentÑthereplaymemory,separatetargetFurthermore,wealsoshowthattherepresentationslearnedbyDQN
Q-networkanddeepconvolutionalnetworkarchitectureÑbydisablingareabletogeneralizetodatageneratedfrompoliciesotherthanits
themanddemonstratingthedetrimentaleffectsonperformance.ownÑinsimulationswherewepresentedasinputtothenetworkgame
WenextexaminedtherepresentationslearnedbyDQNthatunder-statesexperiencedduringhumanandagentplay,recordedtherepre-
pinnedthesuccessfulperformanceoftheagentinthecontextofthegamesentationsofthelasthiddenlayer,andvisualizedtheembeddingsgen-
SpaceInvaders\(seeSupplementaryVideo1forademonstrationoftheeratedbythet-SNEalgorithm\(ExtendedDataFig.1andSupplementary
performanceofDQN,byusingatechniquedevelopedforthevisual-Discussion.ExtendedDataFig.2providesanadditionalillustrationof
25
izationofhigh-dimensionaldatacalledÔt-SNEÕ\(Fig.4.Asexpected,howtherepresentationslearnedbyDQNallowittoaccuratelypredict
thet-SNEalgorithmtendstomaptheDQNrepresentationofpercep-stateandactionvalues.
tuallysimilarstatestonearbypoints.Interestingly,wealsofoundinstancesItisworthnotingthatthegamesinwhichDQNexcelsareextremely
inwhichthet-SNEalgorithmgeneratedsimilarembeddingsforDQNvariedintheirnature,fromside-scrollingshooters\(RiverRaidtobox-
representationsofstatesthatarecloseintermsofexpectedrewardbutinggames\(Boxingandthree-dimensionalcar-racinggames\(Enduro.
26FEBRUARY2015|VOL518|NATURE|531
©2015Macmillan Publishers Limited. All rights reserved


=== Page 4 ===

RESEARCHLETTER
V
Figure4|Two-dimensionalt-SNEembeddingoftherepresentationsinthepredictshighstatevaluesforbothfull\(toprightscreenshotsandnearly
lasthiddenlayerassignedbyDQNtogamestatesexperiencedwhileplayingcompletescreens\(bottomleftscreenshotsbecauseithaslearnedthat
SpaceInvaders.TheplotwasgeneratedbylettingtheDQNagentplayforcompletingascreenleadstoanewscreenfullofenemyships.Partially
25
2hofrealgametimeandrunningthet-SNEalgorithmonthelasthiddenlayercompletedscreens\(bottomscreenshotsareassignedlowerstatevaluesbecause
representationsassignedbyDQNtoeachexperiencedgamestate.Thelessimmediaterewardisavailable.Thescreensshownonthebottomright
pointsarecolouredaccordingtothestatevalues\(V,maximumexpectedrewardandtopleftandmiddlearelessperceptuallysimilarthantheotherexamplesbut
ofastatepredictedbyDQNforthecorrespondinggamestates\(rangingarestillmappedtonearbyrepresentationsandsimilarvaluesbecausethe
fromdarkred\(highestVtodarkblue\(lowestV.Thescreenshotsorangebunkersdonotcarrygreatsignificanceneartheendofalevel.With
correspondingtoaselectednumberofpointsareshown.TheDQNagentpermissionfromSquareEnixLimited.
Indeed,incertaingamesDQNisabletodiscoverarelativelylong-termrealizationofsuchaprocessinthemammalianbrain,withthetime-
strategy\(forexample,Breakout:theagentlearnstheoptimalstrategy,compressedreactivationofrecentlyexperiencedtrajectoriesduring
21,22
whichistofirstdigatunnelaroundthesideofthewallallowingtheballofflineperiods\(forexample,wakingrestprovidingaputativemech-
anismbywhichvaluefunctionsmaybeefficientlyupdatedthrough
tobesentaroundthebacktodestroyalargenumberofblocks;seeSup-
22
interactionswiththebasalganglia.Inthefuture,itwillbeimportant
plementaryVideo2forillustrationofdevelopmentofDQNÕsperfor-
toexplorethepotentialuseofbiasingthecontentofexperiencereplay
manceoverthecourseoftraining.Nevertheless,gamesdemandingmore
towardssalientevents,aphenomenonthatcharacterizesempirically
temporallyextendedplanningstrategiesstillconstituteamajorchal-
29
observedhippocampalreplay,andrelatestothenotionofÔprioritized
lengeforallexistingagentsincludingDQN\(forexample,MontezumaÕs
30
sweepingÕinreinforcementlearning.Takentogether,ourworkillus-
Revenge.
tratesthepowerofharnessingstate-of-the-artmachinelearningtech-
Inthiswork,wedemonstratethatasinglearchitecturecansuccess-
niqueswithbiologicallyinspiredmechanismstocreateagentsthatare
fullylearncontrolpoliciesinarangeofdifferentenvironmentswithonly
capableoflearningtomasteradiversearrayofchallengingtasks.
veryminimalpriorknowledge,receivingonlythepixelsandthegame
scoreasinputs,andusingthesamealgorithm,networkarchitectureand
OnlineContentMethods,alongwithanyadditionalExtendedDatadisplayitems
hyperparametersoneachgame,privyonlytotheinputsahumanplayerandSourceData,areavailableintheonlineversionofthepaper;referencesunique
24,26
tothesesectionsappearonlyintheonlinepaper.
wouldhave.Incontrasttopreviouswork,ourapproachincorpo-
ratesÔend-to-endÕreinforcementlearningthatusesrewardtocontinu-
Received10July2014;accepted16January2015.
ouslyshaperepresentationswithintheconvolutionalnetworktowards
salientfeaturesoftheenvironmentthatfacilitatevalueestimation.This
1.Sutton,R.&Barto,A.ReinforcementLearning:AnIntroduction\(MITPress,1998.
2.Thorndike,E.L.AnimalIntelligence:Experimentalstudies\(Macmillan,1911.
principledrawsonneurobiologicalevidencethatrewardsignalsduring
3.Schultz,W.,Dayan,P.&Montague,P.R.Aneuralsubstrateofpredictionand
perceptuallearningmayinfluencethecharacteristicsofrepresentations
reward.Science275,1593Ð1599\(1997.
27,28
withinprimatevisualcortex.Notably,thesuccessfulintegrationof
4.Serre,T.,Wolf,L.&Poggio,T.Objectrecognitionwithfeaturesinspiredbyvisual
reinforcementlearningwithdeepnetworkarchitectureswascriticallycortex.Proc.IEEE.Comput.Soc.Conf.Comput.Vis.Pattern.Recognit.994Ð1000
21Ð23\(2005.
dependentonourincorporationofareplayalgorithminvolvingthe
5.Fukushima,K.Neocognitron:Aself-organizingneuralnetworkmodelfora
storageandrepresentationofrecentlyexperiencedtransitions.Conver-
mechanismofpatternrecognitionunaffectedbyshiftinposition.Biol.Cybern.36,
gentevidencesuggeststhatthehippocampusmaysupportthephysical193Ð202\(1980.
532|NATURE|VOL518|26FEBRUARY2015
©2015Macmillan Publishers Limited. All rights reserved


=== Page 5 ===

LETTERRESEARCH
6.Tesauro,G.TemporaldifferencelearningandTD-Gammon.Commun.ACM38,23.Lin,L.-J.Reinforcementlearningforrobotsusingneuralnetworks.Technical
58Ð68\(1995.Report,DTICDocument\(1993.
7.Riedmiller,M.,Gabel,T.,Hafner,R.&Lange,S.Reinforcementlearningforrobot24.Riedmiller,M.NeuralfittedQiteration-firstexperienceswithadataefficient
soccer.Auton.Robots27,55Ð73\(2009.neuralreinforcementlearningmethod.Mach.Learn.:ECML,3720,317Ð328
8.Diuk,C.,Cohen,A.&Littman,M.L.Anobject-orientedrepresentationforefficient\(Springer,2005.
reinforcementlearning.Proc.Int.Conf.Mach.Learn.240Ð247\(2008.25.VanderMaaten,L.J.P.&Hinton,G.E.Visualizinghigh-dimensionaldatausing
9.Bengio,Y.LearningdeeparchitecturesforAI.FoundationsandTrendsinMachinet-SNE.J.Mach.Learn.Res.9,2579Ð2605\(2008.
Learning2,1Ð127\(2009.
26.Lange,S.&Riedmiller,M.Deepauto-encoderneuralnetworksinreinforcement
10.Krizhevsky,A.,Sutskever,I.&Hinton,G.ImageNetclassificationwithdeep
learning.Proc.Int.Jt.Conf.Neural.Netw.1Ð8\(2010.
convolutionalneuralnetworks.Adv.NeuralInf.Process.Syst.25,1106Ð1114\(2012.
27.Law,C.-T.&Gold,J.I.Reinforcementlearningcanaccountforassociative
11.Hinton,G.E.&Salakhutdinov,R.R.Reducingthedimensionalityofdatawith
andperceptuallearningonavisualdecisiontask.NatureNeurosci.12,655
neuralnetworks.Science313,504Ð507\(2006.
\(2009.
12.Bellemare,M.G.,Naddaf,Y.,Veness,J.&Bowling,M.Thearcadelearning
28.Sigala,N.&Logothetis,N.K.Visualcategorizationshapesfeatureselectivityinthe
environment:Anevaluationplatformforgeneralagents.J.Artif.Intell.Res.47,
primatetemporalcortex.Nature415,318Ð320\(2002.
253Ð279\(2013.
29.Bendor,D.&Wilson,M.A.Biasingthecontentofhippocampalreplayduringsleep.
13.Legg,S.&Hutter,M.UniversalIntelligence:adefinitionofmachineintelligence.
NatureNeurosci.15,1439Ð1444\(2012.
MindsMach.17,391Ð444\(2007.
30.Moore,A.&Atkeson,C.Prioritizedsweeping:reinforcementlearningwithlessdata
14.Genesereth,M.,Love,N.&Pell,B.Generalgameplaying:overviewoftheAAAI
andlessrealtime.Mach.Learn.13,103Ð130\(1993.
competition.AIMag.26,62Ð72\(2005.
SupplementaryInformationisavailableintheonlineversionofthepaper.
15.Bellemare,M.G.,Veness,J.&Bowling,M.Investigatingcontingencyawareness
usingAtari2600games.Proc.Conf.AAAI.Artif.Intell.864Ð871\(2012.
AcknowledgementsWethankG.Hinton,P.DayanandM.Bowlingfordiscussions,
16.McClelland,J.L.,Rumelhart,D.E.&Group,T.P.R.ParallelDistributedProcessing:
A.CainandJ.Keeneforworkonthevisuals,K.KellerandP.Rogersforhelpwiththe
ExplorationsintheMicrostructureofCognition\(MITPress,1986.
visuals,G.Wayneforcommentsonanearlierversionofthemanuscript,andtherestof
17.LeCun,Y.,Bottou,L.,Bengio,Y.&Haffner,P.Gradient-basedlearningappliedto
theDeepMindteamfortheirsupport,ideasandencouragement.
documentrecognition.Proc.IEEE86,2278Ð2324\(1998.
18.Hubel,D.H.&Wiesel,T.N.ShapeandarrangementofcolumnsincatÕsstriate
AuthorContributionsV.M.,K.K.,D.S.,J.V.,M.G.B.,M.R.,A.G.,D.W.,S.L.andD.H.
cortex.J.Physiol.165,559Ð568\(1963.
conceptualizedtheproblemandthetechnicalframework.V.M.,K.K.,A.A.R.andD.S.
19.Watkins,C.J.&Dayan,P.Q-learning.Mach.Learn.8,279Ð292\(1992.
developedandtestedthealgorithms.J.V.,S.P.,C.B.,A.A.R.,M.G.B.,I.A.,A.K.F.,G.O.and
20.Tsitsiklis,J.&Roy,B.V.Ananalysisoftemporal-differencelearningwithfunction
A.S.createdthetestingplatform.K.K.,H.K.,S.L.andD.H.managedtheproject.K.K.,D.K.,
approximation.IEEETrans.Automat.Contr.42,674Ð690\(1997.
D.H.,V.M.,D.S.,A.G.,A.A.R.,J.V.andM.G.B.wrotethepaper.
21.McClelland,J.L.,McNaughton,B.L.&OÕReilly,R.C.Whytherearecomplementary
learningsystemsinthehippocampusandneocortex:insightsfromthesuccessesAuthorInformationReprintsandpermissionsinformationisavailableat
andfailuresofconnectionistmodelsoflearningandmemory.Psychol.Rev.102,www.nature.com/reprints.Theauthorsdeclarenocompetingfinancialinterests.
419Ð457\(1995.Readersarewelcometocommentontheonlineversionofthepaper.Correspondence
22.OÕNeill,J.,Pleydell-Bouverie,B.,Dupret,D.&Csicsvari,J.Playitagain:reactivationandrequestsformaterialsshouldbeaddressedtoK.K.\(korayk@google.comor
ofwakingexperienceandmemory.TrendsNeurosci.33,220Ð229\(2010.D.H.\(demishassabis@google.com.
26FEBRUARY2015|VOL518|NATURE|533
©2015Macmillan Publishers Limited. All rights reserved


=== Page 6 ===

RESEARCHLETTER
Ourexperimentalsetupamountstousingthefollowingminimalpriorknow-
METHODS
ledge:thattheinputdataconsistedofvisualimages\(motivatingouruseofacon-
Preprocessing.WorkingdirectlywithrawAtari2600frames,whichare2103160
volutionaldeepnetwork,thegame-specificscore\(withnomodification,number
pixelimageswitha128-colourpalette,canbedemandingintermsofcomputation
ofactions,althoughnottheircorrespondences\(forexample,specificationofthe
andmemoryrequirements.Weapplyabasicpreprocessingstepaimedatreducing
upÔbuttonÕandthelifecount.
theinputdimensionalityanddealingwithsomeartefactsoftheAtari2600emu-
Evaluationprocedure.Thetrainedagentswereevaluatedbyplayingeachgame
lator.First,toencodeasingleframewetakethemaximumvalueforeachpixelcolour
30timesforupto5mineachtimewithdifferentinitialrandomconditions\(Ôno-
valueovertheframebeingencodedandthepreviousframe.Thiswasnecessaryto
opÕ;seeExtendedDataTable1andane-greedypolicywithe50.05.Thispro-
removeflickeringthatispresentingameswheresomeobjectsappearonlyineven
cedureisadoptedtominimizethepossibilityofoverfittingduringevaluation.The
frameswhileotherobjectsappearonlyinoddframes,anartefactcausedbythe
randomagentservedasabaselinecomparisonandchosearandomactionat10Hz
limitednumberofspritesAtari2600candisplayatonce.Second,wethenextract
whichiseverysixthframe,repeatingitslastactiononinterveningframes.10Hzis
theYchannel,alsoknownasluminance,fromtheRGBframeandrescaleitto
aboutthefastestthatahumanplayercanselecttheÔfireÕbutton,andsettingthe
84384.Thefunctionwfromalgorithm1describedbelowappliesthispreprocess-
randomagenttothisfrequencyavoidsspuriousbaselinescoresinahandfulofthe
ingtothemmostrecentframesandstacksthemtoproducetheinputtothe
games.Wedidalsoassesstheperformanceofarandomagentthatselectedanaction
Q-function,inwhichm54,althoughthealgorithmisrobusttodifferentvaluesof
at60Hz\(thatis,everyframe.Thishadaminimaleffect:changingthenormalized
m\(forexample,3or5.
DQNperformancebymorethan5%inonlysixgames\(Boxing,Breakout,Crazy
Codeavailability.Thesourcecodecanbeaccessedathttps://sites.google.com/a/
Climber,DemonAttack,KrullandRobotank,andinallthesegamesDQNout-
deepmind.com/dqnfornon-commercialusesonly.
performedtheexperthumanbyaconsiderablemargin.
Modelarchitecture.ThereareseveralpossiblewaysofparameterizingQusinga
Theprofessionalhumantesterusedthesameemulatorengineastheagents,and
neuralnetwork.BecauseQmapshistoryÐactionpairstoscalarestimatesoftheir
playedundercontrolledconditions.Thehumantesterwasnotallowedtopause,
Q-value,thehistoryandtheactionhavebeenusedasinputstotheneuralnetwork
saveorreloadgames.AsintheoriginalAtari2600environment,theemulatorwas
24,26
bysomepreviousapproaches.Themaindrawbackofthistypeofarchitecture
runat60Hzandtheaudiooutputwasdisabled:assuch,thesensoryinputwas
isthataseparateforwardpassisrequiredtocomputetheQ-valueofeachaction,
equatedbetweenhumanplayerandagents.Thehumanperformanceistheaverage
resultinginacostthatscaleslinearlywiththenumberofactions.Weinsteadusean
rewardachievedfromaround20episodesofeachgamelastingamaximumof5min
architectureinwhichthereisaseparateoutputunitforeachpossibleaction,and
each,followingaround2hofpracticeplayingeachgame.
onlythestaterepresentationisaninputtotheneuralnetwork.Theoutputscor-
Algorithm.Weconsidertasksinwhichanagentinteractswithanenvironment,
respondtothepredictedQ-valuesoftheindividualactionsfortheinputstate.The
inthiscasetheAtariemulator,inasequenceofactions,observationsandrewards.
mainadvantageofthistypeofarchitectureistheabilitytocomputeQ-valuesforall
Ateachtime-steptheagentselectsanactionafromthesetoflegalgameactions,
t
possibleactionsinagivenstatewithonlyasingleforwardpassthroughthenetwork.
A~f1,...,Kg.Theactionispassedtotheemulatorandmodifiesitsinternalstate
Theexactarchitecture,shownschematicallyinFig.1,isasfollows.Theinputto
andthegamescore.Ingeneraltheenvironmentmaybestochastic.TheemulatorÕs
theneuralnetworkconsistsofan8438434imageproducedbythepreprocess-
internalstateisnotobservedbytheagent;insteadtheagentobservesanimage
ingmapw.Thefirsthiddenlayerconvolves32filtersof838withstride4withthe
d
x[Rfromtheemulator,whichisavectorofpixelvaluesrepresentingthecurrent
t
31,32
inputimageandappliesarectifiernonlinearity.Thesecondhiddenlayercon-
screen.Inadditionitreceivesarewardrrepresentingthechangeingamescore.
t
volves64filtersof434withstride2,againfollowedbyarectifiernonlinearity.
Notethatingeneralthegamescoremaydependonthewholeprevioussequenceof
Thisisfollowedbyathirdconvolutionallayerthatconvolves64filtersof333with
actionsandobservations;feedbackaboutanactionmayonlybereceivedaftermany
stride1followedbyarectifier.Thefinalhiddenlayerisfully-connectedandcon-
thousandsoftime-stepshaveelapsed.
sistsof512rectifierunits.Theoutputlayerisafully-connectedlinearlayerwitha
33
Becausetheagentonlyobservesthecurrentscreen,thetaskispartiallyobserved
singleoutputforeachvalidaction.Thenumberofvalidactionsvariedbetween4
andmanyemulatorstatesareperceptuallyaliased\(thatis,itisimpossibletofully
and18onthegamesweconsidered.
understandthecurrentsituationfromonlythecurrentscreenx.Therefore,
t
Trainingdetails.Weperformedexperimentson49Atari2600gameswhereresults
sequencesofactionsandobservations,s~x,a,x,:::,a,x,areinputtothe
t112t{1t
12,15
wereavailableforallothercomparablemethods.Adifferentnetworkwastrained
algorithm,whichthenlearnsgamestrategiesdependinguponthesesequences.All
oneachgame:thesamenetworkarchitecture,learningalgorithmandhyperpara-
sequencesintheemulatorareassumedtoterminateinafinitenumberoftime-
metersettings\(seeExtendedDataTable1wereusedacrossallgames,showingthat
steps.ThisformalismgivesrisetoalargebutfiniteMarkovdecisionprocess\(MDPi
ourapproachisrobustenoughtoworkonavarietyofgameswhileincorporating
nwhicheachsequenceisadistinctstate.Asaresult,wecanapplystandardrein-
onlyminimalpriorknowledge\(seebelow.Whileweevaluatedouragentsonunmodi-
forcementlearningmethodsforMDPs,simplybyusingthecompletesequences
t
fiedgames,wemadeonechangetotherewardstructureofthegamesduringtraining
asthestaterepresentationattimet.
only.Asthescaleofscoresvariesgreatlyfromgametogame,weclippedallposi-
Thegoaloftheagentistointeractwiththeemulatorbyselectingactionsinaway
tiverewardsat1andallnegativerewardsat21,leaving0rewardsunchanged.
thatmaximizesfuturerewards.Wemakethestandardassumptionthatfuturerewards
Clippingtherewardsinthismannerlimitsthescaleoftheerrorderivativesand
arediscountedbyafactorofcpertime-step\(cwassetto0.99throughout,and
makesiteasiertousethesamelearningrateacrossmultiplegames.Atthesametime,T
X
0
t{t
definethefuturediscountedreturnattimetasR~cr0,inwhichTisthe
itcouldaffecttheperformanceofouragentsinceitcannotdifferentiatebetween
tt
0
t~t
rewardsofdifferentmagnitude.Forgameswherethereisalifecounter,theAtari
time-stepatwhichthegameterminates.Wedefinetheoptimalaction-value
2600emulatoralsosendsthenumberoflivesleftinthegame,whichisthenusedto
!
functionQ"s,a#asthemaximumexpectedreturnachievablebyfollowingany
marktheendofanepisodeduringtraining.
!
policy,afterseeingsomesequencesandthentakingsomeactiona,Q"s,a#~
Intheseexperiments,weusedtheRMSProp\(seehttp://www.cs.toronto.edu/
max$RDs~s,a~a,p%inwhichpisapolicymappingsequencestoactions\(or
pttt
,tijmen/csc321/slides/lecture_slides_lec6.pdfalgorithmwithminibatchesofsize
distributionsoveractions.
32.Thebehaviourpolicyduringtrainingwase-greedywitheannealedlinearly
Theoptimalaction-valuefunctionobeysanimportantidentityknownasthe
from1.0to0.1overthefirstmillionframes,andfixedat0.1thereafter.Wetrained
Bellmanequation.Thisisbasedonthefollowingintuition:iftheoptimalvalue
foratotalof50millionframes\(thatis,around38daysofgameexperienceintotala
!00
Q"s,a#ofthesequences9atthenexttime-stepwasknownforallpossibleactions
ndusedareplaymemoryof1millionmostrecentframes.
a9,thentheoptimalstrategyistoselecttheactiona9maximizingtheexpectedvalue
FollowingpreviousapproachestoplayingAtari2600games,wealsouseasimple
!00
ofrzcQ"s,a#:
15
frame-skippingtechnique.Moreprecisely,theagentseesandselectsactionson
%&
everykthframeinsteadofeveryframe,anditslastactionisrepeatedonskipped
!!00
Q"s,a#~0rzcmaxQ"s,a#Ds,a
s
0
frames.Becauserunningtheemulatorforwardforonesteprequiresmuchlessa
computationthanhavingtheagentselectanaction,thistechniqueallowstheagent
toplayroughlyktimesmoregameswithoutsignificantlyincreasingtheruntime.
Thebasicideabehindmanyreinforcementlearningalgorithmsistoestimate
Weusek54forallgames.
theaction-valuefunctionbyusingtheBellmanequationasaniterativeupdate,
00
ThevaluesofallthehyperparametersandoptimizationparameterswereselectedQ"s,a#~0$rzcmax0Q"s,a#Ds,a%.Suchvalueiterationalgorithmsconverge
sa
iz1i
!
byperforminganinformalsearchonthegamesPong,Breakout,Seaquest,Spacetotheoptimalaction-valuefunction,Q?Qasi??.Inpractice,thisbasicapproach
i
InvadersandBeamRider.Wedidnotperformasystematicgridsearchowingtoisimpractical,becausetheaction-valuefunctionisestimatedseparatelyforeach
thehighcomputationalcost.Theseparameterswerethenheldfixedacrossallothersequence,withoutanygeneralization.Instead,itiscommontouseafunctionapprox-
!
games.ThevaluesanddescriptionsofallhyperparametersareprovidedinExtendedimatortoestimatetheaction-valuefunction,Q"s,a;h#<Q"s,a#.Inthereinforce-
DataTable1.mentlearningcommunitythisistypicallyalinearfunctionapproximator,but
©2015Macmillan Publishers Limited. All rights reserved


=== Page 7 ===

LETTERRESEARCH
sometimesanonlinearfunctionapproximatorisusedinstead,suchasaneuralreplaythebehaviourdistributionisaveragedovermanyofitspreviousstates,
network.Werefertoaneuralnetworkfunctionapproximatorwithweightshasasmoothingoutlearningandavoidingoscillationsordivergenceintheparameters.
Q-network.AQ-networkcanbetrainedbyadjustingtheparametershatiterationNotethatwhenlearningbyexperiencereplay,itisnecessarytolearnoff-policy
i
itoreducethemean-squarederrorintheBellmanequation,wheretheoptimal\(becauseourcurrentparametersaredifferenttothoseusedtogeneratethesam-
!00
targetvaluesrzcmax0Q"s,a#aresubstitutedwithapproximatetargetvalues
ple,whichmotivatesthechoiceofQ-learning.
a
'\(
{{
00
y~rzcmax0Qs,a;h,usingparametershfromsomepreviousiteration.
Inpractice,ouralgorithmonlystoresthelastNexperiencetuplesinthereplay
a
ii
ThisleadstoasequenceoflossfunctionsL\(hthatchangesateachiterationi,
memory,andsamplesuniformlyatrandomfromDwhenperformingupdates.This
ii
!"
approachisinsomerespectslimitedbecausethememorybufferdoesnotdiffer-
2
L"h#~"E0$yDs,a%{Q"s,a;h##
iis,a,rsi
entiateimportanttransitionsandalwaysoverwriteswithrecenttransitionsowing
!"
2
tothefinitememorysizeN.Similarly,theuniformsamplinggivesequalimpor-
~0"y{Q"s,a;h##zE$V0$y%%:
s,a,r,sis,a,rs
tancetoalltransitionsinthereplaymemory.Amoresophisticatedsamplingstrat-
egymightemphasizetransitionsfromwhichwecanlearnthemost,similarto
Notethatthetargetsdependonthenetworkweights;thisisincontrastwiththe
30
prioritizedsweeping.
targetsusedforsupervisedlearning,whicharefixedbeforelearningbegins.At
2
ThesecondmodificationtoonlineQ-learningaimedatfurtherimprovingthe
eachstageofoptimization,weholdtheparametersfromthepreviousiterationh
i
stabilityofourmethodwithneuralnetworksistouseaseparatenetworkforgen-
fixedwhenoptimizingtheithlossfunctionL\(h,resultinginasequenceofwell-
ii
eratingthetargetsyintheQ-learningupdate.Moreprecisely,everyCupdateswe
j
definedoptimizationproblems.Thefinaltermisthevarianceofthetargets,which
^^
clonethenetworkQtoobtainatargetnetworkQanduseQforgeneratingthe
doesnotdependontheparametershthatwearecurrentlyoptimizing,andmay
i
Q-learningtargetsyforthefollowingCupdatestoQ.Thismodificationmakesthe
j
thereforebeignored.Differentiatingthelossfunctionwithrespecttotheweights
algorithmmorestablecomparedtostandardonlineQ-learning,whereanupdate
wearriveatthefollowinggradient:
thatincreasesQ\(s,aoftenalsoincreasesQ\(s,aforallaandhencealsoincreases
%#$&
ttt11
'\(
{
00thetargety,possiblyleadingtooscillationsordivergenceofthepolicy.Generating
j
+L"h#~0rzcmaxQs,a;h{Q"s,a;h#+Q"s,a;h#:
his,a,r,sihi
iii
0
a
thetargetsusinganoldersetofparametersaddsadelaybetweenthetimeanupdate
toQismadeandthetimetheupdateaffectsthetargetsy,makingdivergenceor
j
Ratherthancomputingthefullexpectationsintheabovegradient,itisoften
oscillationsmuchmoreunlikely.
computationallyexpedienttooptimizethelossfunctionbystochasticgradient
Wealsofoundithelpfultocliptheerrortermfromtheupdaterzcmax0Q
a
'\(
19
{
00
descent.ThefamiliarQ-learningalgorithmcanberecoveredinthisframework
s,a;h{Q"s,a;h#tobebetween21and1.Becausetheabsolutevalueloss
i
i
byupdatingtheweightsaftereverytimestep,replacingtheexpectationsusing
functionjxjhasaderivativeof21forallnegativevaluesofxandaderivativeof1
{
singlesamples,andsettingh~h.
i{1
iforallpositivevaluesofx,clippingthesquarederrortobebetween21and1cor-
Notethatthisalgorithmismodel-free:itsolvesthereinforcementlearningtask
respondstousinganabsolutevaluelossfunctionforerrorsoutsideofthe\(21,1i
directlyusingsamplesfromtheemulator,withoutexplicitlyestimatingthereward
nterval.Thisformoferrorclippingfurtherimprovedthestabilityofthealgorithm.
0
andtransitiondynamicsP"r,sDs,a#.Itisalsooff-policy:itlearnsaboutthegreedy
Algorithm1:deepQ-learningwithexperiencereplay.
0
policya~argmaxQ"s,a;h#,whilefollowingabehaviourdistributionthatensures
0
a
InitializereplaymemoryDtocapacityN
adequateexplorationofthestatespace.Inpractice,thebehaviourdistributionis
Initializeaction-valuefunctionQwithrandomweightsh
oftenselectedbyane-greedypolicythatfollowsthegreedypolicywithprobability2
^
Initializetargetaction-valuefunctionQwithweightsh5h
12eandselectsarandomactionwithprobabilitye.
Forepisode51,Mdo
TrainingalgorithmfordeepQ-networks.Thefullalgorithmfortrainingdeep
Initializesequences~fxgandpreprocessedsequencew~w"s#
111
1
Q-networksispresentedinAlgorithm1.Theagentselectsandexecutesactions
Fort51,Tdo
accordingtoane-greedypolicybasedonQ.Becauseusinghistoriesofarbitrary
Withprobabilityeselectarandomactiona
t
lengthasinputstoaneuralnetworkcanbedifficult,ourQ-functioninsteadworks
otherwiseselecta~argmaxQ"w"s#,a;h#
tt
a
onafixedlengthrepresentationofhistoriesproducedbythefunctionwdescribed
Executeactionainemulatorandobserverewardrandimagex
ttt11
above.ThealgorithmmodifiesstandardonlineQ-learningintwowaystomakeit
Sets~s,a,xandpreprocessw~w"s#
tz1tttz1tz1
tz1
suitablefortraininglargeneuralnetworkswithoutdiverging.'\(
23Storetransitionw,a,r,winD
tt*
ttz1
First,weuseatechniqueknownasexperiencereplayinwhichwestorethe
Samplerandomminibatchoftransitionsw,a,r,wfromD
jj
agentÕsexperiencesateachtime-step,e5\(s,a,r,s,inadatasetD5{e,É,e},jjz1
ttttt11t1t
\(
pooledovermanyepisodes\(wheretheendofanepisodeoccurswhenatermi-
rifepisodeterminatesatstepjz1
j
*
Sety~
nalstateisreachedintoareplaymemory.Duringtheinnerloopofthealgorithm,j{
0
^
rzcmax0Qw,a;hotherwise
ja
jz1
weapplyQ-learningupdates,orminibatchupdates,tosamplesofexperience,
**
2
\(s,a,r,s9,U\(D,drawnatrandomfromthepoolofstoredsamples.ThisapproachPerformagradientdescentstepony{Qw,a;hwithrespecttothe
jj
j
hasseveraladvantagesoverstandardonlineQ-learning.First,eachstepofexperiencenetworkparametersh
^
ispotentiallyusedinmanyweightupdates,whichallowsforgreaterdataefficiency.
EveryCstepsresetQ~Q
Second,learningdirectlyfromconsecutivesamplesisinefficient,owingtothestrong
EndFor
correlationsbetweenthesamples;randomizingthesamplesbreaksthesecorrela-
EndFor
tionsandthereforereducesthevarianceoftheupdates.Third,whenlearningon-
31.Jarrett,K.,Kavukcuoglu,K.,Ranzato,M.A.&LeCun,Y.Whatisthebestmulti-stage
policythecurrentparametersdeterminethenextdatasamplethattheparameters
architectureforobjectrecognition?Proc.IEEE.Int.Conf.Comput.Vis.2146Ð2153
aretrainedon.Forexample,ifthemaximizingactionistomoveleftthenthetrain-
\(2009.
ingsampleswillbedominatedbysamplesfromtheleft-handside;ifthemaximiz-
32.Nair,V.&Hinton,G.E.RectifiedlinearunitsimproverestrictedBoltzmann
ingactionthenswitchestotherightthenthetrainingdistributionwillalsoswitch.
machines.Proc.Int.Conf.Mach.Learn.807Ð814\(2010.
Itiseasytoseehowunwantedfeedbackloopsmayariseandtheparameterscouldget
33.Kaelbling,L.P.,Littman,M.L.&Cassandra,A.R.Planningandactinginpartially
20
stuckinapoorlocalminimum,orevendivergecatastrophically.Byusingexperienceobservablestochasticdomains.ArtificialIntelligence101,99Ð134\(1994.
©2015Macmillan Publishers Limited. All rights reserved


=== Page 8 ===

RESEARCHLETTER
ExtendedDataFigure1|Two-dimensionalt-SNEembeddingofthepointsandDQNplay\(bluepointssuggeststhattherepresentationslearned
representationsinthelasthiddenlayerassignedbyDQNtogamestatesbyDQNdoindeedgeneralizetodatageneratedfrompoliciesotherthanits
experiencedduringacombinationofhumanandagentplayinSpaceown.Thepresenceinthet-SNEembeddingofoverlappingclustersofpoints
25
Invaders.Theplotwasgeneratedbyrunningthet-SNEalgorithmonthelastcorrespondingtothenetworkrepresentationofstatesexperiencedduring
hiddenlayerrepresentationassignedbyDQNtogamestatesexperiencedhumanandagentplayshowsthattheDQNagentalsofollowssequencesof
duringacombinationofhuman\(30minandagent\(2hplay.Thefactthatstatessimilartothosefoundinhumanplay.Screenshotscorrespondingto
thereissimilarstructureinthetwo-dimensionalembeddingscorrespondingtoselectedstatesareshown\(human:orangeborder;DQN:blueborder.
theDQNrepresentationofstatesexperiencedduringhumanplay\(orange
©2015Macmillan Publishers Limited. All rights reserved


=== Page 9 ===

LETTERRESEARCH
ExtendedDataFigure2|Visualizationoflearnedvaluefunctionsontwoallactionsarearound0.7,reflectingtheexpectedvalueofthisstatebasedon
games,BreakoutandPong.a,Avisualizationofthelearnedvaluefunctiononpreviousexperience.Attimepoint2,theagentstartsmovingthepaddle
thegameBreakout.Attimepoints1and2,thestatevalueispredictedtobe,17towardstheballandthevalueoftheÔupÕactionstayshighwhilethevalueofthe
andtheagentisclearingthebricksatthelowestlevel.EachofthepeaksinÔdownÕactionfallsto20.9.ThisreflectsthefactthatpressingÔdownÕwouldlead
thevaluefunctioncurvecorrespondstoarewardobtainedbyclearingabrick.totheagentlosingtheballandincurringarewardof21.Attimepoint3,
Attimepoint3,theagentisabouttobreakthroughtothetoplevelofbricksandtheagenthitstheballbypressingÔupÕandtheexpectedrewardkeepsincreasing
thevalueincreasesto,21inanticipationofbreakingoutandclearingauntiltimepoint4,whentheballreachestheleftedgeofthescreenandthevalue
largesetofbricks.Atpoint4,thevalueisabove23andtheagenthasbrokenofallactionsreflectsthattheagentisabouttoreceivearewardof1.Note,
through.Afterthispoint,theballwillbounceattheupperpartofthebricksthedashedlineshowsthepasttrajectoryoftheballpurelyforillustrative
clearingmanyofthembyitself.b,Avisualizationofthelearnedaction-valuepurposes\(thatis,notshownduringthegame.WithpermissionfromAtari
functiononthegamePong.Attimepoint1,theballismovingtowardstheInteractive,Inc.
paddlecontrolledbytheagentontherightsideofthescreenandthevaluesof
©2015Macmillan Publishers Limited. All rights reserved


=== Page 10 ===

RESEARCHLETTER
ExtendedDataTable1|Listofhyperparametersandtheirvalues
ThevaluesofallthehyperparameterswereselectedbyperforminganinformalsearchonthegamesPong,Breakout,Seaquest,SpaceInvadersandBeamRider.Wedidnotperformasystematicgridsearchowing
tothehighcomputationalcost,althoughitisconceivablethatevenbetterresultscouldbeobtainedbysystematicallytuningthehyperparametervalues.
©2015Macmillan Publishers Limited. All rights reserved


=== Page 11 ===

LETTERRESEARCH
12,15
ExtendedDataTable2|ComparisonofgamesscoresobtainedbyDQNagentswithmethodsfromtheliteratureandaprofessional
humangamestester
12
BestLinearLearneristhebestresultobtainedbyalinearfunctionapproximatorondifferenttypesofhanddesignedfeatures.Contingency\(SARSAagentfiguresaretheresultsobtainedinref.15.Notethe
figuresinthelastcolumnindicatetheperformanceofDQNrelativetothehumangamestester,expressedasapercentage,thatis,1003\(DQNscore2randomplayscore/\(humanscore2randomplayscore.
©2015Macmillan Publishers Limited. All rights reserved


=== Page 12 ===

RESEARCHLETTER
ExtendedDataTable3|TheeffectsofreplayandseparatingthetargetQ-network
DQNagentsweretrainedfor10millionframesusingstandardhyperparametersforallpossiblecombinationsofturningreplayonoroff,usingornotusingaseparatetargetQ-network,andthreedifferentlearning
rates.Eachagentwasevaluatedevery250,000trainingframesfor135,000validationframesandthehighestaverageepisodescoreisreported.Notethattheseevaluationepisodeswerenottruncatedat5min
leadingtohigherscoresonEndurothantheonesreportedinExtendedDataTable2.Notealsothatthenumberoftrainingframeswasshorter\(10millionframesascomparedtothemainresultspresentedin
ExtendedDataTable2\(50millionframes.
©2015Macmillan Publishers Limited. All rights reserved


=== Page 13 ===

LETTERRESEARCH
ExtendedDataTable4|ComparisonofDQNperformancewithlin-
earfunctionapproximator
TheperformanceoftheDQNagentiscomparedwiththeperformanceofalinearfunctionapproximator
onthe5validationgames\(thatis,whereasinglelinearlayerwasusedinsteadoftheconvolutional
network,incombinationwithreplayandseparatetargetnetwork.Agentsweretrainedfor10million
framesusingstandardhyperparameters,andthreedifferentlearningrates.Eachagentwasevaluated
every250,000trainingframesfor135,000validationframesandthehighestaverageepisodescoreis
reported.Notethattheseevaluationepisodeswerenottruncatedat5minleadingtohigherscoreson
EndurothantheonesreportedinExtendedDataTable2.Notealsothatthenumberoftrainingframes
wasshorter\(10millionframesascomparedtothemainresultspresentedinExtendedDataTable2
\(50millionframes.
©2015Macmillan Publishers Limited. All rights reserved
