                          Enhancing Modern SAT Solver With Machine Learning Method                                                                                                         GLSVLSI‚Äô25, June 30‚ÄìJuly 02, 2025, New Orleans, LA, USA
                          negated literal nodes. This process generates a comprehensive rep-                                                                                                  Table 1: SAT Dataset
                          resentation of the variables. The normalized adjacency matrix Ì†µÌ±Ä‚Ä≤
                          incorporates aggregation operations into the computation process                                                                               SATDataset                   SATLIB            SATCOMP                 Overall
                          within WGCN. The values in Ì†µÌ±Ä‚Ä≤ serve as weights applied in the                                                                                      #CNF                      59604                4531                 64135
                          WGCN.Ì†µÌ±Ä‚Ä≤isusedtocontrolmessagepassingandensurenumerical                                                                                        #Variables                      276                24410                  3782
                          stability and is calculated from the adjacency matrix Ì†µÌ±Ä as follows                                                                              #Clauses                      491                79780                 62890
                          to prevent gradient explosion:                                                                                                             #BackboneVar.                   151(55%)            2685(11%)             1058(28%)
                                                                 Ì†µÌ±Ä‚Ä≤ =                 Ì†µÌ±Ä                                                (6)
                                                                            √ç2Ì†µÌ±â √ç2Ì†µÌ±â Ì†µÌ±Ä
                                                                               Ì†µÌ±ñ=1      Ì†µÌ±ó =1    Ì†µÌ±ñ Ì†µÌ±ó                                                                                    Table 2: UNSATDataset
                          Following the iterations of WGCN, we derive vectors that capture
                          the structural information of literals and input them into the MLP                                                                          UNSATDataset                        GEN             SATCOMP                 Overall
                          to estimate the probability Ì†µÌ±ùÌ†µÌ±£ of each variable Ì†µÌ±£:                                                                                               #CNF                        61356                3781                 65137
                                     Ì†µÌ±ùÌ†µÌ±£  =softmax(Ì†µÌ∞ø2 ¬∑Ì†µÌ±ÖÌ†µÌ±íÌ†µÌ∞øÌ†µÌ±à (Ì†µÌ∞ø1 ¬∑ (‚ÑéÌ†µÌ±£ ‚äï ‚ÑéÌ†µÌ±£+Ì†µÌ±â ) + Ì†µÌ±ê1) + Ì†µÌ±ê2))                                  (7)                             #Variables                        1167                4180                  2351
                          Here, Ì†µÌ∞ø1 and Ì†µÌ∞ø2 are learnable weight matrices in MLP, and Ì†µÌ±ê1 and                                                                              #Clauses                        4912               86790                 57382
                          Ì†µÌ±ê2 are learnable bias vectors.                                                                                                          #UNSAT-coreVar.                     978(83%)            1631(39%)             1246(53%)
                          5 Experiments                                                                                                                 of unsatisfiability, which Drat-Trim then processes to extract the
                          5.1         Datasets                                                                                                          UNSATcore. Although the UNSAT core identified by Drat-Trim
                          Wetrain the two Graph Neural Network (GNN) models on distinct                                                                 [22] may not be minimal, it remains highly valuable for solving
                          datasets to specialize in different problem domains for both SAT                                                              UNSATinstances.
                          andUNSATinstances.                                                                                                                 Table 2 presents detailed statistics of the resulting dataset. We
                               For the SAT dataset, we develop a comprehensive collection of                                                            generate 61356 CNF formulas with labels, while the SAT competi-
                          CNFformulaswithlabeledbackbonevariablesspecificallydesigned                                                                   tion randomtrackscontribute3781labeledformulas.Consequently,
                          for training our GNN model. The CNF formulas were sourced from                                                                ourUNSATdatasetencompassesadiversesetofformulas:thenum-
                          twoprimaryrepositories: the SATLIB benchmark library and the                                                                  ber of variables ranges from five to 23641 (average: 2351), and the
                          Random tracks in SAT Competition [5] from 2010 to 2023. The                                                                   numberofclauses ranges from 20 to 161124 (average: 57382). The
                          SATLIBbenchmarklibrary[1] is a well-established resource that                                                                 average percentage of UNSAT-core variables is 53%.
                          housesavarietyofwidelyrecognizedbenchmarkinstances,serving                                                                    5.2         Training and GNNModelPerformance
                          as a foundational challenge for the SAT community. The Random
                          tracks in SAT competitions provide an extensive array of challeng-                                                            We partition the two datasets into distinct training and testing
                          ing random SAT problems, further enriching the diversity of our                                                               subsets, ensuring no overlap between them. Specifically, 85% of the
                          dataset.BackbonevariablesineachCNFformulawerelabeledusing                                                                     dataisallocatedfortraining,whiletheremaining15%isreservedfor
                          publicly available tools, with particular emphasis on the state-of-                                                           testing. To assess performance, we benchmark our results against
                          the-art backbone extractor CadiBack [8], which has been recently                                                              several baselines, including ML-Glucose and the standard baseline
                          introduced and is considered among the most advanced tools in                                                                 glucosesolver,aswellasML-KissatandthestandardbaselineKissat
                          this domain.                                                                                                                  solver.
                               Table 1 presents detailed statistics of the resulting dataset. From                                                           Wemodifythecross-entropylossbyadoptingtheFocalloss[16].
                          SATLIB, we collect 59604 SAT formulas with labels, encompassing                                                               Training is conducted using the AdamW optimizer with an initial
                          seven types of problems: random SAT, graph coloring, planning,                                                                learning rate of 0.001 and a weight decay coef√ècient of 0.01. The
                          boundedmodelchecking,Latin square, circuit fault analysis, and                                                                batch size is set to ten, and the number of epochs is set to 20 for
                          all-interval series. Additionally, the SAT competition random tracks                                                          each training run. Overall, training for both tasks take a total of 10
                          contributed 4531 labeled formulas. Consequently, our SAT dataset                                                              hours on a commodity computer.
                          encompassesadiversesetofformulas,withthenumberofvariables                                                                          Table 3 presents the average prediction performance for both
                          rangingfromfiveto41647(average:3782)andthenumberofclauses                                                                     tasks in terms of precision, recall, F1 score, and accuracy. The GNN
                          rangingfrom15to134621(average:62890).Theaveragepercentage                                                                     modelachievesaclassification accuracy of 91.2% for backbone vari-
                          of backbone variables is 28%.                                                                                                 ables and 93.3% for UNSAT-corevariables,withprecisionexceeding
                               For the UNSATdataset, we develop a collection of CNF formulas                                                            90%andanF1scoreabove0.85forbothcategories.Theseresults
                          withlabeledUNSAT-corevariablesspecificallydesignedfortraining                                                                 indicate that the GNN model effectively learns to classify both
                          our GNNmodel.TheCNFformulasaresourcedfromtwoprimary                                                                           backbone and UNSAT-core variables.
                          repositories: generated UNSAT instances using CNFgen [2] and                                                                       WeexperimentwithdifferentlossfunctionstoaddressbothSAT
                          HardSATGEN[14],andRandomtracksinSATCompetitionfrom                                                                            andUNSATinstanceseffectively. As shown in Fig.4, our analysis
                          2010 to 2023. Variables in each CNF formula are labeled using                                                                 reveals that Focal loss achieves better performance compared to
                          publicly available tools: Kissat and Drat-Trim. Specifically, when                                                            KLDivandcross-entropy,whicharecommonlyusedinearliermeth-
                          Kissat attempts to solve an UNSAT instance, it generates a proof                                                              ods for classification tasks. This outcome confirms our decision
                                                                                                                                             890
