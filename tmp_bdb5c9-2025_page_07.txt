                                   Preprint.
                                   then attempt to “fill in the details” by determining values or routines to populate these initial concepts’
                                   parameters using type annotations to identify which other concepts should be investigated.
                                   3.5    CONTINUAL CONCEPT LEARNING
                                  Along-held goal in machine learning is to develop lifelong learning systems that continually self-
                                   improve without manual intervention. A memory system that can leverage the learning signal present
                                   at test-time via continual updates is one approach to achieve this ambition. Our memory write
                                   operations are lightweight queries that can ingest solution traces derived from both the system and
                                   external sources, making continual updates practical at scale.
                                   However, continual updates also introduce dependencies on evaluation order. If solving problem i
                                   induces a memory update that enables problem j to be solved, then performance differs between
                                  (..., x , ..., x , ...) and (..., x ..., x , ...).   Inference batching further complicates this: even if x
                                         i       j                    j      i                                                                                i
                                   precedes xj, they may appear in the same batch, so the model attempts xj before xi has updated
                                   memory. This introduces an accuracy–throughput trade-off. In all settings, we initialize our external
                                   memory with seed data (problems and solutions). We explicitly evaluate continual updates in
                                   subsection 5.3’s experiments, confirming their efficacy. In other experiments, we used fixed memory
                                   to prioritize throughput and avoid the potentially confounding effect of order dependencies.
                                   4    EXPERIMENTS
                                   BenchmarkSelection. WeevaluateourproposedframeworkonARC-AGI-1(Chollet,2019),a
                                   benchmark explicitly designed to evaluate intelligence as “efficient acquisition of new skills” instead
                                   of “fixed possession/memorization of skills.” Each ARC puzzle encodes a transformation rule that
                                   maps input to output pixel grids. The objective of each puzzle is to infer its rule given several
                                   examples of input-output grid pairs, and produce the corresponding output grid for several input test
                                   cases. We find that the abstract domain of pixel grid transforms provides a meaningfully challenging
                                   testbed to simulate frontier domains without requiring expert knowledge to evaluate trajectories–a
                                   confluence of desirable properties for evaluating a continual concept learning system.
                                  ARC-AGI-1containsapublicvalidation split containing 400 puzzles with a difficulty distribution
                                   matching that of the private evaluation. Following Akyürek et al. (2025), we evaluate a randomly
                                   selected 100-puzzle subset of the public val split. This makes repeated runs for more stable estimates
                                   feasible given cost and the sampling variance we observed. Li et al. (2024) manually authored Python
                                   solutions for 160 puzzles from the public train split to act as “seeds” to recombine into their synthetic
                                   dataset. We reuse these solutions to seed our memory rather than training data.
                                   Models.      To build on frontier models, we experiment primarily with OpenAI’s o4-mini. At the
                                   time of writing, o4-mini is second only to Grok 4, but o4-mini’s lower price puts it on the Pareto
                                   frontier of cost and performance ARC-Prize (2025). For auxiliary tasks such as concept abstraction
                                   and non-reasoning selection, we use OpenAI’s GPT-4.1 to conserve token usage. Early experiments
                                   also evaluated the open-weight DeepSeek R1, which has the benefit of visible thinking traces, but its
                                   8000output token limit consistently yielded unfinished solutions in initial testing.
                                   Evaluation.       While the official evaluation harness queries models to directly predict output grids for
                                   test cases, we instead use a program synthesis approach that queries for a transformation function to
                                   convert input to output grids. The code artifact provides more signal for reflection and also allows
                                   us to test proposed logic against reference pairs for feedback. We evaluate performance under 0, 1,
                                   and 2 retries with this execution feedback. We follow official ARC-AGI scoring (two attempts per
                                   puzzle), and account for sampling variance by averaging over extra runs (see details in Appendix B).
                                  Theprimary memorybaseline we compare against is a re-implementation of Suzgun et al. (2025)’s
                                   DC-Cu(labeled “cheatsheet”) that uses a frozen memory to match other settings.
                                                                                                7
