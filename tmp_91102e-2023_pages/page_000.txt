This ICCV paper is the Open Access version, provided by the Computer Vision Foundation.

Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.

LeaF: Learning Frames for 4D Point Cloud Sequence Understanding

Yunze Liu |,
' Tsinghua University,

Junyu Chen ', Zekai Zhang',
? Shanghai Artificial Intelligence Laboratory,

Jingwei Huang*, Li Yi!?4,
3 Shanghai Qi Zhi Institute

4 Huawei

Abstract

We focus on learning descriptive geometry and motion
features from 4D point cloud sequences in this work. Exist-
ing works usually develop generic 4D learning tools with-
out leveraging the prior that a 4D sequence comes from a
single 3D scene with local dynamics. Based on this obser
vation, we propose to learn region-wise coordinate frames
that transform together with the underlying geometry. With
such frames, we can factorize geometry and motion to fa-
cilitate a feature-space geometric reconstruction for more
effective 4D learning. To learn such region frames, we de-
velop a rotation equivariant network with a frame stabiliza-
tion strategy. To leverage such frames for better spatial-
temporal feature learning, we develop a frame-guided 4D
learning scheme. Experiments show that this approach sig-
nificantly outperforms previous state-of-the-art methods on
a wide range of 4D understanding benchmarks.

1. Introduction

We have recently witnessed a surge of interest in under-
standing point cloud sequences in 4D (3D space + 1D time).
As the direct sensory input in a large number of modem AI
applications including robotics and AR/VR, point cloud se-
quences can faithfully depict the geometry and motion of a
dynamic scene, and therefore become critical for an intelli-
gent agent to perceive and interact with the physical world.

However, learning on such 4D data is very challenging
and is still in an immature stage. 4D point cloud sequences
usually couple 3D geometry and its dynamic motion to-
gether, resulting in quite redundant data in a very high di-
mensional space. This causes severe learning issues against
an effective and compact spatial-temporal representation.

Some existing efforts tackle the challenge through novel
4D backbone designs [8, 30, 7]. However, most of these
works treat the point cloud sequence as unstructured 4D
data and exploit generic 4D leaming methods without lever-
aging the prior that the whole 4D sequence just depicts a
single 3D scene with dynamic objects. As a result, such
4D learning is usually not super effective and the learned

604

wa SIL

»®WA

ind

ann

tl tH

Frame-amaware Frame-aware

Spatial-temporal Conv

Spatial-temp oral Conv
Figure 1. Frame-aware spatial-temporal convolution. We propose
to learn frames for point cloud sequence. Upon obtaining the
frames, we could easily align the geometry regardless of the un-
derlying motion toward a more canonical and complete geometry
understanding. The motion-agnostic geometric features also allow

+r temporal association toward a better motion understanding.

spatial-temporal feature barely outperforms the spatial fea-
ture alone. Another line of works [7] uses self-supervised
representation leaming to encourage geometry and motion
learning in a loosely decoupled manner. However, the de-
coupling still happens on the whole-scene level with a spe-
cial focus on camera ego-motion, restricting their efficacy
in modeling local dynamics on the object-level. We envi-
sion that successful geometry and motion decoupling is the
key toward effective 4D representation learning. This es-
sentially requires depicting the low-dimensional manifold
of the dynamic scenes from the redundant high-dimensional
4D data. This highly correlates to dynamic scene recon-
struction which requires understanding both camera ego-
motion and object motion and is an ongoing research
topic itself. Instead of explicitly reconstructing the dy-
namic scene without a quality guarantee, we seek a more
lightweight and flexible solution.

Our solution is based upon the following observation.
For a specific region in the scene, we can understand its
dynamic motion through establishing geometry-based and
temporally-consistent local coordinate frames. Specifically,
if we can establish a local coordinate frame based upon the
3D geometry in each timestamp and also make sure such

