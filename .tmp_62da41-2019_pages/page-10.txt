                                       Banino, Balaguer, Blundell
                  In PonderNet, however, we propose that naively minimizing the number of steps (subject
                to good performance) is not necessarily a good objective. Instead, we propose that matching
                a prior halting distribution has multiple beneﬁts: a) it provides an incentive for exploring
                alternative halting strategies; b) it provides robustness of the learnt step function, which
                may improve generalization; c) the KL is in same units as information-theoretic losses such
                as cross-entropy; and d) it provides an incentive to not ponder for longer than the prior.
                  NotethatinPonderNet, wecomputethelossforeverypossiblenumberofcomputational
                steps, and then minimize the expectation (weighted average) over those. This is unlike
                in ACT where the expectation is taken over the predictions, and a loss is computed by
                comparing the average prediction with the target. This has the consequence that combining
                multiple networks is easier in ACT than in PonderNet. One could easily chain multiple
                ACTmodules next to each other, and the size of the network during training would grow
                linearly with the number of modules. However, the network size when chaining PonderNet
                modules grows exponentially because the loss would need to be estimated conditioned on
                each PonderNet module halting at each step.
                  InPonderNetwehaveintroducedtwolosshyper-parametersλp andβ,incomparisontoa
                single hyper-parameter τ in ACT that trades-oﬀ accuracy with computational complexity.
                We note that, while τ and β are superﬁcially similar (they both apply a weight to the
                regularization term), their eﬀect is not equivalent because the regularization of ACT and
                PonderNet have diﬀerent interpretation.
                A.3 Evaluation
                ACT’s predictions are computed identically during training and evaluation. In both con-
                texts, the maximum number of steps NACT is determined based on the inputs, and the
                prediction is computed as a weighted average over the predictions in all steps. In Ponder-
                Net, training and evaluation are performed diﬀerently. During evaluation, the network halts
                probabilistically by sampling Λ , and either outputs the current prediction or performs an
                                         n
                additional computational step. During training, we are not interested in the predictions per
                se but in the expected loss over steps, and so estimate this up to a maximum number of
                steps N (the higher the better). This estimate will improve with higher probability that the
                network has halted at some point during the ﬁrst N steps (i.e. the cumulative probability
                of halting).
                Appendix B. Parity.
                B.1 Training and evaluation details
                For this experiment we used the Parity task as explained by Graves (2016).
                  All the models used the same architecture, a simple RNN with a single hidden layer con-
                taining 128 tanh units and a single logistic sigmoid output unit. All models were optimized
                using Adam (Kingma and Ba, 2014), with learning rate ﬁxed to 0.0003. The networks were
                trained with binary cross-entropy loss to predict the corresponding target, 1 if there was an
                odd number of ones and 0 if there was an even number of ones. We used minibatches of
                size 128. For both architectures the weights were optimised using Adam (Kingma and Ba,
                2014), with learning rate ﬁxed to 0.0003. For PonderNet we sampled uniformly 10 values
                                                   10
