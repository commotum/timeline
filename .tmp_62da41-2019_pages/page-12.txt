                                              Banino, Balaguer, Blundell
                  available at https://bit.ly/3frofUI. For completeness, we describe the hyperparameters
                  used on Table 3.
                     We also performed a search on hyperparameters to train on our tasks, with ranges
                  reported on Table 4.
                                  Parameter name                       Value
                                  Optimizer algorithm                  Adam
                                  Learning rate                         3e-4
                                  Input embedding size                   128
                                  Attention type              as in Vaswani et al. (2017)
                                  Attention hidden size                  512
                                  Attention number of heads               8
                                  Transition function               MLP(1 Layer)
                                  Transition hidden size                 128
                                  Attention dropout rate                 0.1
                                  Activation function                  RELU
                                  N                                      10
                                  β                                     0.01
                                     Table 3: Hyperparameters used for bAbI experiments.
                                          Parameter name             Value
                                          Attention hidden size    {128, 512}
                                          Transition hidden size   {128, 512}
                                          λp                     uniform(0, 1.0]
                          Table 4: Hyperparameters ranges used to search over with PonderNet on bAbI.
                  Appendix D. Paired Associative Inference
                  D.1 PAI - Task details
                  For this task we used the dataset published in Banino et al. (2020), also the task is available
                  at https://github.com/deepmind/deepmind-research/tree/master/memo
                     To build the dataset, Banino et al. (2020) started with raw images from the ImageNet
                  dataset (Deng et al., 2009), which were embedded using a pre-trained ResNet (He et al.,
                  2016), resulting in embeddings of size 1000. Here we are focusing on the dataset with
                  sequences of length three (i.e. A−B−C) items, which is composed of 1e6 training images,
                  1e5 evaluation images and 2e5 testing images.
                  A single entry in the batch is built by selecting N = 16 sequences from the relevant pool
                  (e.g. training) and it’s composed by three items:
                     • a memory,
                     • a query,
                     • a target.
                                                            12
