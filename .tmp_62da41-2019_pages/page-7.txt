                                            Ponder Net: Learning to Ponder
                      Length                                          UT     MEMO         PonderNet
                      3 items (trained on: A-B-C - accuracy on A-C)   85.60  98.26(0.67)  97.86(3.78)
                   Table 2: Inference trial accuracy. PonderNet results chosen by validation loss, averaged on 3 seeds.
                   For Universal Transformer (UT) and MEMO the results were taken from Banino et al. (2020)
                   4. Discussion
                   We introduced PonderNet, a new algorithm for learning to adapt the computational com-
                   plexity of neural networks. It optimizes a novel objective function that combines prediction
                   accuracy with a regularization term that incentivizes exploration over the pondering time.
                   We demonstrated on the parity task that a neural network equipped with PonderNet can
                   increase its computation to extrapolate beyond the data seen during training. Also, we
                   showed that our methods achieved the highest accuracy in complex domains such as ques-
                   tion answering and multi-step reasoning. Finally, adapting existing recurrent architectures
                   to work with PonderNet is very easy: it simply requires to augment the step function with
                   an additional halting unit, and to add an extra term to the loss. Critically, we showed that
                   this extra loss term is robust to the choice of λp, the hyper-parameter that deﬁnes a prior
                   on how likely is that the network will halt, which is an important advancement over ACT.
                   References
                   Andrea Banino, Adri`a Puigdom`enech Badia, Raphael K¨oster, Martin J. Chadwick, Vinicius
                     Zambaldi, Demis Hassabis, Caswell Barry, Matthew Botvinick, Dharshan Kumaran, and
                     Charles Blundell. MEMO: A deep network for ﬂexible combination of episodic memories.
                     In International Conference on Learning Representations, 2020.
                   Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural
                     networks for eﬃcient inference. In Proceedings of the 34th International Conference on
                     Machine Learning-Volume 70, pages 527–536. JMLR. org, 2017.
                   Victor Campos Camunez, Brendan Jou, Xavier Gir´o Nieto, Jordi Torres Vin˜als, and Shih-
                     Fu Chang. Skip RNN: learning to skip state updates in recurrent neural networks. In
                     Sixth International Conference on Learning Representations: Monday April 30-Thursday
                     May03, 2018, Vancouver Convention Center, Vancouver:[proceedings], pages 1–17, 2018.
                   JunyoungChung,SungjinAhn,andYoshuaBengio. Hierarchicalmultiscalerecurrentneural
                     networks. arXiv preprint arXiv:1609.01704, 2016.
                   Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhut-
                     dinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv
                     preprint arXiv:1901.02860, 2019.
                   Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Luk    asz Kaiser.
                     Universal transformers. arXiv preprint arXiv:1807.03819, 2018.
                                                             7
