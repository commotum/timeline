                                          Banino, Balaguer, Blundell
                    Next we moved to test the ability of PonderNet to allow extrapolation. To do this we
                 consider input vectors of 96 elements instead. We train the network on input vectors up
                 from integers ranging from 1 to 48 elements and we then evaluate on integers between 49
                 and 96. Figure 1b shows that PonderNet was able to achieve almost perfect accuracy on
                 this hard extrapolation task, whereas ACT remained at chance level. It is interesting to see
                 how PonderNet increased its thinking time to 5 steps, which is almost twice as much as the
                 ones in the interpolation set (see Fig. 1a), showing the capability of our method to adapt
                 its computation to the complexity of the task.
                 3.2 bAbI
                 We then turn our attention to the bAbI question answering dataset (Weston et al., 2015),
                 which consists of 20 diﬀerent tasks. This task was chosen as it proved to be diﬃcult for
                 standard neural network architecture that do not employ adaptive computation (Dehghani
                 et al., 2018). In particular we trained our model on the joint 10k training set. Also, please
                 see Appendix C for further training and evaluation details.
                    Table 1 reports the averaged accuracy of our model and the other baselines on bAbI.
                 Our model is able to match state of the art results, but it achieves them faster and with a
                 lower average error. The comparison with Universal transformer (Dehghani et al., 2018, UT)
                 is interesting as it uses the same transformer architecture as PonderNet, but the compute
                 time is optimised with ACT. Interestingly, to solve 20 tasks, Universal Transformer takes
                 10161 steps, whereas our methods 1658, hence conﬁrming that approach uses less compute
                 than ACT.
                  Architecture                                  Average Error Tasks Solved
                  Memory Networks (Sukhbaatar et al., 2015)        4.2± 0.2          17
                  DNC(Graves, 2016)                                3.8± 0.6          18
                  Universal Transformer (Dehghani et al., 2018)    0.29± 1.4         20
                  Transformer+PonderNet                            0.15± 0.9         20
                   Table 1: bAbI. Test results chosen by validation loss. Average error is calculated over 5 seeds
                 3.3 Paired associative inference
                 Finally, we tested PonderNet on the Paired associative inference task (PAI) (Banino et al.,
                 2020). This task is thought to capture the essence of reasoning – the appreciation of distant
                 relationships among elements distributed across multiple facts or memories and it has been
                 shown to beneﬁt from the addition of adaptive computation. Please refer to Appendix D
                 for further details on the task and the training regime.
                    Table 2 reports the averaged accuracy of our model and the other baselines on PAI.
                 Our model is able to match the results of MEMO, which was speciﬁcally designed with this
                 task in mind. More interestingly, our model although is using the same architecture as UT
                 (Dehghani et al., 2018) is able to achieve higher accuracy. For the complete set of results
                 please see Table 7 in Appendix D.
                                                       6
