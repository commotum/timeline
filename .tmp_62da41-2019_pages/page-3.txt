                                               Ponder Net: Learning to Ponder
                       The output yˆ is a learned prediction conditioned on the dynamic number of steps
                                      n
                    n ∈ {1,...,N}. We rely on the value of λn to learn the optimal value of n. We deﬁne a
                    Bernoulli random variable Λ in order to represent a Markov process for the halting with
                                                 n
                    two states “continue” (Λ = 0) and “halt” (Λ = 1). The decision process starts from state
                                             n                    n
                    “continue” (Λ = 0). We set the transition probability:
                                  0
                                             P(Λn =1|Λn−1 = 0) = λn       ∀1≤n≤N                             (1)
                    that is the conditional probability of entering state “halt” at step n conditioned that there
                    has been no previous halting. Note that “halt” is a terminal state. We can then estimate the
                    unconditioned probability that the halting happened in steps 0,1,2,...,N where N is the
                    maximum number of steps allowed before halting. We derive this probability distribution
                    p as a generalization of the geometric distribution:
                     n
                                                                n−1
                                                        pn = λn Y(1−λj)                                      (2)
                                                                 j=1
                    which is a valid probability distribution if we integrate over an inﬁnite number of possible
                    computation steps (N → ∞).
                                            ˆ                                                            ˆ
                       The prediction yˆ ∼ Y made by PonderNet is sampled from a random variable Y with
                                                ˆ
                    probability distribution P(Y = y ) = p . In other words, the prediction of PonderNet is
                                                      n      n
                    the prediction made at the step n at which it halts. This is in contrast with ACT, where
                    model predictions are always weighted averages across steps. Additionally, PonderNet is
                    more generic in this regard: if one wishes to do so, it is straightforward to calculate the
                    expected prediction across steps, similar to how it is done in ACT.
                    2.3 Maximum number of pondering steps
                    Since in practice we can only unroll the step function for a limited number of iterations, we
                    must correct for this so that the sum of probabilities p  sums to 1. We can do this in two
                                                                            n
                    ways. One option here is to normalize the probabilities p so that they sum up to 1 (this
                                                                               n
                    is equivalent to conditioning the probability of halting under the knowledge that n ≤ N).
                    Alternatively, we could assign any remaining halting probability to the last step, so that
                              P
                    p  =1− N−1p instead of as previously deﬁned.
                     N          n=1 n
                       In our experiments, we specify the maximum number of steps using two diﬀerent criteria.
                    In evaluation, and under known temporal or computational limitations, N can be set naively
                    as a constant (or not set any limit, i.e. N → ∞). For training, we found that a more
                    eﬀective (and interpretable) way of parameterizing N is by deﬁning a minimum cumulative
                                                                                         P
                    probability of halting. N is then the smallest value of n such that     n  p > 1−ε, with
                                                                                            j=1 j
                    the hyper-parameter ε positive near 0 (in our experiments 0.05).
                    2.4 Training loss
                    The total loss is composed of reconstruction L     and regularization L     terms:
                                                                   Rec                      Reg
                                                                 3
