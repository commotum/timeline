                                                                           Banino, Balaguer, Blundell
                             sensitive to the choice of a hyper-parameter that trades-oﬀ accuracy and computation cost.
                             Additionally, the gradient for the cost of computation can only back-propagate through the
                             last computational step, leading to a biased estimation of the gradient. Another approach
                             is represented by Adaptive Early Exit Networks (Bolukbasi et al., 2017) where the forward
                             pass of an existing network is terminated at evaluation time if it is likely that the part
                             of the network used so far already predicts the correct answer. More recently, work has
                             investigated the use of REINFORCE (Williams, 1992) to perform conditional computation.
                             Adiscrete latent variable is used to dynamically adjust the number of computation steps.
                             This approach has been applied to recurrent neural networks (Chung et al., 2016; Banino
                             et al., 2020), but has the downside that the estimated gradients have high variance, requiring
                             large batch sizes to train them.                      A parallel line of research has explored using similar
                             techniques to reduce the computation by skipping elements from a sequence of processed
                             inputs (Yu et al., 2017; Campos Camunez et al., 2018).
                                   In this paper we present PonderNet that builds on these previous ideas. PonderNet is
                             fully diﬀerentiable which allows for low-variance gradient estimates (unlike REINFORCE).
                             It has unbiased gradient estimates (unlike ACT). We achieve this by reformulating the
                             halting policy as a probabilistic model. This has consequences in all aspects of the model:
                                  1. Architecture: in PonderNet, the halting node predicts the probability of halting con-
                                      ditional on not having halted before. We exactly compute the overall probability of
                                      halting at each step as a geometric distribution.
                                  2. Loss: we don’t regularize PonderNet to explicitly minimize the number of comput-
                                      ing steps, but incentivize exploration instead. The pressure of using computation
                                      eﬃciently happens naturally as a form of Occam’s razor.
                                  3. Inference: PonderNet is probabilistic both in terms of number of computational steps
                                      and the prediction produced by the network.
                             2. Methods
                             2.1 Problem setting
                             We consider a supervised setting, where we want to learn a function f : x → y from
                                                                     (1)        (k)                       (1)        (k)
                             data (x,y), with x = {x                    , ..., x    } and y = {y              , ..., y   }. We propose a new general
                             architecture for neural networks that modiﬁes the forward pass, as well as a novel loss
                             function to train it.
                             2.2 Step recurrence and halting process
                             ThePonderNet architecture requires a step function s of the form yˆ ,h                                          , λ   =s(x,h ), as
                                                                                                                                  n     n+1     n               n
                             well as an initial state h 1. The output yˆ and λ are respectively the network’s prediction
                                                                 0                          n           n
                             and scalar probability of halting at step n. The step function s can be any neural network,
                             such as MLPs, LSTMs, or encoder-decoder architectures such as transformers. We apply
                             the step function recurrently up to N times.
                               1. Alternatively, one can consider a step function of the form yˆ ,h                     , λ  =s(h )together with an encoder
                                                                                                              n    n+1     n        n
                                  e of the form h = e(x).
                                                      0
                                                                                                  2
