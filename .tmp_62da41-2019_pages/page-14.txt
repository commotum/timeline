                                                       Banino, Balaguer, Blundell
                      the input. On each step, the state was updated by applying the encoder layer once, that
                      is: h    =encoder(h ). Note that in this case PonderNet only received information about
                           n+1               n
                      the inputs through its state. The prediction was computed by applying the decoder layer
                      an equal number of times to the pondering step, that is yˆ          =decoder(...(decoder(h        )).
                                                                                     n+1                            n+1
                      With this architecture, PonderNet was able to optimize how many times to apply the
                      encoder and the decoder layers to improve its performance in this task.
                         The weights were optimised using Adam (Kingma and Ba, 2014), using polynomial
                      weight decay with a maximum learning rate of 0.0003 and learning rate linear warm-up for
                      the ﬁrst epoch. The mini-batch size was of size 128. For completeness, we describe the
                      hyperparameters used on Table 5. We also performed a search on hyperparameters to train
                      on our tasks, with ranges reported on Table 6.
                                         Parameter name                              Value
                                         Optimizer algorithm                         Adam
                                         Input embedding size                         256
                                         Attention type                   as in Vaswani et al. (2017)
                                         Attention hidden size                        512
                                         Attention number of heads                      8
                                         Transition function                    MLP(2 Layers)
                                         Transition hidden size                       128
                                         Attention dropout rate                        0.1
                                         β                                            0.01
                                             Table 5: Hyperparameters used for PAI experiments.
                                                  Parameter name                  Value
                                                  Attention hidden size         {256, 512}
                                                  Transition hidden size       {128, 1024}
                                                  λp                          uniform(0, 0.5]
                                                  N                                [7, 10]
                                Table 6: Hyperparameters ranges used to search over with PonderNet on PAI.
                      D.3 PAI - Results based on query type
                      The result reported below in Table 7 are from the evaluation set at the end of training.
                      Each evaluation set contains 600 items.
                                                 Table 7: Paired Associative - length 3: A-B-C
                                                  Trial   MEMO           UT       PonderNet
                                                  Type
                                                  A-B     99.82(0.30)    97.43    98.01(2.39)
                                                  B-C     99.76(0.38)    98.28    97.43(1.97)
                                                  A-C     98.26(0.67)    85.60    97.86(3.78)
                                                                       14
