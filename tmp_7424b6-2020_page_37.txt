                                          Table 6.1: Most Biased Descriptive Words in 175B Model
                  Top10MostBiasedMaleDescriptiveWordswithRaw         Top10MostBiasedFemaleDescriptiveWordswithRaw
                  Co-Occurrence Counts                               Co-Occurrence Counts
                  Average Number of Co-Occurrences Across All Words: Average Number of Co-Occurrences Across All Words:
                 17.5                                                23.9
                  Large (16)                                         Optimistic (12)
                  Mostly (15)                                        Bubbly (12)
                  Lazy (14)                                          Naughty (12)
                  Fantastic (13)                                     Easy-going (12)
                  Eccentric (13)                                     Petite (10)
                  Protect (10)                                       Tight (10)
                  Jolly (10)                                         Pregnant (10)
                  Stable (9)                                         Gorgeous (28)
                  Personable (22)                                    Sucked (8)
                  Survive (7)                                        Beautiful (158)
                of 1 and top p of 0.9 for every prompt in our dataset. For gender, we had prompts such as "He was very", "She
                                                                                         9
                was very", "He would be described as","She would be described as" . Welookedattheadjectives and
                adverbs in the top 100 most favored words using an off-the-shelf POS tagger [LB02]. We found females were more
                often described using appearance oriented words such as ”beautiful” and ”gorgeous” as compared to men who were
                moreoften described using adjectives that span a greater spectrum.
                Table 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each
                wordco-occurred with a pronoun indicator. “Most Favored” here indicates words which were most skewed towards a
                category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective,
                wehavealsoincluded the average for the number of co-occurrences across all qualifying words for each gender.
                6.2.2  Race
                To investigate racial bias in GPT-3, we seeded the model with prompts such as - "The {race} man was very",
                "The {race} woman was very" and "People would describe the {race} person as" and generated 800
                samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White
                or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that
                language models produce text of differing sentiment when varying features such as occupation [HZJ+19], we explored
                howraceimpacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred
                disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive
                words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid:
                -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet).
                It should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that
                focused on racial features; these results are not from the models talking about race in the wild but talking about race in
                an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply
                looking at word co-occurrences, the resulting sentiment can reﬂect socio-historical factors - for instance, text relating to
                a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated
                with a negative sentiment under this testing methodology.
                Across the models we analyzed, ‘Asian’ had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the
                other hand, ’Black’ had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences
                narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and
                highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data.
                   9Weonlyusedmaleandfemalepronouns. This simplifying assumption makes it easier to study co-occurrence since it does not
                require the isolation of instances in which ‘they’ refers to a singular noun from those where it didn’t, but other forms of gender bias
                are likely present and could be studied using different approaches.
                                                                  37
