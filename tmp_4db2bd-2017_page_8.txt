                         Published as a conference paper at ICLR 2017
                         search procedure on top of their predictions, which has the opportunity to correct for the presence of
                         functions that the neural network failed to predict. Note that a sequence-to-sequence model trained
                         onprogramsofaﬁxedlengthcouldnotbeexpectedtoexhibit this kind of generalization ability.
                         5.3  ALTERNATIVE MODELS
                         Encoder Weevaluatedreplacingthefeed-forward architecture encoder (Sect. 4.3) with an RNN, a
                         natural baseline. Using a GRU-based RNN we were able to achieve results almost as good as using
                         the feed-forward architecture, but found the RNN encoder more difﬁcult to train.
                         Decoder We also considered a purely neural network-based approach, where an RNN decoder
                         is trained to predict the entire program token-by-token. We combined this with our feed-forward
                         encoder by initializing the RNN using the pooled ﬁnal layer of the encoder. We found it substantially
                         moredifﬁcult to train an RNN decoder as compared to the independent binary classiﬁers employed
                         above. Beam search was used to explore likely programs predicted by the RNN, but it only lead to a
                         solution comparable with the other techniques when searching for programs of lengths T ≤ 2, where
                         the search space size is very small (on the order of 103). Note that using an RNN for both the encoder
                         and decoder corresponds to a standard sequence-to-sequence model. However, we do do not rule out
                         that a more sophisticated RNN decoder or training procedure could be possibly more successful.
                         6   RELATED WORK
                         Machine Learning for Inductive Program Synthesis.    There is relatively little work on using
                         machine learning for programming by example. The most closely related work is that of Menon
                         et al. (2013), in which a hand-coded set of features of input-output examples are used as “clues.”
                         When a clue appears in the input-output examples (e.g., the output is a permutation of the input),
                         it reweights the probabilities of productions in a probabilistic context free grammar by a learned
                         amount. This work shares the idea of learning to guide the search over program space conditional on
                         input-output examples. One difference is in the domains. Menon et al. (2013) operate on short string
                         manipulation programs, where it is arguably easier to hand-code features to recognize patterns in the
                         input-output examples (e.g., if the outputs are always permutations or substrings of the input). Our
                         workshowsthatthere are strong cues in patterns in input-output examples in the domain of numbers
                         and lists. However, the main difference is the scale. Menon et al. (2013) learns from a small (280
                         examples), manually-constructed dataset, which limits the capacity of the machine learning model
                         that can be trained. Thus, it forces the machine learning component to be relatively simple. Indeed,
                         Menonetal.(2013)usealog-linearmodelandrelyonhand-constructedfeatures.LIPSautomatically
                         generates training data, which yields datasets with millions of programs and enables high-capacity
                         deep learning models to be brought to bear on the problem.
                         Learning Representations of Program State.   Piech et al. (2015) propose to learn joint embed-
                         dings of program states and programs to automatically extend teacher feedback to many similar
                         programs in the MOOC setting. This work is similar in that it considers embedding program states,
                         but the domain is different, and it otherwise speciﬁcally focuses on syntactic differences between
                         semantically equivalent programs to provide stylistic feedback. Li et al. (2016) use graph neural
                         networks (GNNs) to predict logical descriptions from program states, focusing on data structure
                         shapes instead of numerical and list data. Such GNNs may be a suitable architecture to encode states
                         appearing when extending our DSL to handle more complex data structures.
                         LearningtoInfer.   Veryrecently, Alemi et al. (2016) used neural sequence models in tandem with
                         an automated theorem prover. Similar to our Sort and Add strategy, a neural network component
                         is trained to select premises that the theorem prover can use to prove a theorem. A recent exten-
                         sion (Loos et al., 2017) is similar to our DFS enumeration strategy and uses a neural network to guide
                         the proof search at intermediate steps. The main differences are in the domains, and that they train
                         onanexisting corpus of theorems. More broadly, if we view a DSL as deﬁning a model and search
                         as a form of inference algorithm, then there is a large body of work on using discriminatively-trained
                         models to aid inference in generative models. Examples include Dayan et al. (1995); Kingma &
                                                                  ¨
                         Welling (2014); Shotton et al. (2013); Stuhlmuller et al. (2013); Heess et al. (2013); Jampani et al.
                         (2015).
                                                                    8
