                          [18] Guido Montúfar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of
                               linear regions of deep neural networks. Advances in neural information processing systems, 27,
                               2014.
                          [19] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli.
                               Exponential expressivity in deep neural networks through transient chaos. Advances in neural
                               information processing systems, 29, 2016.
                          [20] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan
                               Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is
                               predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.
                          [21] William Merrill, Ashish Sabharwal, and Noah A Smith. Saturated transformers are constant-
                               depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:
                               843–856, 2022.
                          [22] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Transformers, parallel computation, and
                               logarithmic depth. In Forty-first International Conference on Machine Learning, 2024. URL
                               https://openreview.net/forum?id=QCZabhKQhB.
                          [23] William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space
                               models. In Forty-first International Conference on Machine Learning, 2024. URL https:
                               //openreview.net/forum?id=QZgo9JZpLq.
                          [24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
                               Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
                               models. arXiv preprint arXiv:2001.08361, 2020.
                          [25] Juergen Schmidhuber and Sepp Hochreiter. Long short-term memory. Neural Computation
                               MIT-Press, 1997.
                          [26] Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism
                               of pattern recognition unaffected by shift in position. Biological cybernetics, 36(4):193–202,
                               1980.
                          [27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
                               Gomez, Ł ukasz Kaiser, and Illia Polosukhin.     Attention is all you need.  In I. Guyon,
                               U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-
                               itors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,
                               Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/
                               3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
                          [28] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time.
                               arXiv preprint arXiv:2501.00663, 2024.
                          [29] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by
                               back-propagating errors. nature, 323(6088):533–536, 1986.
                          [30] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
                               Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications
                               of the ACM, 63(11):139–144, 2020.
                          [31] Shaden Alshammari, John Hershey, Axel Feldmann, William T Freeman, and Mark Hamilton.
                               I-con: A unifying framework for representation learning. arXiv preprint arXiv:2504.16929,
                               2025.
                          [32] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,
                               AdamTrischler, and Yoshua Bengio. Learning deep representations by mutual information
                               estimation and maximization. In International Conference on Learning Representations, 2019.
                               URLhttps://openreview.net/forum?id=Bklr3j0cKX.
                          [33] Diederik P Kingma and Jimmy Ba. Adam: A methodforstochastic optimization. arXiv preprint
                               arXiv:1412.6980, 2014.
                                                                        12
