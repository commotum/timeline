=== Page 1 ===
LETTERCommunicatedbyPeterDayanMakingWorkingMemoryWork:AComputationalModelofLearninginthePrefrontalCortexandBasalGangliaRandallC.OReillyoreilly@psych.colorado.eduMichaelJ.Frankfrankmj@psych.colorado.eduDepartmentofPsychology,UniversityofColoradoBoulder,Boulder,CO80309,U.S.A.Theprefrontalcortexhaslongbeenthoughttosubservebothworkingmemory(theholdingofinformationonlineforprocessing)andexecutivefunctions(decidinghowtomanipulateworkingmemoryandperformprocessing).Althoughmanycomputationalmodelsofworkingmemoryhavebeendeveloped,themechanisticbasisofexecutivefunctionremainselusive,oftenamountingtoahomunculus.Thisarticlepresentsanattempttodeconstructthishomunculusthroughpowerfullearningmechanismsthatallowacomputationalmodeloftheprefrontalcortextocontrolbothitselfandotherbrainareasinastrategic,task-appropriatemanner.Theselearningmechanismsarebasedonsubcorticalstructuresinthemidbrain,basalganglia,andamygdala,whichtogetherformanactor-criticarchitecture.Thecriticsystemlearnswhichprefrontalrepresentationsaretaskrelevantandtrainstheactor,whichinturnprovidesadynamicgatingmechanismforcontrollingworkingmemoryupdating.Computationally,thelearningmechanismisdesignedtosimultaneouslysolvethetemporalandstructuralcreditassignmentproblems.Themodelsperformancecomparesfavorablywithstandardbackpropagation-basedtemporallearningmechanismsonthechal-lenging1-2-AXworkingmemorytaskandotherbenchmarkworkingmemorytasks.1IntroductionThisletterpresentsacomputationalmodelofworkingmemorybasedontheprefrontalcortexandbasalganglia(thePBWMmodel).Themodelrepresentsaconvergenceoftwologicallyseparablebutsynergisticgoals:understandingthecomplexinteractionsbetweenthebasalganglia(BG)andprefrontalcortex(PFC)inworkingmemoryfunctionanddevelopingacomputationallypowerfulmodelofworkingmemorythatcanlearntoperformcomplextemporallyextendedtasks.Suchtasksrequirelearningwhichinformationtomaintainovertime(andwhattoforget)andhowtoNeuralComputation18,283328(2006)C2005MassachusettsInstituteofTechnology

=== Page 2 ===
284R.OReillyandM.Frankassigncreditorblametoeventsbasedontheirtemporallydelayedcon-sequences.Themodelshowshowtheprefrontalcortexandbasalgangliacaninteracttosolvetheseproblemsbyimplementingaexibleworkingmemorysystemwithanadaptivegatingmechanism.Thismechanismcanswitchbetweenrapidupdatingofnewinformationintoworkingmemoryandrobustmaintenanceofexistinginformationalreadybeingmaintained(Hochreiter&Schmidhuber,1997;OReilly,Braver,&Cohen,1999;Braver&Cohen,2000;Cohen,Braver,&OReilly,1996;OReilly&Munakata,2000).Itistrainedinthemodelusingaversionofreinforcementlearningmech-anismsthatarewidelythoughttobesupportedbythebasalganglia(e.g.,Sutton,1988;Sutton&Barto,1998;Schultzetal.,1995;Houk,Adams,&Barto,1995;Schultz,Dayan,&Montague,1997;Suri,Bargas,&Arbib,2001;Contreras-Vidal&Schultz,1999;Joel,Niv,&Ruppin,2002).Atthebiologicallevelofanalysis,thePBWMmodelbuildsonexistingworkdescribingthedivisionoflaborbetweenprefrontalcortexandbasalganglia(Frank,Loughry,&OReilly,2001;Frank,2005).Inthispriorwork,wedemonstratedthatthebasalgangliacanperformdynamicgatingviathemodulatorymechanismofdisinhibition,allowingonlytask-relevantinformationtobemaintainedinPFCandpreventingdistractinginforma-tionfrominterferingwithtaskdemands.Themechanismsforsupport-ingsuchfunctionsareanalogoustothebasalgangliaroleinmodulatingmoreprimitivefrontalsystem(e.g.,facilitatingadaptivemotorresponseswhilesuppressingothers;Mink,1996).However,todate,nomodelhasattemptedtoaddressthemoredifcultquestionofhowtheBGknowswhatinformationistaskrelevant(whichwashard-wiredinpriormodels).Thepresentmodellearnsthisdynamicgatingfunctionalityinanadap-tivemannerviareinforcementlearningmechanismsthoughttodependonthedopaminergicsystemandassociatedareas(e.g.,nucleusaccum-bens,basal-lateralamygdala,midbraindopaminenuclei).Inaddition,theprefrontalcortexrepresentationsthemselveslearnusingbothHebbiananderror-drivenlearningmechanismsasincorporatedintotheLeabramodelofcorticallearning,whichcombinesanumberofwell-acceptedmechanismsintoonecoherentframework(OReilly,1998;OReilly&Munakata,2000).Atthecomputationallevel,themodelismostcloselyrelatedtothelongshort-termmemory(LSTM)model(Hochreiter&Schmidhuber,1997;Gers,Schmidhuber,&Cummins,2000),whichuseserrorbackpropagationtotraindynamicgatingsignals.TheimpressivelearningabilityoftheLSTMmodelcomparedtootherapproachestotemporallearningthatlackdynamicgat-ingarguesfortheimportanceofthiskindofmechanism.However,itissomewhatdifculttoseehowLSTMitselfcouldactuallybeimplementedinthebrain.ThePBWMmodelshowshowsimilarlypowerfullevelsofcom-putationallearningperformancecanbeachievedusingmorebiologicallybasedmechanisms.Thismodelhasdirectimplicationsforunderstandingexecutivedysfunctioninneurologicaldisorderssuchasattentiondecithyperactivitydisorder(ADHD)andParkinsonsdisease,whichinvolvethe

=== Page 3 ===
MakingWorkingMemoryWork285interactionbetweendopamine,basalganglia,andprefrontalcortex(Frank,Seeberger,&OReilly,2004;Frank,2005).AfterpresentingthePBWMmodelanditscomputational,biological,andcognitivebases,wecompareitsperformancewiththatofseveralotherstandardtemporallearningmodelsincludingLSTM,asimplerecurrentnetwork(SRN;Elman,1990;Jordan,1986),andreal-timerecurrentback-propagationlearning(RBP;Robinson&Fallside,1987;Schmidhuber,1992;Williams&Zipser,1992).2WorkingMemoryFunctionalDemandsandAdaptiveGatingTheneedforanadaptivegatingmechanismcanbemotivatedbythe1-2-AXtask(seeFigure1;Franketal.,2001),whichisacomplexworkingmemorytaskinvolvingbothgoalsandsubgoalsandisusedasatestcaselaterinthearticle.Numberandletterstimuli(1,2,A,X,B,Y)appearoneatatimeinsequence,andtheparticipantisaskedtodetectoneoftwotargetsequences,dependingonwhetherheorshelastsawa1ora2(whichthusservesastaskstimuli).Inthe1task,thetargetisAfollowedbyX,andfor2,itisB-Y.Thus,thetaskdemandstimulideneanouterloopofactivemaintenance(maintenanceoftaskdemands)withinwhichtherecanbeanumberofinnerloopsofactivemaintenancefortheA-Xlevelsequences.Thistaskimposesthreecriticalfunctionaldemandsontheworkingmemorysystem:Rapidupdating:Aseachstimuluscomesin,itmustberapidlyencodedinworkingmemory.1LAXLRBLXLLBL2RYtimeouter loopinner loopsFigure1:The1-2-AXtask.Stimuliarepresentedoneatatimeinasequence.Theparticipantrespondsbypressingtherightkey(R)tothetargetsequence;otherwise,aleftkey(L)ispressed.Ifthesubjectlastsawa1,thenthetargetsequenceisanAfollowedbyanX.Ifa2waslastseen,thenthetargetisaBfollowedbyaY.Distractorstimuli(e.g.,3,C,Z)maybepresentedatanypointandaretobeignored.Themaintenanceofthetaskstimuli(1or2)constitutesatemporalouterlooparoundmultipleinner-loopmemoryupdatesrequiredtodetectthetargetsequence.

=== Page 4 ===
286R.OReillyandM.FrankSensoryInputWorkingMemoryGatingopencloseda) Updateb) MaintainACAAFigure2:Illustrationofactivegating.Whenthegateisopen,sensoryinputcanrapidlyupdateworkingmemory(e.g.,encodingthecueitemAinthe1-2-AXtask),butwhenitisclosed,itcannot,therebypreventingotherdistractinginfor-mation(e.g.,distractorC)frominterferingwiththemaintenanceofpreviouslystoredinformation.Robustmaintenance:Thetaskdemandstimuli(1or2)intheouterloopmustbemaintainedinthefaceofinterferencefromongoingprocessingofinner-loopstimuliandirrelevantdistractors.Selectiveupdating:Onlysomeelementsofworkingmemoryshouldbeupdatedatanygiventime,whileothersaremaintained.Forexample,intheinner-loop,AsandXsshouldbeupdatedwhilethetaskdemandstimulus(1or2)ismaintained.Thersttwoofthesefunctionaldemands(rapidupdatingandrobustmaintenance)aredirectlyinconictwitheachotherwhenviewedintermsofstandardneuralprocessingmechanisms,andthusmotivatetheneedforadynamicgatingmechanismtoswitchbetweenthesemodesofoperation(seeFigure2;Cohenetal.,1996;Braver&Cohen,2000;OReillyetal.,1999;OReilly&Munakata,2000;Franketal.,2001).Whenthegateisopen,workingmemoryisupdatedbyincomingstimulusinformation;whenitisclosed,currentlyactiveworkingmemoryrepresentationsarerobustlymaintained.2.1DynamicGatingviaBasalGangliaDisinhibition.Oneofthecen-tralpostulatesofthePBWMmodelisthatthebasalgangliaprovideaselec-tivedynamicgatingmechanismforinformationmaintainedviasustainedactivationinthePFC(seeFigure3).AsreviewedinFranketal.(2001),thisideaisconsistentwithawiderangeofdataandothercomputationalmodelsthathavebeendevelopedlargelyinthedomainofmotorcontrol,butalsoinworkingmemory(Wickens,1993;Houk&Wise,1995;Wickens,Kotter,&Alexander,1995;Dominey,Arbib,&Joseph,1995;Berns&Sejnowski,1995,1998;Jackson&Houghton,1995;Beiser&Houk,1998;Kropotov&

=== Page 5 ===
MakingWorkingMemoryWork287SNrPosterior CortexFrontal CortexGPeD2D1dorsalstriatum+excitatoryinhibitoryGoNoGothalamusVA,VL,MDFigure3:Thebasalgangliaareinterconnectedwithfrontalcortexthroughaseriesofparallelloops,eachoftheformshown.Workingbackwardfromthethalamus,whichisbidirectionallyexcitatorywithfrontalcortex,theSNr(sub-stantianigraparsreticulata)istonicallyactiveandinhibitingthisexcitatorycir-cuit.WhendirectpathwayGoneuronsindorsalstriatumre,theyinhibittheSNr,andthusdisinhibitfrontalcortex,producingagating-likemodulationthatwearguetriggerstheupdateofworkingmemoryrepresentationsinprefrontalcortex.TheindirectpathwayNoGoneuronsofdorsalstriatumcounteractthiseffectbyinhibitingtheinhibitoryGPe(globuspallidus,externalsegment).Etlinger,1999;Amos,2000;Nakahara,Doya,&Hikosaka,2001).Specically,inthemotordomain,variousauthorssuggestthattheBGarespecializedtoselectivelyfacilitateadaptivemotoractions,whilesuppressingothers(Mink,1996).Thissamefunctionalitymayholdformoreadvancedtasks,inwhichtheactiontofacilitateistheupdatingofprefrontalworkingmemoryrepresentations(Franketal.,2001;Frank,2005).TosupportrobustactivemaintenanceinPFC,ourmodeltakesadvantageofintrinsicbistabil-ityofPFCneurons,inadditiontorecurrentexcitatoryconnections(Fellous,Wang,&Lisman,1998;Wang,1999;Durstewitz,Kelc,&Gunturkun,1999;Durstewitz,Seamans,&Sejnowski,2000a).Herewepresentasummaryofourpreviouslydevelopedframework(Franketal.,2001)forhowtheBGachievesgating:RapidupdatingoccurswhendirectpathwayspinyGoneuronsinthedorsalstriatumre.Goringdirectlyinhibitsthesubstantianigra

=== Page 6 ===
288R.OReillyandM.Frankparsreticulata(SNr)andreleasesitstonicinhibitionofthethalamus.Thisthalamicdisinhibitionenables,butdoesnotdirectlycause(i.e.,gates),aloopofexcitationintothePFC.TheeffectofthisexcitationinthemodelistotogglethestateofbistablecurrentsinthePFCneurons.StriatalGoneuronsinthedirectpathwayareincompetition(intheSNr,ifnotthestriatum;Mink,1996;Wickens,1993)withNoGoneu-ronsintheindirectpathwaythateffectivelyproducemoreinhibitionofthalamicneuronsandthereforepreventgating.RobustmaintenanceoccursviaintrinsicPFCmechanisms(bistability,recurrence)intheabsenceofGoupdatingsignals.ThisissupportedbytheNoGoindirectpathwayringtopreventupdatingofextraneousinformationduringmaintenance.Selectiveupdatingoccursbecausethereareparallelloopsofconnec-tivitythroughdifferentareasofthebasalgangliaandfrontalcortex(Alexander,DeLong,&Strick,1986;Graybiel&Kimura,1995;Middle-ton&Strick,2000).WerefertotheseparatelyupdatablecomponentsofthePFC/BGsystemasstripes,inreferencetorelativelyisolatedgroupsofinterconnectedneuronsinPFC(Levitt,Lewis,Yoshioka,&Lund,1993;Pucak,Levitt,Lund,&Lewis,1996).Wepreviouslyes-timatedthatthehumanfrontalcortexcouldsupportroughly20,000suchstripes(Franketal.,2001).3LearningWhentoGateintheBasalGangliaFigure4providesasummaryofhowbasalgangliagatingcansolvethe1-2-AXtask.ThisgurealsoillustratesthatthelearningprobleminthebasalgangliaamountstolearningwhentoreaGoversusNoGosignalinagivenstripebasedonthecurrentsensoryinputandmaintainedPFCactivations.Withoutsuchalearningmechanism,ourmodelwouldrequiresomekindofintelligenthomunculustocontrolgating.Thus,thedevelop-mentofthislearningmechanismisakeystepinbanishingthehomunculusfromthedomainofworkingmemorymodels(cf.thecentralexecutiveofBaddeleys,1986,model).Therearetwofundamentalproblemsthatmustbesolvedbythelearningmechanism:Temporalcreditassignment:Thebenetsofhavingencodedagivenpieceofinformationintoprefrontalworkingmemoryaretypicallyavailableonlylaterintime(e.g.,encodingthe1taskdemandhelpslateronlywhenconfrontedwithanA-Xsequence).Thus,theproblemistoknowwhichprioreventswerecriticalforsubsequentgood(orbad)performance.Structuralcreditassignment:ThenetworkmustdecidewhichPFCstripesshouldencodewhichdifferentpiecesofinformationatagiventime.Whensuccessfulperformanceoccurs,itmustreinforcethosestripesthat

=== Page 7 ===
MakingWorkingMemoryWork289{11{C1{A1A{X1ARa)b)c)d)CSNrStriatum GoThalPFCStimFigure4:IllustrationofhowthebasalgangliagatingofdifferentPFCstripescansolvethe1-2-AXtask(lightcolor=active;dark=notactive).(a)The1taskisgatedintoananteriorPFCstripebecauseacorrespondingstriatalstriperedGo.(b)ThedistractorCfailstorestriatialGoneurons,soitwillnotbemaintained;however,itdoeselicittransientPFCactivity.Notethatthe1persistsbecauseofgating-inducedrobustmaintenance.(c)TheAisgatedin.(d)Arightkeypressmotoractionisactivated(usingthesameBG-mediateddisinhibitionmechanism)basedonXinputplusmaintainedPFCcontext.actuallycontributedtothissuccess.Thisformofcreditassignmentiswhatneuralnetworkmodelsaretypicallyverygoodatdoing,butclearlythisformofstructuralcreditassignmentinteractswiththetemporalcreditassignmentproblem,makingitmorecomplex.ThePBWMmodelusesareinforcement-learningalgorithmcalledPVLV(inreferencetoitsPavlovianlearningmechanisms;OReilly,Frank,Hazy,&Watz,2005)tosolvethetemporalcreditassignmentproblem.Thesimu-lateddopaminergic(DA)outputofthisPVLVsystemmodulatesGoversusNoGoringactivityinastripe-wisemannerinBG-PFCcircuitstofacili-tatestructuralcreditassignment.Eachoftheseisdescribedindetailbelow.Themodel(seeFigure5)hasanactor-criticstructure(Sutton&Barto,1998),wherethecriticisthePVLVsystemthatcontrolstheringofsimulatedmid-brainDAneuronsandtrainsbothitselfandtheactor.Theactoristhebasalgangliagatingsystem,composedoftheGoandNoGopathwaysinthedor-salstriatumandtheirassociatedprojectionsthroughBGoutputstructurestothethalamus,andthenbackuptothePFC.TheDAsignalscomputed

=== Page 8 ===
290R.OReillyandM.Frank(gating)(modulation)BG: Gating(Actor)PVLV: DA(Critic)Posterior Cortex:I/O MappingSensory InputMotor OutputPFC: Context,Goals, etcFigure5:OverallarchitectureofthePBWMmodel.Sensoryinputsaremappedtomotoroutputsviaposteriorcortical(hidden)layers,asinastandardneuralnetworkmodel.ThePFCcontextualizesthismappingbyrepresentingrelevantpriorinformationandgoals.Thebasalganglia(BG)updatethePFCrepresen-tationsviadynamicgating,andthePVLVsystemdrivesdopaminergic(DA)modulationoftheBGsoitcanlearnwhentoupdate.TheBG/PVLVsystemconstitutesanactor-criticarchitecture,wheretheBGperformsupdatingactionsandthePVLVsystemcritiquesthepotentialrewardvalueoftheseactions,withtheresultingmodulationshapingfutureactionstobemorerewarding.byPVLVdrivebothperformanceandlearningeffectsviaoppositeeffectsonGoandNoGoneurons(Frank,2005).Specically,DAisexcitatoryontotheGoneuronsviaD1receptorsandinhibitoryontoNoGoneuronsviaD2receptors(Gerfen,2000;Hernandez-Lopezetal.,2000).Thus,positiveDAbursts(abovetoniclevelring)tendtoincreaseGoringanddecreaseNoGoring,whiledipsinDAring(belowtoniclevels)havetheoppositeeffect.ThechangeinactivationstateasaresultofthisDAmodulationcanthendrivelearninginanappropriateway,asdetailedbelowandinFrank(2005).3.1TemporalCreditAssignment:ThePVLVAlgorithm.Theringpatternsofmidbraindopamine(DA)neurons(ventraltegmentalarea,VTA,andsubstantianigraparscompacta,SNc;bothstronglyinnervatedbythebasalganglia)exhibitthepropertiesnecessarytosolvethetemporalcreditassignmentproblembecausetheyappeartolearntoreforstimulithatpre-dictsubsequentrewards(e.g.,Schultz,Apicella,&Ljungberg,1993;Schultz,1998).ThispropertyisillustratedinschematicforminFigure6aforasimplePavlovianconditioningparadigm,whereaconditionedstimulus(CS,e.g.,atone)predictsasubsequentunconditionedstimulus(US,i.e.,areward).Figure6bshowshowthispredictiveDAringcanreinforceBGGoringtomaintainastimulus,whensuchmaintenanceleadstosubsequentreward.

=== Page 9 ===
MakingWorkingMemoryWork291DAPFC(spans the delay)DAa)b)(causes updating)(maint in PFC)CSCSBGGo(reinforces Go)US/rUS/rFigure6:(a)Schematicofdopamine(DA)neuralringforaconditionedstimu-lus(CS,e.g.,atone)thatreliablypredictsasubsequentunconditionedstimulus(US,i.e.,areward,r).Initially,DAresatthepointofreward,butthenoverre-peatedtrialslearnstoreattheonsetofthestimulus.(b)ThisDAringpatterncansolvethetemporalcreditassignmentproblemforPFCactivemaintenance.Here,thePFCmaintainsthetransientinputstimulus(initiallybychance),lead-ingtoreward.AstheDAsystemlearns,itbeginstoreDAburstsatstimulusonset,byvirtueofPFCbridgingthegap(inplaceofasustainedinput).DAringatstimulusonsetreinforcestheringofbasalgangliaGoneurons,whichdriveupdatinginPFC.Specically,theDAringcanmovefromthetimeofarewardtotheonsetofastimulusthat,ifmaintainedinthePFC,leadstothissubsequentre-ward.BecausethisDAringoccurswhenthestimuluscomeson,itiswelltimedtofacilitatethestorageofthisstimulusinPFC.Inthemodel,thisoccursbyreinforcingtheconnectionsbetweenthestimulusandtheGogat-ingneuronsinthestriatum,whichthencauseupdatingofPFCtomaintainthestimulus.Notethatothermodelshaveleveragedthissamelogic,buthavetheDAringitselfcauseupdatingofworkingmemoryviadirectDAprojectionstoPFC(OReillyetal.,1999;Braver&Cohen,2000;Cohenetal.,1996;OReilly&Munakata,2000;Rougier&OReilly,2002;OReilly,Noelle,Braver,&Cohen,2002).ThedisadvantageofthisglobalDAsignalisthatit

=== Page 10 ===
292R.OReillyandM.FrankwouldupdatetheentirePFCeverytime,makingitdifculttoperformtaskslikethe1-2-AXtask,whichrequiremaintenanceofsomerepresentationswhileupdatingothers.TheapparentlypredictivenatureoftheDAringhasalmostuniversallybeenexplainedintermsofthetemporaldifferences(TD)reinforcementlearningmechanism(Sutton,1988;Sutton&Barto,1998;Schultzetal.,1995;Houketal.,1995;Montague,Dayan,&Sejnowski,1996;Surietal.,2001;Contreras-Vidal&Schultz,1999;Joeletal.,2002).TheearlierDAgatingmodelscitedaboveandanearlierversionofthePBWMmodel(OReilly&Frank,2003)alsousedthisTDmechanismtocapturetheessentialpropertiesofDAringintheBG.However,considerablesubsequentexplorationandanalysisofthesemodelshasledustodevelopanon-TDbasedaccountoftheseDAringpatterns,whichabandonsthepredictionframeworkonwhichitisbased(OReillyetal.,2005).Inbrief,TDlearningdependsonsequentialchainingofpredictionsfromonetimesteptothenext,andanyweaklink(i.e.,unpredictableevent)canbreakthischain.Inmanyofthetasksfacedbyourmodels(e.g.,the1-2-AXtask),thesequenceofstimulusstatesisalmostcompletelyunpredictable,andthissignicantlydisruptstheTDchainingmechanism,asshowninOReillyetal.(2005).Insteadofrelyingonpredictionastheengineoflearning,wehavedevelopedafundamentallyassociativePavlovianlearningmechanismcalledPVLV,whichconsistsoftwosystems:primaryvalue(PV)andlearnedvalue(LV)(OReillyetal.,2005;seeFigure7).ThePVsystemisjusttheFigure7:PVLVlearningmechanism.(a)StructureofPVLV.ThePV(primaryvalue)systemlearnsaboutprimaryrewardsandcontainstwosubsystems:theexcitatory(PVe)drivesexcitatoryDAburstsfromprimaryrewards(US=un-conditionedstimulus),andtheinhibitory(PVi)learnstocancelthesebursts(usingtimingorotherreliablesignals).Anatomically,thePVecorrespondstothelateralhypothalamus(LHA),whichhasexcitatoryprojectionstothemid-brainDAnucleiandrespondstoprimaryrewards.ThePVicorrespondstothestriosome-patchneuronsintheventralstriatum(V.Str.),whichhavedirectin-hibitoryprojectionsontotheDAsystem,andlearntoreatthetimeofexpectedrewards.TheLV(learnedvalue)systemlearnstoreforconditionedstimuli(CS)thatarereliablyassociatedwithreward.Theexcitatorycomponent(LVe)drivesDAburstingandcorrespondstothecentralnucleusoftheamygdala(CNA),whichhasexcitatoryDAprojectionsandlearnstorespondtoCSs.Theinhibitorycomponent(LVi)isjustlikethePVi,exceptitinhibitsCS-associatedbursts.(b)Applicationtothesimpleconditioningparadigmdepictedinthepreviousgure,wherethePVilearns(basedonthePVerewardvalueateachtimestep)tocanceltheDAburstatthetimeofreward,whiletheLVelearnsapositiveCSassociation(onlyatthetimeofreward)anddrivesDAburstsatCSonset.ThephasicnatureofCSring,despiteasustainedCSinput,requiresanoveltydetectionmechanismofsomeform;wesuggestasynapticdepressionmechanismashavingbenecialcomputationalproperties.

=== Page 11 ===
MakingWorkingMemoryWork293excitatoryinhibitoryLV iTimingCSDAPV ieLVePVUS(cereb.)(CNA)(LHA)(V. Str.)(V. Str.)(VTA/SNc)CSb)a)TimingDAPViLVeUS/PVe

=== Page 12 ===
294R.OReillyandM.FrankRescorla-Wagner/delta-rulelearningalgorithm(Rescorla&Wagner,1972;Widrow&Hoff,1960),trainedbytheprimaryrewardvaluert(i.e.,theUS)ateachtimestept(wheretimestepscorrespondtodiscreteeventsintheenvironment,suchasthepresentationofaCSorUS).Forsimplicity,considerasinglelinearunitthatcomputesanexpectedrewardvalueVtpvbasedonweightswticomingfromsensoryandotherinputsxti(e.g.,includingtimingsignalsfromthecerebellum):Vtpv=ixtiwti(3.1)(ouractualvaluerepresentationusesadistributedrepresentation,asde-scribedintheappendix).TheerrorinthisexpectedrewardvaluerelativetotheactualrewardpresentattimetrepresentsthePVsystemscontributiontotheoverallDAsignal:tpv=rtVtpv.(3.2)Notethatallofthesetermsareinthecurrenttimestep,whereasthesimilarequationinTDinvolvestermsacrossdifferentadjacenttimesteps.ThisdeltavaluethentrainstheweightsintothePVrewardexpectation,wti=tpvxti,(3.3)wherewtiisthechangeinweightvalueand0<<1isalearningrate.Asthesystemlearnstoexpectprimaryrewardsbasedonsensoryandotherinputs,thedeltavaluedecreases.Thiscanaccountforthecancellationofthedopamineburstatthetimeofreward,asobservedintheneuralrecordingdata(seeFigure7b).Whenaconditionedstimulusisactivatedinadvanceofaprimaryre-ward,thePVsystemisactuallytrainedtonotexpectrewardatthistime,becauseitisalwaystrainedbythecurrentprimaryrewardvalue,whichiszerointhiscase.Therefore,weneedanadditionalmechanismtoaccountfortheanticipatoryDAburstingatCSonset,whichinturniscriticalfortraininguptheBGgatingsystem(seeFigure6).Thisisthelearnedvalue(LV)system,whichistrainedonlywhenprimaryrewardsareeitherpresentorexpectedbythePVandisfreetoreatothertimeswithoutadaptingitsweights.Therefore,theLVisprotectedfromhavingtolearnthatnoprimaryrewardisactuallypresentatCSonset,becauseitisnottrainedatthattime.Inotherwords,theLVsystemisfreetosignalrewardassociationsforstim-ulievenattimeswhennoprimaryrewardisactuallyexpected.ThisresultsintheanticipatorydopaminespikingatCSonset(seeFigure7b),withoutrequiringanunbrokenchainofpredictiveeventsbetweenstimulusonsetandsubsequentreward,asinTD.Thus,thisanticipatorydopaminespiking

=== Page 13 ===
MakingWorkingMemoryWork295bytheLVsystemisreallyjustsignalingarewardassociation,notarewardprediction.AsdetailedinOReillyetal.(2005),thisPV/LVdivisionprovidesagoodmappingontothebiologyoftheDAsystem(seeFigure7a).Excitatorypro-jectionsfromthelateralhypothalamus(LHA)andcentralnucleusoftheamygdala(CNA)areknowntodriveDAburstsinresponsetoprimaryrewards(LHA)andconditionedstimuli(CNA)(e.g.,Cardinal,Parkinson,Hall,&Everitt,2002).Thus,weconsiderLHAtorepresentr,whichwealsolabelasPVetodenotetheexcitatorycomponentoftheprimaryvaluesys-tem.TheCNAcorrespondstotheexcitatorycomponentoftheLVsystemdescribedabove(LVe),whichlearnstodriveDAburstsinresponsetocon-ditionedstimuli.TheprimaryrewardsystemVpvthatcancelsDAringatrewarddeliveryisassociatedwiththestriosome/patchneuronsintheven-tralstriatum,whichhavedirectinhibitoryprojectionsintotheDAsystem(e.g.,Joel&Weiner,2000),andlearntoreatthetimeofexpectedprimaryrewards(e.g.,Schultz,Apicella,Scarnati,&Ljungberg,1992).Werefertothisastheinhibitorypartoftheprimaryvaluesystem,PVi.Forsymmetryandimportantfunctionalreasonsdescribedlater,wealsoincludeasimilarinhibitorycomponenttotheLVsystem,LVi,whichisalsoassociatedwiththesameventralstriatumneurons,butslowlylearnstocancelDAburstsassociatedwithCSonset.(ForfulldetailsonPVLV,seeOReillyetal.,2005,andtheequationsintheappendix.)3.2StructuralCreditAssignment.ThePVLVmechanismjustdescribedprovidesasolutiontothetemporalcreditassignmentproblem,andweusetheoverallPVLVvaluetosimulatemidbrain(VTA,SNc)dopamineneu-ronringrates(deviationsfrombaseline).Toprovideasolutiontothestructuralcreditassignmentproblem,theglobalPVLVDAsignalcanbemodulatedbytheGoversusNoGoringofthedifferentPFC/BGstripes,sothateachstripegetsadifferentiatedDAsignalthatreectsitscontribu-tiontotheoverallrewardsignal.Specically,wehypothesizethattheSNcprovidesamorestripe-specicDAsignalbyvirtueofinhibitoryprojectionsfromtheSNrtotheSNc(e.g.,Joel&Weiner,2000).Asnotedabove,theseSNrneuronsaretonicallyactiveandareinhibitedbytheringofGoneu-ronsinthestriatum.Thus,totheextentthatastriperesastrongGosignal,itwilldisinhibittheSNcDAprojectiontoitself,whilethosethatareringNoGowillremaininhibitedandnotreceiveDAsignals.WesuggestthatthisinhibitoryprojectionfromSNrtoSNcproducesashuntingpropertythatnegatesthesynapticinputsthatproduceburstsanddips,whilepre-servingtheintrinsicallygeneratedtonicDAringlevels.Mathematically,thisresultsinamultiplicativerelationship,suchthatthedegreeofGoringmultipliesthemagnitudeoftheDAsignalitreceives(seetheappendixfordetails).ItremainstobedeterminedwhethertheSNcprojectionssupportstripe-specictopography(seeHaber,Fudge,&McFarland,2000,fordata

=== Page 14 ===
296R.OReillyandM.Franksuggestiveofsomeleveloftopography),butitisimportanttoemphasizethattheproposedmechanisminvolvesonlyamodulationintheamplitudeofphasicDAchangesinagivenstripeandnotqualitativelydifferentringpatternsfromdifferentSNcneurons.Thus,verycarefulquantitativeparal-lelDArecordingstudiesacrossmultiplestripeswouldberequiredtotestthisidea.Furthermore,itispossiblethatthismodulationcouldbeachievedthroughothermechanismsoperatinginthesynapticterminalsregulatingDArelease(Joel&Weiner,2000),inadditiontoorinsteadofoverallringratesofSNcneurons.Whatisclearfromtheresultspresentedbelowisthatthenetworksaresignicantlyimpairedatlearningwithoutthiscreditas-signmentmechanism,sowefeelitislikelytobeimplementedinthebraininsomemanner.3.3DynamicsofUpdatingandLearning.Inadditiontosolvingthetemporalandstructuralcreditassignmentproblems,thePBWMmodeldependscriticallyonthetemporaldynamicsofactivationupdatingtosolvethefollowingfunctionaldemands:Withinonestimulus-responsetimestep,thePFCmustprovideastablecontextrepresentationreectingongoinggoalsorpriorstimuluscon-text,anditmustalsobeabletoupdatetoreectappropriatechangesincontextforsubsequentprocessing.Therefore,thesystemmustbeabletoprocessthecurrentinputandmakeanappropriateresponsebeforethePFCisallowedtoupdate.ThisoffsetupdatingofcontextrepresentationsisalsocriticalfortheSRNnetwork,asdiscussedlater.InstandardLeabra,therearetwophasesofactivationupdating:aminusphasewhereastimulusisprocessedtoproducearesponse,followedbyaplusphasewhereanyfeedback(whenavailable)ispre-sented,allowingthenetworktocorrectitsresponsenexttime.BothofthesephasesmustoccurwithastablePFCcontextrepresentationforthefeedbacktobeabletodrivelearningappropriately.Furthermore,theBGGo/NoGoringtodecidewhethertoupdatethecurrentPFCrepresentationsmustalsobeappropriatelycontextualizedbythesestablePFCcontextrepresentations.Therefore,inPBWM,weaddathirdupdatephasewherePFCrepresentationsupdate,basedonBGGo/NoGoringthatwascomputedintheplusphase(withthepriorPFCcontextactive).Biologically,thiswouldoccurinamorecontin-uousfashion,butwithappropriatedelayssuchthatPFCupdatingoccursaftermotorresponding.ThePVLVsystemmustlearnaboutthevalueofmaintainingagivenPFCrepresentationatthetimeanoutputresponseismadeandre-warded(ornot).ThisrewardlearningisbasedonadaptingsynapticweightsfromPFCrepresentationsactiveatthetimeofreward,notbasedonanytransientsensoryinputsthatinitiallyactivatedthose

=== Page 15 ===
MakingWorkingMemoryWork297PFCrepresentations,whichcouldhavebeenmanytimestepsearlier(andlongsincegone).AfterBGGoringupdatesPFCrepresentations(duringthethirdphaseofsettling),thePVLVcriticcanthenevaluatethevalueofthenewPFCstatetoprovideatrainingsignaltoGo/NoGounitsinthestriatum.Thistrainingsignalisdirectlycontingentonstriatalactions:Didtheupdateresultinagood(asdeterminedbyPVLVassocia-tions)PFCstate?Ifgood(DAburst),thenincreasethelikelihoodofGoringnexttime.Ifbad(DAdip),thendecreasetheGoringlikelihoodandincreaseNoGoring.ThisoccursviadirectDAmodulationoftheGo/NoGoneuronsinthethirdphase,whereburstsincreaseGoanddecreaseNoGoactivationsanddipshavetheoppositeeffect(Frank,2005).Thus,theGo/NoGounitslearnusingthedeltaruleovertheirstatesinthesecondandthirdphasesofsettling,wherethethirdphasereectstheDAmodulationfromthePVLVevaluationofthenewPFCstate.Tosummarize,thetemporalcreditassignmenttimetravelofperceivedvalue,fromthepointofrewardbacktothecriticalstimulithatmustbemaintained,mustbebasedstrictlyonPFCstatesandnotsensoryinputs.Butthiscreatesacatch-22becausethesePFCstatesreectinputsonlyafterupdatinghasoccurred(OReilly&Munakata,2000),sothesystemcannotknowthatitwouldbegoodtoupdatePFCtorepresentcurrentinputsuntilithasalreadydoneso.ThisissolvedinPBWMbyhavingonesystem(PVLV)forsolvingthetemporalcreditassignmentproblem(basedonPFCstates)andadifferentone(striatum)fordecidingwhentoupdatePFC(basedoncurrentsensoryinputsandpriorPFCcontext).ThePVLVsystemthenevaluatesthestriatalupdatingactionsafterupdatinghasoccurred.Thisamountstotrial-and-errorlearning,withthePVLVsystemprovidingimmediatefeedbackforstriatalgatingactions(andthisfeedbackisinturnbasedonpriorlearningbythePVLVsystem,takingplaceatthetimeofprimaryrewards).Thesystem,likemostreinforcementlearningsystems,requiressufcientexplorationofdifferentgatingactionstondthosethatareuseful.TheessentiallogicofthesedynamicsinthePBWMmodelisillustratedinFigure8inthecontextofasimplestoreignorerecall(SIR)workingmemorytask(whichisalsosimulated,asdescribedlater).TherearetwoadditionalfunctionalfeaturesofthePBWMmodel:(1)amechanismtoen-surethatstriatalunitsarenotstuckinNoGomode(whichwouldpreventthemfromeverlearning)andtointroducesomerandomexploratoryr-ing,and(2)acontrast-enhancementeffectofdopaminemodulationontheGo/NoGounitsthatselectivelymodulatesthoseunitsthatwereactuallyactiverelativetothosethatwerenot.Thedetailsofthesemechanismsaredescribedintheappendix,andtheiroverallcontributionstolearning,along

=== Page 16 ===
298R.OReillyandM.FrankPFCDAInputÐ+++SgnwRSSgnÐ+++SggSXXwnnÐ+++SgngnISSsyndeprewGO = store S, DAlikes Ð> GO firingreinforcedNOGO = ignore I,DA stopped firingfm syn. depressprev gate = S storedÐ> rew on R, assocS (in PFC!) w/ rewin DA sysstate 1: successful Recallstate 2: storing Sstate 3: ignoring IStriatumFigure8:Phase-basedsequenceofoperationsinthePBWMmodelforthreeinputstatesofasimpleStore,Ignore,Recalltask.ThetaskistostoretheSstim-ulus,maintainitoverasequenceofI(ignore)stimuli,andthenrecalltheSwhenanRisinput.Fourkeylayersinthemodelarerepresentedinsimpleform:PFC,sensoryInput,Striatum(withGo=gandNoGo=nunits),andoverallDAring(ascontrolledbyPVLV).Thethreephasespertrial(,+,++=PFCupdate)areshownasasequenceofstatesforthesamelayer(i.e.,thereisonlyonePFClayerthatrepresentsonethingatatime).Windicateskeyweightchanges,andthefontsizeforstriatalgandnunitsindicateseffectsofDAmodulation.SyndepindicatessynapticdepressionintotheDAsystem(LV)thatpreventssustainedringtothePFCSrepresentation.Instate1,thenetworkhadpreviouslystoredtheS(throughrandomGoring)andisnowcorrectlyrecallingitonanRtrial.TheunexpectedrewarddeliveredintheplusphaseproducesaDAburst,andtheLVpartofPVLV(notshown)learnstoas-sociatethestateofthePFCwithreward.State2showstheconsequenceofthislearning,where,sometrialslater,anSinputisactiveandthePFCismaintainingsomeotherinformation(X).Basedonexistingweights,theSinputtriggersthestriatalGoneuronstoreintheplusphase,causingPFCtoupdatetorepresenttheS.Duringthisupdatephase,theLVsystemrecognizesthisS(inthePFC)asrewarding,causingaDAburst,whichincreasesringofGounits,andresultsinincreasedweightsfromSinputstostriatalGounits.Instate3,theGounits(byexistingweights)donotreforthesubsequentignore(I)input,sotheScontinuestobemaintained.ThemaintainedSinPFCdoesnotcontinuetodriveaDAburstduetosynapticdepression,sothereisnoDA-drivenlearning.IfaGoweretorefortheIinput,theresultingIrepresentationinPFCwouldlikelytriggerasmallnegativeDAburst,discouragingsuchringagain.Thesamelogicholdsfornegativefeedbackbycausingnonrewardassociationsformaintenanceofuselessinformation.

=== Page 17 ===
MakingWorkingMemoryWork299withthecontributionsofalltheseparablecomponentsofthesystem,areevaluatedafterthebasicsimulationresultsarepresented.3.4ModelImplementationDetails.TheimplementedPBWMmodel,showninFigure9(withfourstripes),usestheLeabraframework,describedHidden(Posterior  Cortex)DAXYZABC123InputLROutputSNcSNrThalXYZABC123XYZABC123XYZABC123XYZABC123PFC(Context)PVLV(Critic)Striatum(Gating,  Actor)GoNoGoGoNoGoGoNoGoGoNoGoGoNoGoGoNoGoGoNoGoGoNoGoLViLVePViPVeFigure9:Implementedmodelasappliedtothe1-2-AXtask.TherearefourstripesinthismodelasindicatedbythegroupsofunitswithinthePFCandStria-tum(andthefourunitsintheSNcandSNrThallayers).PVerepresentsprimaryreward(rorUS),whichdriveslearningoftheprimaryvalueinhibition(PVi)partofPVLV,whichcancelsprimaryrewardDAbursts.Thelearnedvalue(LV)partofPVLVhastwoopposingexcitatoryandinhibitorycomponents,whichalsodifferinlearningrate(LVe=fastlearningrate,excitatoryonDAbursts;LVi=slowlearningrate,inhibitoryonDAbursts).Allofthesereward-valuelayersencodetheirvaluesascoarse-codeddistributedrepresentations.VTAandSNccomputetheDAvaluesfromthesePVLVlayers,andSNcprojectsthismodulationtotheStriatum.GoandNoGounitsalternate(frombottomlefttoupperright)intheStriatum.TheSNrThallayercomputesGo-NoGointhecorre-spondingstripeandmediatescompetitionusingkWTAdynamics.TheresultingactivitydrivesupdatingofPFCmaintenancecurrents.PFCprovidescontextforInput/Hidden/Outputmappingareas,whichrepresentposteriorcortex.

=== Page 18 ===
300R.OReillyandM.Frankindetailintheappendix(OReilly,1998,2001;OReilly&Munakata,2000).Leabrausespointneuronswithexcitatory,inhibitory,andleakconduc-tancescontributingtoanintegratedmembranepotential,whichisthenthresholdedandtransformedviaanx/(x+1)sigmoidalfunctiontopro-ducearatecodeoutputcommunicatedtootherunits(discretespikingcanalsobeused,butproducesnoisierresults).Eachlayerusesak-winners-take-all(kWTA)functionthatcomputesaninhibitoryconductancethatkeepsroughlythekmostactiveunitsaboveringthresholdandkeepstherestbelowthreshold.UnitslearnaccordingtoacombinationofHebbiananderror-drivenlearning,withthelattercomputedusingthegeneralizedrecir-culationalgorithm(GeneRec;OReilly,1996),whichcomputesbackprop-agationderivativesusingtwophasesofactivationsettling,asmentionedearlier.ThecorticallayersinthemodelusestandardLeabraparametersandfunctionality,whilethebasalgangliasystemsrequiresomeadditionalmechanismstoimplementtheDAmodulationofGo/NoGounits,andtogglingofPFCmaintenancecurrentsfromGoring,asdetailedintheappendix.Insomeofthemodels,wehavesimpliedthePFCrepresentationssothattheydirectlyreecttheinputstimuliinaone-to-onefashion,whichsimplyallowsustotransparentlyinterpretthecontentsofPFCatanygivenpoint.However,thesePFCrepresentationscanalsobetrainedwithrandominitialweights,asexploredbelow.TheabilityofthePFCtodevelopitsownrepresentationsisacriticaladvanceovertheSRNmodel,forexample,asexploredinotherrelatedwork(Rougier,Noelle,Braver,Cohen,&OReilly,2005).4SimulationTestsWeconductedsimulationcomparisonsbetweenthePBWMmodelandasetofbackpropagation-basednetworksonthreedifferentworkingmem-orytasks:(1)the1-2-AXtaskasdescribedearlier,(2)atwo-storeversionoftheStore-Ignore-Recall(SIR)task(OReilly&Munakata,2000),wheretwodifferentitemsneedtobeseparatelymaintained,and(3)asequencememorytaskmodeledafterthephonologicalloop(OReilly&Soto,2002).Thesetasksprovideadiversebasisforevaluatingthesemodels.Thebackpropagation-basedcomparisonnetworkswere:Asimplerecurrentnetwork(SRN;Elman,1990;Jordan,1986)withcross-entropyoutputerror,nomomentum,anerrortoleranceof.1(outputerr<.1countsas0),andahysteresisterminupdatingthecontextlayersof.5(cj(t)=.5hj(t1)+.5cj(t1),wherecjisthecontextunitforhiddenunitactivationhj).Learningrate(lrate),hys-teresis,andhiddenunitsizeweresearchedforoptimalvaluesacrossthisandtheRBPnetworks(withinplausibleranges,usingroundnum-bers,e.g.,lratesof.05,.1,.2,and.5;hysteresisof0,.1,.2,.3,.5,and

=== Page 19 ===
MakingWorkingMemoryWork301.7,hiddenunitsof25,36,49,and100).Forthe1-2-AXtask,optimalperformancewaswith100hiddenunits,hysteresisof.5,andlrateof.1.FortheSIR-2task,49hiddenunitswereusedduetoextremelengthoftrainingrequired,andalrateof.01wasrequiredtolearnatall.Forthephonologicallooptask,196hiddenunitsandalrateof.005performedbest.Areal-timerecurrentbackpropagationlearningnetwork(RBP;Robin-son&Fallside,1987;Schmidhuber,1992;Williams&Zipser,1992),withthesamebasicparametersastheSRN,andatimeconstantforintegratingactivationsandbackpropagatederrorsof1,andthegapbetweenbackpropagationsandthebackproptimewindowsearchedinthesetof6,8,10,and16timesteps.Twotimestepswererequiredforactivationtopropagatefromtheinputtotheoutput,sotheeffec-tivebackpropagationtimewindowacrossdiscreteinputeventsinthesequenceishalfoftheactualtimewindow(e.g.,16=8events,whichrepresentstwoormoreouter-loopsequences).Bestperformancewasachievedwiththelongesttimewindow(16).Alongshort-termmemory(LSTM)model(Hochreiter&Schmidhuber,1997)withforgetgatesasspeciedinGers(2000),withthesamebasicbackpropagationparametersastheothernetworks,andfourmemorycells.4.1The1-2-AXTask.ThetaskwastrainedasinFigure1,withthelengthoftheinner-loopsequencesrandomlyvariedfromonetofour(i.e.,onetofourpairsofA-X,B-Y,andsoon,stimuli).Specically,eachsequenceofstimuliwasgeneratedbyrstrandomlypickinga1or2,andthenloopingforonetofourtimesoverthefollowinginner-loopgenerationroutine.Halfofthetime(randomlyselected),apossibletargetsequence(if1,thenA-X;if2,thenB-Y)wasgenerated.Theotherhalfofthetime,arandomsequencecomposedofanA,B,orC,followedbyanX,Y,orZ,wasrandomlygenerated.Thus,possibletargets(A-X,B-Y)representatleast50%oftrials,butactualtargets(A-Xinthe1task,B-Yinthe2task)appearonly25%oftimeonaverage.ThecorrectoutputwastheLunit,exceptonthetargetsequences(1-A-Xor2-B-Y),whereitwasanR.ThePBWMnetworkreceivedarewardifitproducedthecorrectoutput(andreceivedthecorrectoutputontheoutputlayerintheplusphaseofeachtrial),whilethebackpropagationnetworkslearnedfromtheerrorsignalcomputedrelativetothiscorrectoutput.Oneepochoftrainingconsistedof25outer-loopsequences,andthetrainingcriterionwas0errorsacrosstwoepochsinarow(oneepochcansometimescontainonlyafewtargets,makingalucky0possible).Forparametersearchingresults,trainingwasstoppedafter10,000epochsforthebackpropagationmodelsifthenetworkhadfailedtolearnbythispointandwasscoredasafailuretolearn.Forstatistics,20differentnetworksofeachtypewererun.

=== Page 20 ===
302R.OReillyandM.FrankPBWMLSTMRBPSRNAlgorithm050010001500200025003000Epochs to Criterion1-2-AX Task Training TimeFigure10:Trainingtimetoreachcriterion(0errorsintwosuccessiveepochsof25outer-loopsequences)onthe1-2-AXtaskforthePBWMmodelandthreebackpropagation-basedcomparisonalgorithms.LSTM=longshort-termmem-orymodel.RBP=recurrentbackpropagation(real-timerecurrentlearning).SRN=simplerecurrentnetwork.ThebasicresultsfornumberofepochsrequiredtoreachthecriteriontraininglevelareshowninFigure10.TheseresultsshowthatthePBWMmodellearnsthetaskatroughlythesamespeedasthecomparisonback-propagationnetworks,withtheSRNtakingsignicantlylonger.However,themainpointisnotincomparingthequantitativeratesoflearning(itispossiblethatdespiteasystematicsearchforthebestparameters,otherpa-rameterscouldbefoundtomakethecomparisonnetworksperformbetter).Rather,theseresultssimplydemonstratethatthebiologicallybasedPBWMmodelisinthesameleagueasexistingpowerfulcomputationallearningmechanisms.Furthermore,theexplorationofparametersforthebackpropagationnet-worksdemonstratesthatthe1-2-AXtaskrepresentsachallengingworkingmemorytask,requiringlargenumbersofhiddenunitsandlongtemporal-integrationparametersforsuccessfullearning.Forexample,theSRNnet-workrequired100hiddenunitsanda.5hysteresisparametertolearnreliably(hysteresisdeterminesthewindowoftemporalintegrationofthecontextunits)(seeTable1).FortheRBPnetwork,thenumberofhiddenunitsandthetimewindowforbackpropagationexhibitedsimilarresults(seeTable2).Specically,timewindowsoffewerthaneighttimestepsre-sultedinfailurestolearn,andthebestresults(intermsofaveragelearningtime)wereachievedwiththemosthiddenunitsandthelongestbackprop-agationtimewindow.

=== Page 21 ===
MakingWorkingMemoryWork303Table1:EffectsofVariousParametersonLearningperformanceintheSRN.Hidden-layersizesforSRN(lrate=.1,hysteresis=.5)Hiddens253649100Successrate4%26%86%100%Averageepochs5367635050792994HysteresisforSRN(100hiddens,lrate=.1)Hysteresis.1.2.3.5.7Successrate0%0%38%100%98%AverageepochsNANA691329943044LearningratesforSRN(100hiddens,hysteresis=.5)lrate.05.1.2Successrate100%100%96%Averageepochs339029943308Notes:Successrate=percentageofnetworks(outof50)thatlearnedtocriterion(0errorsfortwoepochsinarow)within10,000epochs.Averageepochs-av-eragenumberofepochstoreachcriterionforsuccessfulnetworks.Theoptimalperformanceiswith100hiddenunits,learningrate.1,andhysteresis.5.Suf-cientlylargevaluesforthehiddenunitsandhysteresisparametersarecriticalforsuccessfullearning,indicatingthestrongworkingmemorydemandsofthistask.Table2:EffectsofVariousParametersonLearningPerformanceintheRBPNetwork.TimewindowforRBP(lrate=.1,100hiddens)Window681016Successrate6%96%96%96%Averageepochs1389625424353Hidden-layersizeforRBP(lrate=.1,window=16)Hiddens253649100Successrate96%100%96%96%Averageepochs831650687353Notes:Theoptimalperformanceiswith100hiddenunits,timewindow=16.AswiththeSRN,therelativelylargesizeofthenetworkandlongtimewindowsrequiredindicatethestrongworkingmemorydemandsofthetask.4.2TheSIR-2Task.ThePBWMandcomparisonbackpropagational-gorithmswerealsotestedonasomewhatmoreabstracttask(whichhasnotbeentestedinhumans),whichrepresentsperhapsthesimplest,mostdirectformofworkingmemorydemands.Inthisstoreignorerecall(SIR)task(seeTable3),thenetworkmuststoreanarbitraryinputpatternforarecalltestthatoccursafteravariablenumberofinterveningignoretrials(OReilly&Munakata,2000).Stimuliarepresentedduringtheignoretrialsandmustbeidentied(output)bythenetworkbutdonotneedtobemain-tained.TaskswiththissamebasicstructurewerethefocusoftheoriginalHochreiterandSchmidhuber(1997)workontheLSTMalgorithm,where

=== Page 22 ===
304R.OReillyandM.FrankTable3:ExampleSequenceofTrialsintheSIR-2Task,ShowingWhatIsInput,WhatShouldBeMaintainedinEachofTwoStores,andtheTargetOutput.TrialInputMaint-1Maint-2Output1I-DD2S1-AAA3I-BAB4S2-CACC5I-AACA6I-EACE7R1ACA8I-ACA9I-CCC10S1-DDCD11I-EDCE12R1DCD13I-BCB14R2CCNotes:I=Ignoreunitactive,S1/2=Store1/2unitactive,R1/2=Recallunit1/2active.Thefunctionalmeaningofthesetaskcontrolinputsmustbediscoveredbythenetwork.Twoversionswererun.Inthesharedrepresentationsversion,onesetofvestimulusinputswasusedtoencodeAE,regardlessofwhichcontrolinputwaspresent.Inthededicatedrepresenta-tionsversion,thereweredifferentstimulusrepresentationsforeachofthethreecategoriesofstimulusinputs(S1,S2,andI),foratotalof15stimulusinputunits.Thesharedrepresentationsversionprovedimpossiblefornongatednetworkstolearn.theydemonstratedthatthedynamicgatingmechanismwasabletogateintheto-be-storedstimulus,maintainitinthefaceofanessentiallyarbi-trarynumberofinterveningtrialsbyhavingthegateturnedoff,andthenrecallthemaintainedstimulus.TheSIR-2versionofthistaskaddstheneedtoindependentlyupdateandmaintaintwodifferentstimulusmemories,insteadofjustone,whichshouldprovideabettertestofselectiveupdating.Weexploredtwoversionsofthistaskonethathadasinglesetofsharedstimulusrepresentations(A-E)andanotherwithdedicatedstim-ulusrepresentationsforeachofthethreedifferenttypesoftaskcontrolinputs(S1,S2,I).Inthededicatedrepresentationsversion,thestimulusinputsconveyeddirectlytheirfunctionalroleandmadethecontrolinputssomewhatredundant(e.g.,theI-Astimulusunitshouldalwaysbeignored,whiletheS1-Astimulusshouldalwaysbestoredintherststimulusstore).Incontrast,astimulusinthesharedrepresentationversionisambiguous;sometimesanAshouldbeignored,sometimesstoredinS1,andothertimesstoredinS2,dependingontheconcomitantcontrolinput.Thisdifferenceinstimulusambiguitymadeabigdifferenceforthenongatingnetworks,asdiscussedbelow.Thenetworkshad20inputunits(separateAEstimuli

=== Page 23 ===
MakingWorkingMemoryWork305foreachofthreedifferenttypesofcontrolinputs(S1,S2,I)=15units,andthe5controlunits:S1,S2,I,R1,R2).Oneachtrial,acontrolinputandcorrespondingstimuluswererandomlyselectedwithuniformprobability,whichmeansthatS1andS2maintenanceendedupbeingrandomlyinter-leavedwitheachother.Thus,thenetworkwasrequiredtodevelopatrulyindependentformofupdatingandmaintenanceforthesetwoitems.AsFigure11ashows,threeoutoffouralgorithmssucceededinlearningthededicatedstimulusitemsversionofthetaskwithinroughlycompa-rablenumbersofepochs,whiletheSRNmodelhadaverydifculttime,takingonaverage40,090epochs.Wesuspectthatthisdifcultymayreectthelimitationsoftheonetimestepoferrorbackpropagationavailableforthisnetwork,makingitdifcultforittospanthelongerdelaysthatoftenoccurred(Hochreiter&Schmidhuber,1997).Interestingly,thesharedstimulusrepresentationsversionofthetask(seeFigure11b)clearlydividedthegatingnetworksfromthenongatedones(indeed,thenongatednetworksRBPandSRNwerecompletelyunabletoachieveamorestringentcriterionoffourzero-errorepochsinarow,whereasbothPBWMandLSTMreliablyreachedthislevel).Thismaybeduetothefactthatthereisnowaytoestablishaxedsetofweightsbetweenaninputstimulusandaworkingmemoryrepresentationinthistaskversion.Theappropriatememoryrepresentationtomaintainagivenstimulusmustbedeterminedentirelybythecontrolinput.Inotherwords,thecontrolinputmustactasagateonthefateofthestimulusinput,muchasthegateinputonatransistordeterminestheprocessingoftheotherinput.Moregenerally,dynamicgatingenablesaformofdynamicvariablebinding,asillustratedinFigure12forthisSIR-2task.ThetwoPFCstripesinthisexampleactasvariableslotsthatcanholdanyofthestimulusinputs;whichslotagiveninputgetsboundtoisdeterminedbythegatingsystemasdrivenbythecontrolinput(S1orS2).Thisabilitytodynamicallyrouteastimulustodifferentmemorylocationsisverydifculttoachievewithoutadynamicgatingsystem,asourresultsindicate.Nevertheless,itisessentialtoemphasizethatdespitethisadditionalexibilityprovidedbytheadaptivegatingmechanism,thePBWMnetworkisbynomeansafullygeneral-purposevariablebindingsystem.ThePFCrepresentationsmuststilllearntoencodethestimulusinputs,andotherpartsofthenetworkmustlearntorespondappropriatelytothesePFCrepresentations.Therefore,unlikeatraditionalsymboliccomputer,itisnotpossibletostoreanyarbitrarypieceofinformationinagivenPFCstripe.Figure13providesimportantconrmationthatthePVLVlearningmech-anismisdoingwhatweexpectittointhistask,asrepresentedinearlierdiscussionoftheSIRtask(e.g.,seeFigure8).Specically,weexpectthatthesystemwillgeneratelargepositiveDAburstsforStoreeventsandnotforIgnoreevents.ThisisbecausetheStoresignalshouldbepositivelyassoci-atedwithcorrectperformance(andthusreward),whiletheIgnoresignalshouldnotbe.Thisisexactlywhatisobserved.

=== Page 24 ===
306R.OReillyandM.FrankPBWMLSTMRBPSRNAlgorithm110100100010000Epochs to CriterionSIR 2 Store, 5 Dedicated ItemsPBWMLSTMRBPSRNAlgorithm110100100010000100000Epochs to CriterionSIR 2 Store, 2 Shared Itemsa)b)Figure11:Trainingtimetoreachcriterion(0errorsin2consecutiveepochsof100trialseach)ontheSIR-2taskforthePBWMmodelandthreebackpropagation-basedcomparisonalgorithms,for(a)dedicatedstimulusitems(stimulusset=5items,AE)and(b)sharedstimulusitems(stimulusset=2items,AB).LSTM=longshort-termmemorymodel.RBP=recur-rentbackpropagation(real-timerecurrentlearning).SRN=simplerecurrentnetwork.TheSRNdoessignicantlyworseinbothcases(notethelogarithmicscale),andwithshareditems,thenongatednetworkssufferconsiderablyrela-tivetothegatedones,mostlikelybecauseofthevariablebindingfunctionalitythatagatingmechanismprovides,asillustratedinFigure12.4.3ThePhonologicalLoopSequentialRecallTask.Thenalsimula-tiontestinvolvesasimpliedmodelofthephonologicalloop,basedonearlierwork(OReilly&Soto,2002).Thephonologicalloopisaworking

=== Page 25 ===
MakingWorkingMemoryWork307AS1AGoNo"S1""S2"AA"S1""S2"S2NoGoa)b)PFCInputBGFigure12:Gatingcanachieveaformofdynamicvariablebinding,asillustratedintheSIR-2task.Thestorecommand(S1orS2)candrivegatingsignalsindifferentstripesintheBG,causingtheinputstimulusitem(A,B,...)tobestoredintheassociatedPFCstripe.Thus,thesameinputitemcanbeencodedinadifferentneuralvariableslotdependingonotherinputs.Nevertheless,theseneuralstripesarenotfullygeneralliketraditionalsymbolicvariables;theymustlearntoencodetheinputitems,andotherareasmustlearntodecodetheserepresentations.memorysystemthatcanactivelymaintainashortchunkofphonological(verbal)information(e.g.,Baddeley,1986;Baddeley,Gathercole,&Papagno,1998;Burgess&Hitch,1999;Emerson&Miyake,2003).Inessence,thetaskofthismodelistoencodeandreplayasequenceofphonemeinputs,muchasintheclassicpsychologicaltaskofshort-termserialrecall.Thus,itprovidesasimpleexampleofsequencing,whichhasoftenbeenlinkedwithbasalgangliaandprefrontalcortexfunction(e.g.,Berns&Sejnowski,1998;Domineyetal.,1995;Nakaharaetal.,2001).Aswedemonstratedinourearliermodel(OReilly&Soto,2002),anadaptivelygatedworkingmemoryarchitectureprovidesaparticularlyef-cientandsystematicwayofencodingphonologicalsequences.Becausephonemesareasmallclosedclassofitems,eachindependentlyupdatablePFCstripecanlearntoencodethisbasicvocabulary.Thegatingmecha-nismcanthendynamicallygateincomingphonemesintostripesthatim-plicitlyrepresenttheserialorderinformation.Forexample,agivenstripemightalwaysencodethefthphonemeinasequence,regardlessofwhichphonemeitwas.Thevirtueofthissystemisthatitprovidesaparticularlyefcientbasisforgeneralizationtonovelphonemesequences:aslongaseachstripecanencodeanyofthepossiblephonemesandgatingisbasedonserialpositionandnotphonemeidentity,thesystemwillgeneralizeper-fectlytonovelsequences(OReilly&Soto,2002).Asnotedabove,thisisanexampleofvariablebinding,wherethestripesarevariable-likeslotsforagivenposition,andthegatingbindsagiveninputtoitsassociatedslot.

=== Page 26 ===
308R.OReillyandM.Frank050100150200Epochs-0.10.00.10.20.30.4a)b)Avg DA ValueDA Values Over TrainingStoreIgnoreRecall050100150200Epochs0.00.20.40.60.81.0Avg LVe ValueLVe (CS Assoc) Values Over TrainingIgnoreStoreRecallFigure13:(a)AveragesimulatedDAvaluesinthePBWMmodelfordifferenteventtypesovertraining.Withintherst50epochs,themodellearnsstrong,positiveDAvaluesforbothtypesofstorageevents(Store),whichreinforcesgatingfortheseevents.Incontrast,lowDAvaluesaregeneratedforIgnoreandRecallevents.(b)AverageLVevalues,representingthelearnedvalue(i.e.,CSassociationswithrewardvalue)ofvariouseventtypes.Asthemodellearnstoperformwell,itaccuratelyperceivestherewardatRecallevents.Thisgeneral-izestotheStoreevents,buttheIgnoreeventsarenotreliablyassociatedwithreward,andthusremainatlowlevels.OurearliermodelwasdevelopedinadvanceofthePBWMlearningmechanismsandusedahand-codedgatingmechanismtodemonstratethepoweroftheunderlyingrepresentationalscheme.Incontrast,wetrainedthepresentnetworksfromrandominitialweightstolearnthistask.Each

=== Page 27 ===
MakingWorkingMemoryWork309trainingsequenceconsistedofanencodingphase,wherethecurrentse-quenceofphonemeswaspresentedinorder,followedbyaretrievalphasewherethenetworkhadtooutputthephonemesintheordertheywereencoded.Sequenceswereoflength3,andonly10simulatedphonemeswereused,representedlocallyasoneoutof10unitsactive.Sequenceorderinformationwasprovidedtothenetworkintheformofanexplicittimeinput,whichcountedupfrom1to3duringbothencodingandretrieval.Also,encodingversusretrievalphasewasexplicitlysignaledbytwounitsintheinput.Anexampleinputsequencewouldbe:E-1-B,E-2-A,E-3-G,R-1,R-2,R-3,whereE/Ristheencoding/recallag,thenextdigitspeciesthesequenceposition(time),andthethirdisthephoneme(notpresentintheinputduringretrieval).Thereare1000possiblesequences(103),andthenetworksweretrainedonarandomlyselectedsubsetof300ofthese,withanothernonoverlappingsampleof300usedforgeneralizationtestingattheendoftraining.Bothofthegatednetworks(PBWMandLSTM)hadsixstripesormemorycellsinsteadoffour,giventhatthreeitemshadtobemaintainedatatime,andthenetworksbenetfromhavingextrastripestoexploredifferentgatingstrategiesinparallel.ThePFCrepresentationsinthePBWMmodelweresubjecttolearning(unlikeprevioussimulations,wheretheyweresimplyacopyoftheinput,foranalyticalsimplicity)andhad42unitsperstripe,asintheOReillyandSoto(2002)model,andtherewere100hiddenunits.Therewere24unitspermemorycellintheLSTMmodel(notethatcomputationincreasesasapowerof2permemorycellunitinLSTM,settingarelativelylowupperlimitonthenumberofsuchcells).Figure14showsthetrainingandtestingresults.Bothgatedmodels(PBWM,LSTM)learnedmorerapidlythanthenongatedbackpropagation-basednetworks(RBP,SRN).Furthermore,theRBPnetworkwasunabletolearnunlesswepresentedtheentiresetoftrainingsequencesinaxedorder(othernetworkshadrandomlyorderedpresentationoftrainingsequences).ThiswastrueregardlessoftheRBPwindowsize(evenwhenitwasexactlythelengthofasequence).Also,theSRNcouldnotlearnwithonly100hid-denunits,so196wereused.ForboththeRBPandSRNnetworks,alowerlearningrateof.005wasrequiredtoachievestableconvergence.Inshort,thiswasadifculttaskforthesenetworkstolearn.PerhapsthemostinterestingresultsarethegeneralizationtestresultsshowninFigure14b.AswasdemonstratedintheOReillyandSoto(2002)model,gatingaffordsconsiderableadvantagesinthegeneralizationtonovelsequencescomparedtotheRBPandSRNnetworks.ItisclearthattheSRNnetworkinparticularsimplymemorizesthetrainingsequences,whereasthegatednetworks(PBWM,LSTM)developaverysystematicsolutionwhereeachworkingmemorystripeorslotlearnstoencodeadifferentelementinthesequence.Thisisagoodexampleoftheadvantagesofthevariable-bindingkindofbehaviorsupportedbyadaptivegating,asdiscussedearlier.

=== Page 28 ===
310R.OReillyandM.FrankPBWMLSTMRBPSRNAlgorithm0a)b)100020003000Epochs to CriterionPhono Loop Training TimePBWMLSTMRBPSRNAlgorithm0255075100Generalization Error %Phono Loop GeneralizationFigure14:(a)Learningratesforthedifferentalgorithmsonthephonologicallooptask,replicatingpreviousgeneralpatterns(criterionisoneepochof0error).(b)Generalizationperformance(testingon300novel,untrainedsequences),showingthatthegatingnetworks(PBWMandLSTM)exhibitsubstantiallybettergeneralization,duetotheirabilitytodynamicallygateitemsintoactivememoryslotsbasedontheirorderofpresentation.4.4TestsofAlgorithmComponents.HavingdemonstratedthatthePBWMmodelcansuccessfullylearnarangeofdifferentchallengingwork-ingmemorytasks,wenowtesttheroleofspecicsubcomponentsofthealgorithmtodemonstratetheircontributiontotheoverallperformance.Table4showstheresultsofeliminatingvariousportionsofthemodelin

=== Page 29 ===
MakingWorkingMemoryWork311Table4:ResultsofVariousTestsfortheImportanceofVariousSeparablePartsofthePBWMAlgorithm,ShownasaPercentageofTrainedNetworksThatMetCriterion(SuccessRate).SuccessRate(%)Manipulation12axSIR-2LoopNoHebbian95100100NoDAcontrastenhancement809590NoRandomGoexploration095100NoLVi(slowLVbaseline)159030NoSNrThalDAMod,DA=1.01550NoSNrThalDAMod,DA=0.570200NoSNrThalDAMod,DA=0.2803020NoSNrThalDAMod,DA=0.1554020NoDAmodulationatall000Notes:WiththepossibleexceptionofHebbianlearning,allofthecom-ponentsclearlyplayanimportantroleinoveralllearning,forthereasonsdescribedinthetextasthealgorithmwasintroduced.TheNoSNrThalDAModcaseseliminatestripe-wisestructuralcreditassignment;con-trolsforoveralllevelsofDAmodulationareshown.ThenalNoDAModulationatallconditioncompletelyeliminatestheinuenceofthePVLVDAsystemonStriatumGo/NoGounits,clearlyindicatingthatPVLV(i.e.,learning)iskey.termsofpercentageofnetworkssuccessfullylearningtocriterion.Thisshowsthateachseparablecomponentofthealgorithmplaysanimportantrole,withthepossibleexceptionofHebbianlearning(whichwaspresentonlyintheposteriorcortical(Hidden/Output)portionofthenetwork).Differentmodelsappeartobedifferentiallysensitivetothesemanipula-tions,butallareaffectedrelativetothe100%performanceofthefullmodel.FortheNoSNrThalDAModmanipulation,whicheliminatesstructuralcreditassignmentviathestripe-wisemodulationofDAbytheSNrThallayer,wealsotriedreducingtheoverallstrengthoftheDAmodulationofthestriatumGo/NoGounits,withtheideathattheSNrThalmodulationalsotendstoreduceDAlevelsoverall.Therefore,wewantedtomakesureanyimpairmentwasnotjustaresultofachangeinoverallDAlevels;asignicantimpairmentremainsevenwiththismanipulation.5DiscussionThePBWMmodelpresentedheredemonstratespowerfullearningabilitiesondemonstrablycomplexanddifcultworkingmemorytasks.Wehavealsotesteditinformallyonawiderrangeoftasks,withsimilarlygoodresults.Thismaybethersttimethatabiologicallybasedmechanismforcontrollingworkingmemoryhasbeendemonstratedtocomparefavor-ablywiththelearningabilitiesofmoreabstractandbiologicallyimplausible

=== Page 30 ===
312R.OReillyandM.Frankbackpropagation-basedtemporallearningmechanisms.Otherexistingsim-ulationsoflearninginthebasalgangliatendtofocusonrelativelysimplese-quencingtasksthatdonotrequirecomplexworkingmemorymaintenanceandupdatinganddonotrequirelearningofwheninformationshouldandshouldnotbestoredinworkingmemory.Nevertheless,thecentralideasbehindthePBWMmodelareconsistentwithanumberoftheseexistingmodels(Schultzetal.,1995;Houketal.,1995;Schultzetal.,1997;Surietal.,2001;Contreras-Vidal&Schultz,1999;Joeletal.,2002),therebydemonstrat-ingthatanemergingconsensusviewofbasalganglialearningmechanismscanbeappliedtomorecomplexcognitivefunctions.ThecentralfunctionalpropertiesofthePBWMmodelcanbesummarizedbycomparisonwiththewidelyusedSRNbackpropagationnetwork,whichisarguablythesimplestformofagatedworkingmemorymodel.ThegatingaspectoftheSRNbecomesmoreobviouswhenthenetworkhastosettleovermultipleupdatecyclesforeachinputevent(asinaninteractivenetworkortomeasurereactiontimesfromafeedforwardnetwork).Inthiscase,itisclearthatthecontextlayermustbeheldconstantandbeprotectedfromupdatingduringthesecyclesofupdating(settling),andthenitmustberapidlyupdatedattheendofsettling(seeFigure15).AlthoughtheSRNachievesthisalternatingmaintenanceandupdatingbyat,inabiologicalnetworkitwouldalmostcertainlyrequiresomekindofgatingmechanism.OnceonerecognizesthegatingmechanismhiddenintheSRN,itisnaturaltoconsidergeneralizingsuchamechanismtoachieveamorepowerful,exibletypeofgating.ThisisexactlywhatthePBWMmodelprovides,byaddingthefollow-ingdegreesoffreedomtothegatingsignal:(1)gatingisdynamic,suchthatinformationcanbemaintainedoveravariablenumberoftrialsin-steadofautomaticallygatingeverytrial;(2)thecontextrepresentationsarelearned,insteadofsimplybeingcopiesofthehiddenlayer,allowingthemtodevelopinwaysthatreecttheuniquedemandsofworkingmem-oryrepresentations(e.g.,Rougier&OReilly,2002;Rougieretal.,2005);(3)therecanbemultiplecontextlayers(i.e.,stripes),eachwithitsownsetofrepresentationsandgatingsignals.AlthoughsomeresearchershaveusedaspectrumofhysteresisvariablestoachievesomeofthisadditionalexibilitywithintheSRN,itshouldbeclearthatthePBWMmodelaffordsconsiderablymoreexibilityinthemaintenanceandupdatingofworkingmemoryinformation.Moreover,thesimilargoodperformanceofPBWMandLSTMmodelsacrossarangeofcomplextasksclearlydemonstratestheadvantagesofdynamicgatingsystemsforworkingmemoryfunction.Furthermore,thePBWMmodelisbiologicallyplausible.Indeed,thegeneralfunctionsofeachofitscomponentsweremotivatedbyalargebaseofliteraturespan-ningmultiplelevelsofanalysis,includingcellular,systems,andpsycho-logicaldata.Assuch,thePBWMmodelcanbeusedtoexplorepossiblerolesoftheindividualneuralsystemsinvolvedbyperturbingparameters

=== Page 31 ===
MakingWorkingMemoryWork313inputoutputhiddencontextinputoutputhiddencontextinputoutputhiddencontextprocess event 1(context gate closed)update context(context gate open)process event 2(context gate closed)a)b)c)Figure15:Thesimplerecurrentnetwork(SRN)asagatingnetwork.Whenprocessingofeachinputeventrequiresmultiplecyclesofsettling,thecontextlayermustbeheldconstantoverthesecycles(i.e.,itsgateisclosed,panela).Afterprocessinganevent,thegateisopenedtoallowupdatingofthecontext(copyingofhiddenactivitiestothecontext,panelb).Thisnewcontextisthenprotectedfromupdatingduringtheprocessingofthenextevent(panelc).Incomparison,thePBWMmodelallowsmoreexible,dynamiccontrolofthegatingsignal(insteadofautomaticgatingeachtimestep),withmultiplecontextlayers(stripes)thatcaneachlearntheirownrepresentations(insteadofbeingasimplecopy).tosimulatedevelopment,aging,pharmacologicalmanipulations,andneu-rologicaldysfunction.Forexample,wethinkthemodelcanexplicitlytesttheimplicationsofstriataldopaminedysfunctioninproducingcognitivedecitsinconditionssuchasParkinsonsdiseaseandADHD(e.g.,Franketal.,2004;Frank,2005).Further,recentextensionstotheframeworkhaveyieldedinsightsintopossibledivisionsoflaborbetweenthebasalgangliaandorbitofrontalcortexinreinforcementlearninganddecisionmaking(Frank&Claus,2005).AlthoughthePBWMmodelwasdesignedtoincludemanycentralas-pectsofthebiologyofthePFC/BGsystem,italsogoesbeyondwhatiscurrentlyknownandomitsmanybiologicaldetailsoftherealsystem.Therefore,considerablefurtherexperimentalworkisnecessarytotestthespecicpredictionsandneuralhypothesesbehindthemodel,andfurtherelaborationandrevisionofthemodelwillundoubtedlybenecessary.BecausethePBWMmodelrepresentsalevelofmodelingintermediatebetweendetailedbiologicalmodelsandpowerful,abstractcognitiveandcomputationalmodels,ithasthepotentialtobuildimportantbridgesbe-tweenthesedisparatelevelsofanalysis.Forexample,theabstractACT-RcognitivearchitecturehasrecentlybeenmappedontobiologicalsubstratesincludingtheBGandPFC(Andersonetal.,2004;Anderson&Lebiere,1998),withthespecicroleascribedtotheBGsharingsomecentralas-pectsofitsroleinthePBWMmodel.Ontheotherendofthespectrum,

=== Page 32 ===
314R.OReillyandM.Frankbiologicallybasedmodelshavetraditionallybeenincapableofsimulatingcomplexcognitivefunctionssuchasproblemsolvingandabstractreason-ing,whichmakeextensiveuseofdynamicworkingmemoryupdatingandmaintenancemechanismstoexhibitcontrolledprocessingoveratimescalefromsecondstominutes.ThePBWMmodelshouldinprincipleallowmod-elsofthesephenomenatobedevelopedandtheirbehaviorcomparedwithmoreabstractmodels,suchasthosedevelopedinACT-R.Oneofthemajorchallengestothismodelisaccountingfortheextremeexibilityofthehumancognitiveapparatus.Insteadofrequiringhundredsoftrialsoftrainingonproblemslikethe1-2-AXtask,peoplecanperformthistaskalmostimmediatelybasedonverbaltaskinstructions.Ourcurrentmodelismoreappropriateforunderstandinghowagentscanlearnwhichinformationtoholdinmindviatrialanderror,aswouldberequiredifmonkeysweretoperformthetask.1Understandingthehumancapacityforgenerativitymaybethegreatestchallengefacingoureld,sowecer-tainlydonotclaimtohavesolvedit.Nevertheless,wedothinkthatthemechanismsofthePBWMmodel,andinparticularitsabilitytoexhibitlimitedvariable-bindingfunctionality,arecriticalstepsalongtheway.Itmaybethatoverthe13orsoyearsittakestofullydevelopafunctionalPFC,peoplehavedevelopedasystematicandexiblesetofrepresentationsthatsupportdynamicrecongurationofinput-outputmappingsaccordingtomaintainedPFCrepresentations.Thus,thesePFCvariablescanbeac-tivatedbytaskinstructionsandsupportnoveltaskperformancewithoutextensivetraining.Thisandmanyotherimportantproblems,includingquestionsaboutthebiologicalsubstratesofthePBWMmodel,remaintobeaddressedinfutureresearch.Appendix:ImplementationalDetailsThemodelwasimplementedusingtheLeabraframework,whichisde-scribedindetailinOReillyandMunakata(2000)andOReilly(2001),andsummarizedhere.SeeTable5foralistingofparametervalues,nearlyallofwhichareattheirdefaultsettings.Thesesameparametersandequa-tionshavebeenusedtosimulateover40differentmodelsinOReillyandMunakata(2000)andanumberofotherresearchmodels.Thus,themodelcanbeviewedasaninstantiationofasystematicmodelingframeworkusingstandardizedmechanisms,insteadofconstructingnewmechanismsforeachmodel.(Themodelcanbeobtainedbye-mailingtherstauthoratoreilly@psych.colorado.edu.)1Inpractice,monkeyswouldlikelyrequireanextensiveshapingproceduretolearntherelativelycomplex1-2-AXhierarchicalstructurepiecebypiece.However,wearguethatmuchoftheadvantageofshapingmayhavetodowiththemotivationalstateoftheorganism:itenablessubstantiallevelsofsuccessearlyon,tokeepmotivated.Themodelcurrentlyhasnosuchmotivationalconstraintsandthusdoesnotneedshaping.

=== Page 33 ===
MakingWorkingMemoryWork315Table5:ParametersfortheSimulation.ParameterValueParameterValueEl0.15gl0.10Ei0.15gi1.0Ee1.00ge1.0Vrest0.150.25.02600kin/out1khidden7kPFC4kstriatum7kPVLV1khebb.01.01toPFCkhebb.001toPFC.001Notes:Seetheequationsinthetextforexplanationsofparameters.Allarestandarddefaultparametervaluesexceptforthosewithan*.TheslowerlearningrateofPFCconnectionsproducedbetterresultsandisconsistentwithavarietyofconvergingevidence,suggestingthatthePFClearnsmoreslowlythantherestofcortex(Morton&Munakata,2002).A.1Pseudocode.ThepseudocodeforLeabraisgivenhere,showingexactlyhowthepiecesofthealgorithmdescribedinmoredetailinthesubsequentsectionsttogether.Outerloop:Iterateoverevents(trials)withinanepoch.Foreachevent:1.Iterateoverminus(),plus(+),andupdate(++)phasesofsettlingforeachevent.(a)Atstartofsettling:i.Fornon-PFC/BGunits,initializestatevariables(e.g.,activa-tion,vm).ii.Applyexternalpatterns(clampinputinminus,inputandout-put,externalrewardbasedonminus-phaseoutputs).(b)Duringeachcycleofsettling,forallnonclampedunits:i.Computeexcitatorynetinput(ge(t)orj;equationA.3)(equa-tion24forSNr/Thalunits).ii.ForStriatumGo/NoGounitsin++phase,computeaddi-tionalexcitatoryandinhibitorycurrentsbasedonDAinputsfromSNc(equationA.20).iii.ComputekWTAinhibitionforeachlayer,basedongi(equa-tionA.6):A.Sortunitsintotwogroupsbasedongi:topkandremainingk+1ton.B.Ifbasic,ndkandk+1thhighest;ifaveragebased,computeaverageof1k&k+1n.C.Setinhibitoryconductancegifromgkandgk+1(equa-tionA.5).

=== Page 34 ===
316R.OReillyandM.Frankiv.Computepoint-neuronactivationcombiningexcitatoryinputandinhibition(equationA.1).(c)Aftersettling,forallunits:i.Recordnalsettlingactivationsbyphase(yj,y+j,y++).ii.Atendof+and++phases,togglePFCmaintenancecurrentsforstripeswithSNr/Thalact>threshold(.1).2.Afterthesephases,updatetheweights(basedonlinearcurrentweightvalues):(a)Forallnon-BGconnections,computeerror-drivenweightchanges(equationA.8)withsoftweightbounding(equationA.9),Hebbianweightchangesfromplus-phaseactivations(equationA.7),andoverallnetweightchangeasweightedsumoferror-drivenandHebbian(equationA.10).(b)ForPVunits,weightchangesaregivenbydeltarulecomputedasdifferencebetweenplusphaseexternalrewardvalueandminusphaseexpectedrewards(equationA.11).(c)ForLVunits,onlychangeweights(usingequationA.13)ifPVexpectation>pvorexternalreward/punishmentactuallydelivered.(d)ForStriatumunits,weightchangeisthedeltaruleonDA-modulatedsecond-plusphaseactivationsminusunmodulatedplusphaseacts(equationA.19).(e)Incrementtheweightsaccordingtonetweightchange.A.2PointNeuronActivationFunction.Leabrausesapointneuronactivationfunctionthatmodelstheelectrophysiologicalpropertiesofrealneurons,whilesimplifyingtheirgeometrytoasinglepoint.ThemembranepotentialVmisupdatedasafunctionofionicconductancesgwithreversal(driving)potentialsEasfollows:Vm(t)=cgc(t)gc(EcVm(t)),(A.1)withthreechannels(c)correspondingtoeexcitatoryinput,lleakcurrent,andiinhibitoryinput.Followingelectrophysiologicalconvention,theover-allconductanceisdecomposedintoatime-varyingcomponentgc(t)com-putedasafunctionofthedynamicstateofthenetwork,andaconstantgcthatcontrolstherelativeinuenceofthedifferentconductances.Theexcitatorynetinput/conductancege(t)orjiscomputedastheproportionofopenexcitatorychannelsasafunctionofsendingactivationstimestheweightvalues:j=ge(t)=xiwij=1nixiwij.(A.2)

=== Page 35 ===
MakingWorkingMemoryWork317TheinhibitoryconductanceiscomputedviathekWTAfunctiondescribedinthenextsection,andleakisaconstant.Activationcommunicatedtoothercells(yj)isathresholded()sig-moidalfunctionofthemembranepotentialwithgainparameter:yj(t)=11+1[Vm(t)]+,(A.3)where[x]+isathresholdfunctionthatreturns0ifx<0andxifx>0.Notethatifitreturns0,weassumeyj(t)=0,toavoiddividingby0.Toproducealessdiscontinuousdeterministicfunctionwithasofterthreshold,thefunctionisconvolvedwithagaussiannoisekernel(µ=0,	=.005),whichreectstheintrinsicprocessingnoiseofbiologicalneurons,yj(x)=12
	ez2/(2	2)yj(zx)dz,(A.4)wherexrepresentsthe[Vm(t)]+value,andyj(x)isthenoise-convolvedactivationforthatvalue.Inthesimulation,thisfunctionisimplementedusinganumericallookuptable.A.3k-Winners-Take-AllInhibition.LeabrausesakWTA(k-Winners-Take-All)functiontoachieveinhibitorycompetitionamongunitswithinalayer(area).ThekWTAfunctioncomputesauniformlevelofinhibitorycurrentgiforallunitsinthelayer,suchthatthek+1thmostexcitedunitwithinalayerisgenerallybelowitsringthreshold,whilethekthistypi-callyabovethreshold,gi=gk+1+qgkgk+1,(A.5)where0<q<1(.25defaultusedhere)isaparameterforsettingthein-hibitionbetweentheupperboundofgkandthelowerboundofgk+1.Theseboundaryinhibitionvaluesarecomputedasafunctionofthelevelofinhibitionnecessarytokeepaunitrightatthreshold,gi=ge¯ge(Ee)+gl¯gl(El)Ei,(A.6)wheregeistheexcitatorynetinputwithoutthebiasweightcontribution.ThisallowsthebiasweightstooverridethekWTAconstraint.InthebasicversionofthekWTAfunction,whichisrelativelyrigidaboutthekWTAconstraintandisthereforeusedforoutputlayers,gkandgk+1aresettothethresholdinhibitionvalueforthekthandk+1thmostexcitedunits,respectively.Intheaverage-basedkWTAversion,gkistheaverage

=== Page 36 ===
318R.OReillyandM.Frankgivalueforthetopkmostexcitedunits,andgk+1istheaverageofgifortheremainingnkunits.Thisversionallowsmoreexibilityintheactualnumberofunitsactivedependingonthenatureoftheactivationdistributioninthelayer.A.4HebbianandError-DrivenLearning.Forlearning,Leabrausesacombinationoferror-drivenandHebbianlearning.Theerror-drivencomponentisthesymmetricmidpointversionoftheGeneRecalgorithm(OReilly,1996),whichisfunctionallyequivalenttothedeterministicBoltzmannmachineandcontrastiveHebbianlearning(CHL).Thenetworksettlesintwophasesanexpectation(minus)phase,wherethenetworksactualoutputisproduced,andanoutcome(plus)phase,wherethetargetoutputisexperiencedandthencomputesasimpledifferenceofapre-andpostsynapticactivationproductacrossthesetwophases.ForHebbianlearning,Leabrausesessentiallythesamelearningruleusedincompetitivelearningormixtures-of-gaussians,whichcanbeseenasavariantoftheOjanormalization(Oja,1982).Theerror-drivenandHebbianlearningcompo-nentsarecombinedadditivelyateachconnectiontoproduceanetweightchange.TheequationfortheHebbianweightchangeishebbwij=x+iy+jy+jwij=y+j(x+iwij),(A.7)andforerror-drivenlearningusingCHL,errwij=(x+iy+j)(xiyj),(A.8)whichissubjecttoasoft-weightboundingtokeepwithinthe01range:sberrwij=[err]+(1wij)+[err]wij.(A.9)Thetwotermsarethencombinedadditivelywithanormalizedmixingconstantkhebb:wij=[khebb(hebb)+(1khebb)(sberr)].(A.10)A.5PVLVEquations.SeeOReillyetal.(2005)forfurtherdetailsonthePVLVsystem.Weassumethattimeisdiscretizedintostepsthatcorrespondtoenvironmentalevents(e.g.,thepresentationofaCSorUS).Allofthefollowingequationsoperateonvariablesthatareafunctionofthecurrenttimestept.Weomitthetinthenotationbecauseitwouldberedundant.PVLViscomposedoftwosystems,PV(primaryvalue)andLV(learnedvalue),eachofwhichinturniscomposedoftwosubsystems(excitatoryandinhibitory).Thus,therearefourmainvaluerepresentationlayersin

=== Page 37 ===
MakingWorkingMemoryWork319PVLV(PVe,PVi,LVe,LVi),whichthendrivethedopamine(DA)layers(VTA/SNc).A.5.1ValueRepresentations.ThePVLVvaluelayersusestandardLeabraactivationandkWTAdynamicsasdescribedabove,withthefollowingmodications.Theyhaveathree-unitdistributedrepresentationofthescalarvaluestheyencode,wheretheunitshavepreferredvaluesof(0,.5,1).Theoverallvaluerepresentedbythelayeristheweightedaverageoftheunitsactivationtimesitspreferredvalue,andthisdecodedaverageisdis-playedvisuallyintherstunitinthelayer.Theactivationfunctionoftheseunitsisanoisylinearfunction(i.e.,withoutthex/(x+1)nonlinearity,toproducealinearvaluerepresentation,butstillconvolvedwithgaussiannoisetosoftenthethreshold,asforthestandardunits,equationA.4),withgain=220,noisevariance	=.01,andalowerthreshold=.17.ThekforkWTA(averagebased)is1,andtheqvalueis.9(insteadofthede-faultof.6).Thesevalueswereobtainedbyoptimizingthematchforvaluerepresentedwithvaryingfrequenciesof0-1reinforcement(e.g.,thevalueshouldbecloseto.4whenthelayeristrainedwith40%1valuesand60%0values).Notethathavingdifferentunitsfordifferentvalues,insteadofthetypicaluseofasingleunitwithlinearactivations,allowsmuchmorecom-plexmappingstobelearned.Forexample,unitsrepresentinghighvaluescanhavecompletelydifferentpatternsofweightsthanthoseencodinglowvalues,whereasasingleunitisconstrainedbyvirtueofhavingonesetofweightstohaveamonotonicmappingontoscalarvalues.A.5.2LearningRules.ThePVelayerdoesnotlearnandisalwaysjustclampedtoreectanyreceivedrewardvalue(r).Bydefault,weuseavalueof0toreectnegativefeedback,.5fornofeedback,and1forpositivefeedback(thescaleisarbitrary).ThePVilayerunits(yj)aretrainedateverypointintimetoproduceanexpectationfortheamountofrewardthatwillbereceivedatthattime.Intheminusphaseofagiventrial,theunitssettletoadistributedvaluerepresentationbasedonsensoryinputs.ThisresultsinunitactivationsyjandanoverallweightedaveragevalueacrosstheseunitsdenotedPVi.Intheplusphase,theunitactivations(y+j)areclampedtorepresenttheactualrewardr(a.k.a.PVe).Theweights(wij)intoeachPViunitfromsendingunitswithplus-phaseactivationsx+i,areupdatedusingthedeltarulebetweenthetwophasesofPViunitactivationstates:wij=(y+jyj)x+i.(A.11)ThisisequivalenttosayingthattheUS/rewarddrivesapatternofactivationoverthePViunits,whichthenlearntoactivatethispatternbasedonsensoryinputs.

=== Page 38 ===
320R.OReillyandM.FrankTheLVeandLVilayerslearninmuchthesamewayasthePVilayer(seeequationA.11),exceptthatthePVsystemltersthetrainingoftheLVvalues,suchthattheylearnonlyfromactualrewardoutcomes(orwhenrewardisexpectedbythePVsystembutisnotdelivered),andnotwhennorewardsarepresentorexpected.ThisconditionisPVlter=PVi<minPVe<minPVi>maxPVe>max(A.12)wi=(y+jyj)x+iifPVlter0otherwise,(A.13)whereminisalowerthreshold(.2bydefault),belowwhichnegativefeed-backisindicated,andmaxisanupperthreshold(.8),abovewhichpositivefeedbackisindicated(otherwise,nofeedbackisindicated).Biologically,thislteringrequiresthattheLVsystemsbedrivendirectlybyprimaryrewards(whichisreasonableandrequiredbythebasiclearningruleanyway)andthattheylearnfromDAdipsdrivenbyhighPViexpectationsofrewardthatarenotmet.TheonlydifferencebetweentheLVeandLVisystemsisthelearningrate,whichis.05forLVeand.001forLVi.Thus,theinhibitoryLVisystemservesasaslowlyintegratinginhibitorycancellationmechanismfortherapidlyadaptingexcitatoryLVesystem.ThefourPVandLVdistributedvaluerepresentationsdrivethedopaminelayer(VTA/SNc)activationsintermsofthedifferencebetweentheexcitatoryandinhibitorytermsforeach.Thus,thereisaPVdeltaandanLVdelta:pv=PVePVi(A.14)lv=LVeLVi.(A.15)WiththedifferencesinlearningratebetweenLVe(fast)andLVi(slow),theLVdeltasignalreectsrecentdeviationsfromexpectationsandnottherawexpectationsthemselves,justasthePVdeltareectsdeviationsfromexpectationsaboutprimaryrewardvalues.Thisisessentialforlearningtoconvergeandstabilizewhenthenetworkhasmasteredthetask(astheresultspresentedinthearticleshow).WealsoimposeaminimumvalueontheLVitermof.1,sothatthereisalwayssomeexpectation.ThisensuresthatlowLVelearnedvaluesresultinnegativedeltas.ThesetwodeltasignalsneedtobecombinedtoprovideanoverallDAdeltavalue,asreectedintheringoftheVTAandSNcunits.OnesensiblewayofdoingsoistohavethePVsystemdominateatthetimeofprimary

=== Page 39 ===
MakingWorkingMemoryWork321rewards,whiletheLVsystemdominatesotherwise,usingthesamePV-basedlteringasholdsintheLVlearningrule(seeequationA.13):=pvifPVlterlvotherwise.(A.16)ItturnsoutthataslightvariationofthiswheretheLValwayscontributesworksslightlybetter,andiswhatisusedinthisarticle:=lv+pvifPVlter0otherwise.(A.17)A.5.3SynapticDepressionofLVWeights.TheweightsintotheLVunitsaresubjecttosynapticdepression,whichmakesthemsensitivetochangesinstimulusinputs,andnottostatic,persistentactivations(Abbott,Varela,Sen,&Nelson,1997).Eachincomingweighthasaneffectiveweightvaluewthatissubjecttodepressionandrecoverychangesasfollows,wi=R(wiwi)Dxiwi,(A.18)whereRistherecoveryparameter,Disthedepressionparameter,andwiistheasymptoticweightvalue.Forsimplicity,wecomputethesechangesattheendofeverytrialinsteadofinanonlinemanner,usingR=1andD=1,whichproducesdiscreteone-trialdepressionandrecovery.A.6SpecialBasalGangliaMechanismsA.6.1StriatalLearningFunction.Eachstripe(groupofunits)intheStriatumlayerisdividedintoGoversusNoGoinanalternatingfashion.TheDAinputfromtheSNcmodulatestheseunitactivationsintheupdatephasebyprovidingextraexcitatorycurrenttoGoandextrainhibitorycurrenttotheNoGounitsinproportiontothepositivemagnitudeoftheDAsignal,andviceversafornegativeDAmagnitude.ThisreectstheopposinginuencesofDAontheseneurons(Frank,2005;Gerfen,2000).ThisupdatephaseDAsignalreectsthePVLVsystemsevaluationofthePFCupdatesproducedbygatingsignalsintheplusphase(seeFigure8).LearningonweightsintotheGo/NoGounitsisbasedontheactivationdeltabetweentheupdate(++)andplusphases:wi=xi(y++y+).(A.19)ToreectthendingthatDAmodulationhasacontrast-enhancingfunc-tioninthestriatum(Frank,2005;Nicola,Surmeier,&Malenka,2000;

=== Page 40 ===
322R.OReillyandM.FrankHernandez-Lopez,Bargas,Surmeier,Reyes,&Galarraga,1997)andtopro-ducemoreofacreditassignmenteffectinlearning,theDAmodulationispartiallyafunctionofthepreviousplusphaseactivationstate,ge=[da]+y++(1)[da]+(A.20)where0<<1controlsthedegreeofcontrastenhancement(.5isusedinallsimulations),[da]+isthepositivemagnitudeoftheDAsignal(0ifnegative),y+istheplus-phaseunitactivation,andgeistheextraexcitatorycurrentproducedbytheda(forGounits).Asimilarequationisusedforextrainhibition(gi)fromnegativeda([da])forGounits,andviceversaforNoGounits.A.6.2SNrThalUnits.TheSNrThalunitsprovideasimpliedversionoftheSNr/GPe/Thalamuslayers.TheyreceiveanetinputthatreectsthenormalizedGoNoGoactivationsinthecorrespondingStriatumstripe:j=	Go	NoGo	Go+	NoGo
+(A.21)(where[]+indicatesthatonlythepositivepartistaken;whenthereismoreNoGothanGo,thenetinputis0).ThisnetinputthendrivesstandardLeabrapointneuronactivationdynamics,withkWTAinhibitorycompeti-tiondynamicsthatcausestripestocompetetoupdatethePFC.Thisdy-namicisconsistentwiththenotionthatcompetitionandselectiontakeplaceprimarilyinthesmallerGP/SNrareas,andnotmuchinthemuchlargerstriatum(e.g.,Mink,1996;Jaeger,Kita,&Wilson,1994).TheresultingSNrThalactivationthenprovidesthegatingupdatesignaltothePFC:ifthecorrespondingSNrThalunitisactive(aboveaminimumthreshold;.1),thenactivemaintenancecurrentsinthePFCaretoggled.ThisSNrThalactivationalsomultipliestheperstripeDAsignalfromtheSNc,j=snrj,(A.22)wheresnrjisthesnrunitsactivationforstripej,andistheglobalDAsignal,equationA.16.A.6.3RandomGoFiring.ThePBWMsystemlearnsonlyafterGor-ing,soifitneverresGo,itcanneverlearntoimproveperformance.OnesimplesolutionistoinduceGoringifaGohasnotredaftersomethresholdnumberoftrials.However,thisthresholdwouldhavetobeeithertaskspecicorsetveryhigh,becauseitwouldeffectivelylimitthemaximummaintenancedurationofthePFC(becausebyupdating

=== Page 41 ===
MakingWorkingMemoryWork323PFC,theGoringresultsinlossofcurrentlymaintainedinformation).Therefore,wehaveadoptedasomewhatmoresophisticatedmechanismthatkeepstrackoftheaverageDAvaluepresentwheneachstriperesaGo:dak=dak+(dakdak).(A.23)Ifthisvalueis<0andastripehasnotredGowithin10trials,arandomGoringistriggeredwithsomeprobability(.1).WealsocomparetherelativeperstripeDAaverages,iftheperstripeDAaverageislowbutabovezero,andonestripesdakis.05belowtheaverageofthatoftheotherstripes,if(dak<.1)and(dakda<.05);Go,(A.24)arandomGoistriggered,againwithsomeprobability(.1).Finally,wealsorerandomGoinallstripeswithsomeverylowbaselineprobability(.0001)toencourageexploration.WhenarandomGores,wesettheSNrThalunitactivationtobeaboveGothreshold,andweapplyapositiveDAsignaltothecorrespondingstriatalstripe,sothatithasanopportunitytolearntoreforthisinputpatternonitsowninthefuture.A.6.4PFCMaintenance.PFCactivemaintenanceissupportedinpartbyexcitatoryionicconductancesthataretoggledbyGoringfromtheSNrThallayers.Thisisimplementedwithanextraexcitatoryionchan-nelinthebasicVmupdateequation,A.1.Thischannelhasaconductancevalueof.5whenactive.(SeeFranketal.,2001,forfurtherdiscussionofthiskindofmaintenancemechanism,whichhasbeenproposedbyseveralresearcherse.g.,Lewis&ODonnell,2000;Fellousetal.,1998;Wang,1999;Dilmore,Gutkin,&Ermentrout,1999;Gorelova&Yang,2000;Durstewitz,Seamans,&Sejnowski,2000b.)TherstopportunitytotogglePFCmain-tenanceoccursattheendoftherstplusphaseandthenagainattheendofthesecondplusphase(thirdphaseofsettling).Thus,acompleteupdatecanbetriggeredbytwoGosinarow,anditisalmostalwaysthecasethatifaGoresthersttime,itwillrethenext,becauseStriatumringisprimarilydrivenbysensoryinputs,whichremainconstant.AcknowledgmentsThisworkwassupportedbyONRgrantsN00014-00-1-0246andN00014-03-1-0428andNIHgrantsMH64445andMH069597.ThankstoToddBraver,JonCohen,PeterDayan,DavidJilk,DavidNoelle,NicolasRougier,Tom

=== Page 42 ===
324R.OReillyandM.FrankHazy,DanielCer,andmembersoftheCCNLabforfeedbackanddiscussiononthiswork.ReferencesAbbott,L.F.,Varela,J.A.,Sen,K.,&Nelson,S.B.(1997).Synapticdepressionandcorticalgaincontrol.Science,275,220.Alexander,G.E.,DeLong,M.R.,&Strick,P.L.(1986).Parallelorganizationoffunctionallysegregatedcircuitslinkingbasalgangliaandcortex.AnnualReviewofNeuroscience,9,357381.Amos,A.(2000).Acomputationalmodelofinformationprocessinginthefrontalcortexandbasalganglia.JournalofCognitiveNeuroscience,12,505519.Anderson,J.R.,Bothell,D.,Byrne,M.D.,Douglass,S.,Lebiere,C.,&Qin,Y.(2004).Anintegratedtheoryofthemind.PsychologicalReview,111(4),10361060.Anderson,J.R.,&Lebiere,C.(1998).Theatomiccomponentsofthought.Mahwah,NJ:Erlbaum.Baddeley,A.D.(1986).Workingmemory.NewYork:OxfordUniversityPress.Baddeley,A.,Gathercole,S.,&Papagno,C.(1998).Thephonologicalloopasalan-guagelearningdevice.PsychologicalReview,105,158.Beiser,D.G.,&Houk,J.C.(1998).Modelofcortical-basalganglionicprocessing:Encodingtheserialorderofsensoryevents.JournalofNeurophysiology,79,31683188.Berns,G.S.,&Sejnowski,T.J.(1995).Howthebasalgangliamakedecisions.InA.Damasio,H.Damasio,&Y.Christen(Eds.),Neurobiologyofdecision-making(pp.101113).Berlin:Springer-Verlag.Berns,G.S.,&Sejnowski,T.J.(1998).Acomputationalmodelofhowthebasalgangliaproducessequences.JournalofCognitiveNeuroscience,10,108121.Braver,T.S.,&Cohen,J.D.(2000).Onthecontrolofcontrol:Theroleofdopamineinregulatingprefrontalfunctionandworkingmemory.InS.Monsell&J.Driver(Eds.),Controlofcognitiveprocesses:AttentionandperformanceXVIII(pp.713737).Cambridge,MA:MITPress.Burgess,N.,&Hitch,G.J.(1999).Memoryforserialorder:Anetworkmodelofthephonologicalloopanditstiming.PsychologicalReview,106,551581.Cardinal,R.N.,Parkinson,J.A.,Hall,J.,&Everitt,B.J.(2002).Emotionandmoti-vation:Theroleoftheamygdala,ventralstriatum,andprefrontalcortex.Neuro-scienceandBiobehavioralReviews,26,321352.Cohen,J.D.,Braver,T.S.,&OReilly,R.C.(1996).Acomputationalapproachtoprefrontalcortex,cognitivecontrol,andschizophrenia:Recentdevelopmentsandcurrentchallenges.PhilosophicalTransactionsoftheRoyalSociety(London)B,351,15151527.Contreras-Vidal,J.L.,&Schultz,W.(1999).Apredictivereinforcementmodelofdopamineneuronsforlearningapproachbehavior.JournalofComparativeNeuro-science,6,191214.

=== Page 43 ===
MakingWorkingMemoryWork325Dilmore,J.G.,Gutkin,B.G.,&Ermentrout,G.B.(1999).Effectsofdopaminergicmodulationofpersistentsodiumcurrentsontheexcitabilityofprefrontalcorticalneurons:Acomputationalstudy.Neurocomputing,26,104116.Dominey,P.,Arbib,M.,&Joseph,J.-P.(1995).Amodelofcorticostriatalplasticityforlearningoculomotorassociationsandsequences.JournalofCognitiveNeuroscience,7,311336.Durstewitz,D.,Kelc,M.,&Gunturkun,O.(1999).Aneurocomputationaltheoryofthedopaminergicmodulationofworkingmemoryfunctions.JournalofNeuro-science,19,2807.Durstewitz,D.,Seamans,J.K.,&Sejnowski,T.J.(2000a).Dopamine-mediatedstabi-lizationofdelay-periodactivityinanetworkmodelofprefrontalcortex.JournalofNeurophysiology,83,17331750.Durstewitz,D.,Seamans,J.K.,&Sejnowski,T.J.(2000b).Neurocomputationalmod-elsofworkingmemory.NatureNeuroscience,3(Suppl.),11841191.Elman,J.L.(1990).Findingstructureintime.CognitiveScience,14,179211.Emerson,M.J.,&Miyake,A.(2003).Theroleofinnerspeechintaskswitching:Adual-taskinvestigation.JournalofMemoryandLanguage,48,148168.Fellous,J.M.,Wang,X.J.,&Lisman,J.E.(1998).AroleforNMDA-receptorchannelsinworkingmemory.NatureNeuroscience,1,273275.Frank,M.J.(2005).Dynamicdopaminemodulationinthebasalganglia:Aneu-rocomputationalaccountofcognitivedecitsinmedicatedandnon-medicatedParkinsonism.JournalofCognitiveNeuroscience,17,5172.Frank,M.J.,&Claus,E.D.(2005).Anatomyofadecision:Striato-orbitofrontalinteractionsinreinforcementlearning,decisionmakingandreversal.Manuscriptsubmittedforpublication.Frank,M.J.,Loughry,B.,&OReilly,R.C.(2001).Interactionsbetweenthefrontalcortexandbasalgangliainworkingmemory:Acomputationalmodel.Cognitive,Affective,andBehavioralNeuroscience,1,137160.Frank,M.J.,Seeberger,L.,&OReilly,R.C.(2004).Bycarrotorbystick:CognitivereinforcementlearninginParkinsonism.Science,306,19401943.Gerfen,C.R.(2000).Moleculareffectsofdopamineonstriatalprojectionpathways.TrendsinNeurosciences,23,S64S70.Gers,F.A.,Schmidhuber,J.,&Cummins,F.(2000).Learningtoforget:ContinualpredictionwithLSTM.NeuralComputation,12,24512471.Gorelova,N.A.,&Yang,C.R.(2000).DopamineD1/D5receptoractivationmod-ulatesapersistentsodiumcurrentinratsprefrontalcorticalneuronsinvitro.JournalofNeurophysiology,84,75.Graybiel,A.M.,&Kimura,M.(1995).Adaptiveneuralnetworksinthebasalganglia.InJ.C.Houk,J.L.Davis,&D.G.Beiser(Eds.),Modelsofinformationprocessinginthebasalganglia(pp.103116).Cambridge,MA:MITPress.Haber,S.N.,Fudge,J.L.,&McFarland,N.R.(2000).Striatonigrostriatalpathwaysinprimatesformanascendingspiralfromtheshelltothedorsolateralstriatum.JournalofNeuroscience,20,23692382.Hernandez-Lopez,S.,Bargas,J.,Surmeier,D.J.,Reyes,A.,&Galarraga,E.(1997).D1receptoractivationenhancesevokeddischargeinneostriatalmediumspinyneuronsbymodulatinganL-typeCa2+conductance.JournalofNeuroscience,17,33343342.

=== Page 44 ===
326R.OReillyandM.FrankHernandez-Lopez,S.,Tkatch,T.,Perez-Garci,E.,Galarraga,E.,Bargas,J.,Hamm,H.,&Surmeier,D.J.(2000).D2dopaminereceptorsinstriatalmediumspinyneuronsreduceL-typeCa2+currentsandexcitabilityviaanovelPLC1-IP3-calcineurin-signalingcascade.JournalofNeuroscience,20,89878995.Hochreiter,S.,&Schmidhuber,J.(1997).Longshorttermmemory.NeuralComputa-tion,9,17351780.Houk,J.C.,Adams,J.L.,&Barto,A.G.(1995).Amodelofhowthebasalgangliagenerateanduseneuralsignalsthatpredictreinforcement.InJ.C.Houk,J.L.Davis,&D.G.Beiser(Eds.),Modelsofinformationprocessinginthebasalganglia(pp.233248).Cambridge,MA:MITPress.Houk,J.C.,&Wise,S.P.(1995).Distributedmodulararchitectureslinkingbasalganglia,cerebellum,andcerebralcortex:Theirroleinplanningandcontrollingaction.CerebralCortex,5,95110.Jackson,S.,&Houghton,G.(1995).Sensorimotorselectionandthebasalganglia:Aneuralnetworkmodel.InJ.C.Houk,J.L.Davis,&D.G.Beiser(Eds.),Modelsofinformationprocessinginthebasalganglia(pp.337368).Cambridge,MA:MITPress.Jaeger,D.,Kita,H.,&Wilson,C.J.(1994).Surroundinhibitionamongprojectionneuronsisweakornonexistentintheratneostriatum.JournalofNeurophysiology,72,25552558.Joel,D.,Niv,Y.,&Ruppin,E.(2002).Actor-criticmodelsofthebasalganglia:Newanatomicalandcomputationalperspectives.NeuralNetworks,15,535547.Joel,D.,&Weiner,I.(2000).Theconnectionsofthedopaminergicsystemwiththestriatuminratsandprimates:Ananalysiswithrespecttothefunctionalandcompartmentalorganizationofthestriatum.Neuroscience,96,451.Jordan,M.I.(1986).Attractordynamicsandparallelisminaconnectionistsequen-tialmachine.InProceedingsofthe8thConfererenceoftheCognitiveScienceSociety(pp.531546).Hillsdale,NJ:Erlbaum.Kropotov,J.D.,&Etlinger,S.C.(1999).Selectionofactionsinthebasalganglia-thalamocoritcalcircuits:Reviewandmodel.InternationalJournalofPsychophysiol-ogy,31,197217.Levitt,J.B.,Lewis,D.A.,Yoshioka,T.,&Lund,J.S.(1993).Topographyofpyramidalneuronintrinsicconnectionsinmacaquemonkeyprefrontalcortex(areas9and46).JournalofComparativeNeurology,338,360376.Lewis,B.L.,&ODonnell,P.(2000).VentraltegmentalareaafferentstotheprefrontalcortexmaintainmembranepotentialupstatesinpyramidalneuronsviaD1dopaminereceptors.CerebralCortex,10,11681175.Middleton,F.A.,&Strick,P.L.(2000).Basalgangliaandcerebellarloops:Motorandcognitivecircuits.BrainResearchReviews,31,236250.Mink,J.W.(1996).Thebasalganglia:Focusedselectionandinhibitionofcompetingmotorprograms.ProgressinNeurobiology,50,381425.Montague,P.R.,Dayan,P.,&Sejnowski,T.J.(1996).AframeworkformesencephalicdopaminesystemsbasedonpredictiveHebbianlearning.JournalofNeuroscience,16,19361947.Morton,J.B.,&Munakata,Y.(2002).Activeversuslatentrepresentations:Aneuralnetworkmodelofperseverationanddissociationinearlychildhood.Developmen-talPsychobiology,40,255265.

=== Page 45 ===
MakingWorkingMemoryWork327Nakahara,H.,Doya,K.,&Hikosaka,O.(2001).Parallelcortico-basalgangliamech-anismsforacquisitionandexecutionofvisuomotorsequencesacomputationalapproach.JournalofCognitiveNeuroscience,13,626647.Nicola,S.M.,Surmeier,J.,&Malenka,R.C.(2000).Dopaminergicmodulationofneuronalexcitabilityinthestriatumandnucleusaccumbens.AnuualReviewofNeuroscience,23,185215.Oja,E.(1982).Asimpliedneuronmodelasaprincipalcomponentanalyzer.JournalofMathematicalBiology,15,267273.OReilly,R.C.(1996).Biologicallyplausibleerror-drivenlearningusinglocalacti-vationdifferences:Thegeneralizedrecirculationalgorithm.NeuralComputation,8(5),895938.OReilly,R.C.(1998).Sixprinciplesforbiologically-basedcomputationalmodelsofcorticalcognition.TrendsinCognitiveSciences,2(11),455462.OReilly,R.C.(2001).Generalizationininteractivenetworks:Thebenetsofin-hibitorycompetitionandHebbianlearning.NeuralComputation,13,11991242.OReilly,R.C.,Braver,T.S.,&Cohen,J.D.(1999).Abiologicallybasedcomputa-tionalmodelofworkingmemory.InA.Miyake&P.Shah(Eds.),Modelsofworkingmemory:Mechanismsofactivemaintenanceandexecutivecontrol.(pp.375411).Cam-bridge:CambridgeUniversityPress.OReilly,R.C.,&Frank,M.J.(2003).Makingworkingmemorywork:Acomputationalmodeloflearninginthefrontalcortexandbasalganglia(ICSTech.Rep.03-03,revised8/04).UniversityofColoradoInstituteofCognitiveScience.OReilly,R.C.,Frank,M.J.,Hazy,T.E.,&Watz,B.(2005).Rewardsaretimeless:Theprimaryvalueandlearnedvalue(PVLV)Pavlovianlearningalgorithm.Manuscriptsubmittedforpublication.OReilly,R.C.,&Munakata,Y.(2000).Computationalexplorationsincognitiveneu-roscience:Understandingthemindbysimulatingthebrain.Cambridge,MA:MITPress.OReilly,R.C.,Noelle,D.,Braver,T.S.,&Cohen,J.D.(2002).Prefrontalcortexanddy-namiccategorizationtasks:Representationalorganizationandneuromodulatorycontrol.CerebralCortex,12,246257.OReilly,R.C.,&Soto,R.(2002).Amodelofthephonologicalloop:Generalizationandbinding.InT.G.Dietterich,S.Becker,&Z.Ghahramani(Eds.),Advancesinneuralinformationprocessingsystems,14.Cambridge,MA:MITPress.Pucak,M.L.,Levitt,J.B.,Lund,J.S.,&Lewis,D.A.(1996).Patternsofintrinsicandassociationalcircuitryinmonkeyprefrontalcortex.JournalofComparativeNeurology,376,614630.Rescorla,R.A.,&Wagner,A.R.(1972).AtheoryofPavlovianconditioning:Variationintheeffectivenessofreinforcementandnon-reinforcement.InA.H.Black&W.F.Prokasy(Eds.),ClassicalconditioningII:Theoryandresearch(pp.6499).NewYork:Appleton-Century-Crofts.Robinson,A.J.,&Fallside,F.(1987).Theutilitydrivendynamicerrorpropagationnetwork(Tech.Rep.CUED/F-INFENG/TR.1).Cambridge:CambridgeUniversityEngineeringDepartment.Rougier,N.P.,Noelle,D.,Braver,T.S.,Cohen,J.D.,&OReilly,R.C.(2005).Prefrontalcortexandtheexibilityofcognitivecontrol:Ruleswithoutsymbols.ProceedingsoftheNationalAcademyofSciences,102(20),73387343.

=== Page 46 ===
328R.OReillyandM.FrankRougier,N.P.,&OReilly,R.C.(2002).Learningrepresentationsinagatedprefrontalcortexmodelofdynamictaskswitching.CognitiveScience,26,503520.Schmidhuber,J.(1992).Learningunambiguousreducedsequencedescriptions.InJ.E.Moody,S.J.Hanson,&R.P.Lippmann(Eds.),Advancesinneuralinformationprocessingsystems,4(pp.291298).SanMateo,CA:MorganKaufmann.Schultz,W.(1998).Predictiverewardsignalofdopamineneurons.JournalofNeuro-physiology,80,1.Schultz,W.,Apicella,P.,&Ljungberg,T.(1993).Responsesofmonkeydopamineneuronstorewardandconditionedstimuliduringsuccessivestepsoflearningadelayedresponsetask.JournalofNeuroscience,13,900913.Schultz,W.,Dayan,P.,&Montague,P.R.(1997).Aneuralsubstrateofpredictionandreward.Science,275,1593.Schultz,W.,Romo,R.,Ljungberg,T.,Mirenowicz,J.,Hollerman,J.R.,&Dickinson,A.(1995).Reward-relatedsignalscarriedbydopamineneurons.InJ.C.Houk,J.L.Davis,&D.G.Beiser(Eds.),Modelsofinformationprocessinginthebasalganglia(pp.233248).Cambridge,MA:MITPress.Schultz,W.,Apicella,P.,Scarnati,D.,&Ljungberg,T.(1992).Neuronalactivityinmonkeyventralstriatumrelatedtotheexpectationofreward.JournalofNeuro-science,12,45954610.Suri,R.E.,Bargas,J.,&Arbib,M.A.(2001).Modelingfunctionsofstriataldopaminemodulationinlearningandplanning.Neuroscience,103,6585.Sutton,R.S.(1988).Learningtopredictbythemethodoftemporaldiferences.MachineLearning,3,944.Sutton,R.S.,&Barto,A.G.(1998).Reinforcementlearning:Anintroduction.Cambridge,MA:MITPress.Wang,X.-J.(1999).Synapticbasisofcorticalpersistentactivity:TheimportanceofNMDAreceptorstoworkingmemory.JournalofNeuroscience,19,9587.Wickens,J.(1993).Atheoryofthestriatum.Oxford:PergamonPress.Wickens,J.R.,Kotter,R.,&Alexander,M.E.(1995).Effectsoflocalconnectiv-ityonstriatalfunction:Simulationandanalysisofamodel.Synapse,20,281298.Widrow,B.,&Hoff,M.E.(1960).Adaptiveswitchingcircuits.InInstituteofRadioEngineers,WesternElectronicShowandConvention,ConventionRecord,Part4(pp.96104).NewYork:InstituteofRadioEngineers.Williams,R.J.,&Zipser,D.(1992).Gradient-basedlearningalgorithmsforre-currentnetworksandtheircomputationalcomplexity.InY.Chauvin&D.E.Rumelhart(Eds.),Backpropagation:Theory,architecturesandapplications.Hillsdale,NJ:Erlbaum.ReceivedAugust9,2004;acceptedJune14,2005.
