                                                                                                   SegPoint        9
                                 loss L   andDICElossL         , aims at refining segmentation quality. The weights
                                       bce                 dice
                                 λ , λ     and λ      are utilized to balance the different loss items. The model’s
                                  txt   bce      dice
                                 training is guided by the ground-truth labels ytxt for text and M for masks.
                                 3.6   Instruct3D Dataset Collection
                                 Although3Dinstructionsegmentationand3Dreferringsegmentation[1,4,79]are
                                 both language-based segmentation, 3D referring segmentation guides segmenta-
                                 tion with explicit target object names, e.g., “chair”, lacking more complicated
                                 reasoning instructions, e.g., “Where to sit in the room?”. Besides, they also fall
                                 short in offering multi-target question-answer pairs with target descriptions di-
                                 rectly connected to multiple segmentation masks, which cannot meet a common
                                 requirement in real-world scenarios, like “How to play computer games”.
                                    To enhance the assessment and analysis of instruction segmentation capabil-
                                 ities, we have developed a benchmark, referred to Instruct3D. This benchmark
                                 incorporates 280 scenes specifically selected for instruction segmentation tuning
                                 and evaluation, sourced from the recently introduced ScanNet++ [74] dataset.
                                 Each scene comes with approximately 10 different segmentation instructions,
                                 resulting in 2,565 instruction-point cloud pairings. This dataset is then divided
                                 into two splits: train, and val, containing 2,052, and 513 question-answer pairs,
                                 respectively. Our dataset is uniquely designed to encompass both multi-target
                                 and zero-target scenarios, addressing the real-world requirement of identifying
                                 multiple objects in response to text queries and accounting for situations where
                                 objects mentioned in the text may not be present in the paired point cloud.
                                 Besides, we take into account the characteristics of 3D scenes and incorporate
                                 diverse locations and view descriptions e.g., “something that is used for sitting
                                 while working at a desk. It is the one facing the window.”. The model needs to
                                 have not only reasoning capabilities but also the ability to perceive views and
                                 directions in 3D scenes. These designs underscore the dataset’s practical value.
                                 4    Experiments
                                 4.1   Datasets and Evaluation Metrics
                                 Datasets. Our training data is composed of two types of datasets: (1) semantic
                                 segmentation dataset including ScanNet200 [53], and S3DIS [3]; (2) referring
                                 segmentation dataset consisting of ScanRefer [4], ReferIt3D [1](including Sr3D
                                 and Nr3D), and Multi3DRefer [79]. We design task-specific prompts to facilitate
                                 the joint training of various tasks within a unified framework.
                                 Semantic Segmentation Dataset. We use two strategies to generate tem-
                                 plates. 1) segment the specific category: “USER: <POINT> Can you segment the
                                 {category} category in this point cloud? ASSISTANT: {category} <SEG>.”,
                                 where category is the random chosen category, and <POINT> denotes the place-
                                 holder for tokens of point cloud patches. 2) segment all the categories: “USER:
                                 <POINT> Can you segment all the semantic masks in this point cloud and
                                 output separate masks for each category in the alphabetical order of the
