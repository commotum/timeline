                        Preprint.
                        derive new facts or explore ideas from existing facts, and link each suggestion (idea) with the prior
                        observations that inferred this suggestion. However, intermediate inferences may be implicit or
                        difficult to extract from a trace, or, in the case of specific commercial models, completely hidden. To
                        address this challenge, we generate a post-hoc derivation: an interleaved sequence of observations and
                        thoughts/reasoning steps constructed from the input-output of the final solution. These reconstructed
                        traces are then used to extract situation-suggestion pairs, forming structured memory entries for
                        guiding future analyses. This minimal formulation does not consider the existing contents of memory
                        while adding new entries for simplicity and scaling reasons. It defers handling redundancy and
                        consolidating ideas to the concept selection and problem-solving phases, respectively.
                        PS Abstraction.  We find that directly converting solutions into routines often leads to minor
                        implementation details being recorded as memories, as opposed to our intended abstract concepts.
                        Wemitigate this by preprocessing solutions into pseudocode to prioritize higher-level operations
                        over low-level implementation details. The main abstraction phase proceeds from the generated
                        pseudocode, recording new concepts and revising existing concepts along with their various fields.
                        In the spirit of designing for abstraction, promoting reusability, and minimizing total concept memory
                        length, this operation is aware of existing concepts with a compressed form of memory included
                        in context. We encourage the model to reuse and revise existing concepts by updating concept
                        descriptions, parameter lists, relevance cues, and implementation notes. Importantly, we encourage
                        higher-order functions with routines as arguments in instructions and few-shot demonstrations.
                        All concept abstraction/memory writing operations are scaffolded with few-shot demonstrations,
                        example-rich templates, and comprehensive instructions.
                        3.4  MEMORYREAD: CONCEPTSELECTION
                        Although state-of-the-art models support longer context windows, their maximum length still limits
                        howmuchinformationorhowmanymemoryentriescanbeincludedinasinglequery. Evenwithout
                        this hard limit, including all available memories in context is still undesirable: it can distract the
                        model with irrelevant details and flood it with too many unlikely hypotheses to consider. A more
                        effective strategy is to introduce a selection mechanism only to include the most relevant subset of
                        memoryentries at problem-solving time. This allows the memory store to grow continually without
                        overwhelming the model’s context window.
                        OESelection.   OE-format memory entries were explicitly designed to support memory-based
                        selection. The situation clause acts as a semantic hook to identify relevance. The key idea for
                        OESelection is a preprocessing step that leverages model reasoning capability to parse the abstract
                        domain input into problem descriptions at different levels of abstraction to facilitate matching against
                        our abstract concepts. Since ARC-AGI deals with spatial reasoning, we leverage a vision language
                        modelfor this preprocessing step. We caption each puzzle using a structured prompt that separates
                        concrete observations from speculative transformations. This converts spatially rich input into a
                        natural language format suitable for matching against stored situations. We then query a model for the
                        top-k most relevant entries using the generated description. We explored scoring, thresholding, and
                        cumulative similarity approaches (as in top-p sampling). Observing comparable results, we selected
                        top-k cutoff for simplicity. We consider this a light-weight approach to converting abstract domain
                        inputs into more understandable and memory-aligned representations.
                        PSSelection.  Standard embedding approaches use a single forward pass per embedding, represent-
                        ing a System 1-style fast thinking/intuition-driven understanding from the model. Intuitions may not
                        be sufficient for difficult/frontier tasks and domains. Moreover, the connection between a higher-level
                        concept and a specific problem instance may not be immediately clear because of the abstraction.
                        Toaddress these challenges, we propose reasoning-based selection–using System 2-style thinking
                        to deliberately think and explore. In contrast to the more straightforward playbooks curated in the
                        OEmemory format, the explicitly abstract PS concepts are a set of puzzle pieces with notes on
                        identifying if a piece is relevant (relevance cues) and on how various pieces fit together (type annota-
                        tions). To select from this memory, we propose leveraging model-driven exploration instantiated by
                        recent models’ long-form reasoning with backtracking. PS Selection instructs a reasoning model to
                        systematically explore the problem: first identify initial concepts using relevance cue annotations,
                                                                  6
