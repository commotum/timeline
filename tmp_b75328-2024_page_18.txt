                                        0   LT, Abacus       LT, FIRE         LT, NoPE                0  LT, Abacus        LT, FIRE        LT, NoPE
                                                                                            100                                                           100
                                       50                                                            50
                                      100                                                   80      100                                                   80
                                        0   ST, Abacus       ST, FIRE         ST, NoPE                0  ST, Abacus       ST, FIRE         ST, NoPE
                                                                                            60                                                            60
                                       50                                                            50
                                      100                                                   40      100                                                   40
                                         ST w/ II, Abacus  ST w/ II, FIRE  ST w/ II, NoPE              ST w/ II, Abacus ST w/ II, FIRE  ST w/ II, NoPE
                                     Length of Operand One0                                       Length of Operand One0
                                                                                            20                                                            20
                                       50                                                            50
                                                                                            0                                                             0
                                      100                                                           100
                                         0      50    100 0     50     100 0     50    100             0     50    100 0      50    100 0     50     100
                                                        Length of Operand Two                                        Length of Operand Two
                                  Figure 13: Full 100×100 exact match accuracy plots, taking the mean over three models. Left: Size
                                  30 training data, corresponding to Bottom Left Figure 9; Right: Size 40 training data, corresponding
                                  to Bottom Right Figure 9 and Right of Figure 3.
                                  A.7.2     RemovingMaskingBeforeEquals
                                  Wemaskalltokensbeforetheequals sign in all of our experiments, we hypothesize that with more
                                  training time this constraint may be able to be removed. In Figure 16, we show the effect of training
                                  with the same amount of flops as the other addition experiments without masking before the equals
                                  sign.
                                  A.7.3     Varying Effective Depth
                                  WebegininFigure17byshowingareplicaofFigure4,this time including comparisons to FIRE and
                                  NoPEembeddings. Seeing, yet again, the improvements Abacus Embeddings give for addition.
                                  In Figure 18, we present models with effective depths 8 and more than 16, respectively. In Figure
                                  18 (left), we see that the effective depth 8 models under perform the models with 8 layers in the
                                  recurrent block and two recurrences shown in Figure 4, demonstrating the benefit of recurrence in this
                                  case. We see very high accuracy from all models in Figure 18 (right). Again, the depth 32 recurrent
                                  models outperform the standard models with input injection, even though it only has approximately
                                  a quarter of the parameters and achieves the highest OOD mean accuracy of all models presented.
                                  These ablations show that with Abacus Embeddings the addition task can be learned across many
                                  effective depths to varying degrees of accuracy.
                                  In Figure 19 (left), we remove the input injection to the intermediate layers in the recurrent block,
                                  only keeping input injection to the first layer of the recurrent block. In Figure 19 (right) we divide
                                  the gradients in the recurrent block by the number of recurrences for the looped transformer models
                                  during training. We see very minor performance changes for all models shown in Figure 19, with the
                                  2×8modelimprovingitsperformanceslightly in left plot and the 4×4 model improving slightly
                                  in the right plot. We ablate this design choices as we have to remove the input injection inside of
                                  the recurrent and divide the gradients in the recurrent block by the number of recurrences for the
                                  multiplication models show in Figure 6. Hence, we can conclude there would only be very minor
                                  performance changes in this case for addition.
                                  A.7.4     AddingrandomizedPadding
                                  AbacusEmbeddingsgivestrongpriors for numerical tasks but without them, looped transformers
                                  perform better than the standard transformer architectures we present. The result shown in Figure
                                  20aligns well with the hypothesis that with fewer priors the looped transformer models are able to
                                  generalize better. In this case the priors are reduced as the training data is noised with random pad
                                                                                              18
