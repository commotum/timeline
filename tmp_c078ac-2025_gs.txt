                               CHARTQAPRO:AMoreDiverseandChallengingBenchmark
                                                      for Chart Question Answering
                                          ♣ * †                                     ♣ *                     ♣ ‡                     ♠‡
                       AhmedMasry               , MohammedSaidulIslam                  , Mahir Ahmed            , Aayush Bajaj
                                                ♣‡                           ♣‡                                         ♣♡‡
                                Firoz Kabir        , AaryamanKartha ,MdTahmidRahmanLaskar
                                                     ♣⋆‡                             ♣‡                                      ♣‡
                             MizanurRahman                , Shadikur Rahman ,MehradShahmohammadi
                                                    ♠                            §                     ♣                  ♢△
                               MeghThakkar ,MdRizwanParvez ,EnamulHoque ,ShafiqJoty
                                     ♣YorkUniversity, Canada, ♡Dialpad Inc., Canada, ⋆RBC, Canada
                         ♠MILA-QuebecAIInstitute,Canada,§Qatar Computing Research Institute (QCRI)
                              ♢NanyangTechnological University, Singapore, △Salesforce Research, USA
                                        {masry20, saidulis, mrahmed, mdfkabir, aarykary}@yorku.ca
                                        {tahmid20, mizanurr, shadikur, msm97, enamulh}@yorku.ca
                     {aayush.bajaj, megh.thakkar}@mila.quebec, mparvez@hbku.edu.qa, sjoty@salesforce.com
                                          Abstract
                        Charts are ubiquitous, as people often use
                        them to analyze data, answer questions, and
                        discover critical insights. However, perform-
                        ing complex analytical tasks with charts re-
                        quires significant perceptual and cognitive ef-
                        fort. Chart Question Answering (CQA) sys-
                        tems automate this process by enabling mod-
                        els to interpret and reason with visual repre-
                        sentations of data. However, existing bench-
                        marks like ChartQA lack real-world diversity
                        and have recently shown performance satura-              Figure 1: Performance gap between ChartQA (Masry
                        tion with modern large vision-language models            et al., 2022) and CHARTQAPRO for various LVLMs.
                       (LVLMs). To address these limitations, we in-             formed decisions across various domains such as
                        troduce CHARTQAPRO,anewbenchmarkthat                     finance, journalism, and science (Kim et al., 2020;
                        includes 1,341 charts from 157 diverse sources,          Masryetal., 2024b; Hoque et al., 2022). However,
                        spanning various chart types—including info-             answeringcomplexquestionsaboutchartscanpose
                        graphics and dashboards—and featuring 1,948              significant challenges as the user needs to combine
                        questions in various types, such as multiple-
                        choice, conversational, hypothetical, and unan-          visual perception with cognitive reasoning. Chart
                        swerable questions, to better reflect real-world         Question Answering (CQA) systems aim to assist
                        challenges.   Our evaluations with 21 mod-               users by taking questions about charts as input and
                        els show a substantial performance drop for              generating answers. Unlike traditional visual ques-
                        LVLMsonCHARTQAPRO;e.g.,ClaudeSon-                        tion answeringinvolvingnaturalimagesandscenes,
                        net 3.5 scores 90.5% on ChartQA but only                 CQArequires models to interpret structured data
         arXiv:2504.05506v2  [cs.CL]  10 Apr 202555.81%on CHARTQAPRO,underscoringthevisually, reason over relationships among visual
                        complexity of chart reasoning. We comple-                elements and text, and derive contextual insights.
                        ment our findings with detailed error analy-
                        ses and ablation studies, identifying key chal-             Due to its real-world relevance, CQA has
                        lenges and opportunities for advancing LVLMs             become a key task for evaluating recent
                        in chart understanding and reasoning. We re-             LVLMs (Wang et al., 2024a; OpenAI et al.,
                        lease CHARTQAPROathttps://github.com/vis-                2024; Georgiev et al., 2024; Grattafiori et al.,
                        nlp/ChartQAPro.                                          2024) . These LVLMs have obtained remarkable
                   1    Introduction                                             performance on multimodal tasks, including CQA.
                   Data visualizations such as bar and line charts are           For instance, on ChartQA (Masry et al., 2022),
                   very popular for analyzing data and making in-                Claude Sonnet 3.5 (Anthropic, 2024) achieves an
                       * Equal contribution.                                     accuracy of 90.5%, while GPT4 (OpenAI et al.,
                       † Corresponding author.                                   2024) and Gemini (Georgiev et al., 2024) reach
                       ‡ Equal contribution.                                     85.7%and87.2%,respectively (Figure 1). Open-
                                                                                                                                                                            (d) Multiple-Choice
                                                                                                                                (c) Conversational
                                (a) Mathematical Reasoning                        (b) Visual Reasoning
                                                                                                                                                                                          Question:    What
                                                                                                                                                                                          was  the  percentage
                                                                                                                                                                                          change   in   hate
                                                                                                                                                                                          crimes  motivated  by
                                                                                                                                                                                          religion from 2021 to
                                                                                                                                                                                          2022?
                                                                                                                                                                                          A) 10% decrease
                                                                                                                       Q1: How many peaks does Period 8 have?
                                                                                                                                                                                          B) 15% decrease
                                                                         Question:    At  which  date  the  blue  bar  had  a
                             Question:    Calculate  the  total  percentage  of
                                                                                                                       A1: 2
                                                                                                                                                                                          C) 18% decrease
                                                                         value larger than 500 and the orange bar had a
                             deals  made by buyers from the USA, Japan,
                                                                                                                       Q2: Which event caused the most significant
                                                                                                                                                                                          D) 20% decrease
                                                                         value below 2500?
                             and Singapore combined.
                                                                                                                       spike in tweets per day within Period 5?  
                                                                                                                       A2: Wilson non-indictment
                                                                                                                                                                                          Answer:  B
                             Answer:  17                                 Answer:  March 31
                                                                                                                       Q3: Is this the largest peak in the graph?
                                        (e) Hypothetical
                                                                                                                       A3: Yes
                                                                                                                                                                           (h) Multi-Chart QA
                                                                                   (f) Fact-Checking
                                                                                                                                (g) Unanswerable
                              Question: If the percentage of small business
                                                                                                                                                                  Question: What is the difference in vaccination
                              owners identifying  as  male  decreases  by  10
                                                                          Question:    The  Nintendo  Switch  Lite  sold
                                                                                                                                                                  rates  between  the  South  Asian  and  Mixed
                              percentage  points,  what  would  the  new
                                                                                                                       Question:  What  is  the  total  count  of
                                                                          exactly one-sixth as many units as the Nintendo
                                                                                                                                                                  ethnicities in the 65-69 age group?
                              percentage be?
                                                                                                                       hospitalizations on August 31st?
                                                                          Switch between 2016 and 2020.
                                                                                                                                                                   Answer:  5.50%
                              Answer:  63%                                Answer:  False
                                                                                                                        Answer:  Unanswerable
                            Figure 2: CHARTQAPRO covers a more diverse range of questions compared to existing chart question answering
                            datasets (Table 1), providing an extensive evaluation of chart understanding abilities.
                            source LVLMs also appear to be catching up, with                                            to analyze multiple charts simultaneously. These
                            Qwen2.5-VL(Wangetal.,2024a)reporting89.5%.                                                  types of questions and complex layouts are absent
                            These striking results prompt two core questions:                                           from current benchmarks, suggesting that existing
                            (i) Is chart understanding and reasoning already                                            evaluations do not fully capture the real-world chal-
                            a solved task? and (ii) Have open-source models                                             lenges in chart understanding and create an overly
                            truly matched their closed-source counterparts?                                             optimistic perception of progress in this field.
                                 AcloserlookatChartQArevealskeylimitations.                                                  Toaddress these limitations and rigorously eval-
                            First, its chart images lack visual diversity, coming                                       uate LVLMs’ on chart understanding, we present
                            from a few online sources like Statista and Pew                                             CHARTQAPRO, a comprehensive benchmark of
                            Research Center. It primarily includes only bar,                                            1341charts sourced from 157 diverse online plat-
                            line, and pie charts with numeric labels directly                                           forms.          CHARTQAPRO includes 1948 human-
                            on visual elements, reducing the need for actual                                            written, human-verified question-answer pairs cov-
                            visual reasoning. Second, the benchmark focuses                                             ering factoid, multiple-choice, conversational, hy-
                            largely on factoid questions that require simple data                                       pothetical, multi-chart, and unanswerable queries,
                            extraction or basic arithmetic. Earlier datasets (Ka-                                       making it representative of real-world use cases
                            houet al., 2017; Chaudhry et al., 2020; Singh and                                           (see Figure 2). Beyond bar, line, and pie charts,
                            Shekhar, 2020) suffer from similar issues, and are                                          CHARTQAPROfeaturesimageswithcomplexvisu-
                            also curated from synthetic data or templated ques-                                         alizations such as multi-chart layouts, infographics,
                            tions. Although a recent work, CharXiv (Wang                                                and dashboards, introducing greater visual and an-
                            et al., 2024b), addresses some of these limitations,                                        alytical complexity. Inspired by conversational and
                            it relies on charts sourced exclusively from papers                                         multi-document QA in text such as CoQA (Reddy
                            onarXiv, limiting visual and topical diversity, and                                         et al., 2019) and HotpotQA (Yang et al., 2018),
                            also lacking numerous real-world question types.                                            somequestions also require multi-turn interactions
                                 In contrast, real-world charts encompass diverse                                       or referencing accompanying paragraphs, probing
                            domains like economy, health, etc., and a wide va-                                          a broader range of multimodal reasoning skills.
                            riety of question types, including hypothetical (e.g.,                                           Our evaluations reveal a sharp performance
                            future price prediction), multiple-choice (e.g., in ed-                                     drop for both closed- and open-source models on
                            ucational exams), conversational (e.g., in decision-                                        CHARTQAPRO(Figure1). Forexample,theSoTA
                            making meetings) and unanswerable (e.g. due to                                              Claude Sonnet 3.5’s accuracy falls from 90.50%
                            missing data). Additionally, multi-chart layouts                                            to 55.81%, demonstrating that CHARTQAPRO
                            and dashboards are often used in finance, business                                          presents a more challenging and realistic bench-
                            intelligence, and scientific reports, requiring users                                       mark for chart understanding, and that there is
                                                                                              ChartImages                                                                   Question Types
                              Dataset                         Real vs.   # Chart TopicDiversity       Infographics    Accompanying Multi MCQ Conversational Hypothetical Unanswerable FactChecking
                                                             Synthetic   Sources                     &Dashboards        Paragraph      Chart
                              PlotQA(Methanietal., 2020)      Synthetic     1             ✗                 ✗                ✗            ✗       ✗            ✗               ✗               ✗                ✗
                              ChartQA(Masryetal.,2022)          Real        4             ∼                 ✗                ✗            ✗       ✗            ✗               ✗               ✗                ✗
                              CharXiv(Wangetal.,2024b)          Real        1             ✗                 ✗                ✗            ✓       ✗            ✗               ✗               ✓                ✗
                              CHARTQAPRO(Ours)                  Real       157            ✓                ✓                 ✓            ✓       ✓            ✓               ✓               ✓                ✓
                              Table 1: Comparison of CHARTQAPRO with existing chart-based QA benchmarks. Features are grouped into
                              ChartImages(real vs. synthetic data, number of sources, topic diversity, infographics/dashboards, accompanying
                              paragraph, multi-chart support) and Questions Types (MCQ, conversational, hypothetical, unanswerable). ✓=
                              Supported, ✗= Not Supported, ∼= Partially Supported.
                              substantial room for improvement in LVLMs’                                                       types (multiple-choice, conversational, hypotheti-
                              chart reasoning abilities. Moreover, while open-                                                 cal, etc.), offering a more challenging benchmark.
                              source models seemed to match closed-source                                                      Vision-Language Models for Charts                                       Advances
                              ones on ChartQA, they still lag significantly on                                                 in vision-language models have significantly im-
                              CHARTQAPRO with the best, Qwen2-VL-7B                                                            proved chart understanding and reasoning. These
                              (Wangetal., 2024a), achieving only 37.17%. This                                                  models can be categorized into: (i) closed-source,
                              suggests that prior benchmarks might have over-                                                 (ii) open-source general multimodal models, and
                              stated progress due to their limited diversity.                                                 (iii) chart-specific models. Closed-source mod-
                                  Ourcontributions include: (i) a comprehensive                                                els (OpenAI et al., 2024; Georgiev et al., 2024)
                              benchmarkthatevaluates diverse and complex real-                                                 achieve the highest performance on recent chart
                              world chart understanding abilities; (ii) extensive                                              understanding benchmarks (Masry et al., 2022;
                              evaluation of open- and closed-source models, re-                                               Wang et al., 2024b). Open-source general multi-
                              vealing significant performance declines compared                                                modal models (Wang et al., 2024a; Li et al., 2024;
                              to previous benchmarks; (iii) in-depth qualitative                                               Chen et al., 2025; Wu et al., 2024b; Abdin et al.,
                              analyses and ablation studies, identifying key chal-                                             2024; Laurençon et al., 2024; Masry et al., 2025;
                              lenges and future directions for improving LVLMs’                                                Rodriguez et al., 2024) currently lag behind, but
                              chart reasoning abilities.                                                                       are rapidly closing the gap. Chart-specific models
                              2      Related Work                                                                             (Masry et al., 2024b,a; Zhang et al., 2024; Masry
                                                                                                                               et al., 2023) demonstrate strong performance on
                              ChartUnderstandingDatasets                                   Numeroustasks                       standard benchmarks (Masry et al., 2022; Akhtar
                              and benchmarks have been developed to evaluate                                                   et al., 2023b; Kantharaj et al., 2022b; Masry and
                              LVLMs’chartunderstandingabilities,suchasques-                                                    Hoque, 2021). However, their generalization to
                              tion answering (Masry et al., 2022; Wang et al.,                                                 real-world chart understanding remains uncertain
                              2024b), chart summarization (Kantharaj et al.,                                                   due to their reliance on instruction-tuning datasets
                              2022b), fact-checking (Akhtar et al., 2023a,b), and                                             with limited task diversity. CHARTQAPRO offers
                              explanation generation (Kantharaj et al., 2022a).                                                a more comprehensive benchmark, ensuring that
                              Amongthese, chart question answering is the most                                                 modelimprovements reflect real progress in chart
                              commonlyusedforevaluation. Early benchmarks                                                      understanding abilities of these models.
                              like STL-CQA (Singh and Shekhar, 2020) and                                                       3      THECHARTQAPROBENCHMARK
                              Leaf-QA (Chaudhry et al., 2020) relied on syn-
                              thetically generated charts and templated ques-                                                  3.1       Dataset Construction
                              tions. Later benchmarks, such as ChartQA (Masry                                                  Ourdataset construction pipeline consists of three
                              et al., 2022), PlotQA (Methani et al., 2020), and                                                key stages (see Figure 3): (i) Chart Image Col-
                              CharXiv (Wang et al., 2024b), used real-world                                                    lection, (ii) Question-Answer Annotation, and
                              charts and more complex questions requiring ad-                                                 (iii) Question-AnswerReview. Wedetaileachstage
                              vanced visual reasoning. However, these bench-                                                   below:
                              marksextract charts from limited sources (Table 1),
                              cover few question types, and have reached per-                                                  Stage           1       -       Chart             Images              Collection
                              formance saturation due to recent strong LVLMs                                                   CHARTQAPRO prioritizes both visual and
                              (Figure 1). In contrast, CHARTQAPRO sources                                                      topical diversity. We sourced chart images from di-
                              from 157 diverse online domains and includes                                                    verse platforms featuring real-world visualizations,
                              human-written, verified questions across multiple                                                including multi-series line charts, stacked and
                                                                                   Question-Answer
                                                                                                                               Question-Answer Review
                                1 Chart Image Collection                     2                                            3
                                                                                       Annotation
                                                                                                                                              Answer (1)
                                                                          Human-VLM Collaboration
                             Web Crawl
                                                                                                                                  Annotator (1 - org.)
                                                                                                      Question-
                                                                                                                        Question
                                                                                                                         Question
                                                                                         V
                                                                                                     Answer pairs
                                           Manual                                                                                                           Final
                                           Filtering                                                                                                       Answer
                                                                                                                                                                                  Final Corpus
                                                        Chart Corpus
                                                                                                                                              Answer (2)
                             Google Search
                                                                                                                                   Annotator (2)
                                                                       Figure 3: CHARTQAPRO Dataset Construction Process
                           grouped bar charts, dashboards, and infographics.                                                 interactively prompted VLMs to generate ad-
                           Key sources include Pew Research (Pew, 2024),                                                     ditional QA pairs beyond those derived from
                           Tableau (Tableau, 2024), the Public Policy Institute                                              the seed set, encouraging the models to pro-
                           of California (PPIC) (PPIC, 2024), and Our World                                                  duce diverse and novel questions.
                           in Data (OWID) (OWID, 2024) (see Figure 6 for                                                  • Human Refinement: Annotators manually
                           moredetails). For Pew and Tableau, we randomly                                                    reviewed the generated questions to filter the
                           sampled charts from Islam et al. (2024) which                                                     ones that are overly simple (e.g., direct data
                           are already diverse in visual styles, while for                                                   retrieval from charts) or revise the questions
                           other sources, we manually selected charts with                                                   that are unclear or ambiguous.
                           varied formats to enhance dataset diversity. Some
                           charts were accompanied by textual descriptions                                               Akeyfeature of CHARTQAPRO istheinclusion
                           that provided additional context, improving the                                           of unanswerable questions. These questions were
                           interpretability of the corresponding chart images.                                       carefully curated by humans to be closely related to
                                To further expand coverage, we collected an                                          the chart’s topic while unanswerable based solely
                           additional 1041 charts from the web, building                                             on the chart image. Also, CHARTQAPRO features
                           upon prior efforts from ChartInstruct (Masry et al.,                                      questions on chart-text pairs, with some referring
                           2024a) to include dashboards and infographics. In                                         only to the chart, others only to the text, and some
                           total, CHARTQAPRO isacompileddatasetof1341                                                requiring integration of both, posing a greater chal-
                           chart images from 157 online platforms, covering                                          lenge for vision-language models. We present a
                           a broad spectrum of chart types and styles. Addi-                                         brief description of various question types below:
                           tional details are provided in Appendix A.1.                                              Reasoning: Reasoning with charts is a common
                           Stage          2     -     Question-Answer                     Annotation                 real-world task involving visual perception, trend
                            CHARTQAPRO includes five types of question-                                              analysis, and mathematical reasoning. While such
                           answer pairs: (i) Reasoning, (ii) Conversational,                                         questions appear in benchmarks like ChartQA, we
                           (iii)     Multiple-Choice, (iv) Hypothetical, and                                         focus on more complex cases requiring composi-
                           (v) Fact-Checking.                    Nine team members col-                              tional calculations and deeper pattern, trend, and
                           laboratively created these QA pairs, with five                                            outlier analysis (e.g., Figure 2a, b).
                           focusing on reasoning questions and the remaining                                         Conversational: Conversational questions consist
                           four handling other categories. To ensure high-                                           of multiple interrelated QA pairs for a given vi-
                           quality annotations, we adopted a human-VLM                                               sualization, where each question naturally builds
                           collaboration process for each QA type:                                                   upon the previous one. These questions help us
                                                                                                                     assess how well VLMs handle contextual depen-
                                • Curating Seed QApairs: Annotators crafted                                          dencies, such as coreference resolution and logical
                                    a diverse set of seed QA pairs covering dif-                                     or arithmetic reasoning (e.g., Figure 2c).
                                    ferent question types that required complex                                      Multiple-Choice:                    Multiple-choice questions
                                    reasoning.                                                                       (MCQs) are widely used in assessments and ed-
                                                                                                                     ucational materials. We focused on MCQs that
                                • VLM-Assisted Expansion: Using GPT-4o,                                              require complex reasoning, including trend anal-
                                    Gemini, and Claude, we expanded the seed                                         ysis, anomaly detection, extrapolation, and time
                                    set by generating additional QA pairs. We                                        series analysis (e.g., Figure 2d).
                                    decided to employ multiple models to miti-                                           Each question is presented with four answer
                                    gate bias. Each model was prompted with a                                        choices, covering various formats such as dates,
                                    seed QA pair and tasked with generating five                                     percentages, locations, and specific labels derived
                                    new pairs per chart. In addition, annotators                                     from the data.
                 Hypothetical: Hypothetical questions introduce
                 assumptions beyond observable chart data (e.g.,
                 Figure 2e). Answering these questions requires
                 not only extracting information accurately but also
                 making inferences, estimations, or approximations
                 based on patterns and trends present in the visu-
                 alization. These questions add an extra layer of
                 complexity by requiring the model to reason be-
                 yond explicit data points.
                 Fact-Checking: Fact-checking questions involve
                 evaluating a claim about a chart by extracting and
                 verifying relevant data (e.g., Figure 2f). Each claim
                 is classified as either True (confirmed by data) or
                 False (contradicted by data). These questions test
                 the model’s ability to interpret chart information
                 and assess the validity of claims, a crucial skill
                 for misinformation detection, incorrect prediction,    Figure 4:    Distribution of topics per source in
                 fake news detection, etc.                              CHARTQAPRO. The inner ring represents online
                 Stage 3 - Question-Answer Review         After cre-    sources, while the outer ring shows topic distribution
                 ating the QA pairs, we conducted a quality as-         for each source.
                 sessment to ensure accuracy and clarity. Seven         as bars, lines, pies, scatter plots, dashboards, info-
                 annotators, all co-authors with expertise in visu-     graphics, maps, etc. (see Table 2), with bar charts
                 alization, performed this review. Five focused on      being the most common (31.8%), followed by line
                 factoid questions, while the remaining two han-        charts (26.5%).
                 dled other categories. Each annotator reviewed            To further quantify the visual diversity of
                 questions from a category they had not originally      our chart images compared to earlier bench-
                 workedon,thencross-checkedtheirresponseswith           marks—ChartQA(Masryetal.,2022)andCharXiv
                 the category’s original creator. Any identified er-    (Wang et al., 2024b)—we conducted an experi-
                 rors in the questions or answers were collabora-       mentwherewefirst encoded all images from each
                 tively revised until both parties reached an agree-    benchmark into feature vectors using a CLIP vi-
                 ment. In rare instances, ambiguous questions were      sion encoder (Radford et al., 2021) with sentence-
                 modified to resolve disagreements. For subjective      transformers (Reimers and Gurevych, 2019). For
                 questions (e.g., value estimations), minor discrep-    each benchmark, we then computed the pairwise
                 ancies (<1%) were considered acceptable. Overall,      cosine distances among all images. In this context,
                 the initial agreement rate between annotators was      ahigheraveragepairwisedistanceindicatesthatthe
                 66.17%before resolving all discrepancies.              images are less similar and therefore more visually
                                                                        diverse. Our CHARTQAPRO benchmarkexhibits
                 3.2   Dataset Analysis                                 an average distance of 0.53, while ChartQA and
                 3.2.1   Visual Diversity                               CharXiv show averages of 0.26 and 0.27, respec-
                 Unlike the ChartQA (Masry et al., 2022) dataset,       tively. Moreover, Figure 10 in A.3 shows that most
                 whichsources its charts from only four origins, our    pairwise distances in CHARTQAPRO exceed those
                 benchmark incorporates a diverse range of sources.     in the other benchmarks. Theseresultsconclusively
                 These include web charts collected from various        demonstrate that our CHARTQAPRO benchmark is
                 websites and links across the internet, as well as     significantly more diverse than the existing bench-
                 charts from Tableau, Pew Research, PPIC, and           marks, offering a richer and more varied set of
                 OWID.AsshowninFigure4,themajorityofcharts              visual representations.
                 (74%) were collected through web crawling, fol-        3.2.2   Linguistic Diversity
                 lowed by charts from Tableau (14%), covering a
                 diverse range of topics, such as, ‘Politics’, ‘Econ-   We conducted a detailed analysis of the linguis-
                 omy’, ‘Health’, ‘Environment’, ‘Technology’, etc.      tic features of our benchmark dataset (see Ap-
                 Thecorpusalso includes various chart types such        pendix A.3). Unlike existing chart-based bench-
                                                                     ChartTypes                                                                 Question Types
                                    Bar   Line   Pie   Area   Scatter  Bubble    Dashboard    Infographic    Other   Math&Visual Conversational           Fact     Multiple   Hypothetical
                                                                                                                       Reasoning                       Checking     Choice
                           Count    427    355   29     30       8        7          258          190         37          1081              311           244         214          98
                                                          Table 2: Distribution of chart and question types in CHARTQAPRO.
                          marks that focus on short question-answer pairs,                                    paragraph p which the task might use. The objec-
                                                                                                                                 i
                          CHARTQAPRO provides a more diverse and lin-                                         tive is for the multimodal LLM to take c and q as
                                                                                                                                                                            i          i
                          guistically rich dataset. It features 6,638 unique                                  input (along with the prompt) and autoregressively
                          tokens in questions and 1496 in answers, signif-                                    generate the answer a . We provide all our prompts
                                                                                                                                               i
                          icantly surpassing CharXiv (4545) and ChartQA                                       in A.4 to ensure reproducibility and transparency.
                          (2427). Thequestionsin CHARTQAPROarelonger
                          and more varied, averaging 106.05 characters and                                    4.2      Models
                          18 tokens, compared to CharXiv (96.3 charac-                                        Toevaluate the current state-of-the-art in chart un-
                          ters, and 17.2 tokens) and ChartQA (63.25 char-                                     derstanding, we benchmark a diverse set of closed-
                          acters, and 11.5 tokens), while answers remain                                      and open-source models. The closed-source mod-
                          concise at 6.7 characters and 1.18 tokens. Addi-                                    els include: (i) GPT-4o (OpenAI et al., 2024), (ii)
                          tionally, CHARTQAPRO captures real-world vari-                                      Gemini-Flash-1.5 and 2.0 (Georgiev et al., 2024),
                          ability with diverse syntactic structures, informal                                 and (iii) Claude Sonnet 3.5 (Anthropic, 2024). For
                          language, and typographical errors, making it a                                     open-source models, we categorize them based
                          comprehensive benchmark for evaluating complex                                      on parameter size. Models with fewer than 7B
                          question-answering models in the chart domain.                                      parameters include: (i) Intern-VL2.5-1B (Chen
                              Wefurther analyze the linguistic diversity and                                  et al., 2025), (ii) Janus-1.3B (Wu et al., 2024a) (iii)
                          richness of the text in chart images by extract-                                    Qwen-VL2-2B (Wang et al., 2024a), (iv) Intern-
                                                                                     1
                          ing text using the Google OCR API and using                                         VL2.5-2B(Chenetal., 2025), (v) SmolVLM-2.3B
                          two key metrics: lexical diversity and seman-                                       (SmolVLM,2024),(vi) Ovis1.6-Llama3.2-3B (Lu
                          tic diversity (Figure 11). Lexical diversity, mea-                                  et al., 2024), (vii) DeepSeek-VL2-3.4B (Wu et al.,
                          sured via the type-token ratio (TTR), is highest                                    2024b), and (viii) Phi 3.5-Vision-4B (Abdin et al.,
                          for CHARTQAPRO(0.15),followed by ChartQA                                            2024). In the 7-12B parameter range, we evaluate:
                          (0.13) and ChartXiv (0.11), indicating a richer vo-                                 (i) Qwen-VL2-7B (Wang et al., 2024a), (ii) Intern-
                          cabulary in CHARTQAPRO. Semantic diversity,                                         VL2.5-8B(Chenetal.,2025),(iii)Idefics-3-Llama-
                          quantified as the average pairwise cosine distance                                  3.1-8B (Laurençon et al., 2024), (iv) LLaVA-Next-
                          between text embeddings computed using sentence                                     Mistral-7B (Li et al., 2024), (v) Ovis1.6-Gemma2-
                          transformers(ReimersandGurevych,2019),isalso                                        9B(Luetal., 2024), and (vi) Llama 3.2-Vision-11B
                          maximumfor CHARTQAPRO(0.84)comparedto                                               (Grattafiori et al., 2024). In addition, we also evalu-
                          ChartQA (0.75) and ChartXiv (0.78), suggesting                                      ate chart-specific LVLMs: (i)ChartGemma (Masry
                          broader semantic coverage. Overall, these find-                                     et al., 2024b), (ii) ChartInstruct-LLama2 (Masry
                          ings collectively demonstrate that CHARTQAPRO                                       et al., 2024a), (iii) TinyChart (Zhang et al., 2024).
                          exhibits greater linguistic diversity than previous                                 Allmodelsareassessedwiththreepromptingstrate-
                          benchmarks. More details are provided in A.3.1.                                     gies: Direct prompting, Chain-of-Thought (CoT)
                          4     Experiments                                                                   (Wei et al., 2023), and Program-of-Thought (PoT)
                                                                                                              (Chen et al., 2023). All experiments were run on
                          4.1      ProblemFormulation                                                         Google Cloud Platform (GCP) using A100 GPU.
                          We formulate the CHARTQAPRO tasks as mul-                                           4.3      Evaluation Metric
                          timodal question-answering challenges.                                  The         Weenhancetherelaxedaccuracymetriccommonly
                          dataset consists of N examples, denoted as D =                                      used for CQA (Masry et al., 2022; Methani et al.,
                                          N
                          {c ,q ,a }           ,   where each example includes a
                              i   i    i  i=1                                                                 2020)forallthequestiontypes. Specifically, fornu-
                          chart image ci, a question qi, and the correspond-                                  meric answers, we maintain a 5% error margin, but
                          ing ground truth answer a . For certain charts, the
                                                                  i                                           for answers in ‘years’ we require an exact match to
                          formulation also includes a corresponding context                                   avoid bias from minimal differences (e.g., 2008 vs.
                                1https://cloud.google.com/vision/docs/                                        2009). For textual answers (e.g., labels or common
                          ocr                                                                                 words), we employ the ANLS score (Biten et al.,
                                                                                    Direct                                           Chain-of-Thought (CoT)                                    Program-of-Thought (PoT)
                                 Model
                                                           Factoid  MCQ Convers. FactChk. Hypoth. Overall Factoid MCQ Convers. FactChk. Hypoth. Overall Factoid MCQ Convers. FactChk. Hypoth. Overall
                                 HumanBaseline              80.00   94.00    88.70      92.00      70.42     85.02     N/A     N/A      N/A        N/A        N/A       N/A      N/A      N/A      N/A        N/A       N/A       N/A
                                 Closed-Source Models
                                 GPT4-o                     35.76   46.72    34.75      45.49      28.91     37.67    37.40    61.68    33.93      57.37     30.83     41.68     39.22   42.99    38.62      44.67      44.43    40.48
                                 Gemini-Flash-2.0           43.43   60.28    40.25      67.62      24.47     46.85    51.51    69.15    43.84      67.62     39.89     53.66     51.18   57.00    46.34      56.81      44.86    51.44
                                 Gemini-Flash-1.5           39.96   57.00    39.70      47.13      45.31     42.96    42.37    64.01    40.17      56.14     39.42     45.97     45.57   35.51    40.98      50.40      47.26    44.42
                                 Claude Sonnet 3.5          38.84   51.40    44.53      55.60      45.48     43.58    53.61    78.03    43.84      65.16     46.11     55.81     46.58   54.20    46.17      52.04      46.90    48.05
                                 Open-Source Models
                                 Intern-VL2.5-1B            9.15     7.00     6.20      16.63      8.17      9.33      5.45    0.46     14.86      21.17     17.08      8.96     1.07     0.0      0.64       0.40      2.04      0.85
                                 Janus-1.3B                 4.56     1.86     6.74      40.98      5.31      9.21      3.54     0.0     6.05       29.91      6.97      7.03     5.12     1.86     6.61       3.68      3.60      4.74
                                 Qwen-VL2-2B                15.90   27.57    24.26      34.42      12.82     20.68    16.62    30.84    23.89      38.52     13.00     21.90     13.66   23.83    15.22       8.60      3.06     13.86
                                 Intern-VL2.5-2B            13.86   10.74    14.02      45.90      18.92     17.81     9.42    6.07     13.02      36.06     19.23     13.46     1.13     6.07     2.51       2.04      3.06      2.10
                                 SmolVLM-2.3B               13.32   16.82    17.71      46.31      25.21     19.14    13.03    7.47     18.60      36.88     22.15     16.76     4.03    12.61    11.22       5.73      12.52     6.76
                                 Ovis1.6-LLama3.2-3B        12.87    0.46     4.18      40.98      10.17     13.50    14.43    7.45     8.37       35.27     16.60     15.42     17.41    5.60     5.86      30.32      24.10    16.22
                                 DeepSeek-VL2-3.4B          12.20    7.47    19.40      36.88      19.21     16.28     9.63    1.40     18.09      38.11     23.25     14.33     10.27    3.27    15.94      22.54      17.43    12.30
                                 Phi 3.5-Vision-4B          17.48   30.37    28.54      41.99      37.27     24.73    10.55    32.71    27.20      8.19       8.16     15.23     10.34   32.71    16.62       0.0       5.10     12.24
                                 Qwen-VL2-7B                30.70   44.85    35.68      48.36      37.23     35.59    32.95    46.26    37.60      50.40     29.65     37.17     11.74   44.85    20.42      28.96      10.64    18.86
                                 Intern-VL2.5-8B            35.21   25.70    32.26      53.27      29.61     35.67    29.53    23.36    28.87      56.14     27.73     31.99     26.14   18.69    11.43      34.83      22.60    23.88
                                 Idefics-3-LLama-3.1-8B     20.69    2.29    31.96      10.76      36.83     20.03    20.06    2.29     30.98      11.14     35.36     19.51     10.06    5.41    19.41       7.62      18.60    11.16
                                 LLaVA-Next-Mistral-7B      15.35   35.98    21.09      41.80      17.79     21.97     9.43    4.20     19.30      38.93     21.71     14.74     4.93     2.33     3.72      13.79      13.26     5.98
                                 Ovis1.6-Gemma2-9B          30.25    4.67    28.93      27.86      28.21     26.83    18.09    12.42    17.68      25.05     20.49     18.39     22.59   20.56    17.33      32.37      25.30    22.89
                                 LLama3.2-Vision-11B        12.34    2.33     0.19      27.18      10.93     11.09    19.65    47.66    19.15      44.45     13.10     25.43     19.69   39.25    19.28      27.45      23.72    22.95
                                 Chart-Specific Models
                                 ChartGemma-3B              6.86     0.0     16.00       1.22      6.53      6.84     11.01    1.86     15.21      2.45      15.02      9.80     12.69    0.0     10.14      14.18      21.61    11.52
                                 TinyChart-3B               8.52     7.00    17.46      33.19      16.06     13.25     8.97    6.07     11.05      28.27     14.24     11.67     5.64     0.0      4.11       0.0       15.92     4.59
                                 ChartInstruct-LLama2-7B    7.09     0.0      3.77       0.0       6.91      4.88      3.83     0.0     4.43       0.40      10.65      3.42     0.09     0.31     1.69       2.04       0.0      0.61
                                Table 3:            Accuracy (%) on CHARTQAPRO by Prompt Type (main headers) and Question Type (sub-
                                headers).             Each Prompt Type block has five question types plus an Overall sub-column.                                                                                     Color cod-
                                ing for comparison:                          humanbaseline ,                    closed-source models ,                       open-source models below 7B parameters ,
                                 open-source models between 7-12B parameters , chart-specific models . We bold the best score within each
                                model category.
                                2019). Finally, multiple-choice questions (e.g., a, b,                                                  soning (as in CoT or PoT), suggesting they may
                                c, d) and fact-checking tasks (true, false) are eval-                                                   lack sufficient training or alignment with step-by-
                                uated using an exact-match criterion. Additional                                                        step answer styles. Finally, chart-specific models
                                details are provided in A.5.                                                                            perform poorly under all setups, indicating that
                                4.4        MainResults                                                                                  they may be heavily overfitted to particular visual
                                                                                                                                        and question types and thus generalize poorly to
                                Table 3 presents each model’s performance on                                                            broader chart-based QA scenarios.
                                the CHARTQAPRO dataset under three prompt-                                                                   Overall, these findings indicate that none of the
                                ing strategies (Direct, Chain-of-Thought, and                                                           models have achieved near-human-level chart un-
                                Program-of-Thought) and across five question                                                            derstanding (See A.6), leaving considerable room
                                types. Closed-source models consistently outper-                                                        for improvement—a result that contrasts sharply
                                form open-source counterparts in all prompting se-                                                      with the previously reported high accuracies on
                                tups, and they also benefit from more extensive rea-                                                    previous datasets (Figure 1 and Appendix A.7).
                                soning strategies (CoT or PoT), which boost over-                                                       4.5        Qualitative Analysis
                                all accuracy. Notably, Chain-of-Thought yields the
                                highest scores, with Claude Sonnet 3.5 achieving                                                        Weexamined150randomsamplestofindcommon
                                the top accuracy of 55.81%, while GPT4o ranks                                                           failure patterns and discovered three major error
                                lowest among the closed-source group. We also                                                           categories. Figure 5 presents representative errors,
                                observe that conversational, hypothetical, and fac-                                                     while additional examples are provided in A.8.
                                toid queries pose the greatest challenge for these                                                      Visual Perception: A common source of error is
                                models,whereasfact-checkingandmultiple-choice                                                           the failure to accurately recognize data values from
                                questions yield relatively higher accuracy—likely                                                       chart images. This often occurs when charts are
                                because the narrower range of possible answers                                                          overcrowded with visual elements (e.g., bars, lines)
                                increases the likelihood of a correct response.                                                         orwhendatavaluesarenotexplicitlyshown,requir-
                                     In contrast, open-source models below 7B pa-                                                       ing inference based on geometric properties like
                                rameters (highlighted in blue) exhibit substantially                                                    height or area. While both open-source and closed-
                                lower performance across all prompt types, of-                                                          source models struggle with visual perception, it is
                                ten falling below 20% overall accuracy. However,                                                        the primary issue for closed-source models.
                                certain open-source models in the 7–12B range                                                           Instruction Following: Open-source and chart-
                                (shaded in orange) show more promise; for in-                                                           specific models struggle to generate proper chain-
                                stance, Qwen2-VL-7B and InternVL-2.5-8B both                                                            of-thought (CoT) or program-of-thought (PoT) re-
                                exceed 30%. Surprisingly, these models often per-                                                       sponseswhenexplicitlyprompted. Manygenerated
                                form worse when asked to produce long-form rea-                                                         programs even fail to execute due to runtime errors.
                                                                                                                   (b) Instruction Following Error / SmolVLM-3.4B                                   (c) Math Reasoning Error / InternVL-2.5-8B
                                     (a) Visual Perception Error / Gemini Flash 2.0
                                                                                                                    Prompt Style: Program-of-Thought (PoT)
                                     Question: what date did the linked cases first make up all of
                                                                                                                    Question:  What was the total contribution of the top 3
                                     the total cases? answer in the format dd/mm.
                                                                                                                    companies  (boeing,  caterpillar,  and  home  depot)  to  the
                                     Gound Truth: 05/06                                                                                                                                            Question:  What is the average of the line values? 
                                                                                                                    dow's rise since it first closed above 24,000?
                                     Model Response: The linked cases equal the total cases
                                                                                                                    Gound Truth: 336.3                                                             Gound Truth: 15.92
                                     when the unlinked cases are 0. The graph shows that the
                                                                                                                    Model Response: 
                                     unlinked cases are 0 on June 3 and between June 8 and                                                                                                         Model Response: 9.6
                                                                                                                    125.9 + 125.9 + 73.6
                                     June 9.
                                                                                                                    Answer: 305.4
                                     The answer is 03/06.
                                      Figure 5: Sample errors across three categories: Visual Perception, Instruction Following, and Math Reasoning.
                                                                     ChartType(A)                AnswerType(B)         ParagraphPresence(C)             performance decline on such complex visuals.
                                     Model                   Chart   Dashboard   Infographic  Normal   Unanswerable    NoPara      With Para
                                     Closed-Source Models
                                     GPT4-o                  39.63     44.49        47.74      39.71       50.13        40.04        52.29              Answer Type: Among closed-source models,
                                     Gemini-Flash-2.0        52.34     54.64        58.70      51.44       63.14        52.29        62.44
                                     Gemini-Flash-1.5        43.93     49.03        51.61      47.22       40.65        44.16        57.65
                                     Claude Sonnet 3.5       54.63     57.42        59.30      57.63       47.98        54.33        65.29              GPT-4o and Gemini Flash 2.0 handle unanswer-
                                     Open-Source Models                                                                                                 able questions relatively well, while Gemini Flash
                                     Qwen-VL2-2B             21.20     19.41        19.93      21.02       19.24        21.16        17.59
                                     SmolVLM-2.3B            18.88     15.15        16.36      19.99        8.49        18.30        14.65
                                     Phi 3.5-Vision-4B       26.15     20.96        23.12      28.72        7.66        25.12        22.19              1.5 and Claude Sonnet 3.5 show lower robustness.
                                     Qwen-VL2-7B             37.18     31.61        33.43      37.13       28.99        35.30        37.47
                                     InternVL2.5-8B          36.74     35.10        32.38      31.41       53.92        35.08        39.50              Similarly, open-source models generally perform
                                     LLama-3.2-Vision-11B    23.96     26.32        31.27      29.09        9.75        25.14        27.29
                                     Chart-Specific Models                                                                                              worse on unanswerable questions. Chart-specific
                                     ChartGemma               7.01      4.74        8.98        8.38        0.27         6.94        6.24
                                     ChartInstruct-LLama2     5.97      2.84        2.48        6.03         0.0         5.64         0.0               models, however, struggle significantly, with per-
                                     TinyChart               13.75     11.20        13.69      16.28        0.27        15.25        0.38
                                                                                                                                                        formance near zero, highlighting their limited abil-
                                   Table 4: Ablation results on CHARTQAPRO across                                                                       ity to handle ambiguous or missing information.
                                    three independent dimensions.                                            (A) Chart Type ,
                                     (B) Answer Type , (C) Paragraph Presence .                                                                         ParagraphPresence:                                   Closed-source models can
                                   Additionally, Llama 3.2 Vision-11B (Grattafiori                                                                      effectively utilize the additional context. Among
                                    et al., 2024) performs poorly in the direct-answer                                                                  open-source models, smaller models struggle with
                                    setup(11.09%accuracy),oftenignoringtheprompt                                                                        this added context, while larger models are more
                                    and persistently generating CoT explanations, sug-                                                                  robust. Chart-specific models perform poorly with
                                    gesting overfitting to CoT-style training.                                                                          added context, likely due to overfitting, except for
                                    MathReasoning: Whileall models struggle with                                                                        ChartGemma(Masryetal., 2024b).
                                    complex mathematical operations in our bench-                                                                             Overall, our analysis shows that while closed-
                                    mark, closed-source models mitigate this issue                                                                      source models generally lack in recognizing data
                                    to some extent by effectively utilizing long rea-                                                                   values (visual perception), open-source and chart-
                                    soning traces, such as Chain-of-Thought (CoT)                                                                       specific models struggle with visual complexity,
                                    or Program-of-Thought (PoT), allowing them to                                                                       ambiguous information, and added context, high-
                                    break down problems into steps and leverage exter-                                                                  lighting the need for improvementstomatchclosed-
                                    nal tools (e.g., Python). In contrast, open-source                                                                  source models in chart understanding. We present
                                    models fail to utilize these prompting strategies. In                                                               exemplar details in A.9 and Figure 13.
                                    the direct-answer setup, they particularly struggle                                                                 5        Conclusion
                                    to perform multiple mathematical operations and
                                    generate the final answer correctly.                                                                                We introduced CHARTQAPRO, a more diverse
                                                                                                                                                        and challenging benchmark for chart question
                                    4.6         Ablation Studies                                                                                        answering, designed to push the limits of cur-
                                   Table 4 shows ablation results on CHARTQAPRO                                                                         rent vision-language models (VLMs) in real-world
                                    onthree independent dimensions: (A) Chart Type,                                                                     chart reasoning.                         By incorporating 1341 charts
                                   (B) Answer Type, and (C) Paragraph Presence.                                                                         from 157 sources and a broad spectrum of ques-
                                                                                                                                                        tion types—including factoid, multiple-choice,
                                    ChartType: Closed-source models demonstrate                                                                         fact-checking, conversational, and hypothetical
                                    greater robustness to complex visual layouts, such                                                                  queries—our benchmark reveals significant perfor-
                                    as dashboards and infographics. In contrast, both                                                                   mancegapsbetweenexisting models and human-
                                    open-source and chart-specific models exhibit a                                                                     level understanding.                               Our extensive evaluation
                 showsthateventhestrongestclosed-source models           search.
                 experience substantial performance drops, under-        Ethical Considerations
                 scoring that chart reasoning remains an unsolved
                 challenge. Throughdetailederroranalysisandabla-         During the dataset collection process, we carefully
                 tion studies, we identify key areas for improvement,    considered several ethical aspects to ensure the
                 paving the way for future advancements in multi-        integrity of our work. All collected images under-
                 modal reasoning. We hope CHARTQAPRO serves              went a thorough manual review by the authors to
                 as a catalyst for developing more robust and capa-      filter out any content that could be considered harm-
                 ble models for real-world chart comprehension.          ful or offensive. Additionally, our benchmark does
                    Asfuturework,weplanonexpandingthebench-              not feature any proprietary data, as all charts were
                 markbyintroducingdynamicandinteractivecharts            sourced from publicly available online platforms.
                 and dashboards, as current benchmarks only use          We plan to release the dataset only for research
                 screenshots of the charts – which often does not        purposes.
                 happen in real-world scenarios. We also aim to             The question-answer (QA) generation process
                 curate a large-scale training dataset in reasoning      was carried out exclusively by the authors, all of
                 formats following recent advances in LLM train-         whomare researchers with expertise in chart un-
                 ing, hoping to develop significantly more proficient    derstanding. While large vision-language models
                 chart understanding and reasoning models.               (LVLMs)wereusedasassistancetools in the QA
                                                                         expansion process, all questions and answers were
                 Limitations                                             manually reviewed and refined to ensure accuracy,
                 While CHARTQAPRO is designed to compre-                 coherence, and ethical neutrality. No external or
                 hensively evaluate chart understanding, there are       paidannotatorswereinvolvedinthisstudy. Instead,
                 a few limitations to consider. First, our bench-        all individuals who contributed to dataset annota-
                 markprimarily focuses on chart question answer-         tion were granted co-authorship to recognize their
                 ing (ChartQA) as the core evaluation task. While        contributions. All annotators were informed that
                 this task effectively measures a model’s ability to     their annotations would be included in the dataset
                 extract, interpret, and reason over chart data, other   released for research purposes. Finally, AI writ-
                 chart-related tasks—such as chart-to-summary gen-       ing assistants were used to refine the writing and
                 eration or chart-to-code translation—are also valu-     enhance the paper’s presentation.
                 able and remain unexplored in this work.                Acknowledgement
                    Second, although we carefully tuned prompts          We would like to thank the anonymous review-
                 to ensure fair and consistent evaluation across all     ers for their helpful feedback. This research was
                 models,performancemayvaryslightlybyapplying             supported by the Natural Sciences Engineering Re-
                 further prompt engineering techniques. While cer-       search Council (NSERC) of Canada and Canada
                 tain models might benefit from additional prompt        Foundation for Innovation (CFI). Additionally, it
                 engineering, we do not expect such adjustments          received support through a Google Cloud Platform
                 to lead to substantial improvements or change the       (GCP) credits award from Google’s PaliGemma
                 overall findings in our study.                          AcademicProgram.
                    Third,     the    dashboards      included     in
                 CHARTQAPRO are static screenshots rather
                 than interactive elements. In real-world scenarios,     References
                 most dashboards often allow users to hover, filter,     Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed
                 or manipulate data dynamically, which can impact           Awadallah, Ammar Ahmad Awan, Nguyen Bach,
                 howinsights are extracted. Since our benchmark             AmitBahree,ArashBakhtiari, Jianmin Bao, Harkirat
                 does not incorporate interactivity, models are             Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck,
                 evaluated solely on the static visual and textual          Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav
                 information presented in the images.                       Chaudhary, Dong Chen, Dongdong Chen, and 110
                                                                            others. 2024. Phi-3 technical report: A highly capa-
                    Despite these limitations, CHARTQAPRO pro-              ble language model locally on your phone. Preprint,
                 vides a rigorous and diverse benchmark that high-          arXiv:2404.14219.
                 lights key challenges in chart reasoning and serves     Mubashara Akhtar, Oana Cocarascu, and Elena Sim-
                 as a valuable resource to advance multimodal re-           perl. 2023a. Reading and reasoning over chart im-
                     ages for evidence-based automated fact-checking. In       Hinsvark, and 542 others. 2024. The llama 3 herd of
                     Findings of the Association for Computational Lin-        models. Preprint, arXiv:2407.21783.
                     guistics: EACL 2023, pages 399–414, Dubrovnik,         EnamulHoqueandManeeshAgrawala.2019. Search-
                     Croatia. Association for Computational Linguistics.       ing the visual style and structure of d3 visualiza-
                  Mubashara Akhtar, Nikesh Subedi, Vivek Gupta, Sa-            tions. In IEEE Transactions on Visualization and
                     har Tahmasebi, Oana Cocarascu, and Elena Sim-             ComputerGraphics (Proc IEEE InfoVis 2019), vol-
                     perl. 2023b. Chartcheck: An evidence-based fact-          ume26,pages1236–1245.IEEE.
                     checking dataset over real-world chart images. arXiv   EnamulHoque,ParsaKavehzadeh,andAhmedMasry.
                     preprint arXiv:2311.07453.                                2022. Chart question answering: State of the art and
                  Anthropic. 2024. Introducing the next generation of          future directions. Preprint, arXiv:2205.03966.
                     claude.                                                MohammedSaidulIslam,MdTahmidRahmanLaskar,
                  Ali Furkan Biten, Ruben Tito, Andres Mafla,                  MdRizwanParvez,EnamulHoque,andShafiqJoty.
                     Lluis Gomez, Marçal Rusiñol, Ernest Valveny,              2024. DataNarrative: Automated data-driven story-
                     C. V. Jawahar, and Dimosthenis Karatzas. 2019.            telling with visualizations and texts. In Proceedings
                     Scene text visual question answering.     Preprint,       of the 2024 Conference on Empirical Methods in
                     arXiv:1905.13648.                                         Natural Language Processing, pages 19253–19286,
                                                                               Miami,Florida, USA.AssociationforComputational
                  R. Chaudhry, S. Shekhar, U. Gupta, P. Maneriker,             Linguistics.
                     P. Bansal, and A. Joshi. 2020. Leaf-qa: Locate, en-    Samira Ebrahimi Kahou, Adam Atkinson, Vincent
                     code attend for figure question answering. In 2020        Michalski, Ákos Kádár, Adam Trischler, and Yoshua
                     IEEE Winter Conference on Applications of Com-            Bengio. 2017. Figureqa: An annotated figure dataset
                     puter Vision (WACV), pages 3501–3510.                     for visual reasoning. CoRR, abs/1710.07300.
                  Wenhu Chen, Xueguang Ma, Xinyi Wang, and                  Shankar Kantharaj, Xuan Long Do, Rixie Tiffany Ko
                     William W. Cohen. 2023.      Program of thoughts          Leong, Jia Qing Tan, Enamul Hoque, and Shafiq Joty.
                     prompting: Disentangling computation from rea-            2022a. Opencqa: Open-ended question answering
                     soning for numerical reasoning tasks.     Preprint,       with charts. arXiv preprint arXiv:2210.06628.
                     arXiv:2211.12588.
                  Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu,             Shankar Kantharaj, Rixie Tiffany Ko Leong, Xiang
                     ZhangweiGao,ErfeiCui,JinguoZhu,ShenglongYe,               Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque,
                     Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang,            and Shafiq Joty. 2022b.    Chart-to-text: A large-
                     QingyunLi, Yimin Ren, Zixuan Chen, Jiapeng Luo,           scale benchmark for chart summarization. Preprint,
                     Jiahao Wang, Tan Jiang, Bo Wang, and 23 others.           arXiv:2203.06486.
                     2025. Expanding performance boundaries of open-        DaeHyunKim,EnamulHoque,andManeeshAgrawala.
                     source multimodal models with model, data, and            2020. Answering questions about charts and generat-
                     test-time scaling. Preprint, arXiv:2412.05271.            ing visual explanations. In Proceedings of the 2020
                  Alexey    Dosovitskiy,   Lucas    Beyer,    Alexander        CHI Conference on Human Factors in Computing
                     Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,               Systems, pages 1–13.
                     Thomas Unterthiner, Mostafa Dehghani, Matthias         HugoLaurençon, Andrés Marafioti, Victor Sanh, and
                     Minderer, Georg Heigold, Sylvain Gelly, Jakob             LéoTronchon.2024. Building and better understand-
                     Uszkoreit, and Neil Houlsby. 2021.       An image         ing vision-language models: insights and future di-
                     is worth 16x16 words: Transformers for image              rections. Preprint, arXiv:2408.12637.
                     recognition at scale. In International Conference on
                     Learning Representations.                              Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang,
                  Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin            Feng Li, Hao Zhang, Kaichen Zhang, Yanwei
                     Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent,        Li, Ziwei Liu, and Chunyuan Li. 2024.        Llava-
                     Zhufeng Pan, Shibo Wang, Soroosh Mariooryad, Yi-          onevision: Easy visual task transfer. arXiv preprint
                     fan Ding, Xinyang Geng, Fred Alcober, Roy Frostig,        arXiv:2408.03326.
                     Mark Omernick, Lexi Walker, Cosmin Paduraru,           Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Wei-
                     Christina Sorokin, Andrea Tacchetti, and 1117 oth-        hua Luo, Kaifu Zhang, and Han-Jia Ye. 2024. Ovis:
                     ers. 2024. Gemini 1.5: Unlocking multimodal un-           Structural embedding alignment for multimodal large
                     derstanding across millions of tokens of context.         language model. Preprint, arXiv:2405.20797.
                     Preprint, arXiv:2403.05530.                            AhmedMasry,XuanLongDo,JiaQingTan,ShafiqJoty,
                  AaronGrattafiori, Abhimanyu Dubey, Abhinav Jauhri,           and Enamul Hoque. 2022. ChartQA: A benchmark
                     Abhinav Pandey, Abhishek Kadian, Ahmad Al-                for question answering about charts with visual and
                     Dahle, Aiesha Letman, Akhil Mathur, Alan Schel-           logical reasoning. In Findings of the Association for
                     ten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh          Computational Linguistics: ACL 2022, pages 2263–
                     Goyal, Anthony Hartshorn, Aobo Yang, Archi Mi-            2279,Dublin,Ireland.AssociationforComputational
                     tra, Archie Sravankumar, Artem Korenev, Arthur            Linguistics.
                   AhmedMasryandEnamulHoque. 2021. Integrating                  Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
                      image data extraction and table parsing methods for          Sentence embeddings using siamese bert-networks.
                      chart question answering. Chart Question Answering           In Proceedings of the 2019 Conference on Empirical
                      Workshop, in conjunction with the Conference on              MethodsinNatural Language Processing. Associa-
                      Computer Vision and Pattern Recognition (CVPR),              tion for Computational Linguistics.
                      pages 1–5.                                                Juan Rodriguez, Xiangru Jian, Siba Smarak Panigrahi,
                   Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do,                    Tianyu Zhang, Aarash Feizi, Abhay Puri, Akshay
                      Enamul Hoque, and Shafiq Joty. 2023. Unichart:               Kalkunte, François Savard, Ahmed Masry, Shravan
                      A universal vision-language pretrained model for             Nayak, Rabiul Awal, Mahsa Massoud, Amirhossein
                      chart comprehension and reasoning.           Preprint,       Abaskohi, Zichao Li, Suyuchen Wang, Pierre-André
                      arXiv:2305.14761.                                            Noël, Mats Leon Richter, Saverio Vadacchino, Shub-
                                                                                   bam Agarwal, and 24 others. 2024. Bigdocs: An
                   Ahmed Masry, Juan A. Rodriguez, Tianyu Zhang,                   open and permissively-licensed dataset for training
                      Suyuchen Wang, Chao Wang, Aarash Feizi, Ak-                  multimodal models on document and code tasks.
                      shay Kalkunte Suresh, Abhay Puri, Xiangru Jian,              Preprint, arXiv:2412.04626.
                      Pierre-André Noël, Sathwik Tejaswi Madhusud-              Hrituraj Singh and Sumit Shekhar. 2020. STL-CQA:
                      han, Marco Pedersoli, Bang Liu, Nicolas Chapa-               Structure-based transformers with localization and
                      dos, Yoshua Bengio, Enamul Hoque, Christopher                encoding for chart question answering. In Proceed-
                      Pal, Issam H. Laradji, David Vazquez, and 3 others.          ings of the 2020 Conference on Empirical Methods
                      2025. Alignvlm: Bridging vision and language la-             in Natural Language Processing (EMNLP), pages
                      tent spaces for multimodal understanding. Preprint,          3275–3284, Online. Association for Computational
                      arXiv:2502.01341.                                            Linguistics.
                   AhmedMasry,MehradShahmohammadi,MdRizwan                      SmolVLM.2024. Smolvlm-smallyetmighty vision
                      Parvez, Enamul Hoque, and Shafiq Joty. 2024a.                language model.
                      Chartinstruct: Instruction tuning for chart compre-
                      hension and reasoning. Preprint, arXiv:2403.09028.        Statista. 2024. Statista.
                   Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aarya-              Tableau. 2024. Tableau public.
                      manKartha, Enamul Hoque, and Shafiq Joty. 2024b.
                      Chartgemma: Visual instruction-tuning for chart rea-      Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi-
                      soning in the wild. Preprint, arXiv:2407.04172.              hao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin
                   Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra,               Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei
                      and Pratyush Kumar. 2020. Plotqa: Reasoning over             Du, Xuancheng Ren, Rui Men, Dayiheng Liu,
                      scientific plots. In Proceedings of the IEEE/CVF Win-        ChangZhou,Jingren Zhou, and Junyang Lin. 2024a.
                      ter Conference on Applications of Computer Vision            Qwen2-vl: Enhancing vision-language model’s per-
                      (WACV).                                                      ception of the world at any resolution. arXiv preprint
                                                                                   arXiv:2409.12191.
                   OpenAI,JoshAchiam,StevenAdler,SandhiniAgarwal,               Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen,
                      Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-                YitaoLiu,RichardZhu,KaiquLiang,XindiWu,Hao-
                      man, Diogo Almeida, Janko Altenschmidt, Sam Alt-             tian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev
                      man, Shyamal Anadkat, Red Avila, Igor Babuschkin,            Arora, and Danqi Chen. 2024b. Charxiv: Charting
                      Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-          gaps in realistic chart understanding in multimodal
                      ing Bao, Mohammad Bavarian, Jeff Belgum, and                 llms. Preprint, arXiv:2406.18521.
                      262 others. 2024. Gpt-4 technical report. Preprint,
                      arXiv:2303.08774.                                         Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
                                                                                   Bosma,BrianIchter, Fei Xia, Ed Chi, Quoc Le, and
                   OWID.2024. Ourworldindata.                                      DennyZhou.2023. Chain-of-thoughtpromptingelic-
                                                                                   its reasoning in large language models. Preprint,
                   Pew. 2024. Pew research center.                                 arXiv:2201.11903.
                   PPIC. 2024. Public policy institute of california (ppic).    Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang
                                                                                   Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda
                   Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya              Xie, Xingkai Yu, Chong Ruan, and Ping Luo. 2024a.
                      Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-           Janus: Decoupling visual encoding for unified mul-
                      try, Amanda Askell, Pamela Mishkin, Jack Clark,              timodal understanding and generation.        Preprint,
                      Gretchen Krueger, and Ilya Sutskever. 2021. Learn-           arXiv:2410.13848.
                      ing transferable visual models from natural language
                      supervision. Preprint, arXiv:2103.00020.                  Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao
                                                                                   Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang
                   Siva Reddy, Danqi Chen, and Christopher D. Manning.             Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie,
                      2019. Coqa: A conversational question answering              YuWu,KaiHu,JiaweiWang,YaofengSun,Yukun
                      challenge. Preprint, arXiv:1808.07042.                       Li, Yishi Piao, Kang Guan, Aixin Liu, and 8 others.
                     2024b. Deepseek-vl2: Mixture-of-experts vision-        A Appendices
                     language models for advanced multimodal under-
                     standing. Preprint, arXiv:2412.10302.                  A.1   Dataset Construction
                  Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-         In this section, we outline the sources from which
                     gio, William W. Cohen, Ruslan Salakhutdinov, and       wecollected all the chart images.
                     Christopher D. Manning. 2018. Hotpotqa: A dataset
                     for diverse, explainable multi-hop question answer-    • Pew.    The Pew Research Center (Pew, 2024)
                     ing. Preprint, arXiv:1809.09600.                       publishes data reports on social issues, public opin-
                  Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan,              ion, and demographic trends, often using charts
                    Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. 2024.      and text to tell a clear data story. For our dataset,
                    Tinychart: Efficient chart understanding with visual    wecollected a subset of images from a larger cor-
                     token merging and program-of-thoughts learning.        pus compiled by (Islam et al., 2024). This corpus,
                    Preprint, arXiv:2404.16635.                             which includes 22,760 figures (charts and other
                                                                            images) scraped from the Pew Research website
                                                                            uptoMarch14,2024,providedourinitial pool of
                                                                            images. From this pool, we selected a subset and
                                                                            then further filtered it. We excluded simple statis-
                                                                            tical charts and basic visualizations like single bar
                                                                            or line charts, focusing instead on visually diverse
                                                                            charts covering a range of topics. We further col-
                                                                            lected the paragraphs associated with these chart
                                                                            images. The associated paragraphs not only de-
                                                                            scribe the visualized data but also offer additional
                                                                            context not explicitly mentioned in the charts, en-
                                                                            hancing their interpretive value.
                                                                            • Tableau.     We used Tableau Public (Tableau,
                                                                            2024) as a source for our dataset. Tableau Public
                                                                            allows users to create and share interactive dash-
                                                                            boards made up of data visualizations on a variety
                                                                            of topics. We sourced the chart images for our
                                                                            dataset from a larger corpus collected and curated
                                                                            by(Islam et al., 2024). Due to the complex nature
                                                                            of the dashboard representation, they manually cu-
                                                                            rated the data, focusing on dashboards with stories
                                                                            presented in a paginated format, where each page
                                                                            included both text and a corresponding chart. The
                                                                            final Tableau corpus from (Islam et al., 2024) con-
                                                                            sists of 100 dashboards covering a diverse range
                                                                            of topics and chart images. From this pool, we
                                                                            manually selected our own Tableau corpus based
                                                                            on specific criteria. We ensured that the selected
                                                                            dashboards included a variety of chart images, ac-
                                                                            companying paragraphs of reasonable length, and
                                                                            a broad representation of topics.
                                                                            • OWID. OurWorldinData(OWID)(OWID,
                                                                            2024) is a non-profit online platform that provides
                                                                            research and data on a wide range of global is-
                                                                            sues, including poverty, disease, hunger, climate
                                                                            change, and inequality. We sourced chart images
                                                                            from OWIDfocusingonincluding a diverse range
                                                                            of complex charts, i.e., multi-series line charts and
                  multi-column bar charts to enhance the dataset.          each source in Figure 6 and our different questions
                  • PPIC.    ThePublicPolicyInstituteofCalifornia          categories in Figure 7.
                  (PPIC) (PPIC, 2024) is an independent research in-       A.2   ComplexVisualizations
                  stitute dedicated to informing public policy in Cali-    Multi-chart images, infographics, and dashboards
                  fornia. Through data-driven research and analysis,       all vital data visualizations that serve different pur-
                  PPICexaminesawiderangeofpolicyareas,includ-              poses. Multi-chart images combine multiple charts
                  ing the economy, education, environment, and gov-        in a single visual often for comparison or to present
                  ernance. Similar to OWID corpus we sourced chart         different aspects of a dataset. Infographics inte-
                  images that excluded simple statistical charts and       grate text, images, and charts to explain concepts
                  basic visualizations like single bar or line charts,     or tell a story, focusing on clarity and engagement
                  focusinginsteadonvisuallydiversechartscovering           rather than detailed data analysis. Dashboards orga-
                  a range of topics to enhance the dataset.                nize charts, tables, and key metrics in a structured
                  • WebCharts. We built WebCharts corpus by                layout, providing an overview of important data for
                  leveraging prior work from efforts from Chart-           quick interpretation and decision-making. Table 5
                  Gemma (Masry et al., 2024b) and ChartInstruct            presents examples of each type for reference.
                  (Masry et al., 2024a). Their chart image collec-
                  tion process began with a seed list of 157 web-          A.3   Dataset Analysis
                  sites known to host charts (originally compiled          A.3.1   Visual Diversity
                  by Hoque and Agrawala (2019)), then querying             Figure 8 shows example charts from diverse topics
                  Google Images using terms like “chart images”,           in our CHARTQAPRO benchmark.
                 “graphs”, and “visual data.” This initial search
                  yielded a large number of images, which we then          A.3.2   Linguistic Diversity
                  filtered using a binary Vision Transformer (ViT)         In our analysis, we first quantified the lexical diver-
                  (Dosovitskiy et al., 2021) classifier to identify and    sity of each dataset by computing the Type-Token
                  isolate chart images. Any remaining non-chart im-        Ratio (TTR). Let T denote the total number of to-
                  ages were manually removed to ensure accuracy.           kens (i.e., words) extracted from a dataset and U
                  This process, starting with the seed list and refined    the number of unique tokens. The TTR is given by
                  through image search and classification, ultimately
                  gave us a pool of 41,000 chart images. From this                                      U
                  larger set, we carefully selected 800 charts, priori-                        TTR= T.
                  tizing visual and topical diversity. Our final selec-
                  tion emphasizes high visual quality and represen-        Higher TTR values indicate a richer vocabulary
                  tation across a range of chart styles, formats, and      and, consequently, greater lexical diversity. Our
                  subject matter. In addition, we manually curated         experiments revealed that the ChartQAPro dataset
                  200 infographic charts, which serve to highlight         achieved a TTR of 0.1516, compared to 0.1377 for
                  data visualization trends aimed at storytelling and      ChartQAand0.1189forChartxiv.
                  public engagement.                                         To assess semantic diversity, we computed the
                    Theextensive coverage of our dataset stands in         average pairwise cosine distance between text em-
                  contrast to prior datasets, which often relied on a      beddings.    We obtained vector representations
                  limited numberofsources,suchasStatista(Statista,         for each text using the Sentence-BERT model
                  2024) or Pew (Pew, 2024), and exhibited restricted       all-MiniLM-L6-v2. Foragiventextsamplei,
                  stylistic variation. By incorporating a significantly    let vi denote its embedding. The cosine distance
                  larger pool of sources, our dataset ensures broader      between two embeddings vi and vj is calculated
                  domaincoverageandricherstylistic representation,         as                             v ·v
                  addressing critical limitations in existing chart cor-              d(v ,v ) = 1−         i   j  .
                                                                                          i   j         ∥v ∥∥v ∥
                  pora. In addition to collecting the chart images, we                                     i     j
                  also gathered metadata associated with them, in-        Wethencomputedtheoverallsemanticdiversity as
                  cluding the URL, alt text, and other relevant details.   the average of these distances over all unique pairs,
                  Finally, the careful curation process resulted in a                                   X
                  diverse collection of 1341 chart images spanning                 D =           2          d(v ,v ),
                  various types and styles. We provide samples from                  avg    N(N−1)              i   j
                                                                                                        i<j
                            Pew                            OWID                               PPIC
                                                                                 Question:    What  is  the  difference  between  the
                                                                                 summation  of  two  least  populated  groups'  average
                                                                                 and  average  of  females  rounded  up  to  2  decimal
                                                                                 points?
                 Question:  What is the average confidence
                                              Question:  What is the increase in the median age
                 level  in  religious  leaders  across  the  years
                                              from 2050 to 2075 in years?
                                                                                 Answer:  22.5
                 shown (2016, 2018, 2019)?
                                              Answer:  2.9
                 Answer:  0.5
                   WebCharts (Infographics)
                                                          Tableau                          WebCharts
                                              Q1: What is the ratio of the popularity of the top genre
                                              to the least popular genre? 
                                              A1: 2
                                                                                 Question:  In which year did the Maximum Personal
                                              Q2: Can you estimate the mode value of Hip-Hop over
                                                                                 Income Tax Rate peak?
                                              the years?
                                              A2: 60
                                                                                 A) 1945
                                              Q3: Is this always more or less than Electronic music
                                                                                 B) 1963
                                              over the years?
                                                                                 C) 1981
                                              A3: More
                                                                                 D) 1953
                 Question:    In  2021,  Quebec  had  twice  as
                                              Q4: What is the percentage of popularity for Rock and
                 many non-financial cooperatives as Ontario.
                                              Pop genres combined?
                                              A4: 23%
                 Answer:  True                                                   Answer:  A
                Figure 6: Example of chart images collected from different sources and their corresponding QA pairs in
                CHARTQAPRO.
                where N is the total number of text samples. A    A.5   Evaluation Metric
                higher value of D    indicates that the texts are
                                 avg                              Weevaluate ChartQA model predictions using a re-
                moresemantically dispersed. ChartQAPro showed     laxed correctness metric that handles numeric, tex-
                an average cosine distance of 0.8439, compared to tual, and list-based responses through three cases:
                0.7558 for ChartQA and 0.7831 for Chartxiv.
                  Overall, these metrics—lexical diversity (TTR)    1. MCQ&FactCheckingAnswers: Weuse
                and semantic diversity (average pairwise co-           exact match to evaluate these two types of
                sine distance computed using Sentence-BERT             questions.
                all-MiniLM-L6-v2)—demonstrate that the              2. Numeric Answers: For numeric answers (ex-
                ChartQAProdataset is linguistically more diverse       cluding years), a small relative error is al-
                than the previous benchmarks. Figure 11 illustrates    lowed. Let t and p denote the target and pre-
                these findings, showing that ChartQAPro outper-        dicted numbers, respectively. The relative er-
                forms ChartQA and Chartxiv with higher TTR and         ror is defined as
                semantic diversity.
                A.4   PromptsforModelsEvaluation                                      E=|p−t|.
                                                                                             |t|
                To promote transparency and reproducibility, we        Theprediction is deemed correct if
                providetheexactpromptsusedtoevaluateourmod-
                els. Table 6 presents the prompts for the Direct                 E≤ϵ, withϵ=0.05.
                QuestionAnsweringsetup,Table7detailsthosefor
                the Chain-of-Thought setup, and Table 8 outlines    3. Year Answers: For answers representing
                the prompts for the Program-of-Thought setup.          years, an exact match is required to prevent
                                 false positives (e.g., 2009 and 2010 would                                  A.7       PerformanceComparisonwithPrevious
                                 otherwise yield an error rate below 0.05).                                            Benchmarks
                             4. Textual Answers: For non-numeric textual                                     Table 10 compares the performance of Claude
                                 answers, we use the Average Normalized                                      Sonnet 3.5,             the top-performing model,                         on
                                 Levenshtein Similarity (ANLS) metric (Biten                                 CHARTQAPRO against its results on two prior
                                 et al., 2019) rather than strict matching.                                  chart-reasoning benchmarks: ChartQA (Masry
                                                                                                             et al., 2022) and CharXiv (Wang et al., 2024b).
                             Asingle target–prediction pair is evaluated by
                          the function C(t,p):                                                               A.8       Error Analysis
                                                                                                            Figure12presentssamplemodelerrorsacrossthree
                                      
                                        ExactM(p,t), ifquestionisMCQorFactChecking,
                                      
                                      
                                      
                                      
                                      
                                                                                                            categories: visual perception failures, instruction-
                                      
                                        ExactM(p,t), iftandpareyears,
                                      
                                      
                                      
                            C(t,p) =                                                |p −t|                  following issues (CoT, PoT, direct), and mathemat-
                                        1,                if t and p are numeric and         ≤0.05,
                                                                                       |t|
                                                                                                            ical reasoning mistakes.
                                      
                                                                                    |p −t|
                                      
                                      
                                        0,                if t and p are numeric and         >0.05,
                                      
                                      
                                                                                       |t|
                                      
                                      
                                                                                                            A.9       Ablations Results
                                        ANLS(p,t),        otherwise.
                                                                                                   (1)       Figure 13 presents sample errors from open-source
                          List-based Answers:                  For responses provided as                     models—Phi 3.5 Vision 4B (Abdin et al., 2024),
                          lists (encoded as strings), we first parse the lists                               Llama3.2Vision11B(Grattafiori et al., 2024), and
                          and then compute the score for each corresponding                                  TinyChart (Zhang et al., 2024)—across three cate-
                          target–prediction pair. Let                                                        gories: complex visuals, unanswerable questions,
                          T =[t ,t ,...,t ]                 and       P =[p ,p ,...,p ].                     and charts with accompanying paragraphs.
                                    1    2           N                           1    2            N
                          Theoverall score for the list is
                                                                     N
                                        C (T,P)= 1 XC(t,p).                                        (2)
                                           list                N                i   i
                                                                    i=1
                          Overall Evaluation:                 Thefinal accuracy is com-
                          puted by averaging the scores over all M examples:
                                                                         M
                                              Accuracy = 1 XCj.
                                                                  Mj=1
                             This metric tolerates minor numeric errors, en-
                          forces exact matching for years to avoid mislead-
                          ing correctness from near-miss values, and uses the
                         ANLSscore (Biten et al., 2019) to assign partial
                          credit for nearly correct textual answers (e.g., “Fe-
                          male” vs. “Females”). We will open-source the
                          evaluation metric code to ensure reproducibility
                          and facilitate further research.
                          A.6      HumanBaselineSetup
                          Toapproximate an upper bound on model perfor-
                          mance,weconductedahumanbaselineexperiment.
                         Anexpertin-house graduate student answered 50
                          randomly sampled questions from each category
                          (Factoid, Conversational, etc.) using the exact same
                          prompts provided to the models to ensures consis-
                          tency and fairness. The resulting accuracies are re-
                          ported in Table 3 under the Direct prompting setup,
                          as Chain-of-Thought and Program-of-Thought for-
                          mats do not directly apply to human responses.
                               Multi-Chart Image                                 Infographic
                     Combinesmultiple charts to compare data        Integrates text and visuals to tell a story
                                                          Dashboard
                                           Displays key metrics for quick interpretation
                   Table 5: Examples of Multi-Chart Images, Infographics, and Dashboards, with distinct background colors for clarity.
                         Mathematical Reasoning              Visual Reasoning
                                                                                               Fact-Checking
                                                                                     Question:  Only three countries have a higher GDP per
                  Question:    If  the  rate  of  increase  in  Mathematics
                                                    Question:  What is the label of the line that remains in
                                                                                     capita  than  the  average  GDP  per  capita  of  the  top  10
                  proficiency between 2009 and 2011 ..., what would be the
                                                    the middle most of the time?
                                                                                     countries.
                  exact percentage ... in English-language arts in 2014?
                                                    Answer:  Index of Services
                                                                                     Answer:  True
                  Answer:  59%
                             Hypothetical
                                                              Conversational                   Multiple-Choice
                                                                                     Question:  If the Dow Jones Industrial Average continued
                                                    Q1: How many different scenarios are presented in the
                                                                                     its overall growth trend from 2011 into 2012, what would be
                                                    image?
                                                                                     the projected value at the end of 2012?
                                                    A1: 4
                                                    Q2:  Which  scenario(s)  shows  the  most  significant
                                                                                     A) 12,800
                                                    temperature reduction from 2015 to 2022?
                                                                                     B) 16,200 
                                                    A2: [Policies and action, Optimistic scenario]
                  Question:    If  the  percentage  of  women  sandwich
                                                                                     C) 13,577 
                  caregivers doubled, what would be the leading category
                                                    Q3: Which one of these two has a larger percentage
                                                                                     D) 16,000
                  of type of caregiver?
                                                    decrease of temperature from its original value?
                                                    A3: Optimistic scenario
                                                                                     Answer:  A
                  Answer:  Care for children
                                                                                        Unanswerable
                                    Multi-Chart QA
                                                                     Question:  What is the average rate of change of the job market in the Bay Area between
                  Question:  Do the views overtime and watch time always follow the same pattern?
                                                                     January 2001 and September 2024?
                  Answer:  No
                                                                     Answer:  Unanswerable
                                   Figure 7: More examples of different question types in CHARTQAPRO.
                                                      Economy
                        Politics
                                                       Health
                       Environment
                                                    International Affairs
                       Technology
          Figure 8: Examples of different charts related to major topics, i.e., ‘Politics’, ‘Environment’, ‘Economy’, ‘Health’,
          ‘Technology’, ‘International Affairs’ etc. in CHARTQAPRO.
                                (a) VLM Generated (Correct)                            (b) VLM Generated (Incorrect)
                                                                           Question:  If the energy efficiency improves from current
                                                                           to  80.00%,  and  the  production  cost  decreases
                                                                           proportionally, what would be the new production cost?
                   Question:    What  percentage  of  Americans  consider
                                                                           Assume the production (TWh) remains constant and that
                   increasing security along the U.S.-Mexico border to be
                                                                           the  proportional  relationship  between  energy  efficiency
                   either “Very important” or “Somewhat important”?
                                                                           and production cost is directly linear.
                   Answer:  68%                                            Answer:  79193.91 (66916.15)
                  Figure 9: Examples of VLM-assisted question-and-answer pairs, where: (a) the VLM generates a question along
                  with a correct answer, marked in Green text, (b) the VLM generates a question, but the answer is incorrect, marked
                  in Red text.
                     Category             Prompt Template
                     Factoid              Youaregivenafactoid question that you need to answer based on the provided image.
                                          Your answer should be a single word, number, or phrase. If the question is unanswerable based on
                                          the information in the provided image, your answer should be unanswerable. Do not generate units.
                                          But if numerical units such as million, m, billion, B, or K are required, use the exact notation
                                          showninthechart.
                                          If there are multiple answers, put them in brackets using this format [’Answer1’, ’Answer2’].
                                          Remembertogeneratethefinal answer only without any additional text!
                                          Question: <question>
                     Multi Choice         Youaregiven a question along with different possible answers. You need to select the correct answer
                                          from them based on the provided image.
                                          Your answer should be one of the options letters only: a, b, c or d (just the letter itself without any
                                          additional text). If the question is unanswerable based on the information in the provided image, your
                                          answer should be unanswerable.
                                          If there are multiple answers, put them in brackets using this format [’Answer1’, ’Answer2’].
                                          Remembertogeneratethefinal answer only without any additional text!
                                          Question: <question>
                     Hypothetical         Youaregivenahypothetical question that you need to answer based on the provided image.
                                          Your answer should be a single word, number, or phrase. If the question is unanswerable based on
                                          the information in the provided image, your answer should be unanswerable. Do not generate units.
                                          But if numerical units such as million, m, billion, B, or K are required, use the exact notation
                                          showninthechart.
                                          If there are multiple answers, put them in brackets using this format [’Answer1’, ’Answer2’].
                                          Remembertogeneratethefinal answer only without any additional text!
                                          Question: <question>
                     Fact Checking        Youaregivenafact statement that you need to assess based on the provided image.
                                          Your answer should be either true or false (without any additional text). If the question is
                                          unanswerable based on the information in the provided image, your answer should be unanswerable.
                                          If there are multiple answers, put them in brackets using this format [’Answer1’, ’Answer2’].
                                          Remembertogeneratethefinal answer only without any additional text!
                                          Question: <question>
                     Conversational       You are given a multi-turn conversation, and your job is to answer the final question based on the
                                          conversation history and the information in the provided image.
                                          Your answer should be a single word, number, or phrase. If the question is unanswerable based on
                                          the information in the provided image, your answer should be unanswerable. Do not generate units.
                                          But if numerical units such as million, m, billion, B, or K are required, use the exact notation
                                          showninthechart.
                                          If there are multiple answers, put them in brackets using this format [’Answer1’, ’Answer2’].
                                          Remembertogeneratethefinal answer only without any additional text!
                                          Conversation: <conversation>  Question: <question>
                                       Table 6: Prompt Templates for Each Question Category in the Direct setup.
                     Category             Prompt Template
                     Factoid              Youaregivenafactoid question that you need to answer based on the provided image.
                                          Youneedtothinkstep-by-step, but your final answer should be a single word, number, or phrase. If
                                          the question is unanswerable based on the information in the provided image, your answer should be
                                          unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or K are
                                          required, use the exact notation shown in the chart.
                                          If there are multiple final answers, put them in brackets using this format [’Answer1’, ’Answer2’]. .
                                          Remembertothinkstep-by-step and format the final answer in a separate sentence like "The answer is
                                          X"
                                          Question: <question>
                     Multi Choice         Youaregiven a question along with different possible answers. You need to select the correct answer
                                          from them based on the provided image.
                                          Youneedtothinkstep-by-step, but your final answer should be one of the options letters only: a, b, c
                                          or d (just the letter itself without any additional text). If the question is unanswerable based on the
                                          information in the provided image, your answer should be unanswerable.
                                          If there are multiple final answers, put them in brackets using this format [’Answer1’, ’Answer2’]. .
                                          Remembertothinkstep-by-step and format the final answer in a separate sentence like "The answer is
                                          X"
                                          Question: <question>
                     Hypothetical         Youaregivenahypothetical question that you need to answer based on the provided image.
                                          Youneedtothinkstep-by-step, but your final answer should be a single word, number, or phrase. If
                                          the question is unanswerable based on the information in the provided image, your answer should be
                                          unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or K are
                                          required, use the exact notation shown in the chart.
                                          If there are multiple final answers, put them in brackets using this format [’Answer1’, ’Answer2’].
                                          Remembertothinkstep-by-step and format the final answer in a separate sentence like "The answer is
                                          X"
                                          Question: <question>
                     Fact Checking        Youaregiven a fact statement that you need to assess based on the information in the provided image.
                                          You need to think step-by-step, but your final answer should be either true or false (without any
                                          additional text). If the question is unanswerable based on the information in the provided image, your
                                          answer should be unanswerable.
                                          If there are multiple final answers, put them in brackets using this format [’Answer1’, ’Answer2’]. .
                                          Remembertothinkstep-by-step and format the final answer in a separate sentence like "The answer is
                                          X"
                                          Question: <question>
                     Conversational       You are given a multi-turn conversation, and your job is to answer the final question based on the
                                          conversation history and the information in the provided image.
                                          Youneedtothinkstep-by-step, but your final answer should be a single word, number, or phrase. If
                                          the question is unanswerable based on the information in the provided image, your answer should be
                                          unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or K are
                                          required, use the exact notation shown in the chart.
                                          If there are multiple final answers, put them in brackets using this format [’Answer1’, ’Answer2’]. .
                                          Remembertothinkstep-by-step and format the final answer in a separate sentence like "The answer is
                                          X"
                                          Conversation: <conversation>  Question: <question>
                               Table 7: Prompt Templates for Each Question Category under the Chain of Thought Setup
                     Category             Prompt Template
                     Factoid              Youaregivenafactoid question that you need to answer based on the provided image.
                                          Youneedtowriteanexecutable python code that calculates and prints the final answer, but your final
                                          answer should be a single word, number, or phrase. If the question is unanswerable based on the
                                          information in the provided image, your answer should be unanswerable. Do not generate units. But
                                          if numerical units such as million, m, billion, B, or K are required, use the exact notation shown in the
                                          chart.
                                          If there are multiple final answers, put them in brackets using this format [’Answer1’, ’Answer2’].
                                          Remembertoreturnapythoncodeonlywithoutanyadditional text.
                                          Question: <question>
                     Multi Choice         Youaregiven a question along with different possible answers. You need to select the correct answer
                                          from them based on the provided image.
                                          Youneedtowriteanexecutable python code that calculates and prints the final answer, but your final
                                          answershouldbeoneoftheoptionslettersonly: a, b, c or d (just the letter itself without any additional
                                          text). If the question is unanswerable based on the information in the provided image, your answer
                                          should be unanswerable.
                                          If there are multiple final answers, put them in brackets using this format [’Answer1’, ’Answer2’].
                                          Remembertoreturnapythoncodeonlywithoutanyadditional text.
                                          Question: <question>
                     Hypothetical         Youaregivenahypothetical question that you need to answer based on the provided image.
                                          Youneedtowriteanexecutable python code that calculates and prints the final answer, but your final
                                          answer should be a single word, number, or phrase. If the question is unanswerable based on the
                                          information in the provided image, your answer should be unanswerable. Do not generate units. But
                                          if numerical units such as million, m, billion, B, or K are required, use the exact notation shown in the
                                          chart.
                                          If there are multiple final answers, put them in brackets using this format [’Answer1’, ’Answer2’].
                                          Remembertoreturnapythoncodeonlywithoutanyadditional text.
                                          Question: <question>
                     Fact Checking        Youaregiven a fact statement that you need to assess based on the information in the provided image.
                                          Youneedtowriteanexecutable python code that calculates and prints the final answer, but your final
                                          answer should be either true or false (without any additional text). If the question is unanswerable
                                          based on the information in the provided image, your answer should be unanswerable.
                                          If there are multiple final answers, put them in brackets using this format [’Answer1’, ’Answer2’].
                                          Remembertoreturnapythoncodeonlywithoutanyadditional text.
                                          Question: <question>
                     Conversational       You are given a multi-turn conversation, and your job is to answer the final question based on the
                                          conversation history and the information in the provided image.
                                          Youneedtowriteanexecutable python code that calculates and prints the final answer, but your final
                                          answer should be a single word, number, or phrase. If the question is unanswerable based on the
                                          information in the provided image, your answer should be unanswerable. Do not generate units. But
                                          if numerical units such as million, m, billion, B, or K are required, use the exact notation shown in the
                                          chart.
                                          If there are multiple final answers, put them in brackets using this format [’Answer1’, ’Answer2’].
                                          Remembertoreturnapythoncodeonlywithoutanyadditional text.
                                          Conversation: <conversation>  Question: <question>
                               Table 8: Prompt Templates for Each Question Category in the Program-of-Thought setup.
                      Category                Prompt Template
                      Reasoning               Generate some of the most difficult Factoid Questions alongside the Corresponding Answers for the
                                              given image.
                                              Thequestionscouldberelatedtonumericalorvisualreasoning. AndtheAnswerscouldbeanumber,
                                              text label, or a common phrase (Yes, No).
                                              Youshouldrespond in an Array of JSON objects format with the following keys: (i) Question, and
                                              (ii) Answer.
                      Multiple-Choice         I will upload some charts, graphs, infographics or other data visualizations. Generate five multiple-
                                              choice questions.
                                              Eachquestion should contain four options and one correct answer.
                                              Questions should require some complex calculations such as trend analysis, anomaly detection,
                                              extrapolation, or time series analysis.
                                              For the correct answer, show your calculations as well.
                      Hypothetical            YouareanAIthatgeneratesconcise and specific hypothetical questions based on chart images. Your
                                              task is to analyze the chart and generate a short, data-driven hypothetical question that explores
                                              future trends, impacts, or extrapolations based on the data.
                                              Avoid adding unnecessary explanations or context like ‘Based on the chart data...’ or ‘A meaningful
                                              hypothetical question could be...’.
                                              Keepthequestion focused and directly related to the chart. The question should make an assumption
                                              about future trends, impacts, or extrapolations based on the data.
                      Fact-Checking           ### Task Description:
                                              Given a chart image in the input, your task is the following:
                                              1. Analyze the given chart image and generate ‘3’ to ‘5’ pairs of claims and verdicts about its data.
                                              Half of the claims should be supported by the chart’s data, while the other half are refuted.
                                              2. Avoid using terms like ‘rows’, ‘columns’, or ‘elements’ from the data table; refer to ‘chart’ or
                                              ‘chart image’ instead. If the claim is supported, the verdict should be ‘True’. If the claim is refuted,
                                              the verdict should be ‘False’, followed by a brief explanation.
                                              3. The claims should cover comparisons of values or trends, basic statistical values (maximum,
                                              minimum,mean,median,mode)withoutusingexactnumbersfromthechart.
                                              4. Ensure a diverse range of claims addressing various visual aspects of the chart, resulting in 3-5
                                              turns of claims and verdicts.
                                              5. Generate the claims in between ‘<claim >’ tags, and the verdicts/answers in between ‘<answer >’
                                              tags, without any additional explanation.
                      Conversational          Showmeconversational question answering for analyzing the <chart type >. Make sure this looks
                                              like a proper conversation that makes references to previous questions/answers.
                                              Makesureall the questions are such that the answer is concise and all questions require arithmetic
                                              and logical reasoning.
                                              Please make sure to ask mathematical and visual reasoning questions that require multiple complex
                                              operations (e..g, ‘sum’, ‘min’, ‘max’, ‘diff’, ‘ratio’, ...etc).
                                               Table 9: Prompt Templates for generating questions using VLMs.
                           Benchmark                          Description                                                    Accuracy(%)
                           ChartQA(Masryetal.,2022)           Standard benchmark for chart reasoning                              90.50
                           CharXiv(Wangetal.,2024b)           Scientific charts from arXiv, limiting diversity                    60.20
                            CHARTQAPRO(Ours)                  Diverse in chart sources, topics, styles, and question types        55.81
                    Table 10: Performance of Claude Sonnet 3.5 across three chart-reasoning benchmarks. The lower accuracy on
                    CHARTQAPRO(55.81%)illustrates its increased difficulty compared to ChartQA (90.50%), highlighting the need
                    for more robust chart understanding capabilities.
        Figure 10: Box plot of pairwise cosine distances among
        chart images. CHARTQAPRO exhibits a higher median
        andconsistentlylargerdistances,indicatingsignificantly
        greater visual diversity.
        Figure 11: Linguistic Diversity Comparison Across Datasets. The figure shows lexical diversity (TTR) and semantic
        diversity (cosine distance) for ChartQA, Chartxiv, and ChartQAPro. Higher TTR and semantic diversity indicate
        richer vocabulary and broader semantic coverage. ChartQAPro exhibits the highest diversity.
                                                     Visual Perception Errors                                                    Instruction Following Errors
                                                                                                                                                                                                                  Math Reasoning Errors
                                                 (a) Claude Sonnet 3.5 - CoT Failure
                                                                                                                                (c) Ovis1.6-Gemma2-9B - CoT Failure                                           (e) Qwen2-VL-7B - Math Reasoning Error
                                       Question: determine the airline with the highest increase
                                                                                                                                                                                                           Question:  What is average of the leftmost green
                                       in ghg emissions from 2008 to 2014
                                                                                                                     Prompt Style: Chain-of-Thought (CoT)
                                                                                                                                                                                                           bar and the rightmost light green bar? 
                                       Gound Truth: United Airlines
                                                                                                                     Question:  while the line representing 'multiplier of 6 + cost
                                       Model Response: 
                                                                                                                                                                                                           Gound Truth: 26.5
                                       To answer this question, I'll follow these steps:
                                                                                                                     of  refinancing  operations'  generally  stays  above  the  line
                                       1. Identify airlines with data for both 2008 and 2014.
                                                                                                                                                                                                           Model Response: 66
                                       2. Calculate the change in emissions for each of these airlines between 2008 and 2014.
                                                                                                                     representing 'annual cost to banks of negative deposit rate'
                                       3. Compare the increases to find the largest.\
                                       Looking at the graph:
                                                                                                                     for a significant period, it dips below zero around late 2020
                                       1. Airlines with clear data for both 2008 and 2014 include:
                                         - American Airlines - Delta Air Lines   - United Airlines - Lufthansa Group - Air France KLM - Emirates
                                                                                                                     and early 2021.
                                       2. Calculating approximate changes from 2008 to 2014:
                                                                                                                                                                                                          (f) Phi 3.5 Vision 4B - Math Reasoning Error
                                        - American Airlines: ~30 to 42 (increase of ~12)
                                        - Delta Air Lines: ~33 to 34 (increase of ~1)
                                        - United Airlines: ~28 to 31 (increase of ~3)
                                                                                                                     Gound Truth: False
                                        - Lufthansa Group: ~28 to 28 (no significant change)
                                        - Air France KLM: ~28 to 28 (no significant change)
                                        - Emirates: ~22 to 26 (increase of ~4)
                                                                                                                     Model Response: The answer is True.
                                       3. The largest increase is for American Airlines at approximately 12 million tonnes.
                                       The answer is American Airlines.
                                                                                                                              (d) Llama 3.2-Vision 11B - Direct Failure
                                                      (b) GPT4o - Visual Perception
                                      Question: what is the approximate ratio of peak daily cases
                                      in the us compared to the eu?
                                      a) 1.1:1
                                                                                                                    Prompt Style: Direct
                                      b) 1.3:1
                                      c) 1.5:1
                                                                                                                    Question:  What was the average of changes for                                      Question:    What  is  the  ratio  of  second  most
                                      d) 1.7:1
                                                                                                                    the regions that experienced an increase?                                           contributor  platform  to  gaming  revenue  in  2012
                                      Gound Truth: B
                                                                                                                                                                                                        rounded to 2 decimal points?
                                                                                                                    Gound Truth: 3.6
                                      Model Response: 
                                                                                                                    Model Response: 
                                                                                                                                                                                                        Gound Truth: 0.37
                                      To determine the approximate ratio of peak daily cases in the US compared to the EU, we need to
                                      analyze the graph presented.
                                                                                                                    To find the average of changes for the regions that experienced an increase,
                                      1. **Identify Peak Cases**: Look at the peak of the US curve (red) and the peak of the EU curve (blue). 
                                                                                                                    we need to first identify the regions with positive changes. These regions are
                                                                                                                                                                                                        Model Response: 0.16
                                      2. **Estimate Values**: From the graph:
                                                                                                                    Asia Pacific (10.0%), Europe (8.0%), and Latin America (4.4%). 
                                       - The peak daily cases for the US appear to be around 40,000.
                                                                                                                    Next, we add up these values: 10.0 + 8.0 + 4.4 = 22.4. 
                                       - The peak for the EU appears to be around 25,000.
                                      3. **Calculate Ratio**: The ratio can be calculated as follows:
                                                                                                                    Finally, we divide this sum by the number of regions (3) to get the average:
                                       - US peak / EU peak = 40,000 / 25,000 = 1.6.
                                                                                                                    22.4 / 3 = 7.47. 
                                      4. **Select Closest Answer**: The closest provided option that reflects this ratio is:
                                                                                                                    Therefore,  the  average  of  changes  for  the  regions  that  experienced  an
                                      - c) 1.5:1.
                                      Based on this analysis, the final answer is:
                                                                                                                    increase is 7.47.
                                      The answer is c.
                                     Figure 12: Sample errors across three categories: Visual Perception, Instruction Following (CoT, PoT, Direct), and
                                     Mathematical Reasoning.
                                     Complex Visuals (Dashboards / Infographics)                                       Unanswerable Questions                                        Charts with Accompanying Paragraph
                                                  (a) Phi 3.5 Vision - Complex Visual                           (d) Phi 3.5 Vision - Unanswerable Question                          (g) Phi 3.5 Vision - Accompanying Paragraph
                                                                                                                                                                                                                     "The share of Americans who are mostly
                                                                                                                                                                                                                     concerned about AI in daily life is up 14
                                                                                                                                                                                                                     percentage points since December 2022,
                                                                                                                                                                                                                     when 38% expressed this view. Concern
                                                                                                                                                                                                                     about AI outweighs excitement across all
                                                                                                                                                                                                                     major demographic groups. Still, there are
                                                                                                                                                                                                                     some notable differences, particularly by age.
                                                                                                                                                                                                                     About six-in-ten adults ages 65 and older
                                                                                                                                                                                                                     (61%) are mostly concerned about the
                                                                                                                                                                                                                     growing use of AI in daily life, while 4% are
                                                                                                                                                                                                                     mostly excited. That gap is much smaller
                                                                                                                                                                                                                     among those ages 18 to 29: 42% are more
                                                                                                                                                                                                                     concerned and 17% are more excited. The
                                                                                                                                                                                                                     rise in concern about AI has taken place
                                                                                                                                                                                                                     alongside growing public awareness. Nine-in-
                                                                                                                                                                                                                     ten adults have heard either a lot (33%) or a
                                                                                                                                                                                                                     little (56%) about artificial intelligence. The
                                                                                                                                                                                                                     share who have heard a lot about AI is up 7
                                                                                                                                                                                                                     points since December 2022. Those who
                                                                                                                                                                                                                     have heard a lot about AI are 16 points more
                                                                                                                                                                                                                     likely now than they were in December 2022
                                                                                                                                                                                                                     to express greater concern than excitement
                                                                                                                                                                                                                     about it. Among this most aware group,
                                                                                                                                                                                                                     concern now outweighs excitement by 47% to
                                                                                                                                                                                                                     15%. In December, this margin was 31% to
                                                                                                                                                                                                                     23%."
                                                                                                          Question:  what was the total market capitalization of                Question:  what is the average percentage of u.s. adults
                                    Question:  what was the average click rate per week for
                                                                                                          pg&e on september 11, 2017?                                           who feel equally excited and concerned about ai over the
                                    the first eight weeks of 2024?
                                                                                                                                                                                three years?
                                    Gound Truth: 6.5K                                                     Gound Truth: Unanswerable
                                                                                                                                                                                Gound Truth: 42.33
                                    Model Response: 3.1                                                   Model Response: $71                                                   Model Response: 45.67
                                                 (b) Qwen2-VL-7B - Complex Visual                               (e) Qwen2-VL-7B - Unanswerable Question                             (h) Qwen2-VL-7B - Accompanying Paragraph
                                                                                                                                                                                                                             Labour productivity across the
                                                                                                                                                                                                                             Ox-Cam Arc. labor productivity
                                                                                                                                                                                                                             varies considerably across the
                                                                                                                                                                                                                             Ox-Cam Arc, from 27% higher
                                                                                                                                                                                                                             than the national average in
                                                                                                                                                                                                                             Chiltern, to 33% below the
                                                                                                                                                                                                                             national average in East
                                                                                                                                                                                                                             Northhamptonshire"
                                                                                                                                                                                                                             Arc labor productivity over last
                                                                                                                                                                                                                             10 years. Buckinghamshire's
                                                                                                                                                                                                                             labour productivity (relative to
                                                                                                                                                                                                                             the UK average) has dropped
                                                                                                                                                                                                                             year-on-year since 2010."
                                                                                                                                                                                                                             Average annual productivity
                                                                                                                                                                                                                             growth (2014-2018) by LEP.
                                                                                                                                                                                                                             Buckinghamshire has
                                                                                                                                                                                                                             experienced slow productivity
                                                                                                                                                                                                                             growth over the last five years
                                                                                                                                                                                                                             (2014-18), the third lowest of all
                                                                                                                                                                                                                             38 LEP areas."
                                                                                                                                                                               Question:  how many local authority areas have a gva
                                                                                                                                                                               per hour worked index that crosses the 100 mark?
                                                                                                                                                                                Gound Truth: 9
                                                                                                          Question:    In  which  year  did  spain's                            Model Response: 2
                                    Question:  what  is  the  difference  between  the  peak
                                                                                                          unemployment rate reach its highest point?
                                                                                                                                                                                    (i) TinyChart - Accompanying Paragraph
                                    conversion value in may and the value marked in red ?
                                                                                                                                                                                                                   Americans judgments about the potential impact of
                                                                                                                                                                                                                   this set of applications are varied and, for portions
                                    Gound Truth: 48                                                       Gound Truth: Unanswerable
                                                                                                                                                                                                                   of the public, marked by
                                                                                                                                                                                                                   uncertainty......................Another concern for
                                    Model Response: 147                                                   Model Response: 2013
                                                                                                                                                                                                                   Americans is tied to the potential impact of these
                                                                                                                                                                                                                   emerging technologies on social equity. For
                                                                                                                                                                                                                   instance, 57% of Americans say the widespread
                                                                                                                                                                                                                   use of brain chips for enhanced cognitive function
                                                                                                                (f) TinyChart - Unanswerable Question
                                                 (c) TinyChart - Complex Visual
                                                                                                                                                                                                                   would increase the gap between higher- and lower-
                                                                                                                                                                                                                   income Americans, while just 10% say it would
                                                                                                                                                                                                                   decrease the gap. There are similar patterns in
                                                                                                                                                                                                                   views about the widespread use of driverless cars
                                                                                                                                                                                                                   and gene editing for babies to greatly reduce the
                                                                                                                                                                                                                   risk of serious disease during their lifetime.
                                                                                                                                                                        Q:  which ai application is viewed most positively by the public?
                                                                                                                                                                        A: Face Recognition Technology
                                                                                                                                                                        Q: how much more positive are they than unsure?
                                                                                                                                                                        A: 19
                                                                                                                                                                        Q: for all three ai applications seen, are they on average viewed as
                                                                                                                                                                        good, bad, or are people mostly unsure about them?
                                                                                                                                                                        A: good
                                                                                                                                                                        Q: is this also the case for human enhancement application, or are
                                                                                                                                                                        they viewed more as bad on average?
                                                                                                                                                                        A: no
                                     Q:  what  is  the  highest  rotor  bearing  temperature
                                                                                                                                                                        Q: which of these types of applications are viewed most negatively
                                     recorded among all turbines?
                                                                                                                                                                        for society then?
                                     A: 30.0
                                                                                                                                                                        A: Robotics Exoskeletons
                                                                                                           Question:  who was the leader of the coup that
                                     Q:  which  turbine  has  the  lowest  rotor  bearing
                                                                                                                                                                        Q: specifically, are these type of applications seen as good for social
                                                                                                           took initiated in 2021?
                                                                                                                                                                        equity according to the text?
                                     temperature?
                                                                                                           Gound Truth: Unanswerable
                                                                                                                                                                        Gound Truth: No
                                     Gound Truth: R80721
                                     Model Response: 18.2                                                  Model Response: Sisi                                         Model Response: than a good (26%) idea for society.
                                                Figure 13: Sample errors from open-source models across different categories in CHARTQAPRO.
