                                Inference Machines which ditch the belief propagation algorithm altogether and instead train a series
                                of regressors to output the correct marginals by passing messages on a graph. Wei et al. [2016]
                                applies this idea to pose estimation using a series of convolutional layers and Deng et al. [2016]
                                introduces a recurrent node update for the same domain.
                                There is rich literature on combining symbolic reasoning and logic with sub-symbolic distributed
                                representations which goes all the way back to the birth of the idea of parallel distributed processing
                                McCulloch and Pitts [1943]. See [Raedt et al., 2016, Besold et al., 2017] for two recent surveys.
                                Here we describe only a few recent methods. Seraﬁni and Garcez [2016] introduces the Logic
                                Tensor Network (LTN) which describes a ﬁrst order logic in which symbols are grounded as vector
                                embeddings, and predicates and functions are grounded as tensor networks. The embeddings and
                                tensor networks are then optimized jointly to maximize a fuzzy satisﬁability measure over a set of
                                knownfacts and fuzzy constraints. Šourek et al. [2015] introduces the Lifted Relational Network
                                which combines relational logic with neural networks by creating neural networks from lifted rules
                                and training examples, such that the connections between neurons created from the same lifted rules
                                shares weights. Our approach differs fundamentally in that we do not aim to bridge symbolic and
                                sub-symbolic methods. Instead we stay completely in the sub-symbolic realm. We do not introduce or
                                consider any explicit logic, aim to discover (fuzzy) logic rules, or attempt to include prior knowledge
                                in the form of logical constraints.
                                AmosandKolter[2017]Introduces OptNet, a neural network layer that solve quadratic programs
                                usinganefﬁcientdifferentiablesolver. OptNetistrainedtosolve4x4Sudokusamongstotherproblems
                                and beats the deep convolutional network baseline as described in Park [2016]. Unfortunately we
                                cannot compare to OptNet directly as it has computational issues scaling to 9x9 Sudokus (Brandon
                                Amos,2018,personal communication).
                                Sukhbaatar et al. [2016] proposes the Communication Network (CommNet) for learning multi-agent
                                cooperation and communication using back-propagation. It is similar to our recurrent relational
                                network, but differs in key aspects. The messages passed between all nodes at a given step are the
                                same, corresponding to the average of all the node hidden states. Also, it is not trained to minimize
                                the loss on every step of the algorithm.
                                Acknowledgments
                                We’dlike to thank the anonymous reviewers for the valuable comments and suggestions, especially
                                reviewer 2 who suggested the age arithmetic task. This research was supported by the NVIDIA
                                Corporation with the donation of TITAN X GPUs.
                                References
                                Brandon AmosandJZicoKolter. Optnet: Differentiable optimization as a layer in neural networks.
                                   arXiv preprint arXiv:1703.00443, 2017.
                                Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks
                                   for learning about objects, relations and physics. In Advances in Neural Information Processing
                                   Systems, pages 4502–4510, 2016.
                                Heiko Bauke. Passing messages to lonely numbers. Computing in Science & Engineering, 10(2):
                                   32–40, 2008.
                                Tarek R Besold, Artur d’Avila Garcez, Sebastian Bader, Howard Bowman, Pedro Domingos, Pas-
                                   cal Hitzler, Kai-Uwe Kühnberger, Luis C Lamb, Daniel Lowd, Priscila Machado Vieira Lima,
                                   et al.  Neural-symbolic learning and reasoning: A survey and interpretation. arXiv preprint
                                   arXiv:1711.03902, 2017.
                                Zhiwei Deng, Arash Vahdat, Hexiang Hu, and Greg Mori. Structure inference machines: Recurrent
                                   neural networks for analyzing relations in group activity recognition. In Proceedings of the IEEE
                                   Conference on Computer Vision and Pattern Recognition, pages 4772–4781, 2016.
                                Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
                                   message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
                                                                                        9
