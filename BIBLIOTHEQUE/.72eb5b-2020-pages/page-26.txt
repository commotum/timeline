      227:26                       Shraddha Barke, Hila Peleg, and Nadia Polikarpova
      iteratively adds less likely productions if no solution is found. Although very general, this scheme
      is less efocient than best-first search: it can waste resources searching with an insufocient grammar,
      andhastorevisit the same programs again once the search is restarted with a larger grammar.
        Finally, Metal and Concord, which are based on reinforcement learning (RL), do not perform
      traditional backtracking search at all. Instead, at each branching point, they simply choose a single
      production that has the highest score according to the current RL policy; a sequence of such
      decisions is called a policy rollout. If a rollout does not lead to a solution, the policy is updated
      according to a reward function explained below and a new rollout is performed from scratch.
      LearningProbabilistic Models. Approaches to learning probabilistic models of programs can be
      classified into two categories: pre-training and learning on the fly. In the first category, [Menon
      et al. 2013], EuPhony, and NGDS are trained using a large corpus of human-designed synthesis
      problems and their gold standard solutions (the latter can be provided by a human or synthesized
      usingsize-based enumeration). Such datasets are costly to obtain: because these models are domain-
      specific, a new training corpus has to be designed for each domain. In contrast, DeepCoder learns
      from randomly sampled programs and inputs; it is, however, unclear how effective this technique
      is for domains beyond the highly restricted DSL in the paper. Unlike all these approaches, Probe
      requires no pre-training, and hence can be used on a new domain without any up-front cost; if
      a pre-trained PCFG for the domain is available, however, Probe can also be initialized with this
      model(although we have not explored this avenue in the present work).
        DreamCoder,Metal,andConcordarerelatedtothejust-in-timeapproachof Probeinthe
      sense that they update their probabilistic model on the fly. DreamCoder learns a probabilistic
      model from full solutions to a subset of synthesis problems from a corpus, whereas Probe learns a
      problem-specific model from partial solutions to a single synthesis problem.
        TheRL-basedtools Metal and Concord start with a pre-trained RL policy and then fine-tune
      it for the specific task during synthesis. Note that off-line training is vital for the performance of
      these tools, while Probe is effective even without a pre-trained model. The reward mechanism
      in Metal is similar to Probe: it rewards a policy based on the fraction of input-output examples
      solved by its rollout. Concord instead rewards its policies based on infeasibility information from a
      deductive reasoning engine: productions that expand to infeasible programs have lower probability
      in the next rollout. Although the Concord paper reports that its reward mechanism outperforms
      that of Metal, we conjecture that rewards based on partial solutions are simply not as good a fit
      for RL as they are for bottom-up enumeration: as we discuss in Sec. 5.2, it is crucial to learn from
      shortest partial solutions to avoid irrelevant syntactic features; policy rollouts do not guarantee
      that short solutions are generated first. Finally, Concordâ€™s reward mechanism requires expensive
      solver invocations to check infeasibility of partial programs, while Probeâ€™s reward mechanism
      incurs practically no overhead compared to unguided search.
      LeveragingPartialSolutionstoGuideSynthesis.LaSy[Perelmanetal.2014]andFrAngel[Shi
      et al. 2019] are component-based synthesis techniques that leverage information from partial
      solutions to generate new programs. LaSy explicitly requires the user to arrange input-output
      examples in the order of increasing difoculty, and then synthesizes a sequence of programs, where
      í µí±–th program passes the first í µí±– examples. Each following program is not synthesized from scratch,
      but rather by modifying the previous program; hence intermediate programs serve as Å‚stepping
      stonesÅ¾ for synthesis. Probe puts less burden on the user: it does not require the examples to be
      arranged in a sequence, and instead identifies partial solutions that satisfy any subset of examples.
        Similar to Probe, FrAngel leverages partial solutions that satisfy any subset of the example
      specification. FrAngel generates new programs by randomly combining fragments from partial
      solutions. Probe is similar to FrAngel and LaSy in that it guides the search using syntactic
      Proc. ACMProgram. Lang., Vol. 4, No. OOPSLA, Article 227. Publication date: November 2020.
