                   227:14                                                                                       Shraddha Barke, Hila Peleg, and Nadia Polikarpova
                   Selection schemes. Based on these objectives, we designed three selection schemes, which make
                   different trade-offs and are described below from most to least selective. Note that all selection
                   schemesneedtopreserveinformationaboutpromisingpartialsolutionsbetweendifferentsynthesis-
                   learning cycles, to avoid rewarding the same solution again after synthesis restarts. We evaluate the
                   effectiveness of these schemes in comparison to the baseline (using all partial solutions) in Sec. 6.
                       (1) Largest Subset: This scheme selects a single cheapest program (first enumerated) that
                   satisfies the largest subset of examples encountered so far across all synthesis cycles. Consequently,
                   the number of promising partial solutions it selects is always smaller than the size of E. Among
                   partial solutions in Fig. 12, this scheme picks a single program Ì†µÌ±É0.
                       (2) First Cheapest: This scheme selects a single cheapest program (first enumerated) that
                   satisfies a unique subset of examples. The partial solutions {Ì†µÌ±É0,Ì†µÌ±É2} from Fig. 12 are selected by
                   this scheme. This scheme still rewards a small number of partial solutions, but allows different
                   approaches to be considered.
                       (3) All Cheapest: This scheme selects all cheapest programs (enumerated during a single cycle)
                   that satisfy a unique subset of examples. The partial solutions {Ì†µÌ±É0,Ì†µÌ±É2,Ì†µÌ±É3} are selected by this
                   scheme. Specifically, Ì†µÌ±É2 and Ì†µÌ±É3 satisfy the same subset of examples; both are considered since they
                   have the same cost. This scheme considers more partial solutions than First Cheapest, which
                   refines the ability to reward different approaches.
                   5.3       UpdatingthePCFG
                   Procedure Update uses the set of promising partial solution PSol to compute the new probability
                   for each production rule R ‚àà R using the formula:
                                                        Ì†µÌ±ù  (R)(1‚àíFit)                                                                      |E ‚à©E[Ì†µÌ±É]|
                                           Ì†µÌ±ù(R) =        Ì†µÌ±¢                         where             Fit =              max
                                                                 Ì†µÌ±ç                                               {Ì†µÌ±É ‚ààPSol|R‚ààtr(Ì†µÌ±É)}             |E|
                   where Ì†µÌ±ç denotes the normalization factor, and Fit is the highest proportion of input-output
                   examples that any partial solution derived using this rule satisfies. Recall that Ì†µÌ±ùÌ†µÌ±¢ is the uniform
                   distribution for G. This rule assigns higher probabilities to rules that occur in partial solutions that
                   satisfy many input-output examples.
                   5.4       Restarting the Search
                   EverytimethePCFGisupdatedduringalearningphase,Proberestartsthebottom-upenumeration
                   fromscratch, i.e. empties the bank B (and the evaluation cache E) and resets the current cost Lvl to
                   zero. At a first glance this seems like a waste of computation: why not just resume the enumeration
                   from the current state? The challenge is that any update to the PCFG renders the program bank
                   outdated, and updating the bank to match the new PCFG requires the amount of computation
                   and/or memory that does not pay off in relation to the simpler approach of restarting the search.
                   Let us illustrate these design trade-offs with an example.
                       Consider again the synthesis problem in Fig. 11, and two programs encountered during the
                   first synthesis cycle: the program 0 with cost 5 and the program (indexof arg "+") with cost 15.
                   Note that both programs evaluate to 0 on all three example inputs, i.e. they belong to the same
                   observational equivalence class [0,0,0]; hence the latter program is discarded by observational
                   equivalence reduction, while the former, discovered first, is chosen as the representative of its
                   equivalence class and appears in the current bank B.
                       NowassumethatduringthesubsequentlearningphasethePCFGchangedinsuchawaythat
                   the new costs of these two programs are cost(0) = 10 and cost((indexof arg "+")) = 7. Let us
                   examine different options for the subsequent synthesis cycle.
                   Proc. ACMProgram. Lang., Vol. 4, No. OOPSLA, Article 227. Publication date: November 2020.
