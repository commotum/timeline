              Just-in-Time Learning for Bottom-Up Enumerative Synthesis                                                  227:25
              et al. 2018], or bottom-up, by gradually building up a program tree from the leaves towards the
              root [Albarghouthi et al. 2013; Alur et al. 2013; Peleg and Polikarpova 2020; Udupa et al. 2013].
              These two strategies have complementary strengths and weaknesses, similar to backward chaining
              andforwardchaining in proof search.
                 Oneimportant advantage of bottom-up enumeration for inductive synthesis is the ability to
              prune the search space using observational equivalence (OE), i.e. discard a program that behaves
              equivalently to an already enumerated program on the set of inputs from the semantic specification.
              OEwasfirst proposed in [Albarghouthi et al. 2013; Udupa et al. 2013] and since then has been
              successfully used in many bottom-up synthesizers [Alur et al. 2018; Peleg and Polikarpova 2020;
              Wanget al. 2017a], including Probe. Top-down enumeration techniques cannot fully leverage
              OE, because incomplete programs they generate cannot be evaluated on the inputs. Instead, these
              synthesizersprunethespacebasedonothersyntacticandsemanticnotionsofprogramequivalence:
              for example, [Frankle et al. 2016; Gvero et al. 2013; Osera and Zdancewic 2015] only produce
              programs in a normal form; [Feser et al. 2015; Kneuss et al. 2013; Smith and Albarghouthi 2019]
              perform symmetryreduction based on equational theories (either built-in or user-provided); finally,
              EuPhony [Lee et al. 2018] employs a weaker version of OE for incomplete programs, which
              compares their complete parts observationally and their incomplete parts syntactically.
              Guiding Synthesis with Probabilistic Models. Recent years have seen proliferation of proba-
              bilistic models of programs [Allamanis et al. 2018], which can be used, in particular, to guide
              program synthesis. The general idea is to prioritize the exploration of grammar productions based
              on scores assigned by a probabilistic model; the specific technique, however, varies depending
              on(1) the context taken into consideration by the model when assigning scores, and (2) how the
              scores are taken into account during search. Like Probe, [Balog et al. 2016; Koukoutos et al. 2017;
              Menonetal.2013] use a PCFG, which assigns scores to productions independently of their context
              within the synthesized program; unlike Probe, however, these techniques select the PCFG once, at
              the beginning of the synthesis process, based on a learned mapping from semantic specifications
              to scores. On the opposite end of the spectrum, Metal [Si et al. 2019] and Concord [Chen et al.
              2020] use graph-based and sequence-based models, respectively, to condition the scores on the
              entire partial program that is being extended. In between these extremes, EuPhony uses a learned
              context in the form of a probabilistic higher-order grammar [Bielik et al. 2016], while NGDS [Kalyan
              et al. 2018] conditions the scores on the local specification propagated top-down by the deductive
              synthesizer. The more context a model takes into account, the more precise the guidance it provides,
              but also the harder it is to learn. Another consideration is that neural models, used in [Chen et al.
              2020; Kalyan et al. 2018; Si et al. 2019] incur a larger overhead than simple grammar-based models,
              used in Probe and [Balog et al. 2016; Koukoutos et al. 2017; Lee et al. 2018; Menon et al. 2013],
              since they have to invoke a neural network at each branching point during search.
                 As for using the scores to guide search, most existing techniques are specific to top-down
              enumeration. They include prioritized depth-first search [Balog et al. 2016], branch and bound
              search [Kalyan et al. 2018], and variants of best-first search [Koukoutos et al. 2017; Lee et al. 2018;
              Menon et al. 2013]. In contrast to these approaches, Probe uses the scores to guide bottom-up
              enumerationwithobservationalequivalencereduction.Probeâ€™senumerationisessentiallyabottom-
              upversionofbest-firstsearch,anditempiricallyperformsbetterthanthetop-downbest-firstsearch
              in EuPhony; one limitation, however, is that our algorithm is specific to PCFGs and extending it to
              models that require more context is not straightforward.
                 DeepCoder[Balogetal.2016]alsoproposesaschemetheycallsortandadd,whichisnotspecific
              to top-downenumerationandcanbeusedinconjunctionwithanysynthesisalgorithm:thisscheme
              runs synthesis with a reduced grammar, containing only productions with highest scores, and
                                      Proc. ACMProgram. Lang., Vol. 4, No. OOPSLA, Article 227. Publication date: November 2020.
