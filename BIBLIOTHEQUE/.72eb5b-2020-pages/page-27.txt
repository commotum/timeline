              Just-in-Time Learning for Bottom-Up Enumerative Synthesis                                                  227:27
              information learned from partial solutions, but we achieve that by updating the weights of useful
              productions in a probabilistic grammar and using it to guide bottom-up enumerative search.
                 Ourprevious work, Bester [Peleg and Polikarpova 2020] proposes a technique to accumulate
              multiple partial solutions during bottom-up enumerative synthesis with minimum overhead. Probe
              is a natural extension of Bester: it leverages these accumulated partial solutions to guide search.
                 During top-down enumeration, [Koukoutos et al. 2017] employs an optimization strategy where
              the cost of an incomplete (partial) program is lowered if it satisfies some of the examples. This
              optimization encourages the search to complete a partial program that looks promising, but unlike
              Probe, offers no guidance on which are the likely productions to complete it with. Moreover, this
              optimization only works on partial programs that can be evaluated on some examples. Probe’s
              bottom-up search generates complete programs that can always be evaluated on all examples.
              8 CONCLUSIONANDFUTUREWORK
              We have presented a new program synthesis algorithm we dub guided bottom-up search with
              just-in-time-learning. This algorithm combines the pruning power of observational equivalence
              with guidance from probabilistic models. Moreover, our just-in-time learning is able to bootstrap a
              probabilistic model during synthesis by leveraging partial solutions, and hence does not require
              training data, which can be hard to obtain.
                 Wehaveimplementedthisalgorithm in a tool called Probe that works with the popular SyGuS
              input format. We evaluated Probe on 140 synthesis benchmarks from three different domains. Our
              evaluation demonstrates that Probe is more efocient than unguided enumerative search and a
              state-of-the-art guided synthesizer EuPhony, and while Probe is less efocient than CVC4, our
              solutions are of higher quality.
                 In future work, we are interested in instantiating Probe in new application domains. We expect
              just-in-time learning to work for programs over structured data structures, e.g. lists and tree
              transformations. Just-in-time learning also requires that example specifications cover a range from
              simple to more complex, so that Probe can discover short partial solutions and learn from them.
              Luckily, users seem to naturally provide examples that satisfy this property, as indicated by SyGuS
              benchmarks whose specifications are taken from StackOverflow. Generalizing these observations
              is an exciting direction for future work. Another interesting direction is to consider Probe in the
              context of program repair, where similarity to the original faulty program can serve as a prior to
              initialize the PCFG.
              ACKNOWLEDGMENTS
              Theauthors would like to thank the anonymous reviewers for their feedback on the draft of this
              paper. This work was supported by the National Science Foundation under Grants No. 1955457,
              1911149, and 1943623.
              REFERENCES
              2018. Euphony Benchmark Suite. https://github.com/wslee/euphony/tree/master/benchmarks
              AwsAlbarghouthi, Sumit Gulwani, and Zachary Kincaid. 2013. Recursive program synthesis. In International Conference on
                 Computer Aided Verification. Springer, 934ś950.
              Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for big code
                 andnaturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 1ś37.
              EthemAlpaydin. 2014. Introduction to Machine Learning (3 ed.). MIT Press, Cambridge, MA.
              Rajeev Alur, Rastislav Bodík, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh,
                Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. 2013. Syntax-guided synthesis. In Formal Methods in
                 Computer-Aided Design, FMCAD 2013, Portland, OR, USA, October 20-23, 2013. 1ś8. http://ieeexplore.ieee.org/document/
                 6679385/
                                      Proc. ACMProgram. Lang., Vol. 4, No. OOPSLA, Article 227. Publication date: November 2020.
