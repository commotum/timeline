                      tional autoencoder [32]. [33] propose VGNAE that applies L2-normalization
                      before propagation to alleviate the issue that an isolated node embed got
                      close to zero by VGAE [31]. There are recent works of TokenGT [34] that
                      applies the transformer to the graph-structured dataset. In addition, TAPE
                      [35] that utilizes the vast knowledge of LLM to enrich text information and
                      use it as a GNN feature, and GPT4Graph [36] that applies LLM itself to
                      various graph-related tasks.
                      3. Preliminary
                      3.1. Perceiver
                         Jaegle et al. [5] proposes the Perceiver, a general perception module with
                      iterative attention for handling the diverse and high-dimensional multimodal
                      inputs. The Perceiver has two components of arrays, input arrays x, and
                      latent arrays z. For given data instances, the Perceiver transforms the given
                                                               M×C
                      data instances as the inputs arrays x ∈ R     where M and C denote the
                      dataset properties such as the image size and the number of channels. The
                                         N×D
                      latent arrays z ∈ R     are learnable latent representations, where N and
                      Dare the number of latent and the latent dimension, respectively. The Per-
                      ceiver adopts the two types of attention, cross-attention and self-attention,
                      andbothattentionsarequery-key-valueattention, similar to the Transformer
                      [1]. First, the cross-attention computes the attention between the latent ar-
                      rays and the input arrays. The latent arrays are queries, and they attend
                      to the important information from the key, the input arrays. The relevant
                      input features for the given tasks are incorporated into the latent array with
                      cross-attention. Second, the Perceiver adopts the self-attention for the latent
                                                          7
