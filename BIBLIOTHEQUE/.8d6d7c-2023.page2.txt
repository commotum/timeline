                       fuses features from both modalities by efficiently encoding object instances and semantic classes as
                       concise queries. Moreover, we propose a learned tracking framework that maintains a history of
                       previously observed object tracks, allowing us to overcome occlusions without hand-crafted heuris-
                       tics. This gives us an elegant way to reason in space and time about all the tasks that constitute 4D
                       panoptic segmentation. We demonstrate the effectiveness of 4D-Former on both nuScenes [12] and
                       SemanticKITTI [13] benchmarks and show that we significantly outperform the state-of-the-art.
                       2   Related Work
                       3D Panoptic Segmentation:   This task combines semantic and instance segmentation, but does
                       not require temporally consistent object tracks. Current approaches often utilize a multi-branch
                       architecture to independently predict semantic and instance labels. A backbone network is used
                       to extract features from the LiDAR point cloud with various representations e.g. points [14], vox-
                       els [1, 3], 2D range views [15, 16], or birds-eye views [17]. Subsequently, the network branches
                       into two paths to generate semantic and instance segmentation predictions. Typically, instance pre-
                       dictions are obtained through deterministic [18, 19, 20] or learnable clustering [6, 21], proposal
                       generation [22], or graph-based methods [23, 24]. These methods are not optimized end-to-end.
                       Several recent work [25, 26, 27] extends the image-level approach from Cheng et al. [28] to per-
                       form panoptic segmentation in the LiDAR domain in an end-to-end fashion. We adopt a similar
                       approach to predict semantic and instance masks from learned queries, however, our queries attend
                       to multi-modal features whereas the former utilizes only LiDAR inputs.
                       LiDAR Tracking: This task involves predicting temporally consistent bounding-boxes for the
                       objects in the input LiDAR sequence. We classify existing approaches into two main groups:
                       tracking-by-detection and end-to-end methods. The tracking-by-detection paradigm [7, 8, 29] has
                       been widely researched, and generally consists of a detection framework followed by a tracking
                       mechanism. Since LiDARpointcloudstypically lack appearance information but offer more spatial
                       and geometric cues, existing approaches usually rely on motion cues for tracking (e.g. Kalman Fil-
                       ters [30], Hungarian matching [31] or Greedy Algorithm [8] for association). Recently, end-to-end
                       frameworks [32] have also emerged where a single network performs per-frame detection and tem-
                       poral association. In contrast to these, 4D-Former utilizes both LiDAR and image modalities, and
                       performs point-level instance tracking and semantic segmentation with a single unified framework.
                       4D Panoptic Segmentation:   This is the task we tackle in our work, and it involves extending
                       3D panoptic segmentation to include temporally consistent instance segmentation throughout the
                       input sequence. Most existing methods [9, 11, 33] employ a sliding-window approach which tracks
                       instances within a short clip of upto 5 frames. 4D-PLS [9] models object tracklets as Gaussian dis-
                       tributions and segments them by clustering per-point spatio-temporal embeddings over the 4D input
                       volume. 4D-StOP [11] proposes a center-based voting technique to generate track proposals which
                       are then aggregated using learned geometric features. These methods associate instances across
                       clips using mask IoU in overlapping frames. CA-Net [10], on the other hand, learns contrastive em-
                       beddings for objects to associate per-frame predictions over time. Recently, concurrent work from
                       Zhu et al. [34] develops rotation-equivariant networks which provide more robust feature learning
                       for 4D panoptic segmentation. Different to these, 4D-Former utilizes multimodal inputs, and adopts
                       a transformer-based architecture which models semantic classes and objects as concise queries.
                       LiDAR and Camera Fusion: Multimodal approaches have recently become popular for object
                       detection and semantic segmentation. Existing methods can be grouped into two categories: (1)
                       point-level fusion methods, which typically involve appending camera features to each LiDAR point
                       [35, 36, 37] or fusing the two modalities at the feature level [38, 39, 40]. (2) Proposal-level fusion,
                       whereobjectdetection approaches [41, 42] employ transformer-based architectures which represent
                       object as queries and then fuse them with camera features. Similarly, Li et al. [43] perform seman-
                       tic segmentation by modeling semantic classes as queries which attend to scene features from both
                       modalities. 4D-Former, on the other hand, tackles 4D panoptic segmentation whereas the aforemen-
                       tioned methods perform single-frame semantic segmentation or object detection.
                                                                 2
