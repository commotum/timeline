             note that the attention operation for each head is deﬁned as       3Dconvolutional ﬁlters from 2D ﬁlters for video classiﬁca-
                                                                              tion is to “inﬂate” them by replicating the ﬁlters along the
                                                           >
                                                       QK                       temporal dimension and averaging them [8, 21] as
                  Attention(Q,K,V) = Softmax            √d      V.     (7)
                                                           k                                    1
                                                                                          E= [E         , . . . , E  , . . . , E ].      (8)
             In self-attention, the queries Q = XW , keys K = XW ,                                  image       image       image
                                                     q                  k                       t
             andvaluesV = XWv arelinearprojectionsoftheinputX
             with X,Q,K,V ∈ RN×d. Note that in the unfactorised                 We consider an additional strategy, which we denote as
             case (Model 1), the spatial and temporal dimensions are            “central frame initialisation”, where E is initialised with ze-
             merged as N = n ·n ·n .                                            roes along all temporal positions, except at the centre btc,
                                t   h    w                                                                                              2
                The main idea here is to modify the keys and values for                         E=[0,...,E          , . . . , 0].        (9)
             eachquerytoonlyattendovertokensfromthesamespatial-                                                image
             and temporal index by constructing K ,V         ∈ Rnh·nw×d
                                                      s   s                     Therefore, the 3D convolutional ﬁlter effectively behaves
             and K ,V ∈ Rnt×d, namely the keys and values corre-
                    t    t                                                      like “Uniform frame sampling” (Sec. 3.2) at initialisation,
             sponding to these dimensions. Then, for half of the atten-         whilealsoenablingthemodeltolearntoaggregatetemporal
             tion heads, we attend over tokens from the spatial dimen-          information from multiple frames as training progresses.
             sion by computing Ys = Attention(Q,Ks,Vs), and for
             the rest we attend over the temporal dimension by comput-          Transformer weights for Model 3 The transformer
             ing Yt = Attention(Q,Kt,Vt). Given that we are only                block in Model 3 (Fig. 5) differs from the pretrained ViT
             changing the attention neighbourhood for each query, the           model [17], in that it contains two multi-headed self atten-
             attention operation has the same dimension as in the unfac-        tion (MSA) modules. In this case, we initialise the spatial
             torised case, namely Y ,Y ∈ RN×d. We then combine
                                      s   t                                     MSAmodulefromthepretrained module, and initialise all
             the outputs of multiple heads by concatenating them and            weights of the temporal MSA with zeroes, such that Eq. 5
             using a linear projection [67], Y = Concat(Y ,Y )W .
                                                             s    t    O        behaves as a residual connection [26] at initialisation.
             3.4. Initialisation by leveraging pretrained models
                ViT [17] has been shown to only be effective when               4. Empirical evaluation
             trained on large-scale datasets, as transformers lack some of         Weﬁrstpresentourexperimentalsetupandimplementa-
             the inductive biases of convolutional networks [17]. How-          tion details in Sec. 4.1, before ablating various components
             ever, even the largest video datasets such as Kinetics [34],       ofourmodelinSec.4.2. Wethenpresentstate-of-the-artre-
             have several orders of magnitude less labelled examples            sults on ﬁve datasets in Sec. 4.3. Our code will be released
             whencomparedtotheirimagecounterparts[15,38,57]. As                         1
                                                                                publicly .
             aresult, training large models from scratch to high accuracy
             is extremely challenging. To sidestep this issue, and enable       4.1. Experimental Setup
             more efﬁcient training we initialise our video models from         Networkarchitectureandtraining          Ourbackbonearchi-
             pretrained image models. However, this raises several prac-        tecture follows that of ViT [17] and BERT [16]. We con-
             tical questions, speciﬁcally on how to initialise parameters       sider ViT-Base (ViT-B, L=12, NH=12, d=768), ViT-Large
             not present or incompatible with image models. We now              (ViT-L, L=24, N =16, d=1024), and ViT-Huge (ViT-H,
             discuss several effective strategies to initialise these large-                      H
                                                                                L=32, N =16, d=1280), where L is the number of trans-
             scale video classiﬁcation models.                                           H
                                                                                former layers, each with a self-attention block of N   heads
                                                                                                                                    H
             Positional embeddings       A positional embedding p is            and hidden dimension d. We also apply the same naming
             added to each input token (Eq. 1). However, our video              scheme to our models (e.g., ViViT-B/16x2 denotes a ViT-
             models have nt times more tokens than the pretrained im-           Basebackbonewithatubeletsizeofh×w×t = 16×16×2).
             age model. As a result, we initialise the positional embed-        In all experiments, the tubelet height and width are equal.
                                                              n ·n ×d           Notethatsmaller tubelet sizes correspond to more tokens at
             dings by “repeating” them temporally from R w h            to      the input, and thus more computation.
               n ·n ·n ×d
             R t h w      .  Therefore, at initialisation, all tokens with         We train our models using synchronous SGD and mo-
             the same spatial index have the same embedding which is            mentum, a cosine learning rate schedule and TPU-v3 ac-
             then ﬁne-tuned.                                                    celerators.  We initialise our models from a ViT image
             Embeddingweights, E Whenusingthe “tubelet embed-                   model trained either on ImageNet-21K [15] (unless other-
             ding” tokenisation method (Sec. 3.2), the embedding ﬁlter          wise speciﬁed) or the larger JFT [57] dataset. Exact experi-
             E is a 3D tensor, compared to the 2D tensor in the pre-            mental hyperparameters are detailed in the supplementary.
             trained model, E      . A common approach for initialising            1https://github.com/google-research/scenic
                               image
                                                                             6840
