=== Page 1 ===
Machin
e
 Learning
,
 8
,
 279-29
2
 (1992
)
©
 199
2
 Kluwe
r
 Academi
c
 Publishers
,
 Boston
.
 Manufacture
d
 i
n
 Th
e
 Netherlands
.
Technical
 Note
Q,-Learnin
g
CHRISTOPHE
R
 J.C.H
.
 WATKIN
S
25b
 Framfield
 Road,
 Highbury,
 London
 N5
 IUU,
 England
PETE
R
 DAYA
N
Centre
 for
 Cognitive
 Science,
 University
 of
 Edinburgh,
 2
 Buccleuch
 Place,
 Edinburgh
 EH8
 9EH,
 Scotland
Abstract
.
 Q-learnin
g
 (Watkins
,
 1989
)
 i
s
 a
 simpl
e
 wa
y
 fo
r
 agent
s
 t
o
 lear
n
 ho
w
 t
o
 ac
t
 optimall
y
 i

n
 controlle
d
 Markovia
n
domains
.
 I
t
 amount
s
 t
o
 a
n
 incrementa
l
 metho
d
 fo
r
 dynami
c
 programmin
g
 whic
h
 impose
s
 limite
d
 computationa
l
demands
.
 I
t
 work
s
 b
y
 successivel
y
 improvin
g
 it
s
 evaluation
s
 o
f
 th
e
 qualit
y
 o
f
 particula
r
 action
s
 a
t
 particula
r
 states
.
Thi
s
 pape
r
 present
s
 an
d
 prove
s
 i
n
 detai
l
 a
 convergenc
e
 theore
m
 fo
r
 Q,-learnin
g
 base
d
 o
n
 tha
t
 outline
d
 i
n
 Watkin
s

(1989)
.
 W
e
 sho
w
 tha
t
 Q-learnin
g
 converge
s
 t
o
 th
e
 optimu
m
 action-value
s
 wit
h
 probabilit
y
 1
 s
o
 lon
g
 a
s
 al
l
 action
s
ar
e
 repeatedl
y
 sample
d
 i
n
 al
l
 state
s
 an
d
 th
e
 action-value
s
 ar
e
 represente
d
 discretely
.
 W
e
 als
o
 sketc
h
 extension
s
t
o
 th
e
 case
s
 o
f
 non-discounted
,
 bu
t
 absorbing
,
 Marko
v
 environments
,
 an
d
 wher
e
 man
y
 Q
 value
s
 ca
n
 b
e
 change
d
eac
h
 iteration
,
 rathe

r
 tha
n
 jus
t
 one
.
Keywords
.
 Q
-learning
,
 reinforcemen
t
 learning
,
 tempora
l
 differences
,
 asynchronou
s
 dynami
c
 programmin
g
1
.
 Introductio
n
Q-learnin
g
 (Watkins
,
 1989
)
 i
s
 a
 for
m
 o
f
 model-fre
e
 reinforcemen
t
 learning
.
 I
t
 ca
n
 als
o
 b
e
viewe
d
 a
s
 a
 metho
d
 o
f
 asynchronou
s
 dynami
c
 programmin
g
 (DP)
.
 I
t
 provide
s
 agent
s
 wit
h
th
e
 capabilit
y
 o
f
 learnin
g
 t
o
 ac
t
 optimall
y
 i
n
 Markovia
n
 domain
s
 b
y
 experiencin
g

 th
e
 con
-
sequence
s
 o
f
 actions
,
 withou
t
 requirin
g
 the
m
 t
o
 buil
d
 map
s
 o
f
 th
e
 domains
.
Learnin
g
 proceed
s
 similarl
y
 t
o
 Sutton'
s
 (1984
;
 1988
)
 metho
d
 o
f
 tempora
l
 difference
s
(TD)
:
 a
n
 agen
t
 trie
s
 a
n
 actio
n
 a
t
 a
 particula
r
 state
,
 an
d
 evaluate
s
 it
s
 consequence
s
 i
n
 term
s
o
f
 th
e
 immediat
e
 rewar
d
 o
r
 penalt
y
 i
t
 receive
s
 an
d
 it
s
 estimat
e

 o
f
 th
e
 valu
e
 o
f
 th
e
 stat
e
t
o
 whic
h
 i
t
 i
s
 taken
.
 B
y
 tryin
g
 al
l
 action
s
 i
n
 al
l
 state
s
 repeatedly
,
 i
t
 learn
s
 whic
h
 ar
e
 bes
t
overall
,
 judge
d
 b
y
 long-ter
m
 discounte
d
 reward
.
 Q-learnin
g
 i
s
 a
 primitiv
e
 (Watkins
,
 1989
)
for
m
 o
f
 learning
,
 but
,
 a
s
 such
,
 i
t
 ca
n
 operat
e
 a
s
 th
e
 basi
s
 o
f
 fa
r
 mor
e
 sophisticate
d
 devices
.
Example
s

 o
f
 it
s
 us
e
 includ
e
 Bart
o
 an
d
 Sing
h
 (1990)
,
 Sutto
n
 (1990)
,
 Chapma
n
 an
d
 Kael
-
blin
g
 (1991)
,
 Mahadeva
n
 an
d
 Connel
l
 (1991)
,
 an
d
 Li
n
 (1992)
,
 wh
o
 develope
d
 i
t
 inde
-
pendently
.
 Ther
e
 ar
e
 als
o
 variou
s
 industria
l
 applications
.
Thi
s
 pape
r
 present
s
 th
e
 proo
f
 outline
d
 b
y
 Watkin
s
 (1989
)
 tha
t
 Q-learnin
g
 converges
.
 Sec
-
tio
n
 2
 describe
s
 th
e
 problem
,
 th
e
 method
,

 an
d
 th
e
 notation
,
 sectio
n
 3
 give
s
 a
n
 overvie
w
o
f
 th
e
 proof
,
 an
d
 sectio
n
 4
 discusse
s
 tw
o
 extensions
.
 Forma
l
 detail
s
 ar
e
 lef
t
 a
s
 fa
r
 a
s
 pos
-
sibl
e
 t
o
 th
e
 appendix
.
 Watkin
s
 (1989
)
 shoul
d
 b
e
 consulte
d
 fo
r
 a
 mor
e
 extensiv
e
 discussio
n
o
f
 Q-learning
,
 includin
g
 it
s
 relationshi
p
 wit
h
 dynami
c
 programmin
g
 an
d
 TD
.
 Se
e
 als
o
 Werbo
s
(1977)
.
55


=== Page 2 ===
28
0
C
.
 WATKIN
S
 AN
D
 P
.
 DAYA
N
2
.
 Th
e
 tas
k
 fo
r
 Q-learnin
g
Conside
r
 a
 computationa
l
 agen
t
 movin
g
 aroun
d
 som
e
 discrete
,
 finit
e
 world
,
 choosin
g
 on
e
fro
m
 a
 finit
e
 collectio
n
 o
f
 action
s
 a
t
 ever
y
 tim
e
 step
.
 Th
e
 worl
d
 constitute
s
 a
 controlle
d
Marko
v
 proces
s
 wit
h
 th
e
 agen
t
 a
s
 a
 controller
.
 A
t
 ste
p
 n
,
 th
e
 agen
t
 i
s
 equippe
d
 t
o
 registe
r
th
e
 stat
e
 x
n
 (
€
 X
)
 o
f
 th
e
 world
,
 a
n
 ca
n
 choos
e
 it
s
 actio
n
 a
n
 (
€
 2)
1
 accordingly
.
 Th
e
 agen
t
receive
s
 a
 probabilisti
c
 rewar
d
 r
n
,
 whos
e
 mea
n
 valu
e
 (
R
Xn
 (a
n
)
 depend
s
 onl
y
 o
n
 th
e
 stat
e
an
d
 action
,
 an
d
 th
e
 stat
e
 o
f
 th
e
 worl
d
 change
s
 probabilisticall
y
 t
o
 y
n
 accordin
g
 t
o
 th
e
 law
:
Th
e
 tas
k
 facin
g
 th
e
 agen
t
 i
s
 tha
t
 o
f
 determinin
g
 a
n
 optima
l
 policy
,
 on
e
 tha
t
 maximize
s
 tota
l
discounte
d
 expecte
d
 reward
.
 B
y
 discounte
d
 reward
,
 w
e
 mea
n
 tha
t
 reward
s
 receive
d
 s
 step
s
henc
e
 ar
e
 wort
h
 les
s
 tha
n
 reward
s
 receive
d
 now
,
 b
y
 a
 facto
r
 o
f
 y
s
 (
0
 <
 y
 <
 1)
.
 Unde
r
a
 polic
y
 T
,
 th
e
 valu
e
 o
f
 stat
e
 x
 i
s
becaus
e
 th
e
 agen
t
 expect
s
 t
o
 receiv
e
 (R
x
(r(x))
 immediatel
y
 fo
r
 performin
g
 th
e
 actio
n
 I
recommends
,
 an
d
 the
n
 move
s
 t
o
 a
 stat
e
 tha
t
 i
s
 'worth
'
 V*(y)
 t
o
 it
,
 wit
h
 probabilit
y
P
x
y
 [>(*)]
.
 Th
e
 theor
y
 o
f
 D
P
 (Bellma
n
 &
 Dreyfus
,
 1962
;
 Ross
,
 1983
)
 assure
s
 u
s
 tha
t
 ther
e
i
s
 a
t
 leas
t
 on
e
 optima
l
 stationar
y
 polic
y
 T
*
 whic
h
 i
s
 suc
h
 tha
t
i
s
 a
s
 wel
l
 a
s
 a
n
 agen
t
 ca
n
 d
o
 fro
m
 stat
e
 x
.
 Althoug
h
 thi
s
 migh
t
 loo
k
 circular
,
 i
t
 i
s
 actuall
y
wel
l
 defined
,
 an
d
 D
P
 provide
s
 a
 numbe
r
 o
f
 method
s
 fo
r
 calculatin
g
 V
*
 an
d
 on
e
 T*
(
 assum
-
in
g
 tha
t
 (R
x
(a)
 an
d
 P
xy
[a
]
 ar
e
 known
.
 Th
e
 tas
k
 facin
g
 a
 Q
,
 learne
r
 i
s
 tha
t
 o
f
 determinin
g
a
 f
*
 withou
t
 initiall
y
 knowin
g
 thes
e
 values
.
 Ther
e
 ar
e
 traditiona
l
 method
s
 (e.g.
,
 Sato
,
 Ab
e
&
 Takeda
,
 1988
)
 fo
r
 learnin
g
 (R
x
(a)
 an
d
 P
xy
[a
]
 whil
e
 concurrentl
y
 performin
g
 DP
,
 bu
t
an
y
 assumptio
n
 o
f
 certaint
y
 equivalence
,
 i.e.
,
 calculatin
g
 action
s
 a
s
 i
f
 th
e
 curren
t
 mode
l
wer
e
 accurate
,
 cost
s
 dearl
y
 i
n
 th
e
 earl
y
 stage
s
 o
f
 learnin
g
 (Bart
o
 &
 Singh
,
 1990)
.
 Watkin
s
(1989
)
 classe
s
 ^-learnin
g
 a
s
 incrementa
l
 dynami
c
 programming
,
 becaus
e
 o
f
 th
e
 step-by
-
ste
p
 manne
r
 i
n
 whic
h
 i
t
 determine
s
 th
e
 optima
l
 policy
.
Fo
r
 a
 polic
y
 T
,
 defin
e
 Q
,
 value
s
 (o
r
 action-values
)
 as
:
I
n
 othe
r
 words
,
 th
e
 3
,
 valu
e
 i
s
 th
e
 expecte
d
 discounte
d
 rewar
d
 fo
r
 executin
g
 actio
n
 a
 a
t
stat
e
 x
 an
d
 followin
g
 polic
y
 T
 thereafter
.
 Th
e
 objec
t
 i
n
 Q-learnin
g
 i
s
 t
o
 estimat
e
 th
e
 5
,
5
6


=== Page 3 ===
^-LEARNIN
G
28
1
value
s
 fo
r
 a
n
 optima
l
 policy
.
 Fo
r
 convenience
,
 defin
e
 thes
e
 a
s
 Q*(x,
 a
)
 =
 Q**(x,
 a)
,
 Vx
,
 a
.
I
t
 i
s
 straightforwar
d
 t
o
 sho
w
 tha
t
 V*(x)
 =
 max
a
 Q*(x,
 a
)
 an
d
 tha
t
 i
f
 a
*
 i
s
 a
n
 actio
n
 a
t
 whic
h
th
e
 maximu
m
 i
s
 attained
,
 the
n
 a
n
 optima
l
 polic
y
 ca
n
 b
e
 forme
d
 a
s
 r*(x)
 =
 a*
.
 Herei
n
lie
s
 th
e
 utilit
y
 o
f
 th
e
 Q
,
 values—i
f
 a
n
 agen
t
 ca
n
 lear
n
 them
,
 i
t
 ca
n
 easil
y
 decid
e
 wha
t
 i
t
i
s
 optima
l
 t
o
 do
.
 Althoug
h
 ther
e
 ma
y
 b
e
 mor
e
 tha
n
 on
e
 optima
l
 polic
y
 o
r
 a*
,
 th
e
 5
*
 value
s
ar
e
 unique
.
I
n
 Q-learning
,
 th
e
 agent'
s
 experienc
e
 consist
s
 o
f
 a
 sequenc
e
 o
f
 distinc
t
 stage
s
 o
r
 episodes.
I
n
 th
e
 n
t
h
 episode
,
 th
e
 agent
:
•
 observe
s
 it
s
 curren
t
 stat
e
 x
n
,
•
 select
s
 an
d
 perform
s
 a
n
 actio
n
 a
n
,
•
 observe
s
 th
e
 subsequen
t
 stat
e
 y
n
,
•
 receive
s
 a
n
 immediat
e
 payof
f
 r
n
,
 an
d
•
 adjust
s
 it
s
 Q
n-1
 value
s
 usin
g
 a
 learnin
g
 facto
r
 a
n
,
 accordin
g
 to
:
wher
e
i
s
 th
e
 bes
t
 th
e
 agen
t
 think
s
 i
t
 ca
n
 d
o
 fro
m
 stat
e
 y
.
 O
f
 course
,
 i
n
 th
e
 earl
y
 stage
s
 o
f
 learn
-
ing
,
 th
e
 Q
,
 value
s
 ma
y
 no
t
 accuratel
y
 reflec
t
 th
e
 polic
y
 the
y
 implicitl
y
 defin
e
 (th
e
 maxi
-
mizin
g
 action
s
 i
n
 equatio
n
 2)
.
 Th
e
 initia
l
 Q
,
 values
,
 Q,
0
(X
,
 a)
,
 fo
r
 al
l
 state
s
 an
d
 action
s
ar
e
 assume
d
 given
.
Not
e
 tha
t
 thi
s
 descriptio
n
 assume
s
 a
 look-u
p
 tabl
e
 representatio
n
 fo
r
 th
e
 Q,
n
(x,
 a)
.
Watkin
s
 (1989
)
 show
s
 tha
t
 ^-learnin
g
 ma
y
 no
t
 converg
e
 correctl
y
 fo
r
 othe
r
 representations
.
Th
e
 mos
t
 importan
t
 conditio
n
 implici
t
 i
n
 th
e
 convergenc
e
 theore
m
 give
n
 belo
w
 i
s
 tha
t
th
e
 sequenc
e
 o
f
 episode
s
 tha
t
 form
s
 th
e
 basi
s
 o
f
 learnin
g
 mus
t
 includ
e
 a
n
 infinit
e
 numbe
r
o
f
 episode
s
 fo
r
 eac
h
 startin
g
 stat
e
 an
d
 action
.
 Thi
s
 ma
y
 b
e
 considere
d
 a
 stron
g
 conditio
n
o
n
 th
e
 wa
y
 state
s
 an
d
 action
s
 ar
e
 selected—however
,
 unde
r
 th
e
 stochasti
c
 condition
s
 o
f
th
e
 theorem
,
 n
o
 metho
d
 coul
d
 b
e
 guarantee
d
 t
o
 fin
d
 a
n
 optima
l
 polic
y
 unde
r
 weake
r
 con
-
ditions
.
 Note
,
 however
,
 tha
t
 th
e
 episode
s
 nee
d
 no
t
 for
m
 a
 continuou
s
 sequence—tha
t
 i
s
th
e
 y
 o
f
 on
e
 episod
e
 nee
d
 no
t
 b
e
 th
e
 x
 o
f
 th
e
 nex
t
 episode
.
Th
e
 followin
g
 theore
m
 define
s
 a
 se
t
 o
f
 condition
s
 unde
r
 whic
h
 Q
n
(x,
 a
)
 -
 Q*(x,
 a
)
a
s
 n
 -
 o
.
 Defin
e
 n
i
(x
,
 a
)
 a
s
 th
e
 inde
x
 o
f
 th
e
 i
t
h
 tim
e
 tha
t
 actio
n
 a
 i
s
 trie
d
 i
n
 stat
e
 x
.
5
7


=== Page 4 ===
28
2
C
.
 WATKIN
S
 AN
D
 P
.
 DAYAN
Theore
m
Give
n
 bounde
d
 reward
s
 \r
n
\
 <
 (R
,
 learnin
g
 rate
s
 0
 <
 a
n
 <
 1
,
 an
d
the
n
 Q
n
(x,
 a
)
 -
 G*(x,
 a
)
 a
s
 n
 -
 o
,
 Vx
,
 a
,
 wit
h
 probabilit
y
 1
.
3
.
 Th
e
 convergenc
e
 proo
f
Th
e
 ke
y
 t
o
 th
e
 convergenc
e
 proo
f
 i
s
 a
n
 artificia
l
 controlle
d
 Marko
v
 proces
s
 calle
d
 th
e
 action-
replay
 process
 ARP
,
 whic
h
 i
s
 constructe
d
 fro
m
 th
e
 episod
e
 sequenc
e
 an
d
 th
e
 learnin
g
 rat
e
sequenc
e
 a
n
.
A
 forma
l
 descriptio
n
 of
 the
 ARP
 is
 give
n
 in
 the
 appendix
,
 but
 the
 easies
t
 way
 to
 thin
k
o
f
 i
t
 i
s
 i
n
 term
s
 o
f
 a
 car
d
 game
.
 Imagin
e
 eac
h
 episod
e
 (x
t
,
 a
t
,
 y
t
,
 r
t
,
 a
t
,
)
 writte
n
 o
n
 a
 card
.
Al
l
 th
e
 card
s
 togethe
r
 for
m
 a
n
 infinit
e
 deck
,
 wit
h
 th
e
 firs
t
 episode-car
d
 next-to-botto
m
an
d
 stretchin
g
 infinitel
y
 upwards
,
 i
n
 order
.
 Th
e
 botto
m
 car
d
 (numbere
d
 0
)
 ha
s
 writte
n
 o
n
i
t
 th
e
 agent'
s
 initia
l
 value
s
 Q
0
(x,
 a
)
 fo
r
 al
l
 pair
s
 o
f
 x
 an
d
 a
.
 A
 stat
e
 o
f
 th
e
 ARP
,
 (x
,
 n)
,
consist
s
 o
f
 a
 car
d
 numbe
r
 (o
r
 level)
 n
,
 togethe
r
 wit
h
 a
 stat
e
 x
 fro
m
 th
e
 rea
l
 process
.
 Th
e
action
s
 permitte
d
 i
n
 th
e
 AR
P
 ar
e
 th
e
 sam
e
 a
s
 thos
e
 permitte
d
 i
n
 th
e
 rea
l
 process
.
Th
e
 nex
t
 stat
e
 o
f
 th
e
 ARP
,
 give
n
 curren
t
 stat
e
 (x
,
 n
)
 an
d
 actio
n
 a
,
 i
s
 determine
d
 a
s
 follows
.
First
,
 al
l
 th
e
 card
s
 fo
r
 episode
s
 late
r
 tha
n
 n
 ar
e
 eliminated
,
 leavin
g
 jus
t
 a
 finit
e
 deck
.
 Card
s
ar
e
 the
n
 remove
d
 on
e
 a
t
 a
 tim
e
 fro
m
 to
p
 o
f
 thi
s
 dec
k
 an
d
 examine
d
 unti
l
 on
e
 i
s
 foun
d
 whos
e
startin
g
 stat
e
 an
d
 actio
n
 matc
h
 x
 an
d
 a
,
 sa
y
 a
t
 episod
e
 t
.
 The
n
 a
 biase
d
 coi
n
 i
s
 flipped
,
wit
h
 probabilit
y
 a
,
 o
f
 comin
g
 ou
t
 heads
,
 an
d
 1
 -
 a
,
 o
f
 tails
.
 I
f
 th
e
 coi
n
 turn
s
 u
p
 heads
,
th
e
 episod
e
 recorde
d
 o
n
 thi
s
 car
d
 i
s
 replayed
,
 a
 proces
s
 describe
d
 below
;
 i
f
 th
e
 coi
n
 turn
s
u
p
 tails
,
 thi
s
 car
d
 to
o
 i
s
 throw
n
 awa
y
 an
d
 th
e
 searc
h
 continue
s
 fo
r
 anothe
r
 car
d
 matchin
g
x
 an
d
 a
.
 I
f
 th
e
 botto
m
 car
d
 i
s
 reached
,
 th
e
 gam
e
 stop
s
 i
n
 a
 special
,
 absorbing
,
 state
,
 an
d
jus
t
 provide
s
 th
e
 rewar
d
 writte
n
 o
n
 thi
s
 car
d
 fo
r
 x
,
 a
,
 namel
y
 Qo(x
,
 a)
.
Replayin
g
 th
e
 episod
e
 o
n
 car
d
 t
 consist
s
 o
f
 emittin
g
 th
e
 reward
,
 r
t
 ,
 writte
n
 o
n
 th
e
 card
,
an
d
 the
n
 movin
g
 t
o
 th
e
 nex
t
 stat
e
 (y
t
,
 t
 -
 1
}
 i
n
 th
e
 ARP
,
 wher
e
 y
t
 i
s
 th
e
 stat
e
 t
o
 whic
h
th
e
 rea
l
 proces
s
 wen
t
 o
n
 tha
t
 episode
.
 Car
d
 t
 itsel
f
 i
s
 throw
n
 away
.
 Th
e
 nex
t
 stat
e
 transitio
n
of
 the
 ARP
 wil
l
 be
 take
n
 base
d
 on
 jus
t
 the
 remainin
g
 deck
.
Th
e
 abov
e
 completel
y
 specifie
s
 ho
w
 stat
e
 transition
s
 an
d
 reward
s
 ar
e
 determine
d
 i
n
 th
e
ARP
.
 Defin
e
 P^
t
iy
im
)
 [a
]
 an
d
 (Rj
n)
(a
)
 a
s
 th
e
 transition-probabilit
y
 matrice
s
 an
d
 expecte
d
reward
s
 o
f
 th
e
 ARP
.
 Als
o
 define
:
a
s
 th
e
 probabilitie
s
 that
,
 fo
r
 eac
h
 x
,
 n
 an
d
 a
,
 executin
g
 actio
n
 a
 a
t
 stat
e
 (x,n)
 i
n
 th
e
 AR
P
lead
s
 t
o
 stat
e
 y
 o
f
 th
e
 rea
l
 proces
s
 a
t
 som
e
 lowe
r
 leve
l
 i
n
 th
e
 deck
.
5
8


=== Page 5 ===
Q-LEARNIN
G
28
3
A
s
 define
d
 above
,
 th
e
 AR
P
 i
s
 a
s
 muc
h
 a
 controlle
d
 Marko
v
 proces
s
 a
s
 i
s
 th
e
 rea
l
 pro
-
cess
.
 On
e
 ca
n
 therefor
e
 conside
r
 sequence
s
 o
f
 state
s
 an
d
 controls
,
 an
d
 als
o
 optima
l
 dis
-
counte
d
 C
*
 value
s
 fo
r
 th
e
 ARP.
2
 Not
e
 tha
t
 durin
g
 suc
h
 a
 sequence
,
 episod
e
 card
s
 ar
e
 onl
y
remove
d
 fro
m
 th
e
 deck
,
 an
d
 ar
e
 neve
r
 replaced
.
 Therefore
,
 afte
r
 a
 finit
e
 numbe
r
 o
f
 actions
,
th
e
 botto
m
 car
d
 wil
l
 alway
s
 b
e
 reached
.
3.1.
 Lemmas
Tw
o
 lemma
s
 for
m
 th
e
 hear
t
 o
f
 th
e
 proof
.
 On
e
 show
s
 that
,
 effectivel
y
 b
y
 construction
,
 th
e
optima
l
 Q
,
 valu
e
 fo
r
 AR
P
 stat
e
 (x
,
 n
)
 an
d
 actio
n
 a
 i
s
 jus
t
 Q
n
(x,
 a)
.
 Th
e
 nex
t
 show
s
 tha
t
fo
r
 almos
t
 al
l
 possibl
e
 decks
,
 P^[a]
 converg
e
 t
o
 P
xy
[a]
 an
d
 R
x(n)
(a
)
 converg
e
 t
o
 S
x
(a)
a
s
 n
 -
 o
.
 Informa
l
 statement
s
 o
f
 th
e
 lemma
s
 an
d
 outline
s
 o
f
 thei
r
 proof
s
 ar
e
 give
n
 below
;
consul
t
 th
e
 appendi
x
 fo
r
 th
e
 forma
l
 statements
.
Lemm
a
 A
Q
n
(x,
 a
)
 ar
e
 th
e
 optima
l
 actio
n
 value
s
 fo
r
 AR
P
 state
s
 (x
,
 n
)
 an
d
 AR
P
 action
s
 a
.
Th
e
 AR
P
 wa
s
 directl
y
 constructe
d
 t
o
 hav
e
 thi
s
 property
.
 Th
e
 proo
f
 proceed
s
 b
y
 backward
s
induction
,
 followin
g
 th
e
 AR
P
 dow
n
 throug
h
 th
e
 stac
k
 o
f
 pas
t
 episodes
.
Lemm
a
 B
Lemm
a
 B
 concern
s
 th
e
 convergenc
e
 o
f
 th
e
 AR
P
 t
o
 th
e
 rea
l
 process
.
 Th
e
 firs
t
 tw
o
 step
s
ar
e
 preparatory
;
 th
e
 nex
t
 tw
o
 specif
y
 th
e
 for
m
 o
f
 th
e
 convergenc
e
 an
d
 provid
e
 foundation
s
fo
r
 provin
g
 tha
t
 i
t
 occurs
.
B.
1
Conside
r
 a
 discounted
,
 bounded-reward
,
 finit
e
 Marko
v
 process
.
 Fro
m
 an
y
 startin
g
 stat
e
x
,
 th
e
 differenc
e
 betwee
n
 th
e
 valu
e
 o
f
 tha
t
 stat
e
 unde
r
 th
e
 finit
e
 sequenc
e
 o
f
 s
 action
s
 an
d
it
s
 valu
e
 unde
r
 tha
t
 sam
e
 sequenc
e
 followe
d
 b
y
 an
y
 othe
r
 action
s
 tend
s
 t
o
 0
 a
s
 s
 -
 o
.
Thi
s
 follow
s
 fro
m
 th
e
 presenc
e
 o
f
 th
e
 discoun
t
 facto
r
 whic
h
 weigh
s
 th
e
 (
s
 +
 l)
t
h
 stat
e
b
y
 y
s
 -
 0
 a
s
 s
 -
 o
.
B.
2
Give
n
 an
y
 leve
l
 /
,
 ther
e
 exist
s
 anothe
r
 ye
t
 highe
r
 level
,
 h
,
 suc
h
 tha
t
 th
e
 probabilit
y
 ca
n
b
e
 mad
e
 arbitraril
y
 smal
l
 o
f
 strayin
g
 belo
w
 l
 afte
r
 takin
g
 5
 action
s
 i
n
 th
e
 ARP
,
 startin
g
fro
m
 abov
e
 h
.
Th
e
 probability
,
 startin
g
 a
t
 leve
l
 h
 o
f
 th
e
 AR
P
 o
f
 strayin
g
 belo
w
 an
y
 fixe
d
 leve
l
 /
 tend
s
t
o
 0
 a
s
 h
 -
 o
.
 Therefor
e
 ther
e
 i
s
 som
e
 sufficientl
y
 hig
h
 leve
l
 fo
r
 whic
h
 s
 action
s
 ca
n
b
e
 safel
y
 accommodated
,
 wit
h
 a
n
 arbitraril
y
 hig
h
 probabilit
y
 o
f
 leavin
g
 th
e
 AR
P
 abov
e
 /
.
5
9


=== Page 6 ===
28
4
C
.
 WATKIN
S
 AN
D
 P
.
 DAYA
N
B.
3
Wit
h
 probabilit
y
 1
,
 th
e
 probabilitie
s
 P$[a
]
 an
d
 expecte
d
 reward
s
 (RJ
(n)
(a
)
 i
n
 th
e
 AR
P
 con
-
verg
e
 an
d
 ten
d
 t
o
 th
e
 transitio
n
 matrice
s
 an
d
 expecte
d
 reward
s
 i
n
 th
e
 rea
l
 proces
s
 a
s
 th
e
leve
l
 n
 increase
s
 t
o
 infinity
.
 This
,
 togethe
r
 wit
h
 B.2
,
 make
s
 i
t
 appropriat
e
 t
o
 conside
r
P^\d\
 rathe
r
 tha
n
 th
e
 AR
P
 transitio
n
 matrice
s
 P$^,fy,
m
)[a],
 i.e.
,
 essentiall
y
 ignorin
g
 th
e
leve
l
 a
t
 whic
h
 th
e
 AR
P
 enter
s
 stat
e
 y
.
Th
e
 AR
P
 effectivel
y
 estimate
s
 th
e
 mea
n
 reward
s
 an
d
 transition
s
 o
f
 th
e
 rea
l
 proces
s
 ove
r
al
l
 th
e
 episodes
.
 Sinc
e
 it
s
 ra
w
 dat
a
 ar
e
 unbiased
,
 th
e
 condition
s
 o
n
 th
e
 sum
s
 an
d
 sum
s
o
f
 square
s
 o
f
 th
e
 learnin
g
 rate
s
 c
ni
(
X
,
a
)
 ensur
e
 th
e
 convergenc
e
 wit
h
 probabilit
y
 one
.
B.
4
Conside
r
 executin
g
 a
 serie
s
 o
f
 s
 action
s
 i
n
 th
e
 AR
P
 an
d
 i
n
 th
e
 rea
l
 process
.
 I
f
 th
e
 proba
-
bilitie
s
 P^[a
]
 an
d
 expecte
d
 reward
s
 (
R
(n)
(a
 )
 a
t
 appropriat
e
 level
s
 o
f
 th
e
 AR
P
 fo
r
 eac
h
of
 the
 actions
,
 are
 clos
e
 to
 P
xy
[a
]
 and
 (
R
x
(a),Va
,
 x,
 y,
 respectively
,
 the
n
 the
 valu
e
 of
 the
serie
s
 o
f
 action
s
 i
n
 th
e
 AR
P
 wil
l
 b
e
 clos
e
 t
o
 it
s
 valu
e
 i
n
 th
e
 rea
l
 process
.
Th
e
 discrepanc
y
 i
n
 th
e
 actio
n
 value
s
 ove
r
 a
 finit
e
 numbe
r
 s
 o
f
 action
s
 betwee
n
 th
e
 value
s
o
f
 tw
o
 approximatel
y
 equa
l
 Marko
v
 processe
s
 grow
s
 a
t
 mos
t
 quadraticall
y
 wit
h
 s
.
 So
,
i
f
 th
e
 transitio
n
 probabilitie
s
 an
d
 reward
s
 ar
e
 close
,
 the
n
 th
e
 value
s
 o
f
 th
e
 action
s
 mus
t
b
e
 clos
e
 too
.
3.2.
 The
 theorem
Puttin
g
 thes
e
 together
,
 th
e
 AR
P
 tend
s
 toward
s
 th
e
 rea
l
 process
,
 an
d
 s
o
 it
s
 optima
l
 Q
,
 value
s
d
o
 too
.
 Bu
t
 3
n
(a
,
 x
)
 ar
e
 th
e
 optima
l
 Q
 value
s
 fo
r
 th
e
 n
t
h
 leve
l
 o
f
 th
e
 AR
P
 (b
y
 Lemm
a
 A)
,
an
d
 s
o
 ten
d
 t
o
 Q*(x,
 a)
.
Assume
,
 withou
t
 los
s
 o
f
 generality
,
 tha
t
 Q
0
(X
,
 a
)
 <
 R/(
l
 -
 y
)
 an
d
 tha
t
 (
R
 >
 1
.
Give
n
 e
 >
 0
,
 choos
e
 s
 suc
h
 tha
t
B
y
 B.3
,
 wit
h
 probabilit
y
 1
,
 i
t
 i
s
 possibl
e
 t
o
 choos
e
 /
 sufficientl
y
 larg
e
 suc
h
 tha
t
 fo
r
n
 >
 l
,
 an
d
 Va
,
 x
,
 y
,
an
d
B
y
 B.2
,
 choos
e
 h
 sufficientl
y
 larg
e
 suc
h
 tha
t
 fo
r
 n
 >
 h
,
 th
e
 probability
,
 afte
r
 takin
g
s
 actions
,
 o
f
 endin
g
 u
p
 a
t
 a
 leve
l
 lowe
r
 tha
n
 l
 i
s
 les
s
 tha
n
 min{(e(
l
 -
 y)/6s(A),
(e/3s(
s
 +
 1)<R)}
.
 Thi
s
 mean
s
 tha
t
6
0


=== Page 7 ===
^-LEARNIN
G
28
5
an
d
wher
e
 th
e
 prime
s
 o
n
 P'
(n
)
 an
d
 (R'
(n
}
 indicat
e
 tha
t
 thes
e
 ar
e
 conditiona
l
 o
n
 th
e
 leve
l
 i
n
th
e
 AR
P
 afte
r
 th
e
 s
t
h
 ste
p
 bein
g
 greate
r
 tha
n
 /
.
Then
,
 fo
r
 n
 >
 h
,
 b
y
 B.4
,
 compar
e
 th
e
 valu
e
 Q
ARp
((x
,
 n)
,
 a
1
,...
,
 a
s
)
 o
f
 takin
g
 ac
-
tion
s
 a
1
,
 ...,
 a
s
 a
t
 stat
e
 x
 i
n
 th
e
 ARP
,
 wit
h
 Q,(x
,
 a
1
,
 ...,
 a
s
)
 o
f
 takin
g
 the
m
 i
n
 th
e
rea
l
 process:
3
Where
,
 i
n
 equatio
n
 4
,
 th
e
 firs
t
 ter
m
 count
s
 th
e
 cos
t
 o
f
 condition
s
 fo
r
 B.
2
 no
t
 holding
,
a
s
 th
e
 cos
t
 o
f
 strayin
g
 belo
w
 /
 i
s
 bounde
d
 b
y
 2s<R
 /(
1
 -
 7)
.
 Th
e
 secon
d
 ter
m
 i
s
 th
e
 cost
,
fro
m
 B.4
,
 o
f
 th
e
 incorrec
t
 reward
s
 an
d
 transitio
n
 probabilities
.
However
,
 b
y
 B.1
,
 th
e
 effec
t
 o
f
 takin
g
 onl
y
 s
 action
s
 make
s
 a
 differenc
e
 o
f
 les
s
 tha
n
 e/
6
fo
r
 bot
h
 th
e
 AR
P
 an
d
 th
e
 rea
l
 process
.
 Als
o
 sinc
e
 equatio
n
 4
 applie
s
 t
o
 an
y
 se
t
 o
f
 ac
-
tions
,
 i
t
 applie
s
 perforc
e
 t
o
 a
 se
t
 o
f
 action
s
 optima
l
 fo
r
 either
 th
e
 AR
P
 o
r
 th
e
 rea
l
 proc
-
ess
.
 Therefor
e
So
,
 wit
h
 probabilit
y
 1
,
 Q
n
(x,
 a
)
 -
 Q*(x
,
 a
)
 a
s
 n
 -
 o
 a
s
 required
.
4
.
 Discussion
s
 an
d
 conclusion
s
Fo
r
 th
e
 sak
e
 o
f
 clarity
,
 th
e
 theore
m
 prove
d
 abov
e
 wa
s
 somewha
t
 restricted
.
 Tw
o
 par
-
ticula
r
 extension
s
 t
o
 th
e
 versio
n
 o
f
 ^-learnin
g
 describe
d
 abov
e
 hav
e
 bee
n
 use
d
 i
n
 prac
-
tice
.
 On
e
 i
s
 th
e
 non-discounte
d
 cas
e
 (
7
 =
 1)
,
 bu
t
 fo
r
 a
 Marko
v
 proces
s
 wit
h
 absorbin
g
goa
l
 states
,
 an
d
 th
e
 othe
r
 i
s
 t
o
 th
e
 cas
e
 wher
e
 man
y
 o
f
 th
e
 5
 value
s
 ar
e
 update
d
 i
n
 eac
h
iteratio
n
 rathe
r
 tha
n
 jus
t
 on
e
 (Barto
,
 Bradtk
e
 &
 Singh
,
 1991)
.
 Th
e
 convergenc
e
 resul
t
 hold
s
fo
r
 bot
h
 o
f
 these
,
 an
d
 thi
s
 sectio
n
 sketche
s
 th
e
 modification
s
 t
o
 th
e
 proo
f
 tha
t
 ar
e
 necessary
.
A
 proces
s
 wit
h
 absorbin
g
 goa
l
 state
s
 ha
s
 on
e
 o
r
 mor
e
 state
s
 whic
h
 ar
e
 boun
d
 i
n
 th
e
 en
d
t
o
 tra
p
 th
e
 agent
.
 Thi
s
 ultimat
e
 certaint
y
 o
f
 bein
g
 trappe
d
 play
s
 th
e
 rol
e
 tha
t
 7
 <
 1
 playe
d
i
n
 th
e
 earlie
r
 proof
,
 i
n
 ensurin
g
 tha
t
 th
e
 valu
e
 o
f
 stat
e
 x
 unde
r
 an
y
 polic
y
 r
,
 V
w
(x),
 i
s
bounded
,
 an
d
 tha
t
 lemm
a
 B.
1
 holds
,
 i.e.
,
 tha
t
 th
e
 differenc
e
 betwee
n
 considerin
g
 infinit
e
an
d
 finit
e
 (s
)
 number
s
 o
f
 action
s
 tend
s
 t
o
 0
 a
s
 s
 -
 o
.
Sinc
e
 th
e
 proces
s
 woul
d
 alway
s
 ge
t
 trappe
d
 wer
e
 i
t
 allowe
d
 t
o
 run
,
 fo
r
 ever
y
 stat
e
 x
 ther
e
i
s
 som
e
 numbe
r
 o
f
 action
s
 u(x)
 suc
h
 tha
t
 n
o
 matte
r
 wha
t
 the
y
 are
,
 ther
e
 i
s
 a
 probabilit
y
p(x)
 >
 0
 o
f
 havin
g
 reache
d
 on
e
 o
f
 th
e
 goa
l
 state
s
 afte
r
 executin
g
 thos
e
 actions
.
 Tak
e
6
1


=== Page 8 ===
28
6
C
.
 WATKIN
S
 AN
D
 P
.
 DAYA
N
u
*
 =
 max
x
{u(x)}
,
 an
d
 p
*
 =
 min
x
{p(x)}
 >
 0
 (sinc
e
 ther
e
 i
s
 onl
y
 a
 finit
e
 numbe
r
 o
f
states)
.
 The
n
 a
 crud
e
 uppe
r
 boun
d
 fo
r
 V*(x)
 i
s
sinc
e
 i
n
 eac
h
 «
 *
 step
s
 th
e
 agen
t
 earn
s
 a
 rewar
d
 o
f
 les
s
 tha
n
 u
 *R
,
 an
d
 ha
s
 probabilit
y
 les
s
tha
n
 (
1
 —
 p*
)
 o
f
 no
t
 havin
g
 bee
n
 trapped
.
 Similarly
,
 th
e
 effec
t
 o
f
 measurin
g
 th
e
 rewar
d
afte
r
 onl
y
 o
u
*
 step
s
 i
s
 les
s
 tha
n
 (
1
 -
 p*)*u*
®
 -
 0
 a
s
 j
 -
 o
,
 an
d
 s
o
 a
n
 equivalen
t
o
f
 lemm
a
 B.
1
 doe
s
 hold
.
Changin
g
 mor
e
 tha
n
 on
e
 Q
 valu
e
 o
n
 eac
h
 iteratio
n
 require
s
 a
 mino
r
 modificatio
n
 t
o
 th
e
actio
n
 repla
y
 proces
s
 AR
P
 suc
h
 tha
t
 a
n
 actio
n
 ca
n
 b
e
 take
n
 a
t
 an
y
 leve
l
 a
t
 whic
h
 i
t
 wa
s
execute
d
 i
n
 th
e
 rea
l
 process—i.e.
,
 mor
e
 tha
n
 on
e
 actio
n
 ca
n
 b
e
 take
n
 a
t
 eac
h
 level
.
 A
s
lon
g
 a
s
 th
e
 stochasti
c
 convergenc
e
 condition
s
 i
n
 equatio
n
 3
 ar
e
 stil
l
 satisfied
,
 th
e
 proo
f
require
s
 n
o
 non-trivia
l
 modification
.
 Th
e
 Q
n
(x
,
 a
)
 value
s
 ar
e
 stil
l
 optima
l
 fo
r
 th
e
 modifie
d
ARP
,
 an
d
 thi
s
 stil
l
 tend
s
 t
o
 th
e
 rea
l
 proces
s
 i
n
 th
e
 origina
l
 manner
.
 Intuitively
,
 th
e
 proo
f
relie
s
 o
n
 th
e
 AR
P
 estimatin
g
 reward
s
 an
d
 transitio
n
 function
s
 base
d
 o
n
 man
y
 episodes
,
an
d
 thi
s
 i
s
 jus
t
 speede
d
 u
p
 b
y
 changin
g
 mor
e
 tha
n
 on
e
 Q
 valu
e
 pe
r
 iteration
.
Althoug
h
 th
e
 pape
r
 ha
s
 s
o
 fa
r
 presente
d
 a
n
 apparen
t
 dichotom
y
 betwee
n
 9-learnin
g
 an
d
method
s
 base
d
 o
n
 certaint
y
 equivalence
,
 suc
h
 a
s
 Sato
,
 Ab
e
 an
d
 Taked
a
 (1988)
,
 i
n
 fac
t
 ther
e
i
s
 mor
e
 o
f
 a
 continuum
.
 I
f
 th
e
 agen
t
 ca
n
 remembe
r
 th
e
 detail
s
 o
f
 it
s
 learnin
g
 episodes
,
then
,
 afte
r
 alterin
g
 th
e
 learnin
g
 rates
,
 i
t
 ca
n
 us
e
 eac
h
 o
f
 the
m
 mor
e
 tha
n
 onc
e
 (whic
h
 i
s
equivalen
t
 t
o
 puttin
g
 card
s
 tha
t
 wer
e
 throw
n
 away
,
 bac
k
 in
,
 lowe
r
 dow
n
 o
n
 th
e
 AR
P
 stack)
.
Thi
s
 biase
s
 th
e
 Q-learnin
g
 proces
s
 toward
s
 th
e
 particula
r
 sampl
e
 o
f
 th
e
 reward
s
 an
d
 transi
-
tion
s
 tha
t
 i
t
 ha
s
 experienced
.
 I
n
 th
e
 limi
t
 o
f
 re-presentin
g
 'old
'
 card
s
 infinitel
y
 often
,
 thi
s
reus
e
 amount
s
 t
o
 th
e
 certaint
y
 equivalenc
e
 ste
p
 o
f
 calculatin
g
 th
e
 optima
l
 action
s
 fo
r
 th
e
observe
d
 sampl
e
 o
f
 th
e
 Markovia
n
 environmen
t
 rathe
r
 tha
n
 th
e
 actua
l
 environmen
t
 itself
.
Th
e
 theore
m
 abov
e
 onl
y
 prove
s
 th
e
 convergenc
e
 o
f
 a
 restricte
d
 versio
n
 o
f
 Watkins
'
 (1989
)
comprehensiv
e
 Q-learnin
g
 algorithm
,
 sinc
e
 i
t
 doe
s
 no
t
 permi
t
 update
s
 base
d
 o
n
 th
e
 reward
s
fro
m
 mor
e
 tha
n
 on
e
 iteration
.
 Thi
s
 additio
n
 wa
s
 pioneere
d
 b
y
 Sutto
n
 (1984
;
 1988
)
 i
n
 hi
s
TD(X
)
 algorithm
,
 i
n
 whic
h
 a
 rewar
d
 fro
m
 a
 ste
p
 take
n
 r
 iteration
s
 previousl
y
 i
s
 weighte
d
b
y
 X
r
,
 wher
e
 X
 <
 1
.
 Unfortunately
,
 th
e
 theore
m
 doe
s
 no
t
 exten
d
 triviall
y
 t
o
 thi
s
 case
,
 an
d
alternativ
e
 proo
f
 method
s
 suc
h
 a
s
 thos
e
 i
n
 Kushne
r
 an
d
 Clar
k
 (1978
)
 ma
y
 b
e
 required
.
Thi
s
 pape
r
 ha
s
 presente
d
 th
e
 proo
f
 outline
d
 b
y
 Watkin
s
 (1989
)
 tha
t
 ^.-learnin
g
 converge
s
wit
h
 probabilit
y
 on
e
 unde
r
 reasonabl
e
 condition
s
 o
n
 th
e
 learnin
g
 rate
s
 an
d
 th
e
 Markovia
n
environment
.
 Suc
h
 a
 guarante
e
 ha
s
 previousl
y
 elude
d
 mos
t
 method
s
 o
f
 reinforcemen
t
learning
.
Acknowledgments
W
e
 ar
e
 ver
y
 gratefu
l
 t
o
 And
y
 Barto
,
 Graem
e
 Mitchison
,
 Stev
e
 Nowlan
,
 Satinde
r
 Singh
,
Ric
h
 Sutto
n
 an
d
 thre
e
 anonymou
s
 reviewer
s
 fo
r
 thei
r
 valuabl
e
 comment
s
 o
n
 multifariou
s
aspect
s
 o
f
 Q-learnin
g
 an
d
 thi
s
 paper
.
 Suc
h
 clarit
y
 a
s
 i
t
 possesse
s
 owe
s
 t
o
 Ric
h
 Sutton'
s
6
2


=== Page 9 ===
^-LEARNIN
G
28
7
tireles
s
 efforts
.
 Suppor
t
 wa
s
 fro
m
 Philip
s
 Researc
h
 Laboratorie
s
 an
d
 SERC
.
 PD'
s
 curren
t
addres
s
 i
s
 CNL
,
 Th
e
 Sal
k
 Institute
,
 P
O
 Bo
x
 85800
,
 Sa
n
 Diego
,
 C
A
 92186-5800
,
 USA
.
Note
s
1
.
 I
n
 general
,
 th
e
 se
t
 o
f
 availabl
e
 action
s
 ma
y
 diffe
r
 fro
m
 stat
e
 t
o
 state
.
 Her
e
 w
e
 assum
e
 i
t
 doe
s
 not
,
 t
o
 simplif
y
th
e
 notation
.
 Th
e
 theore
m
 w
e
 presen
t
 ca
n
 straightfowardl
y
 b
e
 extende
d
 t
o
 th
e
 genera
l
 case
.
2
.
 Th
e
 discoun
t
 facto
r
 fo
r
 th
e
 AR
P
 wil
l
 b
e
 take
n
 t
o
 b
e
 y
,
 th
e
 sam
e
 a
s
 fo
r
 th
e
 rea
l
 process
.
3
.
 Th
e
 bar
s
 ove
r
 th
e
 £
 indicat
e
 tha
t
 th
e
 su
m
 i
s
 ove
r
 onl
y
 a
 finit
e
 numbe
r
 o
f
 actions
,
 wit
h
 0
 termina
l
 reward
.
Appendi
x
Th
e
 action-repla
y
 proces
s
Th
e
 definitio
n
 o
f
 th
e
 AR
P
 i
s
 contingen
t
 o
n
 a
 particula
r
 sequenc
e
 o
f
 episode
s
 observe
d
i
n
 th
e
 rea
l
 process
.
 Th
e
 stat
e
 spac
e
 o
f
 th
e
 AR
P
 i
s
 {(x,
 n)},
 fo
r
 x
 a
 stat
e
 o
f
 th
e
 rea
l
 proces
s
an
d
 n
 >
 1
,
 togethe
r
 wit
h
 one
,
 special
,
 absorbin
g
 state
,
 an
d
 th
e
 actio
n
 spac
e
 i
s
 {a
}
 fo
r
a
 a
n
 actio
n
 fro
m
 th
e
 rea
l
 process
.
Th
e
 stochasti
c
 rewar
d
 an
d
 stat
e
 transitio
n
 consequen
t
 o
n
 performin
g
 actio
n
 a
 a
t
 stat
e
{x
,
 n
)
 i
s
 give
n
 a
s
 follows
.
 Fo
r
 convenience
,
 defin
e
 n
i
 =
 n
i
(x
,
 a)
,
 a
s
 th
e
 inde
x
 o
f
 th
e
 i
t
h
tim
e
 actio
n
 a
 wa
s
 trie
d
 a
t
 stat
e
 x
.
 Defin
e
i
f
 x
,
 a
 ha
s
 bee
n
 execute
d
 befor
e
 episod
e
 n
otherwis
e
suc
h
 tha
t
 n
i
*
 i
s
 th
e
 las
t
 tim
e
 befor
e
 episod
e
 n
 tha
t
 x
,
 a
 wa
s
 exeucte
d
 i
n
 th
e
 rea
l
 process
.
I
f
 i
*
 =
 0
,
 th
e
 rewar
d
 i
s
 se
t
 a
s
 Q
0
(x
,
 a)
,
 an
d
 th
e
 AR
P
 absorbs
.
 Otherwise
,
 le
t
b
e
 th
e
 inde
x
 o
f
 th
e
 episod
e
 tha
t
 i
s
 replayed
 o
r
 taken
,
 chose
n
 probabilisticall
y
 fro
m
 th
e
collectio
n
 o
f
 existin
g
 sample
s
 fro
m
 th
e
 rea
l
 process
.
 I
f
 i
e
 =
 0
,
 the
n
 th
e
 rewar
d
 i
s
 se
t
 a
t
Qo(x,
 a
)
 an
d
 th
e
 AR
P
 absorbs
,
 a
s
 above
,
 Otherwise
,
 takin
g
 i
e
 provide
s
 rewar
d
 r
n
i
e
,
 an
d
cause
s
 a
 stat
e
 transitio
n
 t
o
 (y
n
i
e
,
 n
e
 —
 1
)
 whic
h
 i
s
 a
t
 leve
l
 n
i
e
 -
 1
.
 Thi
s
 las
t
 poin
t
 i
s
crucial
,
 takin
g
 a
n
 actio
n
 i
n
 th
e
 AR
P
 alway
s
 cause
s
 a
 stat
e
 transitio
n
 t
o
 a
 lowe
r
 level—s
o
i
t
 ultimatel
y
 terminates
.
 Th
e
 discoun
t
 facto
r
 i
n
 th
e
 AR
P
 i
s
 7
,
 th
e
 sam
e
 a
s
 i
n
 th
e
 rea
l
 process
.
6
3


=== Page 10 ===
28
8
C
.
 WATKIN
S
 AN
D
 P
.
 DAYAN
Lemm
a
 A
:
 Q
n
 ar
e
 optima
l
 fo
r
 th
e
 AR
P
Q
n
(x,
 a
)
 ar
e
 th
e
 optima
l
 actio
n
 value
s
 fo
r
 AR
P
 state
s
 {x
,
 n
)
 an
d
 AR
P
 action
s
 a
.
 Tha
t
 i
s
an
d
Proo
f
B
y
 induction
.
 Fro
m
 th
e
 constructio
n
 o
f
 th
e
 ARP
,
 Q
0
(X
,
 a
)
 i
s
 th
e
 optimal—indee
d
 th
e
 onl
y
possible—actio
n
 valu
e
 o
f
 (x
,
 0)
,
 a
.
 Therefore
,
Henc
e
 th
e
 theore
m
 hold
s
 fo
r
 n
 =
 0
.
Suppos
e
 tha
t
 th
e
 value
s
 o
f
 £
n-1
,
 a
s
 produce
d
 b
y
 th
e
 ^-learnin
g
 rule
,
 ar
e
 th
e
 optima
l
actio
n
 value
s
 fo
r
 th
e
 AR
P
 a
t
 leve
l
 n
 —
 1
,
 tha
t
 i
s
Thi
s
 implie
s
 tha
t
 th
e
 V
n-1
(x
)
 ar
e
 th
e
 optima
l
 value
s
 V
*
 fo
r
 th
e
 AR
P
 a
t
 th
e
 n
 -
 1
t
h
 level
,
tha
t
 i
s
No
w
 conside
r
 th
e
 case
s
 i
n
 tryin
g
 t
o
 perfor
m
 actio
n
 a
 i
n
 (x
,
 n)
.
 I
f
 x
,
 a
 =
 x
n
,
 a
n
,
 the
n
 thi
s
i
s
 th
e
 sam
e
 a
s
 performin
g
 a
 i
n
 (x
,
 n
 -
 1)
,
 an
d
 £
n
(x
,
 a
)
 =
 Q
n-1
(x,a)
.
 Therefore
,
Otherwise
,
 performin
g
 a
n
 i
n
 {x
n
,
 n
}
•
 wit
h
 probabilit
y
 1
 -
 a
n
 i
s
 exactl
y
 th
e
 sam
e
 a
s
 performin
g
 a
n
 i
n
 (x
n
,
 n
 -
 1)
,
 o
r
•
 wit
h
 probabilit
y
 a
n
 yield
s
 immediat
e
 rewar
d
 r
n
 an
d
 ne
w
 stat
e
 (y
n
,
 n
 —
 1)
.
Therefor
e
 th
e
 optima
l
 actio
n
 valu
e
 i
n
 th
e
 AR
P
 o
f
 {x
n
,
 n}
,
 a
n
 i
s
fro
m
 th
e
 inductio
n
 hypothesi
s
 an
d
 th
e
 $
n
 interatio
n
 formul
a
 i
n
 equatio
n
 1
.
Hence
,
 Q,
n
(x,
 a
)
 =
 Q*
ARP
({x
,
 n)
,
 a)
,
 Va
,
 x
,
 a
s
 required
.
6
4


=== Page 11 ===
3-LEARNIN
G
28
9
Lemm
a
 B
B.1
 Discounting
 infinite
 sequences
Conside
r
 a
 discounted
,
 bounded-reward
,
 finit
e
 Marko
v
 proces
s
 wit
h
 transitio
n
 matri
x
P
xy
[a]
.
 Fro
m
 an
y
 startin
g
 stat
e
 x
,
 th
e
 differenc
e
 betwee
n
 th
e
 valu
e
 o
f
 tha
t
 stat
e
 unde
r
 an
y
se
t
 o
f
 s
 action
s
 an
d
 unde
r
 thos
e
 sam
e
 s
 action
s
 followe
d
 b
y
 an
y
 arbitrar
y
 polic
y
 tend
s
 t
o
0
 a
s
 s
 -
 o
.
Proo
f
Ignorin
g
 th
e
 valu
e
 o
f
 th
e
 s
 +
 1
t
h
 stat
e
 incur
s
 a
 penalt
y
 o
f
Bu
t
 i
f
 al
l
 reward
s
 ar
e
 bounde
d
 b
y
 R
,
 |
 V*(x)\
 <
 R/(
l
 -
 7)
,
 an
d
 s
o
B.2
 The
 probability
 of
 straying
 below
 level
 l is
 executing
 s
 actions
 can
 be
 make
 arbitrarily
small
Give
n
 an
y
 leve
l
 l
,
 ther
e
 exist
s
 anothe
r
 ye
t
 highe
r
 level
,
 h
,
 suc
h
 tha
t
 th
e
 probabilit
y
 ca
n
b
e
 mad
e
 arbitraril
y
 smal
l
 o
f
 strayin
g
 belo
w
 /
 afte
r
 takin
g
 s
 action
s
 i
n
 th
e
 ARP
,
 startin
g
fro
m
 abov
e
 h
.
Proo
f
Defin
e
 i
h
 a
s
 th
e
 larges
t
 i
 suc
h
 tha
t
 n'(x,
 a
)
 ^
 n
,
 an
d
 i
l
 a
s
 th
e
 smalles
t
 suc
h
 tha
t
 n
i
(x
,
 a
)
 >
 l
.
Then
,
 definin
g
 x
n
o
 =
 1
,
 th
e
 probabilit
y
 o
f
 strayin
g
 belo
w
 l
 startin
g
 fro
m
 (x
,
 n)
,
 n
 >
 l
executin
g
 actio
n
 a
 is
:
where
,
 a
s
 before
,
 n
i
 =
 n
i
(x
,
 a)
.
 Bu
t
 njl
;/
(
l
 -
 «„'
)
 <
 exp(
-
 E)t,
;
 «„«
)
 -
 0
 a
s
 n
 an
d
henc
e
 i
h
 -
 o
.
 Furthermore
,
 sinc
e
 th
e
 stat
e
 an
d
 actio
n
 space
s
 ar
e
 finite
,
 give
n
 n
,
 ther
e
exist
s
 som
e
 leve
l
 n
1
 suc
h
 tha
t
 startin
g
 abov
e
 ther
e
 fro
m
 an
y
 (x
,
 a
)
 lead
s
 t
o
 a
 leve
l
 abov
e
/
 wit
h
 probabilit
y
 a
t
 leas
t
 1
 -
 17
.
 Thi
s
 argumen
t
 iterate
s
 fo
r
 th
e
 secon
d
 actio
n
 wit
h
 n
1
 a
s
th
e
 ne
w
 lowe
r
 limit
.
 n
 ca
n
 b
e
 chose
n
 appropriatel
y
 t
o
 se
t
 th
e
 overal
l
 probabilit
y
 o
f
 strayin
g
belo
w
 /
 les
s
 tha
n
 an
y
 arbitrar
y
 e
 >
 0
.
6
5


=== Page 12 ===
29
0
C
.
 WATKIN
S
 AN
D
 P
.
 DAYA
N
B.
 3
 Rewards
 and
 transition
 probabilities
 converge
 with
 probabability
 1
Wit
h
 probabilit
y
 1
,
 th
e
 probabilitie
s
 P%\a]
 an
d
 expecte
d
 reward
s
 (RJ
n)
(a
)
 i
n
 th
e
 AR
P
 con
-
verg
e
 an
d
 ten
d
 t
o
 th
e
 transitio
n
 matrice
s
 an
d
 expecte
d
 reward
s
 i
n
 th
e
 rea
l
 proces
s
 a
s
 th
e
leve
l
 n
 increase
s
 t
o
 infinity
.
Proo
f
A
 standar
d
 theore
m
 i
n
 stochasti
c
 convergenc
e
 (e.g.
,
 theore
m
 2.3.
1
 o
f
 Kushne
r
 &
 Clark
,
1978
)
 state
s
 tha
t
 i
f
 X
n
 ar
e
 update
d
 accordin
g
 t
o
wher
e
 0
 <
 8
B
 <
 1
,
 £,"
i
 3
n
 =
 o
,
 £?!
,
 p
%
 <
 o
,
 an
d
 £
n
,
 ar
e
 bounde
d
 rando
m
 variable
s
wit
h
 mea
n
 E
,
 the
n
wit
h
 probabilit
y
 1
.
I
f
 R
(x,n)
(a
)
 i
s
 th
e
 expecte
d
 immediat
e
 rewar
d
 fo
r
 performin
g
 actio
n
 a
 fro
m
 stat
e
 x
 a
t
 leve
l
n
 i
n
 th
e
 ARP
,
 the
n
 &
(x,n
)
 (a
)
 satisfie
s
wher
e
 th
e
 R
 an
d
 th
e
 a
 satisf
y
 th
e
 condition
s
 o
f
 th
e
 theore
m
 wit
h
 E
 =
 6
x
(a),
 an
d
rememberin
g
 tha
t
 n
i
 i
s
 th
e
 i
t
h
 occasio
n
 o
n
 whic
h
 actio
n
 a
 wa
s
 trie
d
 a
t
 stat
e
 x
.
 Therefor
e
®
(x,n)
(a
)
 -
 R
x
((a
)
 a
s
 n
 -
 o
,
 wit
h
 probbilit
y
 one
.
 Also
,
 sinc
e
 ther
e
 i
s
 onl
y
 a
 finit
e
 num
-
be
r
 o
f
 state
s
 an
d
 actions
,
 th
e
 convergenc
e
 i
s
 uniform
.
Similarly
,
 defin
e
a
s
 a
 (rando
m
 variable
)
 indicato
r
 functio
n
 o
f
 th
e
 n
t
h
 transition
,
 mea
n
 valu
e
 P
xy
(a)
.
 Then
,
wit
h
 P^}\a]
 a
s
 th
e
 probabilit
y
 o
f
 endin
g
 u
p
 a
t
 stat
e
 y
 base
d
 o
n
 a
 transitio
n
 fro
m
 stat
e
 x
usin
g
 actio
n
 a
 a
t
 leve
l
 n
 i
n
 th
e
 ARP
,
an
d
 so
,
 b
y
 th
e
 theorem
,
 P^\a}
 -
 P
xy
[a]
 (th
e
 transitio
n
 matri
x
 i
n
 th
e
 rea
l
 process
)
 a
s
n
 -
 o
,
 wit
h
 probabilit
y
 one
.
Since
,
 i
n
 addition
,
 al
l
 observation
s
 fro
m
 th
e
 rea
l
 proces
s
 ar
e
 independent
,
 and
,
 b
y
 B.2
,
th
e
 probabilit
y
 o
f
 strayin
g
 belo
w
 a
 fixe
d
 leve
l
 k
 ca
n
 b
e
 mad
e
 arbitraril
y
 small
,
 th
e
 transi
-
tio
n
 probabilitie
s
 an
d
 expecte
d
 reward
s
 fo
r
 a
 singl
e
 ste
p
 conditional
 o
n
 endin
g
 u
p
 a
t
 a
 leve
l
greate
r
 tha
n
 k
 als
o
 converg
e
 t
o
 P
xy
[a]
 an
d
 R
x
(a)
 a
s
 n
 -
 o
.
6
6


=== Page 13 ===
Q-LEARNIN
G
29
1
B.4
 Close
 rewards
 and
 transitions
 imply
 close
 values
Le
t
 P!y[a],
 fo
r
 i
 =
 1
 ..
.
 s
 b
e
 th
e
 transitio
n
 matrice
s
 o
f
 s
 Marko
v
 chains
,
 an
d
 RJ(a
)
 b
e
th
e
 rewar
d
 functions
.
 Conside
r
 th
e
 s-ste
p
 chai
n
 forme
d
 fro
m
 th
e
 concatenatio
n
 o
f
 these
—
i.e.
,
 startin
g
 fro
m
 stat
e
 x
1
,
 mov
e
 t
o
 stat
e
 x
2
 accordin
g
 t
o
 /^[fl]]
,
 the
n
 stat
e
 X
3
,
 accordin
g
t
o
 P
x2x3
[a
2
],
 an
d
 s
o
 on
,
 wit
h
 commensurate
 rewards
.
 Give
n
 n >
 0
,
 i
f
 P
i
[a
]
 ar
e
 withi
n
n/
R
 o
f
 P
xy
[a],
 Va
,
 x
,
 y
,
 an
d
 R
x
(a
)
 ..
.
 R
x
(a
)
 ar
e
 withi
n
 n
 o
f
 R
x
(a)
,
 Va
,
 x
,
 the
n
 th
e
valu
e
 o
f
 th
e
 s
 action
s
 i
n
 th
e
 concatenate
d
 chai
n
 i
s
 withi
n
 n
s(
s
 +
 l)/
2
 o
f
 thei
r
 valu
e
 i
n
 th
e
rea
l
 process
.
Proo
f
Define
:
a
s
 th
e
 expecte
d
 rewar
d
 i
n
 th
e
 rea
l
 proces
s
 fo
r
 executin
g
 tw
o
 actions
,
 a
1
 an
d
 a
2
 a
t
 stat
e
 x
,
an
d
a
s
 th
e
 equivalen
t
 i
n
 th
e
 concatenate
d
 chai
n
 fo
r
 exactl
y
 th
e
 sam
e
 actions
.
Then
,
 sinc
e
 /
 R
x
(a
)
 -
 &
x
(a)\
 <
 n
 an
d
 P
xy
[a]
 -
 P
xy
[a]
 <
 n/R
,
 Va
,
 i
,
 x
,
 y
,
Similarly
,
 fo
r
 s
 actions
,
Thi
s
 applie
s
 t
o
 th
e
 AR
P
 i
f
 th
e
 reward
s
 an
d
 transitio
n
 matrice
s
 a
t
 th
e
 successivel
y
 lowe
r
level
s
 ar
e
 sufficientl
y
 clos
e
 t
o
 thos
e
 i
n
 th
e
 rea
l
 process—th
e
 mai
n
 bod
y
 o
f
 th
e
 theore
m
quantifie
s
 th
e
 cos
t
 o
f
 thi
s
 conditio
n
 failing
.
6
7


=== Page 14 ===
29
2
C
.
 WATKIN
S
 AN
D
 P
.
 DAYA
N
Reference
s
Barto
,
 A.G.
,
 Bradtke
,
 S.J
.
 &
 Singh
,
 S.P
.
 (1991)
.
 Real-time
 learning
 an
d
 control
 using
 asynchronous
 dynamic
programming.
 (COIN
S
 technica
l
 repor
t
 91-57)
.
 Amherst
:
 Universit
y
 o
f
 Massachusetts
.
Barto
,
 A.G
.
 &
 Singh
,
 S.P
.
 (1990)
.
 O
n
 th
e
 computationa
l
 economic
s
 o
f
 reinforcemen
t
 learning
.
 I
n
 D.S
.
 Touretzky
,
J
.
 Elman
,
 T.J
.
 Sejnowsk
i
 &
 G.E
.
 Hinton
,
 (Eds.)
,
 Proceedings
 o
f
 th
e
 1990
 Connections
t
 Models
 Summer
 School.
Sa
n
 Mateo
,
 CA
:
 Morga
n
 Kaufmann
.
Bellman
,
 R.E
.
 &
 Dreyfus
,
 S.E
.
 (1962)
.
 Applied
 dynamic
 programming.
 RAN
D
 Corporation
.
Chapman
,
 D
.
 &
 Kaelbling
,
 L.P
.
 (1991)
.
 Inpu
t
 generalizatio
n
 i
n
 delaye
d
 reinforcemen
t
 learning
:
 A
n
 algorith
m
an
d
 performanc
e
 comparisons
.
 Proceedings
 o
f
 th
e
 1991
 International
 Joint
 Conference
 o
n
 Artificial
 Intelligence
(pp
.
 726-731)
.
Kushner
,
 H
.
 &
 Clark
,
 D
.
 (1978)
.
 Stochastic
 approximation
 methods
 fo
r
 constrained
 an
d
 unconstrained
 systems.
Berlin
,
 Germany
:
 Springer-Verlag
.
Lin
,
 L
.
 (1992)
.
 Self-improvin
g
 reactiv
e
 agent
s
 base
d
 o
n
 reinforcemen
t
 learning
,
 plannin
g
 an
d
 teaching
.
 Machine
Learning,
 8.
Mahadeva
n
 &
 Connel
l
 (1991)
.
 Automati
c
 programmin
g
 o
f
 behavior-base
d
 robot
s
 usin
g
 reinforcemen
t
 learning
.
Proceedings
 o
f
 th
e
 1991
 National
 Conference
 o
n
 A
I
 (pp
.
 768-773)
.
Ross
,
 S
.
 (1983)
.
 Introduction
 t
o
 stochastic
 dynamic
 programming.
 Ne
w
 York
,
 Academi
c
 Press
.
Sato
,
 M.
,
 Abe
,
 K
.
 &Takeda
,
 H
.
 (1988)
.
 Learnin
g
 contro
l
 o
f
 finit
e
 Marko
v
 chain
s
 wit
h
 explici
t
 trade-of
f
 betwee
n
estimatio
n
 an
d
 control
.
 IEEE
 Transactions
 o
n
 Systems,
 Ma
n
 an
d
 Cybernetics,
 18
,
 pp
.
 677-684
.
Sutton
,
 R.S
.
 (1984)
.
 Temporal
 credit
 assignment
 i
n
 reinforcement
 learning.
 Ph
D
 Thesis
,
 Universit
y
 o
f
 Massachusetts
,
Amherst
,
 MA
.
Sutton
,
 R.S
.
 (1988)
.
 Learnin
g
 t
o
 predic
t
 b
y
 th
e
 method
s
 o
f
 tempora
l
 difference
.
 Machine
 Learning,
 3
,
 pp
.
 9-44
.
Sutton
.
 R.S
.
 (1990)
.
 Integrate
d
 architecture
s
 fo
r
 learning
,
 planning,
 an
d
 reactin
g
 base
d
 o
n
 approximatin
g
 dynami
c
programming
.
 Proceedings
 o
f
 th
e
 Seventh
 International
 Conference
 o
n
 Machine
 Learning.
 Sa
n
 Mateo
,
 CA
:
Morga
n
 Kaufmann
.
Watkins
,
 C.J.C.H
.
 (1989)
.
 Learning
 fro
m
 delayed
 rewards.
 Ph
D
 Thesis
,
 Universit
y
 o
f
 Cambridge
,
 England
.
Werbos
,
 P.J
.
 (1977)
.
 Advance
d
 forecastin
g
 method
s
 fo
r
 globa
l
 crisi
s
 warnin
g
 an
d
 model
s
 o
f
 intelligence
.
 General
Systems
 Yearbook,
 22
,
 pp
.
 25-38
.
6
8


