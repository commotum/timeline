                         DEPTH-ADAPTIVE TRANSFORMER
                          MahaElbayad∗                      Jiatao Gu, Edouard Grave, Michael Auli
                          Univ. Grenoble Alpes              Facebook AI Research
                                                              ABSTRACT
                                State of the art sequence-to-sequence models for large scale tasks perform a ﬁxed
                                numberofcomputationsforeachinputsequenceregardlessofwhetheritiseasyor
                                hard to process. In this paper, we train Transformer models which can make out-
                                put predictions at different stages of the network and we investigate different ways
                                to predict how much computation is required for a particular sequence. Unlike
                                dynamic computation in Universal Transformers, which applies the same set of
                                layers iteratively, we apply different layers at every step to adjust both the amount
                                of computation as well as the model capacity. On IWSLT German-English trans-
                                lation our approach matches the accuracy of a well tuned baseline Transformer
                                while using less than a quarter of the decoder layers.
                         1   INTRODUCTION
                        Thesizeofmodernneuralsequencemodels(Gehringetal.,2017;Vaswanietal.,2017;Devlinetal.,
                        2019) can amount to billions of parameters (Radford et al., 2019). For example, the winning entry
                        of the WMT’19 news machine translation task in English-German used an ensemble totaling two
                        billion parameters (Ng et al., 2019). While large models are required to do better on hard examples,
                        small models are likely to perform as well on easy ones, e.g., the aforementioned ensemble is prob-
                        ably not required to translate a short phrase such as "Thank you". However, current models apply
                        the same amount of computation regardless of whether the input is easy or hard.
                        In this paper, we propose Transformers which adapt the number of layers to each input in order to
                        achieve a good speed-accuracy trade off at inference time. We extend Graves (2016; ACT) who
                        introduced dynamic computation to recurrent neural networks in several ways: we apply different
                        layers at each stage, we investigate a range of designs and training targets for the halting module and
                        weexplicitly supervise through simple oracles to achieve good performance on large-scale tasks.
                        Universal Transformers (UT) rely on ACT for dynamic computation and repeatedly apply the same
                        layer (Dehghani et al., 2018). Our work considers a variety of mechanisms to estimate the network
                        depth and applies a different layer at each step. Moreover, Dehghani et al. (2018) ﬁx the number
                        of steps for large-scale machine translation whereas we vary the number of steps to demonstrate
        arXiv:1910.10073v4  [cs.CL]  14 Feb 2020substantial improvements in speed at no loss in accuracy. UT uses a layer which contains as many
                        weights as an entire standard Transformer and this layer is applied several times which impacts
                        speed. Our approach does not increase the size of individual layers. We also extend the resource
                        efﬁcient object classiﬁcation work of Huang et al. (2017) and Bolukbasi et al. (2017) to structured
                        prediction where dynamic computation decisions impact future computation. Related work from
                        computervisionincludesTeerapittayanonetal.(2016);Figurnovetal.(2017)andWangetal.(2018)
                        whoexplored the idea of dynamic routing either by exiting early or by skipping layers.
                        WeencodetheinputsequenceusingastandardTransformerencodertogeneratetheoutputsequence
                        with a varying amount of computation in the decoder network. Dynamic computation poses a chal-
                        lenge for self-attention because omitted layers in prior time-steps may be required in the future.
                        We experiment with two approaches to address this and show that a simple approach works well
                        (§2). Next, we investigate different mechanisms to control the amount of computation in the de-
                        coder network, either for the entire sequence or on a per-token basis. This includes multinomial
                        andbinomialclassiﬁerssupervisedbythemodellikelihoodorwhethertheargmaxisalreadycorrect
                        as well as simply thresholding the model score (§3). Experiments on IWSLT14 German-English
                           ∗Workdoneduringaninternship at Facebook AI Research.
                                                                    1
                                                                                                         C
                                                                             State      Copied state      n Classiﬁer        Copy
                                                                                               C3                                                   C3
                                                   depth                                                  depth
                                                                                               C                      C
                                                   Decoder                                       2        Decoder       2
                                                                                               C1                                    C1                 ×M
                                                                        Decoding step                                                   Decoding step
                                                               (a) Aligned training                                    (b) Mixed training
                                    Figure 1: Training regimes for decoder networks able to emit outputs at any layer. Aligned training
                                    optimizesalloutputclassiﬁersC simultaneouslyassumingallprevioushiddenstatesforthecurrent
                                                                               n
                                    layer are available. Mixed training samples M paths of random exits at which the model is assumed
                                    to have exited; missing previous hidden states are copied from below.
                                    translation (Cettolo et al., 2014) as well as WMT’14 English-French translation show that we can
                                    match the performance of well tuned baseline models at up to 76% less computation (§4).
                                     2     ANYTIME STRUCTURED PREDICTION
                                    We ﬁrst present a model that can make predictions at different layers. This is known as anytime
                                    prediction for computer vision models (Huang et al., 2017) and we extend it to structured prediction.
                                     2.1     TRANSFORMER WITH MULTIPLE OUTPUT CLASSIFIERS
                                    WebaseourapproachontheTransformersequence-to-sequencemodel(Vaswanietal.,2017). Both
                                    encoder and decoder networks contain N stacked blocks where each has several sub-blocks sur-
                                    rounded by residual skip-connections. The ﬁrst sub-block is a multi-head dot-product self-attention
                                    and the second a position-wise fully connected feed-forward network. For the decoder, there is an
                                    additional sub-block after the self-attention to add source context via another multi-head attention.
                                    Given a pair of source-target sequences (x,y), x is processed with the encoder to give representa-
                                    tions s = (s ,...,s            ). Next, the decoder generates y step-by-step. For every new token y input
                                                     1          |x|                                                                                             t
                                                                                                                                                n
                                    to the decoder at time t, the N decoder blocks process it to yield hidden states (h )                                    :
                                                                                                                                                t  1≤n≤N
                                                                           0                          n                  n−1
                                                                         h =embed(y ),              h =block (h               , s),                                  (1)
                                                                           t                 t        t             n    ≤t
                                                                                                        th
                                    where blockn is the mapping associated with the n                      block and embed is a lookup table.
                                    The output distribution for predicting the next token is computed by feeding the activations of the
                                                               N
                                    last decoder layer h          into a softmax normalized output classiﬁer W:
                                                               t
                                                                                 p(y      |hN) = softmax(WhN)                                                        (2)
                                                                                      t+1    t                          t
                                    Standard Transformers have a single output classiﬁer attached to the top of the decoder network.
                                    However, for dynamic computation we need to be able to make predictions at different stages of the
                                                                                                                                                                   n
                                    network. To achieve this, we attach output classiﬁers C                      parameterized by W to the output h of
                                                                                                               n                            n                      t
                                    each of the N decoder blocks:
                                                                              ∀n, p(y         |hn) = softmax(W hn)                                                   (3)
                                                                                         t+1    t                       n t
                                    TheclassiﬁerscanbeparameterizedindependentlyorwecansharetheweightsacrosstheN blocks.
                                     2.2     TRAINING MULTIPLE OUTPUT CLASSIFIERS
                                    Dynamiccomputation enables the model to use any of the N exit classiﬁers instead of just the ﬁnal
                                    one. Some of our models can choose a different output classiﬁer at each time-step which results in
                                    an exponential number of possible output classiﬁer combinations in the sequence length.
                                                                                                      2
                                           Weconsider two possible ways to train the decoder network (Figure 1). Aligned training optimizes
                                           all classiﬁers simultaneously and assumes all previous hidden states required by the self-attention
                                           are available. However, at test time this is often not the case when we choose a different exit for
                                           every token which leads to misaligned states. Instead, mixed training samples several sequences of
                                           exits for a given sentence and exposes the model to hidden states from different layers.
                                           Generally, for a given output sequence y, we have a sequence of chosen exits (n1,...,n|y|) and we
                                           denote the block at which we exit at time t as nt.
                                           2.2.1       ALIGNED TRAINING
                                                                                                                n−1              n−1
                                           Aligned training assumes all hidden states h                               , . . . , ht      are available in order to compute self-
                                                                                                                1
                                           attention and it optimizes N loss terms, one for each exit (Figure 1a):
                                                                                                            |y|                                                       N
                                                            n                      n                n      X n                                          P1 X                         n
                                                       LL =logp(yt|h                    ),     LL =               LL ,        L (x,y)=−                                    ω LL .               (4)
                                                            t                      t−1                                t          dec                           ω             n
                                                                                                            t=1                                             n n n=1
                                           The compound loss L                    (x,y) is a weighted average of N terms w.r.t. to (ω ,...ω ). We found
                                                                             dec                                                                                   1          N
                                           that uniform weights achieve better BLEU compared to other weighing schemes (c.f. Appendix A).
                                           At inference time, not all time-steps will have hidden states for the current layer since the model
                                           exited early. In this case, we simply copy the last computed state to all upper layers, similar to mixed
                                           training (§2.2.2). However, we do apply layer-speciﬁc key and value projections to the copied state.
                                           2.2.2       MIXED TRAINING
                                           Aligned training assumes that all hidden states of the previous time-steps are available but this as-
                                           sumptionisunrealisticsinceanearlyexitmayhavebeenchosenpreviously. Thiscreatesamismatch
                                           betweentraining and testing. Mixed training reduces the mismatch by training the model to use hid-
                                           den states from different blocks of previous time-steps for self-attention. We sample M different
                                           exit sequences (n(m),...n(m))                                 and evaluate the following loss:
                                                                      1             |y|    1≤m≤M
                                                                                 |y|                                                                M
                                                                                X                     nt                                      1 X                 (m)             (m)
                                                LL(n ,...,n               ) =          logp(y |h            ),     L (x,y)=−                             LL(n          , . . . , n     ).       (5)
                                                         1            |y|                         t   t−1            dec                    M                     1               |y|
                                                                                 t=1                                                              m=1
                                           When nt < N, we copy the last evaluated hidden state hn to the subsequent layers so that the
                                                                                                                                        t
                                           self-attention of future time steps can function as usual (see Figure 1b).
                                           3      ADAPTIVE DEPTH ESTIMATION
                                           We present a variety of mechanisms to predict the decoder block at which the model will stop
                                           and output the next token, or when it should exit to achieve a good speed-accuracy trade-off. We
                                           consider two approaches: sequence-speciﬁc depth decodes all output tokens using the same block
                                           (§3.1) while token-speciﬁc depth determines a separate exit for each individual token (§3.2).
                                           We model the distribution of exiting at time-step t with a parametric distribution qt where qt(n)
                                           is the probability of computing block ,...,block                                  and then emitting a prediction with C . The
                                                                                                      1                   n                                                              n
                                           parameters of qt are optimized to match an oracle distribution q∗ with cross-entropy:
                                                                                                                                             t
                                                                                          L (x,y)=XH(q∗(x,y),q (x))                                                                             (6)
                                                                                             exit                            t              t
                                                                                                                  t
                                           The exit loss (L              ) is back-propagated to the encoder-decoder parameters. We simultaneously
                                                                     exit
                                           optimize the decoding loss (Eq. (4)) and the exit loss (Eq. (6)) balanced by a hyper-parameter α to
                                           ensure that the model maintains good generation accuracy. The ﬁnal loss takes the form:
                                                                                          L(x,y) = L               (x,y)+αL (x,y),                                                              (7)
                                                                                                              dec                     exit
                                           In the following we describe for each approach how the exit distribution q is modeled (illustrated
                                                                                                                                                               t
                                           in Figure 2) and how the oracle distribution q∗ is inferred.
                                                                                                                t
                                                                                                                       3
                                                                                               1                      C
                                                                State       Copied state         Halting decision      n Classiﬁer        Copy
                                                                                                                      C3                                          C3
                                       depth        s          2                  depth                                       depth
                                                                                                                                        S                        C
                                       Decoder    C2          C2           C2     Decoder    C2                               Decoder    C2
                                                                                            2            1            3                 C           S            C
                                                                                                          C1                                         C1
                                                                 Decoding step                              Decoding step                               Decoding step
                                          (a) Sequence-speciﬁc depth              (b) Token-speciﬁc - Multinomial           (c) Token-speciﬁc - Geometric-like
                                    Figure 2: Variants of the adaptive depth prediction classiﬁers. Sequence-speciﬁc depth uses a multi-
                                    nomial classiﬁer to choose an exit for the entire output sequence based on the encoder output s (2a).
                                    It then outputs a token at this depth with classiﬁer C . The token-speciﬁc multinomial classiﬁer
                                                                                                             n
                                    determines the exit after the ﬁrst block and proceeds up to the predicted depth before outputting the
                                    next token (2b). The token geometric-like classiﬁer (2c) makes a binary decision after every block
                                    to dictate whether to continue (C) to the next block or to stop (S) and emit an output distribution.
                                     3.1     SEQUENCE-SPECIFIC DEPTH:
                                    For sequence-speciﬁc depth, the exit distribution q and the oracle distribution q∗ are independent of
                                    the time-step so we drop subscript t. We condition the exit on the source sequence by feeding the
                                    average s of the encoder outputs to a multinomial classiﬁer:
                                                                 s = 1 Xs , q(n|x)=softmax(W s+b )∈RN,                                                               (8)
                                                                       |x|         t                                   h       h
                                                                              t
                                    where W andb aretheweights and biases of the halting mechanism. We consider two oracles to
                                                 h        h
                                    determine which of the N blocks should be chosen. The ﬁrst is based on the sequence likelihood
                                    and the second looks at an aggregate of the correctly predicted tokens at each block.
                                    Likelihood-based:            This oracle is based on the likelihood of the entire sequence after each block
                                    andweoptimizeitwiththeDiracdeltacenteredaroundtheexitwiththehighestsequencelikelihood.
                                                                                     ∗                                n
                                                                                   q (x,y) = δ(argmaxLL ).
                                                                                                             n
                                    Weaddaregularization term to encourage lower exits that achieve good likelihood:
                                                                                 ∗                                n
                                                                                q (x,y) = δ(argmaxLL −λn).                                                           (9)
                                                                                                          n
                                    Correctness-based:             Likelihood ignores whether the model already assigns the highest score to
                                    the correct target. Instead, this oracle chooses the lowest block that assigns the largest score to
                                    the correct prediction. For each block, we count the number of correctly predicted tokens over
                                    the sequence and choose the block with the most number of correct tokens. A regularization term
                                    controls the trade-off between speed and accuracy.
                                                       n                                       n             ∗                               n
                                                    C =#{t|yt =argmaxp(y|h                          )},     q (x,y) = δ(argmaxC −λn).                              (10)
                                                                                   y           t−1                                    n
                                    Oracles based on test metrics such as BLEU are feasible but expensive to compute since we would
                                    need to decode every training sentence N times. We leave this for future work.
                                     3.2     TOKEN-SPECIFIC DEPTH:
                                    Thetoken-speciﬁc approach can choose a different exit at every time-step. We consider two options
                                    for the exit distribution q at time-step t: a multinomial with a classiﬁer conditioned on the ﬁrst
                                                                       t
                                                                   1                                                                   n
                                    decoder hidden state h and a geometric-like where an exit probability χ                               is estimated after each
                                                                   t                                      n                            t
                                    block based on the activations of the current block h .
                                                                                                          t
                                                                                                      4
                             Multinomial q :
                                             t
                                                             q (n|x,y     ) = softmax(W h1 +b ),                                  (11)
                                                              t        <t                  h t      h
                             Themostprobable exit argmaxq (n|x,y             ) is selected at inference.
                                                                 t        <t
                             Geometric-like q :
                                                t
                                                         ∀n∈[1..N−1], χn = sigmoid(w>hn+b ),                                      (12)
                                                                          t                  h t     h
                                                                                 Q          0
                                                                             n            n
                                                                         χ         (1−χ ), if n<N
                                                                             t            t
                                                                                 0
                                                                               n <n
                                                         qt(n|x,y<t)=        Q           0                                        (13)
                                                                                (1−χn ), otherwise
                                                                                       t
                                                                             0
                                                                            n <N
                             where, d is the dimension of the decoder states, W         ∈ RN×d and w ∈ Rd are the weights of
                                                                                     h                   h
                             the halting mechanisms, and bh their biases. During inference the decoder exits when the halting
                             signal χn exceeds a threshold τ which we tune on the valid set to achieve a better accuracy-speed
                                      t                        n
                             trade-off. If thresholds (τ )         have not been exceeded, then we default to exiting at block N.
                                                        n 1≤n<N
                             Thetwoclassiﬁersaretrainedtominimizethecross-entropywithrespecttoeitheronethefollowing
                             oracle distributions:
                             Likelihood-based:      At each time-step t, we choose the block whose exit classiﬁer has the highest
                             likelihood plus a regularization term weighted by λ to encourage lower exits.
                                                                 ∗                          n
                                                                q (x,y) = δ(argmaxLL −λn)                                         (14)
                                                                 t                   n      t
                             This oracle ignores the impact of the current decision on the future time-steps and we therefore
                             consider smoothing the likelihoods with an RBF kernel.
                                                    0 2             |y|
                                        0     −|t−t |         n    X 0 n ∗                                          n
                                                   σ       g                                                     g
                                  κ(t,t ) = e          ,  LL =         κ(t,t )LL 0,     q (x,y) = δ(argmaxLL −λn),                (15)
                                                              t                    t     t                   n      t
                                                                    0
                                                                   t =1
                             wherewecontrolthesizeofthesurroundingcontextwithσ thekernelwidth. Werefertothisoracle
                             as LL(σ,λ)includingthecasewhereweonlylookatthelikelihoodofthecurrenttokenwithσ → 0.
                             Correctness-based:       Similar to the likelihood-based oracle we can look at the correctness of the
                             prediction at time-step t as well as surrounding positions. We deﬁne the target q∗ as follows:
                                                                                                                  t
                                                                                                  |y|
                                                    n                            n          n    X 0 n
                                                                                          f
                                                  C =1[y =argmaxp(y|h               )],   C =         κ(t,t ) C ,                 (16)
                                                    t        t         y         t−1        t                   t
                                                                                                 t0=1
                                                   ∗                         n
                                                                           f
                                                  q (x,y) = δ(argmaxC −λn).                                                       (17)
                                                   t                   n    t
                             Conﬁdence thresholding         Finally, we consider thresholding the model predictions (§2), i.e., exit
                                                                                                      n
                             when the maximum score of the current output classiﬁer p(yt+1|h ) exceeds a hyper-parameter
                                                                                                      t
                             threshold τ . This does not require training and the thresholds τ = (τ ,...,τ               ) are simply
                                         n                                                                  1        N−1
                             tuned on the valid set to maximize BLEU. Concretely, for 10k iterations, we sample a sequence of
                                                      N−1
                             thresholds τ ∼ U(0,1)         , decode the valid set with the sampled thresholds and then evaluate the
                             BLEUscoreandcomputational cost achieved with this choice of τ. After 10k evaluations we pick
                             the best performing thresholds, that is τ with the highest BLEU in each cost segment.
                             4    EXPERIMENTS
                             4.1    EXPERIMENTAL SETUP
                             Weevaluate on several benchmarks and measure tokenized BLEU (Papineni et al., 2002):
                             IWSLT’14 German to English (De-En). We use the setup of Edunov et al. (2018) and train on
                             160Ksentencepairs. WeuseN = 6blocks,afeed-forwardnetwork(ffn)ofintermediate-dimension
                                                                                 5
                                                   Uniform     n=1 n=2 n=3 n=4 n=5 n=6 Average
                                   Baseline            -        34.2     35.3     35.6     35.7     35.6      35.9      35.4
                              Aligned (ωn = 1)       35.5       34.1     35.5     35.8     36.1     36.1      36.2      35.6
                                MixedM =1            34.1       32.9     34.3     34.5     34.5     34.6      34.5      34.2
                                MixedM =3            35.1       33.9     35.2     35.4     35.5     35.5      35.5      35.2
                                MixedM =6            35.3       34.2     35.4     35.8     35.9     35.8      35.9      35.5
                            Table 1: Aligned vs. mixed training on IWSLT De-En. We report valid BLEU for a uniformly
                            sampledexitn ∼ U([1..6])ateachtoken,aﬁxedexitn ∈ [1..6]foralltokens, aswellastheaverage
                            BLEUovertheﬁxedexits. Asbaseline we show six standard Transformer models with 1-6 blocks.
                            1024, 4 heads, dropout 0.3, embedding dimension d        =512for the encoder and d        =256for
                                                                                 enc                              dec
                            the decoder. Embeddings are untied with 6 different output classiﬁers. We evaluate with a single
                            checkpoint and a beam of width 5.
                            WMT’14English to French (En-Fr). We also experiment on the much larger WMT’14 English-
                            French task comprising 35.5m training sentence pairs. We develop on 26k held out pairs and test
                            on newstest14. The vocabulary consists of 44k joint BPE types (Sennrich et al., 2016). We use
                            a Transformer big architecture and tie the embeddings of the encoder, the decoder and the output
                            classiﬁers ((W )        ; §2.1). We average the last ten checkpoints and use a beam of width 4.
                                           n 1≤n≤6
                            Models are implemented in fairseq (Ott et al., 2019) and are trained with Adam (Kingma & Ba,
                            2015). We train for 50k updates on 128 GPUs with a batch size of 460k tokens for WMT’14 En-Fr
                            andon2GPUswith8ktokensperbatchforIWSLT’14De-En. Tostabilizetraining,were-normalize
                            the gradients if the norm exceeds g    =3.
                                                               clip
                            For models with adaptive exits, we ﬁrst train without exit prediction (α = 0 in Eq. (7)) using the
                            aligned mode (c.f. §2.2.1) for 50k updates and then continue training with α 6= 0 until convergence.
                            Theexitpredictionclassiﬁersareparameterizedbyasinglelinearlayer(Eq.(8))withthesameinput
                            dimensionastheembeddingdimension,e.g.,1024forabigTransformer;theoutputdimensionisN
                            for a multinomial classiﬁer or one for geometric-like. We exit when χt,n > 0.5 for geometric-like
                            classiﬁers.
                            4.2   TRAINING MULTIPLE OUTPUT CLASSIFIERS
                            We ﬁrst compare the two training regimes for our model (§2.2). Aligned training performs self-
                            attention on aligned states (§2.2.1) and mixed training exposes self-attention to hidden states from
                            different blocks (§2.2.2).
                            Wecompare the two training modes when choosing either a uniformly sampled exit or a ﬁxed exit
                            n=1,...,6atinferencetimeforeverytime-step. Thesampledexitexperimentteststherobustness
                            to mixed hidden states and the ﬁxed exit setup simulates an ideal setting where all previous states
                            are available. As baselines we show six separate standard Transformers with N ∈ [1..6] decoder
                            blocks. All models are trained with an equal number of updates and mixed training with M=6 paths
                            is most comparable to aligned training since the number of losses per sample is identical.
                            Table1showsthatalignedtrainingoutperformsmixedtrainingbothforﬁxedexitsaswellasforran-
                            domly sampled exits. The latter is surprising since aligned training never exposes the self-attention
                            mechanismtohiddenstatesfromotherblocks. Wesuspectthatthisisduetotheresidualconnections
                            whichcopyfeaturesfromlowerblockstosubsequentlayersandwhichareubiquitousinTransformer
                            models (§2). Aligned training also performs very competitively to the individual baseline models.
                            Aligned training is conceptually simple and fast. We can process a training example with N exits
                            in a single forward/backward pass while M passes are needed for mixed training. In the remaining
                            paper, we use the aligned mode to train our models. Appendix A reports experiments with weighing
                            the various output classiﬁers differently but we found that a uniform weighting scheme worked
                            well. On our largest setup, WMT’14 English-French, the training time of an aligned model with six
                            output classiﬁers increases only marginally by about 1% compared to a baseline with a single output
                            classiﬁer keeping everything else equal.
                                                                             6
                                                         (a) Token-specﬁc                             (b) Sequence-speciﬁc depth                          (c) Conﬁdence thresholding
                                                35.5                                                 35.5                                                35.5
                                                35.0                                                 35.0                                                35.0
                                                34.5                                                 34.5                                                34.5
                                              BLEU                     Baseline                    BLEU                                                BLEU
                                                34.0                   Aligned                       34.0                                                34.0                  Baseline
                                                                       Tok-CMultinomial                                                  Baseline                              Aligned
                                                                       Tok-LLMultinomial                                                 Aligned                               Tok-CGeometric-like
                                                33.5                   Tok-CGeometric-like           33.5                                Seq-LL          33.5                  Tok-LLGeometric-like
                                                                       Tok-LLGeometric-like                                              Seq-C                                 Conﬁdencethresholding
                                                     1       2       3       4       5       6           1       2       3       4       5       6            1       2       3       4      5       6
                                                                 Average exit (AE)                                   Average exit (AE)                                   Average exit (AE)
                                           Figure 3: Trade-off between speed (average exit or AE) and accuracy (BLEU) for depth-adaptive
                                           methods on the IWSLT14 De-En test set.
                                                                                 6                            σ →0                6
                                                                                 5                             σ = 1
                                                                              (AE)                                             5.5
                                                                                 4                                           (AE)
                                                                              xit                                            xit  5
                                                                              e                                              e
                                                                                 3                                             4.5
                                                                              erage                                          erage
                                                                              v  2                                           v    4
                                                                              A                                              A                             λ=0.01
                                                                                 1                                             3.5                         λ=0.05
                                                                                     0           0.2         0.4                      0         1         2         3
                                                                                    Regularization parameter λ                            RBFkernelwidthσ
                                                                                   (a) Effect of λ on AE                          (b) Effect of σ on AE
                                           Figure 4: Effect of the hyper-parameters σ and λ on the average exit (AE) measured on the valid set
                                           of IWSLT’14 De-En.
                                           4.3       ADAPTIVE DEPTH ESTIMATION
                                           Next, we train models with aligned states and compare adaptive depth classiﬁers in terms of BLEU
                                           as well as computational effort. We measure the latter as the average exit per output token (AE).
                                           Asbaselines we use again six separate standard Transformers with N ∈ [1..6] with a single output
                                           classiﬁer. We also measure the performance of the aligned mode trained model for ﬁxed exits n ∈
                                           [1..6]. For the adaptive depth token-speciﬁc models (Tok), we train four combinations: likelihood-
                                           based oracle (LL) + geometric-like, likelihood-based oracle (LL) + multinomial, correctness based
                                           oracle (C) + geometric-like and correctness-based oracle (C) + multinomial. Sequence-speciﬁc
                                           models(Seq)aretrainedwiththecorrectnessoracle(C)andthelikelihoodoracle(LL)withdifferent
                                           values for the regularization weight λ. All parameters are tuned on the valid set and we report results
                                           onthe test set for a range of average exits.
                                           Figure 3 shows that the aligned model (blue line) can match the accuracy of a standard 6-block
                                           Transformer (black line) at half the number of layers (n = 3) by always exiting at the third block.
                                           Thealigned model outperforms the baseline for n = 2,...,6.
                                           For token speciﬁc halting mechanisms (Figure 3a) the geometric-like classiﬁers achieves a better
                                           speed-accuracy trade-off than the multinomial classiﬁers (ﬁlled vs. empty triangles). For geometric-
                                           like classiﬁers, the correctness oracle outperforms the likelihood oracle (Tok-C geometric-like vs.
                                           Tok-LLgeometric-like) but the trend is less clear for multinomial classiﬁers. At the sequence-level,
                                           likelihood is the better oracle (Figure 3b).
                                           The rightmost Tok-C geometric-like point (σ = 0, λ = 0.1) achieves 34.73 BLEU at AE = 1.42
                                           which corresponds to similar accuracy as the N = 6 baseline at 76% fewer decoding blocks.
                                                                                                                         7
                                                                               (a) BLEU vs. AE (test)                           (b) BLEUvs. FLOPs(test)
                                                                          44.0                                                44.0
                                                                          43.5                                                43.5
                                                                          43.0                                                43.0
                                                                        BLEU42.5                Baseline                    BLEU42.5                 Baseline
                                                                                                Aligned                                              Aligned
                                                                                                Seq-LL                                               Seq-LL
                                                                          42.0                  Tok-CPoisson                  42.0                   Tok-CPoisson
                                                                                                Tok-LLPoisson                                        Tok-LLPoisson
                                                                          41.5                  Conﬁdencethresholding         41.5                   Conﬁdencethresholding
                                                                               1       2       3       4      5       6                   2         3        4         5
                                                                                           Average exit (AE)                                    Average FLOPs         ·108
                                                   Figure 5: Speed and accuracy on the WMT’14 English-French benchmark (c.f. Figure 3).
                                           The best accuracy of the aligned model is 34.95 BLEU at exit 5 and the best comparable Tok-C
                                           geometric-like conﬁguration achieves 34.99 BLEU at AE = 1.97, or 61% fewer decoding blocks.
                                           When ﬁxing the budget to two decoder blocks, Tok-C geometric-like with AE = 1.97 achieves
                                           BLEU35, a 0.64 BLEU improvement over the baseline (N = 2) and aligned which both achieve
                                           BLEU 34.35.
                                           Conﬁdence thresholding (Figure 3c) performs very well but cannot outperform Tok-C geometric-
                                           like.
                                           Ablation of hyper-parameters                            In this section, we look at the effect of the two main hyper-
                                           parameters on IWSLT’14 De-En: λ the regularization scale (c.f. Eq. (9)), and the RBF kernel width
                                           σ used to smooth the scores (c.f. Eq. (15)). We train Tok-LL Geometric-like models and evaluate
                                                                                                                n
                                           them with their default thresholds (exit if χ                            >0.5). Figure 4a shows that higher values of λ lead
                                                                                                                t
                                           to lower exits. Figure 4b shows the effect of σ for two values of λ. In both curves, we see that wider
                                           kernels favor higher exits.
                                           4.4       SCALING THE ADAPTIVE-DEPTH MODELS
                                           Finally, we take the best performing models form the IWSLT benchmark and test them on the large
                                           WMT’14English-French benchmark. Results on the test set (Figure 5a) show that adaptive depth
                                           still shows improvements but that they are diminished in this very large-scale setup. Conﬁdence
                                           thresholding works very well and sequence-speciﬁc depth approaches improve only marginally over
                                           the baseline. Tok-LL geometric-like can match the best baseline result of BLEU 43.4 (N = 6) by
                                           using only AE = 2.40 which corresponds to 40% of the decoder blocks; the best aligned result
                                           of BLEU 43.6 can be matched with AE = 3.25. In this setup, Tok-LL geometric-like slightly
                                           outperforms the Tok-C counterpart.
                                           Conﬁdence thresholding matches the accuracy of the N=6 baseline with AE 2.5 or 59% fewer de-
                                           coding blocks. However, conﬁdence thresholding requires computing the output classiﬁer at each
                                           block to determine whether to halt or continue. This is a large overhead since output classiﬁers pre-
                                           dict 44k types for this benchmark (§4.1). To better account for this, we measure the average number
                                           of FLOPsperoutputtoken(detailsinAppendixB).Figure5bshowsthattheTok-LLgeometric-like
                                           approach provides a better trade-off when the overhead of the output classiﬁers is considered.
                                           4.5       QUALITATIVE RESULTS
                                           The exit distribution for a given sample can give insights into what a Depth-Adaptive Transformer
                                                                                                                                                                          e
                                           decoder considers to be a difﬁcult task. In this section, for each hypothesis y, we will look at
                                           the sequence of selected exits (n ,...,n                               ) and the probability scores (p ,...p                             ) with p         =
                                                                                                 1             e                                                     1           e               t
                                                     nt                                                       |y|                                                               |y|
                                           p(ye |h         ) i.e. the conﬁdence of the model in the sampled token at the selected exit.
                                                 t   t−1
                                           Figures 6 and 7 show hypotheses from the WMT’14 En-Fr and IWSLT’14 De-En test sets, respec-
                                           tively. For each hypothesis we state the exits and the probability scores. In Figure 6a, predicting
                                                                                                                         8
                                      6                                                  1      6                                                              1
                                      5                                                  0.8    5                                                              0.8
                                      4                                                  0.6    4                                                              0.6
                                      3                                                         3
                                     Exit2                                         >     0.4    2                                                         >    0.4   Score
                                                                                   s                                 vraient                              s
                                      1    Chi@@rac,   le  Premierministre,étaitprésent.</0.2   1    Maisles passagersnedepass’   attendreàdeschangementsimmédiats.</0.2
                                  (a) Src: Chi@@rac , the Prime Minister , was there . (b) Src: But passengers shoul@@dn’t expect changes to
                                      Ref: Chi@@rac , Premier ministre , est là .                 happen immediately .
                                                                                                  Ref: Mais les passagers ne devraient pas s’ attendre à
                                                                                                  des changements immédiats .
                                  Figure6: ExamplesfromtheWMT’14En-Frtestset(newstest14)withTok-LLgeometric-likedepth
                                  estimation. Token exits are in blue and conﬁdence scores are in gray. The ‘@@’ are due to BPE or
                                  subword tokenization. For each example the source (Src) and the reference (Ref) are provided in
                                  the caption.
                                                         6                                                                          1
                                                         5                                                                          0.8
                                                         4                                                                          0.6
                                                         3
                                                        Exit                                                                   >    0.4   Score
                                                         2                                                                     s
                                                                                                                               /    0.2
                                                         1   you  can per@@formthistricktoyourfriendsandneighb@@ors.thankyou.  <
                                               (a) Src: diesen trick können sie ihren freunden und nachbarn vor@@führen . danke .
                                                   Ref: there is a trick you can do for your friends and neighb@@ors . thanks .
                                  Figure 7: ExamplefromtheIWSLT’14De-EntestsetwithTok-LLgeometric-likedepthestimation.
                                  See Figure 6 for more details.
                                  ‘présent’(meaning‘present’)ishard. Astraightforwardtranslationis‘étaitlà’butthemodelchooses
                                  ‘present’ which is also appropriate. In Figure 6b, the model uses more computation to predict the
                                  deﬁnite article ‘les’ since the source has omitted the article for ‘passengers’.
                                  Aclear trend in both benchmarks is that the model requires less computation near the end of de-
                                  coding to generate the end of sequence marker </s> and the preceding full-stop when relevant. In
                                  Figure 8, we show the distribution of the exits at the beginning and near the end of test set hypothe-
                                  ses. We consider the beginning of a sequence to be the ﬁrst 10% of tokens and the end as the last
                                  10%oftokens. The exit distributions are shown for three models on WMT’14 En-Fr: Model1 has
                                  anaverageexit of AE = 2.53, Model exits at AE = 3.79 on average and Model with AE = 4.68.
                                                                                 2                                                     3
                                  Within the same models, deep exits late are used at the beginning of the sequence and early exits are
                                  selected near the end. For heavily regularized models such as Model with AE = 2.53, the disparity
                                                                                                                       1
                                  between beginning and end is less severe as the model exits early most of the time. Model2 and
                                  Model are less regularized (higher AE) and tend to use late exits at the beginning of the sequence
                                           3
                                  and early exits near the end. On the other hand, the more regularized Model1 with AE = 2.53 exits
                                                                      Model1                     Model2                     Model3
                                                        0.6                 Beginning
                                                                            End
                                                        0.4
                                                     requency0.2
                                                     F
                                                              1    2   3    4   5    6   1    2   3    4    5   6    1   2    3   4    5   6
                                                                                                   Exit
                                  Figure 8: WMT’14 En-Fr test set: exit distributions in the beginning (relative-position: rpos<0.1)
                                  and near the end (rpos>0.9) of the hypotheses of three models.
                                                                                                9
                                                            Model        Model          Model
                                                                 1             2             3
                                                  0.9-1.0
                                                  0.8-0.9
                                                  0.7-0.8
                                                  0.6-0.7
                                                  0.5-0.6
                                                  0.4-0.5
                                                 Conﬁdence0.3-0.4
                                                  0.2-0.3
                                                  0.1-0.2
                                                  0.0-0.1
                                                         1 2 3 4 5 6   1 2 3 4 5 6   1 2 3 4 5 6
                                                                           Exit
                          Figure 9: Joint histogram of the exits and the conﬁdence scores for 3 Tok-LL geometric-like models
                          onnewstest14.
                          early most of the time. There is also a correlation between the model probability and the amount
                          of computation, particularly in models with low AE. Figure 9 shows the joint histogram of the
                          scores and the selected exit. For both Model and Model , low exits (n ≤ 2) are used in the high
                                                                    1           2
                          conﬁdence range [0.8 − 1] and high exits (n ≥ 4) are used in the low-conﬁdence range [0 − 0.5].
                          Model has a high average exit (AE = 4.68) so most tokens exit late, however, in low conﬁdence
                                3
                          ranges the model does not exit earlier than n = 5.
                          5   CONCLUSION
                          Weextendedanytimepredictiontothestructuredpredictionsettingandintroducedsimplebuteffec-
                          tive methods to equip sequence models to make predictions at different points in the network. We
                          compared a number of different mechanisms to predict the required network depth and ﬁnd that a
                          simple correctness based geometric-like classiﬁer obtains the best trade-off between speed and ac-
                          curacy. Results show that the number of decoder layers can be reduced by more than three quarters
                          at no loss in accuracy compared to a well tuned Transformer baseline.
                          ACKNOWLEDGMENTS
                          WethankLaurensvanderMaatenforfruitful comments and suggestions.
                          REFERENCES
                          Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural networks
                            for efﬁcient inference. In Proc. of ICML, 2017.
                          M.Cettolo,J.Niehues,S.Stüker,L.Bentivogli,andM.Federico. Reportonthe11thiwsltevaluation
                            campaign. In IWSLT, 2014.
                          Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal
                            transformers. In Proc. of ICLR, 2018.
                          Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
                            bidirectional transformers for language understanding. In Proc. of NAACL, 2019.
                          Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc’Aurelio Ranzato. Classical
                            structured prediction losses for sequence to sequence learning. In Proc. of NAACL, 2018.
                          Michael Figurnov, Artem Sobolev, and Dmitry P. Vetrov. Probabilistic adaptive computation time.
                            In ArXiv preprint, 2017.
                          Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional
                            sequence to sequence learning. In Proc. of ICML, 2017.
                          Alex Graves. Adaptive computation time for recurrent neural networks. In ArXiv preprint, 2016.
                                                                       10
           GaoHuang,DanluChen,TianhongLi,FelixWu,LaurensvanderMaaten,andKilianQWeinberger.
            Multi-scale dense networks for resource efﬁcient image classiﬁcation. In Proc. of ICLR, 2017.
           D. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. of ICLR, 2015.
           NathanNg,KyraYee,AlexeiBaevski,MyleOtt,MichaelAuli,andSergeyEdunov. Facebookfair’s
            wmt19newstranslation task submission. In Proc. of WMT, 2019.
           Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
            and Michael Auli. Fairseq: A fast, extensible toolkit for sequence modeling. In Proc. of NAACL,
            2019.
           K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: a method for automatic evaluation of
            machine translation. In Proc. of ACL, 2002.
           Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
            models are unsupervised multitask learners. In Technical report, OpenAI., 2019.
           R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword
            units. In Proc. of ACL, 2016.
           Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via
            early exiting from deep neural networks. In ICPR, 2016.
           A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, and I. Polosukhin.
            Attention is all you need. In Proc. of NeurIPS, 2017.
           Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dy-
            namic routing in convolutional networks. In Proc. of ECCV, 2018.
                               11
                            APPENDIX A LOSS SCALING
                            In this section we experiment with different weights for scaling the output classiﬁer losses. Instead
                            ofuniformweighting,webiastowardsspeciﬁcoutputclassiﬁersbyassigninghigherweightstotheir
                            losses. Table 2 shows that weighing the classiﬁers equally provides good results.
                                                Uniform     n=1 n=2 n=3 n=4 n=5 n=6 Average
                                  Baseline          -        34.2     35.3     35.6      35.7     35.6     35.9       35.4
                                   ω =1           35.5       34.1     35.5     35.8      36.1     36.1     36.2       35.6
                                   ωn=n           35.3       32.2     35.0     35.8      36.0     36.2     36.3       35.2
                                    n √
                                  ωn =    n       35.4       33.3     35.2     35.8      35.9     36.1     36.1       35.4
                                         √
                                ω =1/ n           35.6       34.5     35.4     35.7      35.8     35.8     35.9       35.5
                                  n
                                 ωn = 1/n         35.3       34.7     35.3     35.5      35.7     35.8     35.8       35.5
                                                                   (a) IWSLT De-En - Valid
                                                Uniform     n=1 n=2 n=3 n=4 n=5 n=6 Average
                                  Baseline          -        33.7     34.6     34.6      34.6     34.6     34.8       34.5
                                   ω =1           34.4       33.2     34.4     34.8      34.9     35.0     34.9       34.5
                                    n
                                   ω =n           34.2       31.4     33.8     34.7      34.8     34.8     34.9       34.1
                                    n √
                                  ωn =    n       34.4       32.5     34.1     34.8      34.9     35.0     35.1       34.4
                                         √
                                ω =1/ n           34.6       33.7     34.3     34.6      34.8     34.8     34.9       34.5
                                  n
                                 ωn = 1/n         34.2       33.8     34.3     34.5      34.6     34.7     34.7       34.4
                                                                    (b) IWSLT De-En - Test
                            Table 2: Aligned training with different weights (ωn) on IWSLT De-En. For each model we report
                            BLEUonthedevset evaluated with a uniformly sampled exit n ∼ U([1..6]) for each token and a
                            ﬁxed exit n ∈ [1..6] throughout the sequence. The average corresponds to the average BLEU over
                            the ﬁxed exits.
                            Gradient scaling     Adding intermediate supervision at different levels of the decoder results in
                            richer gradients for lower blocks compared to upper blocks. This is because earlier layers affect
                            more loss terms in the compound loss of Eq. (4). To balance the gradients of each block in the
                            decoder, we scale up the gradients of each loss term (−LL ) when it is updating the parameters
                                                                                          n
                            of its associated block (block    with parameters θ ) and revert it back to its normal scale before
                                                           n                    n
                            back-propagating it to the previous blocks. Figure 10 and Algorithm 1 illustrate this gradient scaling
                            procedure. Theθ areupdatedwithγ -ampliﬁedgradientsfromtheblock’ssupervisionand(N−n)
                                              n                   n
                            gradients from the subsequent blocks. We choose γn = γ(N − n) to control the ratio γ:1 as the
                            ratio of the block supervision to the subsequent blocks’ supervisions.
                            Table 3 shows that gradient scaling can beneﬁt the lowest layer at the expense of higher layers.
                            However, no scaling generally works very well.
                                             γ ∇LL         γ    ∇LL             γ    ∇LL                    γ ∇LL
                                              n      n      n+1      n+1         n+2      n+2                N      N
                                      n−1     block ; θ       block     ; θ        block     ; θ      . . .   block ; θ
                                    h               n   n           n+1   n+1            n+2   n+2                 N N
                                                       ∇LL                   ∇LLn+2                 ∇LL
                                                            n+1                                          N
                                                                              ∇LL
                                                        ∇LLn+2                     N
                                                         ∇LLN
                                                          Figure 10: Illustration of gradient scaling.
                                                                              12
                          Algorithm 1 Pseudo-code for gradient scaling (illustrated for a single step t)
                           1: for n ∈ 1..N do  n−1
                           2:    hn = block (h     )
                                  t         n  t
                                         n                  n
                           3:    p(yt+1|h ) = softmax(Wnh )
                                         t                  t
                                         n                                n
                           4:    p(y   |h ) = SCALE_GRADIENT(p(y        |h ),γ )
                                    t+1  t                           t+1  t   n
                                                n                        n   1
                           5:    if n < N then h = SCALE_GRADIENT(h ,            )
                                                t                        t  γ
                           6: end for                                        n+1
                           7: function SCALE_GRADIENT(Tensor x, scale γ)
                           8:    return γx+(1−γ)STOP_GRADIENT(x)
                           9:    . STOP_GRADIENT in PyTorch with x.detach().
                          10: end function
                                          Uniform    n=1 n=2 n=3 n=4 n=5 n=6 Average
                                Baseline      -       34.2    35.3     35.6    35.7    35.6     35.9     35.4
                                   ∅        35.5      34.1    35.5     35.8    36.1    36.1     36.2     35.6
                                γ = 0.3     35.1      33.7    34.7     35.3    35.7    35.8     36.0     35.2
                                γ = 0.5     35.4      34.8    35.4     35.6    35.6    35.7     35.6     35.4
                                γ = 0.7     34.9      34.6    35.1     35.1    35.2    35.4     35.3     35.1
                                γ = 0.9     34.9      34.8    35.3     35.3    35.3    35.4     35.5     35.3
                                γ = 1.1     35.1      34.9    35.2     35.3    35.3    35.3     35.3     35.2
                                                             (a) IWSLT De-En - Valid
                                          Uniform    n=1 n=2 n=3 n=4 n=5 n=6 Average
                                Baseline      -       33.7    34.6     34.6    34.6    34.6     34.8     34.5
                                   ∅        34.4      33.2    34.4     34.8    34.9    35.0     34.9     34.5
                                γ = 0.3     34.2      32.8    33.9     34.3    34.6    34.8     35.0     34.2
                                γ = 0.5     34.5      33.8    34.2     34.6    34.5    34.7     34.7     34.6
                                γ = 0.7     34.0      33.7    34.2     34.3    34.3    34.3     34.3     34.2
                                γ = 0.9     34.1      34.0    34.2     34.3    34.4    34.4     34.4     34.3
                                γ = 1.1     34.2      34.0    34.3     34.3    34.3    34.3     34.2     34.2
                                                             (b) IWSLT De-En - Test
                          Table 3: Aligned training with different gradient scaling ratios γ : 1 on IWSLT’14 De-En. For each
                          model we report the BLEU4 score evaluated with a uniformly sampled exit n ∼ U([1..6]) for each
                          token and a ﬁxed exit n ∈ [1..6]. The average corresponds to the average BLEU4 of all ﬁxed exits.
                                                                      13
                             APPENDIX B FLOPSAPPROXIMATION
                             This section details the computation of the FLOPS we report. The per token FLOPS are for the
                             decoder network only since we use an encoder of the same size for all models. We breakdown
                             the FLOPS of every operation in Algorithm 2 (blue front of the algorithmic statement). We omit
                             non-linearities, normalizations and residual connections. The main operations we account for are
                             dot-products and by extension matrix-vector products since those represent the vast majority of
                             FLOPS(weassumebatchsizeonetosimplifythecalculation).
                                     Parameters
                               dd    decoder embedding dimension.                           Operation    FLOPS
                               de    encoder embedding dimension.
                               df    Thefeed-forward network dimension.               Dot-product (d)    2d−1
                              |x|    source length.                                Linear d   →d         2d d
                                                                                           in      out      in out
                                 t   Current time-estep (t ≥ 1).
                                V    output vocabulary size.
                                Table 4: FLOPS of basic operations, key parameters and variables for the FLOPS estimation.
                             With this breakdown, the total computational cost at time-step t of a decoder block that we actually
                             gothrough, denoted with FC, is:
                                                              2
                                             FC(x,t) = 12d +4d d +4td +4|x|d +4[[FirstCall]]|x|d d ,
                                                              d      f d        d         d                      d e
                             where the cost of mapping the source’ keys and values is incurred the ﬁrst time the block is called
                             (ﬂagged with FirstCall). This occurs at t = 1 for the baseline model but it is input-dependent with
                             depth adaptive estimation and may never occur if all tokens exit early.
                             If skipped, a block still has to compute the keys and value of the self-attention block so the self-
                                                                                                                                   2
                             attention of future time-steps can function. We will denote this cost with FS and we have FS = 4d .
                                                                                                                                   d
                             Depending on the halting mechanism, an exit prediction cost, denoted wit FP, is added:
                                                       Sequence-speciﬁc depth:      FP(t,q(t)) = 2[[t = 1]]Ndd
                                                   Token-speciﬁc Multinomial:       FP(t,q(t)) = 2Ndd
                                                Token-speciﬁc Geometric-like:       FP(t,q(t)) = 2ddq(t)
                                                       Conﬁdencethresholding:       FP(t,q(t)) = 2q(t)Vdd
                                                                (i)                                   (i)
                             For a set of source sequences {x     }     and generated hypotheses {y      }    , the average ﬂops per
                             token is:                             i∈I                                    i∈I
                                                                 PP (i)                                
                              Baseline (N blocks):      P 1(i)          |y  | NFC(x(i),t)+2Vdd
                                                          i |y  |   i   t=1                                                              
                                                                 PP (i)
                                    Adaptive depth:     P 1(i)          |y  | q(t)FC(x(i),t) +(N −q(t))FS+FP(t,q(t))+2Vdd
                                                          i |y  |   i   t=1
                             In the case of conﬁdence thresholding the ﬁnal output prediction cost (2V dd) is already accounted
                             for in the exit prediction cost FP.
                                                                                14
                            Algorithm 2 Adaptive decoding with Tok-geometric-like
                             1: Input: source codes s, incremental state
                             2: Initialization: t = 1, y1 = <s>
                             3: for n ∈ 1...N do
                             4:     FirstCall[n] = True.   . A ﬂag signaling if the source’ keys and values should be evaluated.
                             5: end for
                             6: while yt 6= </s> do
                             7:     Embedthelast output token y .
                             8:     for n ∈ 1...N do              t
                             9:         . Self-attention.
                            10:         - Map the input into a key (k) and value (v). FLOPS=4d2
                            11:         - Map the input into a query q. FLOPS=2d2               d
                                                                                   d
                            12:         - Score the memory keys with q to get the attention weights α. FLOPS=4tdd
                            13:         - Map the attention output. FLOPS=2d2
                            14:         . Encoder-Decoder interaction.         d
                            15:         if FirstCall[n] then
                            16:            Mapthesourcestates into keys and values for the nth block. FLOPS=4|x|d d
                                                                                                                        e d
                            17:            FirstCall[n] = False
                            18:         endif
                            19:         - Map the input into a query q. FLOPS=2d2
                                                                                   d
                            20:         - Score the memory keys with q to get the attention weights α. FLOPS=4|x|dd
                            21:         - Map the attention output. FLOPS=2d2
                            22:         Feed-forward network. FLOPS=4d d d
                                                                           d f
                            23:         Estimate the halting probability χ   . FLOPS=2d
                            24:         if χ   >0.5then                   t,n             d
                                           t,n
                            25:            Exit the loop (Line 8)
                            26:         endif
                            27:     endfor
                            28:     if n < N then
                            29:         . Skipped blocks.
                            30:         for ns ∈ n +1...N do                                                        2
                            31:            Copyandmapthecopiedstateintoakey(k)andvalue(v). FLOPS=4dd
                            32:         endfor
                            33:     endif
                            34:     Project the ﬁnal state and sample a new output token. FLOPS=2V dd
                            35:     t++
                            36: end while
                                                                             15
