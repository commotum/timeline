                                                  Generative Pretraining from Pixels
                   MarkChen1 AlecRadford1 RewonChild1 JeffWu1 HeewooJun1 PrafullaDhariwal1 DavidLuan1
                                                                     Ilya Sutskever1
                                        Abstract                                ported strong results using a single layer of learned features
                    Inspired by progress in unsupervised representa-            (Coates et al., 2011), or even random features (Huang et al.,
                    tion learning for natural language, we examine              2014; May et al., 2017). The approach fell out of favor as
                    whether similar models can learn useful repre-              the state of the art increasingly relied on directly encoding
                    sentations for images. We train a sequence Trans-           prior structure into the model and utilizing abundant su-
                    formertoauto-regressively predict pixels, without           pervised data to directly learn representations (Krizhevsky
                    incorporatingknowledgeofthe2Dinputstructure.                et al., 2012; Graves & Jaitly, 2014). Retrospective study of
                    Despitetrainingonlow-resolutionImageNetwith-                unsupervised pre-training demonstrated that it could even
                    outlabels, weﬁndthataGPT-2scalemodellearns                  hurt performance in modern settings (Paine et al., 2014).
                    strong image representations as measured by lin-            Instead, unsupervised pre-training ﬂourished in a differ-
                    ear probing, ﬁne-tuning, and low-data classiﬁca-            ent domain. After initial strong results for word vectors
                    tion. On CIFAR-10, we achieve 96.3% accuracy                (Mikolov et al., 2013), it has pushed the state of the art
                    with a linear probe, outperforming a supervised             forward in Natural Language Processing on most tasks (Dai
                    WideResNet,and99.0%accuracywithfullﬁne-                     &Le, 2015; Peters et al., 2018; Howard & Ruder, 2018;
                    tuning, matching the top supervised pre-trained             Radford et al., 2018; Devlin et al., 2018). Interestingly, the
                    models. An even larger model trained on a mix-              training objective of a dominant approach like BERT, the
                    ture of ImageNet and web images is competitive              prediction of corrupted inputs, closely resembles that of the
                    with self-supervised benchmarks on ImageNet,                Denoising Autoencoder, which was originally developed for
                    achieving 72.0% top-1 accuracy on a linear probe            images.
                    of our features.                                            Asahigherdimensional,noisier,andmoreredundantmodal-
                                                                                ity than text, images are believed to be difﬁcult for genera-
               1. Introduction                                                  tive modeling. Here, self-supervised approaches designed to
                                                                                encourage the modeling of more global structure (Doersch
               Unsupervised pre-training played a central role in the resur-    et al., 2015) have shown signiﬁcant promise. A combination
               gence of deep learning. Starting in the mid 2000’s, ap-          of new training objectives (Oord et al., 2018), more recent
               proaches such as the Deep Belief Network (Hinton et al.,         architectures (Gomez et al., 2017), and increased model ca-
               2006) and Denoising Autoencoder (Vincent et al., 2008)           pacity (Kolesnikov et al., 2019) has allowed these methods
               were commonly used in neural networks for computer vi-           to achieve state of the art performance in low data settings
                                                                                   ´
               sion (Lee et al., 2009) and speech recognition (Mohamed          (Henaff et al., 2019) and sometimes even outperform super-
               et al., 2009). It was believed that a model which learned        vised representations in transfer learning settings (He et al.,
               the data distribution P(X) would also learn beneﬁcial fea-       2019; Misra & van der Maaten, 2019; Chen et al., 2020).
               tures for the subsequent supervised modeling of P(Y |X)          Given that it has been a decade since the original wave of
               (Lasserreetal.,2006;Erhanetal.,2010). However,advance-           generative pre-training methods for images and considering
               ments such as piecewise linear activation functions (Nair        their substantial impact in NLP, this class of methods is due
               &Hinton, 2010), improved initializations (Glorot & Ben-          for a modern re-examination and comparison with the recent
               gio, 2010), and normalization strategies (Ioffe & Szegedy,       progress of self-supervised methods. We re-evaluate genera-
               2015; Ba et al., 2016) removed the need for pre-training in      tive pre-training on images and demonstrate that when using
               order to achieve strong results. Other research cast doubt       a ﬂexible architecture (Vaswani et al., 2017), a tractable and
               on the beneﬁts of deep unsupervised representations and re-      efﬁcient likelihood based training objective (Larochelle &
                  1OpenAI, San Francisco, CA, USA. Correspondence to: Mark      Murray, 2011; Oord et al., 2016), and signiﬁcant compute
               Chen<mark@openai.com>.                                           resources (2048 TPU cores), generative pre-training is com-
                                                                                petitive with other self-supervised approaches and learns
