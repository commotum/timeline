                          Published as a conference paper at ICLR 2021
                          David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual confer-
                            ence on Computational learning theory, pp. 164–170, 1999.
                          Hossein Mobahi. Training recurrent neural networks by diffusion. CoRR, abs/1601.04114, 2016.
                            URLhttp://arxiv.org/abs/1601.04114.
                          Y. E. Nesterov. A method for solving the convex programming problem with convergence rate
                            o(1/k2). Dokl. Akad. Nauk SSSR, 269:543–547, 1983. URL https://ci.nii.ac.jp/
                            naid/10029946121/en/.
                          Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
                            digits in natural images with unsupervised feature learning. 2011.
                          BehnamNeyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
                            ization in deep learning. In Advances in neural information processing systems, pp. 5947–5956,
                            2017.
                          Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc V. Le, and Ruoming Pang.
                            Domain adaptive transfer learning with specialist models. CoRR, abs/1811.07056, 2018. URL
                            http://arxiv.org/abs/1811.07056.
                          Eric Arazo Sanchez, Diego Ortego, Paul Albert, Noel E. O’Connor, and Kevin McGuinness. Un-
                            supervised label noise modeling and loss correction.   CoRR, abs/1904.11238, 2019.    URL
                            http://arxiv.org/abs/1904.11238.
                          Nitish Shirish Keskar and Richard Socher. Improving Generalization Performance by Switching
                            from AdamtoSGD. arXive-prints, art. arXiv:1712.07628, December 2017.
                          Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
                            ter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.
                            arXiv e-prints, art. arXiv:1609.04836, September 2016.
                          Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
                            Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine
                            learning research, 15(1):1929–1958, 2014.
                          Xu Sun, Zhiyuan Zhang, Xuancheng Ren, Ruixuan Luo, and Liangyou Li. Exploring the Vul-
                            nerability of Deep Neural Networks: A Study of Parameter Corruption. arXiv e-prints, art.
                            arXiv:2006.05620, June 2020.
                          Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-
                            thinking the inception architecture for computer vision, 2015.
                          Mingxing Tan and Quoc V. Le. EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural
                            Networks. arXiv e-prints, art. arXiv:1905.11946, May 2019.
                          Colin Wei and Tengyu Ma. Improved sample complexities for deep neural networks and robust
                            classiﬁcation via an all-layer margin. In International Conference on Learning Representations,
                            2020.
                          Longhui Wei, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang, and Qi Tian. Circumventing
                            Outliers of AutoAugment with Knowledge Distillation. arXiv e-prints, art. arXiv:2003.11342,
                            March2020.
                          Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
                            value of adaptive gradient methods in machine learning. In Advances in neural information pro-
                            cessing systems, pp. 4148–4158, 2017.
                          HanXiao,KashifRasul,andRolandVollgraf. Fashion-mnist: anovelimagedatasetforbenchmark-
                            ing machine learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/
                            abs/1708.07747.
                          Yoshihiro Yamada, Masakazu Iwamura, and Koichi Kise.        Shakedrop regularization.  CoRR,
                            abs/1802.02375, 2018. URL http://arxiv.org/abs/1802.02375.
                                                                       12
