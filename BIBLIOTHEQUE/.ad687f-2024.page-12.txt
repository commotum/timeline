                A ProofofTheorem1                                    Thedistance calculation specifically compares the
                                                                                                                ˆ
                Proof. All we need is to prove that for each x ∈     original RoPE, f(·,·), to the scaled RoPE, f(·,·),
                  d                                                  with token positions beginning at zero. It aims to
                R , each n ∈ N\{0,··· ,L − 1} and each i =           quantify the alterations in position embedding due
                0,...,2c − 1 we can find m ∈ {0,··· ,L − 1} ,        to the scaling process.
                          ˜           ˜
                such that f(x,m) = f(x,n) . By definition, it is
                                  i          i                         In contrast, our feature gap metric is tailored
                equivalent to solving the equations:                 for a more practical and common scenario, where
                          (Rd    Wx) =(Rd Wx)                        modelsaretrainedorfine-tuned on short sequences
                             ˜         i      ˜        i
                             Θ,m              Θ,n                    using the already scaled RoPE embeddings. This
                for m, given i, n, and x.                            setting emphasizes the generalization gap of the
                   The RoPE feature matrix Rd       is defined as    RoPEfeatures between training and testing posi-
                                                Θ,m                  tion ranges. The core hypothesis is that a smaller
                block-diagonal with 2 × 2 blocks given by Equa-      discrepancy in the RoPE features of new token po-
                tion 3. Hence, given i, x and n, the equation re-
                ducestoequalityofalinearcombinationoftrigono-        sitions relative to those encountered during training
                metric functions:                                    correlates with enhanced model generalization to
                                                                     novel token positions. Our metric diverges from
                          ˜           ˜           ˜          ˜
                   acosmθi+bsinmθi =acosnθi+bsinnθi                  the “embedded vector distance” in two significant
                for a,b ∈ R, depending on x and i. This equality     aspects to better align with our use-case:
                                  ˜     ˜
                clearly holds if mθi − nθi is a multiple of 2π:         • The distance computation shifts to compare
                                        ˜                                 scaled RoPE across different token positions,
                                (m−n)θi =2πk,                             reflecting the operational context where train-
                for some k ∈ Z. By our construction, 2π is a              ing involves short sequences (train-short) and
                                                           ˜
                                                          θ               testing involves longer sequences (test-long).
                                                           i
                natural number. Hence, to finish the proof that we
                can solve our initial equation for m, we need to        • Wemodifythetokenposition ranges, k and j,
                showthat we can find integer k to satisfy:                to represent token positions observed during
                                                                        training (in-distribution) and testing (out-of-
                           n−2πk ∈{0,··· ,L−1}                            distribution), respectively, to directly measure
                                ˜
                                θi                                        the generalization gap.
                for n ∈ N\{0,··· ,L − 1}. This is where we             This adaptation of the metric allows for a more
                use the pre-critical dimension condition: for i =    targeted evaluation of the model’s ability to gener-
                0,...,2c − 1, by definition of c, we have the in-    alize across different token positional distributions,
                equality 0 ≤ 2π < L. Taking k = ⌊nθi⌋ will give      whichiscriticalinscenarioswheresequencelength
                              ˜                     2π
                              θi
                us the required range for m and hence finish the     varies significantly between training and deploy-
                proof.                                               ment.
                                                                     C DetailedExperimentSettings
                B ComparisonBetweenFeatureGapand                     In this section, we provide the detailed experi-
                     EmbeddedVectorDistance                          ment settings for both our synthetic task evalua-
                Our proposed feature gap metric, as defined in       tion on POSGEN and LLM-based evaluations on
                Equation 8, shares similarities with the “embed-     both upstream language modeling evaluation and
                ded vector distance” metric introduced by Xiong      downstream real-world application evaluations.
                et al. (2023):                                       C.1   Synthetic Task Evaluation on POSGEN
                      ˆ                                   ˆ
                 d(f,f) = max       min      dist[f(x,k),f(x,j)]     For the synthetic task experiments in Section 6.1.1,
                           x∈X k∈{0,···,N−1}                         we train a two-layer Transformer on each of the
                                        ˆ
                                j∈{0,···,N−1}                 (9)    subtasks, with each layer following the configu-
                               d
                where X ⊂ R represents the set of vectors re-        ration of a T5-Small model (Raffel et al., 2020).
                quiring positional embedding. This equation as-      For each subtask, we train the model with different
                sesses the discrepancy in Rotary Position Embed-     position embeddings on a training set with 10,000
                ding (RoPE) before and after a scaling operation.    sequence samples of length 64. The validation and
                                                                 597
