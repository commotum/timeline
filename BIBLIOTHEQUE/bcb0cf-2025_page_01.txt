                       Published as a conference paper at ICLR 2025
                       MINDOVERBODY: ADAPTIVE THINKING USING DYNAMIC
                       COMPUTATION
                                         ∗                                      ∗                            ∗
                        Mrinal Mathur                    BarakA.Pearlmutter                      Sergey Plis
                        TReNDSCenter†                    Dept of Computer Science                TReNDSCenter†
                        Georgia State University         MaynoothUniversity                      Georgia State University
                        Atlanta, GA, USA                 Co. Kildare, W23 A3HY, Ireland          Atlanta, GA, USA
                                                                     ABSTRACT
                                While the human brain efficiently handles various computations with a limited number of
                                neurons, traditional deep learning networks require a significant increase in parameters to
                                improve performance. Yet, these parameters are used inefficiently as the networks employ
                                the same amount of computation for inputs of the same size, regardless of the input’s
                                complexity. We address this inefficiency by introducing self-introspection capabilities to
                                the network, enabling it to adjust the number of used parameters based on the internal
                                representation of the task and adapt the computation time based on the task complexity.
                                This enables the network to adaptively reuse parameters across tasks, dynamically adjusting
                                computational effort to match input complexity. We demonstrate the effectiveness of this
                                method on language modeling and computer vision tasks. Notably, our model achieves
                                96.62% accuracy on ImageNet with just a three-layer network, surpassing much larger
                                ResNet-50 and EfficientNet. When applied to a transformer architecture, the approach
                                achieves 95.8%/88.7% F1 scores on the SQuAD v1.1/v2.0 datasets at negligible parameter
                                cost. These results showcase the potential for dynamic and reflective computation, con-
                                tributing to the creation of intelligent systems that efficiently manage resources based on
                                input data complexity.
                       1    INTRODUCTION
                       Thecomplexity and scale of deep learning models have skyrocketed, propelling significant advancements
                       across diverse fields involving images, text, and even robots. However, adaptation of computation to problem
                       difficulty is still one of the most challenging aspects in deep learning. Traditional architectures process inputs
                       through a fixed number of layers, which can waste computational resources on simple tasks or be insufficient
                       for more complex ones (Canziani et al., 2016; Wang et al., 2016; Bai et al., 2023). This one-size-fits-all
                       approach does not account for the varying difficulties of input data, leading to inefficiencies and suboptimal
                       performance (Huang et al., 2017b; Fregoso-Aparicio et al., 2021).
                       Drawinginspiration from the human brain’s dynamic allocation and reuse of neurons to efficiently handle
                       multiple tasks (Schulz & Gershman, 2019), we propose the Model INtrospection for a Dynamically adaptive
                       (MIND)model. TheMINDmodelincludestwonetworks: theprimaryprediction network and an auxiliary
                       introspection network. By assessing the representation of each input in the prediction model, the introspection
                       network determines the computational capacity to employ. This process involves selecting the number of
                       layers to iterate through in the prediction model, thereby adapting the computational effort to the complexity
                       of the input.
                          ∗Correspondence to: mmathur4@gsu.edu, barak@pearlmutter.net,s.m.plis@gmail.com
                          †TheGeorgia State University/Georgia Institute of Technology/Emory University Center for Translational Research in
                       Neuroimaging and Data Science (TReNDS Center).
                                                                       1
