             1536                            G.Hinton,S.Osindero,andY.-W.Teh
             Figure 5: A hybrid network. The top two layers have undirected connections
             and form an associative memory. The layers below have directed, top-down
             generativeconnectionsthatcanbeusedtomapastateoftheassociativememory
             toanimage.Therearealsodirected,bottom-uprecognitionconnectionsthatare
             usedtoinferafactorial representation in one layer from the binary activities in
             the layer below. In the greedy initial learning, the recognition connections are
             tied to the generative connections.
             aredirected.Theundirectedconnectionsatthetopareequivalenttohaving
             inﬁnitely many higher layers with tied weights. There are no intralayer
             connections, and to simplify the analysis, all layers have the same number
             of units. It is possible to learn sensible (though not optimal) values for the
             parametersW0byassumingthattheparametersbetweenhigherlayerswill
             be used to construct a complementary prior for W0. This is equivalent to
             assuming that all of the weight matrices are constrained to be equal. The
             task of learning W0 under this assumption reduces to the task of learning
             anRBM,andalthoughthisisstilldifﬁcult,goodapproximatesolutionscan
             befoundrapidlybyminimizingcontrastivedivergence.OnceW0 hasbeen
             learned, the data can be mapped through WT to create higher-level “data”
             at the ﬁrst hidden layer.            0
               If the RBMisaperfectmodeloftheoriginaldata,thehigher-level“data”
             willalreadybemodeledperfectlybythehigher-levelweightmatrices.Gen-
             erally,however,theRBMwillnotbeabletomodeltheoriginaldataperfectly,
             and we can make the generative model better using the following greedy
             algorithm:
