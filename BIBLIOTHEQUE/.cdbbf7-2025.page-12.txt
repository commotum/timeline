                                                                                           per scene compared to nuScenes’ 10 sweeps, resulting in
                                                                                           about 3,000 (Argoverse2) and 9,500 (Waymo) valid cells in
                                                                                           average for 180×180×11voxels,indicating rather higher
                                                                                           sparsity than 18,000 (nuScenes). Based on the statistics, we
                                                                                           believe that our approach is highly efficient for 3D object
                                                                                           detection using general LiDAR sensors.
                                                                                           B.2. Visualization of Feature Elimination
                                                                                           Fig. 6 visualizes the voxels removed through our feature
                 Figure 5. Histogram of valid cell counts per LiDAR sample (10             elimination scheme. As shown in the examples, the elimi-
                 sweeps) in the nuScenes train set, implying the distribution of the       nated features are primarily placed on backgrounds such as
                 count of valid voxel features with the voxel resolution of 180 ×          roads.
                 180 × 11. The red arrow denotes the number of cells for BEV               B.3. Qualitative Results
                 feature map with the resolution of 180 × 180. The average value
                 (blue arrow) for the sparse voxel features is much smaller than the       Fig. 7 shows visual 3D object detection results from a top
                 number of BEV features (red arrow), which is further reduced by           view. As demonstrated in the left and right examples, our
                 our additional feature sparsification (green arrow).                      approach can detect long-range small objects. In the mid-
                                                                                           dle example, it shows better handling of an occluded object
                 A.4. Additional Evaluation Detail                                         compared with CMT [39].
                 In the analysis of computational costs, we measure Flops of               C. Discussion of Limitation and Future Work
                 multi-modal 3D object detectors. To the best of our knowl-
                 edge, there has been no well-established approach to calcu-               Our voxel features are derived from voxels that contain at
                 late the computational complexity of multi-modal 3D ob-                   least one point. Furthermore, our multi-modal fusion ap-
                 ject detector and models with sparse data and sparse opera-               proach only combines LiDAR voxel features with their cor-
                 tions to our best knowledge. Therefore, we manually com-                  responding image features, resulting in sparse data. There-
                 puted them. Specifically, we separately inference LiDAR-                  fore, our approachmaybeunabletohandleanyregionwith-
                 only-based and camera-only-based branches, and manually                   out LiDAR points. Nevertheless, we demonstrate that our
                 combine their values. Furthermore, the computational cost                 approacheffectivelydetectslong-rangeobjectswithfewLi-
                 ofsparseconvolution-basedlayerscanvaryaccordingtothe                      DARpoints,comparedtoBEV-basedapproaches,asshown
                 sparsity of LiDAR data. We assume the number of valid in-                 in Table 6 in the main paper. Additionally, the sparsity of
                 termediate features as 20,000 in the layers for simplicity.               LiDAR data depends on the hardware specification of the
                                                                                           LiDAR sensor, meaning the efficiency of our model could
                 B. Additional Experiments                                                 vary. However, we believe that our additional feature elimi-
                 B.1. DeeperanalysisofLiDARstatisticsonVarious                             nation scheme provides a viable solution to this limitation.
                         LiDARSensors                                                         Despite our achievements, we believe there are further
                                                                                           future directions that could enhance this innovative ap-
                 Ourkeymotivation is based on the statistic that the number                proach, as we have presented several sparse feature-specific
                 of valid transformer tokens in sparse 3D voxel features is                designs. We have focused on transformer keys in this work,
                 significantly smaller than the number of 2D dense grids in                but exploring new query designs like iterative query re-
                 the BEVspace.Inthissection,weconfirmthatthisassump-                       finement could be interesting. Other research areas could
                 tion generally holds across different datasets using various              include using sparse features for different 3D perception
                 LiDARsensors.Fig. 5 visualizes the LiDAR statistics from                  tasks, not just 3D object detection.
                 the nuScenes dataset [3], showing that sparse 3D voxel fea-
                 tures yield fewer tokens than dense BEV features. However,                Potential negative societal impact          3D object detection
                 in this case, the number of tokens for sparse features can                is a crucial task for autonomous driving. In the current
                 vary due to differing sparsity in each scene. We emphasize                paradigm, the planning of autonomous vehicles often relies
                 againthatourfeatureeliminationschemestandardizesthese                     ontheperformanceof3Dobjectdetection.Asourapproach
                 varying numberstoaconstant,suchas10,000,whichisfur-                       enhances the performance of 3D object detection, it can be
                 ther reduced at the same time.                                            utilized to improve the overall performance of autonomous
                     WealsoanalyzedLiDARstatisticsfromArgoverse2[38]                       driving. However, 3Dobjectdetectionmodelsmaystillpro-
                 andWaymoopen[31]datasets(Table.9).Weadoptedcom-                           duce errors when encountering corner cases, subsequently
                 mon configurations of using the datasets shown in Voxel-                  posing a potential risk of influencing incorrect decisions in
                 NeXT[6]andDSVT[33].TheyuseasingleLiDARsweep                               autonomous vehicles.
