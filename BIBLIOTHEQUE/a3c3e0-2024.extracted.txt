                                                     TheThirty-EighthAAAIConferenceonArtiﬁcialIntelligence(AAAI-24)
                  X4D-SceneFormer:EnhancedSceneUnderstandingon4DPointCloudVideos
                                                through Cross-Modal Knowledge Transfer
                                      4,1*               2,3*            2,3†                      2,3                 1                       3,2
                     Linglin Jing         , Ying Xue         , Xu Yan        , Chaoda Zheng , Dong Wang , Ruimao Zhang ,
                                                                     1               4              1             3,2†
                                                 ZhigangWang ,HuiFang ,BinZhao ,ZhenLi
                                                                      1Shanghai AI laboratory
                                                                      2FNii, CUHK-Shenzhen
                                                                      3SSE, CUHK-Shenzhen
                                                  4Department of Computer Science, Loughborough University
                                               {jinglinglin}@pjlab.org.cn, {xuyan1@link., lizhen@}cuhk.edu.cn
                                          Abstract                                       l
                                                                                                            Point Cloud                     Point Cloud 
                                                                                          Moda
                                                                                         e
                  The f㘶eld of 4D point cloud understanding is rapidly devel-                                 Model                           Model
                  oping with the goal of analyzing dynamic 3D point cloud se-            ngl
                                                                                          4D Point Cloud                   4D Point Cloud
                                                                                         ) Si
                  quences. However, it remains a challenging task due to the             (a
                  sparsity and lack of texture in point clouds. Moreover, the            l
                                                                                                            Multi-modal                    Multi-modal 
                  irregularity of point cloud poses a diff㘶culty in aligning tem-
                                                                                                              Model                           Model
                                                                                          Moda
                                                                                         i
                  poral information within video sequences. To address these             t
                  issues, we propose a novel cross-modal knowledge transfer               4D Point Cloud &                4D Point Cloud &
                                                                                           RGB Sequences                   RGB Sequences
                  framework, called X4D-SceneFormer. This framework en-                  (b) Mul
                                                                                         r
                                                                                                            Point Cloud 
                  hances 4D-Scene understanding by transferring texture pri-                                                                Point Cloud 
                                                                                         nsfe
                                                                                                              Model
                  ors from RGB sequences using a Transformer architecture                                                                     Model
                                                                                         ra
                                                                                          T
                  with temporal relationship mining. Specif㘶cally, the frame-                                Knowledge 
                                                                                                              Transfer
                  work is designed with a dual-branch architecture, consisting           dge
                                                                                         e
                                                                                                                                   Training and Inference
                  of an 4D point cloud transformer and a Gradient-aware Im-                                  Gradient-
                                                                                                               aware 
                  ageTransformer(GIT).TheGITcombinesvisualtextureand                                                               Training only
                                                                                                            Transformer
                  temporal correlation features to offer rich semantics and dy-          ) Knowl
                                                                                         (c
                                                                                                                                   Inference Phase
                  namics for better point cloud representation. During training,                 Training Phase
                  we employ multiple knowledge transfer techniques, includ-
                  ing temporal consistency losses and masked self-attention, to        Figure 1: 4D Cross-Modal Knowledge Transfer. (a) Previ-
                  strengthen the knowledge transfer between modalities. This           ous 4D point cloud analysis methods take point cloud only
                  leads to enhanced performance during inference using single-         astheirinput.(b)Althoughcross-modalapproachesenhance
                  modal 4D point cloud inputs. Extensive experiments demon-            performance, they introduce extra computation overhead in
                  strate the superior performance of our framework on various          both training and inference. (c) Our method takes additional
                  4D point cloud video understanding tasks, including action           2D images during the training for 4D cross-modal knowl-
                  recognition, action segmentation and semantic segmentation.          edge transfer. During the inference, the point cloud model
                  Theresults achieve 1st places, i.e., 85.3% (+7.9%) accuracy          can be independently deployed.
                  and47.3%(+5.0%)mIoUfor4Dactionsegmentationandse-
                  mantic segmentation, on the HOI4D challenge, outperform-
                  ing previous state-of-the-art by a large margin. We release
                  the code at https://github.com/jinglinglingling/X4D.                 geometric information in 3D space, a facet particularly ad-
                                                                                       vantageous for real-world interactions. These attributes are
                                       Introduction                                    pivotal for understanding 3Ddynamicenvironments,includ-
                                                                                       ing tasks like action recognition/segmentation (Hoai, Lan,
               Exploring point cloud sequences in 4D (integrating 3D                   and De la Torre 2011; Jing et al. 2022), and 4D semantic
               space with 1D time) has garnered considerable interest in               segmentation (Xie, Tian, and Zhu 2020).
               recent years (Fan and Kankanhalli 2021; Wen et al. 2022;                   Previous works in 4D point cloud representation learning
               Liu et al. 2022) due to their capacity to offer a wealth of dy-         predominantly stem from extending existing 3D point cloud
               namicinformationwithinour3Denvironment.Comparedto                       models (Fan and Kankanhalli 2021; Wen et al. 2022) to 4D,
               conventional videos, 4D point clouds deliver direct access to           which involves incorporating additional temporal learning
                                                                                       modules that enable feature interactions across time (Xiao
                   *These authors contributed equally.                                 et al. 2022; Zhong et al. 2022). However, due to the sparsity
                   †Corresponding authors: Xu Yan and Zhen Li.                         andlackoftextureinpointclouds,thesemethodsarelimited
               Copyright © 2024, Association for the Advancement of Artif㘶cial         in capturing comprehensive semantic details. Nevertheless,
               Intelligence (www.aaai.org). All rights reserved.                       such semantic information remains crucial, particularly for
                                                                                 2670
                                                  TheThirty-EighthAAAIConferenceonArtiﬁcialIntelligence(AAAI-24)
              tasks demandingmeticulousreasoning,suchas4Dsemantic                   former,toenhanceknowledgetransferforthepointcloud
              segmentationandactionsegmentation.Aplausibleapproach                  model.Notably,thesetechniques are only applied during
              to address this limitation involves integrating supplemen-            training, ensuring that the point cloud model can be in-
              tary texture information from RGB images to enhance the               dependently deployed during inference.
              4Dpointcloudrepresentation, similar to methods employed             • Effectiveness: Extensive experiments on three tasks
              in previous cross-modal studies (Cui et al. 2021; Yan et al.          showthat our method outperforms previous state-of-the-
              2022). Nevertheless, as shown in Figure 1(b), the concurrent          art methods by a large margin. This highlights the supe-
              processing of data from two modalities unavoidably intro-             riority of our approach in 4D point cloud understanding.
              duces additional network designs and computational over-
              head, posing challenges in online 4D video tasks.                                      Related Works
                 In this paper, we present a solution to address the
              aforementioned challenges through cross-modal knowl-               Image-based Video Analysis
              edge transfer. This approach eff㘶ciently transfers color and       Previous image-based video analysis approaches (Crasto
              texture-aware knowledge from 2D images to an arbitrary             et al. 2019) extract the global feature via RNN or 1D
              point-based model, while avoiding extra computational cost         CNN (Lea et al. 2017). After that, the following works
              during the inference phase, as depicted in Figure 1(c). Our        enhance the performance through using two-stream net-
              framework stands apart from prior cross-modal approaches           work (Ju et al. 2023), pooling techniques (Fernando et al.
              that solely focus on knowledge transfer between static frame       2016) and extracting averaged features from stridden sam-
              pairs (Crasto et al. 2019). Notably, it places extra empha-        pled frames (Wang et al. 2016). In contrast, 3D CNNs (Fer-
              sis on ensuring motion and temporal alignment during the           nando et al. 2016) or sparse 3D convolution (Graham, En-
              knowledge transfer.                                                gelcke, and Van Der Maaten 2018) jointly learn spatial-
                 The proposed framework, X4D-SceneFormer, takes                  temporal features by organizing 2D frames into 3D struc-
              multi-modal data (i.e., 4D point cloud, RGB sequences) as          tures to learn temporal relations implicitly. Recently, Vision
              input during training, and achieves superior performance           Transformer (ViT) (Dosovitskiy et al. 2020) proposes a pure
              using only point cloud data during inference. Specif㘶cally,        transformerarchitecturereplacingallconvolutionswithself-
              there are two branches in our training framework, process-         attention, and achieved excellent results. Built on the ViT ar-
              ing point cloud and RGB sequence independently. For the            chitecture, Timesformer and ViViT (Arnab et al. 2021) ex-
              point branch, we simply deploy an off-the-shelf 4D point           tend 2Dspatial self-attention to the 3D spatial-temporal vol-
              cloud processor for the sake of simplicity. For the image          ume.
              branch, we introduce a Gradient-aware Image Transformer
              (GIT) to learn strong image semantics. GIT takes into ac-          4DPointCloudProcessing
              count the temporal gradient (TG) as an added input from
              adjacent image frames to enhance its comprehension of mo-          Therearetwomainstreamsfor4DPointcloudvideomodel-
              tion dynamics. Additionally, multi-level consistency losses        ing:(1)voxel-basedand(2)points-basedapproaches.Voxel-
              are introduced to address both motion-related aspects and          based methods f㘶rst convert 4D point cloud into 2D voxel
              temporal alignment. Subsequently, the semantic and motion          sequences, subsequently leveraging 3D convolutions to ex-
              features are integrated into a unif㘶ed visual representation       tract sequential features. For instance, MinkowskiNet(Choy,
              through cross-attention. These merged representations are          Gwak, and Savarese 2019) harnesses 4D sparse convolu-
              then combined with the extracted point cloud representa-           tion, effectively mining features from valid voxel grids.
              tions, forming a stacked input for further processing with a       3DV(Wang et al. 2020) employs temporal rank pooling to
              cross-modal transformer. By employing carefully-designed           fuse point motion within voxel sets, thereafter employing
              attention masks, the cross-modal transformer can be de-            PointNet++ (Qi et al. 2017) to extrapolate point representa-
              ployed with only point cloud inputs during inference, while        tions. On the other hand, traditional points-based methods
              still incorporating multi-modal knowledge. In such a man-          take raw point cloud as input, and exploits RNN (Fan and
              ner, it achieves signif㘶cant improvementsineffectivelylever-       Yang 2019), appending a temporal features (Liu, Yan, and
              aging multi-modal information and ensuring consistent mo-          Bohg 2019) and point spatial-temporal convolutions (Fan
              tion alignment, making it a promising solution for various         et al. 2022) to encode temporal features. Nevertheless, the
              4Dpointcloud tasks.                                                above methods only focus on static scene representations.
                 In summary, the contributions of this work are:                 Recently, P4Transformer (Fan and Kankanhalli 2021) in-
               • Generality: We propose X4D-SceneFormer, the f㘶rst               troduces 4D point coevolution and then learns the tempo-
                 cross-modalknowledgetransferarchitecturefor4Dpoint              ral features in a Transformer architecture. Building on this,
                 cloudunderstanding,wherearbitrarypoint-basedmodels              PPTr (Wen et al. 2022) further boosts the performance by
                 can be easily integrated into this framework for cross-         incorporating primitive planes as prior knowledge, thereby
                 modal knowledge transfer.                                       enhancing the capture of enduring spatial-temporal context
                                                                                 in 4D point cloud videos. PST-Transformer (Fan, Yang,
               • Flexibility: We propose Gradient-aware Image Trans-             andKankanhalli2022)encodesspatio-temporalstructureby
                 former (GIT) to provide temporal-aware and texture-             utilizing video-level self-attention to search related points
                 aware features guidance. We also propose multi-level            adaptively. Notwithstanding these advancements, the exist-
                 consistency metrics, employing a cross-modal trans-             ing methods typically cater to sparse and texture-limited
                                                                           2671
                                                                  TheThirty-EighthAAAIConferenceonArtiﬁcialIntelligence(AAAI-24)
                     Point Branch                                                                                                                     Image Features          Image 
                                                                                                   k                                                                      representation
                                                                                                   as ad                                                           K,V
                                                                                                    T                                                                          Cross
                                                             Point Representation                  t  e
                                                                                                   n  H        • • •                                                          Attention
                                                                                                   oi
                       4D Point Cloud                                                              P
                     Image Branch         Point Backbone           Cross-Modal                                                                                                 Q
                                                                   Transformer                                 GT           Sliding Window
                                                                                                   k                                                                     P
                                                                      GIT                          as                                                                    L
                                                                                                                                                                         M
                                                                                                    T ad                                                                  
                                                                                                                                                                         &
                                                                                        • • •         e                                                                   
                       RGB Sequence                               Image Feature                                                                                          t
                                                                                                   ageH                                                                  a
                                                                                                   m                                                                     c
                                         Image Backbone                +                           I                                                                     n
                                                                                                               • • •                                                     Co
                                                                Gradient Feature
                                                             Image Representation                                           Sliding Window           Gradient Features
                     Temporal Gradient                                                     Train and Inference        Temporal aware contrastive                            Correlation
                                                  (a) X4D-SceneFormer                      Train only                 Temporal aware consistency   (b) GIT                    Features
                   Figure 2: The architecture of X4D-SceneFormer and GIT. (a) During the training phase, X4D-SceneFormer takes both image
                   sequence and 4D point cloud as input, where the dual branches independently extract representations and are supervised by
                   ground truths. A cross-modal Transformer process is applied between two representations. (b) The Gradient-aware Image
                   Transformer (GIT) employs a sliding window strategy to establish temporal relationships and acquires a correlation feature
                   through the cross-attention. Moreover, GIT applies two temporal-aware criteria in its processes.
                   point cloud inputs, ignoring rich texture and motion infor-                              our architecture consists of two branches, where the upper
                   mation in 2D images.                                                                     branch takes the normal 4D point cloud analysis model as
                                                                                                            the backbone while the other exploits extra RGB sequence
                   Cross-Modality Learning and Knowledge Transfer                                           to extract the prior knowledge. After that, we utilize a cross-
                   Given that point cloud and images are capable of captur-                                 modalTransformertotransformthecross-modalknowledge
                   ing distinct and complementary information pertaining to a                               with masked attention. Besides, several knowledge transfer
                   scene, signif㘶cant endeavors (Yan et al. 2022; Afham et al.                              constraints are applied between the two modalities.
                   2022) have been undertaken to integrate multi-modal fea-                                 ProblemFormulation
                   tures in order to enhance perception. However, the inte-
                   gration of multi-modal methods inevitably introduces ad-                                 Thetaskof4Dpointcloudanalysistakesapointcloudvideo
                   ditional computational burden and requires additional net-                               consisting of T frames with N points as input, which can be
                   work design. As a result, recent works have focused on de-                               denoted as P ∈ RT×N×3. Typically, there are three main
                   veloping stronger single-modal models through the cross-                                 tasks in the 4D point cloud analysis: 4D semantic segmen-
                   modal knowledge transfer. Typically, knowledge transfer                                  tation, action segmentation and action recognition. The de-
                   (KD) was originally proposed to compress integrated clas-                                scription of the above tasks can be formulated as
                   sif㘶ers (teacher) into smaller networks (student) without sig-
                   nif㘶cant performance loss (Hinton, Vinyals, and Dean 2015).                                                     SemSeg : RT×N×3 7→ RT×N,                             (1)
                   Recently, KD has been extended to 3D perception tasks
                   for transferring knowledge across different modalities. Sev-                                                 ActionSeg : RT×N×3 7→ RT,                               (2)
                   eral approaches have been proposed for 3D object detec-                                                  ActionRecog : RT×N×3 7→ R1,                                 (3)
                   tion (Wang et al. 2020), 3D semantic segmentation (Hou
                   et al. 2022), and other tasks (Yang et al. 2021). Moreover,                              wheretheformertwosegmentationtasksperformclassif㘶ca-
                   there are some previous approaches utilize contrastive crite-                            tion on point and frame levels respectively, and the recogni-
                   rion (Zhang et al. 2023) to enhance the knowledge transfer                               tion task identify single action for the whole video.
                   during the training phrase. Inspired by these works, we f㘶rst                               Toassistthesingle-modalmodelduringthetrainingstage,
                   time investigate cross-modal knowledge transfer in the task                              weintroducedRGBsequenceasanadditionalinput,denoted
                   of 4D point cloud analysis. In contrast with previous meth-                              as I ∈ RT×H×W×3 with the size of H × W. Taking 4D
                   ods that solely focus on distilling static spatial information,                          semantic segmentation as an example, the above task will
                   our architecture integrate motion and temporal alignment                                 be modif㘶ed during the training:
                   during the knowledge transfer.
                                                                                                                                     T×N×3           T×H×W×3                T×N
                                                   Methods                                                           SemSeg : R                 ×R                   7→ R          .    (4)
                   In this paper, we introduce a novel cross-modality knowl-                                Duringtheinference, the 4D point cloud model can be inde-
                   edge transfer approach that employs texture and motion pri-                              pendently deployed and the formulation keeps the same as
                   ors to assist 4D point cloud analysis. As shown in Figure 2,                             Eqn. (1).
                                                                                                    2672
                                               TheThirty-EighthAAAIConferenceonArtiﬁcialIntelligence(AAAI-24)
                                                                 point
                Point Representation
                                     Cross-Modal 
                                                                 image
                                     Transformer                                 I            I             I                 I
                Image Representation                                            ˆ
                                                                 mask           ft = αt−n ∗ft−n +...+αft +...+αt+n ∗ft+n, (5)
                                                   Attention Mask                G            G             G                 G
                                                                                ˆ
               (a) Training                                                    ft =βt−n∗ft−1+...+βft +...+βt+n∗ft+1, (6)
                                                                 point       where α and β represent learnable parameters that assign
                                     Cross-Modal 
                Point Representation                                         weight to the motion trajectory at the boundary of actions.
                                     Transformer
                                                                 mask
                                                                             n is the window size. Subsequently, we merge the outputs
              (b) Inference
                                                   Attention Mask            of the sliding window and employ an MLP to generate a
                                                                             gradient-aware correlation feature Fcor:
             Figure 3: Masked attention in the cross-modal transformer.
                                                                P                               cor          ˆI ˆG
             The attention mask prevents point representation Fh from                         F    =MLP([F ;F ]),                   (7)
             attending on image representation FI in training (top three
                                                 h                           where [·;·] is a concatenation operation.
             rows of the mask), avoiding performance drop in inference
             whenFI isnotavailable.
                     h                                                       Temporal-aware contrastive. To improve the differentia-
                                                                             tion between various actions within a single sequence and
             4DPointCloudArchitecture                                        address over-segmentation challenges in action segmenta-
                                                                             tion tasks, we exploit a temporal-aware supervised con-
             The architecture of the 4D point cloud model (Point Back-       trastive loss on the aforementioned correlation feature Fcor.
             bone) is illustrated in the left section of Figure 2. Follow-   Given a set of point cloud/label pairs with a temporal length
                                                                             of T frames, denoted as {Pi,Yi}           , a sequence of
             ing the previous works (Fan and Kankanhalli 2021), We                                             i=1,...,T
             adopt point 4D convolution (P4Conv) as the encoder, gen-        point cloud with various data augmentations can be repre-
                                                                  P                    ˆ                                          ˆ
             erating the 4D point features with the shape of Fl      ∈       sented as P, and the correlation features generated by P are
               T×M×D                                                                     ˆcor
             R         , where M and D are a number of subsampled            denoted as F    . Subsequently, we concatenate the afore-
             points and channels, respectively. After that, several self-    mentioned two predictions along the temporal dimension
             attention layers are applied to extract the sequential infor-   and denote it as Fcor. The temporal-aware contrastive loss
             mation across the sequence dimension. The outcome is a          is formulated as follows:
             D-dimensional high-level feature representation, denoted as
               P                 T
             Fh ={f1,··· ,ft}       .
                                 t=1                                                                   exp(Fcor ·Fcor/τ)
                                                                                   l(k,u) = −log P            k      u         ,   (8)
             Gradient-aware Image Transformer (GIT)                                                         exp Fcor ·Fcor/τ
                                                                                                     j∈A(k)        k     j
             As described in the right part of Figure 2, Gradient-aware            Ltcont = X       1     X l(k,u).                 (9)
             Image Transformer (GIT) is proposed to extract texture and                          |G(k)|
             gradient-aware features from the RGB sequence. It takes a                      k∈M          u∈G(k)
             set of images as input, independently encodes texture and
             gradient features and f㘶nally generates a high-level image      Here, M = [1,2T] is def㘶ned by the length of the
             feature representation FI with a cross-attention module.        concatenated sequence and A(k) = M\{k}. G(k) =
                                      h                                      {u ∈ A(k) : Y = Y }denotesthesetofpositivepairandτ
             Temporal-awareconsistencyandcontrastivelearningareap-                         u     k
             plied during the training to enhance performance.               is a coeff㘶cient temperature. By employingthisapproach,the
             Gradient-aware feature encoding. Inspired by previous           aforementioned loss function not only guarantees the prox-
             work (Xiao et al. 2022) exploiting temporal gradient (TG)       imity of features belonging to the same category within a
             to encode the sequential features, we f㘶rst generate TGs as     given sequence but also facilitates the convergence of fea-
             anextrainputfortheGIT.TheformulationofTGcanbede-                tures from the same frame that has been augmented using
             picted as gt = It − It+n, where t denotes the frame index,      distinct data augmentation.
             nisapredef㘶ned interval number, and I is a section of RGB       Temporal-aware consistency. To effectively utilize differ-
             video. Given the input RGB video I and generated temporal       ent temporal cues within a single sequence, we draw in-
             gradient G = {gt}T , two encoders adopt the same 2D-            spiration from the concept of asymmetric contrastive learn-
                                t=1
             CNNarchitecture to extract low-level frame-based features       ing(Zhangetal.2023).Inthisregard,weemployatemporal-
             FI,FG ∈RT×D.                                                    aware consistency loss to align temporal information be-
             Fusion by sliding window. Since TG is a weak signal that        tween FI and FG. This further enhances the generated
             cannot fully represent motion information, we further pro-      feature and facilitates the prediction of motion trajectory
             poseaslidingwindowmechanismtogenerateafusedcorre-               by capitalizing on the geometric consistency of adjacent
             lation feature by mining the temporal relationship within FI    frames. Given the image and gradient feature FI and FG,
                   G                                  I      I T
             andF .GiventheRGBandTGfeatureF ={ft}                  and       the temporal-aware consistency loss aligns the temporal fea-
               G       G T                                     t=1
             F ={ft }        , the sliding window at the t-th time-step can  ture in a time-misaligned manner, i.e., advance and lag. It
                          t=1
             be described as:                                                can be described as follows:
                                                                        2673
                                                  TheThirty-EighthAAAIConferenceonArtiﬁcialIntelligence(AAAI-24)
                    Method                  Reference                    Test                                  Validation
                                                         Acc     Edit      F1@{10,25,50}          Acc     Edit      F1@{10,25,50}
                    P4Transformer          CVPR2021      71.2    73,1    73.8    69.2    58.2     63.2    65.4    65.9    59.9    45.9
                    PPTr+C2P               CVPR2023      81.1    84.0    85.4    82.5    74.1      -       -        -       -       -
                    Multi-Conv-Res2          HOI4D       84.3    86.6    88.9    86.9    80.7      -       -        -       -       -
                           1
                    DPMix                    HOI4D       85.2    87.8    89.8    88.3    82.9      -       -        -       -       -
                    PPTr(Baseline)         ECCV2022      77.4    80.1    81.7    78.5    69.5     72.3    75.6    74.8    70.3    58.4
                    X4D-SceneFormer3         HOI4D       84.1    91.1    92.5    90.8    84.8     78.9    89.4    88.2    85.1    75.1
                    X4D-SceneFormer           Ours       85.3    91.5    92.6    91.1    85.5     82.6    92.4    91.8    89.4    81.2
                    Improvement                 -        +7.9   +11.4   +10.9   +12.6    +16.0   +10.3   +16.8   +17.0    +19.1   +22.8
              Table 1: The performance of action segmentation on HOI4D validation set and benchmark (CVPR2023-W) challenge. 11st
              solution on HOI4D challenge. 2Runner-up solution in the challenge. 3 We achieve 3rd place without using GIT module.
                                                                                       Method                 Frames     Test     Val
                                                                                                                        mIoU     mIoU
                                                                                       P4Transformer             3       40.1     28.1
                                                                                       PPTr+C2P                 10       42.3       -
                                                                                       PPTr(Baseline)            3       41.4     29.3
                                                                                       X4D-SceneFormer           3       47.3     35.8
                                                                                    Table 2: 4D semantic segmentation on HOI4D dataset.
              Figure 4: Visualization of GT generation for segmentation.
              Since the 2D segmentation ground truths are not available in       their feature representations. Specif㘶cally, we adopt a stack
              HOI4Ddataset,wegainthethe2Dlabelsthroughprojecting                 of transformer layers to jointly encode the two input modal-
              the point cloud labels onto the image.                             ities FI and FP. To avoid performance drop in inference
                                                                                        h       h
                                                                                 whenRGBsequenceisnotavailable,wedesignanattention
                                                                                 mask inspired by (Yang et al. 2021). As shown in Figure 3,
                                                                                 FP doesnotdirectly attend to FI (the top three rows of the
                                                                                   h                              h
                                N                                               mask). Meanwhile, the introduced attention mask allows the
                               X          exp fG ·fI/τ                           modeltoreference both FI and FP when generating the f㘶-
                    Ladv = −       log           i−1   i        ,     (10)                                 h       h
                                       PN exp fG ·fI/τ                          nal output of the image branch (the bottom three rows of the
                               i=2        i=2       i−1    i                     mask). Finally, the output feature is utilized in several 4D
                             N−1                G      I   
                   Llag = − X log         exp fi+1 ·fi /τ       .     (11)       task heads for downstream tasks, such as 4D action segmen-
                                      PN−1exp fG ·fI/τ                          tation.                            P        I
                              i=1        i=1        i+1    i                     Total loss functions. We denote L     and L as the task su-
                 Finally, the temporal-aware consistency Ltac is a linear        pervision on the point cloud and image heads. The f㘶nal loss
                                                            GI                   can be described as:
              combination between the above two losses: Ltac = (Ladv +
                                                            GI
              Llag)/2. In such a manner, we introduce a temporal consis-
              tency constraint between differen features.                             L=LP+LI+ω∗Ltcont+(1−ω)∗Ltac,                       (12)
              Gradient-aware feature generation. The GIT involves the            whereωdenotesahyper-parameter,andLtac = Ltac+Ltac.
              utilization of cross-attention blocks to merge the original                                                          GI    PI
              spatial image feature FI with the correlation feature Fcor,
              thereby incorporating both spatial and temporal features.                                Experiments
              Specif㘶cally, the query is generated from Fcor, while the key      Experiments Setup
              and value are obtained from FI during this process. We de-         Datasets. We evaluate our proposed method on two bench-
              scribe the obtained high-level image representation as FI.
                                                                      h          mark datasets, namely HOI4D (Liu et al. 2022) and MSR-
              Cross-modal Transformer                                            Action3D(Li,Zhang,andLiu2010).Theabovedatasetsin-
                                                                                 clude three tasks: 4D action segmentation, 4D action recog-
              To transfer the texture and gradient-aware knowledge from          nition and 4D semantic segmentation.
              GIT to the 4D point cloud model, we design a cross-                  The f㘶rst dataset, HOI4D, contains 2,971 training videos
              modal Transformer to fuse the knowledge from two modal-            and 892 test videos for action segmentation. Each video se-
              ities. First, to ensure temporal consistency between the two       quence has 150 frames with each frame containing 2048
              modalities,weconducttemporal-awareconsistencybetween               points. The dataset contains a total of 579K frames. All
              FI and FP, following a similar approach to Eqn. (10)               frames are annotated with 19 f㘶ne-grained action classes
                h        h                                         tac           in the interactive scene. Moreover, the 4D semantic seg-
              and (11). This temporal consistency is denoted as LPI. We
              thenemployacross-modaltransformermechanismtomerge                  mentation task contains the same training and testing split
                                                                           2674
                                                TheThirty-EighthAAAIConferenceonArtiﬁcialIntelligence(AAAI-24)
                                                       ideo Acc@1                      Iputs
                Method                    Frames      V                                               Acc    Edit     F1@{10,25,50}
                PointNet++                   1            61.61                 Point   RGB TG
                                             8            83.17                  ✓                    72.3   75.6   74.8   70.3    58.4
                     ransformer                                                  ✓       ✓            77.5   76.4   75.7   71.4    59.5
                P4T                         16            89.56                  ✓              ✓ 72.8 76.1 75.2 70.6 58.9
                                            24            90.94                  ✓       ✓      ✓ 76.8 74.2 73.6 69.6 57.5
                                             8            84.02
                     r                                                        Table 4: Ablation study for different inputs. All experiments
                PPT                         16            90.31
                                            24            92.33               conducts without using GIT module.
                                             8            87.16
                     r+C2P
                PPT                         16            91.89
                                            24            94.76
                                             8            81.41
                     r⋆ (Baseline)
                PPT                         16            90.87                        Ground 
                                            24            90.56                        Truth
                                             8            86.47
                               ormer                                                   PPTr
                X4D-SceneF                  16            92.56                       (Baseline)
                                            24            93.90
              Table 3: Action recognition results on MSR-Action3D                      X4D-
                                                                                     SceneFormer
              dataset. ⋆We reproduce PPTr without using primitive f㘶tting.
                                                                              Figure 5: Visualization of action segmentation. PPTr has a
                                                                              serious over-segmentation problem.
              withactionsegmentation.Eachvideosequenceincludes300
              frames of point clouds, with each frame consisting of 8192
              points. Annotations involve 43 indoor semantic categories.      rics, on both test and validation sets. The test set results are
              Thedataset contains a total of 1.2M frames. Due to the non-     sourced from the HOI4D online leaderboard. Its superior-
              public accessibility of the HOI4D test set, we randomly se-     ity is particularly evident in the metrics of edit distance and
              lect 25% of the training data as a validation set.              segmentF1score.Notably,P4TransformerandPPTrconsti-
                The second dataset, MSR-Action3D, consists of 567 hu-         tute the state-of-the-art backbones upon which other meth-
              man point cloud videos with 20 action categories. Each          ods have further built. In particular, X4D-SceneFormer ex-
              frame is sampled by 2,048 points. We maintain the same          hibits improvements of at least 7.8%, 11.4%, and 10.9%
              training/testing split as previous works (Wen et al. 2022;      in terms of accuracy, edit distance, and F1@10 score re-
              Zhangetal. 2023).                                               spectively. The superior performance in edit and F1 scores
              Evaluation metrics. For the task of action segmentation,        demonstrates the effectiveness of our approach in over-
              we exploit the metric of frame-wise accuracy (Acc), seg-        segmentation issues, validating the effectiveness of our pro-
              ment edit distance (Edit), and segment F1 score with over-      posed temporal consistency metrics.
              lapping threshold k% (F1@k) during the evaluation. Al-          HOI4D semantic segmentation. Table 2 provides the re-
              though frame-wise accuracy is commonly used as a met-           sults, showing a mIoU of 47.3% on the test set and 35.8%
              ric for action segmentation, this measure is not sensitive to   on the validation set. The performance enhancement in the
              over-segmentation errors. The segmental edit score is pre-      4Dsemanticsegmentationtaskhighlightstheeff㘶cacyofour
              sented in (Lea et al. 2017) and used to evaluate the case       approachincapturingf㘶ne-grainedfeatures.Whencompared
              of over-segmentation, and the segmental F1 scores measure       to previous methods, our approach achieves superior results,
              the quality of the prediction. For the task of 4D semantic      which is attributed to the temporal alignment representation
              segmentation, we rely on the mean Intersection over Union       and robust generalization capabilities facilitated by cross-
              (mIoU)asourevaluation metric. Finally, the top-1 accuracy       modalknowledgetransferandtemporalconsistencymetrics.
              is employed as the evaluation metric in the task of 3D action   MSR-Action3D. The detailed results are presented in Ta-
              recognition.                                                    ble 3. WereproducetheresultsofthePPTrwithouttheprim-
              ComparisonwithState-of-the-arts                                 itive f㘶tting as our baseline. Considering the MSR-Action3D
                                                                              dataset lacks RGB data, we project the point clouds to the
              HOI4Daction segmentation. Table 1 demonstrates the re-          depth map as the input of the image branch. Our approach
              sults on HOI4D dataset for the task of action segmentation,     demonstrates signif㘶cant performance improvements across
              where we compare our method with previously published           various sequence lengths. While our results are slightly be-
              methods(FanandKankanhalli2021;Wenetal.2022;Zhang                low C2P (Zhang et al. 2023), this is primarily due to em-
              et al. 2023) and other two unpublished methods on leader-       ploying a weaker baseline and using projected depth as the
              board (Multi-Conv-Res and DPMix). X4D-SceneFormer               input of image branch. Still, we improve the performance
              outperformsall comparative methodsacrossevaluationmet-          upon baseline model by 5%. This observation underscores
                                                                        2675
                                               TheThirty-EighthAAAIConferenceonArtiﬁcialIntelligence(AAAI-24)
                          Method         Acc   Edit    F1@{10,25,50}            Fusion            Acc    Edit    F1@{10,25,50}
               (a)   X4D-SceneFormer     76.8  74.2   73.6   69.6  57.5         concat            79.7   87.8   86.8   84.6   77.8
               (b)     +Correlation      81.2  82.5   80.8   78.9  69.8         sum.              79.5   87.6   86.5   84.3   77.5
               (c)   +Sliding Window     81.9  84.5   82.3   81.1  72.6         self-attention    81.5   89.9   88.7   86.6   79.8
               (d)       +Ltcont         82.2  87.9   86.4   85.2  76.3         cross-attention   82.6   92.4   91.8   89.4   81.2
               (e)        +Ltac          82.6  92.4   91.8   89.4  81.2
                                                                                Table 6: Ablation study for fusion mechanism in GIT.
                      Table 5: Ablations study for GIT module.
                                                                                     Distillation  Acc    Edit    F1@{10,25,50}
                    85                                                        (a)     Transfer     53.4   56.2  59.3   53.4  40.8
                    80                                                        (b)    L2distance    61.2   61.1  63.5   58.2  45.6
                                                                              (c)  KLdivergence    71.6   74.8  74.3   69.3  57.1
                    75                                                        (d)    Cosine Sim    73.8   79.2  78.1   73.5  62.0
                                                                              (e)      Ours        82.6   92.4  91.8   89.4  81.2
                    70
                                                                              Table 7: Ablation study on various distillation baselines.
                    65
                    60
                          P4Transformer             PPTr                       Table 6 further illustrates the fusion strategy in GIT. It
                                                                             shows that cross-attention is the most effective manner of
                   Figure 6: Results using different 4D backbones.           bridging RGB and TG features. As illustrated in Figure 5,
                                                                             our framework, incorporating GIT, demonstrates a superior
                                                                             capacity on HOI4DActionSegmentationdataset, especially
             that X4D-SceneFormer is not only well-suited to 4D tasks        for over-segmentation problem.
             but also effectively addresses traditional video analysis.      Different point backbones. Figure 6 demonstrates the re-
                                                                             sults via using different point backbone. Our model respec-
             ComprehensiveAnalysis                                           tively boosts the performance of P4Transformer and PPTr
                                                                             by 12% and 10%, which further verif㘶es the generality of
             All ablation experiments are conducted on HOI4D valida-         our proposed model.
             tion set with the task of action segmentation.                  Comparison of knowledge transfer. To further demon-
             The effect of different inputs. To validate the effective-      strate the effectiveness of our cross-modal knowledge trans-
             ness of different input, we conducted ablation studies using    fer framework, weconductaseriesofexperimentsinvolving
             various inputs through replacing GIT module with sim-           various classic distillation approaches. As depicted in Ta-
             ple concatenation. As demonstrated in Table 4, using extra      ble 7, the application of transfer learning methods (a) (Zhen
             RGB sequence as input with the cross-modal transformer          et al. 2020) between the point branch and the image branch
             substantially improves performance, conf㘶rming the effec-       yields unsatisfactory results. Furthermore, considering the
             tiveness of the cross-modal strategies. However, naively in-    widespread use of the teacher-student framework, we ex-
             creasing additional temporal gradient (TG) based on this        plored multiple experiments employing different distance
             foundation results in performance degradation. We primar-       functions between the modalities (b-d (Hinton, Vinyals, and
             ily attribute this to the temporal inconsistency between the    Dean 2015)). However, despite the relatively improved per-
             twomodalities.WenamethismodelasX4D-SceneFormer-                 formance of the cosine similarity loss, it still falls short of
             Vanilla. Subsequently, we provide an explanation through        our proposed framework. The primary factor is the temporal
             follow-up experiments to illustrate the reasons behind the      inconsistency inherent in two modalities.
             exceptional performance of our GIT method.
             Designanalysis of GIT. Table 5 illustrates the effectiveness                         Conclusion
             of each component in gradient-aware image Transformer
             (GIT). To improve the X4D-SceneFormer-Vanilla (-V) dis-         In this paper, we present X4D-SceneFormer, a novel 4D
             cussed before , we generate the correlation feature through     cross-modal knowledge transfer framework that leverages
             merging RGB sequence and TG with cross-attention. The           texture priors from RGB sequences to enhance 4D point
             results demonstrate that the correlation feature signif㘶cantly  cloud analysis. Our framework consists of a 4D point
             improves the performance, and the introduction of a sliding     cloudtransformerandaGradient-awareImageTransformer,
             window further increase the result, especially for the edit     whicharetrained with several knowledge transfer criteria to
             distance with 2% improvement. Moreover, the introduced          ensure temporal alignment and consistency between modal-
             temporalconsistencycriterion lead to a substantial improve-     ities. We show that our framework can achieve state-of-the-
             ment in both edit distance (+8%) and F1 scores (+9%). The       art results on various 4D point cloud video understanding
             outcomes demonstrate that the integration of cross-modal        tasks, such as action recognition and semantic segmentation,
             knowledge transfer and temporal consistency design effec-       using only single-modal 3D point cloud inputs. Our work
             tively addresses the inherent over-segmentation challenge in    opens up new possibilities for 4D point cloud analysis that
             4Dpointcloud video tasks.                                       uses extra image priors to enhance performance.
                                                                       2676
                                               TheThirty-EighthAAAIConferenceonArtiﬁcialIntelligence(AAAI-24)
                               Acknowledgments                               eling. IEEE Transactions on Pattern Analysis and Machine
             This work is partially supported by Shenzhen General            Intelligence, 45(2): 2181–2192.
             Program No. JCYJ20220530143600001, by the Basic Re-             Fan, H.; Yu, X.; Ding, Y.; Yang, Y.; and Kankanhalli, M.
             search Project No. HZQB-KCZYZ-2021067ofHetaoShen-               2022. Pstnet: Point spatio-temporal convolution on point
             zhen HKS&TCooperationZone,byShenzhen-HongKong                   cloud sequences. arXiv preprint arXiv:2205.13713.
             Joint Funding No. SGDX20211123112401002, by NSFC                Fan,Y.;andKankanhalli,M.2021.Point4dtransformernet-
             with Grant No. 62293482, by Shenzhen Outstanding Tal-           works for spatio-temporal modeling in point cloud videos.
             ents Training Fund, by Guangdong Research Project No.           In Proceedings of the IEEE/CVF conference on computer
             2017ZT07X152 and No. 2019CX01X104, by the Guang-                vision and pattern recognition, 14204–14213.
             dong Provincial Key Laboratory of Future Networks of In-        Fernando, B.; Gavves, E.; Oramas, J.; Ghodrati, A.; and
             telligence (Grant No. 2022B1212010001), by the Guang-           Tuytelaars, T. 2016. Rank pooling for action recognition.
             dong Provincial Key Laboratory of Big Data Comput-              IEEE transactions on pattern analysis and machine intelli-
             ing, The Chinese University of Hong Kong, Shenzhen,             gence, 39(4): 773–787.
             by the NSFC 61931024&81922046, by the Shenzhen                  Graham, B.; Engelcke, M.; and Van Der Maaten, L. 2018.
             Key Laboratory of Big Data and Artif㘶cial Intelligence          3d semantic segmentation with submanifold sparse convo-
             (Grant No. ZDSYS201707251409055), and the Key Area              lutional networks. In Proceedings of the IEEE conference
             R&D Program of Guangdong Province with grant No.                oncomputervision and pattern recognition, 9224–9232.
             2018B030338001, by zelixir biotechnology company Fund,
             by Tencent Open Fund. This work is partially supported by       Hinton, G.; Vinyals, O.; and Dean, J. 2015.        Distill-
             the Shanghai AI Laboratory, National Key R&D Program            ing the knowledge in a neural network.     arXiv preprint
             of China (2022ZD0160100), the National Natural Science          arXiv:1503.02531.
             FoundationofChina(62376222),andYoungEliteScientists             Hoai, M.; Lan, Z.-Z.; and De la Torre, F. 2011. Joint seg-
             Sponsorship Program by CAST (2023QNRC001).                      mentation and classif㘶cation of human actions in video. In
                                   References                                CVPR2011,3265–3272.IEEE.
                                                                             Hou,Y.;Zhu,X.;Ma,Y.;Loy,C.C.;andLi,Y.2022. Point-
             Afham, M.; Dissanayake, I.; Dissanayake, D.; Dharmasiri,        to-voxel knowledge distillation for lidar semantic segmenta-
             A.; Thilakarathna, K.; and Rodrigo, R. 2022. Crosspoint:        tion. In Proceedings of the IEEE/CVF conference on com-
             Self-supervised cross-modal contrastive learning for 3d         puter vision and pattern recognition, 8479–8488.
             point cloud understanding. In Proceedings of the IEEE/CVF       Jing, L.; Wang, Y.; Chen, T.; Dora, S.; Ji, Z.; and Fang, H.
             Conference on Computer Vision and Pattern Recognition,          2022. Towards more eff㘶cient few-shot learning based hu-
             9902–9912.                                                      mangesture recognition via dynamic vision sensors. In The
                                                               ˇ ´           British Machine Vision Conference (BMVC).
             Arnab, A.; Dehghani, M.; Heigold, G.; Sun, C.; Lucic, M.;
             and Schmid, C. 2021. Vivit: A video vision transformer. In      Ju, C.; Zheng, K.; Liu, J.; Zhao, P.; Zhang, Y.; Chang, J.;
             Proceedings of the IEEE/CVF international conference on         Tian, Q.; and Wang, Y. 2023. Distilling Vision-Language
             computer vision, 6836–6846.                                     Pre-training to Collaborate with Weakly-Supervised Tempo-
             Choy, C.; Gwak, J.; and Savarese, S. 2019.      4d spatio-      ral Action Localization. In Proceedings of the IEEE/CVF
             temporal convnets: Minkowski convolutional neural net-          Conference on Computer Vision and Pattern Recognition,
             works. In Proceedings of the IEEE/CVFconferenceoncom-           14751–14762.
             puter vision and pattern recognition, 3075–3084.                Lea,C.;Flynn,M.D.;Vidal,R.;Reiter,A.;andHager,G.D.
             Crasto, N.; Weinzaepfel, P.; Alahari, K.; and Schmid, C.        2017. Temporal convolutional networks for action segmen-
             2019. Mars:Motion-augmentedrgbstreamforactionrecog-             tation and detection. In proceedings of the IEEE Conference
             nition. In Proceedings of the IEEE/CVF conference on com-       onComputerVision and Pattern Recognition, 156–165.
             puter vision and pattern recognition, 7882–7891.                Li, W.; Zhang, Z.; and Liu, Z. 2010. Action recognition
             Cui, Y.; Chen, R.; Chu, W.; Chen, L.; Tian, D.; Li, Y.; and     based on a bag of 3d points. In 2010 IEEE computer soci-
             Cao, D. 2021. Deep learning for image and point cloud fu-       ety conference on computer vision and pattern recognition-
             sion in autonomous driving: A review. IEEE Transactions         workshops, 9–14. IEEE.
             onIntelligent Transportation Systems, 23(2): 722–739.           Liu, X.; Yan, M.; and Bohg, J. 2019. Meteornet: Deep learn-
             Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,        ing on dynamic 3d point cloud sequences. In Proceedings
             D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;      of the IEEE/CVF International Conference on Computer Vi-
             Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16    sion, 9246–9255.
             words: Transformers for image recognition at scale. arXiv       Liu,Y.;Liu,Y.;Jiang,C.;Lyu,K.;Wan,W.;Shen,H.;Liang,
             preprint arXiv:2010.11929.                                      B.; Fu, Z.; Wang, H.; and Yi, L. 2022. HOI4D: A 4D ego-
             Fan, H.; and Yang, Y. 2019.    PointRNN: Point recurrent        centric dataset for category-level human-object interaction.
             neural network for moving point cloud processing. arXiv         In Proceedings of the IEEE/CVF Conference on Computer
             preprint arXiv:1910.08287.                                      Vision and Pattern Recognition, 21013–21022.
             Fan, H.; Yang, Y.; and Kankanhalli, M. 2022. Point spatio-      Qi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017. Pointnet++:
             temporal transformer networks for point cloud video mod-        Deep hierarchical feature learning on point sets in a metric
                                                                        2677
                                             TheThirty-EighthAAAIConferenceonArtiﬁcialIntelligence(AAAI-24)
             space. Advances in neural information processing systems,
             30.
             Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.;
             and Van Gool, L. 2016. Temporal segment networks: To-
             wards good practices for deep action recognition. In Euro-
             pean conference on computer vision, 20–36. Springer.
             Wang,Y.;Xiao,Y.;Xiong,F.;Jiang,W.;Cao,Z.;Zhou,J.T.;
             andYuan,J.2020.3dv:3ddynamicvoxelforactionrecogni-
             tion in depth video. In Proceedings of the IEEE/CVF confer-
             ence on computer vision and pattern recognition, 511–520.
             Wen,H.;Liu,Y.;Huang,J.;Duan,B.;andYi,L.2022. Point
             PrimitiveTransformerforLong-Term4DPointCloudVideo
             Understanding. In European Conference on Computer Vi-
             sion, 19–35. Springer.
             Xiao, J.; Jing, L.; Zhang, L.; He, J.; She, Q.; Zhou, Z.;
             Yuille, A.; and Li, Y. 2022. Learning from temporal gradi-
             ent for semi-supervised action recognition. In Proceedings
             of the IEEE/CVF Conference on Computer Vision and Pat-
             tern Recognition, 3252–3262.
             Xie, Y.; Tian, J.; and Zhu, X. X. 2020. Linking points with
             labels in 3D: A review of point cloud semantic segmenta-
             tion. IEEE Geoscience and remote sensing magazine, 8(4):
             38–59.
             Yan, X.; Gao, J.; Zheng, C.; Zheng, C.; Zhang, R.; Cui, S.;
             and Li, Z. 2022. 2dpass: 2d priors assisted semantic seg-
             mentation on lidar point clouds. In European Conference on
             Computer Vision, 677–695. Springer.
             Yang, Z.; Zhang, S.; Wang, L.; and Luo, J. 2021.  Sat:
             2d semantics assisted training for 3d visual grounding. In
             Proceedings of the IEEE/CVF International Conference on
             Computer Vision, 1856–1866.
             Zhang, Z.; Dong, Y.; Liu, Y.; and Yi, L. 2023. Complete-
             to-Partial 4D Distillation for Self-Supervised Point Cloud
             Sequence Representation Learning. In Proceedings of the
             IEEE/CVF Conference on Computer Vision and Pattern
             Recognition, 17661–17670.
             Zhen, L.; Hu, P.; Peng, X.; Goh, R. S. M.; and Zhou, J. T.
             2020. Deepmultimodaltransferlearningforcross-modalre-
             trieval. IEEE Transactions on Neural Networks and Learn-
             ing Systems, 33(2): 798–810.
             Zhong, J.-X.; Zhou, K.; Hu, Q.; Wang, B.; Trigoni, N.; and
             Markham, A. 2022. No pain, big gain: classify dynamic
             point cloud sequences with static models by f㘶tting feature-
             level space-time surfaces. In Proceedings of the IEEE/CVF
             Conference on Computer Vision and Pattern Recognition,
             8510–8520.
                                                                    2678
