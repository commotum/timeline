100 100 :—-

=e Scaling Whith - 8 layers fixed —e- Transformer
-® Scaling Depth - 512 hidden size flxed Recurrent Transformer
80-4 80-4 HRM
zz
iy oe
ge 60 - 60 4
a
o
< 404 40 4
20 4 _—-» 20 4
T T T T T T T T T T T T T
27™ 54M 109M 218M 436M 872M 8 16 32 64 we 0256512
Parameters Depth / Transformer layers computed

Figure 2: The necessity of depth for complex reasoning. Left: On Sudoku-Extreme Full, which
require extensive tree-search and backtracking, increasing a Transformer’s width yields no perfor-
mance gain, while increasing depth is critical. Right: Standard architectures saturates, failing to
benefit from increased depth. HRM overcomes this fundamental limitation, effectively using its
computational depth to achieve near-perfect accuracy.

avoids the rapid convergence of standard recurrent models through a process we term “hierarchi-
cal convergence.” The slow-updating H-module advances only after the fast-updating L-module
has completed multiple computational steps and reached a local equilibrium, at which point the
L-module is reset to begin a new computational phase.

Furthermore, we propose a one-step gradient approximation for training HRM, which offers im-
proved efficiency and eliminates the requirement for BPTT. This design maintains a constant mem-
ory footprint (O(1) compared to BPTT’s O(T) for T timesteps) throughout the backpropagation
process, making it scalable and more biologically plausible.

Leveraging its enhanced effective depth, HRM excels at tasks that demand extensive search and
backtracking. Using only 1,000 input-output examples, without pre-training or CoT supervi-
sion, HRM learns to solve problems that are intractable for even the most advanced LLMs. For
example, it achieves near-perfect accuracy in complex Sudoku puzzles (Sudoku-Extreme Full) and
optimal pathfinding in 30x30 mazes, where state-of-the-art CoT methods completely fail (0% ac-
curacy). In the Abstraction and Reasoning Corpus (ARC) AGI Challenge?”**”? - a benchmark
of inductive reasoning - HRM, trained from scratch with only the official dataset (~1000 exam-
ples), with only 27M parameters and a 30x30 grid context (900 tokens), achieves a performance
of 40.3%, which substantially surpasses leading CoT-based models like 03-mini-high (34.5%)
and Claude 3.7 8K context (21.2%), despite their considerably larger parameter sizes and con-
text lengths, as shown in Figure 1. This represents a promising direction toward the development
of next-generation AI reasoning systems with universal computational capabilities.

2 Hierarchical Reasoning Model

We present the HRM, inspired by three fundamental principles of neural computation observed in
the brain:

¢ Hierarchical processing: The brain processes information across a hierarchy of cortical ar-
eas. Higher-level areas integrate information over longer timescales and form abstract repre-

sentations, while lower-level areas handle more immediate, detailed sensory and motor process-
ing202221,
