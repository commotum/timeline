algorithm*’ to adaptively determine the number of segments. A Q-head uses the final state of the

H-module to predict the Q-values Q™ = (Q™ 1, Q™ rinse) Of the “halt” and “continue” actions:

Q" = 0(0Q28""),

where o denotes the sigmoid function applied element-wise. The halt or continue action is chosen
using a randomized strategy as detailed next. Let M...x denote the maximum number of segments
(a fixed hyperparameter) and M,,;, denote the minimum number of segments (a random variable).
The value of Minin is determined stochastically: with probability ¢, itis sampled uniformly from the
set {2,--- , Mmax} (to encourage longer thinking), and with probability 1 —, it is set to 1. The halt
action is selected under two conditions: when the segment count surpasses the maximum threshold
Mrnax, or when the estimated halt value Onatt exceeds the estimated continue value Qeominue and the
segment count has reached at least the minimum threshold M nin.

The Q-head is updated through a Q-learning algorithm, which is defined on the following episodic
Markov Decision Process (MDP). The state of the MDP at segment m is 2™, and the action space
is {halt, continue}. Choosing the action “halt” terminates the episode and returns a binary reward
indicating prediction correctness, ie., 1{9’” = y}. Choosing “continue” yields a reward of 0 and
the state transitions to z*!. Thus, the Q-learning targets for the two actions G” = (G,, G™ sinue)
are given by

Tae = 19" = y},

Sm-+1 :
Am halt 7 ifm > Nias 5
Continue =

sm+l Aym4+l :
max(Qrt, Qi ue) otherwise .

We can now define the loss function of our learning procedure. The overall loss for each supervision
segment combines both the Q-head loss and the sequence-to-sequence loss:

Lt = Loss(#™, y) + BINARYCROSSENTROPY(Q™, G™) .

Minimizing the above loss enables both accurate predictions and nearly optimal stopping decisions.

Selecting the “halt” action ends the supervision loop. In practice, sequences are processed in
batches, which can be easily handled by substituting any halted sample in the batch with a fresh
sample from the dataloader.

Figure 5 presents a performance comparison between two HRM variants: one incorporating ACT
and another employing a fixed computational step count equivalent to ACT’s Minax parameter. It
shows that ACT effectively adapts its computational resources based on task complexity, achieving
significant computational savings with minimal impact on performance.

Inference-time scaling An effective neural model should exploit additional computational re-
sources during inference to enhance performance. As illustrated in Figure 5-(c), HRM seamlessly
achieves inference-time scaling by simply increasing the computational limit parameter, Minax
without requiring further training or architectural modifications.

Additional compute is especially effective for tasks that demand deeper reasoning. On Sudoku—
a problem that often requires long-term planning—HRM exhibits strong inference-time scaling.
On the other hand, we find that extra computational resources yield minimal gains in ARC-AGI
challenge, as solutions generally require only a few transformations.
