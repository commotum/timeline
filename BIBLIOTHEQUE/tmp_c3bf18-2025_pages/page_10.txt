62.00

Mean Backtracks

2
®
4
3
1
5
1
‘
3

(a) ARC-AGI (b) Sudoku-Hard. (c) Maze navigation (d) Sudoku-Extreme subset difficulty
Figure 6: Left: Visualization of benchmark tasks. Right: Difficulty of Sudoku-Extreme examples.
with weights initialized via truncated LeCun Normal initialization*”**’, while the scale and bias
parameters are excluded from RMSNorm. All parameters are optimized using the Adam-atan2 op-

timizer®°, a scale-invariant variant of Adam®', combined with a constant learning rate that includes
linear warm-up.

3 Results

This section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an
overview of the baseline models and their results. Figure 6-(a,b,c) presents a visual representa-
tion of the three benchmark tasks, which are selected to evaluate various reasoning abilities in AI
models.

3.1 Benchmarks

ARC-AGI Challenge The ARC-AGI benchmark evaluates general fluid intelligence through IQ-
test-like puzzles that require inductive reasoning’. The initial version, ARC-AGI-1, presents chal-
lenges as input-label grid pairs that force AI systems to extract and generalize abstract rules from
just a few examples. Each task provides a few input-output demonstration pairs (usually 2-3) and
a test input. An AI model has two attempts to produce the correct output grid. Although some be-
lieve that mastering ARC-AGI would signal true artificial general intelligence, its primary purpose
is to expose the current roadblocks in AGI progress. In fact, both conventional deep learning meth-
ods and CoT techniques have faced significant challenges with ARC-AGI-1, primarily because it
requires the ability to generalize to entirely new tasks”*.

Addressing the limitations identified in ARC-AGI-1, ARC-AGI-2 significantly expands the bench-
mark by providing a more comprehensive and carefully refined collection of tasks. These new
tasks emphasize deeper compositional reasoning, multi-step logic, contextual rule application, and
symbolic abstraction. Human calibration studies show these tasks are challenging but doable for
people, while being much harder for current AI systems, offering a clearer measure of general
reasoning abilities’.

10
