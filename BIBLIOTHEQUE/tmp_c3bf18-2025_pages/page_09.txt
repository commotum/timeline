(a) ACT Compute Spent (b) ACT Performance (c) Inference-time scaling

100.0 100.0 —

“Fixed -
97.5 9 o ACT (nes limit 9754
95.04 95.04
e254 9254

90.0 4 90.0 4

a754 a754

45.0 4 45.0 4

T T T a5 4 T T T a5 4 T T T T

2 4 a 2 4 a 2 4 a 16
M (Fixed) oF Mrmgx (ACT M (Fixed) oF May (ACT) Inference Mmex

Fixed
ACT (Meg limit)

Accuracy %
Accuracy %

-OTraln Migs = 2
Train Mage = 4
Train Mage = 8

Mean Compute Steps
HN we ms o@

Figure 5: Effectiveness of Adaptive Computation Time (ACT) on the Sudoku-Extreme-Fuil. (a)
Mean compute steps used by models with ACT versus models with a fixed number of compute steps
(M). ACT maintains a low and stable number of average compute steps even as the maximum limit
(Minax) increases. (b) Accuracy comparison. The ACT model achieves performance comparable
to the fixed-compute model while utilizing substantially fewer computational steps on average. (c)
Inference-time scalability. Models trained with a specific Max can generalize to higher compu-
tational limits during inference, leading to improved accuracy. For example, a model trained with
Max = 8 continues to see accuracy gains when run with My... = 16 during inference.

Stability of Q-learning in ACT The deep Q-learning that underpins our ACT mechanism is
known to be prone to instability, often requiring stabilization techniques such as replay buffers
and target networks**, which are absent in our design. Our approach, however, achieves stability
through the intrinsic properties of our model and training procedure. Recent theoretical work by
Gallici et al.” shows that Q-learning can achieve convergence if network parameters are bounded,
weight decay is incorporated during training, and post-normalization layers are implemented. Our
model satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a
layer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an L.,-
constrained optimization problem, ensuring that model parameters remain bounded by 1/A°°.

Architectural details We employ a sequence-to-sequence architecture for HRM. Both input and
output are represented as token sequences: x = (21,...,2;) and y = (y1,..., yr) respectively.
The model includes an embedding layer f; that converts discrete tokens into vector representa-
tions, and an output head fo(z;@0) = softmax(@oz) that transforms hidden states into token prob-
ability distributions 7. For small-sample experiments, we replace softmax with stablemax*! to
improve generalization performance. The sequence-to-sequence loss is averaged over all tokens,
Loss(g,y) = 4 an log p(y), where p(y;) is the probability that distribution %; assigns to token
y:. The initial hidden states z° are initialized by sampling from a truncated normal distribution with
standard deviation of 1, truncation of 2, and kept fixed throughout training.

Both the low-level and high-level recurrent modules f; and fy are implemented using encoder-
only Transformer** blocks with identical architectures and dimensions. These modules take mul-
tiple inputs, and we use straightforward element-wise addition to combine them, though more
sophisticated merging techniques such as gating mechanisms could potentially improve perfor-
mance and is left for future work. For all Transformer blocks in this work—including those in
the baseline models—we incorporate the enhancements found in modern LLMs (based on Llama>*
architectures). These improvements include Rotary Positional Encoding**, Gated Linear Units*,
RMSNorm**, and the removal of bias terms from linear layers.

Furthermore, both HRM and recurrent Transformer models implement a Post-Norm architecture
