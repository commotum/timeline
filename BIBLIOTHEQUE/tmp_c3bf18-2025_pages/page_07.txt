The gradients of the low-level fixed point. 24 and 24, can also be approximated using another

> 86, 6; ’
application of the 1-step gradient:
Oz, Of, Oz Oft
30, 06,’ 80; 80;
By substituting Equation (3) back into Equation (2), we arrive at the final simplified gradients.

3)

Before defining our loss function, we must first introduce two key elements of our proposed
method: deep supervision and adaptive computational time.

Deep supervision Inspired by the principle that periodic neural oscillations regulate when learning
occurs in the brain*®, we incorporate a deep supervision mechanism into HRM, as detailed next.

Given a data sample (x, y), we run multiple forward passes of the HRM model, each of which we
refer to as a segment. Let M denote the total number of segments executed before termination.
For each segment m € {1,..., M}, let 2™ = (2347, 2?) represent the hidden state at the
conclusion of segment m, encompassing both high-level and low-level state components.

At each segment m, we apply a deep supervision step as follows:

1. Given the state z™~! from the previous segment, compute the next state 2" and its associated
output 7 through a forward pass in the HRM model:

(2,9) — HRM(2"""', 2; 8)
2. Compute the loss for the current segment:

L™ — Loss(g™, y)

3. Update parameters:

6 <— OPTIMIZERSTEP(O, VeL™)

The crucial aspect of this procedure is that the hidden state z™ is “detached” from the computa-
tion graph before being used as the input state for the next segment. Consequently, gradients from
segment m + 1 do not propagate back through segment m, effectively creating a 1-step approxi-
mation of the gradient of the recursive deep supervision process*?*’. This approach provides more
frequent feedback to the H-module and serves as a regularization mechanism, demonstrating supe-
rior empirical performance and enhanced stability in deep equilibrium models when compared to
more complex, Jacobian-based regularization techniques*?*!. Figure 4 shows pseudocode of deep
supervision training.

Adaptive computational time (ACT) The brain dynamically alternates between automatic think-
ing (“System 1”) and deliberate reasoning (“System 2”)**. Neuroscientific evidence shows that
these cognitive modes share overlapping neural circuits, particularly within regions such as the
prefrontal cortex and the default mode network**4. This indicates that the brain dynamically mod-
ulates the “runtime” of these circuits according to task complexity and potential rewards*-*®,

Inspired by the above mechanism, we incorporate an adaptive halting strategy into HRM that en-
ables “thinking, fast and slow”. This integration leverages deep supervision and uses the Q-learning
