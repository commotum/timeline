1 Introduction

Deep learning, as its name suggests, emerged from the idea of stacking more layers to achieve
increased representation power and improved performance'*. However, despite the remarkable
success of large language models, their core architecture is paradoxically shallow*. This imposes
a fundamental constraint on their most sought-after capability: reasoning. The fixed depth of stan-
dard Transformers places them in computational complexity classes such as AC® or TC", prevent-
ing them from solving problems that require polynomial time**®. LLMs are not Turing-complete
and thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic rea-
soning that is necessary for deliberate planning or symbolic manipulation tasks”*. For example,
our results on the Sudoku task show that increasing Transformer model depth can improve per-
formance,' but performance remains far from optimal even with very deep models (see Figure 2),
which supports the conjectured limitations of the LLM scaling paradigm’.

The LLMs literature has relied largely on Chain-of-Thought (CoT) prompting for reasoning!®.
CoT externalizes reasoning into token-level language by breaking down complex tasks into sim-
pler intermediate steps, sequentially generating text using a shallow model!!. However, CoT for
reasoning is a crutch, not a satisfactory solution. It relies on brittle, human-defined decompositions
where a single misstep or a misorder of the steps can derail the reasoning process entirely '*!>. This
dependency on explicit linguistic steps tethers reasoning to patterns at the token level. As a result,
CoT reasoning often requires significant amount of training data and generates a large number of
tokens for complex reasoning tasks, resulting in slow response times. A more efficient approach is
needed to minimize these data requirements *.

Towards this goal, we explore “latent reasoning”, where the model conducts computations within
its internal hidden state space'>'®. This aligns with the understanding that language is a tool for
human communication, not the substrate of thought itself'’; the brain sustains lengthy, coherent
chains of reasoning with remarkable efficiency in a latent space, without constant translation back
to language. However, the power of latent reasoning is still fundamentally constrained by a model’s
effective computational depth. Naively stacking layers is notoriously difficult due to vanishing gra-
dients, which plague training stability and effectiveness ''*. Recurrent architectures, a natural al-
ternative for sequential tasks, often suffer from early convergence, rendering subsequent computa-
tional steps inert, and rely on the biologically implausible, computationally expensive and memory
intensive Backpropagation Through Time (BPTT) for training’.

The human brain provides a compelling blueprint for achieving the effective computational depth
that contemporary artificial models lack. It organizes computation hierarchically across corti-
cal regions operating at different timescales, enabling deep, multi-stage reasoning”??!**. Recur-
rent feedback loops iteratively refine internal representations, allowing slow, higher-level areas to
guide, and fast, lower-level circuits to execute—subordinate processing while preserving global
coherence**5, Notably, the brain achieves such depth without incurring the prohibitive credit-
assignment costs that typically hamper recurrent networks from backpropagation through time !*”.

Inspired by this hierarchical and multi-timescale biological architecture, we propose the Hierar-
chical Reasoning Model (HRM). HRM is designed to significantly increase the effective compu-
tational depth. It features two coupled recurrent modules: a high-level (H) module for abstract,
deliberate reasoning, and a low-level (L) module for fast, detailed computations. This structure

1Simply increasing the model width does not improve performance here.
