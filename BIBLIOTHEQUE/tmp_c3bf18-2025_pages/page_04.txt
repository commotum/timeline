¢ Temporal Separation: These hierarchical levels in the brain operate at distinct intrinsic timescales,
reflected in neural rhythms (e.g., slow theta waves, 4-8 Hz and fast gamma waves, 30-100
Hz)*°!, This separation allows for stable, high-level guidance of rapid, low-level computa-
tions,

¢ Recurrent Connectivity: The brain features extensive recurrent connections. These feedback
loops enable iterative refinement, yielding more accurate and context-sensitive representations
at the cost of additional processing time. Additionally, the brain largely avoids the problematic
deep credit assignment problem associated with BPTT”’.

The HRM model consists of four learnable components: an input network f;(-; 7), a low-level re-
current module f,(-;@,), a high-level recurrent module fy (-; 8), and an output network fo(-; 90).
The model’s dynamics unfold over N high-level cycles of T low-level timesteps each”. We index
the total timesteps of one forward pass by i = 1,..., N x T. The modules f; and fy each keep a
hidden state—z’. for f;, and 24, for f—which are initialized with the vectors z° and z®,, respec-
tively.

The HRM maps an input vector xz to an output prediction vector ¥ as follows. First, the input x is
projected into a working representation % by the input network:

At each timestep 7, the L-module updates its state conditioned on its own previous state, the H-
module’s current state (which remains fixed throughout the cycle), and the input representation.
The H-module only updates once per cycle (i.e., every T timesteps) using the L-module’s final
state at the end of that cycle:

a, = fi (2h ey 86) ,

i fa (iy! 2p ';@u) ifi =0 (mod T),
ty i-l :
rams otherwise .

Finally, after N full cycles, a prediction ¥ is extracted from the hidden state of the H-module:

0 = fol2i";9o) .

This entire N7-timestep process represents a single forward pass of the HRM. A halting mecha-
nism (detailed later in this section) determines whether the model should terminate, in which case
y will be used as the final prediction, or continue with an additional forward pass.

Hierarchical convergence Although convergence is crucial for recurrent networks, standard RNNs
are fundamentally limited by their tendency to converge too early. As the hidden state settles toward
a fixed point, update magnitudes shrink, effectively stalling subsequent computation and capping
the network’s effective depth. To preserve computational power, we actually want convergence to
proceed very slowly—but engineering that gradual approach is difficult, since pushing convergence
too far edges the system toward instability.

2While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are concep-
tual abstractions and do not map directly to specific neural oscillation frequencies.
