on this finding, we propose a one-step approximation of the HRM gradient-using the gradient of
the last state of each module and treating other states as constant. The gradient path is, therefore,

Output head — final state of the H-module — final state of the L-module — input embedding

The above method needs O(1) memory, does not require unrolling through time, and can be easily
implemented with an autograd framework such as PyTorch, as shown in Figure 4. Given that
each module only needs to back-propagate errors through its most recent local synaptic activity,
this approach aligns well with the perspective that cortical credit assignment relies on short-range,
temporally local mechanisms rather than on a global replay of activity patterns.

Unrolling aver time

The one-step gradient approximation is theoretically

BIPOW H

grounded in the mathematics of Deep Equilibrium Mod- =» + ta
els (DEQ)** which employs the Implicit Function Theo- 5 3 : 5
ploy Pp. z 5 F z
van ooh apr te
rem (IFT) to bypass BPTT, as detailed next. Consider an 5 5 id =
. . . . . ONo Grad
idealized HRM behavior where, during high-level cycle ““"€?
k, the L-module repeatedly updates until its state zz con- Get ment, x, Ne2, T=2):
. . . . ei it_embedding (x)
verges to a local fixed point z7. This fixed point, given Maen
the current high-level state okt can be expressed as with torch no_graaQ):
for _i in range(N * T- 4):
k-1l = zL = L_net(zL, 2H, x)
a = fil2t, 2H £85) . if Cit) 4T sO:
zH = H_net(zH, zL)
The H-module then performs a single update using this # instep grad
converged L-state: inenueten a
return (2H, zL), output_head(zH)
k k-1
cy fuley 1 aa Ox) . # Deep Supervision
for x, y_true in train_dataloader:
With a proper mapping /, the updates to the high-level for step in range(N_supervision) :
state can be written in a more compact form as 2% = 2s yhat © mma, x)
Fi (25+; @,8), where 6 = (67,61), and the fixed-point Loss = softmax_cross_entropy(y_hat, y_true)
. a wd — JF
can be written as 27, = F(z%;%,0). Let Jp = ez be lose ackwara(
the Jacobian of *, and assume that the matrix J — J+ is opt.stepQ

opt.zero_grad()

invertible at 27, and that the mapping F is continuously
differentiable. The Implicit Function Theorem then al- Figure 4: Top: Diagram of HRM with
lows us to calculate the exact gradient of fixed point 2%, approximate gradient. Bottom: Pseu-
with respect to the parameters @ without explicit back- docode of HRM with deep supervision

propagation: training in PyTorch.
Oz -10F
2H _ f(y “| 1
(Ire) Fp “ (1)

Calculating the above gradient requires evaluating and inverting matrix (J — J) that can be com-
putationally expensive. Given the Neumann series expansion,

(-Jeyt=I4+ Jp + Jet SR+...,
the so-called J-step gradient*’ approximates the series by considering only its first term, i.e. (7 —
Jy)~1 ss I, and leads to the following approximation of Equation (1):

Oz,  Ofu Oty Of Oz, Oxy  Ofu Oz
On O6n’ 00, Oz 86,’ 00; Axe Abr”

(2)
