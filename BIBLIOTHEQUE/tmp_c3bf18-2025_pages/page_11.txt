Sudoku-Extreme Sudoku is a 9X9 logic puzzle, requiring each row, column, and 3 x3 block to
contain the digits 1-9 exactly once. A prediction is considered correct if it exactly matches the
puzzle’s unique solution. Sudoku’s complex logical structure makes it a popular benchmark for
evaluating logical reasoning in machine learning®™**,

The most frequently used Sudoku dataset in research, namely the Kaggle dataset, can be fully
solved using elementary single-digit techniques®°. The minimal 17-clue puzzles°’, another widely-
used collection, might seem more challenging due to its small number of clues. However, this
perception is misleading—since 17 represents the minimum number of clues required to guarantee
a unique Sudoku solution, these hints need to be highly orthogonal to each other. This orthogonal
arrangement leads to many direct, easily-resolved solution paths *’.

We introduce Sudoku-Extreme, a more challenging dataset that is compiled from the aforemen-
tioned easy datasets as well as puzzles recognized by the Sudoku community as exceptionally
difficult for human players:

« Easy puzzles compiled from Kaggle, 17-clue, plus unbiased samples from the Sudoku puzzle
distribution®’: totaling 1 149 158 puzzles.

« Challenging puzzles compiled from Magictour 1465, Forum-Hard and Forum-Extreme subsets:
totaling 3 104 157 puzzles.

The compiled data then undergo a strict 90/10 train-test split, ensuring that the test set puzzles
cannot be derived through equivalent transformations of any training samples. Sudoku-Extreme is
a down-sampled subset of this data containing 1000 training examples. We use Sudoku-Extreme in
our main experiments (Figure 1), which focuses on small-sample learning scenarios. To guarantee
convergence and control overfitting effects in our analysis experiments (Figures 2, 3 and 5), we use
the complete training data, Sudoku-Extreme-Full, containing 3 831 994 examples.

We measure puzzle difficulty by counting the number of search backtracks (“guesses”) required
by a smart Sudoku solver program tdoku, which uses propositional logic to reduce the number of
guesses ®’. Our Sudoku-Extreme dataset exhibits a mean difficulty of 22 backtracks per puzzle, sig-
nificantly higher than existing datasets, including recent handmade puzzles Sudoku-Bench® which
average just 0.45 backtracks per puzzle. These subset complexity levels are shown in Figure 6-(d).

Maze-Hard This task involves finding the optimal path in a 30x30 maze, making it interpretable
and frequently used for training LLMs in search tasks®”°7', We adopt the instance generation
procedure of Lehnert et al.7', but introduce an additional filter to retain only those instances whose
difficulty exceeds 110. Here, “difficulty” is defined as the length of the shortest path, which aligns
with the linear time complexity of the wavefront breadth-first search algorithm on GPUs”. A path
is considered correct if it is valid and optimal—that is, the shortest route from the start to the goal.
The training and test set both include 1000 examples.

3.2 Evaluation Details

For all benchmarks, HRM models were initialized with random weights and trained in the sequence-
to-sequence setup using the input-output pairs. The two-dimensional input and output grids were
flattened and then padded to the maximum sequence length. The resulting performance is shown in
Figure 1. Remarkably, HRM attains these results with just ~1000 training examples per task—and
without pretraining or CoT labels.

11
