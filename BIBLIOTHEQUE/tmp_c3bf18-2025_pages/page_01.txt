2506.21734v3 [cs.AI] 4 Aug 2025

.
.

arXiv

Hierarchical Reasoning Model

Guan Wang?’ Jin Li!, Yuhao Sun?, Xing Chen’, Changling Liu’,
Yue Wu', Meng Lut, Sen Song”!, Yasin Abbasi Yadkori!t

'Sapient Intelligence, Singapore

Abstract

Reasoning, the process of devising and executing complex goal-oriented action sequences,
remains a critical challenge in AI. Current large language models (LLMs) primarily employ
Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive
data requirements, and high latency. Inspired by the hierarchical and multi-timescale pro-
cessing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel
recurrent architecture that attains significant computational depth while maintaining both train-
ing stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass
without explicit supervision of the intermediate process, through two interdependent recurrent
modules: a high-level module responsible for slow, abstract planning, and a low-level mod-
ule handling rapid, detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training samples. The
model operates without pre-training or CoT data, yet achieves nearly perfect performance on
challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes.
Furthermore, HRM outperforms much larger models with significantly longer context windows
on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial
general intelligence capabilities. These results underscore HRM’s potential as a transformative
advancement toward universal computation and general-purpose reasoning systems.

ARC-AGI-1 ARC-AGI-2 Sudoku-Extreme (9x9) Maze-Hard (30x30)
Cross Frequency HRM 960training examples 1120 training examples 1000 training examples 1000 training examples
Coupling Output 40.3 5.0 60 55.0 80 745
Mota t 40 345, 5 a
representation vw 2
“Braye 4otz tigre] | 0000 xp a0 4) 8 & “0 bid
8 O72 2 ev fE es Co ee
NY = Fae ECCI Pte 8 BER x Bens
Fa = a = au 8 au 8
FI z 5 i a7 8 = wai eee B fs ESB
a wit BBs 8 e ge 8 w4ge 8
lave evel Lowtevet | | (0000000 pias 172% E Bea Eaa Zs
Faster 6% a
representation AVAVAY oo a0 3 a0 00 00 3.0 oo 00 00 3.0
40Hz ° ° 0 9
Input {) = Update Chain-ofthought, pretrained Direct prediction, small-sample learning

Figure 1: Left: HRM is inspired by hierarchical processing and temporal separation in the brain. It
has two recurrent networks operating at different timescales to collaboratively solve tasks. Right:
With only about 1000 training examples, the HRM (~27M parameters) surpasses state-of-the-art
CoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles
(Sudoku-Extreme, Maze-Hard) where CoT models failed completely. The HRM was randomly
initialized, and it solved the tasks directly from inputs without chain of thoughts.

Tsinghua University ' Corresponding author. Contact: research@sapient . inc.
Code available at: github. com/sapientinc/HRM
