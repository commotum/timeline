70.

71.

72.

73.

TA.

75.

76.

77.

78.

79.

80.

81.

82.

83.
84.

DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng.
Dualformer: Controllable fast and slow thinking by learning with randomized reasoning
traces, 2025.

Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael
Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search
dynamics bootstrapping. In First Conference on Language Modeling, 2024.

Mubbasir Kapadia, Francisco Garcia, Cory D. Boatright, and Norman I. Badler. Dynamic
search on the gpu. In 2013 IEEE/RSJ International Conference on Intelligent Robots and
Systems, pages 3332-3337, 2013. doi: 10.1109/IROS.2013.6696830.

Isaac Liao and Albert Gu. Arc-agi without pretraining, 2025. URL https:
//i1i1a02345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_
without_pretraining. html.

Lorenzo Posani, Shuqi Wang, Samuel P Muscinelli, Liam Paninski, and Stefano Fusi.
Rarely categorical, always high-dimensional: how the neural code changes along the cortical
hierarchy. bioRxiv, pages 2024-11, 2025.

Mattia Rigotti, Omri Barak, Melissa R. Warden, Xiao-Jing Wang, Nathaniel D. Daw, Earl K.
Miller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks.
Nature, 497:585—-590, 2013. doi: 10.1038/naturel 2160.

Valerio Mante, David Sussillo, Krishna V. Shenoy, and William T. Newsome. Context-
dependent computation by recurrent dynamics in prefrontal cortex. Nature, 503(7474):78-84,
2013. doi: 10.1038/nature 12742.

Earl K. Miller and Jonathan D. Cohen. An integrative theory of prefrontal cortex function.
Annual Review of Neuroscience, 24(1): 167-202, 2001. doi: 10.1146/annurev.neuro.24.1.167.
Wolfgang Maass. Real-time computing without stable states: a new framework for neural
computation based on perturbations. Neural Computation, 14(11):2531-2560, 2002. doi:
10.1162/089976602760407955.

Ege Altan, Sara A. Solla, Lee E. Miller, and Eric J. Perreault. Estimating the dimensionality
of the manifold underlying multi-electrode neural recordings. PLoS Computational Biology,
17(11):e 1008591, 2021. doi: 10.1371 /journal.pebi.1008591.

Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the
terminal phase of deep learning training. Proceedings of the National Academy of Sciences,
117(40):24652-24663, 2020. doi: 10.1073/pnas.2015509117.

Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via
layer—peeled model: Minority collapse in imbalanced training. Proceedings of the National
Academy of Sciences, 118(43):e2103091118, 2021. doi: 10.1073/pnas.2103091118.

Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu.
A geometric analysis of neural collapse with unconstrained features. In Advances in Neural
Information Processing Systems, volume 34 of NeurIPS, pages 29820-29834, 2021.

Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.

Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka
Grabska-Barwitiska, Sergio Gémez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John

Agapiou, et al. Hybrid computing using a neural network with dynamic external memory.
Nature, 538(7626):47 1-476, 2016.

23
