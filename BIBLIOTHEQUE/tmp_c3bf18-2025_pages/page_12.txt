For ARC-AGI challenge, we start with (1) all demonstration and test input-label pairs from the
training set, and (2) all demonstration pairs along with test inputs from the evaluation set. The
dataset is augmented by applying translations, rotations, flips, and color permutations to the puz-
zles. Each task example is prepended with a learnable special token that represents the puzzle it
belongs to. At test time, we proceed as follows for each test input in the evaluation set: (1) Gener-
ate and solve 1000 augmented variants and, for each, apply the inverse-augmentation transform to
obtain a prediction. (2) Choose the two most popular predictions as the final outputs.? All reported
results are obtained by comparing the outputs with the withheld test labels from the evaluation set.

We augment Sudoku puzzles by applying band and digit permutations, while data augmentation is
disabled for Maze tasks. Both tasks undergo only a single inference pass.

For ARC-AGI, the scores of the CoT models are taken from the official leaderboard’, while for
Sudoku and Maze, the scores are obtained by evaluating through the corresponding API.

In Figure 1, the baselines are grouped based on whether they are pre-trained and use CoT, or neither.
The “Direct pred” baseline means using “direct prediction without CoT and pre-training”, which
retains the exact training setup of HRM but swaps in a Transformer architecture. Interestingly, on
ARC-AGI-1, “Direct pred” matches the performance of Liao and Gu”, who built a carefully de-
signed, domain-specific equivariant network for learning the ARC-AGI task from scratch, without
pre-training. By substituting the Transformer architecture with HRM’s hierarchical framework and
implementing ACT, we achieve more than a twofold performance improvement.

On the Sudoku-Extreme and Maze-Hard benchmarks, the performance gap between HRM and the
baseline methods is significant, as the baselines almost never manage to solve the tasks. These
benchmarks that demand lengthy reasoning traces are particularly difficult for CoT-based methods.
With only 1000 training examples, the “Direct pred” baseline—which employs an 8-layer Trans-
former identical in size to HRM—fails entirely on these challenging reasoning problems. When
trained on the larger Sudoku-Extreme-Full dataset, however, “Direct pred” can solve some easy
Sudoku puzzles and reaches 16.9% accuracy (see Figure 2). Lehnert et al.’! showed that a large
vanilla Transformer model with 175M parameters, trained on 1 million examples across multiple
trials, achieved only marginal success on 30x30 Maze tasks, with accuracy below 20% using the
pass@64 evaluation metric.

3.3 Visualization of intermediate timesteps

Although HRM demonstrates strong performance on complex reasoning tasks, it raises an intrigu-
ing question: what underlying reasoning algorithms does the HRM neural network actually imple-
ment? Addressing this question is important for enhancing model interpretability and developing a
deeper understanding of the HRM solution space.

While a definitive answer lies beyond our current scope, we begin our investigation by analyzing
state trajectories and their corresponding solution evolution. More specifically, at each timestep
i and given the low-level and high-level state pair (zi, and z1,) we perform a preliminary forward
pass through the H-module to obtain 2* = f7(z1,, 21; Ox) and its corresponding decoded prediction
gy = fo(2;80). The prediction y' is then visualized in Figure 7.

In the Maze task, HRM appears to initially explore several potential paths simultaneously, subse-
quently eliminating blocked or inefficient routes, then constructing a preliminary solution outline

3The ARC-AGI allows two attempts for each test input.

12
