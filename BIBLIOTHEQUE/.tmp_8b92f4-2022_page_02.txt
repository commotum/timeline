                                                  Under review as a conference paper at ICLR 2025
                                  054                     Input                      Output
                                  055                                                                                                                                                                                        …
                                  056
                                  057                                                                  Reconstruct 
                                  058                                                                               Transformer                       cross-attention
                                  059
                                  060                        Tokenize & Pad                                                                 Encoder                                Decoder
                                  061
                                  062
                                  063                                                                                                                                                                                    Object 
                                  064                                                                                                                                                                                    Positional 
                                                                                                                                                                                                                         Encoding
                                  065
                                                                                                                                                                                                                         2D 
                                  066                                                                                                                                                                                    Positional 
                                  067                                                                 Flatten   + + + + + + + + + + + + + + + + + + Encoding
                                  068                                                                                                                                                                                 …
                                  069                  Pad Tokens        End-of-grid Tokens      Newline Tokens
                                  070
                                  071             Figure 1: Overview of our ViTARC framework contribution. An ARC input image is first to-
                                  072             kenized into pixels and padded with visual tokens including end-of-grid tokens that mark the end
                                  073             of the image grid, newline tokens that indicate the end of one row, and pad tokens which are used
                                  074             to pad the image into a fixed maximum size (not drawn in full to maintain clarity). 2D Positional
                                  075             Encodings and Object Positional Encodings are then added to each token before being passed into
                                  076             the transformer. The output tokens are reconstructed into a valid two-dimensional grid.
                                  077
                                  078             1. AvanillaVisionTransformer(ViT)failsontheARC:DespitetheARCgrids’relativelysimple
                                  079                  structure compared to the much larger, noisier natural images they are typically evaluated on, a
                                  080                  vanilla ViT performs extremely poorly on 90% of the tasks with an overall test accuracy of
                                  081                  18%(cf. Figure3,Section3). Thisisdespiteusingatrainingsetofonemillionexamplespertask.
                                  082                  Followingafailureanalysis,wehypothesizethatthevanillaViTfailsbecauseitcannotaccurately
                                  083                  model spatial relationships between the objects in an ARC grid and the grid boundaries.
                                  084             2. A 2D visual representation significantly boosts ViT reasoning performance: Using a 2D
                                  085                  representation strategy based on visual tokens to represent the ARC input-output pairs, VITARC
                                  086                  solves 66% of all test instances – a marked improvement (cf. Section 4). About 10% of the
                                  087                  tasks remain poorly solved. After further failure analysis on these tasks, we discover that certain
                                  088                  complex visual structures are difficult for VITARC. We hypothesize this is due to limitations
                                  089                  of the transformer architecture itself in that it is designed to prioritize token embeddings over
                                  090                  positional encodings that can make it challenging to capture intricate spatial relationships.
                                  091             3. Positional Information further enhances ViT reasoning abilities: We improved VITARC’s
                                  092                  spatial awareness by learning to combine absolute, relative, and object positional information (cf.
                                  093                  Section 5), resulting in substantial performance boosts, with some ARC tasks progressing from
                                  094                  unsolved to fully solved (Figure 3). The final test accuracy is 75%, with more than half of the
                                  095                  tasks being solved to an accuracy of 95% or more.
                                  096
                                  097                                        Task A: 6e02f1e3                                   Task B: 49d1d64f                                    Task C: f25fbde4
                                  098
                                  099
                                  100
                                  101
                                  102
                                  103
                                  104             Figure2: ThreeexampleARCtasks. Foreachtask,thefirstcolumnscontainexampleinput-output
                                  105             pairs from the “training” instances, and the last column contains the “test” instance. The goal is to
                                  106             use the training instances to solve the test instance. The Vanilla ViT setup (Section 3) was only able
                                                                               1
                                  107             to solve Task A . Our ViTARC-VT (Section 4) was able to solve Task A and B but still failed at
                                                  Task C. Our final model ViTARC (Section 5) achieves near 100% accuracy on all three tasks.
                                                                                                                                           2
