ORM PRM _ Majority Vote # Problems

AP Calculus 68.9% 86.7% 80.0% 45
AP Chemistry 68.9% 80.0% 71.7% 60
AP Physics 77.8% 86.7% 82.2% 45
AMC10/12 49.1% 53.2% 32.8% 84
Aggregate 63.8% 72.9% 61.3% 234

Table 1: We measure out-of-distribution generalization using recent STEM tests.
We evaluate the outcome-supervised RM, the process-supervised RM, and ma-
jority voting using 100 test samples per problem.

relative lack of diversity limits the possible upside from active learning.

We also performed a preliminary investigation into the impact of iteratively
retraining PRMgelector throughout data collection. Between iterations, we re-
trained PRMgelector using all currently labeled data. Unfortunately, we observed
instability in this process which we were unable to diagnose. The resulting
reward models performed no better than the models described above. We expect
some form of iterative retraining to be beneficial in active learning, but we
currently have no concrete evidence to support this claim. We consider this a
compelling direction for future research.

5 OOD Generalization

To get some measure of out-of-distribution generalization, we evaluate our large-
scale ORM and PRM on a held-out set of 224 STEM questions, pulled from the
most recent AP Physics, AP Calculus, AP Chemistry, AMC10, and AMC12 ex-
ams. Since these tests were released after the pre-training dataset was compiled,
we can have high confidence that the model has not seen these problems. We
report the best-of-100 performance of the ORM, PRM and majority voting in
Table 1. We observe results similar to those in Section 3: the PRM outperforms
both the ORM and majority voting. This shows us that the PRM can tolerate
a modest amount of distribution shift and that its strong performance holds up
on fresh test questions.

6 Discussion

6.1 Credit Assignment

One clear advantage of process supervision is that it provides more precise
feedback than outcome supervision. A reward model trained with outcome
supervision faces a difficult credit-assignment task â€” to generalize well, it must
determine where an incorrect solution went wrong. This is particularly difficult
for hard problems: most model-generated solutions contain an error somewhere,
so the marginal value of a negative label from outcome supervision is low. In

10
