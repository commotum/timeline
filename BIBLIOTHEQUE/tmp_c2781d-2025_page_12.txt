           Preprint, Under Review.
           WoosukKwon,ZhuohanLi,SiyuanZhuang,YingSheng,LianminZheng,CodyHaoYu,JosephE.
            Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
            serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating
            Systems Principles, 2023.
           NathanLambert,JacobMorrison,ValentinaPyatkin, ShengyiHuang,HamishIvison,FaezeBrahman,
            Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\” ulu 3: Pushing frontiers in
            open language model post-training. arXiv preprint arXiv:2411.15124, 2024.
           Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,
            and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint
            arXiv:2307.03172, 2023.
           Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
            ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
            Bkg6RiCqY7.
           LuMa,HaoLiang,MeiyiQiang,LexiangTang,XiaochenMa,ZhenHaoWong,JunboNiu,Chengyu
            Shen, Runming He, Bin Cui, et al. Learning what reinforcement learning can’t: Interleaved online
            fine-tuning for hardest questions. arXiv preprint arXiv:2506.07527, 2025.
           DavidQMayne,JamesBRawlings,ChristopherVRao,andPierreOMScokaert. Constrainedmodel
            predictive control: Stability and optimality. Automatica, 36(6):789–814, 2000.
           Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky.
            State of what art? a call for multi-prompt llm evaluation. Transactions of the Association for
            Computational Linguistics, 12:933–949, 2024.
           Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang,
            MelihElibol, Zongheng Yang, William Paul, Michael I Jordan, et al. Ray: A distributed framework
            for emerging {AI} applications. In 13th USENIX symposium on operating systems design and
            implementation (OSDI 18), pp. 561–577, 2018.
           Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke
                               `
            Zettlemoyer, Percy Liang, Emmanuel Candes, and Tatsunori Hashimoto. s1: Simple test-time
            scaling. arXiv preprint arXiv:2501.19393, 2025.
           Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan,
                          ´
            Eduardo Pignatelli, Łukasz Kucinski, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack
                        ¨
            Parker-Holder, and Tim Rocktaschel. BALROG: Benchmarking agentic LLM and VLM reasoning
            on games. In The Thirteenth International Conference on Learning Representations, 2025a. URL
            https://openreview.net/forum?id=fp6t3F669F.
           Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan,
                          ´
            Eduardo Pignatelli, Łukasz Kucinski, Lerrel Pinto, Rob Fergus, et al. Balrog leaderboard, 2025b.
            URLhttps://balrogai.com.
           BenPrystawski, Michael Li, and Noah Goodman. Why think step by step? reasoning emerges from
            the locality of experience. Advances in Neural Information Processing Systems, 36:70926–70947,
            2023.
           Nate Rahn, Pierluca D’Oro, and Marc G. Bellemare. Controlling large language model agents
            with entropic activation steering. CoRR, abs/2406.00244, 2024. URL https://doi.org/10.
            48550/arXiv.2406.00244.
           Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwaraknath Gnaneshwar,
                            ¨
            Acyr Locatelli, Robert Kirk, Tim Rocktaschel, Edward Grefenstette, and Max Bartolo. Procedural
            knowledgeinpretrainingdrivesreasoninginlargelanguagemodels. InTheThirteenthInternational
            Conference on Learning Representations, 2025. URL https://openreview.net/forum?
            id=1hQKHHUsMx.
           Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
            Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
            go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.
                               12
