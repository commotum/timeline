           Appendix
           A BackgroundandRelatedWork
           In this paper, we focus on relatively small transformer models performing mathematical tasks, plac-
           ingitintoalongestablishedcorpusofworksthatstudyinterestingphenomenainacontrolledsetting,
           and advance our understanding of the underlying mechanisms in larger models in the wild, see e.g.
           Poweretal. (2022); Garg et al. (2022); Charton (2024); Dohmatob et al. (2024).
           One such example is the study of “grokking”, first observed with modular arithmetic - a phe-
           nomenon where models generalize long after achieving 100% accuracy on their (small) training
           set (Power et al., 2022; Liu et al., 2022b, 2023). On the surface, grokking shares similarities with
           our work: a small training dataset is iterated for many epochs, the phenomenon is isolated in clean
           experiments on synthetic data, and it contradicts traditional wisdom regarding overfitting (Mohri
           et al., 2018). But there are important differences: in grokking, delayed learning occurs, we observe
           nosuchdelay;grokkingoccursfor“tiny”trainingsamples(hundredsorthousandsofexamples),our
           models use millions (even for modular multiplication); grokking is very sensitive to the optimizer
           used, our findings are robust across optimizers (Appendix D.5), and, of course, no two-set approach
           is documented in the grokking setting.
           Another related setting is “benign overfitting” (Bartlett et al., 2020; Belkin, 2021; Bartlett et al.,
           2021), where an over-parametrized model perfectly fits noisy data, without harming prediction ac-
           curacy. One could argue that our work presents a quantitative manifestation of benign overfitting,
           inasmuch as decreasing the data budget increases model over-parametrization. However, this would
           not account for the decrease in performance once the data budget falls below a certain number (one
           could argue that overfitting is no longer benign, then), nor for the possibility of two-set training.
           Our work is related to, but different from, curriculum learning (CL) (Bengio et al., 2009; Wang
           et al., 2022), where training data is presented in a meaningful order, usually from “easy” to “hard”
           samples. Two-set training, differs from curriculum learning in at least two important ways: in CL,
           datasets are curated, our subsets are completely random; in CL, the training distribution shifts over
           time, while our subsets are static. Our ablations show that curating the repeated set, or changing it
           over time, as in CL, brings no improvement on performance (and may even have an adverse effect).
           Lastly, our work touches upon the expansive area of out-of-distribution (OOD) generalization (Gul-
           rajani & Lopez-Paz, 2021; Lopez-Paz, 2025), which studies generalization when train and test dis-
           tributions differ. Curiously, while our two-set approach increases the frequency of some training
           examples, because the repeated set is chosen at random, the training set remains distributionally
           equivalent to the test set. Thus, our study falls outside the usual framework of OOD studies.
                               7
