                  Table 1: Comparison of input encoding methods using ViViT-B                            Table 2: Comparison of model architectures using ViViT-B as the
                  and spatio-temporal attention on Kinetics. Further details in text.                    backbone,andtubeletsizeof16×2. WereportTop-1accuracyon
                                                                   Top-1 accuracy                        Kinetics 400 (K400) and action accuracy on Epic Kitchens (EK).
                                                                                                         Runtime is during inference on a TPU-v3.
                           Uniform frame sampling                         78.5                                                             K400     EK     FLOPs      Params    Runtime
                                                                                                                                                                9          6
                           Tubelet embedding                                                                                                               (×10 )     (×10 )     (ms)
                           Randominitialisation [24]                      73.2                              Model1: Spatio-temporal        80.0    43.1     455.2      88.9       58.9
                                                                                                            Model2: Fact. encoder          78.8    43.7     284.4     115.1       17.4
                           Filter inﬂation [8]                            77.6                              Model3: Fact. self-attention   77.4    39.1     372.3     117.3       31.7
                           Central frame                                  79.2                              Model4: Fact. dot product      76.3    39.5     277.1      88.9       22.9
                                                                                                            Model2: Ave. pool baseline     75.8    38.8     283.9      86.7       17.3
                  Datasets       We evaluate the performance of our proposed                             encoding method for all subsequent experiments.
                  models on a diverse set of video classiﬁcation datasets:
                      Kinetics [34] consists of 10-second videos sampled at                              Model variants            We compare our proposed model vari-
                  25fps from YouTube. We evaluate on both Kinetics 400                                   ants (Sec. 3.3) across the Kinetics 400 and Epic Kitchens
                  and 600, containing 400 and 600 classes respectively. As                               datasets, both in terms of accuracy and efﬁciency, in Tab. 2.
                  these are dynamic datasets (videos may be removed from                                 In all cases, we use the “Base” backbone and tubelet size of
                  YouTube), we note our dataset sizes are approximately 267                              16 × 2. Model 2 (“Factorised Encoder”) has an additional
                  000and446000respectively.                                                              hyperparameter, the number of temporal transformers, Lt.
                      Epic Kitchens-100 consists of egocentric videos captur-                            Weset Lt = 4 for all experiments and show in the supple-
                  ing daily kitchen activities spanning 100 hours and 90 000                             mentary that the model is not sensitive to this choice and
                  clips [13]. We report results following the standard “action                           thus does not affect our conclusions.
                  recognition” protocol. Here, each video is labelled with a                                  The unfactorised model (Model 1) performs the best
                  “verb” and a “noun” and we therefore predict both cate-                                on Kinetics 400. However, it can also overﬁt on smaller
                  gories using a single network with two “heads”. The top-                               datasets such as Epic Kitchens, where we ﬁnd our “Fac-
                  scoring verb and action pair predicted by the network form                             torised Encoder” (Model 2) to perform the best. We also
                  an “action”, and action accuracy is the primary metric.                                consider an additional baseline (last row), based on Model
                      Moments in Time [44] consists of 800 000, 3-second                                 2, where we do not use any temporal transformer, and sim-
                  YouTube clips that capture the gist of a dynamic scene in-                             ply average pool the frame-level representations from the
                  volving animals, objects, people, or natural phenomena.                                spatial encoder before classifying.                This average pooling
                      Something-Something v2 (SSv2) [25] contains 220 000                                baseline performs the worst, and has a larger accuracy drop
                  videos, with durations ranging from 2 to 6 seconds. In con-                            onEpicKitchens,suggestingthatthisdatasetrequires more
                  trast to the other datasets, the objects and backgrounds in                            detailed modelling of temporal relations.
                  the videos are consistent across different action classes, and                              As described in Sec. 3.3, all factorised variants of our
                  this dataset thus places more emphasis on a model’s ability                            model use signiﬁcantly fewer FLOPs than the unfactorised
                  to recognise ﬁne-grained motion cues.                                                  Model 1, as the attention is computed separately over
                  Inference        The input to our network is a video clip of 32                        spatial- and temporal-dimensions. Model 4 adds no addi-
                  framesusingastrideof2,unlessotherwisementioned,sim-                                    tional parameters to the unfactorised Model 1, and uses the
                  ilar to [20, 19]. Following common practice, at inference                              least compute. The temporal transformer encoder in Model
                  time, we process multiple views of a longer video and aver-                            2operates on only nt tokens, which is why there is a barely
                  age per-view logits to obtain the ﬁnal result. Unless other-                           a change in compute and runtime over the average pool-
                  wise speciﬁed, we use a total of 4 views per video (as this                            ing baseline, even though it improves the accuracy substan-
                  is sufﬁcient to “see” the entire video clip across the various                         tially (3% on Kinetics and 4.9% on Epic Kitchens). Fi-
                  datasets), and ablate these and other design choices next.                             nally, Model 3 requires more compute and parameters than
                                                                                                         the other factorised models, as its additional self-attention
                  4.2. Ablation study                                                                    block means that it performs another query-, key-, value-
                  Input encoding           We ﬁrst consider the effect of different                      and output-projection in each transformer layer [67].
                  input encoding methods (Sec. 3.2) using our unfactorised                               Model regularisation                 Pure-transformer          architectures
                  model(Model1)andViViT-BonKinetics400. Aswepass                                         such as ViT [17] are known to require large training
                  32-frame inputs to the network, sampling 8 frames and ex-                              datasets, and we observed overﬁtting on smaller datasets
                  tracting tubelets of length t = 4 correspond to the same                               like Epic Kitchens and SSv2, even when using an ImageNet
                  number of tokens in both cases. Table 1 shows that tubelet                             pretrained model. In order to effectively train our models
                  embedding initialised using the “central frame” method                                 onsuchdatasets, we employed several regularisation strate-
                  (Eq. 9) performs well, outperforming the commonly-used                                 gies that we ablate using our “Factorised encoder” model
                  “ﬁlter inﬂation” initialisation method [8, 21] by 1.6%, and                            in Tab. 3. We note that these regularisers were originally
                  “uniform frame sampling” by 0.7%. We therefore use this                                proposed for training CNNs, and that [63] have recently
                                                                                                     6841
