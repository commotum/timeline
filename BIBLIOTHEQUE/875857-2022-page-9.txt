               Lawrence, N.; Loewen, P.; Forbes, M.; Backstrom, J.; and                 Advances in Neural Information Processing Systems, volume 32.
               Gopaluni, B. 2020. Almost Surely Stable Deep Dynamics. In                Curran Associates, Inc.
               Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin,        Revay, M.; and Manchester, I. 2020. Contracting implicit recur-
               H., eds., Advances in Neural Information Processing Systems, vol-        rent neural networks: Stable models with improved trainability. In
               ume33,18942–18953.CurranAssociates, Inc.                                 Learning for Dynamics and Control, 393–403. PMLR.
               LeCun, Y.; Cortes, C.; and Burges, C. 2010.               MNIST          Ruthotto, L.; and Haber, E. 2019. Deep neural networks motivated
               handwritten digit database.      ATT Labs [Online]. Available:           bypartial differential equations. Journal of Mathematical Imaging
               http://yann.lecun.com/exdb/mnist, 2.                                     and Vision, 1–13.
               Liao, R.; Xiong, Y.; Fetaya, E.; Zhang, L.; Yoon, K.; Pitkow, X.;        Ruthotto, L.; and Haber, E. 2021. An Introduction to Deep Gener-
               Urtasun, R.; and Zemel, R. 2018. Reviving and improving recur-           ative Modeling. arXiv preprint arXiv:2103.05180.
               rent back-propagation. In International Conference on Machine            Ruthotto, L.; Osher, S. J.; Li, W.; Nurbekyan, L.; and Fung, S. W.
               Learning, 3082–3091. PMLR.                                               2020. Amachinelearningframeworkforsolvinghigh-dimensional
               Lin, A. T.; Fung, S. W.; Li, W.; Nurbekyan, L.; and Osher, S. J.         meanﬁeld game and mean ﬁeld control problems. Proceedings of
               2021. Alternating the population and control neural networks to          the National Academy of Sciences, 117(17): 9183–9193.
               solve high-dimensional stochastic mean-ﬁeld games. Proceedings                 ´
               of the National Academy of Sciences, 118(31).                            Sokolic, J.; Giryes, R.; Sapiro, G.; and Rodrigues, M. R. 2017. Ro-
               Look, A.; Doneva, S.; Kandemir, M.; Gemulla, R.; and Pe-                 bust large margin deep neural networks. IEEE Transactions on
               ters, J. 2020.   Differentiable Implicit Layers.   arXiv preprint        Signal Processing, 65(16): 4265–4280.
               arXiv:2010.07078.                                                        Udell, M.; and Townsend, A. 2019. Why are big data matrices
               Lorraine, J.; Vicol, P.; and Duvenaud, D. 2020. Optimizing mil-          approximately low rank? SIAM Journal on Mathematics of Data
               lions of hyperparameters by implicit differentiation. In Interna-        Science, 1(1): 144–160.
               tional Conference on Artiﬁcial Intelligence and Statistics, 1540–        Van der Maaten, L.; and Hinton, G. 2008. Visualizing data using
               1552. PMLR.                                                              t-SNE. Journal of machine learning research, 9(11).
               Lu, Y.; Zhong, A.; Li, Q.; and Dong, B. 2018. Beyond ﬁnite layer         Von Neumann, J. 1959. On the theory of games of strategy. Con-
               neural networks: Bridging deep architectures and numerical differ-       tributions to the Theory of Games, 4: 13–42.
               ential equations. In International Conference on Machine Learn-          Weinan, E. 2017. A proposal on machine learning via dynamical
               ing, 3276–3285. PMLR.                                                    systems. Communications in Mathematics and Statistics, 5(1): 1–
               Lu,Z.;Pu,H.;Wang,F.;Hu,Z.;andWang,L.2017.Theexpressive                   11.
               power of neural networks: A view from the width. arXiv preprint          Winston,E.;andKolter,J.Z.2020. Monotoneoperatorequilibrium
               arXiv:1709.02540.                                                        networks. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan,
               Luketina, J.; Berglund, M.; Greff, K.; and Raiko, T. 2016. Scalable      M.F.; and Lin, H., eds., Advances in Neural Information Process-
               gradient-based tuning of continuous regularization hyperparame-          ing Systems, volume 33, 10718–10728. Curran Associates, Inc.
               ters. In International conference on machine learning, 2952–2960.        Zhang, Q.; Gu, Y.; Mateusz, M.; Baktashmotlagh, M.; and Eriks-
               PMLR.                                                                    son, A. 2020. Implicitly deﬁned layers in neural networks. arXiv
               Moore, E. H. 1920. On the reciprocal of the general algebraic ma-        preprint arXiv:2003.01822.
               trix. Bulletin of the American Mathematical Society, 26: 394–395.
               Netzer, Y.; Wang, T.; Coates, A.; Bissacco, A.; Wu, B.; and Ng,
               A. Y. 2011. Reading digits in natural images with unsupervised
               feature learning. In NIPS Workshop on Deep Learning and Unsu-
               pervised Feature Learning.
               Onken, D.; and Ruthotto, L. 2020.        Discretize-Optimize vs.
               Optimize-Discretize for Time-Series Regression and Continuous
               Normalizing Flows. arXiv preprint arXiv:2005.13420.
               Onken, D.; Wu Fung, S.; Li, X.; and Ruthotto, L. 2021.        OT-
               Flow: Fast and Accurate Continuous Normalizing Flows via Op-
               timal Transport. Proceedings of the AAAI Conference on Artiﬁcial
               Intelligence, 35(10): 9223–9232.
               Osher, S.; Shi, Z.; and Zhu, W. 2017. Low dimensional manifold
               model for image processing. SIAM Journal on Imaging Sciences,
               10(4): 1669–1690.
               Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.; DeVito,
               Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A. 2017. Auto-
               matic differentiation in PyTorch.
               Penrose, R. 1955. A generalized inverse for matrices. In Mathe-
               matical Proceedings of the Cambridge Philosophical Society, vol-
               ume51,406–413.CambridgeUniversity Press.
                    ´
               Peyre, G. 2009. Manifold models for signals and images. Com-
               puter vision and image understanding, 113(2): 249–260.
               Rajeswaran, A.; Finn, C.; Kakade, S. M.; and Levine, S. 2019.
               Meta-LearningwithImplicitGradients. InWallach,H.;Larochelle,
                                          ´
               H.;Beygelzimer,A.;d'Alche-Buc,F.;Fox,E.;andGarnett,R.,eds.,
                                                                                 6656
