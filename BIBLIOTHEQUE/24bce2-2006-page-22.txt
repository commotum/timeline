             1548                           G.Hinton,S.Osindero,andY.-W.Teh
                The superior classiﬁcation performance of discriminative learning
                 methods holds only for domains in which it is not possible to learn
                 a good generative model. This set of domains is being eroded by
                 Moore’slaw.
             AppendixA: ComplementaryPriors
               A.1 General Complementarity. Consider a joint distribution over ob-
             servables,x,andhiddenvariables,y.Foragivenlikelihoodfunction, P(x|y),
             we deﬁne the corresponding family of complementary priors to be those
             distributions, P(y), for which the joint distribution, P(x,y) = P(x|y)P(y),
             leadstoposteriors, P(y|x),thatexactlyfactorize,thatis,leadstoaposterior
             that can be expressed as P(y|x) = j P(yj|x).
               Not all functional forms of likelihood admit a complementary prior. In
             this appendix, we show that the following family constitutes all likelihood
             functions admitting a complementary prior,
                 P(x|y)=  1  exp	j(x,yj)+β(x)

                         (y)      j
                            	                        

                       =exp     j(x,yj)+β(x)−log(y) ,               (A.1)
                               j
             where  is the normalization term. For this assertion to hold, we need to
             assume positivity of distributions: that both P(y) > 0andP(x|y) > 0 for
             everyvalueofyandx.Thecorrespondingfamilyofcomplementarypriors
             then assumes the form
                 P(y) = 1 exp	log(y)+αj(yj)
,                       (A.2)
                        C                j
             where C is a constant to ensure normalization. This combination of func-
             tional forms leads to the following expression for the joint,
                 P(x,y) = 1 exp	j(x,yj)+β(x)+αj(yj)
.              (A.3)
                          C       j                 j
               To prove our assertion, we need to show that every likelihood func-
             tion of form equation A.1 admits a complementary prior and vice versa.
             First, it can be directly veriﬁed that equation A.2 is a complementary prior
             for the likelihood functions of equation A.1. To show the converse, let us
             assume that P(y) is a complementary prior for some likelihood function
             P(x|y). Notice that the factorial form of the posterior simply means that the
