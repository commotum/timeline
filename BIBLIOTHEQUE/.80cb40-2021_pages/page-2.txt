                                            per-pixel classification loss                                 building                           sky                                     Ø
                                                                         K                                                                                                              K + 1
                                                                        }                                                                                                              }
                                                                                         prediction 1                    prediction 2                          prediction N
                                                               per-pixel class
                                                                 predictions                                                                                    per-pixel binary mask loss
                                                                                                                                                                per-mask classification loss
                                         Figure 1: Per-pixel classiﬁcation vs. mask classiﬁcation. (left) Semantic segmentation with per-
                                         pixel classiﬁcation applies the same classiﬁcation loss to each location. (right) Mask classiﬁcation
                                         predicts a set of binary masks and assigns a single class to each mask. Each prediction is supervised
                                         with a per-pixel binary mask loss and a classiﬁcation loss. Matching between the set of predictions
                                         and ground truth segments can be done either via bipartite matching similarly to DETR [3] or by
                                         ﬁxed matching via direct indexing if the number of predictions and classes match, i.e., if N = K.
                                         each consisting of a class prediction and a mask embedding vector. The mask embedding vector is
                                         used to get the binary mask prediction via a dot product with the per-pixel embedding obtained from
                                         an underlying fully-convolutional network. The new model solves both semantic- and instance-level
                                         segmentation tasks in a uniﬁed manner: no changes to the model, losses, and training procedure are
                                         required. Speciﬁcally, for semantic and panoptic segmentation tasks alike, MaskFormer is supervised
                                         with the same per-pixel binary mask loss and a single classiﬁcation loss per mask. Finally, we design
                                         a simple inference strategy to blend MaskFormer outputs into a task-dependent prediction format.
                                         Weevaluate MaskFormer on ﬁve semantic segmentation datasets with various numbers of categories:
                                         Cityscapes [13] (19 classes), Mapillary Vistas [31] (65 classes), ADE20K [49] (150 classes), COCO-
                                         Stuff-10K [2] (171 classes), and ADE20K-Full [49] (847 classes). While MaskFormer performs on
                                         parwithper-pixelclassiﬁcationmodelsforCityscapes,whichhasafewdiverseclasses,thenewmodel
                                         demonstrates superior performance for datasets with larger vocabulary. We hypothesize that a single
                                         class prediction per mask models ﬁne-grained recognition better than per-pixel class predictions.
                                         MaskFormerachievesthenewstate-of-the-art on ADE20K (55.6 mIoU) with Swin-Transformer [27]
                                         backbone, outperforming a per-pixel classiﬁcation model [27] with the same backbone by 2.1 mIoU,
                                         while being more efﬁcient (10% reduction in parameters and 40% reduction in FLOPs).
                                         Finally, we study MaskFormer’s ability to solve instance-level tasks using two panoptic segmentation
                                         datasets: COCO [26, 22] and ADE20K [49]. MaskFormer outperforms a more complex DETR
                                         model[3] with the same backbone and the same post-processing. Moreover, MaskFormer achieves
                                         the new state-of-the-art on COCO (52.7 PQ), outperforming prior state-of-the-art [38] by 1.6 PQ.
                                         Ourexperiments highlight MaskFormer’s ability to unify instance- and semantic-level segmentation.
                                         2      Related Works
                                         Both per-pixel classiﬁcation and mask classiﬁcation have been extensively studied for semantic
                                         segmentation. In early work, Konishi and Yuille [23] apply per-pixel Bayesian classiﬁers based on
                                         local image statistics. Then, inspired by early works on non-semantic groupings [11, 33], mask
                                         classiﬁcation-based methods became popular demonstrating the best performance in PASCAL VOC
                                         challenges [16]. Methods like O2P [4] and CFM [14] have achieved state-of-the-art results by
                                         classifying mask proposals [5, 36, 1]. In 2015, FCN [28] extended the idea of per-pixel classiﬁcation
                                         to deep nets, signiﬁcantly outperforming all prior methods on mIoU (a per-pixel evaluation metric
                                         which particularly suits the per-pixel classiﬁcation formulation of segmentation).
                                         Per-pixel classiﬁcation became the dominant way for deep-net-based semantic segmentation since
                                         the seminal work of Fully Convolutional Networks (FCNs) [28]. Modern semantic segmentation
                                         models focus on aggregating long-range context in the ﬁnal feature map: ASPP [6, 7] uses atrous
                                         convolutions with different atrous rates; PPM [46] uses pooling operators with different kernel sizes;
                                         DANet[17],OCNet[45],andCCNet[21]usedifferentvariants of non-local blocks [39]. Recently,
                                         SETR[47]andSegmenter[34]replacetraditional convolutional backbones with Vision Transformers
                                         (ViT) [15] that capture long-range context starting from the very ﬁrst layer. However, these concur-
                                         rent Transformer-based [37] semantic segmentation approaches still use a per-pixel classiﬁcation
                                                                                                                    2
