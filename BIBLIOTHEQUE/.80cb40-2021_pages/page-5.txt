                                             3.4      Mask-classiÔ¨Åcation inference
                                             First, we present a simple general inference procedure that converts mask classiÔ¨Åcation outputs
                                                              N
                                             {(p ,m )}               to either panoptic or semantic segmentation output formats. Then, we describe a
                                                  i      i    i=1
                                             semantic inference procedure speciÔ¨Åcally designed for semantic segmentation. We note, that the
                                             speciÔ¨Åc choice of inference strategy largely depends on the evaluation metric rather than the task.
                                             General inference partitions an image into segments by assigning each pixel [h,w] to one of the N
                                             predicted probability-mask pairs via argmax                                         p (c ) ¬∑ m [h,w]. Here c is the most likely class
                                                                                                                      i:c 6=‚àÖ      i   i         i                       i
                                             label c = argmax                                     p (c) for eachiprobability-mask pair i. Intuitively, this procedure
                                                        i                    c‚àà{1,...,K,‚àÖ} i
                                             assigns a pixel at location [h,w] to probability-mask pair i only if both the most likely class probability
                                             p (c ) and the mask prediction probability m [h,w] are high. Pixels assigned to the same probability-
                                               i    i                                                                i
                                             maskpairiformasegmentwhereeachpixelislabelledwithci. Forsemanticsegmentation,segments
                                             sharing the same category label are merged; whereas for instance-level segmentation tasks, the index
                                             i of the probability-mask pair helps to distinguish different instances of the same class. Finally, to
                                             reduce false positive rates in panoptic segmentation we follow previous inference strategies [3, 22].
                                             SpeciÔ¨Åcally, weÔ¨Ålteroutlow-conÔ¨Ådencepredictionspriortoinferenceandremovepredictedsegments
                                             that have large parts of their binary masks (m > 0.5) occluded by other predictions.
                                                                                                                       i
                                             Semantic inference is designed speciÔ¨Åcally for semantic segmentation and is done via a simple
                                             matrix multiplication. We empirically Ô¨Ånd that marginalization over probability-mask pairs, i.e.,
                                                                            P
                                             argmax                             N p(c)¬∑m [h,w],yieldsbetterresults than the hard assignment of each pixel
                                                           c‚àà{1,...,K}          i=1 i                i
                                             to a probability-mask pair i used in the general inference strategy. The argmax does not include the
                                            ‚Äúno object‚Äù category (‚àÖ) as standard semantic segmentation requires each output pixel to take a label.
                                                                                                                                      P
                                             Note, this strategy returns a per-pixel class probability                                    N p(c)¬∑m [h,w]. However,weobserve
                                                                                                                                          i=1 i                i
                                             that directly maximizing per-pixel class likelihood leads to poor performance. We hypothesize, that
                                             gradients are evenly distributed to every query, which complicates training.
                                             4      Experiments
                                             We demonstrate that MaskFormer seamlessly uniÔ¨Åes semantic- and instance-level segmentation
                                             tasks by showing state-of-the-art results on both semantic segmentation and panoptic segmentation
                                             datasets. Then, we ablate the MaskFormer design conÔ¨Årming that observed improvements in semantic
                                             segmentation indeed stem from the shift from per-pixel classiÔ¨Åcation to mask classiÔ¨Åcation.
                                             Datasets.           We study MaskFormer using four widely used semantic segmentation datasets:
                                             ADE20K [49] (150 classes) from the SceneParse150 challenge [48], COCO-Stuff-10K [2] (171
                                             classes), Cityscapes [13] (19 classes), and Mapillary Vistas [31] (65 classes). In addition, we use
                                             the ADE20K-Full [49] dataset annotated in an open vocabulary setting (we keep 874 classes that are
                                             presentinbothtrainandvalidationsets). ForpanoticsegmenationevaluationweuseCOCO[26,2,22]
                                             (80 ‚Äúthings‚Äù and 53 ‚Äústuff‚Äù categories) and ADE20K-Panoptic [49, 22] (100 ‚Äúthings‚Äù and 50 ‚Äústuff‚Äù
                                             categories). Please see the appendix for detailed descriptions of all used datasets.
                                             Evaluationmetrics. ForsemanticsegmentationthestandardmetricismIoU(meanIntersection-over-
                                             Union) [16], a per-pixel metric that directly corresponds to the per-pixel classiÔ¨Åcation formulation.
                                             Tobetter illustrate the difference between segmentation approaches, in our ablations we supplement
                                             mIoUwithPQSt (PQstuff)[22], a per-region metric that treats all classes as ‚Äústuff‚Äù and evaluates
                                             each segment equally, irrespective of its size. We report the median of 3 runs for all datasets, except
                                             for Cityscapes where we report the median of 5 runs. For panoptic segmentation, we use the standard
                                             PQ(panoptic quality) metric [22] and report single run results due to prohibitive training costs.
                                             Baseline models. On the right we                                                                per-pixel                        transformer            per-pixel
                                             sketchtheusedper-pixelclassiÔ¨Åcation                                                               loss                             decoder         Ì†µ„åµ√óÌ†µ„åµ   loss
                                             baselines. The PerPixelBaselineuses                                                                  t                                                       t
                                             thepixel-levelmoduleofMaskFormer                                                       pixel         u                            pixel                      u
                                                                                                               backbone                           p      backbone                                         p
                                                                                                                                  decoder         t                          decoder                      t
                                             and directly outputs per-pixel class                                                                 ou                                                      ou
                                             scores. For a fair comparison, we de-                                                           Ì†µ„åµ√óÌ†µ„åµ√óÌ†µ„åµ                                  Ì†µ„åµ√óÌ†µ„åµ√óÌ†µ„åµ      Ì†µ„åµ√óÌ†µ„åµ√óÌ†µ„åµ
                                             sign PerPixelBaseline+ which adds                                       (a) PerPixelBaseline                             (b) PerPixelBaseline+
                                             the transformer module and mask em-
                                             bedding MLPtothePerPixelBaseline. Thus, PerPixelBaseline+ and MaskFormer differ only in the
                                             formulation: per-pixel vs. mask classiÔ¨Åcation. Note that these baselines are for ablation and we
                                             compare MaskFormer with state-of-the-art per-pixel classiÔ¨Åcation models as well.
                                                                                                                             5
