                                Acknowledgments                                 words: Transformers for image recognition at scale. arXiv
              This research was supported by the MSIT(Ministry of Sci-          preprint arXiv:2010.11929.
              ence and ICT), Korea, under the ITRC(Information Tech-            Hassani, A.; Walton, S.; Shah, N.; Abuduweili, A.; Li, J.;
              nology Research Center) support program(IITP-2024-RS-             and Shi, H. 2022. Escaping the Big Data Paradigm with
              2023-00258649, 80%) supervised by the IITP(Institute for          CompactTransformers. arXiv:2104.05704.
              Information & Communications Technology Planning &                                         ´
              Evaluation) and was partly supported by the IITP grant            He,K.;Gkioxari,G.;Dollar,P.;andGirshick,R.2017. Mask
              funded by the Korea government (MSIT) (No.RS-2022-                r-cnn. In Proceedings of the IEEE international conference
              00143524, Development of Fundamental Technology and               oncomputervision, 2961–2969.
              Integrated Solution for Next-Generation Automatic Artifi-         Heo, B.; Park, S.; Han, D.; and Yun, S. 2024. Rotary po-
              cial Intelligence System) and (No.RS2023-00225630, De-            sition embedding for vision transformer.    arXiv preprint
              velopment of Artificial Intelligence for Text-based 3D            arXiv:2403.13298.
              MovieGeneration).                                                 Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple
                                                                                layers of features from tiny images.
                                     References                                 Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-
                                                                                                ´
              Ba, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-       manan, D.; Dollar, P.; and Zitnick, C. L. 2014. Microsoft
              malization. arXiv preprint arXiv:1607.06450.                      coco: Common objects in context.      In Computer Vision–
              Bello, I.; Zoph, B.; Vaswani, A.; Shlens, J.; and Le, Q. V.       ECCV 2014: 13th European Conference, Zurich, Switzer-
              2019. Attention augmented convolutional networks. In Pro-         land, September 6-12, 2014, Proceedings, Part V 13, 740–
              ceedings of the IEEE/CVF international conference on com-         755. Springer.
              puter vision, 3286–3295.                                          Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin,
              Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,       S.; and Guo, B. 2021. Swin transformer: Hierarchical vi-
              A.; and Zagoruyko, S. 2020. End-to-end object detection           sion transformer using shifted windows. In Proceedings of
              with transformers. In European conference on computer vi-         the IEEE/CVFinternational conference on computer vision,
              sion, 213–229. Springer.                                          10012–10022.
              Chang, S.; Wang, P.; Lin, M.; Wang, F.; Zhang, D. J.; Jin,        Loshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-
              R.; and Shou, M. Z. 2023. Making vision transformers ef-          cay Regularization. In International Conference on Learn-
              ficient from a token sparsification view. In Proceedings of       ing Representations.
              the IEEE/CVF Conference on Computer Vision and Pattern            Raghu, M.; Unterthiner, T.; Kornblith, S.; Zhang, C.; and
              Recognition, 6195–6205.                                           Dosovitskiy, A. 2021. Do vision transformers see like con-
              Chen,K.;Wang,J.;Pang,J.;Cao,Y.;Xiong,Y.;Li,X.;Sun,                volutional neural networks? Advances in neural information
              S.; Feng, W.; Liu, Z.; Xu, J.; Zhang, Z.; Cheng, D.; Zhu, C.;     processing systems, 34: 12116–12128.
              Cheng, T.; Zhao, Q.; Li, B.; Lu, X.; Zhu, R.; Wu, Y.; Dai,        Shaw, P.; Uszkoreit, J.; and Vaswani, A. 2018.        Self-
              J.; Wang, J.; Shi, J.; Ouyang, W.; Loy, C. C.; and Lin, D.        attention with relative position representations.     arXiv
              2019. MMDetection: Open MMLabDetection Toolbox and                preprint arXiv:1803.02155.
              Benchmark. arXiv preprint arXiv:1906.07155.                       Strudel, R.; Garcia, R.; Laptev, I.; and Schmid, C. 2021.
              Chen, Z.; Duan, Y.; Wang, W.; He, J.; Lu, T.; Dai, J.; and        Segmenter: Transformer for semantic segmentation. In Pro-
              Qiao, Y. 2022. Vision transformer adapter for dense predic-       ceedings of the IEEE/CVF international conference on com-
              tions. arXiv preprint arXiv:2205.08534.                           puter vision, 7262–7272.
              Chu, X.; Tian, Z.; Wang, Y.; Zhang, B.; Ren, H.; Wei, X.;         Touvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,
              Xia, H.; and Shen, C. 2021a. Twins: Revisiting the design of               ´
                                                                                A.; and Jegou, H. 2021. Training data-efficient image trans-
              spatial attention in vision transformers. Advances in neural      formers & distillation through attention.  In International
              information processing systems, 34: 9355–9366.                    conference on machine learning, 10347–10357. PMLR.
              Chu,X.;Tian,Z.;Zhang,B.;Wang,X.;andShen,C.2021b.                  Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
              Conditional positional encodings for vision transformers.         L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
              arXiv preprint arXiv:2102.10882.                                  tention is all you need. Advances in neural information pro-
              Contributors, M. 2020.    MMSegmentation: OpenMMLab               cessing systems, 30.
              Semantic Segmentation Toolbox and Benchmark.          https:      Wang, Y.; Xu, Z.; Wang, X.; Shen, C.; Cheng, B.; Shen, H.;
              //github.com/open-mmlab/mmsegmentation.                           and Xia, H. 2021. End-to-end video instance segmentation
              Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-       with transformers. In Proceedings of the IEEE/CVF con-
              Fei, L. 2009. Imagenet: A large-scale hierarchical image          ference on computer vision and pattern recognition, 8741–
              database. In 2009 IEEE conference on computer vision and          8750.
              pattern recognition, 248–255. Ieee.                               Wu,K.;Peng,H.;Chen,M.;Fu,J.;andChao,H.2021. Re-
              Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,          thinking and improving relative position encoding for vision
              D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;        transformer. In Proceedings of the IEEE/CVF International
              Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16      Conference on Computer Vision, 10033–10041.
