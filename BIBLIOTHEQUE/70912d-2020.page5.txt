                                                                    Generative Pretraining from Pixels
                 models have the same width and connectivity pattern at                   Table 1. Comparing linear probe accuracies between our models
                 every layer. Our ﬁrst experiment studies how representa-                 and state-of-the-art models utilizing unsupervised ImageNet trans-
                 tion quality varies over one set of candidate representations:           fer or supervised ImageNet transfer.
                 different layers of a generative model. We observe a very
                 different behavior from supervised learning: representations                  Model            Acc     UnsupTransfer      SupTransfer
                 ﬁrst improveasafunctionofdepth,andthen,startingaround                         CIFAR-10
                 the middle layer, begin to deteriorate until the penultimate                  ResNet-152       94             √                 √
                 layer (Figure 2).                                                             SimCLR           95.3
                 Thisbehaviorpotentiallysuggeststhatthesegenerativemod-                        iGPT-L           96.3           √
                 els operate in two phases. In the ﬁrst phase, each position                   CIFAR-100
                 gathers information from its surrounding context in order                     ResNet-152       78.0           √                 √
                 to build a more global image representation. In the second                    SimCLR           80.2           √
                 phase, this contextualized input is used to solve the condi-                  iGPT-L           82.8
                 tional next pixel prediction task. This could resemble the                    STL-10
                 behavior of encoder-decoder architectures common across                       AMDIM-L          94.2           √
                 deep learning, but learned within a monolithic architecture                   iGPT-L           95.5           √
                 via a pre-training objective.
                 Consequently, when evaluating a generative model with                    several model capacities, with higher capacity models
                 a linear probe, it is important to search for the best layer.            achieving better validation losses. This highlights the im-
                 Taking the ﬁnal layer on CIFAR-10 decreases performance                  portance of scale for our approach. Note that for a given
                 by 2.4%, the difference between a baseline and a state-of-               validation loss value, bigger models also perform better.
                 the-art result. In all settings, we ﬁnd that the dependence of
                 representation quality on depth is strongly unimodal.                    4.3. Linear Probes on CIFAR and STL-10
                 4.2. Better Generative Models Learn Better                               In addition to CIFAR-10, we also evaluate linear probes on
                      Representations                                                     CIFAR-100 and STL-10 (Figure 2) to check whether the
                                                                                          learned representations are useful across multiple datasets.
                                                                                          Forthisevaluationsetting, we achieve state-of-the-art across
                                                                                          the entire spectrum of pre-training approaches (Table 1).
                                                                                          For example, on CIFAR-10, our model achieves 96.3%, out-
                                                                                          performing both SimCLR (pre-trained on ImageNet without
                                                                                          labels) and a ResNet-152 (pre-trained on ImageNet with
                                                                                          labels). In fact, on all three datasets a linear classiﬁer ﬁt to
                                                                                          the representations of iGPT-L outperforms the end-to-end
                                                                                          supervised training of a WideResNet baseline.
                                                                                          Note that our model is trained at the same input resolution
                Figure 3. Plot of representation quality as a function of validation      (IR) as CIFAR, whereas models trained at the standard Im-
                 generative loss. Each line tracks a model throughout generative          ageNet IR may experience distribution shock upon linear
                 pre-training: the dotted markers denote checkpoints at steps 65K,        evaluation. As a counterpoint, though STL-10 has an IR
                                                                                                2
                131K, 262K, 524K, and 1000K. The positive slope suggests a link           of 96 ×3, we still outperform AMDIM-L when we down-
                 between improved generative performance and improved represen-           sample to 322 × 3 before linear probing. We also note that
                 tation quality. Larger models produce better representations than        ﬁne-tuning should allow models trained at high IR to adjust
                 smaller ones both at the end of training and at the same value of        to low resolution input.
                 validation loss. iGPT-XL is not shown since it was trained on a
                 different dataset.                                                       4.4. Linear Probes on ImageNet
                 Usingthelinearprobeasatoolformeasuringrepresentation                     Recently, there has been a resurgence of interest in unsuper-
                 quality, we investigate whether better generative models (as             vised and self-supervised learning on ImageNet, evaluated
                 measured by log-prob on held-out data) also learn better                 using linear probes on ImageNet. This is a particularly
                 representations.                                                         difﬁcult setting for us, since we cannot efﬁciently train at
                                                                                          the standard ImageNet input resolution (IR). Indeed, when
                                                                                                                                                        2
                 In Figure 3, we see that as validation loss on the auto-                 training iGPT-L with a model resolution (MR) of 32 , we
                 regressive objective decreases throughout training, linear               achieve only 60.3% best-layer linear probe accuracy. As
                 probe accuracy increases as well. This trend holds across                with CIFAR-10, scale is critical to our approach: iGPT-
