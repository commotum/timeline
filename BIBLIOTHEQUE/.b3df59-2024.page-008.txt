                                                    TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                      100                                                                             align well with TTT’s ability to adapt to latent structural
                               Zero-Shot
                               ICL                            86.7                                    regularities during test-time.
                               TTT                                                    85.7
                                                 81.9                       78.7         80.1
                       80                                                                             Conversely, tasks requiring explicit step-by-step computa-
                                    68.8                   69.5                   71.0                tion show limited gains with TTT. For instance, Boolean
                                              60.5                      63.8
                       60                                            58.0                             Expressionsdeclinedfrom85.7%to80.4%underTTT.This
                                          53.6          53.2                                          task’s algorithmic nature—dependent on sequential reason-
                      Accuracy (%)40                                                                  ing rather than pattern-based transduction—and its likely
                                                                                                      pre-training exposure suggest TTT’s updates may not re-
                                18.5                                                                  solve its specific demands. While these particular obser-
                       20                                                                             vations align with our hypothesis, the reason certain tasks
                              3.6                                                                     benefit more from TTT remains an open question.
                        0       Dyck      Ruin Names    Movie Reco   Hyperbaton    Boolean Ex
                              Languages                 mmendation                 pressions
                   Figure 9. BIG-Bench Hard results for tasks with the largest                        6. Related Work
                   TTT-ICLscoredifferences. The four tasks on the left show the                       Test-time training            The idea of updating model param-
                   most significant improvements with TTT over ICL, while the task                    eters at test-time using instance-specific data traces back
                   on the right has the lowest TTT score relative to ICL. Full task-                  to early work on local learning (Bottou & Vapnik, 1992).
                   specific results are given in Appendix F.2.                                        Morerecently, Sun et al. (2020) propose a simple test-time
                   Sharedadapter UnlikeonARC,usingasharedadapter                                      self-supervision scheme to adapt an image classifier when
                   improvesperformanceonBBH,indicatingthattasksinBBH                                  facing distribution shifts. In language modeling, Hardt &
                   do not confound each other during training. On the ARC                             Sun(2024) fine-tune on retrieved neighbors at test-time for
                                                                                                                                    ¨
                   dataset, each puzzle has the same input format, so distin-                         notable gains, while Hubotter et al. (2025) optimize retrieval
                   guishing among multiple tasks is difficult, and we may have                        via active data selection.
                   conflicting gradients with a single adapter. In BBH, however,
                   distinguishing tasks is trivial (the instructions differ in plain                  ARCchallenge AbstractionandReasoningCorpus(ARC;
                   text), and many tasks are mutually helpful. For instance,                          Chollet, 2019; Chollet et al., 2025) is a collection of ex-
                   updating on Logical Deduction Five Objects also aids Logi-                         tremely challenging few-shot visual reasoning problems.
                   cal Deduction Three Objects, without hurting Word Sorting.                         Most approaches to ARC fall into two main categories:
                   Althoughthisisnolongertest-timetraining on distinct tasks                          program synthesis and fully neural. Program synthesis ap-
                   presented individually at test time, it can be interpreted as                      proaches(Buttetal.,2024;Wangetal.,2024;Lietal.,2025;
                   TTTontheentiredataset presented collectively at test time.                         Greenblatt, 2024) first try to find the transformation function
                                                                                                      f, and then apply it to the test example. Fully neural ap-
                   5.4. Task-Specific Analysis                                                        proaches (Veldkamp et al., 2023; Bober-Irizar & Banerjee,
                                                                                                      2024) try to directly predict the output ytest, only implicitly
                   Our task-specific results show that performance improve-                           modeling f. In this work, we use a fully neural approach,
                   ments from TTT are highly task-dependent. Among the                                using an LM to predict the test outputs. Recent work has
                   27 tasks in BBH, TTT results in a performance decline of                           explored hybrid methods, leveraging inference scaling and
                   at least 2% compared to ICL in only 2 tasks. In contrast,                          deep learning-guided program synthesis (Greenblatt, 2024;
                   12 tasks show an improvement of at least 2%, with 9 of                             Lietal., 2025). Similarly, we find that integrating our neural
                   these showing improvements of at least 5%. The four tasks                          model with program synthesis improves performance.
                   with the most significant performance boost from TTT over
                   ICLorzero-shot and the task with the most significant per-                         7. Conclusion
                   formance decrease are shown in Figure 9. These tasks in
                   order of TTT’s improvement over ICL are Dyck Languages                             Weconductaninvestigationoftest-timetraininganddemon-
                   (parentheses matching), Ruin Names (humorous name modi-                            strate that it can significantly improve LM performance on
                   fications), Movie Recommendation (choosing similar films),                         abstract reasoning and few-shot learning tasks, namely the
                   Hyperbaton (adjective ordering), and Boolean Expression                            Abstraction and Reasoning Corpus (ARC) and BIG-Bench
                   (evaluating a boolean expression). Detailed results for every                      Hard (BBH). Our key contributions include a robust TTT
                   task are given in Appendix F.2.                                                    framework with leave-one-out in-context task construction,
                   WehypothesizethatimprovementsfromTTTmaybedriven                                    the optimization setup, and the inference strategy after TTT.
                   by tasks involving distribution shifts and structured patterns.                    Ourresults reveal the potential of TTT to tackle novel rea-
                   Forexample,taskslikeDyckLanguagesandHyperbatonfol-                                 soning tasks, suggesting significant promise for test-time
                   low clear grammatical or programmatic rules, which could                           methods in advancing the next generation of LMs.
                                                                                                   8
