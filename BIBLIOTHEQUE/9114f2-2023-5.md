# Letâ€™s Verify Step by Step (2023)
Source: 9114f2-2023.pdf

## Core reasons
- Process supervision is shown to train far more reliable reward models than outcome supervision on the challenging MATH benchmark, framing the paper as a methodological advance in ML training and evaluation.
- The paper couples this supervision study with active learning plus the PRM800K release, which together deepen the practical foundations for constructing aligned reward models at scale.

## Evidence extracts
- "We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set." (p. 1)
- "We release PRM800K, the full dataset of human feedback used to train our state-of-the-art reward model, with the hope that removing this significant barrier to entry will catalyze related research on the alignment of large language models." (p. 13)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
