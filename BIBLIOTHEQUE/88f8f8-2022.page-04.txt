                4  Wen. et al.
                limited memory and fails to benefit from long-range temporal dependencies.
                Based on this, we propose Point Primitive Transformer(PPTr) which enjoys all
                three properties: point-convolution based, long-term supported and point-track
                avoided.
                Primitive Fitting. Primitive fitting is a long-standing problem of grouping
                points into specific geometric shapes such as plane, cuboid, cylinder and so on.
                Such process approximates and abstracts 3D shapes from low-level digitized
                point data to a succinct high-level parameterized representation. Two main-
                streamsolutionsofprimitivefittingingeometrycommunityareRANSAC[13][35]
                and region grow [31] [33]. Recently, neural networks have been developed by
                several works [43] [24] [42] [20] [38] to segment primitives. Because primitives
                extremely simplifies point data while keeps a relatively precise description of
                3D geometry, they are widely applied to downstream tasks like instance seg-
                mentation [20], reconstruction [5]and animation [37]. For example, [14] utilizes
                primitive shapes that are rich in underlying structures to reconstruct scanned
                object and transfer the structural information onto new objects. To directly deal
                with large-scale scenes, [23] distils organization of point cloud by partitioning
                heavy points into light shapes, showing the power of such compact yet rich rep-
                resentation. We inject primitives into our network, intending to spatially provide
                geometric-aware enhancement on local primitive region and temporally leverage
                long-range information in a memory efÏcient way.
                Transformer Network. Transformer is a powerful deep neural network based
                on self-attention mechanism [39] and is particularly suitable for modelling long-
                range dependencies [36] [7] [3]. It was firstly proposed in [39] for machine trans-
                lation task and further extended to vision community [1] [4] [8] [9] [40] [21] [27]
                . Very recently, Swin Transformer [30] proposes a hierarchical design for vision
                modelingatvariousscalesandyieldsimpressiveresults.SimilartoCNNs[22][17],
                Swin transformer builds hierarchical feature maps by merging image patches
                when layers go deeper, and strikes a balance between efÏciency and effectiveness
                by limiting self-attention to local windows while also supporting cross-window
                connection. In 4D point cloud understanding, prior leading work [10] performs
                self-attention globally and fails to leverage long-term dependencies effectively. As
                such, we design a hierarchical Primitive Point Transformer(PPTr) to alleviate
                ineffectiveness of global-wise attention and introduce intra-primitive point trans-
                former and primitive transformer that perform self-attention at point level and
                primitive level respectively. Intensive experiments have shown that our network
                outperforms the state-of-the-art methods for both 4D semantic segmentation
                and 4D action recognition.
                3 Pilot study: How does P4Transformer perform on
                  long-term point cloud videos?
                  4D point cloud video understanding has obtained much attention recently
                and researchers are actively seeking for backbones to capture descriptive spatial-
                temporal features. Among them, P4Transformer [10] is the leading one achieving
