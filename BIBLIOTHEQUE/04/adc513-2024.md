# SEED-Bench: Benchmarking Multimodal Large Language Models (Not specified in the paper.)
Source: adc513-2024.pdf

## Core reasons
- The paper proposes SEED-Bench as a benchmark for evaluating MLLM capabilities.
- It centers on a large-scale, human-annotated multiple-choice question set covering many evaluation dimensions.

## Evidence extracts
- "Wepropose SEED-Bench, a comprehensive benchmark" (p. 2)
- "24K human-annotated multiple-choice questions covering
27evaluation dimensions." (p. 3)

## Classification
Class name: Data, Benchmarks & Measurement
Class code: 4

$$
\boxed{4}
$$
