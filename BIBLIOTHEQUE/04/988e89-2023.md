# The Perils & Promises of Fact-checking with Large Language Models (2023)
Source: 988e89-2023.pdf

## Core reasons
- The paper centers on benchmarking GPT-3.5 and GPT-4 on the PolitiFact and Data Commons fact-checking datasets with and without contextual retrieval, positioning the work as a measurement study of LLM reliability.
- It quantifies how external context improves accuracy (above 80%/89% for non-ambiguous verdicts) and explores multilingual performance, reinforcing that the contribution is about evaluating and measuring fact-checking capability across conditions.

## Evidence extracts
- "Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions." (Abstract)
- "Notably, even without contextual information, GPT-3.5 and GPT-4 demonstrate good performance of 63-75% accuracy on average, which further improves to above 80% and 89% for non-ambiguous verdicts when context is incorporated." (Section 4 Conclusion)

## Classification
Class name: Data, Benchmarks & Measurement
Class code: 4

$$
\boxed{4}
$$
