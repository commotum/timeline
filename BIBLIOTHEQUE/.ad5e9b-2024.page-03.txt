                         For TB between 450M and 1.05 billion examples, models trained on 25M DB achieve the best
                         performances. Throughout training, the models trained on small data budgets learn fastest. Past
                         a certain TB, they overfit their training data, and their performance saturates. On the other hand,
                         models trained on large or unlimited DB perform the worst. For a TB of one billion examples,
                         models trained on 100M or unlimited data budgets only predict 37 and 27 GCD, way worse than
                         models trained on 25 and 50M (62 and 60). When learning GCD, smaller data budgets and more
                         frequent repetition allow for faster learning, and much better performance.
                         Modularmultiplication, with a TB of 600 million, tells a different story. Models with DB of 10M
                         or less, or 100M or more do not learn the task, and achieve about chance level accuracies, predicting
                         all outcomes as 0, for an accuracy slightly over 3% (Table 1). On the other hand, models trained
                         on 25 and 50M distinct examples (repeated 24 and 12 times on average) do learn the task: 25%
                         of models trained with this DB achieve 99% accuracy, and a majority achieves 50%. On this task,
                         learning emerges from repetition: models trained on small DB learn a task inaccessible to models
                         trained on larger DB. (note: increasing TB to 2B examples, some models trained on 100MDB do
                         learn, but none of the unlimited data models do).
                           Table 1: Multiplication modulo 67. Accuracy of models trained on a budget of 600 million data points.
                                                                               Data budget (millions)
                                                                   1    5    10    25      50    100   unlimited
                           Average accuracy (%)                   1.6   3.8  4.4   40.4   59.5   5.4     3.0
                           Numberofmodelsachieving99%accuracy     0/5   0/5  0/5   6/25   7/25   0/30    0/30
                           Numberofmodelsachieving50%+accuracy    0/5   0/5  0/5  13/25  22/25   0/30    0/30
                           Numberofmodelstrained                   5    5     5    25      25    30       30
                         These experiments clearly indicate that repetition helps learning. On both tasks, for a fixed training
                         budget, models trained on a small data budget, i.e. fewer distinct examples, repeated several times,
                         achieve much better performance than models trained from single-use examples, or repeated very
                         few times, as is customary in most recent works on language models (Muennighoff et al., 2023).
                         Smaller data budgets and repeated examples elicit “emergent learning”.
                         4   Two-set training
                         Wenowturntoadifferent problem: how to best use a given data budget? Because repetition helps
                         learning, we want a small subset of repeated examples, but we also observed that, after a certain
                         training budget, models trained on small datasets start overfitting, which cause their performance to
                         saturate. To balance these two effects, we propose two-set training: randomly splitting the training
                         sampleintoasmallsetofS examples,usedwithprobabilityp,andrepeatedmanytimes,andalarge
                         set, used with probability 1 − p, and repeated a few times. By doing so, we hope that the small set
                         fosters learning, while the large set prevents overfitting.
                         On the GCD problem, with a data budget of 100 million examples and a training budget of 600
                         million, models trained on a repeated set of 250,000 examples or less, used with a probability of p
                         of 0.25 or 0.5, predict more than 62 GCD on average (Figure 2), vs 27 in the single-set case. The
                         best results, 69 GCD, are achieved for S = 50,000 and p = 0.25. In this setting, small set examples
                         are repeated 3000 times on average, large set examples 4.5 times.
                         These results extend to unlimited data budgets, using a fixed set of S examples. The best choices
                         of p and S are roughly the same as with a DB of 100M (Figure 3 in Appendix B). For p = 0.25
                         and S = 50,000, two-set training achieves an average performance of 67 GCD on 6 models, a
                         spectacular improvement over models trained on a single set, which predict 25 GCD on average.
                         For smaller DB (25 or 50M), two-set training provides for faster learning (Figure 4, Appendix B).
                         For modular multiplication, we need larger samples of repeated examples: S between 2.5 and
                         10Mexamples for 100M DB, and S = 25M with unlimited DB (Figure 5 Appendix C). Examples
                         fromthesmallsetarerepeatedless: from 20 to 60 times, vs 3000 for the GCD. Yet, two-set training
                         results in improved performances for all data budgets over 25M (Table 2). More than 50% of all
                         the models we trained could learn modular multiplication with over 99% accuracy (90% with more
                         than 50% for DB larger than 50M). In contrast, single-set training achieved 99% accuracy for 25%
                         of models with DB 50M or less, and for none of the models trained on larger data budgets (Table 1).
                                                                    3
