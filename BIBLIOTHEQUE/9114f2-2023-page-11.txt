solve-rate is an additional indication that it has not encountered such problems
via test set contamination. Our generalization results from Section 5 further
strengthen our claim that test set contamination has not significantly impacted
this work, since we observe qualitatively similar results on problems that are
guaranteed to be uncontaminated.

7 Related Work

7.1. Outcome vs Process Supervision

In work closely related to our own, Uesato et al. (2022) compare the impact
of outcome and process supervision in the domain of grade school math. They
found that both methods led to similar final-answer error rates, and that process
supervision achieved those results with less data. While our core methodology is
very similar, there are three main details that differ. First, we use a more capable
model to collect PRM800K dataset and to perform our large-scale experiments.
However, our small-scale results in Section 4 suggest that large-scale models are
not necessary to observe benefits from process supervision. Second, we evaluate
on the MATH dataset, which is significantly more challenging than GSM8K.
Third, we collect a much larger quantity of process supervision data.

On the surface, the results from Uesato et al. (2022) may seem to conflict
with our claim that process supervision leads to better performance. However,
we believe the apparent conflict can be explained by the difference in the scale
of the supervision. The data scaling trend in Figure 4a suggests that a small
amount of process supervision and a large amount of outcome supervision do
in fact lead to similar performance, consistent with the results from Uesato
et al. (2022). The trend also shows that process supervision beats outcome
supervision when scaled up, even when judged based solely on outcomes. This
is consistent with our results in Section 3. We believe these results make a
strong case for using process supervision.

7.2 Synthetic Supervision

Similar to our work in Section 4, Gao et al. (2022) use a large reward model to
supervise the training of smaller models. They study the over-optimization that
occurs during RLHF, with experiments that require large quantities of human
preference data. To work around this challenge, they use a gold-standard reward
model to replace human feedback. Our use of a large-scale reward model to
supervise smaller reward models shares similarities with their approach.

7.3 Natural Language Reasoning

Several recent studies that have examined the reasoning ability of large language
models are implicitly relevant to our work. Lewkowycz et al. (2022) showed that
finetuning models on a large corpus of technical content led to significantly im-
proved performance on MATH. Wang et al. (2022) showed that: self-con.

12
