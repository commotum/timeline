             T. Han et al.                                                                           International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                             Table 7
                             Control ablation study results of different modules on the Area-5 of S3DIS dataset.
                               Attention mechanism                                 Adaptive weights       Virtual node      OA (%)       mAcc (%)        mIoU (%)
                               MLP       Scalar attention     Graph attention
                               âœ“                                                                                            87.1         68.6            61.7
                                         âœ“                                                                                  88.4         71.9            64.6
                                                              âœ“                                                             88.6         73.4            65.5
                                                              âœ“                    âœ“                                        90.1         76.1            70.4
                               âœ“                                                                          âœ“                 87.9         69.1            61.9
                                         âœ“                                                                âœ“                 88.4         72.8            65.0
                                                              âœ“                                           âœ“                 88.9         74.2            66.3
                                                              âœ“                    âœ“                      âœ“                 91.3         78.0            72.3
             Fig. 18. Performance on S3DIS dataset with 6-fold evaluation, where the left subplot represents the overall evaluation of the 6-fold experiments, and the right subplot shows the
             class-level evaluation results (%).
             4.5. Ablation experiment                                                                  AGTcontributes to a better model. We compared the three graph
                                                                                                   Transformer architectures mentioned in the previous Fig. 8. It is ev-
                Tovalidate the necessity and effectiveness of our designed modules,                ident that AGT (70.4%) outperforms naive GT (62.6%) and general
             we conducted comprehensive ablation experiments in the Area-5 of                      GT (66.6%) in terms of mIoU. In the graph, vertex features are em-
             S3DIS dataset to demonstrate their roles and performance within the                   ployedtocapturelocalinformationbetweenpoints,whileedgefeatures
             network.                                                                              contribute to capturing relationships and global information among
                Model Analysis: The effectiveness of each module is discussed and                  vertices. The introduction of edge features strengthens the graph learn-
             the comparison of the experimental results is presented in Table 7.                   ing capabilities, while ASGFormer, in a learnable manner, utilizes the
             MLPrepresents the baseline model without incorporating any attention                  weights of edges to enhance the adaptability of the graph. Trans-
             modules, and the scalar attention follows the standard dot-product                    former allows the network to assign different weights between points,
             attention form. Scalar attention is able to improve performance but                   and embedding edge features helps strengthen the learning of struc-
             slightly falls short compared to graph attention. With the infusion of                tural information. The incorporation of edges into the attention design
             the designed adaptive weights in our model, graph attention explic-                   forms an adaptive mechanism for structural learning. Dynamically
             itly demonstrates a significant improvement both in OA (90.1% vs.                     adjusting adaptive weights aids in distinguishing different semantic
             88.6%), mAcc (76.1% vs. 73.4%), and mIoU (70.4% vs. 65.5%). AGT                       classes.
             not only focuses on the correlation between points but also considers                     Position Embedding is crucial. As mentioned earlier, position
             the similarity of structural properties. On this basis, it enhances the               embedding is also a pivotal factor influencing graph Transformer.
             correspondence between points with similar attributes in an adaptive                  Absolute PE, relative PE and proposed implicit representation method
             manner. Furthermore, aligning with probabilistic graphical models,                    are compared in Table 8, noting that we only discuss the distinct
             AGT increases the probability of assigning the same label to similar                  components across the three strategies, specifically applied to the po-
             points. The second enhancement is reflected in the introduction of                    sition embedding of í µí°¾ and í µí±‰. By leveraging the Laplacian operator
             virtual nodes. With the incorporation of virtual nodes, the networkâ€™s                 to provide relative position information, the network is able to learn
             performance experiences a certain degree of improvement. Although                     the topological structure of the graph. However, non-learnable position
             the changes observed in scalar attention and graph attention are not                  embedding struggles to adapt to changes in the graph and differenti-
             substantial, it still indicates the effectiveness of the design of this               ate objects that share similar structures. ASGFormer exhibits greater
             module. Our ablation study demonstrates the crucial role of adap-                     scale adaptability and unbiasedness compared to absolute position
             tive weights in the network, and the auxiliary role of virtual nodes                  embedding. It incorporates relative position relations into the weight
             contributes to the performance improvement.                                           features to learn graph structure, leading to mIoU of 70.4% (+3.1%
                                                                                                15 
