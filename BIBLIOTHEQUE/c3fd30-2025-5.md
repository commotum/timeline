# Learning Modular Exponentiation with Transformers (2024)
Source: c3fd30-2025.pdf

## Core reasons
- The paper trains and analyzes Transformers on modular exponentiation, emphasizing sampling strategies, embeddings, and mechanistic interpretability rather than positional encoding or dimensional adaptations.
- The contributions are empirical findings about learning dynamics and internal circuits in standard Transformers, not a new computation mechanism or benchmark resource.

## Evidence extracts
- "We train compact 4 - layer encoder226decoder Transformers to predict d and analyze how they come to solve the task. We compare principled sampling schemes for 050 a;b;c;d 051 , probe the learned token embeddings, and use causal interventions 050acti- vation patching051 to localize the computation inside the network." (p. 1)
- "We demonstrate that transformers can learn modular exponentiation with high accuracy using recipro- cal sampling strategies. Key 002ndings include stepwise grokking of related moduli, embedding space reorganization, and specialized circuits for arithmetic operations." (p. 5)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
