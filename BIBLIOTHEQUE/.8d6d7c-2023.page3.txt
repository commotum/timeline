                                                                                                                                                                                                                    Tracks from 
                                                            Images at t
                                                                                                                                                                                                                    iteration i-1
                                                                                  Image encoder
                                                                                                                                                                       Tracklet Masks
                                                                                                                                                                                                      Tracklet 
                                                                                ResNet       FPN
                                                                                                                                                                                                     Association 
                                                                                                                                                                                                       Module
                                                                                                                                                                 t-1
                                                                                                                                              Fusion 
                                                                                                                                                                      t
                                                                                                                                               Block
                                                                                                                           Lidar to 
                                                                                         Point-level 
                                                                                                                            image 
                                                                                           Fusion
                                                   Iteration i   LiDAR                                                    projection          Fusion                 Semantic Masks                   Track Masks
                                                                                                                                               Block
                                                                                                                                                                                              t-1
                                                                                                                                                               t-1
                                                                                   Encoder     Decoder
                                                                                                                                              Header
                                                      t-1
                                                                                                                                                                    t                              t
                                                          t
                                                                                    Point-voxel encoder                                   Panoptic decoder
                                                                                                                                                                             4D Panoptic Predictions for iteration i
                                                                                                                                                                                                                   to iteration i+1
                                                 Figure 1: 4D-Former inference at iteration i. Note that tracking history from i − 1 is used in the
                                                 Tracklet Association Module.
                                                 3       Multimodal4DPanopticSegmentation
                                                 In this paper we propose 4D-Former to tackle 4D panoptic segmentation. The task consists of la-
                                                 belling each 4D LiDARpointwithasemanticclassandatrackIDthatspecifiesaconsistentinstance
                                                 over time. Camera images provide rich additional context to help make more accurate predictions,
                                                 particularly in regions where LiDAR is sparse. To this end, we propose a novel transformer-based
                                                 architecture that effectively combines sparse geometric features from LiDAR with dense contextual
                                                 features from cameras. In particular, it models object instances and semantic classes using concise,
                                                 learnable queries, followed by iterative refinement by self-attention and cross-attention to LiDAR
                                                 and camera image features. Using these queries, our method is able to attend only to regions of
                                                 the sensor data that are relevant, making the multimodal fusion of multiple cameras and LiDAR
                                                 tractable. In order to handle sequences of arbitrary length as well as continuous streams of data
                                                 (e.g., in the onboard setting), 4D-Former operates in a sliding window fashion, as illustrated in
                                                 Fig. 1. At each iteration, 4D-Former takes as input the current LiDAR scan at time t, the past scan
                                                 at t − 1, and the camera images at time t. It then generates semantic and tracklet predictions for
                                                 these two LiDAR scans. To make the tracklet predictions consistent over the entire input sequence,
                                                 we propose a novel Tracklet Association Module (TAM) which maintains a history of previously
                                                 observed object tracks, and associates them based on a learning-based matching objective.
                                                 3.1       MultimodalEncoder
                                                 Our input encoder extracts image features from the camera images, and point-level and voxel-level
                                                 features by fusing information from the LiDAR point clouds and camera features. These features
                                                 are then utilized in our transformer-based panoptic decoder presented in Sec. 3.2.
                                                 Image feature extraction: Assume the driving scene is captured by a set of images of size H ×
                                                 W captured from multiple cameras mounted on the ego-vehicle. We employ a ResNet-50 [44]
                                                 backbone, followed by a Feature Pyramid Network (FPN) [45], to produce a set of multi-scale,
                                                 D−dimensionalfeature maps {I | s = 4,8} for each of the images, where I ∈ RH/s×W/s×D.
                                                                                                            s                                                                               s
                                                 Point/voxel feature extraction:                                 The
                                                 network architecture is inspired by [46]
                                                                                                                                                                   Point-level        Point-level 
                                                 and consists of a point-branch and a                                                                               Fusion             Fusion
                                                                                                                             LiDAR
                                                                                                                                                                      +                  +
                                                                                                                                          MLP             MLP
                                                                                                                                                                                                             +
                                                 voxel-branch. The point-branch learns                                                                                                                               MLP
                                                 point-level embeddings, thus preserving                                                          p2v                        p2v                p2v
                                                                                                                                                                     v2p                 v2p
                                                                                                                                                                                                             v2p
                                                 fine details, whereas the voxel-branch
                                                 performs contextual reasoning using 3D
                                                 sparse convolutional blocks [47] and                                        Figure 2: Overview of point and voxel feature extraction.
                                                 provides multi-scale feature maps. Each                                     p2v: point-to-voxel. v2p: voxel-to-point.
                                                 of the N points in the input LiDAR
                                                 point-cloud is represented as an 8-D feature which include the xyz coordinates, relative timestamp,
                                                 intensity, and 3D relative offsets to the nearest voxel center. An MLP is applied to obtain initial point
                                                                                                                                        3
