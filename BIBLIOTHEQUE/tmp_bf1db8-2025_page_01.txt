                                                                     BIG-BenchExtraHard
                                                               1                          2                       1, 3                          1
                                      MehranKazemi ,BahareFatemi ,HritikBansal ,JohnPalowitch ,
                                                                                1                                      1                      2
                                       Chrysovalantis Anastasiou , Sanket Vaibhav Mehta , Lalit K. Jain ,
                                                                  1                      1                   1                              2
                                         Virginia Aglietti , Disha Jindal , Peter Chen , Nishanth Dikkala ,
                                                        1              2                  1                         1                           1
                                     GladysTyen , Xin Liu , Uri Shalit , Silvia Chiappa , Kate Olszewska ,
                                                               1                       1                   1                     1
                                                    Yi Tay , Vinh Q. Tran , Quoc V. Le , Orhan Firat
                                                        1Google DeepMind, 2Google Research, 3UCLA
                                                 Correspondence: mehrankazemi@google.comandbaharef@google.com
                                                 Abstract                                                                      Performance on BBEH
                                                                                                            Random
                           Current benchmarks for large language model                          Llama-3.1-8B-Instruct
                           (LLM)reasoningpredominantlyfocusonmath-                                   Gemma2-27B-IT               general-purpose models
                           ematical and coding abilities, leaving a gap                                Gemma3 27B
                                                                                                Gemini-2.0-Flash-Lite
                           in evaluating broader reasoning proficiencies.                           Gemini-2.0-Flash
                           One particular exception is the BIG-Bench                                         GPT-4o
                                                                                                 Distill-R1-Qwen-32B                reasoning models
                           dataset, which has served as a crucial bench-                               Deepseek R1
                           mark for evaluating the general reasoning ca-                               o3-mini(high)
                           pabilities of LLMs, thanks to its diverse set                                            0   5   10 15 20 25 30 35 40 45 50
                           of challenging tasks that allowed for a com-                                                     Harmonic Mean Accuracy (%)
                           prehensive assessment of general reasoning                         Figure 1: Model performances on BBEH (harmonic
                           across various skills within a unified frame-                      meanoverindividual task performances).
                           work. However, recent advances in LLMs
                           have led to saturation on BIG-Bench, and its
                           harder version BIG-Bench Hard (BBH). State-                        benchmarks in these domains and the relative ease
                           of-the-art models achieve near-perfect scores                      of evaluating quantitative solutions. However, rea-
                           on many tasks in BBH, thus diminishing its                         soning encompasses a far broader spectrum of cog-
                           utility.   To address this limitation, we intro-                   nitive skills, including logical deduction, temporal
                           duce BIG-Bench Extra Hard (BBEH), a new
                           benchmark designed to push the boundaries                          and spatial understanding, commonsense reason-
                           of LLMreasoning evaluation. BBEH replaces                          ing, and even the ability to comprehend humor.
                           each task in BBH with a novel task that probes                        Toassessthesediversereasoningfacets,thecom-
                           a similar reasoning capability but exhibits sig-                   munity has relied on the BIG-Bench benchmark
                           nificantly increased difficulty. We evaluate vari-                 (Srivastava et al., 2022), specifically its more chal-
                           ous general-purpose and reasoning-specialized                      lenging subset, BIG-Bench Hard (BBH) (Suzgun
                           models on BBEH and observe an accuracy                             et al., 2022). BBH has served as the de facto stan-
                           of 23.9% for the best general-purpose model
                           and 54.2% for the best reasoning-specialized                       dard for evaluating general reasoning in LLMs due
                           model, indicating substantial room for im-                         to its versatility and the wide array of reasoning
                           provement and highlighting the ongoing chal-                       skills it probes. However, the rapid progress in
                           lenge of achieving robust general reasoning in                     LLMdevelopmenthasledtoasaturationofBBH,
                           LLMs. Werelease BBEHpublicly at: https:                            with frontier models achieving over 90% accuracy
                           //github.com/google-deepmind/bbeh.                                 (e.g., Claude 3.5 reported an accuracy of 93.1% on
                      1 Introduction                                                          BBHinJune2024usinga3-shotprompt1). This
                      Reasoning, the ability to draw inferences and con-                      performance ceiling renders BBH less effective in
                      clusions from given information, is a cornerstone                       discriminating between the reasoning abilities of
                      of human intelligence and a critical frontier in the                    the latest generation of LLMs.
                      development of large language models (LLMs).                               To address this challenge, we introduce BIG-
                      While recent research has made significant strides                      BenchExtra Hard (BBEH), a benchmark designed
                      in evaluating the reasoning capabilities of LLMs,                       to assess advanced reasoning capabilities. BBEH
                      the focus has been disproportionately skewed to-                        builds upon BBH by replacing each of its 23 tasks
                      wards math/science and coding. This emphasis                            with a novel counterpart that probes similar reason-
                      is likely driven by the availability of challenging                         1www.anthropic.com/news/claude-3-5-sonnet
                                                                                        26473
                Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26473–26501
                                                  July 27 - August 1, 2025 ©2025 Association for Computational Linguistics
