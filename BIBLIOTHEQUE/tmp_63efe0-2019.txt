                           Published as a conference paper at ICLR 2020
                            DREAMTOCONTROL: LEARNING BEHAVIORS
                            BY LATENT IMAGINATION
                             Danijar Hafner‚àó          TimothyLillicrap       JimmyBa                  MohammadNorouzi
                             University of Toronto    DeepMind               University of Toronto    Google Brain
                             Google Brain
                                                                        Abstract
                                    Learned world models summarize an agent‚Äôs experience to facilitate learning
                                    complex behaviors. While learning world models from high-dimensional sensory
                                    inputs is becoming feasible through deep learning, there are many potential ways
                                    for deriving behaviors from them. We present Dreamer, a reinforcement learning
                                    agent that solves long-horizon tasks from images purely by latent imagination.
                                    WeefÔ¨Åciently learn behaviors by propagating analytic gradients of learned state
                                    values back through trajectories imagined in the compact state space of a learned
                                    world model. On 20 challenging visual control tasks, Dreamer exceeds existing
                                    approaches in data-efÔ¨Åciency, computation time, and Ô¨Ånal performance.
                            1   INTRODUCTION
                           Intelligent agents can achieve goals in complex environments even though
                           they never encounter the exact same situation twice. This ability requires      Dataset of Experience
                           building representations of the world from past experience that enable
                           generalization to novel situations. World models offer an explicit way to
                           represent an agent‚Äôs knowledge about the world in a parametric model that
                           can make predictions about the future.
                           When the sensory inputs are high-dimensional images, latent dynamics
                           models can abstract observations to predict forward in compact state spaces
                           (Watter et al., 2015; Oh et al., 2017; Gregor et al., 2019). Compared to       Learned Latent Dynamics
                           predictions in image space, latent states have a small memory footprint that
                           enables imagining thousands of trajectories in parallel. Learning effective
                           latent dynamics models is becoming feasible through advances in deep
                           learning and latent variable models (Krishnan et al., 2015; Karl et al., 2016;
                           Doerr et al., 2018; Buesing et al., 2018).
                           Behaviors can be derived from dynamics models in many ways. Often,
                           imagined rewards are maximized with a parametric policy (Sutton, 1991;
                           Ha and Schmidhuber, 2018; Zhang et al., 2019) or by online planning
                           (Chua et al., 2018; Hafner et al., 2018). However, considering only rewards
                           within a Ô¨Åxed imagination horizon results in shortsighted behaviors (Wang      Value and Action Learned 
                           et al., 2019). Moreover, prior work commonly resorts to derivative-free         by Latent Imagination
                           optimization for robustness to model errors (Ebert et al., 2017; Chua et al.,
                           2018; Parmas et al., 2019), rather than leveraging analytic gradients offered
                           byneural network dynamics (Henaff et al., 2019; Srinivas et al., 2018).
                           We present Dreamer, an agent that learns long-horizon behaviors from
                           imagespurely by latent imagination. A novel actor critic algorithm accounts
                           for rewards beyond the imagination horizon while making efÔ¨Åcient use of      Figure 1:     Dreamer
                           the neural network dynamics. For this, we predict state values and actions   learns a world model
                           in the learned latent space as summarized in Figure 1. The values optimize   from past experience
                           Bellman consistency for imagined rewards and the policy maximizes the        and efÔ¨Åciently learns
                           values by propagating their analytic gradients back through the dynamics.    farsighted behaviors in
                           In comparison to actor critic algorithms that learn online or by experience  its  latent space by
                           replay (Lillicrap et al., 2015; Mnih et al., 2016; Schulman et al., 2017;    backpropagating value
                           Haarnoja et al., 2018; Lee et al., 2019), world models can interpolate past  estimatesbackthrough
                           experience and offer analytic gradients of multi-step returns for efÔ¨Åcient   imagined trajectories.
                           policy optimization.
                              ‚àóCorrespondence to: Danijar Hafner <mail@danijar.com>.
                                                                            1
                             Published as a conference paper at ICLR 2020
                                  (a) Cup             (b) Acrobot          (c) Hopper            (d) Walker         (e) Quadruped
                             Figure 2: Image observations for 5 of the 20 visual control tasks used in our experiments. The tasks
                             pose a variety of challenges including contact dynamics, sparse rewards, many degrees of freedom,
                             and 3D environments. Several of these tasks could previously not be solved through world models.
                             Thekeycontributions of this paper are summarized as follows:
                              ‚Ä¢ Learning long-horizon behaviors by latent imagination            Model-based agents can be short-
                                 sighted if they use a Ô¨Ånite imagination horizon. We approach this limitation by predicting both
                                 actions and state values. Training purely by imagination in a latent space lets us efÔ¨Åciently learn
                                 the policy by propagating analytic value gradients back through the latent dynamics.
                              ‚Ä¢ Empirical performance for visual control           We pair Dreamer with existing representation
                                 learning methods and evaluate it on the DeepMind Control Suite with image inputs, illustrated in
                                 Figure 2. Using the same hyper parameters for all tasks, Dreamer exceeds previous model-based
                                 and model-free agents in terms of data-efÔ¨Åciency, computation time, and Ô¨Ånal performance.
                             2    CONTROL WITH WORLD MODELS
                             Reinforcement learning       Weformulate visual control as a partially observable Markov decision
                             process (POMDP) with discrete time step t ‚àà [1;T], continuous vector-valued actions a ‚àº p(a |
                                                                                                                          t       t
                             o   , a  ) generated by the agent, and high-dimensional observations and scalar rewards o ,r ‚àº
                              ‚â§t   <t                                                                                        t  t
                             p(o ,r | o    , a  ) generated by the unknown environment. The goal is to develop an agent that
                                t   t   <t   <t                            PT       
                             maximizes the expected sum of rewards E              r . Figure 2 shows a selection of our tasks.
                                                                         p    t=1 t
                             Agent components       The classical components of agents that learn in imagination are dynamics
                             learning, behavior learning, and environment interaction (Sutton, 1991). In the case of Dreamer,
                             the behavior is learned by predicting hypothetical trajectories in the compact latent space of the
                             world model. As outlined in Figure 3 and detailed in Algorithm 1, Dreamer performs the following
                             operations throughout the agent‚Äôs life time, either interleaved or in parallel:
                              ‚Ä¢ Learning the latent dynamics model from the dataset of past experience to predict future re-
                                 wards from actions and past observations. Any learning objective for the world model can be
                                 incorporated with Dreamer. We review existing methods for learning latent dynamics in Section 4.
                              ‚Ä¢ Learning action and value models from predicted latent trajectories, as described in Section 3.
                                 ThevaluemodeloptimizesBellmanconsistency for imagined rewards and the action model is
                                 updated by propagating gradients of value estimates back through the neural network dynamics.
                              ‚Ä¢ Executing the learned action model in the world to collect new experience for growing the dataset.
                             Latent dynamics      Dreamerusesalatent dynamics model that consists of three components. The
                             representation model encodes observations and actions to create continuous vector-valued model
                             states st with Markovian transitions (Watter et al., 2015; Zhang et al., 2019; Hafner et al., 2018). The
                             transition model predicts future model states without seeing the corresponding observations that will
                             later cause them. The reward model predicts the rewards given the model states,
                                                     Representation model:            p(s | s     , a   , o )
                                                     Transition model:                q(st | st‚àí1,at‚àí1) t                        (1)
                                                                                          t   t‚àí1   t‚àí1
                                                     Rewardmodel:                     q(r | s ).
                                                                                          t   t
                             Weusepfordistributionsthatgeneratesamplesintherealenvironmentandq fortheirapproximations
                             that enable latent imagination. SpeciÔ¨Åcally, the transition model lets us predict ahead in the compact
                             latent space without having to observe or imagine the corresponding images. This results in a low
                             memoryfootprint and fast predictions of thousands of imagined trajectories in parallel.
                             The model mimics a non-linear Kalman Ô¨Ålter (Kalman, 1960), latent state space model, or HMM
                             with real-valued states. However, it is conditioned on actions and predicts rewards, allowing the agent
                             to imagine the outcomes of potential action sequences without executing them in the environment.
                                                                                2
                                 Published as a conference paper at ICLR 2020
                                                                                ÃÇ   ÃÇ   ÃÇ      ÃÇ   ÃÇ   ÃÇ     ÃÇ   ÃÇ   ÃÇ
                                                                               v   r   a      v   r   a     v   r   a
                                          ÃÇ             ÃÇ             ÃÇ
                                         r      a      r      a      r
                                                                                                                                                    ÃÇ
                                                                                1   1   1      2   2   2     3   3   3
                                                                                                                                 a        a        a
                                          1      1      2      2      3
                                                                                                                                  1        2        3
                                             ÃÇ             ÃÇ              ÃÇ
                                                                                                                            o        o         o
                                     o      o      o      o       o      o
                                                                                   o
                                                                                                                             1        2         3
                                      1      1       2      2      3      3
                                                                                     1
                                    (a) Learn dynamics from experience          (b) Learn behavior in imagination         (c) Act in the environment
                                 Figure 3: Components of Dreamer. (a) From the dataset of past experience, the agent learns to encode
                                 observations and actions into compact latent states (           ), for example via reconstruction, and predicts
                                 environment rewards ( ). (b) In the compact latent space, Dreamer predicts state values ( ) and
                                 actions (    ) that maximize future value predictions by propagating gradients back through imagined
                                 trajectories. (c) The agent encodes the history of the episode to compute the current model state and
                                 predict the next action to execute in the environment. See Algorithm 1 for pseudo code of the agent.
                                 3     LEARNING BEHAVIORS BY LATENT IMAGINATION
                                 Dreamer learns long-horizon behaviors in the compact latent space of a learned world model by
                                 efÔ¨Åciently leveraging the neural network latent dynamics. For this, we propagate stochastic gradients
                                 of multi-step returns through neural network predictions of actions, states, rewards, and values using
                                 reparameterization. This section describes the main contribution of our paper.
                                 Imagination environment            Thelatent dynamics deÔ¨Åne a Markov decision process (MDP; Sutton,
                                 1991) that is fully observed because the compact model states st are Markovian. We denote imagined
                                 quantities with œÑ as the time index. Imagined trajectories start at the true model states s of observation
                                                                                                                                      t
                                 sequences drawn from the agent‚Äôs past experience. They follow predictions of the transition model
                                 s   ‚àº q(s | s          , a    ), reward model r         ‚àº q(r | s ), and a policy a            ‚àº q(a | s ). The
                                  œÑ          œÑ     œÑ‚àí1     œÑ‚àí1                        œÑ         œÑ     œÑ                     œÑ          œÑ     œÑ
                                                                                                      P
                                 objective is to maximize expected imagined rewards Eq                   ‚àû Œ≥œÑ‚àítrœÑ withrespecttothepolicy.
                                                                                                         œÑ=t
                                 Algorithm 1: Dreamer
                                 Initialize dataset D with S random seed episodes.                         Modelcomponents
                                 Initialize neural network parameters Œ∏,œÜ,œà randomly.                      Representation       p (s | s     , a   , o )
                                 while not converged do                                                                          Œ∏   t    t-1   t-1   t
                                                                                                           Transition           q (s | s     , a   )
                                      for update step c = 1..C do                                                                Œ∏   t    t-1   t-1
                                                                                                           Reward               q (r | s )
                                                                                                                                 Œ∏   t    t
                                          // Dynamics learning                                             Action               q (a | s )
                                                                                       k+L                                       œÜ   t    t
                                          DrawBdatasequences{(a ,o ,r )}                     ‚àºD.
                                                                            t  t   t   t=k                 Value                v (s )
                                          Computemodelstates s ‚àº p (s | s                  , a    , o ).                         œà t
                                                                       t      Œ∏  t    t‚àí1    t‚àí1    t      Hyperparameters
                                          Update Œ∏ using representation learning.                          Seed episodes                             S
                                          // Behavior learning                                             Collect interval                          C
                                                                               t+H
                                          Imagine trajectories {(s ,a )}             from each s .
                                                                       œÑ   œÑ   œÑ=t                 t
                                          Predict rewards E q (r | s ) and values v (s ).                 Batch size                                B
                                                                  Œ∏   œÑ     œÑ                  œà œÑ         Sequence length                           L
                                          Computevalueestimates V (s ) via Equation 6.
                                                                      P Œª œÑ                                Imagination horizon                       H
                                          Update œÜ ‚Üê œÜ+Œ±‚àá                t+H V (s ).
                                                                    œÜ    œÑ=t     Œª œÑ
                                                                      P                               Learningrate                                 Œ±
                                                                         t+H 1                          2
                                                                                                     
                                          Update œà‚Üêœà‚àíŒ±‚àá                           v (s )9V (s )          .
                                                                    œà    œÑ=t 2      œà œÑ        Œª œÑ
                                     // Environment interaction
                                      o ‚Üêenv.reset()
                                       1
                                      for time step t = 1..T do
                                          Computes ‚àºp (s | s               , a    , o ) from history.
                                                       t      Œ∏   t    t‚àí1    t‚àí1    t
                                          Computea ‚àºq (a |s )withtheactionmodel.
                                                        t     œÜ   t    t
                                          Addexploration noise to action.
                                          r ,o      ‚Üêenv.step(a ).
                                            t  t+1                        t
                                     Addexperience to dataset D ‚Üê D ‚à™{(o ,a ,r )T }.
                                                                                      t   t   t t=1
                                                                                            3
                                         Published as a conference paper at ICLR 2020
                                            1000      Cartpole Swingup           1000         Cheetah Run            1000       Quadruped Walk            1000        Walker Walk
                                             800                                  800                                 800                                  800
                                             600                                  600                                 600                                  600
                                             400                                  400                                 400                                  400              Dreamer  (   )
                                           Episode Return200                      200                                 200                                  200              No value ( R)
                                                                                                                                                                            PlaNet     ( R)
                                                0     10      20      30     40     0      10      20     30      40     0     10      20      30     40     0      10      20     30      40
                                                       Imagination Horizon                 Imagination Horizon                  Imagination Horizon                 Imagination Horizon
                                         Figure 4: Imagination horizons. We compare the Ô¨Ånal performance of Dreamer, learning an action
                                         modelwithout value prediction, and online planning using PlaNet. Learning a state value model to
                                         estimate rewards beyond the imagination horizon makes Dreamer more robust to the horizon length.
                                         Theagents use pixel reconstruction for representation learning and an action repeat of R = 2.
                                         Action and value models                    Consider imagined trajectories with a Ô¨Ånite horizon H. Dreamer uses
                                         an actor critic approach to learn behaviors that consider rewards beyond the horizon. We learn an
                                         action model and a value model in the latent space of the world model for this. The action model
                                         implements the policy and aims to predict actions that solve the imagination environment. The value
                                         model estimates the expected imagined rewards that the action model achieves from each state sœÑ,
                                                                Action model:                                   a ‚àº q (a |s )
                                                                                                                  œÑ         œÜ    œÑ      œÑ                                               (2)
                                                                Value model:                                    v (s ) ‚âà E                    Pt+HŒ≥œÑ‚àítr .
                                                                                                                  œà œÑ             q(¬∑|s )                         œÑ
                                                                                                                                        œÑ         œÑ=t
                                         Theaction and value models are trained cooperatively as typical in policy iteration: the action model
                                         aims to maximize an estimate of the value, while the value model aims to match an estimate of the
                                         value that changes as the action model changes.
                                         Weusedenseneuralnetworks for the action and value models with parameters œÜ and œà, respectively.
                                         Theactionmodeloutputsatanh-transformedGaussian(Haarnojaetal.,2018)withsufÔ¨Åcientstatistics
                                         predicted by the neural network. This allows for reparameterized sampling (Kingma and Welling,
                                         2013; Rezende et al., 2014) that views sampled actions as deterministically dependent on the neural
                                         network output, allowing us to backpropagate analytic gradients through the sampling operation,
                                                                                                                         
                                                                           a =tanh ¬µ (s )+œÉ (s ) ,                              ‚àº Normal(0,I).                                         (3)
                                                                             œÑ                 œÜ œÑ            œÜ œÑ
                                         Value estimation               To learn the action and value models, we need to estimate the state values
                                                                                                 t+H
                                         of imagined trajectories {s ,a ,r }                           .  These trajectories branch off of the model states s of
                                                                                  œÑ     œÑ    œÑ œÑ=t                                                                                     t
                                         sequencebatchesdrawnfromtheagent‚Äôsdatasetofexperienceandpredictforwardfortheimagination
                                         horizon H using actions sampled from the action model. State values can be estimated in multiple
                                         waysthat trade off bias and variance (Sutton and Barto, 2018),
                                                                               t+H 
                                                                  .                X
                                                     V (s ) = E                          r     ,                                                                                         (4)
                                                       R œÑ              q ,q               n
                                                                         Œ∏ œÜ
                                                                                  n=œÑ
                                                                               h‚àí1                                         
                                                       k          .               X n‚àíœÑ                    h‚àíœÑ
                                                     V (s ) = E                          Œ≥       r +Œ≥            v (s )           with       h=min(œÑ +k,t+H),                            (5)
                                                       N œÑ              q ,q                      n                œà h
                                                                         Œ∏ œÜ
                                                                                  n=œÑ
                                                                                 H‚àí1
                                                                  .               X n‚àí1 n                          H‚àí1 H
                                                     V (s ) = (1‚àíŒª)                     Œª       V (s )+Œª                 V (s ),                                                         (6)
                                                       Œª œÑ                                         N œÑ                      N œÑ
                                                                                  n=1
                                         where the expectations are estimated under the imagined trajectories. VR simply sums the rewards
                                         fromœÑ until the horizon and ignores rewards beyond it. This allows learning the action model without
                                         a value model, an ablation we compare to in our experiments. Vk estimates rewards beyond k steps
                                                                                                                                        N
                                         with the learned value model. Dreamer uses V , an exponentially-weighted average of the estimates
                                                                                                              Œª
                                         for different k to balance bias and variance. Figure 4 shows that learning a value model in imagination
                                         enables Dreamer to solve long-horizon tasks while being robust to the imagination horizon. The
                                         experimental details and results on all tasks are described in Section 6.
                                                                                                                  4
                             Published as a conference paper at ICLR 2020
                                            Context                  6    10     15     20     25    30     35     40    45     50
                               True
                               Model
                               True
                               Model
                             Figure 5: Reconstructions of long-term predictions. We apply the representation model to the Ô¨Årst 5
                             images of two hold-out trajectories and predict forward for 45 steps using the latent dynamics, given
                             only the actions. The recurrent state space model (RSSM; Hafner et al., 2018) performs accurate
                             long-term predictions, enabling Dreamer to learn successful behaviors in a compact latent space.
                             Learning objective     Toupdate the action and value models, we Ô¨Årst compute the value estimates
                             V (s )forallstatess alongtheimaginedtrajectories. Theobjectivefortheactionmodelq (a | s )
                               Œª œÑ                 œÑ                                                                       œÜ  œÑ    œÑ
                             is to predict actions that result in state trajectories with high value estimates. The objective for the
                             value model v (s ), in turn, is to regress the value estimates,
                                            œà œÑ
                                             t+H                                        t+H                          
                                                X                                            X1                          2
                                 maxE               V (s ) ,        (7)                                                
                                                                              minE                  v (s )‚àíV (s ))           .    (8)
                                        q ,q          Œª œÑ                            q ,q           œà œÑ         Œª œÑ 
                                   œÜ     Œ∏ œÜ                                    œà     Œ∏ œÜ        2
                                                œÑ=t                                          œÑ=t
                             The value model is updated to regress the targets, around which we stop the gradient as typical
                             (Sutton and Barto, 2018). The action model uses analytic gradients through the learned dynamics
                             to maximize the value estimates. To understand this, we note that the value estimates depend on
                             the reward and value predictions, which depend on the imagined states, which in turn depend on
                             the imagined actions. Since all steps are implemented as neural networks, we analytically compute
                                        P               
                             ‚àá E            t+H V (s ) by stochastic backpropagation (Kingma and Welling, 2013; Rezende
                               œÜ qŒ∏,qœÜ      œÑ=t   Œª œÑ
                             et al., 2014). We use reparameterization for continuous actions and latent states and straight-through
                             gradients (Bengio et al., 2013) for discrete actions. The world model is Ô¨Åxed while learning behaviors.
                             In tasks with early termination, the world model also predicts the discount factor from each latent
                             state to weigh the time steps in Equations 7 and 8 by the cumulative product of the predicted discount
                             factors, so terms are weighted down based on how likely the imagined trajectory would have ended.
                             Comparisontoactorcritic methods           Agents using Reinforce gradients (Williams, 1992), such as
                             A3CandPPO(Mnihetal.,2016;Schulmanetal.,2017),employvaluebaselines to reduce gradient
                             variance, while Dreamer backpropagates through the value model. This is similar to deterministic
                             or reparameterized actor critics (Silver et al., 2014), such as DDPG and SAC (Lillicrap et al., 2015;
                             Haarnoja et al., 2018). However, these do not leverage gradients through transitions and only
                             maximize immediate Q-values. MVE and STEVE (Feinberg et al., 2018; Buckman et al., 2018)
                             extend them to multi-step Q-learning with learned dynamics to provide more accurate Q-value targets.
                             Wepredict state values, which is sufÔ¨Åcient for policy optimization since we backpropagate through
                             the dynamics. Refer to Section 5 for a more detailed comparison to related work.
                             4    LEARNING LATENT DYNAMICS
                             Learning behaviors in imagination requires a world model that generalizes well. We focus on latent
                             dynamics models that predict forward in a compact latent space, facilitating long-term predictions
                             and allowing the agent to imagine thousands of trajectories in parallel. Several objectives for learning
                             representations for control have been proposed (Watter et al., 2015; Jaderberg et al., 2016; Oord
                             et al., 2018; Eslami et al., 2018). We review three approaches for learning representations to use with
                             Dreamer: reward prediction, image reconstruction, and contrastive estimation.
                             Reward prediction       Latent imagination requires a representation model p(s | s           , a   , o ),
                                                                                                                 t     t‚àí1   t‚àí1   t
                             transition model q(s | s      , a    , ), and reward model q(r | s ), as described in Section 2. In
                                                   t    t‚àí1   t‚àí1                            t    t
                             principle, this could be achieved by simply learning to predict future rewards given actions and
                             past observations (Oh et al., 2017; Gelada et al., 2019; Schrittwieser et al., 2019). With a large and
                             diverse dataset, such representations should be sufÔ¨Åcient for solving a control task. However, with a
                             Ô¨Ånite dataset and especially when rewards are sparse, learning about observations that correlate with
                             rewards is likely to improve the world model (Jaderberg et al., 2016; Gregor et al., 2019).
                                                                                5
                               Published as a conference paper at ICLR 2020
                                         Dreamer (5e6 steps)       PlaNet (5e6 steps)       D4PG (1e9 steps)        A3C (1e9 steps, proprio)
                                   1000
                                    800
                                    600
                                    400
                                 Episode Return200                          n/an/a              n/an/a
                                      0              Cup                                Run       Run                  Run
                                           Cartpole BalanceWalker Stand CatchWalker WalkCartpoleReacher Easy WalkHopper StandCheetahFingerCartpole SwingupPendulum SwingupFingerWalkerReacher HardCartpoleFinger SpinHopper HopAcrobot Swingup
                                                                  Bal. SparseQuadruped       Turn  HardQuadruped  Turn  Easy    Swi. Sparse
                               Figure 6: Performance comparison to existing methods. Dreamer inherits the data-efÔ¨Åciency of
                               PlaNet while exceeding the asymptotic performance of the best model-free agents. After 5 √ó 106
                               environment steps, Dreamer reaches an average performance of 823 across tasks, compared to PlaNet
                                                                                                 9
                               at 332 and the top model-free D4PG agent at 786 after 10 steps. Results are averages over 5 seeds.
                               Reconstruction        WeÔ¨Årstdescribe the world model used by PlaNet (Hafner et al., 2018) that learns
                               latent dynamics by reconstructing images as shown in Figure 3a. The world model consists of the
                               following components, where the observation model is only used to provide a learning signal,
                                                         Representation model:                p (s | s      , a   , o )
                                                                                               Œ∏   t    t‚àí1   t‚àí1    t
                                                         Observation model:                   q (o | s )
                                                                                               Œ∏   t    t                                    (9)
                                                         Rewardmodel:                         qŒ∏(rt | st)
                                                         Transition model:                    q (s | s      , a   ).
                                                                                               Œ∏   t    t‚àí1   t‚àí1
                               Thecomponentsareoptimized jointly to increase the variational lower bound (ELBO; Jordan et al.,
                               1999) or more generally the variational information bottleneck (VIB; Tishby et al., 2000; Alemi et al.,
                               2016). As derived in Appendix B, the bound includes reconstruction terms for observations and
                               rewards and a KL regularizer. The expectation is taken under the dataset and representation model,
                                                                                
                                               .       X t            t      t                     t  .
                                      J       =E             J +J +J                +const        J = lnq(o |s )
                                        REC        p           O      R       D                     O            t    t                     (10)
                                                        t                                                                         
                                         t  .                      t  .                                     
                                      J =lnq(r |s )              J = ‚àíŒ≤KL p(s |s                , a    , o )   q(s | s     , a    ) .
                                        R           t    t         D                   t    t‚àí1    t‚àí1   t        t    t‚àí1    t‚àí1
                               Weimplementthetransition model as a recurrent state space model (RSSM; Hafner et al., 2018), the
                               representation model by combining the RSSM with a convolutional neural network (CNN; LeCun
                               et al., 1989) applied to the image observation, the observation model as a transposed CNN, and
                               the reward model as a dense network. The combined parameter vector Œ∏ is updated by stochastic
                               backpropagation (KingmaandWelling,2013;Rezendeetal.,2014). Figure5showsvideopredictions
                               of this model. We refer to Appendix A and Hafner et al. (2018) model details.
                               Contrastive estimation         Predicting pixels can require high model capacity. We can also encourage
                               mutual information between model states and observations by instead predicting the states from the
                               images (Guo et al., 2018). This replaces the observation model with a state model,
                                                              State model:                         q (s | o ).                              (11)
                                                                                                    Œ∏   t    t
                               While the reconstruction objective used the fact that the observation marginal is a constant, we
                               nowfacethe state marginal. As shown in Appendix B, this can be estimated via noise contrastive
                               estimation (NCE; Gutmann and Hyv√§rinen, 2010; Oord et al., 2018) by averaging the state model
                                                     0
                               over observations o of the current sequence batch. Intuitively, q(s | o ) makes the state predictable
                                                                    P                                     t    t
                                                                                  0
                               from the current image while ln         o0 q(st | o ) keeps it diverse to prevent collapse,
                                                                                                                              
                                             .       X t           t       t        t .                         X            0
                                     J      = E            J +J +J                  J = lnq(s | o )‚àíln                q(s | o ) .
                                       NCE                   S      R      D          S           t    t                  t                 (12)
                                                      t                                                            o0
                               Weimplementthestate model as a CNN and again optimize the bound with respect to the combined
                               parameter vector Œ∏ using stochastic backpropagation. While avoiding pixel prediction, the amount of
                               information this bound can extract efÔ¨Åciently is limited (McAllester and Statos, 2018). We empirically
                               compare reward, reconstruction, and contrastive objectives in our experiments in Figure 8.
                                                                                       6
                               Published as a conference paper at ICLR 2020
                                         Acrobot Swingup          Cartpole Swingup Sparse          Hopper Hop        1000      Hopper Stand
                                  400                         800                         400
                                  300                         600                                                     750
                                  200                         400                         200                         500
                                 Episode Return100            200                                                     250
                                    0                           0                           0                           0
                                     0.0  0.5  1.0   1.5  2.0    0.0  0.5  1.0   1.5  2.0    0.0  0.5   1.0  1.5  2.0    0.0  0.5   1.0  1.5  2.0
                                        Pendulum Swingup     1000    Quadruped Walk                Walker Run                  Walker Walk
                                                                                          800                        1000
                                  750                         750                                                     750
                                  500                                                     600
                                                              500                         400                         500
                                 Episode Return250            250                         200                         250
                                    0                           0                           0                           0
                                     0.0  0.5  1.0   1.5  2.0    0.0  0.5  1.0   1.5  2.0    0.0  0.5   1.0  1.5  2.0    0.0  0.5   1.0  1.5  2.0
                                          Environment Steps 1e6       Environment Steps 1e6       Environment Steps 1e6       Environment Steps 1e6
                                                       Dreamer    No value     PlaNet    D4PG (1e9 steps)  A3C (1e9 steps, proprio)
                               Figure 7: Dreamer succeeds at visual control tasks that require long-horizon credit assignment, such
                               as the acrobot and hopper tasks. Optimizing only imagined rewards within the horizon via an action
                               model or by online planning yields shortsighted behaviors that only succeed in reactive tasks, such as
                               in the walker domain. The performance on all 20 tasks is summarized in Figure 6 and training curves
                               are shown in Appendix D. See Tassa et al. (2018) for performance curves of D4PG and A3C.
                               5     RELATED WORK
                               Prior works learn latent dynamics for visual control by derivative-free policy learning or online
                               planning, augment model-free agents with multi-step predictions, or use analytic gradients of Q-
                               values or multi-step rewards, often for low-dimensional tasks. In comparison, Dreamer uses analytic
                               gradients to efÔ¨Åciently learn long-horizon behaviors for visual control purely by latent imagination.
                               Control with latent dynamics           E2C(Watter et al., 2015) and RCE (Banijamali et al., 2017) embed
                               images to predict forward in a compact space to solve simple tasks. World Models (Ha and Schmid-
                               huber, 2018) learn latent dynamics in a two-stage process to evolve linear controllers in imagination.
                               PlaNet (Hafner et al., 2018) learns them jointly and solves visual locomotion tasks by latent online
                               planning. SOLAR (Zhang et al., 2019) solves robotic tasks via guided policy search in latent space.
                               I2A(Weberetal., 2017) hands imagined trajectories to a model-free policy, while Lee et al. (2019)
                               and Gregor et al. (2019) learn belief representations to accelerate model-free agents.
                               Imagined multi-step returns           VPN(Ohetal., 2017), MVE (Feinberg et al., 2018), and STEVE
                               (Buckman et al., 2018) learn dynamics for multi-step Q-learning from a replay buffer. AlphaGo
                               (Silver et al., 2017) combines predictions of actions and state values with planning, assuming access
                               to the true dynamics. Also assuming access to the dynamics, POLO (Lowrey et al., 2018) plans
                               to explore by learning a value ensemble. MuZero (Schrittwieser et al., 2019) learns task-speciÔ¨Åc
                               reward and value models to solve challenging tasks but requires large amounts of experience. PETS
                               (Chua et al., 2018), VisualMPC (Ebert et al., 2017), and PlaNet (Hafner et al., 2018) plan online
                               using derivative-free optimization. POPLIN (Wang and Ba, 2019) improves over online planning by
                               self-imitation. Piergiovanni et al. (2018) learn robot policies by imagination with a latent dynamics
                               model. Planning with neural network gradients was shown on small problems (Schmidhuber, 1990;
                               Henaff et al., 2018) but has been challenging to scale (Parmas et al., 2019).
                               Analytic value gradients         DPG (Silver et al., 2014), DDPG (Lillicrap et al., 2015), and SAC
                               (Haarnoja et al., 2018) leverage gradients of learned immediate action values to learn a policy by
                               experience replay. SVG (Heess et al., 2015) reduces the variance of model-free on-policy algorithms
                               by analytic value gradients of one-step model predictions. Concurrent work by Byravan et al. (2019)
                               uses latent imagination with deterministic models for navigation and manipulation tasks. ME-TRPO
                               (Kurutach et al., 2018) accelerates an otherwise model-free agent via gradients of predicted rewards
                               for proprioceptive inputs. DistGBP (Henaff et al., 2017; 2019) uses model gradients for online
                               planning in simple tasks.
                                                                                       7
                                           Published as a conference paper at ICLR 2020
                                                        Acrobot Swingup                          Cheetah Run                            Cup Catch                             Finger Spin
                                               400                                                                        1000                                  1000
                                               300                                   750                                   750                                   750
                                               200                                   500                                   500                                   500
                                             Episode Return100                       250                                   250                                   250
                                                 0                                     0                                     0                                     0
                                                   0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0
                                              1000         Hopper Stand             1000     Pendulum Swingup             1000       Quadruped Run                           Walker Stand
                                                                                                                                                                1000
                                               750                                   750                                   750                                   800
                                               500                                   500                                   500                                   600
                                            Episode Return250                        250                                   250                                   400
                                                 0                                     0                                                                         200
                                                                                                                             0
                                                   0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0
                                                         Environment Steps 1e6                 Environment Steps 1e6                 Environment Steps 1e6                 Environment Steps 1e6
                                                  Dreamer + Reconstruction          Dreamer + Contrastive         Dreamer + Reward only          D4PG (1e9 steps)         A3C (1e9 steps, proprio)
                                           Figure 8: Comparison of representation learning objectives to be used with Dreamer. Pixel recon-
                                           struction performs best for the majority of tasks. The contrastive objective solves about half of the
                                           tasks, while predicting rewards alone was not sufÔ¨Åcient in our experiments. The results suggest that
                                           future developments in learning representations are likely to translate into improved task performance
                                           for Dreamer. The performance curves for all tasks are included in Appendix E.
                                           6      EXPERIMENTS
                                          Weexperimentally evaluate Dreamer on a variety of control tasks. We designed the experiments
                                           to compare Dreamer to current best methods in the literature, and to evaluate its ability to solve
                                           tasks with long horizons, continuous actions, discrete actions, and early termination. We further
                                           compare the orthogonal choice of learning objective for the world model. The source code for all our
                                           experiments and videos of Dreamer are available at https://danijar.com/dreamer.
                                           Control tasks             Weevaluate Dreamer on 20 visual control tasks of the DeepMind Control Suite
                                          (Tassa et al., 2018), illustrated in Figure 2. These tasks pose a variety of challenges, including sparse
                                           rewards, contact dynamics, and 3D scenes. We selected the tasks on which Tassa et al. (2018) report
                                           non-zero performance from image inputs. Agent observations are images of shape 64 √ó 64 √ó 3,
                                           actions range from 1 to 12 dimensions, rewards range from 0 to 1, episodes last for 1000 steps and
                                           haverandomizedinitialstates. WeuseaÔ¨ÅxedactionrepeatofR = 2acrosstasks. Wefurtherevaluate
                                           the applicability of Dreamer to discrete actions and early termination on a subset of Atari games
                                          (Bellemare et al., 2013) and DeepMind Lab levels (Beattie et al., 2016) as detailed in Appendix C.
                                           Implementation                Ourimplementation uses TensorFlow Probability (Dillon et al., 2017). We use a
                                           single Nvidia V100 GPU and 10 CPU cores for each training run. The training time for our Dreamer
                                                                                                          6
                                           implementation is below 5 hours per 10 environment steps on the control suite, compared to 11
                                           hours for online planning using PlaNet, and the 24 hours used by D4PG to reach similar performance.
                                          Weusethesamehyperparametersacross all continuous tasks, and similarly across all discrete tasks,
                                           detailed in Appendix A. The world models are learned via reconstruction unless speciÔ¨Åed.
                                           Baseline methods                Thehighest reported performance on the continuous tasks is achieved by D4PG
                                          (Barth-Maron et al., 2018), an improved variant of DDPG (Lillicrap et al., 2015) that uses distributed
                                           collection, distributional Q-learning, multi-step returns, and prioritized replay. We include the scores
                                           for D4PG with pixel inputs and A3C (Mnih et al., 2016) with state inputs from Tassa et al. (2018).
                                           PlaNet (Hafner et al., 2018) learns the same world model as Dreamer and selects actions via online
                                           planning without an action model and drastically improves over D4PG and A3C in data efÔ¨Åciency. We
                                           re-run PlaNet with R = 2 for a uniÔ¨Åed experimental setup. For Atari, we show the Ô¨Ånal performance
                                           of SimPLe (Kaiser et al., 2019), DQN (Mnih et al., 2015) and Rainbow (Hessel et al., 2018) reported
                                           by Castro et al. (2018), and for DeepMind Lab that of IMPALA (Espeholt et al., 2018) as a guideline.
                                                                                                                       8
                         Published as a conference paper at ICLR 2020
                         Performance Toevaluate the performance of Dreamer, we compare it to state-of-the-art reinforce-
                         mentlearning agents. The results are summarized in Figure 6. With an average score of 823 across
                         tasks after 5 √ó 106 environment steps, Dreamer exceeds the performance of the strong model-free
                                                                        9
                         D4PGagentthatachievesanaverageof786within10 environmentsteps. Atthesametime,Dreamer
                         inherits the data-efÔ¨Åciency of PlaNet, conÔ¨Årming that the learned world model can help to generalize
                         from small amounts of experience. The empirical success of Dreamer shows that learning behaviors
                         bylatent imagination with world models can outperform top methods based on experience replay.
                         Longhorizons Toinvestigate its ability to learn long-horizon behaviors, we compare Dreamer to
                         alternatives for deriving behaviors from the world model at various horizon lengths. For this, we
                         learn an action model to maximize imagined rewards without a value model and compare to online
                         planning using PlaNet. Figure 4 shows the Ô¨Ånal performance for different imagination horizons,
                         conÔ¨Årming that the value model makes Dreamer more robust to the horizon and performs well even
                         for short horizons. Performance curves for all 19 tasks with horizon of 20 are shown in Appendix D,
                         where Dreamer outperforms the alternatives on 16 of 20 tasks, with 4 ties.
                         Representation learning  Dreamercanbeusedwithanydifferentiable dynamics model that pre-
                         dicts future rewards given actions and past observations. Since the representation learning objective
                         is orthogonal to our algorithm, we compare three natural choices described in Section 4: pixel recon-
                         struction, contrastive estimation, and pure reward prediction. Figure 8 shows clear differences in task
                         performance for different representation learning approaches, with pixel reconstruction outperform-
                         ing contrastive estimation on most tasks. This suggests that future improvements in representation
                         learning are likely to translate to higher task performance with Dreamer. Reward prediction alone
                         wasnotsufÔ¨Åcient in our experiments. Further ablations are included in the appendix of the paper.
                         7   CONCLUSION
                         Wepresent Dreamer, an agent that learns long-horizon behaviors purely by latent imagination. For
                         this, we propose an actor critic method that optimizes a parametric policy by propagating analytic
                         gradients of multi-step values back through learned latent dynamics. Dreamer outperforms previous
                         methods in data-efÔ¨Åciency, computation time, and Ô¨Ånal performance on a variety of challenging
                         continuous control tasks with image inputs. We further show that Dreamer is applicable to tasks with
                         discrete actions and early episode termination. Future research on representation learning can likely
                         scale latent imagination to environments of higher visual complexity.
                         Acknowledgements WethankSimonKornblith,BenjaminEysenbach,IanFischer,AmyZhang,
                         Geoffrey Hinton, Shane Gu, Adam Kosiorek, Jacob Buckman, Calvin Luo, and Rishabh Agarwal,
                         and our anonymous reviewers for feedback and discussions. We thank Yuval Tassa for adding the
                         quadruped environment to the control suite.
                                                                     9
           Published as a conference paper at ICLR 2020
           REFERENCES
           A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. arXiv
            preprint arXiv:1612.00410, 2016.
           E. Banijamali, R. Shu, M. Ghavamzadeh, H. Bui, and A. Ghodsi. Robust locally-linear controllable
            embedding. arXiv preprint arXiv:1710.05373, 2017.
           G.Barth-Maron,M.W.Hoffman,D.Budden,W.Dabney,D.Horgan,A.Muldal,N.Heess,andT.Lil-
            licrap. Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617,
            2018.
           C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. K√ºttler, A. Lefrancq, S. Green,
            V. Vald√©s, A. Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.
           M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
            evaluation platform for general agents. Journal of ArtiÔ¨Åcial Intelligence Research, 47:253‚Äì279,
            2013.
           Y. Bengio, N. L√©onard, and A. Courville. Estimating or propagating gradients through stochastic
            neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
           J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee. Sample-efÔ¨Åcient reinforcement learning
            with stochastic ensemble value expansion. In Advances in Neural Information Processing Systems,
            pages 8224‚Äì8234, 2018.
           L. Buesing, T. Weber, S. Racaniere, S. Eslami, D. Rezende, D. P. Reichert, F. Viola, F. Besse,
            K. Gregor, D. Hassabis, et al. Learning and querying fast generative models for reinforcement
            learning. arXiv preprint arXiv:1802.03006, 2018.
           A. Byravan, J. T. Springenberg, A. Abdolmaleki, R. Hafner, M. Neunert, T. Lampe, N. Siegel,
            N. Heess, and M. Riedmiller. Imagined value gradients: Model-based policy optimization with
            transferable latent dynamics models. arXiv preprint arXiv:1910.04142, 2019.
           P. S. Castro, S. Moitra, C. Gelada, S. Kumar, and M. G. Bellemare. Dopamine: A research framework
            for deep reinforcement learning. arXiv preprint arXiv:1812.06110, 2018.
           K. Chua, R. Calandra, R. McAllister, and S. Levine. Deep reinforcement learning in a handful of
            trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems,
            pages 4754‚Äì4765, 2018.
           D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by
            exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
           J. V. Dillon, I. Langmore, D. Tran, E. Brevdo, S. Vasudevan, D. Moore, B. Patton, A. Alemi,
            M.Hoffman,andR.A.Saurous. TensorÔ¨Çowdistributions. arXiv preprint arXiv:1711.10604, 2017.
           A. Doerr, C. Daniel, M. Schiegg, D. Nguyen-Tuong, S. Schaal, M. Toussaint, and S. Trimpe.
            Probabilistic recurrent state-space models. arXiv preprint arXiv:1801.10395, 2018.
           F. Ebert, C. Finn, A. X. Lee, and S. Levine. Self-supervised visual planning with temporal skip
            connections. arXiv preprint arXiv:1710.05268, 2017.
           S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo, A. Ruderman, A. A. Rusu,
            I. Danihelka, K. Gregor, et al. Neural scene representation and rendering. Science, 360(6394):
            1204‚Äì1210, 2018.
           L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley,
            I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner
            architectures. arXiv preprint arXiv:1802.01561, 2018.
           V. Feinberg, A. Wan, I. Stoica, M. I. Jordan, J. E. Gonzalez, and S. Levine. Model-based value
            estimation for efÔ¨Åcient model-free reinforcement learning. arXiv preprint arXiv:1803.00101, 2018.
                               10
           Published as a conference paper at ICLR 2020
           C.Gelada,S.Kumar,J.Buckman,O.Nachum,andM.G.Bellemare. Deepmdp: Learningcontinuous
            latent space models for representation learning. arXiv preprint arXiv:1906.02736, 2019.
           K. Gregor, D. J. Rezende, F. Besse, Y. Wu, H. Merzic, and A. v. d. Oord. Shaping belief states with
            generative environment models for rl. arXiv preprint arXiv:1906.09237, 2019.
           Z. D. Guo, M. G. Azar, B. Piot, B. A. Pires, T. Pohlen, and R. Munos. Neural predictive belief
            representations. arXiv preprint arXiv:1811.06407, 2018.
           M. Gutmann and A. Hyv√§rinen. Noise-contrastive estimation: A new estimation principle for
            unnormalized statistical models. In Proceedings of the Thirteenth International Conference on
            ArtiÔ¨Åcial Intelligence and Statistics, pages 297‚Äì304, 2010.
           D. Ha and J. Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
           T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep
            reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.
           D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent
            dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.
           N. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa. Learning continuous control
            policies by stochastic value gradients. In Advances in Neural Information Processing Systems,
            pages 2944‚Äì2952, 2015.
           M.Henaff, W. F. Whitney, and Y. LeCun. Model-based planning in discrete action spaces. CoRR,
            abs/1705.07177, 2017.
           M.Henaff,W.F.Whitney,andY.LeCun. Model-basedplanningwithdiscreteandcontinuousactions.
            arXiv preprint arXiv:1705.07177, 2018.
           M.Henaff, A. Canziani, and Y. LeCun. Model-predictive policy learning with uncertainty regulariza-
            tion for driving in dense trafÔ¨Åc. arXiv preprint arXiv:1901.02705, 2019.
           M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot,
            M.Azar,andD.Silver. Rainbow: Combining improvements in deep reinforcement learning. In
            Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence, 2018.
           M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu.
            Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.
           M.I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods
            for graphical models. Machine learning, 37(2):183‚Äì233, 1999.
           L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn,
            P. Kozakowski, S. Levine, et al. Model-based reinforcement learning for atari. arXiv preprint
            arXiv:1903.00374, 2019.
           R. E. Kalman. A new approach to linear Ô¨Åltering and prediction problems. Journal of basic
            Engineering, 82(1):35‚Äì45, 1960.
           M.Karl, M. Soelch, J. Bayer, and P. van der Smagt. Deep variational bayes Ô¨Ålters: Unsupervised
            learning of state space models from raw data. arXiv preprint arXiv:1605.06432, 2016.
           D.P.KingmaandJ.Ba.Adam: Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980,
            2014.
           D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
            2013.
           R. G. Krishnan, U. Shalit, and D. Sontag. Deep kalman Ô¨Ålters. arXiv preprint arXiv:1511.05121,
            2015.
           T. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. Model-ensemble trust-region policy
            optimization. arXiv preprint arXiv:1802.10592, 2018.
                               11
           Published as a conference paper at ICLR 2020
           Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel.
            Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541‚Äì551,
            1989.
           A.X.Lee,A.Nagabandi,P.Abbeel,andS.Levine. Stochastic latent actor-critic: Deep reinforcement
            learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019.
           T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
            control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
           K. Lowrey, A. Rajeswaran, S. Kakade, E. Todorov, and I. Mordatch. Plan online, learn ofÔ¨Çine:
            EfÔ¨Åcient learning and exploration via model-based control. arXiv preprint arXiv:1811.01848,
            2018.
           M.C.Machado,M.G.Bellemare,E.Talvitie, J. Veness, M. Hausknecht, and M. Bowling. Revisiting
            the arcade learning environment: Evaluation protocols and open problems for general agents.
            Journal of ArtiÔ¨Åcial Intelligence Research, 61:523‚Äì562, 2018.
           D. McAllester and K. Statos. Formal limitations on the measurement of mutual information. arXiv
            preprint arXiv:1811.04251, 2018.
           V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
            miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
            learning. Nature, 518(7540):529, 2015.
           V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
            Asynchronous methods for deep reinforcement learning. In International Conference on Machine
            Learning, pages 1928‚Äì1937, 2016.
           J. Oh, S. Singh, and H. Lee. Value prediction network. In Advances in Neural Information Processing
            Systems, pages 6118‚Äì6128, 2017.
           A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding.
            arXiv preprint arXiv:1807.03748, 2018.
           P. Parmas, C. E. Rasmussen, J. Peters, and K. Doya. Pipps: Flexible model-based policy search
            robust to the curse of chaos. arXiv preprint arXiv:1902.01240, 2019.
           A. Piergiovanni, A. Wu, and M. S. Ryoo. Learning real-world robot policies by dreaming. arXiv
            preprint arXiv:1805.07813, 2018.
           B. Poole, S. Ozair, A. v. d. Oord, A. A. Alemi, and G. Tucker. On variational bounds of mutual
            information. arXiv preprint arXiv:1905.06922, 2019.
           D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference
            in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
           J. Schmidhuber. Making the world differentiable: On using self-supervised fully recurrent neural
            networks for dynamic reinforcement learning and planning in non-stationary environments. 1990.
           J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart,
            D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned
            model. arXiv preprint arXiv:1911.08265, 2019.
           J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
            algorithms. arXiv preprint arXiv:1707.06347, 2017.
           D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy
            gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning,
            2014.
           D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
            M.Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):
            354, 2017.
                               12
                         Published as a conference paper at ICLR 2020
                         A. Srinivas, A. Jabri, P. Abbeel, S. Levine, and C. Finn. Universal planning networks. arXiv preprint
                           arXiv:1804.00645, 2018.
                         R. S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART
                           Bulletin, 2(4):160‚Äì163, 1991.
                         R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
                         Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel,
                           A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.
                         N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. arXiv preprint
                           physics/0004057, 2000.
                         T. Wang and J. Ba.   Exploring model-based planning with policy networks.  arXiv preprint
                           arXiv:1906.08649, 2019.
                         T. Wang, X. Bao, I. Clavera, J. Hoang, Y. Wen, E. Langlois, S. Zhang, G. Zhang, P. Abbeel, and J. Ba.
                           Benchmarking model-based reinforcement learning. CoRR, abs/1907.02057, 2019.
                         M.Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear
                           latent dynamics model for control from raw images. In Advances in neural information processing
                           systems, pages 2746‚Äì2754, 2015.
                         T. Weber, S. Racani√®re, D. P. Reichert, L. Buesing, A. Guez, D. J. Rezende, A. P. Badia, O. Vinyals,
                           N. Heess, Y. Li, et al. Imagination-augmented agents for deep reinforcement learning. arXiv
                           preprint arXiv:1707.06203, 2017.
                         R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
                           learning. Machine learning, 8(3-4):229‚Äì256, 1992.
                         M. Zhang, S. Vikram, L. Smith, P. Abbeel, M. Johnson, and S. Levine. Solar: deep structured
                           representations for model-based reinforcement learning. In International Conference on Machine
                           Learning, 2019.
                                                                    13
                          Published as a conference paper at ICLR 2020
                          A HYPERPARAMETERS
                          Modelcomponents WeusetheconvolutionalencoderanddecodernetworksfromHaandSchmid-
                          huber (2018), the RSSM of Hafner et al. (2018), and implement all other functions as three dense
                          layers of size 300 with ELU activations (Clevert et al., 2015). Distributions in latent space are
                          30-dimensional diagonal Gaussians. The action model outputs a tanh mean scaled by a factor of
                          5andasoftplus standard deviation for the Normal distribution that is then transformed using tanh
                          (Haarnoja et al., 2018). The scaling factor allows the agent to saturate the action distribution.
                          Learning updates    Wedrawbatchesof50sequencesoflength50totraintheworldmodel,value
                          model, and action model models using Adam (Kingma and Ba, 2014) with learning rates 6 √ó 10‚àí4,
                          8√ó10‚àí5,8√ó10‚àí5,respectivelyandscaledowngradientnormsthatexceed100. Wedonotscale
                          the KL regularizers (Œ≤ = 1) but clip them below 3 free nats as in PlaNet. The imagination horizon is
                          H=15andthesametrajectoriesareusedtoupdatebothactionandvaluemodels. Wecomputethe
                          VŒªtargets with Œ≥ = 0.99 and Œª = 0.95. We did not Ô¨Ånd latent overshooting for learning the model,
                          an entropy bonus for the action model, or target networks for the value model necessary.
                          Environmentinteraction     Thedataset is initialized with S = 5 episodes collected using random
                          actions. We iterate between 100 training steps and collecting 1 episode by executing the predicted
                          modeaction with Normal(0,0.3) exploration noise. Instead of manually selecting the action repeat
                          for each environment as in Hafner et al. (2018) and Lee et al. (2019), we Ô¨Åx it to 2 for all environments.
                          See Figure 12 for an assessment of the robustness to different action repeat values.
                          Discrete control  For experiments on Atari games and DeepMind Lab levels, the action model
                          predicts the logits of a categorical distribution. We use straight-through gradients for the sampling
                          step during latent imagination. The action noise is epsilon greedy where  is linearly scheduled from
                          0.4 ‚Üí 0.1 over the Ô¨Årst 200,000 gradient steps. To account for the higher complexity of these tasks,
                          weuseanimagination horizon of H = 10, scale the KL regularizers by Œ≤ = 0.1, and bound rewards
                          using tanh. We predict the discount factor from the latent state with a binary classiÔ¨Åer that is trained
                          towards the soft labels of 0 and Œ≥.
                                                                       14
                             Published as a conference paper at ICLR 2020
                             B DERIVATIONS
                            WedeÔ¨Ånetheinformation bottleneck objective (Tishby et al., 2000) for latent dynamics models,
                                                   maxI(s      ; (o  , r   ) | a   ) ‚àíŒ≤I(s     , i   | a   ),                  (13)
                                                           1:T    1:T   1:T    1:T          1:T  1:T    1:T
                                                                                                                    .
                            where Œ≤ is scalar and i are dataset indices that determine the observations p(o | i ) = Œ¥(o ‚àí o¬Ø ) as
                                                    t                                                        t   t       t     t
                             in Alemi et al. (2016).
                             Maximizing the objective leads to model states that can predict the sequence of observations and
                             rewards while limiting the amount of information extracted at each time step. This encourages the
                             model to reconstruct each image by relying on information extracted at preceeding time steps to the
                             extent possible, and only accessing additional information from the current image when necessary.
                            Asaresult, the information regularizer encourages the model to learn long-term dependencies.
                             For the generative objective, we lower bound the Ô¨Årst term using the non-negativity of the KL
                             divergence and drop the marginal data probability as it does not depend on the representation model,
                                I(s   ; (o   , r  ) | a   )
                                   1:T    1:T  1:T     1:T
                             = E                      Xlnp(o ,r            | s  , a   ) ‚àílnp(o      , r   | a   )
                                  p(o  ,r   ,s  ,a   )            1:T   1:T   1:T   1:T           1:T  1:T    1:T
                                     1:T 1:T 1:T  1:T
                                                        t                                          const
                             +      X
                             = E        lnp(o    , r    | s  , a   )
                                              1:T   1:T   1:T   1:T
                                   t                                                                                           
                             ‚â• E Xlnp(o ,r              | s  , a   )  ‚àíKL p(o ,r            | s   , a  )  Yq(o | s )q(r | s )
                                   t         1:T   1:T   1:T   1:T              1:T   1:T   1:T   1:T   t       t   t     t   t
                             = E Xlnq(o |s )+lnq(r |s ) .
                                              t   t          t    t
                                     t                                                                                         (14)
                             For the contrastive objective, we subtract the constant marginal probability of the data under the
                            variational encoder, apply Bayes rule, and use the InfoNCE mini-batch bound (Poole et al., 2019),
                                                                                      
                                                       E lnq(o | s )+lnq(r | s )
                                                                t    t          t   t
                                                    +                                            
                                                   = E lnq(o | s )‚àílnq(o )+lnq(r | s )
                                                                t    t          t          t   t 
                                                   = E lnq(s | o )‚àílnq(s )+lnq(r | s )                                         (15)
                                                               t    t          t          t   t          
                                                   ‚â• E lnq(s |o )‚àílnXq(s |o0)+lnq(r |s ) .
                                                                 t   t               t              t   t
                                                                               0
                                                                              o
                             For the second term, we use the non-negativity of the KL divergence to obtain an upper bound,
                                         I(s   ; i   | a   )
                                            1:T  1:T    1:T                                                             
                                     = E                             Xlnp(s |s         , a   , i ) ‚àí lnp(s | s    , a   )
                                          p(o   ,r   ,s  ,a   ,i  )            t   t‚àí1   t‚àí1 t            t   t‚àí1   t‚àí1
                                             1:T  1:T 1:T  1:T 1:T
                                           X                         t                          
                                     = E         lnp(s | s     , a   , o ) ‚àí lnp(s | s    , a   )
                                                       t   t‚àí1   t‚àí1   t          t    t‚àí1   t‚àí1                               (16)
                                            t                                                   
                                     ‚â§ E Xlnp(s |s             , a   , o ) ‚àí lnq(s | s    , a   )
                                                       t   t‚àí1   t‚àí1   t          t    t‚àí1   t‚àí1
                                            t                                                    
                                             X                                                  
                                     = E         KL p(s | s      , a   , o )  q(s | s    , a   )   .
                                                          t   t‚àí1   t‚àí1   t       t   t‚àí1   t‚àí1
                                              t
                            This lower bounds the objective.
                                                                               15
                                           Published as a conference paper at ICLR 2020
                                           C DISCRETECONTROL
                                          Weevaluate Dreamer on a subset of tasks with discrete actions from the Atari suite (Bellemare et al.,
                                           2013) and DeepMind Lab (Beattie et al., 2016). While agents that purely learn through world models
                                           are not yet competitive in these domains (Kaiser et al., 2019), the tasks offer a diverse test bed with
                                          visual complexity, sparse rewards, and early termination. Agents observe 64 √ó 64 √ó 3 images and
                                           select one of between 3 and 18 actions. For Atari, we follow the evaluation protocol of Machado
                                           et al. (2018) with sticky actions. Refer to Figure 9 for these experiments.
                                                                 Boxing                        Choppercommand                           Doubledunk                           Fishingderby
                                                                                                                             20
                                                 100                                10000                                                                           0
                                                   50                                 5000                                    0
                                                Episode Return                                                                                                     50
                                                    0                                    0                                   20
                                                                                                                                                                 100
                                                             0.5    1.0    1.5                    1      2     3      4                  2        4        6                1      2     3     4
                                                                               1e7                                   1e7                                  1e7                                  1e7
                                                              Ice Hockey                            Kangaroo                                Krull                            Kungfumaster
                                                    0                                                                                                          40000
                                                    5                               10000                                  7500                                30000
                                                   10                                                                      5000                                20000
                                                                                      5000
                                               Episode Return15                                                            2500                                10000
                                                   20                                    0                                    0                                     0
                                                              2       4       6                  0.5    1.0    1.5   2.0               0.5     1.0    1.5                   0.5    1.0    1.5    2.0
                                                                               1e7                                   1e7                                  1e7                                  1e7
                                                              Mspacman                           Namethisgame                               Pong                              Tutankham
                                                4000                                10000
                                                3000                                                                         10                                  200
                                                                                      7500
                                                2000                                                                          0
                                                                                      5000                                                                       100
                                               Episode Return1000                                                            10
                                                                                      2500
                                                    0                                                                        20                                     0
                                                             0.5     1.0     1.5                  1     2     3     4                    2        4       6           0     1    2     3     4    5
                                                                               1e7                                   1e7                                  1e7                                  1e7
                                                              Up N Down                               Zaxxon                       Collect Good Objects                       Watermaze
                                              200000                                15000                                  10.0                                    40
                                              150000                                                                         7.5
                                                                                    10000                                    5.0                                   20
                                              100000
                                            Episode Return50000                       5000                                   2.5
                                                    0                                                                        0.0                                    0
                                                                                         0
                                                               2         4        6               0.5     1.0     1.5               0.250.500.751.001.251.50                1    2     3    4     5
                                                            Environment Steps 1e7                Environment Steps 1e7                Environment Steps 1e7                 Environment Steps 1e7
                                                        Dreamer         SimPLe (1e5 steps)         DQN (2e8 steps)         Rainbow (2e8 steps)        IMPALA (1e10 steps)          Random
                                           Figure 9: Performance of Dreamer in environments with discrete actions and early termination.
                                           Dreamerlearns successful behaviors on this subset of Atari games and the object collection level of
                                           DMLab. Wehighlight representation learning for these environments as a direction of future work
                                           that could enable competitive performance across all Atari games and DMLab levels using Dreamer.
                                                                                                                     16
                                           Published as a conference paper at ICLR 2020
                                           D BEHAVIORLEARNING
                                                         Acrobot Swingup                        Cartpole Balance                  Cartpole Balance Sparse                   Cartpole Swingup
                                              1000                                   1000                                  1000                                  1000
                                               750                                    750                                   750                                    750
                                               500                                    500                                   500                                    500
                                             Episode Return250                        250                                   250                                    250
                                                  0                                     0                                      0                                     0
                                                    0     1     2     3     4     5        0    1     2     3     4     5        0     1     2     3     4    5        0     1     2     3     4     5
                                                     Cartpole Swingup Sparse                      Cheetah Run                             Cup Catch                             Finger Spin
                                              1000                                   1000                                  1000                                  1000
                                               750                                    750                                   750                                    750
                                               500                                    500                                   500                                    500
                                             Episode Return250                        250                                   250                                    250
                                                  0                                     0                                      0                                     0
                                                    0     1     2     3     4     5        0    1     2     3     4     5        0     1     2     3     4    5        0     1     2     3     4     5
                                                         Finger Turn Easy                       Finger Turn Hard                         Hopper Hop                            Hopper Stand
                                              1000                                   1000                                  1000                                  1000
                                               750                                    750                                   750                                    750
                                               500                                    500                                   500                                    500
                                             Episode Return250                        250                                   250                                    250
                                                  0                                     0                                      0                                     0
                                                    0     1     2     3     4     5        0    1     2     3     4     5        0     1     2     3     4    5        0     1     2     3     4     5
                                                        Pendulum Swingup                         Quadruped Run                        Quadruped Walk                           Reacher Easy
                                              1000                                   1000                                  1000                                  1000
                                               750                                    750                                   750                                    750
                                               500                                    500                                   500                                    500
                                             Episode Return250                        250                                   250                                    250
                                                  0                                     0                                      0                                     0
                                                    0     1     2     3     4     5        0    1     2     3     4     5        0     1     2     3     4    5        0     1     2     3     4     5
                                                           Reacher Hard                            Walker Run                           Walker Stand                           Walker Walk
                                              1000                                   1000                                  1000                                  1000
                                               750                                    750                                   750                                    750
                                               500                                    500                                   500                                    500
                                             Episode Return250                        250                                   250                                    250
                                                  0                                     0                                      0                                     0
                                                    0     1     2     3     4     5        0    1     2     3     4     5        0     1     2     3     4    5        0     1     2     3     4     5
                                                          Environment Steps 1e6                  Environment Steps 1e6                 Environment Steps 1e6                 Environment Steps 1e6
                                                              Dreamer         No value         PlaNet        D4PG (1e9 steps)         A3C (1e9 steps, proprio)        SLAC (3e6 steps)
                                           Figure 10: Comparison of action selection schemes on the continuous control tasks of the DeepMind
                                           Control Suite from pixel inputs. The lines show mean scores over environment steps and the shaded
                                           areas show the standard deviation across 5 seeds. We compare Dreamer that learns both actions
                                           and values in imagination, to only learning actions in imagination, and Planet that selects actions
                                           byonline planning instead of learning a policy. The baselines include the top model-free algorithm
                                           D4PG,thewell-knownA3Cagent,andthehybridSLACagent.
                                                                                                                      17
                                        Published as a conference paper at ICLR 2020
                                        E REPRESENTATION LEARNING
                                                     Acrobot Swingup                     Cartpole Balance               Cartpole Balance Sparse                Cartpole Swingup
                                           1000                               1000                                1000                               1000
                                            750                                750                                 750                                 750
                                            500                                500                                 500                                 500
                                         Episode Return250                     250                                 250                                 250
                                              0                                   0                                  0                                   0
                                                0      1      2      3     4        0      1     2      3      4       0      1      2      3      4       0      1      2     3      4
                                                 Cartpole Swingup Sparse                   Cheetah Run                          Cup Catch                          Finger Spin
                                           1000                               1000                                1000                               1000
                                            750                                750                                 750                                 750
                                            500                                500                                 500                                 500
                                         Episode Return250                     250                                 250                                 250
                                              0                                   0                                  0                                   0
                                                0      1      2      3     4        0      1     2      3      4       0      1      2      3      4       0      1      2     3      4
                                                     Finger Turn Easy                   Finger Turn Hard                       Hopper Hop                         Hopper Stand
                                           1000                               1000                                1000                               1000
                                            750                                750                                 750                                 750
                                            500                                500                                 500                                 500
                                         Episode Return250                     250                                 250                                 250
                                              0                                   0                                  0                                   0
                                                0      1      2      3     4        0      1     2      3      4       0      1      2      3      4       0      1      2     3      4
                                                    Pendulum Swingup                     Quadruped Run                      Quadruped Walk                        Reacher Easy
                                           1000                               1000                                1000                               1000
                                            750                                750                                 750                                 750
                                            500                                500                                 500                                 500
                                         Episode Return250                     250                                 250                                 250
                                              0                                   0                                  0                                   0
                                                0      1      2      3     4        0      1     2      3      4       0      1      2      3      4       0      1      2     3      4
                                                       Reacher Hard                        Walker Run                         Walker Stand                        Walker Walk
                                           1000                               1000                                1000                               1000
                                            750                                750                                 750                                 750
                                            500                                500                                 500                                 500
                                         Episode Return250                     250                                 250                                 250
                                              0                                   0                                  0                                   0
                                                0      1      2      3     4        0      1     2      3      4       0      1      2      3      4       0      1      2     3      4
                                                      Environment Steps 1e6              Environment Steps 1e6               Environment Steps 1e6               Environment Steps 1e6
                                               Dreamer + Reconstruction       Dreamer + Contrastive       Dreamer + Reward only        D4PG (1e9 steps)       A3C (1e9 steps, proprio)
                                        Figure 11: Comparison of representation learning methods for Dreamer. The lines show mean scores
                                        and the shaded areas show the standard deviation across 5 seeds. We compare generating both
                                        images and rewards, generating rewards and using a contrastive loss to learn about the images, and
                                        only predicting rewards. Image reconstruction provides the best learning signal across most of the
                                        tasks, followed by the contrastive objective. Learning purely from rewards was not sufÔ¨Åcient in our
                                        experiments and might require larger amounts of experience.
                                                                                                              18
                                    Published as a conference paper at ICLR 2020
                                    F ACTIONREPEAT
                                      1000     Acrobot Swingup        1000     Cartpole Balance       1000 Cartpole Balance Sparse    1000    Cartpole Swingup
                                       800                             800                             800                             800
                                       600                             600                             600                             600
                                     Episode Return400                 400                             400                             400
                                       200                             200                             200                             200
                                         0                               0                               0                               0
                                           0.0 0.2  0.4  0.6  0.8  1.0    0.0  0.2  0.4  0.6 0.8  1.0     0.0  0.2  0.4 0.6  0.8  1.0     0.0  0.2 0.4  0.6  0.8  1.0
                                      1000 Cartpole Swingup Sparse 1000          Cheetah Run          1000        Cup Catch           1000       Finger Spin
                                       800                             800                             800                             800
                                       600                             600                             600                             600
                                     Episode Return400                 400                             400                             400
                                       200                             200                             200                             200
                                         0                               0                               0                               0
                                           0.0 0.2  0.4  0.6  0.8  1.0    0.0  0.2  0.4  0.6 0.8  1.0     0.0  0.2  0.4 0.6  0.8  1.0     0.0  0.2 0.4  0.6  0.8  1.0
                                      1000     Finger Turn Easy       1000     Finger Turn Hard       1000       Hopper Hop           1000      Hopper Stand
                                       800                             800                             800                             800
                                       600                             600                             600                             600
                                     Episode Return400                 400                             400                             400
                                       200                             200                             200                             200
                                         0                               0                               0                               0
                                           0.0 0.2  0.4  0.6  0.8  1.0    0.0  0.2  0.4  0.6 0.8  1.0     0.0  0.2  0.4 0.6  0.8  1.0     0.0  0.2 0.4  0.6  0.8  1.0
                                      1000    Pendulum Swingup        1000      Quadruped Run         1000     Quadruped Walk         1000      Reacher Easy
                                       800                             800                             800                             800
                                       600                             600                             600                             600
                                     Episode Return400                 400                             400                             400
                                       200                             200                             200                             200
                                         0                               0                               0                               0
                                           0.0 0.2  0.4  0.6  0.8  1.0    0.0  0.2  0.4  0.6 0.8  1.0     0.0  0.2  0.4 0.6  0.8  1.0     0.0  0.2 0.4  0.6  0.8  1.0
                                      1000       Reacher Hard         1000        Walker Run          1000       Walker Stand         1000       Walker Walk
                                       800                             800                             800                             800
                                       600                             600                             600                             600
                                     Episode Return400                 400                             400                             400
                                       200                             200                             200                             200
                                         0                               0                               0                               0
                                           0.0 0.2  0.4  0.6  0.8  1.0    0.0  0.2  0.4  0.6 0.8  1.0     0.0  0.2  0.4 0.6  0.8  1.0     0.0  0.2 0.4  0.6  0.8  1.0
                                                Environment Steps 1e6           Environment Steps 1e6           Environment Steps 1e6          Environment Steps 1e6
                                        Repeat 1      Repeat 2     Repeat 4      A3C (1e9 steps, proprio)  D4PG (1e9 steps)     PlaNet (1e6 steps)   SLAC (3e6 steps)
                                    Figure 12: Robustness of Dreamer to different control frequencies. Reinforcement learning methods
                                    can be sensitive to this hyper parameter, which could be ampliÔ¨Åed when learning dynamics models
                                    at the control frequency of the environment. For this experiment, we train Dreamer with different
                                    amounts of action repeat. The areas show one standard deviation across 2 seeds. We used a previous
                                    hyper parameter setting for this experiment. We Ô¨Ånd that a value of R = 2 works best across tasks.
                                                                                                  19
                        Published as a conference paper at ICLR 2020
                        G CONTINUOUSCONTROLSCORES
                                                                         A3C       D4PG      PlaNet1    Dreamer
                        Modality                                       proprio      pixels     pixels     pixels
                                                                            9           9          6           6
                        Steps                                             10          10     5√ó10        5√ó10
                        Acrobot Swingup                                 41.90       91.70       3.21      365.26
                        Cartpole Balance                               951.60      992.80     452.56      979.56
                        Cartpole Balance Sparse                        857.40     1000.00     164.74      941.84
                        Cartpole Swingup                               558.40      862.00     312.56      833.66
                        Cartpole Swingup Sparse                        179.80      482.00       0.64      812.22
                        Cheetah Run                                    213.90      523.80     496.12      894.56
                        CupCatch                                       104.70      980.50     455.98      962.48
                        Finger Spin                                    129.40      985.70     495.25      498.88
                        Finger Turn Easy                               167.30      971.40     451.22      825.86
                        Finger Turn Hard                                88.70      966.00     312.55      891.38
                        HopperHop                                         0.50     242.00       0.37      368.97
                        HopperStand                                     27.90      929.90       5.96      923.72
                        PendulumSwingup                                 48.60      680.90       3.27      833.00
                        Quadruped Run                                       ‚àí          ‚àí      280.45      888.39
                        Quadruped Walk                                      ‚àí          ‚àí      238.90      931.61
                        Reacher Easy                                    95.60      967.40     468.50      935.08
                        Reacher Hard                                    39.70      957.10     187.02      817.05
                        Walker Run                                     191.80      567.20     626.25      824.67
                        Walker Stand                                   378.40      985.20     759.19      977.99
                        Walker Walk                                    311.00      968.30     944.70      961.67
                        Average                                        243.70      786.32     332.97      823.39
                           1Were-run PlaNet with Ô¨Åxed action repeat of R = 2 to not tune the this value for each of the 20 tasks. As a
                        result, the scores differ from Hafner et al. (2018).
                                                                   20
