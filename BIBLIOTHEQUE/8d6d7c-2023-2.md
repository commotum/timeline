# 4D-Former: Multimodal 4D Panoptic Segmentation (2023)
Source: 8d6d7c-2023.pdf

## Core reasons
- The paper targets 4D panoptic segmentation of LiDAR point-cloud sequences (3D over time), which is a higher-dimensional perception domain beyond 1D sequence modeling.
- It proposes a transformer-style query architecture with cross-attention and self-attention to fuse LiDAR and image features for this 4D task.

## Evidence extracts
- "4D panoptic segmentation is a challenging but practically useful task that requires every point in a LiDAR point-cloud sequence to be assigned a semantic class label, and individual objects to be segmented and tracked over time." (p. 1)
- "The queries are input to a series of ‘fusion blocks’. Each block is composed of multiple layers where the queries Q are updated by: (1) cross-attending to the voxel features Vi at a given stride, (2) cross-attending to the set of image features Fi ... and (3) self-attending to each other twice intermittently, and also passing through 2× Feedforward Networks (FFN)." (p. 4)

## Classification
Class name: Increasing Transformer's Dimensions
Class code: 2

$$
\boxed{2}
$$
