           Published as a conference paper at ICLR 2020
           REFERENCES
           A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. arXiv
            preprint arXiv:1612.00410, 2016.
           E. Banijamali, R. Shu, M. Ghavamzadeh, H. Bui, and A. Ghodsi. Robust locally-linear controllable
            embedding. arXiv preprint arXiv:1710.05373, 2017.
           G.Barth-Maron,M.W.Hoffman,D.Budden,W.Dabney,D.Horgan,A.Muldal,N.Heess,andT.Lil-
            licrap. Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617,
            2018.
           C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. Küttler, A. Lefrancq, S. Green,
            V. Valdés, A. Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.
           M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
            evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,
            2013.
           Y. Bengio, N. Léonard, and A. Courville. Estimating or propagating gradients through stochastic
            neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
           J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee. Sample-efﬁcient reinforcement learning
            with stochastic ensemble value expansion. In Advances in Neural Information Processing Systems,
            pages 8224–8234, 2018.
           L. Buesing, T. Weber, S. Racaniere, S. Eslami, D. Rezende, D. P. Reichert, F. Viola, F. Besse,
            K. Gregor, D. Hassabis, et al. Learning and querying fast generative models for reinforcement
            learning. arXiv preprint arXiv:1802.03006, 2018.
           A. Byravan, J. T. Springenberg, A. Abdolmaleki, R. Hafner, M. Neunert, T. Lampe, N. Siegel,
            N. Heess, and M. Riedmiller. Imagined value gradients: Model-based policy optimization with
            transferable latent dynamics models. arXiv preprint arXiv:1910.04142, 2019.
           P. S. Castro, S. Moitra, C. Gelada, S. Kumar, and M. G. Bellemare. Dopamine: A research framework
            for deep reinforcement learning. arXiv preprint arXiv:1812.06110, 2018.
           K. Chua, R. Calandra, R. McAllister, and S. Levine. Deep reinforcement learning in a handful of
            trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems,
            pages 4754–4765, 2018.
           D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by
            exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
           J. V. Dillon, I. Langmore, D. Tran, E. Brevdo, S. Vasudevan, D. Moore, B. Patton, A. Alemi,
            M.Hoffman,andR.A.Saurous. Tensorﬂowdistributions. arXiv preprint arXiv:1711.10604, 2017.
           A. Doerr, C. Daniel, M. Schiegg, D. Nguyen-Tuong, S. Schaal, M. Toussaint, and S. Trimpe.
            Probabilistic recurrent state-space models. arXiv preprint arXiv:1801.10395, 2018.
           F. Ebert, C. Finn, A. X. Lee, and S. Levine. Self-supervised visual planning with temporal skip
            connections. arXiv preprint arXiv:1710.05268, 2017.
           S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo, A. Ruderman, A. A. Rusu,
            I. Danihelka, K. Gregor, et al. Neural scene representation and rendering. Science, 360(6394):
            1204–1210, 2018.
           L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley,
            I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner
            architectures. arXiv preprint arXiv:1802.01561, 2018.
           V. Feinberg, A. Wan, I. Stoica, M. I. Jordan, J. E. Gonzalez, and S. Levine. Model-based value
            estimation for efﬁcient model-free reinforcement learning. arXiv preprint arXiv:1803.00101, 2018.
                               10
