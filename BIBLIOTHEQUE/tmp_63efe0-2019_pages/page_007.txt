                               Published as a conference paper at ICLR 2020
                                         Acrobot Swingup          Cartpole Swingup Sparse          Hopper Hop        1000      Hopper Stand
                                  400                         800                         400
                                  300                         600                                                     750
                                  200                         400                         200                         500
                                 Episode Return100            200                                                     250
                                    0                           0                           0                           0
                                     0.0  0.5  1.0   1.5  2.0    0.0  0.5  1.0   1.5  2.0    0.0  0.5   1.0  1.5  2.0    0.0  0.5   1.0  1.5  2.0
                                        Pendulum Swingup     1000    Quadruped Walk                Walker Run                  Walker Walk
                                                                                          800                        1000
                                  750                         750                                                     750
                                  500                                                     600
                                                              500                         400                         500
                                 Episode Return250            250                         200                         250
                                    0                           0                           0                           0
                                     0.0  0.5  1.0   1.5  2.0    0.0  0.5  1.0   1.5  2.0    0.0  0.5   1.0  1.5  2.0    0.0  0.5   1.0  1.5  2.0
                                          Environment Steps 1e6       Environment Steps 1e6       Environment Steps 1e6       Environment Steps 1e6
                                                       Dreamer    No value     PlaNet    D4PG (1e9 steps)  A3C (1e9 steps, proprio)
                               Figure 7: Dreamer succeeds at visual control tasks that require long-horizon credit assignment, such
                               as the acrobot and hopper tasks. Optimizing only imagined rewards within the horizon via an action
                               model or by online planning yields shortsighted behaviors that only succeed in reactive tasks, such as
                               in the walker domain. The performance on all 20 tasks is summarized in Figure 6 and training curves
                               are shown in Appendix D. See Tassa et al. (2018) for performance curves of D4PG and A3C.
                               5     RELATED WORK
                               Prior works learn latent dynamics for visual control by derivative-free policy learning or online
                               planning, augment model-free agents with multi-step predictions, or use analytic gradients of Q-
                               values or multi-step rewards, often for low-dimensional tasks. In comparison, Dreamer uses analytic
                               gradients to efﬁciently learn long-horizon behaviors for visual control purely by latent imagination.
                               Control with latent dynamics           E2C(Watter et al., 2015) and RCE (Banijamali et al., 2017) embed
                               images to predict forward in a compact space to solve simple tasks. World Models (Ha and Schmid-
                               huber, 2018) learn latent dynamics in a two-stage process to evolve linear controllers in imagination.
                               PlaNet (Hafner et al., 2018) learns them jointly and solves visual locomotion tasks by latent online
                               planning. SOLAR (Zhang et al., 2019) solves robotic tasks via guided policy search in latent space.
                               I2A(Weberetal., 2017) hands imagined trajectories to a model-free policy, while Lee et al. (2019)
                               and Gregor et al. (2019) learn belief representations to accelerate model-free agents.
                               Imagined multi-step returns           VPN(Ohetal., 2017), MVE (Feinberg et al., 2018), and STEVE
                               (Buckman et al., 2018) learn dynamics for multi-step Q-learning from a replay buffer. AlphaGo
                               (Silver et al., 2017) combines predictions of actions and state values with planning, assuming access
                               to the true dynamics. Also assuming access to the dynamics, POLO (Lowrey et al., 2018) plans
                               to explore by learning a value ensemble. MuZero (Schrittwieser et al., 2019) learns task-speciﬁc
                               reward and value models to solve challenging tasks but requires large amounts of experience. PETS
                               (Chua et al., 2018), VisualMPC (Ebert et al., 2017), and PlaNet (Hafner et al., 2018) plan online
                               using derivative-free optimization. POPLIN (Wang and Ba, 2019) improves over online planning by
                               self-imitation. Piergiovanni et al. (2018) learn robot policies by imagination with a latent dynamics
                               model. Planning with neural network gradients was shown on small problems (Schmidhuber, 1990;
                               Henaff et al., 2018) but has been challenging to scale (Parmas et al., 2019).
                               Analytic value gradients         DPG (Silver et al., 2014), DDPG (Lillicrap et al., 2015), and SAC
                               (Haarnoja et al., 2018) leverage gradients of learned immediate action values to learn a policy by
                               experience replay. SVG (Heess et al., 2015) reduces the variance of model-free on-policy algorithms
                               by analytic value gradients of one-step model predictions. Concurrent work by Byravan et al. (2019)
                               uses latent imagination with deterministic models for navigation and manipulation tasks. ME-TRPO
                               (Kurutach et al., 2018) accelerates an otherwise model-free agent via gradients of predicted rewards
                               for proprioceptive inputs. DistGBP (Henaff et al., 2017; 2019) uses model gradients for online
                               planning in simple tasks.
                                                                                       7
