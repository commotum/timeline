           Published as a conference paper at ICLR 2020
           C.Gelada,S.Kumar,J.Buckman,O.Nachum,andM.G.Bellemare. Deepmdp: Learningcontinuous
            latent space models for representation learning. arXiv preprint arXiv:1906.02736, 2019.
           K. Gregor, D. J. Rezende, F. Besse, Y. Wu, H. Merzic, and A. v. d. Oord. Shaping belief states with
            generative environment models for rl. arXiv preprint arXiv:1906.09237, 2019.
           Z. D. Guo, M. G. Azar, B. Piot, B. A. Pires, T. Pohlen, and R. Munos. Neural predictive belief
            representations. arXiv preprint arXiv:1811.06407, 2018.
           M. Gutmann and A. Hyvärinen. Noise-contrastive estimation: A new estimation principle for
            unnormalized statistical models. In Proceedings of the Thirteenth International Conference on
            Artiﬁcial Intelligence and Statistics, pages 297–304, 2010.
           D. Ha and J. Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
           T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep
            reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.
           D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent
            dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.
           N. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa. Learning continuous control
            policies by stochastic value gradients. In Advances in Neural Information Processing Systems,
            pages 2944–2952, 2015.
           M.Henaff, W. F. Whitney, and Y. LeCun. Model-based planning in discrete action spaces. CoRR,
            abs/1705.07177, 2017.
           M.Henaff,W.F.Whitney,andY.LeCun. Model-basedplanningwithdiscreteandcontinuousactions.
            arXiv preprint arXiv:1705.07177, 2018.
           M.Henaff, A. Canziani, and Y. LeCun. Model-predictive policy learning with uncertainty regulariza-
            tion for driving in dense trafﬁc. arXiv preprint arXiv:1901.02705, 2019.
           M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot,
            M.Azar,andD.Silver. Rainbow: Combining improvements in deep reinforcement learning. In
            Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.
           M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu.
            Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.
           M.I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods
            for graphical models. Machine learning, 37(2):183–233, 1999.
           L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn,
            P. Kozakowski, S. Levine, et al. Model-based reinforcement learning for atari. arXiv preprint
            arXiv:1903.00374, 2019.
           R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Journal of basic
            Engineering, 82(1):35–45, 1960.
           M.Karl, M. Soelch, J. Bayer, and P. van der Smagt. Deep variational bayes ﬁlters: Unsupervised
            learning of state space models from raw data. arXiv preprint arXiv:1605.06432, 2016.
           D.P.KingmaandJ.Ba.Adam: Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980,
            2014.
           D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
            2013.
           R. G. Krishnan, U. Shalit, and D. Sontag. Deep kalman ﬁlters. arXiv preprint arXiv:1511.05121,
            2015.
           T. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. Model-ensemble trust-region policy
            optimization. arXiv preprint arXiv:1802.10592, 2018.
                               11
