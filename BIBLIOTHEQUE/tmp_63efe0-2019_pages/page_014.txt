                          Published as a conference paper at ICLR 2020
                          A HYPERPARAMETERS
                          Modelcomponents WeusetheconvolutionalencoderanddecodernetworksfromHaandSchmid-
                          huber (2018), the RSSM of Hafner et al. (2018), and implement all other functions as three dense
                          layers of size 300 with ELU activations (Clevert et al., 2015). Distributions in latent space are
                          30-dimensional diagonal Gaussians. The action model outputs a tanh mean scaled by a factor of
                          5andasoftplus standard deviation for the Normal distribution that is then transformed using tanh
                          (Haarnoja et al., 2018). The scaling factor allows the agent to saturate the action distribution.
                          Learning updates    Wedrawbatchesof50sequencesoflength50totraintheworldmodel,value
                          model, and action model models using Adam (Kingma and Ba, 2014) with learning rates 6 √ó 10‚àí4,
                          8√ó10‚àí5,8√ó10‚àí5,respectivelyandscaledowngradientnormsthatexceed100. Wedonotscale
                          the KL regularizers (Œ≤ = 1) but clip them below 3 free nats as in PlaNet. The imagination horizon is
                          H=15andthesametrajectoriesareusedtoupdatebothactionandvaluemodels. Wecomputethe
                          VŒªtargets with Œ≥ = 0.99 and Œª = 0.95. We did not Ô¨Ånd latent overshooting for learning the model,
                          an entropy bonus for the action model, or target networks for the value model necessary.
                          Environmentinteraction     Thedataset is initialized with S = 5 episodes collected using random
                          actions. We iterate between 100 training steps and collecting 1 episode by executing the predicted
                          modeaction with Normal(0,0.3) exploration noise. Instead of manually selecting the action repeat
                          for each environment as in Hafner et al. (2018) and Lee et al. (2019), we Ô¨Åx it to 2 for all environments.
                          See Figure 12 for an assessment of the robustness to different action repeat values.
                          Discrete control  For experiments on Atari games and DeepMind Lab levels, the action model
                          predicts the logits of a categorical distribution. We use straight-through gradients for the sampling
                          step during latent imagination. The action noise is epsilon greedy where  is linearly scheduled from
                          0.4 ‚Üí 0.1 over the Ô¨Årst 200,000 gradient steps. To account for the higher complexity of these tasks,
                          weuseanimagination horizon of H = 10, scale the KL regularizers by Œ≤ = 0.1, and bound rewards
                          using tanh. We predict the discount factor from the latent state with a binary classiÔ¨Åer that is trained
                          towards the soft labels of 0 and Œ≥.
                                                                       14
