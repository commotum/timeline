                                           Published as a conference paper at ICLR 2020
                                                        Acrobot Swingup                          Cheetah Run                            Cup Catch                             Finger Spin
                                               400                                                                        1000                                  1000
                                               300                                   750                                   750                                   750
                                               200                                   500                                   500                                   500
                                             Episode Return100                       250                                   250                                   250
                                                 0                                     0                                     0                                     0
                                                   0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0
                                              1000         Hopper Stand             1000     Pendulum Swingup             1000       Quadruped Run                           Walker Stand
                                                                                                                                                                1000
                                               750                                   750                                   750                                   800
                                               500                                   500                                   500                                   600
                                            Episode Return250                        250                                   250                                   400
                                                 0                                     0                                                                         200
                                                                                                                             0
                                                   0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0       0.0    0.5    1.0    1.5    2.0
                                                         Environment Steps 1e6                 Environment Steps 1e6                 Environment Steps 1e6                 Environment Steps 1e6
                                                  Dreamer + Reconstruction          Dreamer + Contrastive         Dreamer + Reward only          D4PG (1e9 steps)         A3C (1e9 steps, proprio)
                                           Figure 8: Comparison of representation learning objectives to be used with Dreamer. Pixel recon-
                                           struction performs best for the majority of tasks. The contrastive objective solves about half of the
                                           tasks, while predicting rewards alone was not sufﬁcient in our experiments. The results suggest that
                                           future developments in learning representations are likely to translate into improved task performance
                                           for Dreamer. The performance curves for all tasks are included in Appendix E.
                                           6      EXPERIMENTS
                                          Weexperimentally evaluate Dreamer on a variety of control tasks. We designed the experiments
                                           to compare Dreamer to current best methods in the literature, and to evaluate its ability to solve
                                           tasks with long horizons, continuous actions, discrete actions, and early termination. We further
                                           compare the orthogonal choice of learning objective for the world model. The source code for all our
                                           experiments and videos of Dreamer are available at https://danijar.com/dreamer.
                                           Control tasks             Weevaluate Dreamer on 20 visual control tasks of the DeepMind Control Suite
                                          (Tassa et al., 2018), illustrated in Figure 2. These tasks pose a variety of challenges, including sparse
                                           rewards, contact dynamics, and 3D scenes. We selected the tasks on which Tassa et al. (2018) report
                                           non-zero performance from image inputs. Agent observations are images of shape 64 × 64 × 3,
                                           actions range from 1 to 12 dimensions, rewards range from 0 to 1, episodes last for 1000 steps and
                                           haverandomizedinitialstates. WeuseaﬁxedactionrepeatofR = 2acrosstasks. Wefurtherevaluate
                                           the applicability of Dreamer to discrete actions and early termination on a subset of Atari games
                                          (Bellemare et al., 2013) and DeepMind Lab levels (Beattie et al., 2016) as detailed in Appendix C.
                                           Implementation                Ourimplementation uses TensorFlow Probability (Dillon et al., 2017). We use a
                                           single Nvidia V100 GPU and 10 CPU cores for each training run. The training time for our Dreamer
                                                                                                          6
                                           implementation is below 5 hours per 10 environment steps on the control suite, compared to 11
                                           hours for online planning using PlaNet, and the 24 hours used by D4PG to reach similar performance.
                                          Weusethesamehyperparametersacross all continuous tasks, and similarly across all discrete tasks,
                                           detailed in Appendix A. The world models are learned via reconstruction unless speciﬁed.
                                           Baseline methods                Thehighest reported performance on the continuous tasks is achieved by D4PG
                                          (Barth-Maron et al., 2018), an improved variant of DDPG (Lillicrap et al., 2015) that uses distributed
                                           collection, distributional Q-learning, multi-step returns, and prioritized replay. We include the scores
                                           for D4PG with pixel inputs and A3C (Mnih et al., 2016) with state inputs from Tassa et al. (2018).
                                           PlaNet (Hafner et al., 2018) learns the same world model as Dreamer and selects actions via online
                                           planning without an action model and drastically improves over D4PG and A3C in data efﬁciency. We
                                           re-run PlaNet with R = 2 for a uniﬁed experimental setup. For Atari, we show the ﬁnal performance
                                           of SimPLe (Kaiser et al., 2019), DQN (Mnih et al., 2015) and Rainbow (Hessel et al., 2018) reported
                                           by Castro et al. (2018), and for DeepMind Lab that of IMPALA (Espeholt et al., 2018) as a guideline.
                                                                                                                       8
