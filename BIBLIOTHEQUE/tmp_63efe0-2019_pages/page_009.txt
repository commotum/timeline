                         Published as a conference paper at ICLR 2020
                         Performance Toevaluate the performance of Dreamer, we compare it to state-of-the-art reinforce-
                         mentlearning agents. The results are summarized in Figure 6. With an average score of 823 across
                         tasks after 5 × 106 environment steps, Dreamer exceeds the performance of the strong model-free
                                                                        9
                         D4PGagentthatachievesanaverageof786within10 environmentsteps. Atthesametime,Dreamer
                         inherits the data-efﬁciency of PlaNet, conﬁrming that the learned world model can help to generalize
                         from small amounts of experience. The empirical success of Dreamer shows that learning behaviors
                         bylatent imagination with world models can outperform top methods based on experience replay.
                         Longhorizons Toinvestigate its ability to learn long-horizon behaviors, we compare Dreamer to
                         alternatives for deriving behaviors from the world model at various horizon lengths. For this, we
                         learn an action model to maximize imagined rewards without a value model and compare to online
                         planning using PlaNet. Figure 4 shows the ﬁnal performance for different imagination horizons,
                         conﬁrming that the value model makes Dreamer more robust to the horizon and performs well even
                         for short horizons. Performance curves for all 19 tasks with horizon of 20 are shown in Appendix D,
                         where Dreamer outperforms the alternatives on 16 of 20 tasks, with 4 ties.
                         Representation learning  Dreamercanbeusedwithanydifferentiable dynamics model that pre-
                         dicts future rewards given actions and past observations. Since the representation learning objective
                         is orthogonal to our algorithm, we compare three natural choices described in Section 4: pixel recon-
                         struction, contrastive estimation, and pure reward prediction. Figure 8 shows clear differences in task
                         performance for different representation learning approaches, with pixel reconstruction outperform-
                         ing contrastive estimation on most tasks. This suggests that future improvements in representation
                         learning are likely to translate to higher task performance with Dreamer. Reward prediction alone
                         wasnotsufﬁcient in our experiments. Further ablations are included in the appendix of the paper.
                         7   CONCLUSION
                         Wepresent Dreamer, an agent that learns long-horizon behaviors purely by latent imagination. For
                         this, we propose an actor critic method that optimizes a parametric policy by propagating analytic
                         gradients of multi-step values back through learned latent dynamics. Dreamer outperforms previous
                         methods in data-efﬁciency, computation time, and ﬁnal performance on a variety of challenging
                         continuous control tasks with image inputs. We further show that Dreamer is applicable to tasks with
                         discrete actions and early episode termination. Future research on representation learning can likely
                         scale latent imagination to environments of higher visual complexity.
                         Acknowledgements WethankSimonKornblith,BenjaminEysenbach,IanFischer,AmyZhang,
                         Geoffrey Hinton, Shane Gu, Adam Kosiorek, Jacob Buckman, Calvin Luo, and Rishabh Agarwal,
                         and our anonymous reviewers for feedback and discussions. We thank Yuval Tassa for adding the
                         quadruped environment to the control suite.
                                                                     9
