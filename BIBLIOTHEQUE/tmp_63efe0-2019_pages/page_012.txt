           Published as a conference paper at ICLR 2020
           Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel.
            Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551,
            1989.
           A.X.Lee,A.Nagabandi,P.Abbeel,andS.Levine. Stochastic latent actor-critic: Deep reinforcement
            learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019.
           T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
            control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
           K. Lowrey, A. Rajeswaran, S. Kakade, E. Todorov, and I. Mordatch. Plan online, learn ofﬂine:
            Efﬁcient learning and exploration via model-based control. arXiv preprint arXiv:1811.01848,
            2018.
           M.C.Machado,M.G.Bellemare,E.Talvitie, J. Veness, M. Hausknecht, and M. Bowling. Revisiting
            the arcade learning environment: Evaluation protocols and open problems for general agents.
            Journal of Artiﬁcial Intelligence Research, 61:523–562, 2018.
           D. McAllester and K. Statos. Formal limitations on the measurement of mutual information. arXiv
            preprint arXiv:1811.04251, 2018.
           V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
            miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
            learning. Nature, 518(7540):529, 2015.
           V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
            Asynchronous methods for deep reinforcement learning. In International Conference on Machine
            Learning, pages 1928–1937, 2016.
           J. Oh, S. Singh, and H. Lee. Value prediction network. In Advances in Neural Information Processing
            Systems, pages 6118–6128, 2017.
           A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding.
            arXiv preprint arXiv:1807.03748, 2018.
           P. Parmas, C. E. Rasmussen, J. Peters, and K. Doya. Pipps: Flexible model-based policy search
            robust to the curse of chaos. arXiv preprint arXiv:1902.01240, 2019.
           A. Piergiovanni, A. Wu, and M. S. Ryoo. Learning real-world robot policies by dreaming. arXiv
            preprint arXiv:1805.07813, 2018.
           B. Poole, S. Ozair, A. v. d. Oord, A. A. Alemi, and G. Tucker. On variational bounds of mutual
            information. arXiv preprint arXiv:1905.06922, 2019.
           D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference
            in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
           J. Schmidhuber. Making the world differentiable: On using self-supervised fully recurrent neural
            networks for dynamic reinforcement learning and planning in non-stationary environments. 1990.
           J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart,
            D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned
            model. arXiv preprint arXiv:1911.08265, 2019.
           J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
            algorithms. arXiv preprint arXiv:1707.06347, 2017.
           D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy
            gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning,
            2014.
           D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
            M.Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):
            354, 2017.
                               12
