                           Published as a conference paper at ICLR 2020
                            DREAMTOCONTROL: LEARNING BEHAVIORS
                            BY LATENT IMAGINATION
                             Danijar Hafner∗          TimothyLillicrap       JimmyBa                  MohammadNorouzi
                             University of Toronto    DeepMind               University of Toronto    Google Brain
                             Google Brain
                                                                        Abstract
                                    Learned world models summarize an agent’s experience to facilitate learning
                                    complex behaviors. While learning world models from high-dimensional sensory
                                    inputs is becoming feasible through deep learning, there are many potential ways
                                    for deriving behaviors from them. We present Dreamer, a reinforcement learning
                                    agent that solves long-horizon tasks from images purely by latent imagination.
                                    Weefﬁciently learn behaviors by propagating analytic gradients of learned state
                                    values back through trajectories imagined in the compact state space of a learned
                                    world model. On 20 challenging visual control tasks, Dreamer exceeds existing
                                    approaches in data-efﬁciency, computation time, and ﬁnal performance.
                            1   INTRODUCTION
                           Intelligent agents can achieve goals in complex environments even though
                           they never encounter the exact same situation twice. This ability requires      Dataset of Experience
                           building representations of the world from past experience that enable
                           generalization to novel situations. World models offer an explicit way to
                           represent an agent’s knowledge about the world in a parametric model that
                           can make predictions about the future.
                           When the sensory inputs are high-dimensional images, latent dynamics
                           models can abstract observations to predict forward in compact state spaces
                           (Watter et al., 2015; Oh et al., 2017; Gregor et al., 2019). Compared to       Learned Latent Dynamics
                           predictions in image space, latent states have a small memory footprint that
                           enables imagining thousands of trajectories in parallel. Learning effective
                           latent dynamics models is becoming feasible through advances in deep
                           learning and latent variable models (Krishnan et al., 2015; Karl et al., 2016;
                           Doerr et al., 2018; Buesing et al., 2018).
                           Behaviors can be derived from dynamics models in many ways. Often,
                           imagined rewards are maximized with a parametric policy (Sutton, 1991;
                           Ha and Schmidhuber, 2018; Zhang et al., 2019) or by online planning
                           (Chua et al., 2018; Hafner et al., 2018). However, considering only rewards
                           within a ﬁxed imagination horizon results in shortsighted behaviors (Wang      Value and Action Learned 
                           et al., 2019). Moreover, prior work commonly resorts to derivative-free         by Latent Imagination
                           optimization for robustness to model errors (Ebert et al., 2017; Chua et al.,
                           2018; Parmas et al., 2019), rather than leveraging analytic gradients offered
                           byneural network dynamics (Henaff et al., 2019; Srinivas et al., 2018).
                           We present Dreamer, an agent that learns long-horizon behaviors from
                           imagespurely by latent imagination. A novel actor critic algorithm accounts
                           for rewards beyond the imagination horizon while making efﬁcient use of      Figure 1:     Dreamer
                           the neural network dynamics. For this, we predict state values and actions   learns a world model
                           in the learned latent space as summarized in Figure 1. The values optimize   from past experience
                           Bellman consistency for imagined rewards and the policy maximizes the        and efﬁciently learns
                           values by propagating their analytic gradients back through the dynamics.    farsighted behaviors in
                           In comparison to actor critic algorithms that learn online or by experience  its  latent space by
                           replay (Lillicrap et al., 2015; Mnih et al., 2016; Schulman et al., 2017;    backpropagating value
                           Haarnoja et al., 2018; Lee et al., 2019), world models can interpolate past  estimatesbackthrough
                           experience and offer analytic gradients of multi-step returns for efﬁcient   imagined trajectories.
                           policy optimization.
                              ∗Correspondence to: Danijar Hafner <mail@danijar.com>.
                                                                            1
