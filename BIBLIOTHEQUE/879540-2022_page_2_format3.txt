             volution layers [18], recurrent networks [8,42], and object-         • To our knowledge, we first propose the Transformer-
             centric fusion module [24,36,41] have shown favorable re-              based end-to-end 3D detection framework that ex-
             sults on modeling temporal information. On the other hand,             plores the integrated utilization of sequential multi-
             many approaches make use of cross-sensor data, which                   modal data. The proposed method is capable to align
             contains richer textures and broader context than single-              the 4D spatiotemporal cross-sensor information.
             modal input especially for small objects or instances at far         • We propose a simple yet effective data augmentation
             range.   The typical cross-sensor fusion schemes include               technique to preserve both the cross-sensor and cross-
             proposal-level feature concatenation [11, 22], feature pro-            time consistency to facilitate training 3D detectors.
             jection [15,20] and point-wise concatenation [27,31]. How-           • We conduct extensive experiments on the challenging
             ever, existing approaches do not take full advantage of in-            large-scale nuScenes and Waymo datasets. The pro-
             formation fusion across sensors and time simultaneously,               posed LIFT performs well over the state-of-the-art.
             whichpotentiallylimitstheperformanceofmulti-modal3D
             object detection. Though the very recent work [20] makes          2. Related Work
             anearly trial of learning a 4D network, in fact, it uses a pre-
             processingschemetoconcatenatepointsastemporalfusion,              PointCloudObjectDetection. LiDAR-based3Ddetectors
             which treats the information interaction as separate parts.       localize and classify objects from point clouds, which can
             By contrast, as shown in Figure 1(b), we propose to ex-           be broadly grouped into two categories: point-based and
             plicitly model the mutual correlations between cross-sensor       grid-based. The point-based methods [26,39,40] take raw
             data over time, aiming at the full utilization of misaligned      pointsasinputandapplyPointNet[23]toextractpoint-wise
             complementary information.                                        features and generate proposals for each point. The grid-
                Recent advances in sequential modeling [1,30,34] and           based methods [12, 35, 37, 38, 43, 48] propose to convert
             audio-visual fusion [7,29] demonstrates that Transformer,         point clouds into regular grids as input. PointPillars [12]
             as an emerging powerful architecture, is very competent           typically transfers point clouds into a BEV pseudo image,
             in modeling the information interaction for sequential data       while Voxelization [25,35,48] maps point clouds into reg-
             or cross-modal data. That is mainly because that the mu-          ular 3D voxels. Compared to point-based methods, grid-
             tual relationship can be easily encoded by the intrinsic self-    based methods are computationally efficient, accelerating
             attention module in Transformer. However, it is not feasi-        the training on large-scale datasets such as nuScenes [2] and
             ble to directly apply the standard Transformer architecture       Waymo[28]withstate-of-the-art detection performance. In
             for sensor-time fusion in 3D object detection, owing to two       this work, wefollowPointPillars[12]totransferpointcloud
             facts: 1) The massive amount of 3D points as a sequence in-       into a BEV feature map.
             put is computationally prohibitive for Transformer. 2) The        Temporal Fusion.       A straight-forward temporal fusion
             mutual interaction across sensors and time is beyond the          scheme is to concatenate points from adjacent frames [2,
             scope of Transformer.                                             20,35], which yields denser point representation but with-
                To address the above issues, we present a novel LiDAR          out explicit consideration of temporal correlation. Instead,
             Image Fusion Transformer, short for LIFT, to learn the 4D         some recent approaches [8,24,41,42] make further explo-
             spatiotemporal information fusion across sensor and time.         ration to model the temporal information interaction at the
             Specifically, LIFT contains a grid feature encoder and a          featurelevel, includingobject-centricdesign[24,36,41]and
             sensor-time 4D attention network. In the grid feature en-         scene-centric design [8,18,42]. For the object-centric de-
             coder, we fetch camera features for corresponding points          sign, temporal feature fusion is conducted on top of object
             and conduct pillar feature extraction to project both LiDAR       proposals. This helps to aggregate information efficiently
             points and point-wise camera features into the Bird-Eye-          over a long temporal span but depends on the quality of
             View (BEV) space. By keeping a relatively small number            proposal generation. For the scene-centric design, feature
             of grids, we are able to efficiently compute the inter-grid       fusion is performed based on the whole scene. Fast-and-
             mutualinteractionsandtheintra-gridfine-grainedattention.          Furious [18] uses convolution layers to fuse middle-level
             The grid-wise sensor-time relations naturally reside in 4D        features. Furthermore, recurrent networks [8,42] show im-
             and thus can be encoded by an attention network. In more          provements when modeling temporal correlation.         How-
             detail, we design a 4D positional encoding module to locate       ever, the RNN-based methods are computationally inten-
             the tokens across sensors and time, and further reduce com-       sive given the high dimensional features. In this work, we
             putational overhead by sparse window partition and pyra-          propose a novel Transformer-based module to encode the
             mid context structure with enlarged receptive fields. Ad-         interaction relationships across frames. Compared to early
             ditionally, we equip our detector with a novel sensor-time        works [46], our method explores the spatiotemporal corre-
             consistent data augmentation scheme.                              lation in a unified module. In addition, our network is de-
                Inbrief, our contributions can be summarizedasfollows:         signed with cross-sensor fusion together.
                                                                            17173
