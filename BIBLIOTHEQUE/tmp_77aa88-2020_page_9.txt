                       Published as a conference paper at ICLR 2021
                       maximumeigenvalue (λ    ) at convergence (approximately 24 without SAM, 1.0 with SAM), and
                                            max
                       the bulk of the spectrum (the ratio λmax/λ5, commonly used as a proxy for sharpness (Jastrzebski
                       et al., 2020); up to 11.4 without SAM, and 2.6 with SAM).
                       5   RELATED WORK
                       Theideaofsearchingfor“ﬂat”minimacanbetracedbacktoHochreiter&Schmidhuber(1995),and
                       its connection to generalization has seen signiﬁcant study (Shirish Keskar et al., 2016; Dziugaite &
                       Roy, 2017; Neyshabur et al., 2017; Dinh et al., 2017). In a recent large scale empirical study, Jiang
                       et al. (2019) studied 40 complexity measures and showedthatasharpness-basedmeasurehashighest
                       correlation with generalization, which motivates penalizing sharpness. Hochreiter & Schmidhuber
                       (1997) was perhaps the ﬁrst paper on penalizing the sharpness, regularizing a notion related to Min-
                       imumDescription Length (MDL). Other ideas which also penalize sharp minima include operating
                       on diffused loss landscape (Mobahi, 2016) and regularizing local entropy (Chaudhari et al., 2016).
                       Anotherdirection is to not penalize the sharpness explicitly, but rather average weights during train-
                       ing; Izmailov et al. (2018) showed that doing so can yield ﬂatter minima that can also generalize
                       better. However, the measures of sharpness proposed previously are difﬁcult to compute and differ-
                       entiate through. In contrast, SAM is highly scalable as it only needs two gradient computations per
                       iteration. The concurrent work of Sun et al. (2020) focuses on resilience to random and adversarial
                       corruption to expose a model’s vulnerabilities; this work is perhaps closest to ours. Our work has a
                       different basis: we develop SAM motivated by a principled starting point in generalization, clearly
                       demonstrate SAM’s efﬁcacy via rigorous large-scale empirical evaluation, and surface important
                       practical and theoretical facets of the procedure (e.g., m-sharpness). The notion of all-layer margin
                       introduced by Wei & Ma (2020) is closely related to this work; one is adversarial perturbation over
                       the activations of a network and the other over its weights, and there is some coupling between these
                       two quantities.
                       6   DISCUSSION AND FUTURE WORK
                       In this work, we have introduced SAM, a novel algorithm that improves generalization by simulta-
                       neously minimizing loss value and loss sharpness; we have demonstrated SAM’s efﬁcacy through a
                       rigorous large-scale empirical evaluation. We have surfaced a number of interesting avenues for fu-
                       ture work. Onthetheoretical side, the notion of per-data-point sharpness yielded by m-sharpness (in
                       contrast to global sharpness computed over the entire training set, as has typically been studied in the
                       past) suggests an interesting new lens through which to study generalization. Methodologically, our
                       results suggest that SAM could potentially be used in place of Mixup in robust or semi-supervised
                       methods that currently rely on Mixup (giving, for instance, MentorSAM). We leave to future work
                       a more in-depth investigation of these possibilities.
                       7   ACKNOWLEDGMENTS
                       Wethank our colleagues at Google — Atish Agarwala, Xavier Garcia, Dustin Tran, Yiding Jiang,
                       BasilMustafa,SamyBengio—fortheirfeedbackandinsightfuldiscussions. WealsothanktheJAX
                       andFLAXteamsforgoingaboveandbeyondtosupportourimplementation. WearegratefultoSven
                       Gowal for his help in replicating EfﬁcientNet using JAX, and Justin Gilmer for his implementation
                       of the Lanczos algorithm8 used to generate the Hessian spectra. We thank Niru Maheswaranathan
                       for his matplotlib mastery. We also thank David Samuel for providing a PyTorch implementation of
                       SAM9.
                       REFERENCES
                       Naman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang. Revisiting the gener-
                         alization of adaptive gradient methods, 2020. URL https://openreview.net/forum?
                         id=BJl6t64tvr.
                          8https://github.com/google/spectral-density
                          9https://github.com/davda54/sam
                                                                9
