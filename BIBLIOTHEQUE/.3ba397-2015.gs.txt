                                                                                                                      DeepResidualLearningforImageRecognition
                                                                                                         Kaiming He                                                       Xiangyu Zhang                                                                 Shaoqing Ren                                                            Jian Sun
                                                                                                                                                                                               Microsoft Research
                                                                                                                                                      {kahe, v-xiangz, v-shren, jiansun}@microsoft.com
                                                                                                                Abstract                                                                                                                          20                                                                                            20                                                                               
                                                                                                                                                                                                                                                 )
                                                                                                                                                                                                                                                 %
                                                                                                                                                                                                                                                  (                                                                                                                                                            56-layer
                                                                                                                                                                                                                                                 r
                                                Deeper neural networks are more difﬁcult to train. We                                                                                                                                            o
                                                                                                                                                                                                                                                 r                                                                                            r (%)
                                       present a residual learning framework to ease the training                                                                                                                                                 10                                                                                          rro10                                                            20-layer
                                                                                                                                                                                                                                                 ng er                                                             56-layer                    e
                                                                                                                                                                                                                                                 ni                                                                                           t
                                       of networks that are substantially deeper than those used                                                                                                                                                 ai                                                                                           tes
                                                                                                                                                                                                                                                 r
                                                                                                                                                                                                                                                 t
                                       previously. We explicitly reformulate the layers as learn-                                                                                                                                                                                                                  20-layer
                                                                                                                                                                                                                                                   0                                                                                             0 
                                                                                                                                                                                                                                                     0           1           2           3           4            5           6                   0           1           2           3            4           5           6
                                       ing residual functions with reference to the layer inputs, in-                                                                                                                                                                             iter. (1e4)                                                                                  iter. (1e4)
                                       stead of learning unreferenced functions. We provide com-                                                                                                                                             Figure 1. Training error (left) and test error (right) on CIFAR-10
                                       prehensive empirical evidence showing that these residual                                                                                                                                             with 20-layer and 56-layer “plain” networks. The deeper network
                                       networksareeasiertooptimize,andcangainaccuracyfrom                                                                                                                                                    has higher training error, and thus test error. Similar phenomena
                                       considerably increased depth. On the ImageNet dataset we                                                                                                                                              onImageNetispresented in Fig. 4.
                                       evaluate residual nets with a depth of up to 152 layers—8×                                                                                                                                            greatly beneﬁted from very deep models.
                                       deeper than VGG nets [41] but still having lower complex-
                                       ity. An ensemble of these residual nets achieves 3.57% error                                                                                                                                                   Driven by the signiﬁcance of depth, a question arises: Is
                                       ontheImageNettestset. Thisresultwonthe1stplaceonthe                                                                                                                                                   learning better networks as easy as stacking more layers?
                                       ILSVRC 2015 classiﬁcation task. We also present analysis                                                                                                                                              An obstacle to answering this question was the notorious
                                       onCIFAR-10with100and1000layers.                                                                                                                                                                       problem of vanishing/exploding gradients [1, 9], which
                                                The depth of representations is of central importance                                                                                                                                        hamper convergence from the beginning. This problem,
                                       for many visual recognition tasks. Solely due to our ex-                                                                                                                                              however, has been largely addressed by normalized initial-
                                       tremely deep representations, we obtain a 28% relative im-                                                                                                                                            ization [23, 9, 37, 13] and intermediate normalization layers
                                       provement on the COCO object detection dataset. Deep                                                                                                                                                  [16], which enable networks with tens of layers to start con-
                                       residual nets are foundations of our submissions to ILSVRC                                                                                                                                            verging for stochastic gradient descent (SGD) with back-
                                                                                                                                1                                                                                                            propagation [22].
                                       &COCO2015 competitions , where we also won the 1st
                                       places on the tasks of ImageNet detection, ImageNet local-                                                                                                                                                     When deeper networks are able to start converging, a
                                       ization, COCO detection, and COCO segmentation.                                                                                                                                                       degradation problem has been exposed: with the network
                                                                                                                                                                                                                                             depth increasing, accuracy gets saturated (which might be
                                       1. Introduction                                                                                                                                                                                       unsurprising) and then degrades rapidly.                                                                                                 Unexpectedly,
                         arXiv:1512.03385v1  [cs.CV]  10 Dec 2015                                                                                                                                                                            such degradation is not caused by overﬁtting, and adding
                                                Deep convolutional neural networks [22, 21] have led                                                                                                                                         more layers to a suitably deep model leads to higher train-
                                       to a series of breakthroughs for image classiﬁcation [21,                                                                                                                                             ing error, as reported in [11, 42] and thoroughly veriﬁed by
                                       50, 40]. Deep networks naturally integrate low/mid/high-                                                                                                                                              our experiments. Fig. 1 shows a typical example.
                                       level features [50] and classiﬁers in an end-to-end multi-                                                                                                                                                     Thedegradation (of training accuracy) indicates that not
                                       layer fashion, and the “levels” of features can be enriched                                                                                                                                           all systems are similarly easy to optimize. Let us consider a
                                       by the number of stacked layers (depth). Recent evidence                                                                                                                                              shallower architecture and its deeper counterpart that adds
                                       [41, 44] reveals that network depth is of crucial importance,                                                                                                                                         more layers onto it. There exists a solution by construction
                                       and the leading results [41, 44, 13, 16] on the challenging                                                                                                                                           to the deeper model: the added layers are identity mapping,
                                       ImageNet dataset [36] all exploit “very deep” [41] models,                                                                                                                                            and the other layers are copied from the learned shallower
                                       with a depth of sixteen [41] to thirty [16]. Many other non-                                                                                                                                          model. The existence of this constructed solution indicates
                                       trivial visual recognition tasks [8, 12, 7, 32, 27] have also                                                                                                                                         that a deeper model should produce no higher training error
                                               1http://image-net.org/challenges/LSVRC/2015/                                                                                                                        and                       than its shallower counterpart. But experiments show that
                                       http://mscoco.org/dataset/#detections-challenge2015.                                                                                                                                                  our current solvers on hand are unable to ﬁnd solutions that
                                                                                                                                                                                                                                  1
                                         x                                             ImageNet test set, and won the 1st place in the ILSVRC
                                                                                       2015 classiﬁcation competition. The extremely deep rep-
                                       weight layer                                    resentations also have excellent generalization performance
                            F(x)             relu            x                         on other recognition tasks, and lead us to further win the
                                       weight layer                                    1st places on: ImageNet detection, ImageNet localization,
                                                          identity
                               F(x)+x                                                COCOdetection, and COCO segmentation in ILSVRC &
                                             relu                                      COCO2015competitions. Thisstrongevidenceshowsthat
                         Figure 2. Residual learning: a building block.                the residual learning principle is generic, and we expect that
                                                                                       it is applicable in other vision and non-vision problems.
               are comparably good or better than the constructed solution
               (or unable to do so in feasible time).                                  2. Related Work
                  In this paper, we address the degradation problem by                 Residual Representations. In image recognition, VLAD
               introducing a deep residual learning framework.                In-
               stead of hoping each few stacked layers directly ﬁt a                   [18] is a representation that encodes by the residual vectors
               desired underlying mapping, we explicitly let these lay-                with respect to a dictionary, and Fisher Vector [30] can be
               ers ﬁt a residual mapping. Formally, denoting the desired               formulated as a probabilistic version [18] of VLAD. Both
               underlying mapping as H(x), we let the stacked nonlinear                of them are powerful shallow representations for image re-
               layers ﬁt another mapping of F(x) := H(x)−x. The orig-                  trieval and classiﬁcation [4, 48]. For vector quantization,
               inal mappingisrecastintoF(x)+x. Wehypothesizethatit                     encoding residual vectors [17] is shown to be more effec-
               is easier to optimize the residual mapping than to optimize             tive than encoding original vectors.
               the original, unreferenced mapping. To the extreme, if an                   In low-level vision and computer graphics, for solv-
               identity mapping were optimal, it would be easier to push               ing Partial Differential Equations (PDEs), the widely used
               the residual to zero than to ﬁt an identity mapping by a stack          Multigrid method [3] reformulates the system as subprob-
               of nonlinear layers.                                                    lems at multiple scales, where each subproblem is respon-
                  TheformulationofF(x)+xcanberealizedbyfeedfor-                        sible for the residual solution between a coarser and a ﬁner
               ward neural networks with “shortcut connections” (Fig. 2).              scale. An alternative to Multigrid is hierarchical basis pre-
               Shortcut connections [2, 34, 49] are those skipping one or              conditioning [45, 46], which relies on variables that repre-
               more layers. In our case, the shortcut connections simply               sent residual vectors between two scales. It has been shown
               perform identity mapping, and their outputs are added to                [3, 45, 46] that these solvers converge much faster than stan-
               the outputs of the stacked layers (Fig. 2). Identity short-             dard solvers that are unaware of the residual nature of the
               cut connections add neither extra parameter nor computa-                solutions. These methods suggest that a good reformulation
               tional complexity. The entire network can still be trained              or preconditioning can simplify the optimization.
               end-to-end by SGD with backpropagation, and can be eas-                 Shortcut Connections. Practices and theories that lead to
               ily implemented using common libraries (e.g., Caffe [19])               shortcut connections[2,34,49]havebeenstudiedforalong
               without modifying the solvers.                                          time. An early practice of training multi-layer perceptrons
                  We present comprehensive experiments on ImageNet                     (MLPs)istoaddalinear layer connected from the network
               [36] to show the degradation problem and evaluate our                   input to the output [34, 49]. In [44, 24], a few interme-
               method. Weshowthat: 1) Our extremely deep residual nets                 diate layers are directly connected to auxiliary classiﬁers
               are easy to optimize, but the counterpart “plain” nets (that            for addressing vanishing/exploding gradients. The papers
               simply stack layers) exhibit higher training error when the             of [39, 38, 31, 47] propose methods for centering layer re-
               depth increases; 2) Our deep residual nets can easily enjoy             sponses, gradients, and propagated errors, implemented by
               accuracy gains from greatly increased depth, producing re-              shortcut connections. In [44], an “inception” layer is com-
               sults substantially better than previous networks.                      posed of a shortcut branch and a few deeper branches.
                  Similar phenomena are also shown on the CIFAR-10 set                     Concurrent with our work, “highway networks” [42, 43]
               [20], suggesting that the optimization difﬁculties and the              present shortcut connections with gating functions [15].
               effects of our methodarenotjustakintoaparticulardataset.                These gates are data-dependent and have parameters, in
               Wepresentsuccessfully trained models on this dataset with               contrast to our identity shortcuts that are parameter-free.
               over 100 layers, and explore models with over 1000 layers.              When a gated shortcut is “closed” (approaching zero), the
                  On the ImageNet classiﬁcation dataset [36], we obtain                layers in highway networks represent non-residual func-
               excellent results by extremely deep residual nets. Our 152-             tions.    On the contrary, our formulation always learns
               layer residual net is the deepest network ever presented on             residual functions; our identity shortcuts are never closed,
               ImageNet, while still having lower complexity than VGG                  and all information is always passed through, with addi-
               nets [41].   Our ensemble has 3.57% top-5 error on the                  tional residual functions to be learned. In addition, high-
                                                                                   2
                 way networks have not demonstrated accuracy gains with                            ReLU [29] and the biases are omitted for simplifying no-
                 extremely increased depth (e.g., over 100 layers).                                tations. The operation F + x is performed by a shortcut
                                                                                                   connection and element-wise addition. We adopt the sec-
                 3. Deep Residual Learning                                                         ondnonlinearity after the addition (i.e., σ(y), see Fig. 2).
                 3.1. Residual Learning                                                                TheshortcutconnectionsinEqn.(1)introduceneitherex-
                                                                                                   tra parameter nor computation complexity. This is not only
                    Let us consider H(x) as an underlying mapping to be                            attractive in practice but also important in our comparisons
                 ﬁt by a few stacked layers (not necessarily the entire net),                      between plain and residual networks. We can fairly com-
                 with x denoting the inputs to the ﬁrst of these layers. If one                    pare plain/residual networks that simultaneously have the
                 hypothesizes that multiple nonlinear layers can asymptoti-                        same number of parameters, depth, width, and computa-
                 cally approximate complicated functions2, then it is equiv-                       tionalcost(exceptforthenegligibleelement-wiseaddition).
                 alent to hypothesize that they can asymptotically approxi-                            The dimensions of x and F must be equal in Eqn.(1).
                 mate the residual functions, i.e., H(x) − x (assuming that                        If this is not the case (e.g., when changing the input/output
                 the input and output are of the same dimensions).                       So        channels), we can perform a linear projection Ws by the
                 rather than expect stacked layers to approximate H(x), we                         shortcut connections to match the dimensions:
                 explicitly let these layers approximate a residual function
                 F(x) := H(x) − x. The original function thus becomes                                                    y=F(x,{Wi})+Wsx.                                  (2)
                 F(x)+x. Althoughbothformsshouldbeabletoasymptot-                                  WecanalsouseasquarematrixWs inEqn.(1). Butwewill
                 ically approximate the desired functions (as hypothesized),                       show by experiments that the identity mapping is sufﬁcient
                 the ease of learning might be different.                                          for addressing the degradation problem and is economical,
                    This reformulation is motivated by the counterintuitive                        and thus W is only used when matching dimensions.
                 phenomenaaboutthedegradationproblem(Fig.1,left). As                                              s
                 we discussed in the introduction, if the added layers can                             The form of the residual function F is ﬂexible. Exper-
                 beconstructed as identity mappings, a deeper model should                         iments in this paper involve a function F that has two or
                 have training error no greater than its shallower counter-                        three layers (Fig. 5), while more layers are possible. But if
                 part.   The degradation problem suggests that the solvers                         Fhasonlyasinglelayer,Eqn.(1)issimilartoalinearlayer:
                 might have difﬁculties in approximating identity mappings                         y=W1x+x,forwhichwehavenotobservedadvantages.
                 by multiple nonlinear layers. With the residual learning re-                          Wealsonotethatalthough the above notations are about
                 formulation, if identity mappings are optimal, the solvers                        fully-connected layers for simplicity, they are applicable to
                                                                                                   convolutional layers. The function F(x,{W }) can repre-
                 maysimplydrivetheweightsofthemultiple nonlinear lay-                                                                                        i
                 ers toward zero to approach identity mappings.                                    sent multiple convolutional layers. The element-wise addi-
                    In real cases, it is unlikely that identity mappings are op-                   tion is performed on two feature maps, channel by channel.
                 timal, but our reformulation may help to precondition the                         3.3. Network Architectures
                 problem. If the optimal function is closer to an identity
                 mapping than to a zero mapping, it should be easier for the                           Wehavetested various plain/residual nets, and have ob-
                 solver to ﬁnd the perturbations with reference to an identity                     served consistent phenomena. To provide instances for dis-
                 mapping, than to learn the function as a new one. We show                         cussion, we describe two models for ImageNet as follows.
                 byexperiments(Fig.7)thatthelearnedresidualfunctionsin                             Plain Network. Our plain baselines (Fig. 3, middle) are
                 general have small responses, suggesting that identity map-                       mainlyinspiredbythephilosophyofVGGnets[41](Fig.3,
                 pings provide reasonable preconditioning.                                         left). The convolutional layers mostly have 3×3 ﬁlters and
                 3.2. Identity Mapping by Shortcuts                                                follow two simple design rules: (i) for the same output
                    Weadopt residual learning to every few stacked layers.                         feature map size, the layers have the same number of ﬁl-
                 Abuilding block is shown in Fig. 2. Formally, in this paper                       ters; and (ii) if the feature map size is halved, the num-
                 weconsider a building block deﬁned as:                                            ber of ﬁlters is doubled so as to preserve the time com-
                                                                                                   plexity per layer. We perform downsampling directly by
                                        y=F(x,{W})+x.                                   (1)        convolutional layers that have a stride of 2. The network
                                                          i                                        ends with a global average pooling layer and a 1000-way
                 Here x and y are the input and output vectors of the lay-                         fully-connected layer with softmax. The total number of
                 ers considered. The function F(x,{W }) represents the                             weighted layers is 34 in Fig. 3 (middle).
                                                                     i                                 It is worth noticing that our model has fewer ﬁlters and
                 residual mapping to be learned. For the example in Fig. 2                         lower complexity than VGGnets[41](Fig.3, left). Our 34-
                 that has two layers, F = W σ(W x) in which σ denotes
                                                      2      1                                     layer baseline has 3.6 billion FLOPs (multiply-adds), which
                    2This hypothesis, however, is still an open question. See [28].                is only 18% of VGG-19 (19.6 billion FLOPs).
                                                                                               3
                                    VGG-19              34-layer plain                   34-layer residual                       Residual Network. Based on the above plain network, we
                                       image                        image                        image                           insert shortcut connections (Fig. 3, right) which turn the
                      output                                                                                                     network into its counterpart residual version. The identity
                                    3x3 conv, 64
                      size: 224                                                                                                  shortcuts (Eqn.(1)) can be directly used when the input and
                                    3x3 conv, 64
                                                                                                                                 output are of the same dimensions (solid line shortcuts in
                                      pool, /2
                      output                                                                                                     Fig.3). Whenthedimensionsincrease(dottedlineshortcuts
                      size: 112
                                    3x3 conv, 128                                                                                in Fig. 3), we consider two options: (A) The shortcut still
                                    3x3 conv, 128               7x7 conv, 64, /2             7x7 conv, 64, /2                    performs identity mapping, with extra zero entries padded
                                      pool, /2                      pool, /2                     pool, /2
                      output                                                                                                     for increasing dimensions. This option introduces no extra
                      size: 56
                                    3x3 conv, 256                 3x3 conv, 64                 3x3 conv, 64                      parameter; (B) The projection shortcut in Eqn.(2) is used to
                                    3x3 conv, 256                 3x3 conv, 64                 3x3 conv, 64                      match dimensions (done by 1×1 convolutions). For both
                                    3x3 conv, 256                 3x3 conv, 64                 3x3 conv, 64                      options, when the shortcuts go across feature maps of two
                                    3x3 conv, 256                 3x3 conv, 64                 3x3 conv, 64                      sizes, they are performed with a stride of 2.
                                                                  3x3 conv, 64                 3x3 conv, 64                      3.4. Implementation
                                                                  3x3 conv, 64                 3x3 conv, 64
                                      pool, /2                  3x3 conv, 128, /2            3x3 conv, 128, /2                        Our implementation for ImageNet follows the practice
                      output 
                      size: 28
                                    3x3 conv, 512                3x3 conv, 128                3x3 conv, 128                      in [21, 41]. The image is resized with its shorter side ran-
                                    3x3 conv, 512                3x3 conv, 128                3x3 conv, 128                      domly sampled in [256,480] for scale augmentation [41].
                                    3x3 conv, 512                3x3 conv, 128                3x3 conv, 128                      A224×224cropisrandomlysampledfromanimageorits
                                    3x3 conv, 512                3x3 conv, 128                3x3 conv, 128                      horizontal ﬂip, with the per-pixel mean subtracted [21]. The
                                                                 3x3 conv, 128                3x3 conv, 128                      standardcoloraugmentationin[21]isused. Weadoptbatch
                                                                                                                                 normalization (BN) [16] right after each convolution and
                                                                 3x3 conv, 128                3x3 conv, 128
                                                                                                                                 before activation, following [16]. We initialize the weights
                                                                 3x3 conv, 128                3x3 conv, 128
                      output                                                                                                     as in [13] and train all plain/residual nets from scratch. We
                                      pool, /2                  3x3 conv, 256, /2            3x3 conv, 256, /2
                      size: 14                                                                                                   use SGD with a mini-batch size of 256. The learning rate
                                    3x3 conv, 512                3x3 conv, 256                3x3 conv, 256                      starts from 0.1 and is divided by 10 when the error plateaus,
                                    3x3 conv, 512                3x3 conv, 256                3x3 conv, 256                      andthemodelsaretrainedforupto60×104 iterations. We
                                    3x3 conv, 512                3x3 conv, 256                3x3 conv, 256                      use a weight decay of 0.0001 and a momentum of 0.9. We
                                    3x3 conv, 512                3x3 conv, 256                3x3 conv, 256                      donotusedropout [14], following the practice in [16].
                                                                 3x3 conv, 256                3x3 conv, 256                           In testing, for comparison studies we adopt the standard
                                                                 3x3 conv, 256                3x3 conv, 256                      10-crop testing [21]. For best results, we adopt the fully-
                                                                 3x3 conv, 256                3x3 conv, 256                      convolutional form as in [41, 13], and average the scores
                                                                 3x3 conv, 256                3x3 conv, 256                      at multiple scales (images are resized such that the shorter
                                                                 3x3 conv, 256                3x3 conv, 256                      side is in {224,256,384,480,640}).
                                                                 3x3 conv, 256                3x3 conv, 256
                                                                 3x3 conv, 256                3x3 conv, 256                      4. Experiments
                      output 
                                      pool, /2                  3x3 conv, 512, /2            3x3 conv, 512, /2
                       size: 7                                                                                                   4.1. ImageNet Classiﬁcation
                                                                 3x3 conv, 512                3x3 conv, 512
                                                                 3x3 conv, 512                3x3 conv, 512                           Weevaluate our method on the ImageNet 2012 classiﬁ-
                                                                                                                                 cationdataset[36]thatconsistsof1000classes. Themodels
                                                                 3x3 conv, 512                3x3 conv, 512
                                                                                                                                 are trained on the 1.28 million training images, and evalu-
                                                                 3x3 conv, 512                3x3 conv, 512                      ated on the 50k validation images. We also obtain a ﬁnal
                                                                 3x3 conv, 512                3x3 conv, 512                      result on the 100k test images, reported by the test server.
                      output 
                                      fc 4096                      avg pool                     avg pool
                       size: 1                                                                                                   Weevaluate both top-1 and top-5 error rates.
                                      fc 4096                       fc 1000                      fc 1000
                                      fc 1000                                                                                    Plain Networks. We ﬁrst evaluate 18-layer and 34-layer
                                                                                                                                 plain nets. The 34-layer plain net is in Fig. 3 (middle). The
                     Figure 3. Example network architectures for ImageNet. Left: the                                             18-layer plain net is of a similar form. See Table 1 for de-
                     VGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid-                                                 tailed architectures.
                     dle: a plain network with 34 parameter layers (3.6 billion FLOPs).                                               TheresultsinTable2showthatthedeeper34-layerplain
                     Right: a residual network with 34 parameter layers (3.6 billion                                             net has higher validation error than the shallower 18-layer
                     FLOPs). Thedottedshortcutsincreasedimensions. Table1shows                                                   plain net. To reveal the reasons, in Fig. 4 (left) we com-
                     moredetails and other variants.                                                                             pare their training/validation errors during the training pro-
                                                                                                                                 cedure. We have observed the degradation problem - the
                                                                                                                           4
                                                     layer name output size          18-layer             34-layer              50-layer                101-layer               152-layer
                                                        conv1      112×112                                                     7×7,64,stride 2
                                                                                                                            3×3maxpool,stride 2
                                                      conv2 x       56×56        3×3,64             3×3,64              1×1,64                1×1,64                1×1,64 
                                                                                                                                                                                     
                                                                                   3×3,64 ×2            3×3,64 ×3             3×3,64       ×3         3×3,64       ×3          3×3,64      ×3
                                                                                                                            1×1,256               1×1,256               1×1,256 
                                                      conv3 x       28×28        3×3,128            3×3,128             1×1,128               1×1,128               1×1,128 
                                                                                  3×3,128 ×2            3×3,128 ×4            3×3,128      ×4         3×3,128      ×4         3×3,128      ×8
                                                                                                                           1×1,512              1×1,512                1×1,512 
                                                      conv4 x       14×14        3×3,256            3×3,256            1×1,256              1×1,256                1×1,256 
                                                                                  3×3,256 ×2            3×3,256 ×6            3×3,256       ×6       3×3,256       ×23        3×3,256      ×36
                                                                                                                           1×1,1024             1×1,1024               1×1,1024 
                                                      conv5 x         7×7        3×3,512            3×3,512            1×1,512               1×1,512                1×1,512 
                                                                                  3×3,512 ×2            3×3,512 ×3            3×3,512       ×3        3×3,512       ×3        3×3,512       ×3
                                                                                                                             1×1,2048                1×1,2048                 1×1,2048
                                                                      1×1                                             average pool, 1000-d fc, softmax
                                                              FLOPs                  1.8×109              3.6×109               3.8×109                 7.6×109                 11.3×109
                     Table 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down-
                     sampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.
                                                                                                                                                                                                                  
                                       60                                                                                          60
                                       50                                                                                          50
                                      )                                                                                           )
                                       (%                                                                                          (%
                                      r40                                                                                         r40
                                      ro                                                                                          ro
                                      er                                                                                          er
                                                                                                        34-layer
                                                                                                                                                                                                    18-layer
                                       30                                                                                          30
                                                plain-18                                                18-layer                            ResNet-18
                                                plain-34                                                                                    ResNet-34                                               34-layer
                                       20                                                                                          20 
                                          0            10           20            30           40            50                       0            10           20            30           40            50
                                                                          iter. (1e4)                                                                                 iter. (1e4)
                     Figure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain
                     networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to
                     their plain counterparts.
                                                                   plain              ResNet                                   reducing of the training error3. The reason for such opti-
                                          18layers                 27.94               27.88                                   mization difﬁculties will be studied in the future.
                                          34layers                 28.54               25.03                                   Residual Networks. Next we evaluate 18-layer and 34-
                     Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation.                                         layer residual nets (ResNets). The baseline architectures
                     Here the ResNets have no extra parameter compared to their plain                                          are the same as the above plain nets, expect that a shortcut
                     counterparts. Fig. 4 shows the training procedures.                                                       connection is added to each pair of 3×3 ﬁlters as in Fig. 3
                                                                                                                               (right). In the ﬁrst comparison (Table 2 and Fig. 4 right),
                                                                                                                               weuseidentity mapping for all shortcuts and zero-padding
                     34-layer plain net has higher training error throughout the                                               for increasing dimensions (option A). So they have no extra
                     whole training procedure, even though the solution space                                                  parameter compared to the plain counterparts.
                     of the 18-layer plain network is a subspace of that of the                                                     We have three major observations from Table 2 and
                     34-layer one.                                                                                             Fig. 4. First, the situation is reversed with residual learn-
                          We argue that this optimization difﬁculty is unlikely to                                             ing – the 34-layer ResNet is better than the 18-layer ResNet
                     be caused by vanishing gradients. These plain networks are                                                (by 2.8%). More importantly, the 34-layer ResNet exhibits
                     trained with BN [16], which ensures forward propagated                                                    considerably lower training error and is generalizable to the
                     signals to have non-zero variances. We also verify that the                                               validation data. This indicates that the degradation problem
                     backward propagated gradients exhibit healthy norms with                                                  is well addressed in this setting and we manage to obtain
                     BN. So neither forward nor backward signals vanish. In                                                    accuracy gains from increased depth.
                     fact, the 34-layer plain net is still able to achieve compet-                                                  Second, compared to its plain counterpart, the 34-layer
                     itive accuracy (Table 3), suggesting that the solver works                                                     3Wehaveexperimentedwithmoretrainingiterations(3×)andstillob-
                     to some extent. We conjecture that the deep plain nets may                                                served the degradation problem, suggesting that this problem cannot be
                     haveexponentiallylowconvergencerates,whichimpactthe                                                       feasibly addressed by simply using more iterations.
                                                                                                                          5
                                     model                     top-1 err.       top-5 err.                                                64-d                                      256-d
                                     VGG-16[41]                 28.07             9.33                                                                                           1x1, 64
                                                                                                                                    3x3, 64
                                                                                                                                                                                      relu
                                     GoogLeNet[44]                  -             9.15                                                   relu
                                                                                                                                                                                 3x3, 64
                                                                                                                                                                                      relu
                                     PReLU-net[13]              24.27             7.38                                              3x3, 64
                                                                                                                                                                                 1x1, 256
                                     plain-34                   28.54            10.02
                                     ResNet-34 A                25.03             7.76                                                   relu                                         relu
                                     ResNet-34 B                24.52             7.46                               Figure 5. A deeper residual function F for ImageNet. Left: a
                                     ResNet-34 C                24.19             7.40                               building block (on 56×56 feature maps) as in Fig. 3 for ResNet-
                                     ResNet-50                  22.85             6.71                               34. Right: a “bottleneck” building block for ResNet-50/101/152.
                                     ResNet-101                 21.75             6.05
                                     ResNet-152                 21.43             5.71                               parameter-free, identity shortcuts help with training. Next
                   Table 3. Error rates (%, 10-crop testing) on ImageNet validation.                                 weinvestigate projection shortcuts (Eqn.(2)). In Table 3 we
                   VGG-16isbased on our test. ResNet-50/101/152 are of option B                                      comparethreeoptions: (A) zero-padding shortcuts are used
                   that only uses projections for increasing dimensions.                                             for increasing dimensions, and all shortcuts are parameter-
                          method                                          top-1 err.       top-5 err.                free (the same as Table 2 and Fig. 4 right); (B) projec-
                                                                                                  †                  tion shortcuts are used for increasing dimensions, and other
                          VGG[41](ILSVRC’14)                                   -             8.43                    shortcuts are identity; and (C) all shortcuts are projections.
                          GoogLeNet[44](ILSVRC’14)                             -             7.89
                          VGG[41](v5)                                       24.4              7.1                        Table 3 shows that all three options are considerably bet-
                          PReLU-net[13]                                    21.59             5.71                    ter than the plain counterpart. B is slightly better than A. We
                          BN-inception [16]                                21.99             5.81                    argue that this is because the zero-padded dimensions in A
                          ResNet-34 B                                      21.84             5.71                    indeedhavenoresiduallearning. Cismarginallybetterthan
                          ResNet-34 C                                      21.53             5.60                    B, and we attribute this to the extra parameters introduced
                          ResNet-50                                        20.74             5.25                    by many (thirteen) projection shortcuts. But the small dif-
                          ResNet-101                                       19.87             4.60                    ferences amongA/B/Cindicatethatprojectionshortcutsare
                          ResNet-152                                       19.38             4.49                    not essential for addressing the degradation problem. So we
                                                                                                                     donotuseoptionCintherestofthispaper,toreducemem-
                   Table 4. Error rates (%) of single-model results on the ImageNet                                  ory/time complexity and model sizes. Identity shortcuts are
                   validation set (except † reported on the test set).                                               particularly important for not increasing the complexity of
                            method                                             top-5 err. (test)                     the bottleneck architectures that are introduced below.
                            VGG[41](ILSVRC’14)                                        7.32                           Deeper Bottleneck Architectures. Next we describe our
                            GoogLeNet[44](ILSVRC’14)                                  6.66                           deepernetsforImageNet. Becauseofconcernsonthetrain-
                            VGG[41](v5)                                                6.8                           ing time that we can afford, we modify the building block
                                                                                                                                                      4
                            PReLU-net[13]                                             4.94                           as a bottleneck design . For each residual function F, we
                            BN-inception [16]                                         4.82                           use a stack of 3 layers instead of 2 (Fig. 5). The three layers
                            ResNet(ILSVRC’15)                                         3.57                           are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers
                   Table 5. Error rates (%) of ensembles. The top-5 error is on the                                  are responsible for reducing and then increasing (restoring)
                   test set of ImageNet and reported by the test server.                                             dimensions,leavingthe3×3layerabottleneckwithsmaller
                                                                                                                     input/output dimensions. Fig. 5 shows an example, where
                                                                                                                     both designs have similar time complexity.
                   ResNet reduces the top-1 error by 3.5% (Table 2), resulting                                           Theparameter-freeidentityshortcutsareparticularlyim-
                   fromthesuccessfully reduced training error (Fig. 4 right vs.                                      portantforthebottleneckarchitectures. Iftheidentityshort-
                   left). This comparison veriﬁes the effectiveness of residual                                      cut in Fig. 5 (right) is replaced with projection, one can
                   learning on extremely deep systems.                                                               show that the time complexity and model size are doubled,
                        Last, we also note that the 18-layer plain/residual nets                                     as the shortcut is connected to the two high-dimensional
                   are comparably accurate (Table 2), but the 18-layer ResNet                                        ends. So identity shortcuts lead to more efﬁcient models
                   converges faster (Fig. 4 right vs. left). When the net is “not                                    for the bottleneck designs.
                   overly deep” (18 layers here), the current SGD solver is still                                        50-layer ResNet: We replace each 2-layer block in the
                   able to ﬁnd good solutions to the plain net. In this case, the                                        4Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy
                   ResNet eases the optimization by providing faster conver-                                         from increased depth (as shown on CIFAR-10), but are not as economical
                   gence at the early stage.                                                                         asthebottleneckResNets. Sotheusageofbottleneckdesignsismainlydue
                                                                                                                     to practical considerations. We further note that the degradation problem
                   Identity vs. Projection Shortcuts. We have shown that                                             of plain nets is also witnessed for the bottleneck designs.
                                                                                                               6
              34-layer net with this 3-layer bottleneck block, resulting in                        method                    error (%)
              a50-layerResNet(Table1). WeuseoptionBforincreasing                                Maxout[10]                   9.38
              dimensions. This model has 3.8 billion FLOPs.                                       NIN[25]                    8.81
                 101-layer and 152-layer ResNets: We construct 101-                               DSN[24]                    8.22
              layer and 152-layer ResNets by using more 3-layer blocks                                 #layers   #params
              (Table 1). Remarkably, although the depth is signiﬁcantly               FitNet [35]        19        2.5M      8.39
              increased, the 152-layer ResNet (11.3 billion FLOPs) still           Highway[42, 43]       19        2.3M      7.54 (7.72±0.16)
              has lower complexity than VGG-16/19 nets (15.3/19.6 bil-             Highway[42, 43]       32       1.25M      8.80
              lion FLOPs).                                                              ResNet           20       0.27M      8.75
                 The 50/101/152-layer ResNets are more accurate than                    ResNet           32       0.46M      7.51
              the 34-layer ones by considerable margins (Table 3 and 4).                ResNet           44       0.66M      7.17
              We do not observe the degradation problem and thus en-                    ResNet           56       0.85M      6.97
              joy signiﬁcant accuracy gains from considerably increased                 ResNet           110       1.7M      6.43 (6.61±0.16)
              depth. The beneﬁts of depth are witnessed for all evaluation              ResNet          1202      19.4M      7.93
              metrics (Table 3 and 4).
              Comparisons with State-of-the-art Methods. In Table 4              Table 6. Classiﬁcation error on the CIFAR-10 test set. All meth-
                                                                                 odsarewithdataaugmentation. ForResNet-110,werunit5times
              we compare with the previous best single-model results.            and show “best (mean±std)” as in [43].
              Ourbaseline 34-layer ResNets have achieved very compet-
              itive accuracy. Our 152-layer ResNet has a single-model            so our residual models have exactly the same depth, width,
              top-5 validation error of 4.49%. This single-model result          and number of parameters as the plain counterparts.
              outperforms all previous ensemble results (Table 5). We               Weuseaweightdecayof0.0001andmomentumof0.9,
              combine six models of different depth to form an ensemble          and adopt the weight initialization in [13] and BN [16] but
              (only with two 152-layer ones at the time of submitting).          with no dropout. These models are trained with a mini-
              This leads to 3.57% top-5 error on the test set (Table 5).         batch size of 128 on two GPUs. We start with a learning
              This entry won the 1st place in ILSVRC 2015.                       rate of 0.1, divide it by 10 at 32k and 48k iterations, and
              4.2. CIFAR-10 and Analysis                                         terminate training at 64k iterations, which is determined on
                 We conducted more studies on the CIFAR-10 dataset               a 45k/5k train/val split. We follow the simple data augmen-
              [20], which consists of 50k training images and 10k test-          tation in [24] for training: 4 pixels are padded on each side,
              ing images in 10 classes. We present experiments trained           and a 32×32 crop is randomly sampled from the padded
              on the training set and evaluated on the test set. Our focus       image or its horizontal ﬂip. For testing, we only evaluate
              is on the behaviors of extremely deep networks, but not on         the single view of the original 32×32 image.
              pushing the state-of-the-art results, so we intentionally use         Wecomparen = {3,5,7,9}, leading to 20, 32, 44, and
              simple architectures as follows.                                   56-layer networks. Fig. 6 (left) shows the behaviors of the
                 Theplain/residual architectures follow the form in Fig. 3       plain nets. The deep plain nets suffer from increased depth,
              (middle/right). The network inputs are 32×32 images, with          and exhibit higher training error when going deeper. This
              the per-pixel mean subtracted. The ﬁrst layer is 3×3 convo-        phenomenonissimilartothatonImageNet(Fig.4,left)and
              lutions. Then we use a stack of 6n layers with 3×3 convo-          on MNIST(see [42]), suggesting that such an optimization
              lutions on the feature maps of sizes {32,16,8} respectively,       difﬁculty is a fundamental problem.
              with 2n layers for each feature map size. The numbers of              Fig. 6 (middle) shows the behaviors of ResNets. Also
              ﬁlters are {16,32,64}respectively. Thesubsamplingisper-            similar to the ImageNet cases (Fig. 4, right), our ResNets
              formedbyconvolutionswithastrideof2. Thenetworkends                 managetoovercometheoptimizationdifﬁcultyanddemon-
              with a global average pooling, a 10-way fully-connected            strate accuracy gains when the depth increases.
              layer, and softmax. Therearetotally6n+2stackedweighted                We further explore n = 18 that leads to a 110-layer
              layers. The following table summarizes the architecture:           ResNet. In this case, we ﬁnd that the initial learning rate
                                                                                                                                5
                                                                                 of 0.1 is slightly too large to start converging . So we use
                      output map size    32×32      16×16     8×8                0.01towarmupthetraininguntilthetrainingerrorisbelow
                          # layers        1+2n        2n       2n                80%(about400iterations),andthengobackto0.1andcon-
                          # ﬁlters         16         32       64                tinue training. The rest of the learning schedule is as done
                                                                                 previously. This 110-layer network converges well (Fig. 6,
              When shortcut connections are used, they are connected             middle). It has fewer parameters than other deep and thin
              to the pairs of 3×3 layers (totally 3n shortcuts). On this            5With an initial learning rate of 0.1, it starts converging (<90% error)
              dataset we use identity shortcuts in all cases (i.e., option A),   after several epochs, but still reaches similar accuracy.
                                                                             7
                                                            20                                                                                          20                                                                                     20                                      
                                                                                                                                                                                                                         ResNet-20                                      residual-110
                                                                                                                                                                                                                         ResNet-32                                      residual-1202
                                                                                                                                                                                                                         ResNet-44
                                                                                                                                                                                                                         ResNet-56
                                                                                                                              56-layer                                                                                   ResNet-110
                                                           )                                                                                           )                                                                                     )
                                                            (%10
                                                           r                                                                                           r (%10                                                                                r (%10
                                                           ro                                                                                          ro                                                                20-layer            ro
                                                           er                                                                 20-layer                 r                                                                                     r
                                                                                                                                                       e                                                                                     e
                                                              5      plain-20                                                                            5                                                               110-layer              5
                                                                     plain-32
                                                                     plain-44
                                                                     plain-56                                                                                                                                                                   1
                                                              0                                                                                          0                                                                                      0 
                                                               0          1           2           3          4           5           6                     0          1           2           3          4           5           6                     4             5            6
                                                                                               iter. (1e4)                                                                                 iter. (1e4)                                                        iter. (1e4)
                             Figure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error
                             of plain-110 is higher than 60% and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202 layers.
                                                                                                                                                           
                                       3                                                                                               plain-20                                                   training data                           07+12                            07++12
                                                                                                                                       plain-56
                                      d                                                                                                ResNet-20                                                      test data                     VOC07test                         VOC12test
                                      st2                                                                                              ResNet-56
                                                                                                                                       ResNet-110                                                     VGG-16                                73.2                              70.4
                                       1                                                                                                                                                          ResNet-101                                76.4                              73.8
                                          
                                        0                   20                   40                  60                   80                  100
                                                                                   layer index (original)                                                                     Table 7. Object detection mAP (%) on the PASCAL VOC
                                                                                                                                                           
                                       3                                                                                               plain-20                               2007/2012 test sets using baseline Faster R-CNN. See also Ta-
                                                                                                                                       plain-56
                                      d                                                                                                ResNet-20                              ble 10 and 11 for better results.
                                      st2                                                                                              ResNet-56
                                                                                                                                       ResNet-110
                                       1                                                                                                                                                                metric                      mAP@.5                      mAP@[.5,.95]
                                                                                                                                                                                                     VGG-16                              41.5                              21.2
                                        0                   20                   40                  60                   80                  100
                                                                          layer index (sorted by magnitude)                                                                                       ResNet-101                             48.4                              27.2
                             Figure 7. Standard deviations (std) of layer responses on CIFAR-
                             10. The responses are the outputs of each 3×3 layer, after BN and                                                                                Table 8. Object detection mAP (%) on the COCO validation set
                             before nonlinearity. Top: the layers are shown in their original                                                                                 using baseline Faster R-CNN. See also Table 9 for better results.
                             order. Bottom: the responses are ranked in descending order.
                                                                                                                                                                              have similar training error. We argue that this is because of
                             networks such as FitNet [35] and Highway [42] (Table 6),                                                                                         overﬁtting. The 1202-layer network may be unnecessarily
                             yet is among the state-of-the-art results (6.43%, Table 6).                                                                                      large (19.4M) for this small dataset. Strong regularization
                             Analysis of Layer Responses. Fig. 7 shows the standard                                                                                           such as maxout [10] or dropout [14] is applied to obtain the
                             deviations (std) of the layer responses. The responses are                                                                                       best results ([10, 25, 24, 35]) on this dataset. In this paper,
                             the outputs of each 3×3 layer, after BN and before other                                                                                         weusenomaxout/dropout and just simply impose regular-
                             nonlinearity (ReLU/addition).                                             For ResNets, this analy-                                               ization via deep and thin architectures by design, without
                             sis reveals the response strength of the residual functions.                                                                                     distracting from the focus on the difﬁculties of optimiza-
                             Fig. 7 shows that ResNets have generally smaller responses                                                                                       tion. But combining with stronger regularization may im-
                             than their plain counterparts. These results support our ba-                                                                                     prove results, which we will study in the future.
                             sic motivation (Sec.3.1) that the residual functions might                                                                                       4.3. Object Detection on PASCAL and MS COCO
                             be generally closer to zero than the non-residual functions.                                                                                            Our method has good generalization performance on
                             Wealso notice that the deeper ResNet has smaller magni-                                                                                          other recognition tasks. Table 7 and 8 show the object de-
                             tudes of responses, as evidenced by the comparisons among                                                                                        tection baseline results on PASCAL VOC 2007 and 2012
                             ResNet-20, 56, and 110 in Fig. 7. When there are more                                                                                            [5] and COCO[26]. WeadoptFasterR-CNN [32]asthede-
                             layers, an individual layer of ResNets tends to modify the                                                                                       tection method. Here we are interested in the improvements
                             signal less.                                                                                                                                     of replacing VGG-16 [41] with ResNet-101. The detection
                             Exploring Over 1000 layers. We explore an aggressively                                                                                           implementation (see appendix) of using both models is the
                             deep model of over 1000 layers. We set n = 200 that                                                                                              same, so the gains can only be attributed to better networks.
                             leads to a 1202-layer network, which is trained as described                                                                                     Mostremarkably, on the challenging COCO dataset we ob-
                             above. Our method shows no optimization difﬁculty, and                                                                                           tain a 6.0%increaseinCOCO’sstandardmetric(mAP@[.5,
                                             3                                                                                                                                .95]), which is a 28% relative improvement. This gain is
                             this 10 -layer network is able to achieve training error
                             <0.1% (Fig. 6, right).                                    Its test error is still fairly good                                                    solely due to the learned representations.
                             (7.93%, Table 6).                                                                                                                                       Based on deep residual nets, we won the 1st places in
                                   But there are still open problems on such aggressively                                                                                     several tracks in ILSVRC&COCO2015competitions: Im-
                             deep models. The testing result of this 1202-layer network                                                                                       ageNet detection, ImageNet localization, COCO detection,
                             is worse than that of our 110-layer network, although both                                                                                       and COCOsegmentation. The details are in the appendix.
                                                                                                                                                                      8
                                                                                                              ´
                References                                                                      [28] G. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of
                 [1] Y.Bengio,P.Simard,andP.Frasconi. Learninglong-termdependen-                      linear regions of deep neural networks. In NIPS, 2014.
                      cies with gradient descent is difﬁcult. IEEE Transactions on Neural       [29] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted
                      Networks, 5(2):157–166, 1994.                                                   boltzmann machines. In ICML, 2010.
                 [2] C. M. Bishop. Neural networks for pattern recognition. Oxford              [30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for
                      university press, 1995.                                                         image categorization. In CVPR, 2007.
                 [3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam,          [31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by
                      2000.                                                                           linear transformations in perceptrons. In AISTATS, 2012.
                 [4] K. Chatﬁeld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil         [32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards
                      is in the details: an evaluation of recent feature encoding methods.            real-time object detection with region proposal networks. In NIPS,
                      In BMVC,2011.                                                                   2015.
                 [5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-           [33] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection
                      serman. The Pascal Visual Object Classes (VOC) Challenge. IJCV,                 networks on convolutional feature maps. arXiv:1504.06066, 2015.
                      pages 303–338, 2010.                                                      [34] B. D. Ripley. Pattern recognition and neural networks. Cambridge
                 [6] S. Gidaris and N. Komodakis. Object detection via a multi-region &               university press, 1996.
                      semantic segmentation-aware cnn model. In ICCV, 2015.                     [35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and
                 [7] R. Girshick. Fast R-CNN. In ICCV, 2015.                                          Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.
                 [8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-      [36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
                      archies for accurate object detection and semantic segmentation. In             Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet
                      CVPR,2014.                                                                      large scale visual recognition challenge. arXiv:1409.0575, 2014.
                 [9] X. Glorot and Y. Bengio. Understanding the difﬁculty of training           [37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to
                      deep feedforward neural networks. In AISTATS, 2010.                             the nonlinear dynamics of learning in deep linear neural networks.
                [10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and                   arXiv:1312.6120, 2013.
                      Y. Bengio. Maxout networks. arXiv:1302.4389, 2013.                        [38] N.N.Schraudolph. Accelerated gradient descent by factor-centering
                [11] K.HeandJ.Sun. Convolutionalneuralnetworksatconstrainedtime                       decomposition. Technical report, 1998.
                      cost. In CVPR, 2015.                                                      [39] N. N. Schraudolph. Centering neural network gradient factors. In
                [12] K.He,X.Zhang,S.Ren,andJ.Sun. Spatialpyramidpoolingindeep                         Neural Networks: Tricks of the Trade, pages 207–226. Springer,
                      convolutional networks for visual recognition. In ECCV, 2014.                   1998.
                [13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers:          [40] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-
                      Surpassing human-level performance on imagenet classiﬁcation. In                Cun. Overfeat: Integrated recognition, localization and detection
                      ICCV,2015.                                                                      using convolutional networks. In ICLR, 2014.
                [14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and              [41] K. Simonyan and A. Zisserman. Very deep convolutional networks
                      R. R. Salakhutdinov. Improving neural networks by preventing co-                for large-scale image recognition. In ICLR, 2015.
                      adaptation of feature detectors. arXiv:1207.0580, 2012.                   [42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.
                [15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural                 arXiv:1505.00387, 2015.
                      computation, 9(8):1735–1780, 1997.                                        [43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep
                [16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep                  networks. 1507.06228, 2015.
                      networktrainingbyreducinginternalcovariateshift. In ICML, 2015.           [44] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Er-
                [17] H.Jegou,M.Douze,andC.Schmid. Productquantizationfornearest                       han, V. Vanhoucke, and A. Rabinovich. Going deeper with convolu-
                      neighbor search. TPAMI, 33, 2011.                                               tions. In CVPR, 2015.
                [18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and                [45] R. Szeliski. Fast surface interpolation using hierarchical basis func-
                      C.Schmid. Aggregatinglocalimagedescriptorsintocompactcodes.                     tions. TPAMI, 1990.
                      TPAMI,2012.                                                               [46] R. Szeliski. Locally adapted hierarchical basis preconditioning. In
                [19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,              SIGGRAPH,2006.
                      S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for      [47] T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochas-
                      fast feature embedding. arXiv:1408.5093, 2014.                                  tic gradient towards second-order methods–backpropagation learn-
                [20] A. Krizhevsky. Learning multiple layers of features from tiny im-                ing with transformations in nonlinearities. In Neural Information
                      ages. Tech Report, 2009.                                                        Processing, 2013.
                [21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation         [48] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library
                      with deep convolutional neural networks. In NIPS, 2012.                         of computer vision algorithms, 2008.
                [22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,              [49] W. Venables and B. Ripley. Modern applied statistics with s-plus.
                      W. Hubbard, and L. D. Jackel. Backpropagation applied to hand-                  1999.
                      written zip code recognition. Neural computation, 1989.                   [50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-
                                                                 ¨                                    tional neural networks. In ECCV, 2014.
                [23] Y.LeCun,L.Bottou,G.B.Orr,andK.-R.Muller. Efﬁcientbackprop.
                      In Neural Networks: Tricks of the Trade, pages 9–50. Springer, 1998.
                [24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu.       Deeply-
                      supervised nets. arXiv:1409.5185, 2014.
                [25] M.Lin,Q.Chen,andS.Yan. Networkinnetwork. arXiv:1312.4400,
                      2013.
                [26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
                             ´
                      P. Dollar, and C. L. Zitnick. Microsoft COCO: Common objects in
                      context. In ECCV. 2014.
                [27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks
                      for semantic segmentation. In CVPR, 2015.
                                                                                            9
             A. Object Detection Baselines                                   8 images (i.e., 1 per GPU) and the Fast R-CNN step has a
                In this section we introduce our detection method based      mini-batch size of 16 images. The RPN step and Fast R-
             on the baseline Faster R-CNN [32] system. The models are        CNNstepare both trained for 240k iterations with a learn-
             initialized by the ImageNet classiﬁcation models, and then      ing rate of 0.001 and then for 80k iterations with 0.0001.
             ﬁne-tuned on the object detection data. We have experi-            Table 8 shows the results on the MS COCO validation
             mented with ResNet-50/101 at the time of the ILSVRC &           set. ResNet-101 has a 6% increase of mAP@[.5, .95] over
             COCO2015detectioncompetitions.                                  VGG-16,whichisa28%relativeimprovement,solelycon-
                Unlike VGG-16 used in [32], our ResNet has no hidden         tributed by the features learned by the better network. Re-
             fc layers. We adopt the idea of “Networks on Conv fea-          markably, the mAP@[.5, .95]’s absolute increase (6.0%) is
             ture maps” (NoC) [33] to address this issue. We compute         nearly as big as mAP@.5’s (6.9%). This suggests that a
             the full-image shared conv feature maps using those lay-        deeper network can improve both recognition and localiza-
             ers whose strides on the image are no greater than 16 pixels    tion.
             (i.e., conv1, conv2 x, conv3 x, andconv4 x, totally91conv       B. Object Detection Improvements
             layers in ResNet-101; Table 1). We consider these layers as
             analogous to the 13 conv layers in VGG-16, and by doing            For completeness, we report the improvements made for
             so, both ResNet and VGG-16haveconvfeaturemapsofthe              the competitions. These improvements are based on deep
             same total stride (16 pixels). These layers are shared by a     features and thus should beneﬁt from residual learning.
             region proposal network (RPN, generating 300 proposals)
             [32] and a Fast R-CNN detection network [7]. RoI pool-          MSCOCO
             ing [7] is performed before conv5 1. On this RoI-pooled         Boxreﬁnement. Our box reﬁnement partially follows the it-
             feature, all layers of conv5 x and up are adopted for each      erative localization in [6]. In Faster R-CNN, the ﬁnal output
             region, playing the roles of VGG-16’s fc layers. The ﬁnal       is a regressed box that is different from its proposal box. So
             classiﬁcation layer is replaced by two sibling layers (classi-  for inference, we pool a new feature from the regressed box
             ﬁcation and box regression [7]).                                and obtain a new classiﬁcation score and a new regressed
                For the usage of BN layers, after pre-training, we com-      box. We combine these 300 new predictions with the orig-
             pute the BN statistics (means and variances) for each layer     inal 300 predictions. Non-maximum suppression (NMS) is
             on the ImageNet training set. Then the BN layers are ﬁxed       applied on the union set of predicted boxes using an IoU
             during ﬁne-tuning for object detection. As such, the BN         threshold of 0.3 [8], followed by box voting [6]. Box re-
             layers become linear activations with constant offsets and      ﬁnementimproves mAPbyabout2points(Table9).
             scales, and BN statistics are not updated by ﬁne-tuning. We     Global context.  We combine global context in the Fast
             ﬁxtheBNlayersmainlyforreducingmemoryconsumption                 R-CNN step. Given the full-image conv feature map, we
             in Faster R-CNN training.                                       pool a feature by global Spatial Pyramid Pooling [12] (with
             PASCALVOC                                                       a “single-level” pyramid) which can be implemented as
                Following [7, 32], for the PASCAL VOC 2007 test set,         “RoI” pooling using the entire image’s bounding box as the
             weusethe 5k trainval images in VOC 2007 and 16k train-          RoI. This pooled feature is fed into the post-RoI layers to
             val images in VOC 2012 for training (“07+12”). For the          obtain a global context feature. This global feature is con-
             PASCAL VOC 2012 test set, we use the 10k trainval+test          catenated with the original per-region feature, followed by
             imagesinVOC2007and16ktrainvalimagesinVOC2012                    the sibling classiﬁcation and box regression layers. This
             for training (“07++12”). The hyper-parameters for train-        new structure is trained end-to-end.   Global context im-
             ing Faster R-CNN are the same as in [32]. Table 7 shows         proves mAP@.5byabout1point(Table9).
             the results. ResNet-101 improves the mAP by >3% over            Multi-scale testing. In the above, all results are obtained by
             VGG-16. This gain is solely because of the improved fea-        single-scale training/testing as in [32], where the image’s
             tures learned by ResNet.                                        shorter side is s = 600 pixels. Multi-scale training/testing
             MSCOCO                                                          has been developed in [12, 7] by selecting a scale from a
                The MS COCO dataset [26] involves 80 object cate-            feature pyramid, and in [33] by using maxout layers. In
             gories. We evaluate the PASCAL VOC metric (mAP @                our current implementation, we have performed multi-scale
             IoU = 0.5) and the standard COCO metric (mAP @ IoU =            testing following [33]; we have not performed multi-scale
             .5:.05:.95). We use the 80k images on the train set for train-  training because of limited time. In addition, we have per-
             ing and the 40k images on the val set for evaluation. Our       formed multi-scale testing only for the Fast R-CNN step
             detection system for COCO is similar to that for PASCAL         (but not yet for the RPN step). With a trained model, we
             VOC. We train the COCO models with an 8-GPU imple-              computeconvfeaturemapsonanimagepyramid,wherethe
             mentation, and thus the RPN step has a mini-batch size of       image’s shorter sides are s ∈ {200,400,600,800,1000}.
                                                                         10
                                                               training data                                                        COCOtrain                          COCOtrainval
                                                               test data                                                             COCOval                           COCOtest-dev
                                                               mAP                                                               @.5           @[.5, .95]             @.5           @[.5, .95]
                                                               baseline Faster R-CNN (VGG-16)                                   41.5               21.2
                                                               baseline Faster R-CNN (ResNet-101)                               48.4               27.2
                                                                +boxreﬁnement                                                   49.9               29.9
                                                                +context                                                        51.1               30.0               53.3               32.2
                                                                +multi-scale testing                                            53.8               32.5               55.7               34.9
                                                               ensemble                                                                                               59.0               37.4
                                                         Table 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101.
                        system                net                data           mAP areo         bike    bird   boat   bottle  bus     car     cat   chair   cow    table   dog   horse mbike person plant      sheep   sofa   train    tv
                        baseline           VGG-16               07+12            73.2    76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6
                        baseline         ResNet-101             07+12            76.4    79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0
                        baseline+++ ResNet-101             COCO+07+12            85.6    90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8
                      Table 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system “baseline+++”
                      include box reﬁnement, context, and multi-scale testing in Table 9.
                        system                net                data           mAP areo         bike    bird   boat   bottle  bus     car     cat   chair   cow    table   dog   horse mbike person plant      sheep   sofa   train    tv
                        baseline           VGG-16              07++12            70.4    84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5
                        baseline         ResNet-101            07++12            73.8    86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6
                        baseline+++ ResNet-101 COCO+07++12 83.8 92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0
                      Table 11. Detection results on the PASCAL VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/
                      displaylb.php?challengeid=11&compid=4). The baseline is the Faster R-CNN system. The system “baseline+++” include
                      box reﬁnement, context, and multi-scale testing in Table 9.
                      Weselect two adjacent scales from the pyramid following                                                                                                                              val2            test
                      [33]. RoI pooling and subsequent layers are performed on                                                                   GoogLeNet[44](ILSVRC’14)                                     -            43.9
                      the feature maps of these two scales [33], which are merged                                                                our single model (ILSVRC’15)                              60.5            58.8
                      bymaxoutasin[33]. Multi-scaletestingimprovesthemAP                                                                         our ensemble (ILSVRC’15)                                  63.6            62.1
                      byover 2 points (Table 9).
                                                                                                                                      Table12.Ourresults(mAP,%)ontheImageNetdetectiondataset.
                      Usingvalidationdata. Nextweusethe80k+40ktrainvalset                                                             OurdetectionsystemisFasterR-CNN[32]withtheimprovements
                      for training and the 20k test-dev set for evaluation. The test-                                                 in Table 9, using ResNet-101.
                      dev set has no publicly available ground truth and the result
                      is reported by the evaluation server. Under this setting, the                                                   weachieve 85.6% mAPonPASCALVOC2007(Table10)
                      results are an mAP@.5 of 55.7% and an mAP@[.5, .95] of                                                          and 83.8% on PASCAL VOC 2012 (Table 11)6. The result
                      34.9%(Table 9). This is our single-model result.                                                                on PASCAL VOC 2012 is 10 points higher than the previ-
                      Ensemble. InFasterR-CNN,thesystemisdesignedtolearn                                                              ous state-of-the-art result [6].
                      region proposals and also object classiﬁers, so an ensemble                                                     ImageNetDetection
                      can be used to boost both tasks. We use an ensemble for                                                              TheImageNetDetection(DET)taskinvolves200object
                      proposing regions, and the union set of proposals are pro-                                                      categories. The accuracy is evaluated by mAP@.5. Our
                      cessed by an ensemble of per-region classiﬁers. Table 9                                                         object detection algorithm for ImageNet DET is the same
                      shows our result based on an ensemble of 3 networks. The                                                        as that for MS COCO in Table 9. The networks are pre-
                      mAP is 59.0% and 37.4% on the test-dev set. This result                                                         trained on the 1000-class ImageNet classiﬁcation set, and
                      wonthe1stplace in the detection task in COCO 2015.                                                              are ﬁne-tuned on the DET data. We split the validation set
                      PASCALVOC                                                                                                       into two parts (val1/val2) following [8]. We ﬁne-tune the
                           Werevisit the PASCALVOCdatasetbasedontheabove                                                              detection models using the DET training set and the val1
                      model. With the single model on the COCO dataset (55.7%                                                         set. The val2 set is used for validation. We do not use other
                      mAP@.5inTable 9), we ﬁne-tune this model on the PAS-                                                            ILSVRC2015data. OursinglemodelwithResNet-101has
                      CALVOCsets. Theimprovementsofboxreﬁnement,con-                                                                       6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html,
                      text, and multi-scale testing are also adopted. By doing so                                                     submitted on 2015-11-26.
                                                                                                                               11
                    LOC         LOC     testing LOC error classiﬁcation top-5 LOC error                       method                     top-5 localization err
                   method      network         on GTCLS     network   on predicted CLS                                                    val           test
                 VGG’s[41]    VGG-16    1-crop  33.1 [41]                                         OverFeat [40] (ILSVRC’13)              30.0           29.9
                    RPN      ResNet-101 1-crop    13.3                                            GoogLeNet[44](ILSVRC’14)                 -            26.7
                    RPN      ResNet-101 dense     11.7
                    RPN      ResNet-101 dense              ResNet-101       14.4                  VGG[41](ILSVRC’14)                     26.9           25.3
                 RPN+RCNN ResNet-101 dense                 ResNet-101       10.6                  ours (ILSVRC’15)                        8.9           9.0
                 RPN+RCNN ensemble       dense              ensemble         8.9
                Table 13. Localization error (%) on the ImageNet validation. In              Table 14. Comparisons of localization error (%) on the ImageNet
                the column of “LOC error on GT class” ([41]), the ground truth               dataset with state-of-the-art methods.
                class is used. In the “testing” column, “1-crop” denotes testing
                on a center crop of 224×224 pixels, “dense” denotes dense (fully             ports a center-crop error of 33.1% (Table 13) using ground
                convolutional) and multi-scale testing.                                      truth classes. Under the same setting, our RPN method us-
                                                                                             ing ResNet-101 net signiﬁcantly reduces the center-crop er-
                58.8%mAPandourensembleof3modelshas62.1%mAP                                   ror to 13.3%. This comparison demonstrates the excellent
                ontheDETtestset(Table12). Thisresultwonthe1stplace                           performance of our framework. With dense (fully convolu-
                in the ImageNet detection task in ILSVRC 2015, surpassing                    tional) and multi-scale testing, our ResNet-101 has an error
                the second place by 8.5 points (absolute).                                   of 11.7% using ground truth classes. Using ResNet-101 for
                                                                                             predicting classes (4.6% top-5 classiﬁcation error, Table 4),
                C. ImageNet Localization                                                     the top-5 localization error is 14.4%.
                                                                                                 Theaboveresultsareonlybasedontheproposalnetwork
                   The ImageNet Localization (LOC) task [36] requires to                     (RPN) in Faster R-CNN [32]. One may use the detection
                classify and localize the objects. Following [40, 41], we                    network (Fast R-CNN [7]) in Faster R-CNN to improve the
                assume that the image-level classiﬁers are ﬁrst adopted for                  results. But wenoticethatonthisdataset,oneimageusually
                predicting the class labels of an image, and the localiza-                   contains a single dominate object, and the proposal regions
                tion algorithm only accounts for predicting bounding boxes                   highly overlap with each other and thus have very similar
                based on the predicted classes. We adopt the “per-class re-                  RoI-pooled features. As a result, the image-centric training
                gression” (PCR) strategy [40, 41], learning a bounding box                   of Fast R-CNN [7] generates samples of small variations,
                regressor for each class. We pre-train the networks for Im-                  whichmaynotbedesiredforstochastictraining. Motivated
                ageNet classiﬁcation and then ﬁne-tune them for localiza-                    by this, in our current experiment we use the original R-
                tion. We train networks on the provided 1000-class Ima-                      CNN[8]thatisRoI-centric, in place of Fast R-CNN.
                geNet training set.                                                              OurR-CNNimplementationisasfollows. We apply the
                   Our localization algorithm is based on the RPN frame-                     per-class RPN trained as above on the training images to
                work of [32] with a few modiﬁcations. Unlike the way in                      predict bounding boxes for the ground truth class. These
                [32] that is category-agnostic, our RPN for localization is                  predicted boxes play a role of class-dependent proposals.
                designed in a per-class form. This RPN ends with two sib-                    For each training image, the highest scored 200 proposals
                ling 1×1 convolutional layers for binary classiﬁcation (cls)                 are extracted as training samples to train an R-CNN classi-
                and box regression (reg), as in [32]. The cls and reg layers                 ﬁer. The image region is cropped from a proposal, warped
                are both in a per-class from, in contrast to [32]. Speciﬁ-                   to 224×224 pixels, and fed into the classiﬁcation network
                cally, the cls layer has a 1000-d output, and each dimension                 as in R-CNN[8]. Theoutputsofthisnetworkconsistoftwo
                is binary logistic regression for predicting being or not be-                sibling fc layers for cls and reg, also in a per-class form.
                ing an object class; the reg layer has a 1000×4-d output                     This R-CNN network is ﬁne-tuned on the training set us-
                consisting of box regressors for 1000 classes. As in [32],                   ing a mini-batch size of 256 in the RoI-centric fashion. For
                our bounding box regression is with reference to multiple                    testing, the RPN generates the highest scored 200 proposals
                translation-invariant “anchor” boxes at each position.                       for each predicted class, and the R-CNN network is used to
                   AsinourImageNetclassiﬁcation training (Sec. 3.4), we                      update these proposals’ scores and box positions.
                randomly sample 224×224 crops for data augmentation.                             This method reduces the top-5 localization error to
                Weuseamini-batch size of 256 images for ﬁne-tuning. To                       10.6% (Table 13). This is our single-model result on the
                avoid negative samples being dominate, 8 anchors are ran-                    validation set. Using an ensemble of networks for both clas-
                domlysampledforeachimage,wherethesampledpositive                             siﬁcation and localization, we achieve a top-5 localization
                and negative anchors have a ratio of 1:1 [32]. For testing,                  error of 9.0% on the test set. This number signiﬁcantly out-
                the network is applied on the image fully-convolutionally.                   performstheILSVRC14results(Table14),showinga64%
                   Table 13 compares the localization results. Following                     relative reduction of error. This result won the 1st place in
                [41], weﬁrstperform“oracle”testingusingthegroundtruth                        the ImageNet localization task in ILSVRC 2015.
                class as the classiﬁcation prediction. VGG’s paper [41] re-
                                                                                        12
