# MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark (2025)
Source: c47f66-2025.pdf

## Core reasons
- Introduces MMMU-Pro as a benchmark for multimodal understanding and reasoning evaluation, making the main contribution a dataset/benchmark.
- Describes a benchmark construction and evaluation protocol (filtering text-only questions, option augmentation, vision-only input) rather than a new model or mechanism.

## Evidence extracts
- "limitations, we introduce MMMU-Pro, a more ro-
bust benchmark that removes text-only answerable
questions, expands candidate options, and includes
a vision-only input setting to better reflect real-
world multimodal scenarios." (p. 9)
- "Establishing a reliable benchmark for human performance on MMMU-Pro is crucial to evaluating the true
capabilities of multimodal AI models." (p. 16)

## Classification
Class name: Data, Benchmarks & Measurement
Class code: 4

$$
\boxed{4}
$$
