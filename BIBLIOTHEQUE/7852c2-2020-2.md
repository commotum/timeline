# Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation (2020)
Source: 7852c2-2020.pdf

## Core reasons
- Proposes axial-attention by factorizing 2D attention into sequential 1D attentions along height and width, enabling modeling over 2D spatial structure.
- Uses axial-attention blocks as the building blocks for stand-alone attention models targeting image classification and dense prediction.

## Evidence extracts
- "our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classiÔ¨Åcation and dense prediction." (p. 1)
- "The core idea is to factorize 2D attention into two 1D attentions along height- and width-axis sequentially." (p. 2)

## Classification
Class name: Increasing Transformer's Dimensions
Class code: 2

$$
\boxed{2}
$$
