                     AnswerType           Percent            Example        removes articles and does other simple normaliza-
                                                                            tion, and our F score is based on that used by
                     NUMBER                  66.1                  12                         1
                     PERSON                  12.2          Jerry Porter     SQuAD.SinceDROPisnumeracy-focused,wede-
                     OTHER                    9.4               males       ﬁne F to be 0 when there is a number mismatch
                     OTHERENTITIES            7.3            Seahawks              1
                     VERBPHRASE               3.5   Tomarrived at Acre      betweenthegoldandpredictedanswers, regardless
                     DATE                     1.5        3 March 1992       of other word overlap. When an answer has multi-
                  Table 3: Distribution of answer types in training set,    ple spans, we ﬁrst perform a one-to-one alignment
                  according to an automatic named entity recognition.       greedily based on bag-of-word overlap on the set
                                                                            of spans and then compute average F over each
                                                                                                                      1
                                                                            span. When there are multiple annotated answers,
                  trigram patterns in the questions per answer type.        both metrics take a max over all gold answers.
                  Weﬁndthatthedataset offers a huge variety of lin-         5.1   Semantic Parsing
                  guistic constructs, with the most frequent pattern
                  (“Whichteamscored”)appearinginonly4%ofthe                 Semantic parsing has been used to translate nat-
                  span type questions. For number type questions,           ural language utterances into formal executable
                  the 5 most frequent question patterns all start with      languages (e.g., SQL) that can perform discrete
                  “Howmany”,indicating the need to perform count-           operations against a structured knowledge repre-
                  ing and other arithmetic operations. A distribution       sentation, such as knowledge graphs or tabular
                  of the trigrams containing the start of the questions     databases (Zettlemoyer and Collins, 2005; Berant
                  are shown in Figure 1.                                    et al., 2013b; Yin and Neubig, 2017; Chen and
                                                                            Mooney,2011,inter alia). Since many of DROP’s
                  Answeranalysis        Todiscern the level of passage      questions require similar discrete reasoning, it is
                  understanding needed to answer the questions in           appealing to port some of the successful work in
                  DROP,weannotatethesetofspansinthepassage                  semantic parsing to the DROP dataset. Speciﬁ-
                  that are necessary for answering the 350 questions        cally, we use the grammar-constrained semantic
                  mentioned above. We ﬁnd that on an average 2.18           parsing model built by Krishnamurthy et al. (2017)
                  spans need to be considered to answer a question          (KDG) for the WIKITABLEQUESTIONS tabular
                  and the average distance between these spans is           dataset (Pasupat and Liang, 2015).
                  26 words, with 20% of samples needing at least            Sentence representation schemes           We experi-
                  3 spans (see appendix for examples). Finally, we          mented with three paradigms to represent para-
                  assess the answer distribution in Table 3, by run-        graphs as structured contexts: (1) Stanford de-
                  ning the part-of-speech tagger and named entity           pendencies (de Marneffe and Manning, 2008, Syn
                                           5
                  recognizer from spaCy to automatically partition          Dep); which capture word-level syntactic relations,
                  all the answers into various categories. We ﬁnd that      (2) Open Information Extraction (Banko et al.,
                  a majority of the answers are numerical values and        2007, Open IE), a shallow semantic representation
                  proper nouns.                                             which directly links predicates and arguments; and
                                                                                                                           `
                  5    Baseline Systems                                     (3) Semantic Role Labeling (Carreras and Marquez,
                                                                            2005, SRL), which disambiguates senses for pol-
                  In this section we describe the initial baselines         ysemouspredicates and assigns predicate-speciﬁc
                  that we evaluated on the DROP dataset. We used                              6
                                                                            argument roles.     To adhere to KDG’s structured
                  three types of baselines: state-of-the-art semantic       representation format, we convert each of these rep-
                  parsers (§5.1), state-of-the-art reading comprehen-       resentations into a table, where rows are predicate-
                  sion models (§5.2), and heuristics looking for an-        argument structures and columns correspond to
                  notation artifacts (§5.3). We use two evaluation          different argument roles.
                  metrics to compare model performance: Exact-              Logical form language         Our logical form lan-
                  Match, and a numeracy-focused (macro-averaged)            guageidentiﬁes ﬁve basic elements in the table rep-
                  F score, which measures overlap between a bag-
                    1                                                       resentation: predicate-argument structures (i.e., ta-
                  of-words representation of the gold and predicted         blerows),relations(column-headers),strings,num-
                  answers. We employ the same implementation of
                  Exact-Match accuracy as used by SQuAD, which                  6WeusedtheAllenNLPimplementationsofstate-of-the-
                                                                            art modelsforalloftheserepresentations(Gardneretal.,2017;
                     5https://spacy.io/                                     Dozat et al., 2017; He et al., 2017; Stanovsky et al., 2018).
                                                                        2372
