              all the layers (Yu et al. 2023). We refer to this method as          effective for vision transformers on various tasks such as
              ”Layer-wise”. The Layer-wise structure resolves the exist-           image classification, semantic segmentation, and object
              ing limitations and enhances the expressiveness of PE.               detection.
                Interestingly, as shown in Fig 1, we observed results
              that differed from our expectations between the class to-                              Related Work
              ken and GAP methods with PE delivered in the Layer-wise           Vision Transformers
              structure. On image classification, the GAP method demon-
              strates superior performance compared to the class token          The vision transformer design is adapted from Trans-
              method (Chu et al. 2021b). Additionally, the Layer-wise           former (Vaswani et al. 2017), which was designed for nat-
              structure also improvestheperformanceofvisiontransform-           ural language processing (NLP). This adaptation makes it
              ers by enhancing the expressiveness of PE (Yu et al. 2023).       suitable for computer vision tasks such as image classifica-
              However, we observed a conflicting result where perfor-           tion (Dosovitskiy et al. 2020; Touvron et al. 2021; Liu et al.
              mance decreased when the GAP and Layer-wise structure             2021), object detection (Carion et al. 2020; Zhu et al. 2020),
              were applied together. Therefore, to overcome the conflict-       and semantic segmentation (Zheng et al. 2021; Wang et al.
              ing results, we propose a method to maximize the effective-       2021; Strudel et al. 2021).
              ness of PE in the GAP approach.                                   Class Token & Global Average Pooling         ViT (Dosovit-
                We observe that PE exhibits distinct characteristics at         skiy et al. 2020) conducts ablation studies comparing the
              each layer in the Layer-wise structure, which are not seen in     class token and GAP. Additionally, there are other studies
              the original vision transformer. As shown in Fig 2, we find       on the use of GAP and class tokens in vision transform-
              that PE tends to counterbalance the values of token embed-        ers (Raghu et al. 2021; Chu et al. 2021b). Studies such as
              ding passing through the layers in the Layer-wise structure.      CeiT (Yuan et al. 2021a) and T2T-ViT (Yuan et al. 2021b)
              Additionally, we observe that this tendency becomes more          use class token, while others like Swin Transformer (Liu
              pronounced as the layers deepen. We also discover that in         et al. 2021) and CPVT (Chu et al. 2021b) adapt GAP.
              the Layer-wise structure, while the token embedding values        CPVT achieves performance improvements by using GAP
              maintain the counterbalanced effect by PE as they progress        instead of the class token. Although the class token is not
              throughthelayers,asshowninFig2-(b),thedirectionalbal-             inherently translation-invariant, it can become so through
              ance of the token embedding is not adequately compensated         training. By adopting GAP, which is inherently translation-
              by PE after passing through the last layer, even though it        invariant, better improvements in image classification tasks
              is still maintained. Through this observation, we establish       are achieved (Chu et al. 2021b). Furthermore, GAP results
              two hypotheses: (1) in the Layer-wise structure, PE initially     in even less computational complexity because it eliminates
              provides position information, but as the layers deepen, PE       the need to compute the attention interaction between the
              plays a role in counterbalancing the values of token embed-       class token and the image patches.
              ding; (2) after the last layer, it is beneficial for vision trans-
              formers to maintain the directional balance by counterbal-        Position Embeddings in Vision Transformers
              ancing the token embedding values with PE.                        Absolute Position Embedding       In the transformer, abso-
                To validate these hypotheses, we simply add PE to the           lute position embedding is generated through a sinusoidal
              Layer Normalization (LN) that exists outside the layers and       function and added to the input token embedding (Vaswani
              before the MLP head. We call this LN as ”Last LN”. We             et al. 2017). The sinusoidal functions are designed to give
              refer to the method that uses an improved Layer-wise struc-       the position embedding locally consistent similarity, which
              ture, different from the conventional Layer-wise structure,       helps vision transformers focus more effectively on tokens
              anddoesnotdeliverPEtotheLastLNasPVG.Additionally,                 that are close to each other in the input sequence. This local
              werefer to the method that maximizes the role of PE by de-        consistency enhances the model’s ability to capture spatial
              livering it to the Last LN as MPVG. By comparing PVG and          relationships and patterns (Vaswani et al. 2017).
              MPVG,wedemonstrate that MPVG effectively maximizes                  Besides sinusoidal positional embedding, position em-
              PEandthat maintaining the counterbalancing directionality         bedding can also be learnable. Learnable position embed-
              of PE is beneficial for vision transformers. Our experiments      ding is created through training parameters, which are ini-
              validate our hypothesis and demonstrate that MPVG outper-         tialized with a fixed-dimensional tensor and updated along
              forms other methods. The results demonstrate that MPVG            with the model’s parameters during training. Recently,
              consistently performs well for vision transformers.               many models have adopted absolute position embedding
                In this paper, our contributions are as follows:                due to their effectiveness in encoding positional informa-
              1. WeproposeasimpleyeteffectivemethodcalledMPVG,                  tion (Dosovitskiy et al. 2020; Touvron et al. 2021; Liu et al.
                 which maximizes the effect of PE in the GAP method.            2021).
                 Weshow that MPVG leads to better performance in vi-            Relative Position Embedding      In addition to absolute po-
                 sion transformers.                                             sition embedding, there is also relative position embed-
              2. We provide an analysis of the phenomenon observed in           ding (Shaw, Uszkoreit, and Vaswani 2018). Relative PE en-
                 PE when using the Layer-wise structure and offer in-           codes the relative position information between tokens. The
                 sights into the role of PE.                                    first to propose relative PE in computer vision was (Shaw,
              3. Through experiments, we verify that MPVG is generally          Uszkoreit, and Vaswani 2018). Furthermore, (Bello et al.
