                             Published as a conference paper at ICLR 2021
                             2    SHARPNESS-AWARE MINIMIZATION (SAM)
                             Throughoutthepaper,wedenotescalarsasa,vectorsasa,matricesasA,setsasA,andequalityby
                             deÔ¨Ånition as ,. Given a training dataset S , ‚à™n       {(x ,y )} drawn i.i.d. from distribution D, we
                                                                               i=1     i  i
                             seek to learn a model that generalizes well. In particular, consider a family of models parameterized
                             byw‚ààW‚äÜRd;givenaper-data-pointlossfunctionl : W√óX √óY ‚Üí R ,wedeÔ¨Ånethetraining
                                                    P                                                      +
                             set loss L (w) , 1       n   l(w,x ,y ) and the population loss L (w) , E                 [l(w,x,y)].
                                       S          n   i=1        i   i                             D           (x,y)‚àºD
                             Having observed only S, the goal of model training is to select model parameters w having low
                             population loss LD(w).
                             Utilizing L (w) as an estimate of L (w) motivates the standard approach of selecting parameters
                                        S                          D
                             wbysolvingminwLS(w)(possiblyinconjunctionwitharegularizeronw)usinganoptimization
                             procedure such as SGD or Adam. Unfortunately, however, for modern overparameterized mod-
                             els such as deep neural networks, typical optimization approaches can easily result in suboptimal
                             performance at test time. In particular, for modern models, LS(w) is typically non-convex in w,
                             with multiple local and even global minima that may yield similar values of LS(w) while having
                             signiÔ¨Åcantly different generalization performance (i.e., signiÔ¨Åcantly different values of LD(w)).
                             Motivatedbytheconnectionbetweensharpnessofthelosslandscapeandgeneralization,wepropose
                             a different approach: rather than seeking out parameter values w that simply have low training loss
                             valueL (w),weseekoutparametervalueswhoseentireneighborhoodshaveuniformlylowtraining
                                     S
                             loss value (equivalently, neighborhoods having both low loss and low curvature). The following
                             theorem illustrates the motivation for this approach by bounding generalization ability in terms of
                             neighborhood-wise training loss (full theorem statement and proof in Appendix A):
                             Theorem(statedinformally)1. ForanyœÅ > 0,withhighprobabilityovertrainingsetS generated
                             from distribution D,
                                                                                                  2   2
                                                         LD(w)‚â§ max LS(w+)+h(kwk /œÅ ),
                                                                    kk ‚â§œÅ                        2
                                                                       2
                             where h : R    ‚ÜíR isastrictlyincreasing function (under some technical conditions on L (w)).
                                         +       +                                                                           D
                             Tomakeexplicit our sharpness term, we can rewrite the right hand side of the inequality above as
                                                   [ max L (w+)‚àíL (w)]+L (w)+h(kwk2/œÅ2).
                                                   kk2‚â§œÅ S                 S          S                2
                             TheterminsquarebracketscapturesthesharpnessofLS atwbymeasuringhowquicklythetraining
                             loss can be increased by moving from w to a nearby parameter value; this sharpness term is then
                             summedwith the training loss value itself and a regularizer on the magnitude of w. Given that the
                             speciÔ¨Åc function h is heavily inÔ¨Çuenced by the details of the proof, we substitute the second term
                                        2
                             with Œª||w|| for a hyperparameter Œª, yielding a standard L2 regularization term. Thus, inspired by
                                        2
                             thetermsfromthebound,weproposetoselectparametervaluesbysolvingthefollowingSharpness-
                             AwareMinimization (SAM) problem:
                                                 SAM                2                 SAM
                                          minL        (w)+Œª||w||         where      L      (w) , max LS(w+),                    (1)
                                            w    S                  2                 S            |||| ‚â§œÅ
                                                                                                      p
                             where œÅ ‚â• 0 is a hyperparameter and p ‚àà [1,‚àû] (we have generalized slightly from an L2-norm
                             to a p-norm in the maximization over , though we show empirically in appendix C.5 that p = 2 is
                                                                 1
                             typically optimal). Figure 1 shows the loss landscape for a model that converged to minima found
                             by minimizing either L (w) or LSAM(w), illustrating that the sharpness-aware loss prevents the
                                                     S           S
                             model from converging to a sharp minimum.
                                                         SAM
                             In order to minimize L           (w), we derive an efÔ¨Åcient and effective approximation to
                                   SAM                   S
                             ‚àá L        (w)bydifferentiating through the inner maximization, which in turn enables us to apply
                               w S
                             stochastic gradient descent directly to the SAM objective. Proceeding down this path, we Ô¨Årst ap-
                             proximate the inner maximization problem via a Ô¨Årst-order Taylor expansion of LS(w + ) w.r.t. 
                             around 0, obtaining
                                 ‚àó                                                      T                          T
                                 (w) , argmaxL (w+)‚âàargmaxL (w)+ ‚àá L (w)=argmax ‚àá L (w).
                                           kk ‚â§œÅ    S              kk ‚â§œÅ    S             w S           kk ‚â§œÅ       w S
                                              p                        p                                     p
                                1Figure 1 was generated following Li et al. (2017) with the provided ResNet56 (no residual connections)
                             checkpoint, and training the same model with SAM.
                                                                                3
