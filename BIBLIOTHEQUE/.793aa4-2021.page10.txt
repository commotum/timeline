                 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine     Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh
                    Lee, Sharan Narang, Michael Matena, Yanqi Zhou,        Rawat, Sashank J Reddi, and Sanjiv Kumar.
                    Wei Li, and Peter J Liu. 2020. Exploring the lim-      2020. Are Transformers universal approximators of
                    its of transfer learning with a uniﬁed text-to-text    sequence-to-sequence functions?   In International
                    transformer. JournalofMachineLearningResearch,         Conference on Learning Representations.
                    21(140):1–67.
                 Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
                    2018. Self-attention with relative position represen-
                    tations. In Proceedings of the 2018 Conference of
                    the North American Chapter of the Association for
                    Computational Linguistics: Human Language Tech-
                    nologies, Volume 2 (Short Papers), pages 464–468.
                 Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran-
                    cois Chollet, Aidan N Gomez, Stephan Gouws,
                    Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki
                    Parmar, et al. 2018. Tensor2tensor for neural ma-
                    chine translation. arXiv preprint arXiv:1803.07416.
                 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
                    Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
                    Kaiser, and Illia Polosukhin. 2017. Attention is all
                    you need. In Advances in Neural Information Pro-
                    cessing Systems, pages 5998–6008.
                 Alex Wang, Amanpreet Singh, Julian Michael, Fe-
                    lix Hill, Omer Levy, and Samuel Bowman. 2019.
                    Glue: Amulti-taskbenchmarkandanalysisplatform
                    for natural language understanding.  In 7th Inter-
                    national Conference on Learning Representations,
                    ICLR2019.
                 Sinong Wang, Belinda Z. Li, Madian Khabsa, Han
                    Fang, and Hao Ma. 2020. Linformer: Self-attention
                    with linear complexity.
                 Yu-An Wang and Yun-Nung Chen. 2020. What do
                    position embeddings learn? an empirical study of
                    pre-trained language model positional encoding. In
                    EMNLP2020.
                 Adina Williams, Nikita Nangia, and Samuel R. Bow-
                    man. 2018. A broad-coverage challenge corpus for
                    sentence understanding through inference.
                 Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
                    Le, Mohammad Norouzi, Wolfgang Macherey,
                    Maxim Krikun, Yuan Cao, Qin Gao, Klaus
                    Macherey,JeffKlingner,ApurvaShah,MelvinJohn-
                    son, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,
                    Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith
                    Stevens, George Kurian, Nishant Patil, Wei Wang,
                    Cliff Young, Jason Smith, Jason Riesa, Alex Rud-
                    nick, Oriol Vinyals, Greg Corrado, Macduff Hughes,
                    and Jeffrey Dean. 2016. Google’s neural machine
                    translation system: Bridgingthegapbetweenhuman
                    and machine translation.
                 Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G.
                    Carbonell, Ruslan Salakhutdinov, and Quoc V. Le.
                    2019. XLNet: Generalized autoregressive pretrain-
                    ing for language understanding.   arXiv preprint
                    arXiv:1906.08237.
                                                                    2983
