# Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014)
Source: 380806-2014.pdf

## Core reasons
- Proposes dropout as a training-time regularization method that randomly drops units to prevent overfitting.
- Focuses on improving neural network generalization and model averaging, not on positional encodings, dimensional lifting, or new datasets.

## Evidence extracts
- "Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much." (p. 1)
- "Dropout is a technique that addresses both these issues. It prevents overﬁtting and provides a way of approximately combining exponentially many diﬀerent neural network architectures eﬃciently." (p. 2)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
