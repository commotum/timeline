                          14     Wen. et al.
                          planes while k-means clusters fail to serve as a beneficial mid-level representation
                          on Synthia4D.
                                      Table 4. Comparisons between different representations
                                            Method   Synthia 4D [34] MSRAction [25]
                                         P4Transformer   83.16        90.94
                                            K-means      80.70        91.76
                                           BPSS [26]     83.43        91.98
                                             Ours        84.49        92.33
                          OfÒine branch and Online branch. The online branch produces fine primi-
                          tive features with heavy computation while the ofÒine branch produces coarser
                          features efÏciently as a surrogate of the online branch so that the network can
                          process long clips with limited computing resources. For the action recognition
                          task where data clips can already be largely fit into the GPU memory, using
                          an online branch only with fine primitive features is preferred. In this case,
                          just using an ofÒine branch or combining the ofÒine and online branches results
                          in marginal performance degradation with accuracy of 92.13 and 92.27 respec-
                          tively. For the 4D segmentation task, using our online branch independently, the
                          memory could only afford 3 frames and the resulting segmentation mIoU(%) is
                          84.05. This number goes to 84.49 when assisted by the ofÒine branch covering
                          30 frames, confirming the value of the ofÒine branch.
                          6   Conclusions
                              This paper proposes a 4D backbone for long-term point cloud video under-
                          standing. The key idea is to leverage the primitive plane to capture the long-
                          term spatial-temporal context in 4D point cloud videos. Results of experiments
                          showing ablations and state-of-the-art performance on a wide range of 4D tasks
                          including MSR-Action3D action recognition task, 4D semantic segmentation on
                          sythia4D and on HOI4D. This result is very encouraging and suggests future
                          work to explore more possible backbone designs for 4D point cloud understand-
                          ing.
                          References
                           1. Arnab, A., Dehghani, M., Heigold, G., Sun, C., Luˇci´c, M., Schmid, C.: Vivit: A
                              video vision transformer. In: Proceedings of the IEEE/CVF International Confer-
                              ence on Computer Vision. pp. 6836–6846 (2021)
                           2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint
                              arXiv:1607.06450 (2016)
