                         that our architecture does not suffer from a decoder bottleneck. Note that the encoder is not helpful in
                         this experiment since the task is always the same. Our later results on ARC-AGI section 5.6 take this
                         a step further and show that we can learn a single transformer architecture capable of executing over
                         180programsintheARCtrainingdataset.
                                           Training Loss                               Pixel Accuracy
                             1.0                         Task ID
                             0.8                          007bbfb7    Task          re-arc     ARC-AGI
                                                          00d62c1b    007bbfb7        100         100
                             0.6                          017c7c7b
                            Loss                          025d127b    00d62c1b       96.5         100
                             0.4                          045e512c
                                                                      017c7c7b       99.7         100
                             0.2                                      025d127b        100         100
                             0.0                                      045e512c       98.3         100
                                 0    2000  4000  6000  8000 10000
                                           Training Steps
                                      (a) Training loss                 (b) Performance after 10k steps of training
                         Figure 6: Training loss and performance of LPN training on 5 of the re-arc distributions. For each
                         task, only samples from the re-arc generators are used for training. The corresponding ARC-AGI
                         tasks are never seen during training.
                         5.3  Pattern Task
                         The ARC-AGI challenge contains many diverse programs leveraging different knowledge priors.
                         Injecting these priors into LPN by training the model to master ARC-like tasks requires significant
                         compute resources when training from scratch, without an LLM-based initialization. Therefore,
                         to investigate the training dynamics and properties of LPN before such a large-scale training, we
                         develop a simpler task called Pattern task (see figure 7) within the same domain, but using a narrow
                         distribution of pattern-like programs. This specific task always generates fully-black 10x10 inputs
                         withasinglebluepixelatarandomlocationthatdefineswheretheoutputpastesa4x4patternsampled
                         from a uniform distribution. The pattern is program-specific, which means it is the same across
                         different pairs but it varies from one specification to another. This task enables us to demonstrate how
                         deep learning methods may still make errors on such tasks without test-time computation. We later
                         extend this task to study an out-of-distribution setting in section 5.5.
                         Figure 7: Example of input (top row) and output (bottom row) pairs of a specification sampled from
                         the Pattern task. Each sample is a batch of 4 pairs that share the same pattern.
                         Wecompareavarietyoftraining and inference methods in Table 1. We train 1M parameter models
                         with each method for 20k steps with a batch size of 128. Then, we evaluate each training mode with
                         different choices for latent optimization. Note that for all inference methods, we still only evaluate
                         performance with a budget of 1, any inference budget only happens in the latent space before making
                         a single output prediction. We repeat each experiment 3 times with different random seeds and report
                         the mean performance and standard variation in parentheses.
                                                                     11
