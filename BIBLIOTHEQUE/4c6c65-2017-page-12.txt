                        In this task our model used: four convolutional layers with 32, 64, 128 and 256 kernels, ReLU
                    non-linearities, and batch normalization; the questions, which were encoded as ﬁxed-length binary
                    strings, were treated as question embeddings and passed directly to the RN alongside the object
                    pairs; a four-layer MLP consisting of 2000 units per layer with ReLU non-linearities was used for g ;
                                                                                                                          θ
                    and a four-layer MLP consisting of 2000, 1000, 500, and 100 units with ReLU non-linearities used for
                    fφ. An additional ﬁnal linear layer produced logits for a softmax over the possible answers. The
                    softmax output was optimized with a cross-entropy loss function using the Adam optimizer with a
                                       −4
                    learning rate of 1e   and mini-batches of size 64.
                        Wealso trained a comparable MLP based model (CNN+MLP model) on the Sort-of-CLEVR
                    task, to explore the extent to which a standard model can learn to answer relational questions. We
                    used the same CNN and LSTM, trained end-to-end, as described above. However, this time we
                    replaced the RN with an MLP with the same number of layers and number of units per layer. Note
                    that there are more parameters in this model because the input layer of the MLP connects to the
                    full CNN image embedding.
                    E bAbI model for language understanding
                    For the bAbI task, each of the 20 sentences in the support set was processed through a 32 unit LSTM
                    to produce an object. For the RN, gθ was a four-layer MLP consisting of 256 units per layer. For
                    fφ, we used a three-layer MLP consisting of 256, 512, and 159 units, where the ﬁnal layer was a
                    linear layer that produced logits for a softmax over the answer vocabulary. A separate LSTM with
                    32 units was used to process the question. The softmax output was optimized with a cross-entropy
                    loss function using the Adam optimizer with a learning rate of 2e−4.
                    F Dynamic physical system reasoning
                    For the connection inference task the targets were binary vectors representing the existence (or
                    non-existence) of a connection between each ball pair. For a total of 10 objects, the targets were
                      2
                    10 length vectors. For the counting task, the targets were one-hot vectors (of length 10) indicating
                    the number of systems of connected balls. It is important to point out that in the ﬁrst task the
                    supervision signal provided by the targets explicitly informs about the relations that need to be
                    computed. In the second task, the supervision signal (counts of systems) do not provide explicit
                    information about the kind of relations that need to be computed. Therefore, the models that solve
                    the counting task must successfully infer the relations implicitly.
                        Inputs to the RN were state descriptions. Each row of a state description matrix provided
                    information about a particular object (i.e. ball), including its coordinate position and color. Since
                    the system was dynamic, and hence evolved through time, each row contained object property
                    descriptions for 16 consecutive time-frames. For example, a row could be comprised of 33 ﬂoats:
                    16 for the object’s x coordinate position across 16 frames, 16 for the object’s y coordinate position
                    across 16 frames, and 1 for the object’s color. The RN treated each row in this state description
                    matrix as an object. Thus, it had to infer an object description contained information of the object’s
                    properties evolving through time.
                        For the connection inference task, the RN’s g was a four-layer MLP consisting of three layers
                                                                       θ
                    with 1000 units and one layer with 500 units. For fφ, we used a three-layer MLP consisting of 500,
                    100, and 100 units, where the ﬁnal layer was a linear layer that produced logits corresponding to
                    the existence/absence of a connection between each ball pair. The output was optimized with a
                    cross-entropy loss function using the Adam optimizer with a learning rate of 1e−4 and a batch size
                    of 50. The same model was used for the counting task, but this time the output layer of the RN
                    was a linear layer with 10 units. For baseline comparisons we replaced the RNs with MLPs with
                    comparable number of parameters.
                        Please see the supplementary videos:
                        https://www.youtube.com/channel/UCIAnkrNn45D0MeYwtVpmbUQ
                                                                      12
