Akshatha Arodi, Martin Pomsl, Kaheer Suleman, Adam
Trischler, Alexandra Olteanu, and Jackie Chi Kit Che-
ung. 2023. The kitmus test: Evaluating knowledge
integration from multiple sources in natural language
understanding systems.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632-642, Lisbon, Portugal. Association for Compu-
tational Linguistics.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.

Nurendra Choudhary and Chandan K. Reddy. 2023.
Complex logical reasoning over knowledge graphs
using large language models.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models.

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 320-335.

Aparna Elangovan, Jiayuan He, and Karin Verspoor.
2021. Memorization vs. generalization: Quantifying
data leakage in nlp performance evaluation.

Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-
lace, Pieter Abbeel, Sergey Levine, and Dawn Song.
2023. Koala: A dialogue model for academic re-
search. Blog post.

Biyang Guo, Xin Zhang, Ziyuan Wang, Mingqi Jiang,
Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng
Wu. 2023. How close is chatgpt to human experts?
comparison corpus, evaluation, and detection.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2021. Measuring massive multitask language under-
standing.

Ziniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang,
Ziyi Yang, Chenguang Zhu, Kai-Wei Chang, and
Yizhou Sun. 2022. Empowering language models
with knowledge graph reasoning for question answer-
ing.

Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,
Joongbo Shin, Janghoon Han, Gyeonghun Kim, and
Minjoon Seo. 2023. Temporalwiki: A lifelong bench-
mark for training and evaluating ever-evolving lan-
guage models.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2023. Large lan-
guage models are zero-shot reasoners.

Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-
bovskaya, Devang Agrawal, Adam Liska, Tayfun
Terzi, Mai Gimenez, Cyprien de Masson dâ€™ Autume,
Tomas Kocisky, Sebastian Ruder, Dani Yogatama,
Kris Cao, Susannah Young, and Phil Blunsom. 2021.
Mind the gap: Assessing temporal generalization
in neural language models. In Neural Information
Processing Systems.

Shayne Longpre, Kartik Perisetla, Anthony Chen,
Nikhil Ramesh, Chris DuBois, and Sameer Singh.
2022. Entity-based knowledge conflicts in question
answering.

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
When not to trust language models: Investigating
effectiveness of parametric and non-parametric mem-
ories.

Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstrations:
What makes in-context learning work?

Natalya F Noy, Deborah L McGuinness, et al. 2001.
Ontology development 101: A guide to creating your
first ontology.

Yasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and
Greg Durrett. 2021. Creak: A dataset for common-
sense reasoning over entity knowledge.

OpenAI. 2023. Gpt-4 technical report.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
