                                    This ICCV paper is the Open Access version, provided by the Computer Vision Foundation.
                                                 Except for this watermark, it is identical to the accepted version;
                                            the final published version of the proceedings is available on IEEE Xplore.
                                 PanSt3R:Multi-viewConsistent Panoptic Segmentation
                           ˇ
                   Lojze Zust               YohannCabon                 Juliette Marrie               Leonid Antsfeld
                                                               ´ ˆ
                                Boris Chidlovskii             Jerome Revaud              Gabriela Csurka
                                                               Naver Labs Europe
                                                     firstname.lastname@naverlabs.com
                                      Abstract
                  Panoptic segmentation in 3D is a fundamental problem
               in scene understanding. Existing approaches typically rely
               on costly test-time optimizations (often based on NeRF) to
               consolidate 2D predictions of off-the-shelf panoptic seg-
               mentation methods into 3D. Instead, in this work, we pro-     Figure 1. PanSt3R jointly predicts 3D geometry and panoptic seg-
               pose a unified and integrated approach PanSt3R, which         mentation of a scene in a single forward pass.
               eliminates the need for test-time optimization by jointly
               predicting 3D geometry and multi-view-consistent panop-       els [6, 56, 60, 65].
               tic segmentation in a single forward pass. Our approach          Several recent works [21, 27, 45, 51, 66] have extended
               harnesses the 3D representations of MUSt3R, a recent scal-    panoptic segmentation to 3D scenes represented as point
               able multi-view version of DUSt3R, and 2D representa-         clouds, meshes or voxels. These methods typically take a
               tions of DINOv2, then performs joint multi-view panoptic      3D representation (e.g.a point cloud) as input and label it
               prediction via a mask transformer architecture.   We ad-      using neural networks, such as PointNet [41, 42], designed
               ditionally revisit the standard post-processing mask merg-    for direct operation on such data. However, acquiring dense
               ing procedure and introduce a more principled approach        and accurate point clouds requires dedicated sensors and
               for multi-view segmentation. We also introduce a simple       recent models [26, 49, 55, 58, 61] struggle with noisy or
               method for generating novel-view predictions based on the     sparse point clouds derived from unposed images.
               predictions of PanSt3R and vanilla 3DGS. Overall, the pro-       Instead, in this work, we propose to jointly perform 3D
               posed PanSt3R is conceptually simple yet fast and scal-       reconstruction and panoptic segmentation given an uncon-
               able, and achieves state-of-the-art performance on several    strained set of unposed images or video frames. In this
               benchmarks, while being orders of magnitude faster. More      sense our method is closer to NeRF-based [2, 16, 25, 50,
               information and examples available on our project page.       57, 75] or 3DGS-based methods [62] that start from a col-
                                                                             lection of images. These approaches typically rely on posed
               1. Introduction                                               images and off-the-shelf 2D panoptic segmentation mod-
                                                                             els [15], followed by lifting and fusing the 2D panoptic pre-
               Robust understanding of the semantics of 3D scenes is key     dictions to 3D via NeRFs [36] or 3DGS [22].
               to many applications like virtual reality, robot navigation,     While this allows for aggregation of potentially incon-
               or autonomous driving. Such use cases require an accurate     sistent and noisy 2D panoptic labels from multiple images
               decomposition of the 3D environment into separate object      into consistent 3D labels, these methods have several limi-
               instances of known classes. In 2D vision, this joint task     tations: (1) they depend on accurate camera poses, (2) they
               of semantic and instance segmentation, denoted as panop-      require costly test-time optimization to align 2D segmen-
               tic image segmentation [24], consists of instance segmen-     tations with 3D geometry, and (3) they inherently separate
               tation of things classes (i.e.countable objects such as cars) the 2D segmentation and 3D reconstruction pipelines, po-
               andsemanticsegmentationofstuff classes (i.e.uncountable       tentially sacrificing efficiency and accuracy.
               classes such as road or sky). Following [24], a large num-       We argue that 3D reconstruction and 3D panoptic seg-
               ber of solutions have been proposed for 2D panoptic seg-      mentation are two intrinsically connected tasks, both in-
               mentation, based on CNNs [8, 30, 33, 37, 64], Transform-      volving reasoning in terms of 3D geometry of the scene
               ers [14, 15, 29, 43, 67, 79], or more recently diffusion mod- and its instance decomposition. Therefore, we propose to
                                                                        5856
