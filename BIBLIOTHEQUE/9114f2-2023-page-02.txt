2 Methods

We perform a comparison of outcome and process supervision, following a sim-
ilar methodology to Uesato et al. (2022). Outcome supervision can be provided
without humans, since all problems in the MATH dataset have automatically
checkable answers. In contrast, there is no simple way to automate process su-
pervision. We therefore rely on human data-labelers to provide process super-
vision, specifically by labelling the correctness of each step in model-generated
solutions.

We conduct experiments in two separate regimes: large-scale and small-
scale. Each has its own advantages, and they offer complimentary perspectives.
At large-scale, we finetune all models from GPT-4 (OpenAI, 2023). We focus
on advancing the state-of-the-art by training the most reliable ORM and PRM
possible. Unfortunately the training sets for these reward models are not directly
comparable, for reasons we will discuss in Section 3. These models are therefore
not ideal for making an apples-to-apples comparison of outcome and process
supervision. To address this flaw, we also train models at small-scale, where
we can conduct a more direct comparison. In order to remove our dependence
on costly human feedback, we use a large-scale model to supervise small-scale
model training. This setup enables us to conduct several important ablations
that would otherwise be infeasible.

2.1 Scope

At each model scale, we use a single fixed model to generate all solutions. We
call this model the generator. We do not attempt to improve the generator with
reinforcement learning (RL). When we discuss outcome and process supervision,
we are specifically referring to the supervision given to the reward model. We do
not discuss any supervision the generator would receive from the reward model
if trained with RL. Although finetuning the generator with RL is a natural next
step, it is intentionally not the focus of this work.

We instead focus exclusively on how to train the most reliable reward model
possible. We evaluate a reward model by its ability to perform best-of-N search
over uniformly sampled solutions from the generator. For each test problem we
select the solution ranked highest by the reward model, automatically grade it
based on its final answer, and report the fraction that are correct. A reward
model that is more reliable will select the correct solution more often.

2.2 Base Models

All large-scale models are finetuned from the base GPT-4 model (OpenAI, 2023).
This model has been pretrained solely to predict the next token; it has not been
pretrained with any Reinforcement Learning from Human Feedback (RLHF)
(Christiano et al., 2017). The small-scale base models are similar in design to
GPT-4, but they were pretrained with roughly 200 times less compute. As an
additional pretraining step, we finetune all models on a dataset of roughly 1.5B
