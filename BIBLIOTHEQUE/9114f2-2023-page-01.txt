One effective method involves training reward models to discriminate be-

tween desirable and undesirable outputs. The reward model can then be used

in a reinforcement learning pipeline (Ziegler et al., 2019; Stiennon et al
Nakano et al., 2021; Ouyang et al., 2022) or to perform search via rej
pling (Nichols et al., 2020; Shen et al., 2021; Cobbe et al., 2021). Whi

., 2020;
ection sam-
e these

techniques are useful, the resulting system is only as reliable as the reward

model itself. It is therefore important that we study how to mos

train reliable reward models.

In closely related work, Uesato et al. (2022) describe two distinc
ods for training reward models: outcome supervision and proces

effectively

meth-
supervision.

Outcome-supervised reward models (ORMs) are trained using only the final

result of the modelâ€™s chain-of-thought, while proce:

supervised reward models

(PRMs) receive feedback for each step in the chain-of-thought. There are com-
pelling reasons to favor process supervision. It provides more precise feedback,
since it specifies the exact location of any errors that occur. It also has sev-
eral advantages relevant to AI alignment: it is easier for humans to interpret,

and it more directly rewards models for following a human-endorsed chain-of
thought. Within the domain of logical reasoning, models trained with outcome
supervision regularly use incorrect reasoning to reach the correct final answer

(Zelikman et al., 2022; Creswell et al., 2022). Proces
shown to mitigate

1is misaligned behavior (Uesato et al., 2022).

supervision has been

Despite these advantages, Uesato et al. (2022) found that outcome supervi-

sion and process supervision led to similar final performance in the domain of
grade school math. We conduct our own detailed comparison of outcome and
process supervision, with three main differences: we use a more capable base

model, we use significantly more human feedback, and we train and test on the

more challenging MATH dataset (Hendrycks et al., 2021).
Our main contributions are as follows:

1. We show that process supervision can train much more reliable reward

models than outcome supervision. We use our state-of-the-art

set.

2. We show that a large reward model can reliably approximate hu:
pervision for smaller reward models, and that it can be used to efficiently

conduct large-scale data collection ablations.

3. We show that active learning leads to a 2.6x improvement in
efficiency of process supervision.

4. We release our full process supervision dataset, PRM800K, to
related research.

PRM to
solve 78.2% of problems from a representative subset of the MATH test

man su-

he data

promote
