                             Published as a conference paper at ICLR 2021
                                              Dataset          EfÔ¨Åcientnet-b7        EfÔ¨Åcientnet-b7       EfÔ¨Åcientnet-b7
                                                              +SAM(optimal)        +SAM(œÅ=0.05)
                                         FGVC Aircraft              6.80                   7.06                8.15
                                             Flowers                0.63                   0.81                1.16
                                        Oxford IIIT Pets            3.97                   4.15                4.24
                                          Stanford Cars             5.18                   5.57                5.94
                                            CIFAR-10                0.88                   0.88                0.95
                                           CIFAR-100                7.44                   7.56                7.68
                                             Birdsnap               13.64                 13.64                14.30
                                             Food101                7.02                   7.06                7.17
                                       Table 10: Results for the Ô¨Ånetuning experiments, using œÅ = 0.05 for all datasets.
                                 0.10      order                                     )1.0
                                           second                                     t
                                 0.08                                               d ,0.8
                                           first                                    n v
                                                                                    2 d
                                 0.06      metric                                    wa0.6
                                           train_error_rate                          ,t
                                           test_error_rate                            ,
                                                                                    h v
                                                                                    t d0.4
                                 0.04                                               1wa
                                                                                     (0.2       order
                                                                                     s
                                Train error rate0.02                                 o          second
                                                                                     c0.0       first
                                 0.00    10000      20000     30000      40000             0      10000    20000    30000    40000
                                                       step                                                step
                             Figure 4: Training and test error for the Ô¨Årst      Figure 5: Cosine similarity between the Ô¨Årst and
                             and second order version of the algorithm.          second order updates.
                             with the test accuracy and the estimated sharpness (max L(w + ) ‚àí L(w)) at the end of training
                             in Table 11; we report means and standard deviations across 20 runs.
                             For most of the training, one projected gradient step (as used in standard SAM) is sufÔ¨Åcient to
                             obtain a good approximation of the  found with multiple inner maximization steps. We however
                             observe that this approximation becomes weaker near convergence, where doing several iterations
                             of projected gradient ascent yields a better  (for example, on CIFAR-10, the maximum loss found
                             oneachbatchisabout3%morewhendoing5stepsofinnermaximization,comparedtowhendoing
                             a single step). That said, as seen in Table 11, the test accuracy is not strongly affected by the number
                             of inner maximization iterations, though on CIFAR-100 it does seem that several steps outperform
                             a single step in a statistically signiÔ¨Åcant way.
                              Numberofprojected                    CIFAR-10                              CIFAR-100
                                  gradient steps        Test error   Estimated sharpness      Test error    Estimated sharpness
                                         1             2.77               0.17               16.72               0.82
                                                            ¬±0.03              ¬±0.03               ¬±0.08              ¬±0.05
                                         2             2.76               0.82               16.59               1.83
                                                            ¬±0.03              ¬±0.03               ¬±0.08              ¬±0.05
                                         3             2.73               1.49               16.62               2.36
                                                            ¬±0.04              ¬±0.05               ¬±0.09              ¬±0.03
                                         5             2.77               2.26               16.60               2.82
                                                            ¬±0.03              ¬±0.05               ¬±0.06              ¬±0.04
                             Table 11: Test error rate and estimated sharpness (max L(w+)‚àíL(w)) at the end of the training.
                                                                                19
