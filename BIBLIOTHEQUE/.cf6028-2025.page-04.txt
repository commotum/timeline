                           Algorithm1FirstFinishSearch-k(FFS-k)              Algorithm2LastFinishSearch-k(LFS-k)
                           Require: Model M, prompt x, number of sam-        Require: Model M, prompt x, number of sam-
                               ples N, filter size k                             ples N, filter size k
                           Ensure: Final answer y∗                           Ensure: Final answer y∗
                            1: Generate N outputs {y1,...,yN} in parallel     1: Generate N outputs {y1,...,yN} in parallel
                            2: Stop as soon as k traces are complete          2: Sort completed outputs by trace length (de-
                            3: Select these k traces {y(1),...,y(k)}             scending)
                            4: Extract final answers from these k traces      3: Select longest k traces {y(1),...,y(k)}
                            5: return majority-voted answer among them        4: Extract final answers from these k traces
                                                                              5: return majority-voted answer among them
                           but introduces biases in gradient normalization, leading to uneven penalization across
                           trace lengths. R1-32B is a distilled 32B-parameter variant of DeepSeek-R1 that inherits
                           its reasoning-centric behavior, exhibiting similar trace-length-dependent trends at reduced
                           capacity. QwQ-32Bisareasoning-focusedmodelfromQwenthatleveragesstrongerMoE
                           routing, typically producing shorter and more compact reasoning traces. GPT-OSS-120B
                           is a large open-source GPT-style modeltrainedwithextensivereasoningsupervision,serv-
                           ing as a transparent large-scale baseline. Qwen3-32B belongs to the Qwen3 family and
                           emphasizes diverse reasoning domains—STEM, code, and commonsense—yielding qual-
                           itatively distinct reasoning patterns from DeepSeek models. DAPO-32B is a RL-trained
                           reasoning model based on the DAPO algorithm, an open-source alternative to GRPO that
                           claims to mitigate its gradient normalization bias while maintaining sample efficiency.
                           Non-reasoning models. Qwen3-235B-Instruct is a large instruction-tuned MoE model
                           (235B total, 22B active parameters) without explicit reasoning supervision, producing flu-
                           ent but unstructured responses. DeepSeek-Chat is the general-purpose conversational
                           model from DeepSeek, optimized for dialogue and summarization rather than multi-step
                           reasoning, allowing us to assess the impact of TTS on models without reasoning-centric
                           training.
                           2.3  Datasets
                           We evaluate models on two complementary reasoning benchmarks—AIME and GPQA
                           Diamond—whichtogether cover both symbolic-numerical and conceptual reasoning do-
                           mains.
                           TheAmericanInvitational Mathematics Examination (AIME) is a high-school level con-
                           test assessing symbolic and arithmetic reasoning through 30 short-answer problems, each
                           with an integer solution between 0 and 999. We use three recent variants—AIME 2024,
                           AIME 2025-I, and AIME 2025-II—to test consistency across different years and ques-
                           tion distributions. Each problem is formatted as a concise natural-language prompt, and
                           models are instructed to output the final answer within “\boxed” for consistent evalua-
                           tion. AIMEproblemstypicallyrequiremulti-stepdeductivereasoning,involvingalgebraic
                           or combinatorial manipulation, making them ideal for analyzing the accuracy-efficiency
                           trade-offs of TTS strategies.
                           GPQADiamond (Rein et al., 2023) is a graduate-level benchmark designed to test con-
                           ceptual and factual reasoning across physics, biology, and chemistry. Each question is
                           multiple-choice with four options (A–D), and models must output the selected answer
                           in a “\boxed” format for standardized parsing. We employ the Diamond subset, the most
                           challenging and expert-verified split, emphasizing high conceptual depth and factual pre-
                           cision. In contrast to AIME’s numerical reasoning focus, GPQA Diamond evaluates ab-
                           stract and knowledge-grounded reasoning. Together, they provide a comprehensive view
                           of reasoning performance across mathematical, symbolic, and conceptual domains.
                           All model- and dataset-specific hyperparameters are listed in Appendix A.
                           2.4  Metrics
                           Accuracy.    Thismetricisdefinedastheproportionofgeneratedtraceswhosefinalpredic-
                           tion matches the ground-truth answer. Even when parsing is reliable, and there is only a
                                                                           4
