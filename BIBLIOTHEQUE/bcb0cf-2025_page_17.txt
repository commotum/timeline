                                     Published as a conference paper at ICLR 2025
                                     Algorithm 1 Training Procedure for the MIND Model
                                                                                           M
                                     Require: Training data {(x ,y )}                            , initialized parameters θ , θ , hyperparameters λ, β, γ, δ, τ
                                                                                  i    i   i=1                                          P I
                                     Ensure: Trained model parameters
                                       1: for each epoch do
                                       2:         for each minibatch B ⊂ {1,...,M} do
                                       3:               ForwardPass:
                                       4:               for each sample i ∈ B do
                                       5:                    Computeactivations A in the prediction network
                                                                                                  i
                                       6:                    Computelogits z using the introspection network
                                                                                         i
                                       7:                    Computeselection probabilities p                          via Gumbel-Softmax (Eq. 6)
                                                                                                                   i,l
                                       8:                    Obtain relaxed layer selection variables m
                                                                                                                                i,l
                                       9:                    Process x through selected layers with fixed-point iterations to get yˆ
                                     10:                endfor               i                                                                                            i
                                     11:                ComputeLoss:
                                     12:                ComputeL
                                                                          pred
                                     13:                ComputeL                      using Eq. 5
                                                                          introspect
                                     14:                Computetotal loss L                    using Eq. 4
                                                                                         total
                                     15:                BackwardPass:
                                     16:                Computegradients w.r.t. θ                    and θ using phantom gradients
                                     17:                ParameterUpdate:                         P            I
                                     18:                Update θP and θI using an optimizer (e.g., Adam)
                                     19:          endfor
                                     20: end for
                                     Similarly, the gradient with respect to the input z(0) is approximated as:
                                                                                                                         
                                                                                                   ∂L               ∂L ⊤∂f(z∗;θ )
                                                                                                            ≈                                 l   .                                                      (13)
                                                                                                  ∂z(0)            ∂z∗               ∂z(0)
                                     These approximations treat the fixed-point iteration as a feedforward layer during backpropagation, enabling
                                     efficient gradient computation without unrolling the iterations or computing inverse Jacobians. Phantom
                                     gradients have been shown to be effective in training implicit models Dupont et al. (2024). They simplify the
                                     backward pass while maintaining sufficient gradient accuracy for effective optimization.
                                     A.1       GRADIENT FLOW IN THE MIND MODEL
                                     In the MIND model, gradient flow is bifurcated between the prediction network and the introspection network.
                                     For the introspection network, the gradient ∇                                                   Lisisolated from affecting the prediction net-
                                                                                                              introspection model
                                     work’s prediction and is computed independently as:
                                     ∇                         L=                  ∂L                   where W                              are the weights of the introspection network.
                                        introspection model            ∂W                                             introspection model
                                                                             introspection model
                                                                                                                                                                                                         (14)
                                     Following Arya et al. (2022), we employ automatic differentiation that caters to the discrete randomness
                                     introduced by layer selection. The gradients are computed as:
                                                                                                    ∇           L=E ∂L                                                                                 (15)
                                                                                                        discrete              ∂W
                                                                                                                                    discrete
                                     We draw from Bolte et al. (2022) to handle nonsmooth iterative algorithms, using subdifferentials ∂ to
                                     calculate the gradients as:
                                                                                                  ∇               L=∂L(W                         )                                                       (16)
                                                                                                     nonsmooth                      nonsmooth
                                                                                                                 17
