                                                  TheThirty-SixthAAAIConferenceonArtificialIntelligence(AAAI-22)
                                 JFB:Jacobian-Free Backpropagation for Implicit Networks
                                       *1                        *2               3                         4                     4                3
                 SamyWuFung, HowardHeaton, QiuweiLi, DanielMcKenzie, StanleyOsher, WotaoYin
                                        1 Department of Applied Mathematics and Statistics, Colorado School of Mines
                                                                  2 Typal Research, Typal LLC
                                                             3 Alibaba Group (US), Damo Academy
                                              4 Department of Mathematics, University of California, Los Angeles
                              swufung@mines.edu, research@typal.llc, li.qiuwei@alibaba-inc.com, mckenzie@math.ucla.edu
                                          Abstract                                          input data    latent variable   output inference
                  Apromising trend in deep learning replaces traditional feed-                          QΘ               SΘ        y
                  forward networks with implicit networks. Unlike traditional                    d                u
                  networks, implicit networks solve a ﬁxed point equation to
                  compute inferences. Solving for the ﬁxed point varies in                         QΘ                 S
                  complexity, depending on provided data and an error tol-                                              Θ
                  erance. Importantly, implicit networks may be trained with                           RΘ
                  ﬁxedmemorycostsinstarkcontrasttofeedforwardnetworks,                             uk            u?      latent variable
                  whosememoryrequirementsscalelinearly with depth. How-                                            d
                  ever, there is no free lunch — backpropagation through im-                                             feedforward network
                  plicit networks often requires solving a costly Jacobian-based                loop until               implicit network
                  equation arising from the implicit function theorem. We pro-                convergence
                  pose Jacobian-Free Backpropagation (JFB), a ﬁxed-memory
                  approach that circumvents the need to solve Jacobian-based          Figure 1: Feedforward networks act by computing SΘ◦QΘ.
                  equations. JFB makes implicit networks faster to train and          Implicit networks add a ﬁxed point condition using RΘ.
                  signiﬁcantly easier to implement, without sacriﬁcing test ac-       WhenRΘiscontractive (more generally: averaged) repeat-
                  curacy. Our experiments show implicit networks trained with         edly applying R     to update a latent variable uk converges
                  JFBarecompetitivewithfeedforwardnetworksandpriorim-                                  Θ
                                                                                      to a ﬁxed point u? = RΘ(u?;QΘ(d)).
                  plicit networks given the same number of parameters.
                                      Introduction                                    which is illustrated by the red arrows in Figure 1. One can
                                                                                      allow for computation in the latent space U by introducing a
               Anewdirection has emerged from explicit to implicit neu-               self-map RΘ(·;QΘ(d)) and the iteration
               ral networks (Winston and Kolter 2020; Bai, Kolter, and                                  uk+1 = R (uk;Q (d)).                      (2)
               Koltun2019;Bai,Koltun,andKolter2020;Chenetal.2018;                                                  Θ        Θ
               Ghaoui et al. 2019; Dupont, Doucet, and Teh 2019; Jeon,                Iterating k times may be viewed as a weight-tied, input-
               Lee,andChoi2021;Zhangetal.2020;Lawrenceetal.2020;                      injected network, where each feedforward step applies RΘ
               RevayandManchester2020;Looketal.2020;Gould,Hart-                       (Bai, Kolter, and Koltun 2019). As k → ∞, i.e. the la-
               ley, and Campbell 2019). In the standard feedforward set-              tent space portion becomes deeper, the limit of (2) yields a
               ting, a network prescribes a series of computations that map           ﬁxed point equation. Implicit networks capture this “inﬁnite
               input data d to an inference y. Networks can also explic-              depth” behaviour by using RΘ(· ;QΘ(d)) to deﬁne a ﬁxed
               itly leverage the assumption that high dimensional signals             point condition rather than an explicit computation:
               typically admit low dimensional representations in some la-
               tent space (Van der Maaten and Hinton 2008; Osher, Shi,                   N (d) , S (u?) where u? = R (u?,Q (d)),                  (3)
                                                                                           Θ          Θ d               d      Θ d Θ
                                    ´
               and Zhu 2017; Peyre 2009; Elad, Figueiredo, and Ma 2010;               as shown by blue in Figure 1. Special cases of the network
               Udell and Townsend 2019). This may be done by designing                in (3) recover architectures introduced in prior works:
               the network to ﬁrst map data to a latent space via a mapping
               QΘandthenapply a second mapping SΘ to map the latent                   B Taking SΘ to be the identity recovers the well-known
               variable to the inference. Thus, a traditional feedforward EΘ             DeepEquilibriumModel(DEQ)(Bai,Kolter,andKoltun
               maytakethecompositional form                                              2019; Bai, Koltun, and Kolter 2020).
                                                                                      B ChoosingS astheidentity, Q tobeanafﬁnemapand
                                   E (d) = S (Q (d)),                      (1)                       Θ                    Θ
                                    Θ          Θ   Θ                                     R (u,Q (d)) = σ(Wu+Q (d))yieldsMonotoneOp-
                                                                                           Θ       Θ                    Θ
                  *These authors contributed equally.                                    erator Networks(WinstonandKolter2020)aslongasW
               Copyright © 2022, Association for the Advancement of Artiﬁcial            and σ satisfy additional conditions. Allowing S        to be
                                                                                                                                             Θ
               Intelligence (www.aaai.org). All rights reserved.                         linear yields the model proposed in (Ghaoui et al. 2019).
                                                                               6648
