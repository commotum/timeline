                        PublishedasaconferencepaperatICLR2019
                        APPENDIXD DESCRIPTIONOFSOMEOFTHETASKS/DATASETS
                        Here,weprovidesomeadditionaldetailsonthebAbI,subject-verbagreement,LAMBADAlanguagemodeling,
                        andlearningtoexecute(LTE)tasks.
                        D.1   BABIQUESTION-ANSWERING
                        ThebAbiquestion answering dataset (Weston et al., 2015) consists of 20 different synthetic tasks7. The aim
                        is that each task tests a unique aspect of language understanding and reasoning, including the ability of: reasoning
                        from supporting facts in a story, answering true/false type questions, counting, understanding negation and
                        indeﬁniteknowledge,understandingcoreferences,timereasoning,positionalandsizereasoning,path-ﬁnding,and
                        understandingmotivations(toseeexamplesforeachofthesetasks,pleaserefertoTable1in(Westonetal.,2015)).
                        There are two versions of the dataset, one with 1k training examples and the other with 10k examples. It is
                        importantforamodeltobedata-efﬁcienttoachievegoodresultsusingonlythe1ktrainingexamples. Moreover,
                        the original idea is that a single model should be evaluated across all the tasks (not tuning per task), which is
                        thetrainjoint setupinTable1,andthetablespresentedinAppendixE.
                        D.2   SUBJECT-VERBAGREEMENT
                        Subject-verbagreementisthetaskofpredictingnumberagreementbetweensubjectandverbinEnglishsentences.
                        Succeedinginthistaskisastrongindicatorthatamodelcanlearntoapproximatesyntacticstructureandtherefore
                        it was proposedbyLinzenetal.(2016)asproxyforassessingtheabilityofdifferentmodelstocapturehierarchical
                        structure in natural language.
                        Twoexperimentalsetups were proposed by Linzen et al. (2016) for training a model on this task: 1) training
                        with a language modeling objective, i.e., next word prediction, and 2) as binary classiﬁcation, i.e. predicting
                        thenumberoftheverbgiventhesentence. Inthispaper,weusethelanguagemodelingobjective,meaningthat
                        weprovide the model with an implicit supervision and evaluate based on the ranking accuracy of the correct
                        formoftheverbcomparedtotheincorrectformoftheverb.
                        In this task, in order to have different levels of difﬁculty, “agreement attractors” are used, i.e. one or more
                        interveningnounswiththeoppositenumberfromthesubjectwiththegoalofconfusingthemodel. Inthiscase,
                        themodelneedstocorrectlyidentifytheheadofthesyntacticsubjectthatcorrespondstoagivenverbandignore
                        theinterveningattractorsinordertopredictthecorrectformofthatverb. Herearesomeexamplesforthistask
                        in whichsubjectsandthecorrespondingverbsareinboldfaceandagreementattractorsareunderlined:
                         No attractor:        The boy smiles.
                         One attractor:       The number of men is not clear.
                         Two attractors:      The ratio of men to women is not clear.
                         Three attractors:    The ratio of men to women and children is not clear.
                        D.3   LAMBADALANGUAGEMODELING
                        The LAMBADAtask(Papernoetal., 2016) is a broad context language modeling task. In this task, given a
                        narrative passage, the goal is to predict the last word (target word) of the last sentence (target sentence) in the
                        passage. Thesepassagesarespeciﬁcallyselectedinawaythathumansubjectsareeasilyabletoguesstheirlast
                        wordiftheyareexposedtoalongpassage,butnotiftheyonlyseethetargetsentenceprecedingthetargetword8.
                        Hereisasamplefromthedataset:
                         Context:
                                             “Yes, I thought I was going to lose the baby.”
                                             “I was scared too,” he stated, sincerity flooding his eyes.
                                             “You were?” “Yes, of course. Why do you even ask?”
                                             “This baby wasn’t exactly planned for.”
                         Target sentence:
                                             “Do you honestly think that I would want you to have a ________?”
                         Target word:
                                             miscarriage
                        TheLAMBADAtaskconsistsinpredictingthetargetwordgiventhewholepassage(i.e.,thecontextplusthe
                        target sentence). A“controlset”isalsoprovidedwhichwasconstructedbyrandomlysamplingpassagesofthe
                        sameshapeandsizeastheonesusedtobuildLAMBADA,butwithoutﬁlteringtheminanyway. Thecontrol
                           7https://research.fb.com/downloads/babi
                           8http://clic.cimec.unitn.it/lambada/appendix_onefile.pdf
                                                                  15
