                             PublishedasaconferencepaperatICLR2019
                             5    CONCLUSION
                             This paper introduces the Universal Transformer, a generalization of the Transformer model that
                             extendsitstheoreticalcapabilitiesandproducesstate-of-the-artresultsonawiderangeofchallenging
                             sequence modeling tasks, such as language understanding but also a variety of algorithmic tasks,
                             thereby addressing a key shortcoming of the standard Transformer. The Universal Transformer
                             combinesthefollowingkeypropertiesintoonemodel:
                             Weightsharing: FollowingintuitionsbehindweightsharingfoundinCNNsandRNNs,weextendthe
                             Transformerwithasimpleformofweightsharingthatstrikesaneffectivebalancebetweeninductive
                             biasandmodelexpressivity,whichweshowextensivelyonbothsmallandlarge-scaleexperiments.
                             Conditionalcomputation: Inourgoaltobuildacomputationallyuniversalmachine,weequippedthe
                             UniversalTransformerwiththeabilitytohaltorcontinuecomputationthrougharecentlyintroduced
                             mechanism,whichshowsstrongerresultscomparedtotheﬁxed-depthUniversalTransformer.
                             Weareenthusiasticabouttherecentdevelopmentsonparallel-in-timesequencemodels. Byadding
                             computationalcapacityandrecurrenceinprocessingdepth,wehopethatfurtherimprovementsbeyond
                             the basic Universal Transformer presented here will help us build learning algorithms that are both
                             morepowerful,dataefﬁcient,andgeneralizebeyondthecurrentstate-of-the-art.
                             The code used to train and evaluate Universal Transformers is available at https:
                             //github.com/tensorflow/tensor2tensor(Vaswanietal.,2018).
                             Acknowledgements WearegratefultoAshishVaswani,DouglasEck,andDavidDohanfortheir
                             fruitful commentsandinspiration.
                             REFERENCES
                             KarimAhmed,NitishShirishKeskar,andRichardSocher. Weightedtransformernetworkformachinetranslation.
                               arXivpreprintarXiv:1711.02132,2017.
                             JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprintarXiv:1607.06450,
                               2016. URLhttp://arxiv.org/abs/1607.06450.
                             DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointlylearningtoalign
                               andtranslate. CoRR,abs/1409.0473,2014. URLhttp://arxiv.org/abs/1409.0473.
                             KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,andYoshuaBengio.
                               Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR,
                               abs/1406.1078,2014. URLhttp://arxiv.org/abs/1406.1078.
                             Francois Chollet.   Xception: Deep learning with depthwise separable convolutions.       arXiv preprint
                               arXiv:1610.02357,2016.
                             Zewei Chu, Hai Wang, Kevin Gimpel, and David McAllester. Broad context language modeling as reading
                               comprehension. In Proceedings of the 15th Conference of the European Chapter of the Association for
                               ComputationalLinguistics: Volume2,ShortPapers,volume2,pp.52–57,2017.
                             BhuwanDhingra,ZhilinYang,WilliamWCohen,andRuslanSalakhutdinov. Linguisticknowledgeasmemory
                               for recurrent neural networks. arXiv preprint arXiv:1703.02620,2017.
                             Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Neural models for
                               reasoningovermultiplementionsusingcoreference. arXivpreprintarXiv:1804.05922,2018.
                             Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence
                               to sequencelearning. CoRR,abs/1705.03122,2017. URLhttp://arxiv.org/abs/1705.03122.
                             EdouardGrave,ArmandJoulin,andNicolasUsunier. Improvingneurallanguagemodelswithacontinuouscache.
                               arXivpreprintarXiv:1612.04426,2016.
                             Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL
                               http://arxiv.org/abs/1308.0850.
                             AlexGraves. Adaptivecomputationtimeforrecurrentneuralnetworks. arXivpreprintarXiv:1603.08983,2016.
                                                                               10
