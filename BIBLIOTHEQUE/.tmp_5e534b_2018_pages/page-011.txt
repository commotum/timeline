                           PublishedasaconferencepaperatICLR2019
                           Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014. URL
                             http://arxiv.org/abs/1410.5401.
                           CaglarGulcehre,MishaDenil,MateuszMalinowski,AliRazavi,RazvanPascanu,KarlMoritzHermann,Peter
                             Battaglia, Victor Bapst, DavidRaposo,AdamSantoro,etal. Hyperbolicattentionnetworks. arXivpreprint
                             arXiv:1805.09786,2018.
                           MikaelHenaff,JasonWeston,ArthurSzlam,AntoineBordes,andYannLeCun. Trackingtheworldstatewith
                             recurrententitynetworks. arXivpreprintarXiv:1612.03969,2016.
                           SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. Gradientﬂowinrecurrentnets: the
                             difﬁcultyoflearninglong-termdependencies. AFieldGuidetoDynamicalRecurrentNeuralNetworks,2003.
                           A.JoulinandT.Mikolov. Inferringalgorithmicpatternswithstack-augmentedrecurrentnets. InAdvancesin
                             NeuralInformationProcessingSystems,(NIPS),2015.
                           Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning
                             Representations(ICLR),2016. URLhttps://arxiv.org/abs/1511.08228.
                           ŁukaszKaiser,AidanN.Gomez,andFrancoisChollet. Depthwiseseparableconvolutionsforneuralmachine
                             translation. CoRR,abs/1706.03059,2017. URLhttp://arxiv.org/abs/1706.03059.
                           AnkitKumar,OzanIrsoy,PeterOndruska,MohitIyyer,JamesBradbury,IshaanGulrajani,VictorZhong,Romain
                             Paulus,andRichardSocher. Askmeanything: Dynamicmemorynetworksfornaturallanguageprocessing.
                             InInternationalConferenceonMachineLearning,pp.1378–1387,2016.
                           ZhouhanLin,MinweiFeng,CiceroNogueiradosSantos,MoYu,BingXiang,BowenZhou,andYoshuaBengio.
                             Astructuredself-attentivesentenceembedding. arXivpreprintarXiv:1703.03130,2017.
                           Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of lstms to learn syntax-sensitive
                             dependencies. TransactionsoftheAssociationofComputationalLinguistics,4(1):521–535,2016.
                           DenisPaperno,GermánKruszewski,AngelikiLazaridou,NgocQuanPham,RaffaellaBernardi,SandroPezzelle,
                             MarcoBaroni,GemmaBoleda,andRaquelFernandez. Thelambadadataset: Wordpredictionrequiringa
                             broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational
                             Linguistics (Volume1: LongPapers),volume1,pp.1525–1534,2016.
                           Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.        A decomposable atten-
                             tion model.    In Empirical Methods in Natural Language Processing, 2016.       URL https:
                             //arxiv.org/pdf/1606.01933.pdf.
                           Jack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex Graves,
                             andTimLillicrap. Scalingmemory-augmentedneuralnetworkswithsparsereadsandwrites. InAdvances
                             in NeuralInformationProcessingSystems,pp.3621–3629,2016.
                           Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Query-reduction networks for question
                             answering. arXivpreprintarXiv:1606.04582,2016.
                           Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
                             a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research, 15(1):
                             1929–1958,2014.
                           Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.   End-to-end memory networks.
                             In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in
                             Neural Information Processing Systems 28, pp. 2440–2448. Curran Associates, Inc., 2015.  URL
                             http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.
                           Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.    Sequence to sequence learning with neural net-
                             works.   In Advances in Neural Information Processing Systems, pp. 3104–3112, 2014.      URL
                             http://arxiv.org/abs/1409.3215.
                           KeTran, Arianna Bisazza, and Christof Monz. The importance of being recurrent for modeling hierarchical
                             structure. In ProceedingsofNAACL’18,2018.
                           AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,LukaszKaiser,and
                             Illia Polosukhin. Attention is all you need. CoRR, 2017. URLhttp://arxiv.org/abs/1706.03762.
                           Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion
                             Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit.
                             Tensor2tensorforneuralmachinetranslation. CoRR,abs/1803.07416,2018.
                                                                         11
