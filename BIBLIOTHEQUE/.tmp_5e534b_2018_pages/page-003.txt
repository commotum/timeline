                       PublishedasaconferencepaperatICLR2019
                       Ineachrecurrenttime-step,therepresentationofeverypositionisconcurrently(inparallel)revised
                       in twosub-steps: ﬁrst, using a self-attention mechanism to exchangeinformationacrossallpositions
                       in the sequence, thereby generatingavectorrepresentationforeachpositionthatisinformedbythe
                       representations of all other positions at the previous time-step. Then, by applying a transition function
                       (shared across position and time) to the outputs of the self-attention mechanism, independently at
                       eachposition. Astherecurrenttransitionfunctioncanbeappliedanynumberoftimes,thisimplies
                       that UTscanhavevariabledepth(numberofper-symbolprocessingsteps). Crucially,thisisincontrast
                       to most popular neural sequence models, including the Transformer (Vaswani et al., 2017) or deep
                       RNNs,whichhaveconstantdepthasaresultofapplyingaﬁxedstack of layers. We now describe
                       theencoderanddecoderinmoredetail.
                       ENCODER:Givenaninputsequenceoflengthm,westartwithamatrixwhoserowsareinitialized
                       asthed-dimensionalembeddingsofthesymbolsateachpositionofthesequenceH0∈Rm×d. The
                       UTtheniterativelycomputesrepresentationsHt atsteptforallmpositionsinparallelbyapplying
                       the multi-headed dot-product self-attention mechanism from Vaswani et al. (2017), followed by a
                       recurrenttransitionfunction. Wealsoaddresidualconnectionsaroundeachofthesefunctionblocks
                       andapplydropoutandlayernormalization(Srivastavaetal.,2014;Baetal.,2016)(seeFig.2fora
                       simpliﬁeddiagram,andFig.4intheAppendixAforthecompletemodel.).
                       Morespeciﬁcally, we use the scaled dot-product attention which combines queries Q, keys K and
                       valuesV asfollows
                                                                         QKT
                                            ATTENTION(Q,K,V)=SOFTMAX        √     V,                   (1)
                                                                              d
                       wheredisthenumberofcolumnsofQ,K andV. Weusethemulti-headversionwithk heads,as
                       introducedin(Vaswanietal.,2017),
                                MULTIHEADSELFATTENTION(Ht)=CONCAT(head ,...,head )WO                   (2)
                                                                             1       k
                                                    wherehead =ATTENTION(HtWQ,HtWK,HtWV)               (3)
                                                             i                  i      i      i
                       andwemapthestateHttoqueries,keysandvalueswithafﬁneprojectionsusinglearnedparameter
                       matricesWQ∈Rd×d/k,WK∈Rd×d/k,WV ∈Rd×d/kandWO∈Rd×d.
                       Atstept,theUTthencomputesrevisedrepresentationsHt∈Rm×dforallminputpositionsasfollows
                                Ht=LAYERNORM(At+TRANSITION(At))                                        (4)
                                  t                 t−1   t                               t−1   t
                          whereA =LAYERNORM((H         +P )+MULTIHEADSELFATTENTION(H         +P )),    (5)
                       whereLAYERNORM()isdeﬁnedinBaetal.(2016),andTRANSITION()andPtarediscussedbelow.
                       Depending on the task, we use one of two different transition functions: either a separable
                       convolution(Chollet,2016)orafully-connectedneuralnetworkthatconsistsofasinglerectiﬁed-linear
                       activation function between two afﬁne transformations, applied position-wise, i.e. individually to
                       eachrowofAt.
                       Pt ∈ Rm×d above are ﬁxed, constant, two-dimensional (position, time) coordinate embeddings,
                       obtainedbycomputingthesinusoidalpositionembeddingvectorsasdeﬁnedin(Vaswanietal.,2017)
                       for the positions 1≤i≤mandthetime-step1≤t≤T separatelyforeachvector-dimension1≤j≤d,
                       andsumming:
                                                t             2j/d             2j/d
                                              P   =sin(i/10000    )+sin(t/10000   )                    (6)
                                               i,2j
                                              t                2j/d            2j/d
                                            Pi,2j+1=cos(i/10000   )+cos(t/10000    ).                  (7)
                                                               3
