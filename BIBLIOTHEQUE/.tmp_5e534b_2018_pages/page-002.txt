                                         PublishedasaconferencepaperatICLR2019
                                                                                                 Parameters are tied across positions and time steps
                                                h t               Self-Attention       Transition Function       h t+1               Self-Attention       Transition Function       h t+2
                                                 1                                                                1                                                                  1
                                                h t               Self-Attention       Transition Function       h t+1               Self-Attention       Transition Function       h t+2
                                                 2                                                                2                                                                  2
                                                …                       …                      …                   …                       …                      …                  …
                                            Per Position Statesh tSelf-Attention       Transition Function       h t+1               Self-Attention       Transition Function       h t+2
                                                 m                                                                m                                                                   m
                                                                                                                         Time
                                         Figure 1: The Universal Transformer repeatedly reﬁnes a series of vector representations for each
                                         position of the sequence in parallel, by combining information from different positions using
                                         self-attention (see Eqn 2) and applying a recurrent transition function (see Eqn 4) across all time
                                         steps 1≤t≤T. Weshowthisprocessovertworecurrenttime-steps. Arrowsdenotedependencies
                                                                                        0
                                         betweenoperations. Initially, h is initialized with the embedding for each symbol in the sequence.
                                           t
                                         h represents the representation for input symbol 1≤i≤m at recurrent time-step t. With dynamic
                                           i
                                         halting, T is dynamically determinedforeachposition(Section2.2).
                                         biasmaybecrucialforseveralalgorithmicandlanguageunderstandingtasksofvaryingcomplexity:
                                         in contrast to models such as the Neural Turing Machine (Graves et al., 2014), the Neural GPU (Kaiser
                                         &Sutskever,2016)orStackRNNs(Joulin&Mikolov,2015),theTransformerdoesnotgeneralize
                                         welltoinputlengthsnotencounteredduringtraining.
                                         Inthispaper,weintroducetheUniversalTransformer(UT),aparallel-in-timerecurrentself-attentive
                                         sequencemodelwhichcanbecastasageneralizationoftheTransformermodel,yieldingincreased
                                         theoretical capabilities and improved results on a wide range of challenging sequence-to-sequence
                                         tasks. UTscombinetheparallelizabilityandglobalreceptiveﬁeldoffeed-forwardsequencemodels
                                         like the Transformer with the recurrent inductive bias of RNNs, which seems to be better suited to
                                         arangeofalgorithmicandnaturallanguageunderstandingsequence-to-sequenceproblems. Asthe
                                         nameimplies, and in contrast to the standard Transformer, under certain assumptions UTs can be
                                         showntobeTuring-complete(or“computationallyuniversal”,asshowninSection4).
                                         Ineachrecurrentstep,theUniversalTransformeriterativelyreﬁnesitsrepresentationsforallsymbols
                                         in the sequence in parallel using a self-attention mechanism (Parikh et al., 2016; Lin et al., 2017),
                                         followedbyatransformation(sharedacrossallpositionsandtime-steps)consistingofadepth-wise
                                         separable convolution (Chollet, 2016; Kaiser et al., 2017) or a position-wise fully-connected layer
                                         (seeFig1). Wealsoaddadynamicper-positionhaltingmechanism(Graves,2016),allowingthemodel
                                         to choosetherequirednumberofreﬁnementstepsforeachsymboldynamically,andshowfortheﬁrst
                                         timethatsuchaconditionalcomputationmechanismcaninfactimproveaccuracyonseveralsmaller,
                                         structured algorithmicandlinguisticinferencetasks(althoughitmarginallydegradedresultsonMT).
                                         OurstrongexperimentalresultsshowthatUTsoutperformTransformersandLSTMsacrossawide
                                         range of tasks. The added recurrence yields improved results in machine translation where UTs
                                         outperform the standard Transformer. In experiments on several algorithmic tasks and the bAbI
                                         languageunderstandingtask,UTsalsoconsistentlyandsigniﬁcantlyimproveoverLSTMsandthe
                                         standardTransformer. Furthermore,onthechallengingLAMBADAtextunderstandingdatasetUTs
                                         withdynamichaltingachieveanewstateoftheart.
                                         2      MODELDESCRIPTION
                                         2.1      THEUNIVERSALTRANSFORMER
                                         The Universal Transformer (UT; see Fig. 2) is based on the popular encoder-decoder architecture
                                         commonlyusedinmostneuralsequence-to-sequencemodels(Sutskeveretal.,2014;Choetal.,2014;
                                         Vaswanietal.,2017). BoththeencoderanddecoderoftheUToperatebyapplyingarecurrentneural
                                         networktotherepresentationsofeachofthepositionsoftheinputandoutputsequence,respectively.
                                         However,incontrasttomostapplicationsofrecurrentneuralnetworkstosequentialdata,theUTdoes
                                         notrecuroverpositionsinthesequence,butoverconsecutiverevisionsofthevectorrepresentationsof
                                         eachposition(i.e.,over“depth”). Inotherwords,theUTisnotcomputationallyboundbythenumber
                                         ofsymbolsinthesequence,butonlybythenumberofrevisionsmadetoeachsymbol’srepresentation.
                                                                                                                  2
