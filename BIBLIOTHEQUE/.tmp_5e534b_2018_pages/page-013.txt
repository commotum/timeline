                                PublishedasaconferencepaperatICLR2019
                                 APPENDIXA DETAILEDSCHEMAOFTHEUNIVERSALTRANSFORMER
                                                                                                                            Output Probabilities
                                                                                                                                 Softmax
                                                                                                                                      After T steps
                                                                                                   Recurrent      Layer Normalization
                                                                                                   Decoder
                                                                                                   Block                  +
                                                                                                                        Dropout
                                                                                                                   Transition Function
                                     Recurrent      Layer Normalization                                            Layer Normalization
                                     Encoder
                                     Block                  +                                                             +
                                                         Dropout                                                        Dropout
                                                     Transition Function           After T steps                   Multihead Attention
                                                    Layer Normalization                                           Layer Normalization
                                                            +                     For T steps                             +                     For T steps
                                                          Dropout                                                       Dropout
                                                   Multihead Self-Attention                                      Multihead Self-Attention
                                     Timestep embedding     +                                      Timestep embedding     +
                                      Position embedding    +                                       Position embedding    +
                                                   Embed Input Symbols                                           Embed Target Symbols
                                                      Input Sequence                                       Target Sequence (right-shifted by one)
                                Figure4: TheUniversalTransformerwithpositionandstepembeddingsaswellasdropoutandlayer
                                normalization.
                                 APPENDIXB ONTHECOMPUTATIONALPOWEROFUTVSTRANSFORMER
                                With respect to their computational power, the key difference between the Transformer and the Universal
                                Transformerliesinthenumberofsequentialstepsofcomputation(i.e.indepth). WhileastandardTransformer
                                executes a total number of operations that scales with the input size, the number of sequential operations is
                                constant, independentoftheinputsizeanddeterminedsolelybythenumberoflayers. Assumingï¬niteprecision,
                                this propertyimpliesthatthestandardTransformercannotbecomputationallyuniversal. Whenchoosinganumber
                                ofstepsasafunctionoftheinputlength,however,theUniversalTransformerdoesnotsufferfromthislimitation.
                                Notethatthis holds independently of whether or not adaptive computation time is employed but does assume
                                anon-constant,evenifpossiblydeterministic,numberofsteps. Varyingthenumberofstepsdynamicallyafter
                                training is enabled by sharing weights across sequential computation steps in the Universal Transformer.
                                Anintuitive example are functions whose execution requires the
                                sequential processing of each input element. In this case, for any
                                given choice of depth T, one can construct an input sequence of
                                length N > T that cannot be processed correctly by a standard
                                Transformer. Withanappropriate,input-lengthdependentchoiceof
                                sequential steps, however, a Universal Transformer, RNNsorNeural
                                GPUscanexecutesuchafunction.
                                                                                         13
