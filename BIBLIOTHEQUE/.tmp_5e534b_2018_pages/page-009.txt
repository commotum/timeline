                          PublishedasaconferencepaperatICLR2019
                                               Model                                      BLEU
                                               UniversalTransformersmall                   26.8
                                               Transformerbase(Vaswanietal.,2017)          28.0
                                               WeightedTransformerbase(Ahmedetal.,2017)    28.4
                                               UniversalTransformerbase                    28.9
                         Table7: MachinetranslationresultsontheWMT14En-Detranslationtasktrainedon8xP100GPUs
                          in comparabletrainingsetups. Allbaseresultshavethesamenumberofparameters.
                          4   DISCUSSION
                         Whenrunningforaﬁxednumberofsteps,theUniversalTransformerisequivalenttoamulti-layer
                         Transformer with tied parameters across all its layers. This is partly similar to the Recursive
                                                                                                                    5
                         Transformer,whichtiestheweightsofitsself-attentionlayersacrossdepth(Gulcehreetal.,2018) .
                          However,astheper-symbolrecurrenttransitionfunctionscanbeappliedanynumberoftimes,another
                          andpossiblymoreinformativewayofcharacterizingtheUTisasablockofparallelRNNs(onefor
                          eachsymbol,withsharedparameters)evolvingper-symbolhiddenstatesconcurrently,generatedat
                          eachstepbyattendingtothesequenceofhiddenstatesatthepreviousstep. Inthisway,itisrelated
                          to architectures such as the Neural GPU(Kaiser&Sutskever,2016)andtheNeuralTuringMachine
                         (Gravesetal.,2014). UTstherebyretaintheattractivecomputationalefﬁciencyoftheoriginalfeed-
                          forwardTransformermodel,butwiththeaddedrecurrentinductivebiasofRNNs. Furthermore,using
                          adynamichaltingmechanism,UTscanchoosethenumberofprocessingstepsbasedontheinputdata.
                         The connection between the Universal Transformer and other sequence models is apparent from
                          the architecture: if we limited the recurrent steps to one, it would be a Transformer. But it is more
                          interesting to consider the relationship between the Universal Transformer and RNNs and other
                          networkswhererecurrencehappensoverthetimedimension. Superﬁciallythesemodelsmayseem
                          closely related since they are recurrent as well. But there is a crucial difference: time-recurrent models
                          likeRNNscannotaccessmemoryintherecurrentsteps. Thismakesthemcomputationallymoresimilar
                          to automata,sincetheonlymemoryavailableintherecurrentpartisaﬁxed-sizestatevector. UTson
                          theotherhandcanattendtothewholepreviouslayer,allowingittoaccessmemoryintherecurrentstep.
                          GivensufﬁcientmemorytheUniversalTransformeriscomputationallyuniversal–i.e.itbelongsto
                          theclassofmodelsthatcanbeusedtosimulateanyTuringmachine,therebyaddressingashortcoming
                          of the standard Transformer model 6. In addition to being theoretically appealing, our results show
                          that this added expressivity also leads to improved accuracy on several challenging sequence modeling
                          tasks. This closes the gap between practical sequence modelscompetitiveonlarge-scaletaskssuch
                          as machine translation, and computationally universal models such as the Neural Turing Machine
                          ortheNeuralGPU(Gravesetal.,2014;Kaiser&Sutskever,2016),whichcanbetrainedusinggradient
                          descenttoperformalgorithmictasks.
                         Toshowthis,wecanreduceaNeuralGPUtoaUniversalTransformer. Ignoringthedecoderandpa-
                          rameterizingtheself-attentionmodule,i.e. self-attentionwiththeresidualconnection,tobetheidentity
                          function, we assume the transition function to be a convolution. If we now set the total number of
                          recurrentstepsT tobeequaltotheinputlength,weobtainexactlyaNeuralGPU.Notethatthelaststep
                          is wheretheUniversalTransformercruciallydiffersfromthevanillaTransformerwhosedepthcannot
                          scale dynamicallywiththesizeoftheinput. AsimilarrelationshipexistsbetweentheUniversalTrans-
                          formerandtheNeuralTuringMachine,whosesingleread/writeoperationsperstepcanbeexpressedby
                          theglobal,parallelrepresentationrevisionsoftheUniversalTransformer. Incontrasttothesemodels,
                          however, which only perform well on algorithmic tasks, the Universal Transformer also achieves
                          competitiveresultsonrealisticnaturallanguagetaskssuchasLAMBADAandmachinetranslation.
                         Another related model architecture is that of end-to-end Memory Networks (Sukhbaatar et al.,
                          2015). In contrast to end-to-end memory networks, however, the Universal Transformer uses
                          memorycorrespondingtostatesalignedtoindividualpositionsofitsinputsoroutputs. Furthermore,
                          the Universal Transformer follows the encoder-decoder conﬁguration and achieves competitive
                          performanceinlarge-scalesequence-to-sequencetasks.
                            5NotethatinUTboththeself-attentionandtransitionweightsaretiedacrosslayers.
                            6AppendixBillustrateshowUTiscomputationallymorepowerfulthanthestandardTransformer.
                                                                       9
