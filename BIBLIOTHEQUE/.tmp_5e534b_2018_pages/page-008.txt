                             PublishedasaconferencepaperatICLR2019
                                       Model                          Copy                Reverse              Addition
                                                               char-acc    seq-acc   char-acc   seq-acc   char-acc    seq-acc
                                       LSTM                    0.45        0.09      0.66       0.11      0.08        0.0
                                       Transformer             0.53        0.03      0.13       0.06      0.07        0.0
                                       UniversalTransformer    0.91        0.35      0.96       0.46      0.34        0.02
                                       NeuralGPU∗              1.0         1.0       1.0        1.0       1.0         1.0
                             Table4: Accuracy(higherbetter)onthealgorithmictasks. ∗NotethattheNeuralGPUwastrainedwith
                             aspecialcurriculumtoobtaintheperfectresult,whileothermodelsaretrainedwithoutanycurriculum.
                                                                      Copy                Double                Reverse
                                       Model                   char-acc    seq-acc   char-acc   seq-acc   char-acc    seq-acc
                                       LSTM                    0.78        0.11      0.51       0.047     0.91        0.32
                                       Transformer             0.98        0.63      0.94       0.55      0.81        0.26
                                       UniversalTransformer    1.0         1.0       1.0        1.0       1.0         1.0
                             Table5: Character-level(char-acc)andsequence-levelaccuracy(seq-acc)resultsontheMemorization
                             LTEtasks,withmaximumlengthof55.
                                                                    Program               Control              Addition
                                       Model                   char-acc    seq-acc   char-acc   seq-acc   char-acc    seq-acc
                                       LSTM                    0.53        0.12      0.68       0.21      0.83        0.11
                                       Transformer             0.71        0.29      0.93       0.66      1.0         1.0
                                       UniversalTransformer    0.89        0.63      1.0        1.0       1.0         1.0
                             Table 6: Character-level (char-acc) and sequence-level accuracy (seq-acc) results on the Program
                             EvaluationLTEtaskswithmaximumnestingof2andlengthof5.
                             train UTs using positions starting with randomized offsets to further encourage the model to learn
                             position-relative transformations. Results are shown in Table 4. The UT outperformsbothLSTMand
                             vanilla Transformerbyawidemarginonallthreetasks. TheNeuralGPUreportsperfectresultsonthis
                             task (Kaiser &Sutskever,2016),howeverwenotethatthisresultrequiredaspecialcurriculum-based
                             training protocol whichwasnotusedforothermodels.
                             3.5    LEARNINGTOEXECUTE(LTE)
                             As another class of sequence-to-sequence learning problems, we also evaluate UTs on tasks
                             indicatingtheabilityofamodeltolearntoexecutecomputerprograms,asproposedin(Zaremba&
                             Sutskever,2015). Thesetasksincludeprogramevaluationtasks(program,control,andaddition),and
                             memorizationtasks(copy,double,andreverse).
                             We use the mix-strategy discussed in (Zaremba & Sutskever, 2015) to generate the datasets.
                             Unlike(Zaremba&Sutskever,2015),wedonotuseanycurriculumlearningstrategyduringtraining
                             andwemakenouseoftargetsequencesattesttime. Tables5and6presenttheperformanceofan
                             LSTMmodel,Transformer,andUniversalTransformerontheprogramevaluationandmemorization
                             tasks, respectively. UT achieves perfect scores in all the memorization tasks and also outperforms
                             bothLSTMsandTransformersinallprogramevaluationtasksbyawidemargin.
                             3.6    MACHINETRANSLATION
                             WetrainedaUTontheWMT2014English-Germantranslationtaskusingthesamesetupasreportedin
                             (Vaswanietal.,2017)inordertoevaluateitsperformanceonalarge-scalesequence-to-sequencetask.
                             ResultsaresummarizedinTable7. TheUTwithafully-connectedrecurrenttransitionfunction(instead
                             ofseparableconvolution)andwithoutACTimprovesby0.9BLEUoveraTransformerand0.5BLEU
                             overaWeightedTransformerwithapproximatelythesamenumberofparameters(Ahmedetal.,2017).
                                                                                 8
