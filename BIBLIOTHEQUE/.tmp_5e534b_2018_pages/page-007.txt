                           PublishedasaconferencepaperatICLR2019
                                      Model                               Numberofattractors
                                                             0       1        2       3       4       5       Total
                                                       Previousbestresults(Yogatamaetal.,2018):
                                      BestStack-RNN          0.994   0.979    0.965   0.935   0.916   0.880   0.992
                                      BestLSTM               0.993   0.972    0.950   0.922   0.900   0.842   0.991
                                      BestAttention          0.994   0.977    0.959   0.929   0.907   0.842   0.992
                                                                       Ourresults:
                                      Transformer            0.973   0.941    0.932   0.917   0.901   0.883   0.962
                                      UniversalTransformer   0.993   0.971    0.969   0.940   0.921   0.892   0.992
                                      UTw/ACT                0.994   0.969    0.967   0.944   0.932   0.907   0.992
                                      ∆(UTw/ACT-Best)        0       -0.008   0.002   0.009   0.016   0.027   -
                                Table2: Accuracyonthesubject-verbagreementnumberpredictiontask(higherisbetter).
                              Model                               LMPerplexity&(Accuracy)                RCAccuracy
                                                               control     dev         test        control   dev      test
                             NeuralCache(Graveetal.,2016)      129         139         -           -         -        -
                             Dhingraetal.Dhingraetal.(2018)    -           -           -           -         -        0.5569
                             Transformer                       142(0.19)   5122(0.0)   7321(0.0)   0.4102    0.4401   0.3988
                             LSTM                              138(0.23)   4966(0.0)   5174(0.0)   0.1103    0.2316   0.2007
                             UTbase,6steps(ﬁxed)               131(0.32)   279(0.18)   319(0.17)   0.4801    0.5422   0.5216
                             UTw/dynamichalting                130(0.32)   134(0.22)   142(0.19)   0.4603    0.5831   0.5625
                             UTbase,8steps(ﬁxed)               129(0.32)   192(0.21)   202(0.18)   -         -        -
                             UTbase,9steps(ﬁxed)               129(0.33)   214(0.21)   239(0.17)   -         -        -
                           Table3: LAMBADAlanguagemodeling(LM)perplexity(lowerbetter)withaccuracyinparentheses
                           (higher better), and Reading Comprehension (RC) accuracy results (higher better). ‘-’ indicates no
                           reportedresultsinthatsetting.
                           modeling,andteststheabilityofamodeltoincorporatebroaderdiscourseandlongertermcontext
                           whenpredictingthetargetword.
                           The task is evaluated in two settings: as language modeling (the standard setup) and as reading
                           comprehension. In the former (more challenging) case, a model is simply trained for next-word
                           predictiononthetrainingdata,andevaluatedonthetargetwordsattesttime(i.e.themodelistrained
                           to predict all words, not speciﬁcally challenging target words). In the latter setting, introduced by Chu
                           et al. Chu et al. (2017), the target sentence (minus the last word) is used as query for selecting the target
                           wordfromthecontextsentences. Notethatthetarget word appears in the context 81% of the time,
                           makingthissetupmuchsimpler. Howeverthetaskisimpossibleintheremaining19%ofthecases.
                           The results are shown in Table 3. Universal Transformer achieves state-of-the-art results in both
                           the language modeling and reading comprehension setup, outperforming both LSTMs and vanilla
                           Transformers. NotethatthecontrolsetwasconstructedsimilartotheLAMBADAdevelopmentand
                           test sets, but without ﬁltering them in any way, so achieving good results on this set shows a model’s
                           strengthinstandardlanguagemodeling.
                           OurbestﬁxedUTresultsused6steps. However,theaveragenumberofstepsthatthebestUTwith
                                                                                                        ±
                           dynamichaltingtookonthetestdataoverallpositionsandexampleswas8.2 2.1. Inordertoseeif
                           thedynamicmodeldidbettersimplybecauseittookmoresteps,wetrainedtwoﬁxedUTmodelswith
                           8and9stepsrespectively(seelasttworows). Interestingly,thesetwomodelsachievebetterresults
                           comparedtothemodelwith6steps,butdonotoutperformtheUTwithdynamichalting. Thisleads
                           us to believe that dynamic halting may act as a useful regularizer for the model via incentivizing a
                           smallernumbersofstepsforsomeoftheinputsymbols,whileallowingmorecomputationforothers.
                           3.4   ALGORITHMICTASKS
                           Wetrained UTs on three algorithmic tasks, namely Copy, Reverse, and (integer) Addition, all on
                           strings composed of decimal symbols (‘0’-‘9’). In all the experiments, we train the models on
                           sequences of length 40 and evaluated on sequences of length 400 (Kaiser & Sutskever, 2016). We
                                                                            7
