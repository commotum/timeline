                        PublishedasaconferencepaperatICLR2019
                        Figure3: PondertimeofUTwithdynamichaltingforencodingfactsinastoryandquestioninabAbI
                        taskrequiringthreesupportingfacts.
                        oftheper-symbolrecurrentprocessingchain)overallpositionsinallsamplesinthetestdatafortasks
                                                                ±                                    ±
                        requiringthreesupportingfactsishigher(3.8 2.2)thanfortasksrequiringonlytwo(3.1 1.1),which
                                                                                      ±
                        is in turn higher than for tasks requiring only one supporting fact (2.3 0.8). This indicates that the
                        modeladjuststhenumberofprocessingstepswiththenumberofsupportingfactsrequiredtoanswer
                        the questions. Finally, we observe that the histogram of ponder times at different positions is more
                        uniformintasksrequiringonlyonesupportingfactcomparedtotwoandthree,andlikewisefortasks
                        requiringtwocomparedtothree. Especiallyfortasksrequiringthreesupportingfacts,manypositions
                        halt at step 1 or 2 already and only a few get transformed for more steps (see for example Fig 3). This
                        is particularly interesting as the length of stories is indeed much higher in this setting, with more
                        irrelevant facts which the model seemstosuccessfullylearntoignoreinthisway.
                        Similartodynamicmemorynetworks(Kumaretal.,2016),thereisaniterativeattentionprocessin
                        UTsthatallowsthemodeltoconditionitsattentionovermemoryontheresultofpreviousiterations.
                        AppendixFpresentssomeexamplesillustratingthatthereisanotionoftemporalstatesinUT,where
                        themodelupdatesitsstates(memory)ineachstepbasedontheoutputofprevioussteps,andthischain
                        ofupdatescanalsobeviewedasstepsinamulti-hopreasoningprocess.
                         3.2  SUBJECT-VERBAGREEMENT
                        Next, we consider the task of predicting number-agreement between subjects and verbs in English
                        sentences (Linzen et al., 2016). This task acts as a proxy for measuring the ability of a model to
                        capturehierarchical(dependency)structureinnaturallanguagesentences. Weusethedatasetprovided
                        by(Linzenetal.,2016)andfollowtheirexperimentalprotocolofsolvingthetaskusingalanguagemod-
                        eling training setup, i.e. a next word prediction objective, followed by calculating the ranking accuracy
                        ofthetargetverbattesttime. Weevaluatedourmodelonsubsetsofthetestdatawithdifferenttaskdif-
                        ﬁculty, measuredintermsofagreementattractors–thenumberofinterveningnounswiththeopposite
                        numberfromthesubject(meanttoconfusethemodel). Forexample,giventhesentenceThekeystothe
                        cabinet4, the objective during training is to predict the verb are (plural). At test time, we then evaluate
                        therankingaccuracyoftheagreementattractors: i.e.thegoalistorankarehigherthanisinthiscase.
                        Ourresults are summarized in Table 2. The best LSTM with attention from the literature achieves
                        99.18%onthistask(Yogatamaetal.,2018),outperformingavanillaTransformer(Tranetal.,2018).
                        UTssigniﬁcantlyoutperformstandardTransformers,andachieveanaverageresultcomparabletothe
                        currentstate of the art (99.2%). However, we see that UTs (and particularly with dynamic halting) per-
                        formprogressivelybetterthanallothermodelsasthenumberofattractorsincreases(seethelastrow,∆).
                         3.3  LAMBADALANGUAGEMODELING
                        TheLAMBADAtask(Papernoetal.,2016)isalanguagemodelingtaskconsistingofpredictinga
                        missingtargetwordgivenabroadercontextof4-5precedingsentences. Thedatasetwasspeciﬁcally
                        designedsothathumansareabletoaccuratelypredictthetargetwordwhenshownthefullcontext,
                        butnotwhenonlyshownthetargetsentenceinwhichitappears. Itthereforegoesbeyondlanguage
                           4Cabinet(singular)isanagreementattractorinthiscase.
                                                                    6
