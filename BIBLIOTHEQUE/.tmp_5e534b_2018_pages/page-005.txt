                                 PublishedasaconferencepaperatICLR2019
                                          Model                                          10Kexamples                       1Kexamples
                                                                                  train single     train joint      train single    train joint
                                                                                 Previousbestresults:
                                         QRNet(Seoetal.,2016)                     0.3(0/20)        -                -               -
                                         SparseDNC(Raeetal.,2016)                 -                2.9(1/20)        -               -
                                         GA+MAGEDhingraetal.(2017)                -                -                8.7(5/20)       -
                                         MemN2NSukhbaataretal.(2015)              -                -                -               12.4(11/20)
                                                                                      OurResults:
                                         Transformer(Vaswanietal.,2017)           15.2(10/20)      22.1(12/20)      21.8(5/20)      26.8(14/20)
                                         UniversalTransformer(thiswork)           0.23(0/20)       0.47(0/20)       5.31(5/20)      8.50(8/20)
                                         UTw/dynamichalting(thiswork)             0.21(0/20)       0.29(0/20)       4.55(3/20)      7.78(5/20)
                                 Table 1: Average error and number of failed tasks (>5% error) out of 20 (in parentheses; lower is
                                 better in both cases) on the bAbI dataset under the different training/evaluation setups. We indicate
                                 state-of-the-art where available for each, or ‘-’ otherwise.
                                 (called the “ponder time”) in standard recurrent neural networks based on a scalar halting probability
                                 predictedbythemodelateachstep.
                                 InspiredbytheinterpretationofUniversalTransformersasapplyingself-attentiveRNNsinparallel
                                 to all positions in the sequence, we also add a dynamic ACT halting mechanism to each position (i.e. to
                                 eachper-symbolself-attentiveRNN;seeAppendixCformoredetails). Oncetheper-symbolrecurrent
                                 blockhalts,itsstateissimplycopiedtothenextstepuntilallblockshalt,orwereachamaximumnumber
                                 ofsteps. Theﬁnaloutputoftheencoderisthentheﬁnallayerofrepresentationsproducedinthisway.
                                 3     EXPERIMENTSANDANALYSIS
                                 WeevaluatedtheUniversalTransformeronarangeofalgorithmicandlanguageunderstandingtasks,
                                 aswellasonmachinetranslation. WedescribethesetasksanddatasetsinmoredetailinAppendixD.
                                 3.1     BABIQUESTION-ANSWERING
                                 ThebAbiquestionansweringdataset(Westonetal.,2015)consistsof20differenttasks,wherethe
                                 goal is to answer a question given a number of English sentences that encode potentially multiple
                                 supporting facts. The goal is to measure various forms of language understanding by requiring a
                                 certain type of reasoning over the linguistic facts presented in each story. A standard Transformer
                                                                                 2
                                 doesnotachievegoodresultsonthistask . However,wehavedesignedamodelbasedontheUniversal
                                 Transformerwhichachievesstate-of-the-artresultsonthistask.
                                 Toencodetheinput,similartoHenaffetal.(2016),weﬁrstencodeeachfactinthestorybyapplyinga
                                 learnedmultiplicativepositionalmasktoeachword’sembedding,andsummingupallembeddings. We
                                 embedthequestioninthesameway,andthenfeedthe(Universal)Transformerwiththeseembeddings
                                 ofthefactsandquestions.
                                 Asoriginallyproposed,modelscaneitherbetrainedoneachtaskseparately(“trainsingle”)orjointly
                                 on all tasks (“train joint”). Table 1 summarizes our results. We conducted 10 runs with different
                                 initializations and pickedthebestmodelbasedonperformanceonthevalidationset,similartoprevious
                                 work. BoththeUTandUTwithdynamichaltingachievestate-of-the-artresultsonalltasksinterms
                                 ofaverageerrorandnumberoffailedtasks3,inboththe10Kand1Ktrainingregime(seeAppendixE
                                 for breakdownbytask).
                                 Tounderstandtheworkingofthemodelbetter,weanalyzedboththeattentiondistributionsandthe
                                 averageACTpondertimesforthistask(seeAppendixFfordetails). First,weobservethattheattention
                                 distributions start out very uniform, but get progressively sharper in later steps around the correct
                                 supportingfactsthatarerequiredtoanswereachquestion,whichisindeedverysimilartohowhumans
                                 wouldsolvethetask. Second,withdynamichaltingweobservethattheaveragepondertime(i.e.depth
                                     2Weexperimentedwithdifferenthyper-parametersanddifferentnetworksizes,butitalwaysoverﬁts.
                                     3Deﬁnedas>5%error.
                                                                                             5
