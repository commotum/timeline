            capabilities, reaching very competitive results on the bAbI dataset [41] – a dataset that test reasoning
            capabilities of text-based question answering models.
            B CLEVRfrompixels
            Our model (described in Section 4 of the main text) was trained on 70000 scenes from the CLEVR
            dataset and a total of 699989 questions. Images were ﬁrst down-sampled to size 128 × 128, then
            pre-processed with padding to size 136×136, followed by random cropping back to size 128×128
            and slight random rotations between −0.05 and 0.05 rads. We used 10 distributed workers that
            synchronously updated a central parameter server. Each worker learned with mini-batches of size
            64, using the Adam optimizer and a learning rate of 2.5e−4. Dropout of 50% was used on the
            penultimate layer of the RN. In our best performing model each convolutional layer used 24 kernels of
            size 3 ×3 and stride 2, batch normalization, and rectiﬁed linear units. The model stopped improving
            in performance after approximately 1.4 million iterations, at which point training was concluded.
            The model achieved 96.8% accuracy on the validation set. In general, we found that smaller models
            performed best. For example, 128 hidden unit LSTMs performed better than 256 or 512, and CNNs
            with 24 kernels were better than CNNs with more kernels, such as 32, 64, or more.
            Failure cases
            Although our model gets most answers correct, a closer examination of the failure cases help us to
            identify limitations of our architecture. In Table 2, we show some examples of CLEVR questions that
            our model fails to answer correctly, along with the ground-truth answers. Based on our observations,
            we hypothesize that our architecture fails especially when objects are heavily occluded, or whenever
            a high precision object position representation is required. We also observe that many failure cases
            for our model are also challenging for humans.
            C CLEVRfromstate descriptions
            The model that we train on the state description version of CLEVR is similar to the model trained
            on the pixel version of CLEVR, but without the vision processing module. We used a 256 unit LSTM
            for question processing and word-lookup embeddings of size 32. For the RN we used a four-layer
            MLPwith 512 units per layer, with ReLU non-linearities for g . A three-layer MLP consisting of
                                                    θ
            512, 1024 (with 2% dropout) and 29 units with ReLU non-linearities was used for fθ. To train the
            model we used 10 distributed workers that synchronously updated a central parameter server. Each
            worker learned with mini-batches of size 64, using the Adam optimizer and a learning rate of 1e−4.
            D Sort-of-CLEVR
            The Sort-of-CLEVR dataset contains 10000 images of size 75×75, 200 of which were withheld for
            validation. There were 20 questions generated per image (10 relational and 10 non-relational).
              Non-relational questions are split into three categories: (i) query shape, e.g. “What is the shape
            of the red object?”; (ii) query horizontal position, e.g. “Is the red object on the left or right of the
            image?”; (iii) query vertical position, e.g. “Is the red object on the top or bottom of the image?”.
            These questions are non-relational because one can answer them by reasoning about the attributes
            (e.g. position, shape) of a single entity which is identiﬁed by its unique color (e.g. red).
              Relational questions are split into three categories: (i) closest-to, e.g. “What is the shape of the
            object that is closest to the green object?”; (ii) furthest-from, e.g. “What is the shape of the object
            that is furthest from the green object?”; (iii) count, e.g. “How many objects have the shape of the
            green object?”. We consider these relational because answering them requires reasoning about the
            attributes of one or more objects that are deﬁned relative to the attributes of a reference object.
            This reference object is uniquely identiﬁed by its color.
              Questions were encoded as binary strings of length 11, where the ﬁrst 6 bits identiﬁed the color
            of the object to which the question referred, as a one-hot vector, and the last 5 bits identiﬁed the
            question type and subtype.
                                           11
