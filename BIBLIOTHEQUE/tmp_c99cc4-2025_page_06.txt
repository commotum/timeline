                               6                                                                                                                                           Transactions of the Institute of Measurement and Control 00(0)
                               achieved by summing the products of these attention weights                                                                                 with temporal priors, thereby bolstering its ability to handle
                               and their corresponding value vectors, completing the process                                                                               sequential data. In the context of parallel training, this
                               as described                                                                                                                                method is executed in a highly efficient matrix format. The
                                                                                                                                                                           retention step involves processing the input X through a
                                                                         qk
                                                                                   i                                                                                       sequence of transformations and then multiplying it with the
                                                             ai = pﬃﬃﬃﬃﬃ                                                                                                   time decay matrix D to compute the final output. This strat-
                                                                              d
                                                                                 k
                                                                                                               expðÞa                                                      egy not only retains the temporal information within the
                                                             b =softmaxðÞa = P                                              i
                                                                i                            i               n        expðÞa                                  ð2Þ          sequences but also drastically lowers computational complex-
                                                                                                             j =1                  i
                                                                           n                                                                                               ity. Consequently, it empowers the model to more effectively
                                                             O=Xbv                                                                                                         learn the long-range dependencies inherent in sequence data
                                                                                    i   i
                                                                         i =1                                                                                                                                                               
                                                                                                                                                                                                                                 Q= XW Y
                                      In the context of object detection tasks, the model’s atten-                                                                                                                                                   Q
                                                                                                                                                                                                                                                                 
                                                                                                                                                                                                                                 K=ðÞXW Y
                               tion output serves as an indicator of its focus on various                                                                                                                                                            K
                                                                                                                                                                                                                                 V=XW
                               regions within the image. Higher levels of attention output                                                                                                                                                         V
                               signal that the model regards certain areas as encompassing                                                                                                                                    Y =einu0
                                                                                                                                                                                                                                   n        (                                                             ð4Þ
                               critical targets or information. Conversely, lower levels of                                                                                                                                                     gnm,                  n.m
                               attention output imply that the model either overlooks these                                                                                                                                 Dnm=
                               areas or considers them to be of minor significance to the task                                                                                                                                                  0,                      nłm
                                                                                                                                                                                                                                            
                               at hand. Within the attention mechanism designed for learn-                                                                                                             RetentionðÞX = QKT D V
                               ing BEV representations from a combination of multiple
                               images and items as inputs, each BEV query vector is indica-                                                                                       on is calculated as the weighted sum of outputs from all
                               tive of a significant feature or region within the scene. These                                                                             positions m, where the weights are determined by the similar-
                               BEVquery vectors, derived from learning across images cap-                                                                                  ity between the query Q and key K vectors, alongside a time-
                               tured from diverse camera angles, facilitate the model’s capa-                                                                              based decay factor gnm. The decay factor g, raised to the
                               bility to link and recognize features across varying camera                                                                                 power of nm for cases, where n.m, represents how influ-
                               perspectives.                                                                                                                               ence diminishes over time, making distant past events have
                                      RetentiveBEV employs query vectors to acquire a BEV                                                                                  lesser impact on the current state; each query vector Qn is
                               representation of the space surrounding ego car, utilizing                                                                                  generated by transforming the input X through a weight
                               surround-view images, LiDAR point clouds, and the bound-                                                                                    matrix W and is further modified by an element-wise com-
                                                                                                                                                                                                P
                               ing box (bbox) ground truths from the nuScenes data set.                                                                                    plex multiplication with Yn, a complex number embodying
                               These queries are formulated by a set of learnable parameters,                                                                              the positional information with an angle of nu . Similarly,
                                                                      H3W3C                                                                                                                                                                                                           0
                               denoted as Q 2 R                                        , where H and W indicate the spa-                                                   each key vector K                             is produced by transforming X via W
                                                                                                                                                                                                                    m                                                                                         K
                               tial resolution of the RetentiveBEV perception plane in terms                                                                               and undergoing a similar complex multiplication with Y .
                                                                                                                                                                                                                                                                                                             m
                               of BEV grid numbers, and C represents the count of sampling                                                                                 The matrix D                         defines the relationship between positions n
                                                                                                                                                                                                          nm
                               points within the vertical sections across the BEV grid. Each                                                                               and m, adopting a value of gnm to reflect temporal decay for
                               grid on the BEV plane equates to a real-world area of s                                                                                     n.m,and0 for n\m, thus enforcing causal masking to
                               meters, with the plane’s center aligning with the ego car’s                                                                                 ensure information flow is unidirectional, from past to future.
                               coordinate                     system’s                  center.               Before                entering                  the          This decay matrix D, containing causal masks and exponen-
                               RetentiveBEV’s attention mechanism, the query vector Q is                                                                                   tial decay components, underscores the relative distances in a
                               enriched with feature dimensions and spatial priors through a                                                                               one-dimensional sequence, embedding a clear temporal prior
                               process of learnable position encoding.                                                                                                     into text data analysis. The value vector V results from the
                                                                                                                                                                           transformation of X via W . The Hadamard product ()sig-
                                                                                                                                                                                                                                     V
                               Retentive decay matrix                                                                                                                      nifies element-wise multiplication; X is the input data matrix,
                                                                                                                                                                           and W ,W ,W are the trainable matrices used for convert-
                                                                                                                                                                                          P        K        V
                               One-dimensional inputs. To augment the transformer model’s                                                                                  ing X into the respective queries Q,keysK, and values V,
                               performance, a temporal decay mechanism is incorporated                                                                                     enabling the model to efficiently process sequential data while
                               within the retentive mechanism, offering a temporal prior for                                                                               accounting for time-based relevance and dependencies.
                               sequential modeling. This mechanism assigns decay weights
                               based on time when processing one-dimensional input. For                                                                                    2D inputs: MSA, MSA decomposition, and RSCA.
                               any given position n, the output, after undergoing temporal                                                                                        Manhattan distance. Manhattan distance refers to the dis-
                               decay, can be expressed as follows                                                                                                          tance between two points in a grid-like space, which is calcu-
                                                                          n                                                                                                lated as the sum of the absolute differences along the
                                                                         P nm                       inu                imu      T
                                                                                                          0                  0
                                                           o =                   g         ðÞQ e             ðÞK e                  v                         ð3Þ
                                                             n                                   n                 m                  m                                    coordinate axes, rather than the direct Euclidean distance. In
                                                                       m=1                                                                                                 other words, the distance is measured by ‘‘walking the
                                      Through these operations, the retentive mechanism intro-                                                                             blocks’’ rather than taking a straight line.
                               duces time decay and positional encoding into self-attention
                               computations, aiming to mimic the temporal relationships                                                                                           MSA. We involve the decay of one-dimensional input
                               present in sequential data. This approach endows the model                                                                                  shown in equation (3) into self-attention as MaSA. We
