                        1   Introduction
                        Symbolic regression (SR) is an approach to machine learning (ML) in which both the parameters
                        and structure of an analytical model are optimized. SR can be useful when one wishes to describe a
                        process via a mathematical expression, especially a simple expression; thus, it is often applied in the
                        hopes of producing a model of a process that, by virtue of its simplicity, may be easy to interpret.
                        Interpretable ML is becoming increasingly important as model deployments in high stakes societal
                        applications such as Ô¨Ånance and medicine grow [1, 2]. Moreover, the mathematical expressions
                        produced by SR are well-suited to be analyzed and controlled for their out-of-distribution behavior
                        (e.g., in terms of asymptotic behavior, periodicity, etc.). These attractive properties of SR have led to
                        its application in a number of areas, such as physics [3], biology [4], clinical informatics [5], climate
                        modeling [6], Ô¨Ånance [7], and many Ô¨Åelds of engineering [8‚Äì10].
                        SR literature has, in general, fallen short of evaluating and ranking new methods in a way that
                        facilitates their widespread adoption. Our view is that this shortcoming largely stems from a lack of
                        standardized, transparent and reproducible benchmarks, especially those that test a large and diverse
                        array of problems [11]. Although community surveys [11, 12] have led to suggestions for improving
                        benchmarking standards, and even black-listed certain problems, contemporary literature continues
                        to be published that violates those standards. Absent these standards, it is difÔ¨Åcult to assess which
                        methods or family of methods should be considered ‚Äústate-of-the-art‚Äù (SotA).
                        AchievingaÔ¨ÇeetingsenseofSotAiscertainlynotthesingularpursuitofmethodsresearch,yetwithout
                        common,robust benchmarking studies, promising avenues of investigation cannot be well-informed
                        by empirical evidence. We hope the benchmarking platform introduced in this paper improves
                        the cross-pollination between research communities interested in SR, which include evolutionary
                        computation, physics, engineering, statistics, and more traditional machine learning disciplines.
                        In this paper, we describe a large benchmarking effort that includes a dataset repository curated for
                        SR, as well as a benchmarking library designed to allow researchers to easily contribute methods. To
                        achieve this, we incorporated 130 datasets with ground truth forms into the Penn Machine Learning
                        Benchmark(PMLB)[13],includingmetadatadescribing the underlying equations, their units, and
                                                                                                                4
                        various summary statistics. Furthermore, we created a SR benchmark repository called SRBench
                        and sought contributions from researchers in this area. Here we describe this process and the results,
                        which consist of comparisons of 14 contemporary SR methods on hundreds of regression problems.
                        Toourknowledge, this is by far the largest and most comprehensive SR benchmark effort to date,
                        which allows us to make claims concerning current SotA methods for SR with better certainty.
                        Importantly, and in contrast to many previous efforts, the datasets, methods, benchmarking code,
                        and results are completely open-source, reproducible, and revision-controlled, which should allow
                        SRBenchtoexist as a living benchmark for future studies.
                        2   BackgroundandMotivation
                                                                 ÀÜ    ÀÜ    d
                        ThegoalofSRistolearnamappingyÀÜ(x) = œÜ(x,Œ∏) : R ‚Üí Rusingadatasetofpairedexamples
                                      N                      d
                        D={(x,y)} ,withfeaturesx ‚àà R andtargety. SR assumes the existence of an analytical
                                 i  i i=1
                                                  ‚àó     ‚àó
                        model of the form y(x) = œÜ (x,Œ∏ ) +  that would generate the observations in D, and seeks to
                        estimate this model by searching the space of expressions, œÜ, and parameters, Œ∏, in the presence of
                        white noise, .
                           4https://github.com/EpistasisLab/srbench
                                                                    2
