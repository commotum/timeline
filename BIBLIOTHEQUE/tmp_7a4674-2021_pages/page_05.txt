                                          Table 1: Short descriptions of the SR methods benchmarked in our experiment, including references
                                          and links to implementations.
                                            Method                  Year      Description                                         MethodFamily                               Implementation
                                            AFP[38]                 2011      Age-Ô¨Åtness Pareto Optimization                      GP                                      C++/Python (link)
                                            AFP_FE[24]              2011      AFPwithco-evolved Ô¨Åtness estimates;                 GP                                      C++/Python (link)
                                                                              Eureqa-esque
                                            AIFeynman[23]           2020      Physics-inspired method                             Divide and conquer                   Fortran/Python (link)
                                            BSR[16]                 2020      Bayesian Symbolic Regression                        MarkovChainMonteCarlo                         Python (link)
                                            DSR[17]                 2020      DeepSymbolicRegression                              Recurrent neural networks         Python (PyTorch) (link)
                                            EPLEX[39]               2016      -lexicase selection                                GP                                      C++/Python (link)
                                            FEAT[40]                2019      Feature Engineering Automation Tool                 GP                                      C++/Python (link)
                                            FFX[41]                 2011      Fast function extraction                            Randomsearch                            C++/Python (link)
                                            GP-GOMEA[42]            2020      GPversion of the Gene-pool Optimal                  GP                                      C++/Python (link)
                                                                              Mixing Evolutionary Algorithm
                                            gplearn                 2015      Koza-style symbolic regression in Python            GP                                      C++/Python (link)
                                            ITEA[43]                2020      Interaction-Transformation EA                       GP                                   Haskell/Python (link)
                                            MRGP[44]                2014      Multiple Regression Genetic Programming             GP                                               Java (link)
                                            Operon[45]              2019      SRwithNon-linear least squares                      GP                                      C++/Python (link)
                                            SBP-GP[46]              2019      Semantic Back-propagation Genetic                   GP                                      C++/Python (link)
                                                                              Programming
                                          Table 2:        Settings used in the benchmark experiments. ‚ÄúTotal comparisons" refers to the total
                                          evaluatons of an algorithm on a dataset for a given noise level and random seed.
                                                              Setting                       Black-box Problems                       Ground-truth Problems
                                                              No. of datasets               122                                      130
                                                              No. of algorithms             21 (14 SR, 7 ML)                         14
                                                              No. of trials per dataset     10                                       10
                                                              Train/test Split              .75/.25                                  .75/.25
                                                              Hyperparameter Tuning         5-fold Halving Grid Search CV            Tuned set from Black-box problems
                                                              Termination criteria          500k evaluations/train or 48 hours       1Mevaluations or 8 hours
                                                              Levels of target noise        None                                     0, 0.001, 0.01, 0.1
                                                              Total comparisons             26840                                    54600
                                                              ComputingBudget               1.29Mcorehours                           436.8K core hours
                                          continuous updates to results reporting and visualization whenever new experiments are available,
                                          allowing us to maintain a standing leader-board of contemporary SR methods. Ideally these features
                                         will quicken the adoption of SotA approaches throughout the SR research community. Further details
                                          onhowtouseandcontributetoSRBenchareprovidedinSec.A.1.
                                          4     ExperimentDesign
                                         Weevaluated SR methods on two separate tasks. First, we assessed their ability to make accurate
                                          predictions on ‚Äúblack-box‚Äù regression problems (in which the underlying data generating function
                                          remains unknown) while minimizing the complexity of the discovered models. Second, we tested
                                          the ability of each method to Ô¨Ånd exact solutions to synthetic datasets with known, ground-truth
                                          functions, originating from physics and various Ô¨Åelds of engineering.
                                          Thebasic experiment settings are summarized in Tbl. 2. Each algorithm was trained on each dataset
                                         (and level of noise, for ground-truth problems) in 10 repeated trials with a different random state
                                          that controlled both the train/test split and the seed of the algorithm. Datasets were split 75/25%
                                          in training and testing. For black-box regression problems, each algorithm was tuned using 5-fold
                                          cross validation with halving grid search. The SR algorithms were limited to 6 hyperparameter
                                          combinations; the ML methods were allowed more, as shown in Tbls. 4-6. The best hyperparameter
                                          settings were used to tune a Ô¨Ånal estimator and evaluate it according to the metrics described above.
                                          Details for running the experiment are given in Sec. A.1.
                                                                                                                    5
