                                         1   # method: Bayesian Symbolic Regression
                                         2   # contributor: Ying Jin
                                         3   # source: https://github.com/ying531/MCMC-SymReg
                                         4   from bsr.bsr_class import BSR
                                         5
                                         6   hyper_params = []
                                         7   for val, itrNum in zip([100,500,1000],[5000,1000,500]):
                                         8         for treeNum in [3,6]:
                                         9                hyper_params.append(
                                        10                                   {'treeNum': [treeNum],
                                        11                                    'itrNum': [itrNum],
                                        12                                    'val': [val],
                                        13                                   })
                                        14   # default estimator
                                        15   est = BSR(val=100, itrNum=5000, treeNum=3, alpha1=0.4, alpha2=0.4,
                                        16                   beta=-1, disp=False, max_time=2*60*60)
                                        17
                                        18   def complexity(est):
                                        19   """returns final model complexity"""
                                        20         return est.complexity()
                                        21
                                        22   def model(est):
                                        23   """returns final model as string"""
                                        24         return est.model()
                                     Figure 4: An example code contribution, deﬁning the estimator, its hyperparameters, and functions
                                     to return the complexity and symbolic model.
                                     predictors, in which ﬁtness assignment is sped up by optimizing a second population of training
                                     sample indices that best distinguish between equations in the population [51]. Unfortunately we
                                     cannot guarantee that Eureqa currently uses any of these reported algorithms for SR, due to its closed-
                                     source nature. We chose instead to benchmark known algorithms (AFP, AFP_FE) with open-source
                                     implementations, hoping that the resulting study’s conclusions may better inform future methods
                                     development. We note that AFP has been outperformed by a number of other optimization methods
                                     in controlled studies since its release (e.g., [27, 28]).
                                     Constant optimization in Genetic Programming                           Oneoftheclearest improvements over Koza-
                                     style GP has been the adoption of local search methods to handle constant optimization distinctly
                                     from evolutionary learning. Regarding the optimization of constants in GP, several reasons can
                                     explain why backpropagation and gradient descent can be considered to be relatively under-used
                                     in GP (compared to, e.g., evolutionary neural architecture search). For example, early works often
                                     ignored the use of feature standardization (e.g., by z-scoring), the lack of which can harm gradient
                                     propagation [78]. Next to this, GP relies on crafting compositions out of a multitude of operations,
                                     someofwhicharepronetocausevanishingorexploding gradients. Last but not least, to the best of
                                     our knowledge, the ﬁeld lacks a comprehensive study that provides guidelines for the appropriate
                                     hyperparameters for constant optimization (learning rate schedule, iterations, batch size, etc.), and
                                     howtoeffectively balance parameter learning with the evolutionary process.
                                     A.4     Additional Dataset Information
                                     All datasets, including metadata, are available from PMLB. Each dataset is stored using Git Large
                                     File Storage and PMLB is planned for long-term maintenance. PMLB is available under an MIT
                                                                                                     19
