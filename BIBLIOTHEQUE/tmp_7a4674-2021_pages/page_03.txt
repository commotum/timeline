                        Koza [14] introduced SR as an application of genetic programming (GP), a ﬁeld that investigates
                        the use of genetic algorithms (GAs) to evolve executable data structures, i.e. programs. In the
                        case of so-called “Koza-style” GP, the programs to be optimized are syntax trees consisting of
                        functions/operations over input features and constants. Like in other GAs, GP is a process that
                        evolves a population of candidate solutions (e.g., syntax trees) by iteratively producing offspring from
                        parent solutions (e.g., by swapping parents’ subtrees) and eliminating unﬁt solutions (e.g., programs
                        with sub-par behavior). Most SR research to date has emerged from within this sub-ﬁeld and its
                                             5
                        associated conferences.
                        Despite the availability of post-hoc methods for explaining black-box model predictions [15], there
                        have been recent calls to focus on learning interpretable/transparent models explicitly [2]. Perhaps
                        due to this renewed interest in model interpretability, entirely different methods for tackling SR
                        have been proposed [16–22]. These include methods based in Bayesian optimization [16], recurrent
                        neural networks (RNNs) [17], and physics-inspired divide-and-conquer strategies [18, 23]. Some
                        of these papers refer to Eureqa, a commercial, GP-based SR method used to re-discover known
                        physics equations [3], as the “gold standard” for SR [17] and/or the best method for SR “by far” [18].
                        However, Schmidt and Lipson [24] make no claim to being the SotA method for SR, nor is this
                        hypothesis tested in the body of work on which Eureqa is based [25].
                        Although commercial platforms like Eureqa and Wolfram [26] are successful tools for applying
                        SR, they are not designed to support controlled benchmark experiments, and therefore experiments
                        utilizing them have serious caveats. Due to the design of the front-end API for both tools, it is not
                        possible to benchmark either method against others while holding important parameters of such an
                        experiment constant, including the computational effort, number of model evaluations, CPU/memory
                        limits, and ﬁnal solution assessment. More generally, researchers cannot uniquely determine which
                        features of the software and/or experiment lead to observed differences in performance, given that
                        these commercial tools are closed-source. In this light, it is not clear what insights are to be gained
                        when comparing to Eureqa and Wolfram beyond a simple head-to-head comparison. Therefore,
                        rather than benchmark against Eureqa in this paper, we implement its underlying algorithms in an
                        open-source package, which allows our experiment to remain transparent, reproducible, accessible,
                        and controlled. We discuss the algorithms underlying Eureqa in detail in Sec. A.3.
                        Aclose reading of SR literature since 2009 implies that a number of proposed methods would
                        outperform Eureqa in controlled tests, and are therefore suitable choices for benchmarking (e.g. [27,
                        28]). Unfortunately, the widespread adoption of these promising SR approaches is hamstrung by a
                        lack of consensus on good benchmark problems, testing frameworks, and experimental designs. Our
                        effort to establish a common benchmark is motivated by our view that common, robust, standardized
                        benchmarksforSRcouldspeedprogressintheﬁeldbyprovidingaclearbaselinefromwhichtoassert
                        the quality of new approaches. Consider the NN community’s focus on common benchmarks (e.g.
                        ImageNet [29]), frameworks (e.g. TensorFlow, PyTorch) and experiment designs. By contrast, it is
                        commontoobserveresultsinSRliterature that are based on a small number of low dimensional, easy
                        and unrealistic problems, comparing only to very basic GP systems such as those described in [14]
                        nearly thirty years ago. Despite detailed descriptions of these issues [11], community surveys and
                        proposals to “black-list" toy problems [12], toy datasets and comparisons to out-dated SR methods
                        continue to appear in contemporary literature.
                        Theaspects of performance assessment for SR differ from typical regression benchmarking due to the
                        interest in obtaining concise, symbolic expressions. In general, the trade-off between accuracy and
                           5Anon-exhaustive list: GECCO, EuroGP, FOGA, PPSN, and IEEE CEC.
                                                                    3
