                                           Table 4: ML methods and the hyperparameter spaces used in tuning.
                              Method             Hyperparameters
                              AdaBoost           {’learning_rate’: (0.01, 0.1, 1.0, 10.0), ’n_estimators’: (10, 100, 1000)}
                              KernelRidge        {’kernel’: (’linear’, ’poly’, ’rbf’, ’sigmoid’), ’alpha’: (0.0001, 0.01, 0.1, 1), ’gamma’: (0.01,
                                                 0.1, 1, 10)}
                              LassoLars          {’alpha’: (0.0001, 0.001, 0.01, 0.1, 1)}
                              LGBM               {’n_estimators’: (10, 50, 100, 250, 500, 1000), ’learning_rate’: (0.0001, 0.01, 0.05, 0.1, 0.2),
                                                ’subsample’: (0.5, 0.75, 1), ’boosting_type’: (’gbdt’, ’dart’, ’goss’)}
                              LinearRegression   {’ﬁt_intercept’: (True,)}
                              MLP                {’activation’: (’logistic’, ’tanh’, ’relu’), ’solver’: (’lbfgs’, ’adam’, ’sgd’), ’learning_rate’:
                                                 (’constant’, ’invscaling’, ’adaptive’)}
                              RandomForest       {’n_estimators’: (10, 100, 1000), ’min_weight_fraction_leaf’: (0.0, 0.25, 0.5), ’max_features’:
                                                 (’sqrt’, ’log2’, None)}
                              SGD                {’alpha’: (1e-06, 0.0001, 0.01, 1), ’penalty’: (’l2’, ’l1’, ’elasticnet’)}
                              XGB                {’n_estimators’: (10, 50, 100, 250, 500, 1000), ’learning_rate’: (0.0001, 0.01, 0.05, 0.1, 0.2),
                                                ’gamma’: (0, 0.1, 0.2, 0.3, 0.4), ’subsample’: (0.5, 0.75, 1)}
                            the ground-truth problems, with updates to 1) include any mathematical operators needed for those
                            problems and 2) double the evaluation budget.
                            A.6   Additional Results
                            A.6.1   Subgroupanalysis of black-box regression results
                            Manyoftheblack-box problems for regression in PMLB were originally sourced from OpenML.
                            Afewauthorshavenotedthatseveral of these datasets are sourced from Friedman [81]’s synthetic
                            benchmarks. These datasets are generated by non-linear functions that vary in degree of noise,
                            variable interactions, variable importance, and degree of non-linearity. Due to their number, they may
                            have an out-sized effect on results reporting in PMLB. In Fig. 6, we separate out results on this set
                            of problems relative to the rest of PMLB. We do ﬁnd that, relative to the rest of PMLB, the results
                            onthe Friedman datasets distinguish top-ranked methods more strongly than among the rest of the
                            benchmark, on which performance between top-performing methods is more similar. In general,
                            although we do see methods rankings change somewhat when looking at speciﬁc data groupings, we
                            do not observe large differences. An exception is Kernel ridge regression, which performs poorly on
                            the Friedman datasets but very well on the rest of PMLB. We recommend that future revisions to
                            PMLBexpandthedatasetcollection to minimize the effect of any one source of data, and include
                            subgroup analysis to identify which types of problems are best solved by speciﬁc methods.
                            Toget a better sense of the performance variability across methods and datasets, method rankings on
                            each dataset are bi-clustered and visualized in Fig. 7. Methods that perform most similarly across
                            the benchmark are placed adjacent to each other, and likewise datasets that induce similar method
                            rankings are grouped. We note some expected groupings ﬁrst: AFP and AFP_FE, which differ
                            only in ﬁtness estimation, and FEAT and EPLEX, which use the same selection method, perform
                            similarly. We also observe clustering among the Friedman datasets (names beginning with “fri_"),
                            and again note stark differences between methods that perform well on these problems, e.g. Operon,
                            SBP-GP, and FEAT, and those that do not, e.g. MLP. This view of the results also reveals a cluster of
                            SRmethods(AFP,AFP_FE,DSR,gplearn)that perform well on a subset of real-world problems
                            (analcatdata_neavote_523 - vineyard_192) for which linear models also perform well. Interestingly,
                            for that problem subset, Operon’s performance is mediocre relative to its strong performance on other
                            datasets. We also note with surprise that DSR and gplearn exhibit performance similarity on par with
                                                                             22
