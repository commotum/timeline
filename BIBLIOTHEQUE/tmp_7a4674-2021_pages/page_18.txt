                        A Appendix
                        Please refer to https://github.com/EpistasisLab/srbench/ for the most up-to-date guide to
                        SRBench.
                        A.1  RunningtheBenchmark
                        TheREADMEinourGithubrepositoryincludesthesetofcommandstoreproducethebenchmark
                        experiment, which are summarized here. Experiments are launched from the experiments/ folder
                        via the script analyze.py. The script can be conﬁgured to run the experiment in parallel locally,
                        onanLSFjobscheduler,oronaSLURMjobscheduler. Toseethefullsetofoptions,runpython
                        analyze.py -h.
                        After installing and conﬁguring the conda environment, the complete black-box experiment can be
                        started via the command:
                                python analyze.py /path/to/pmlb/datasets -n_trials 10 -results
                               ../results -time_limit 48:00
                        Similarly, the ground-truth regression experiment for Strogatz datasets and a target noise of 0.0 are
                        run by the command:
                                python analyze.py -results ../results_sym_data -target_noise
                               0.0 "/path/to/pmlb/datasets/strogatz*" -sym_data -n_trials 10
                               -time_limit 9:00 -tuned
                        A.2  Contributing a Method
                        Aliving version of the method contribution instructions are described in the Contribution Guide. To
                        illustrate the simplicity of contributing a method, Figure 4 shows the script submitted for Bayesian
                        Symbolic Regression [16]. In addition to the code snippet, authors may either add their code package
                        to the conda/pip environment, or provide an install script. When a pull request is issued by a
                        contributor, new methods and installs are automatically tested on a minimal version of the benchmark.
                        Once the tests pass and the method is approved by the benchmark maintainers, the contribution
                        becomes part of the resource and can be tested via the commands above.
                        A.3  Additional Background and Motivation
                                                                                                           7
                        Eureqa EureqaisacommercialGP-basedSRsoftwarethatwasacquiredbyDataRobotin2017 .
                        Duetoits closed-source nature and incorporation into the DataRobot platform, it is impossible to
                        benchmark its performance while controlling for important experimental variables such as number
                        of evaluations, space and time limits, population size, and so forth. However, the novel algorithmic
                        aspects of Eureqa are rooted in a number of ablation studies [38, 51, 77] that we summarize here.
                        First is its use of directed acyclic graphs for representing equations in lieu of trees, which resulted in
                        morespace-efﬁcientmodelencodingrelativetotrees, withoutasigniﬁcantdifferenceinaccuracy[77].
                        The most signiﬁcant improvement over traditional tournament-based selection is Eureqa’s use of
                        age-ﬁtness Pareto optimization (AFP), a method in which random restarts are incorporated each
                        generation as new offspring, and are protected from competing with older, more ﬁt equations by
                        including age as an objective to be minimized [38]. Eureqa also includes the co-evolution of ﬁtness
                          7https://www.datarobot.com/nutonian/
                                                                 18
