                        simplicity must be considered when evaluating the merits of different models. Furthermore, model
                        simplicity, typically measured as sparsity or model size, is but a proxy for model interpretability; a
                        simple model may still be un-interpretable, or simply wrong [30–32]. With these concerns in mind,
                        datasets with ground truth solutions are useful, in that they allow researchers to assess whether or
                        not the symbolic model regressed by a given method corresponds to a known analytical solution.
                        Nevertheless, benchmarks utilizing synthetic datasets with ground-truth solutions are not sufﬁcient
                        for assessing real-world performance, and so we consider it essential to also evaluate the performance
                        of SR on real-world or otherwise black-box regression problems, relative to SotA ML methods.
                        There have been a few recent efforts to benchmark SR algorithms [33], including a precursor to this
                        workbenchmarkingfourSRmethodson94regressionproblems[34]. Inbothcases,SRmethodswere
                        assessed solely on their ability to make accurate predictions. In contrast, Udrescu and Tegmark [18]
                        proposed 120 new synthetic, physics-based datasets for SR, but compared only to Eureqa and only in
                        terms of solution rates. A major contribution of our work is its signiﬁcantly more comprehensive
                        scope than previous studies. We include 14 SR methods on 252 datasets in comparison to 7 ML
                        methods. Our metrics of comparison are also more comprehensive, and include 1) accuracy, 2)
                        simplicity, and 3) exact or approximate symbolic matches to the ground truth process. Furthermore,
                        wehavemadethebenchmarkopenlyavailable,reproducible, and open for contributions supported by
                        continuous integration [35].
                        3   SRBench
                        Wecreated SRBench to be a reproducible, open-source benchmarking project by pulling together a
                        large set of diverse benchmark datasets, contemporary SR methods, and ML methods around a shared
                        model evaluation and analysis environment. SRBench overcomes several of the issues in current
                        benchmarking literature as described in Sec. 2. For example, it makes it easy for methodologists
                        to benchmark new algorithms over hundreds of problems, in comparison to strong, contemporary
                        reference methods. These improvements allow us to reason with more certainty than in previous
                        workaboutthe SotA methods for SR.
                        In order to establish common datasets, we extended PMLB, a repository of standardized regression
                        and classiﬁcation problems [13, 36], by adding 130 SR datasets with known model forms. PMLB
                        provides utilities for fetching and handling data, recording and visualizing dataset metadata, and
                        contributing new datasets. The SR methods we benchmarked are all contemporary implementations
                        (2011-2020)fromseveralmethodfamilies,asshowninTbl.1. Werequiredcontributorstoimplement
                        a minimal, Scikit-learn compatible [37], Python API for their method. In addition, contributors
                        were required to provide the ﬁnal ﬁtted model as a string that was compatible with the symbolic
                        mathematics library sympy. Note that although we require a Python wrapper, SR implementations
                        in many different languages are supported, as long as the Python API is available and the language
                                                               6
                        environment can be managed via Anaconda .
                        Toensure reproducibility, we deﬁned a common environment (via Anaconda) with ﬁxed versions of
                        packages and their dependencies. In contrast to most SR studies, the full installation code, experiment
                        code, results and analysis are available via the repository for use in future studies. To make SRBench
                        as extensible as possible, we automated the process of incorporating new methods and results into
                        the analysis pipeline. The repository accepts rolling contributions of new methods that meet the
                        minimal API requirements. To achieve this, we created a continuous integration (CI) [35] framework
                        that assures contributions are compatible with the benchmark code as they arrive. CI also supports
                           6https://www.anaconda.com/
                                                                   4
