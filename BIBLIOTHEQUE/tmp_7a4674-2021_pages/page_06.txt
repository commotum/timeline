                           4.1  Symbolic Regression Methods
                           Here we characterize the SR methods summarized in Tbl. 1 by describing how they Ô¨Åt into broader
                           research trends within the SR Ô¨Åeld. The most traditional implementation of GP-based SR we test is
                           gplearn, which initializes a random population of programs/models, and then iterates through the
                           steps of tournament selection, mutation and crossover.
                           Pareto optimization methods [8, 47‚Äì49] are popular evolutionary strategies that exploit Pareto
                           dominance relations to drive the population of models towards a set of efÔ¨Åcient trade-offs between
                           competing objectives. Half of the SR methods we test use Pareto optimization in some form during
                           training. Age-Fitness Pareto optimization (AFP), proposed by Eureqa‚Äôs authors Schmidt and Lipson
                           [38], uses a model‚Äôs age as an objective in order to reduce premature convergence as well as bloat [50].
                           AFP_FEcombinesAFPwithEureqa‚ÄôsmethodforÔ¨Åtnessestimation[51]. Thus we expect AFP_FE
                           and AFPtoperformsimilarly to Eureqa as described in literature.
                           Anotherpromisinglineofresearchhasbeentoleverageprogramsemantics(inthiscase,theequation‚Äôs
                           intermediate and Ô¨Ånal outputs over training samples) more heavily during optimization, rather than
                           compressing that information into aggregate Ô¨Åtness values [52]. -lexicase selection (EPLEX) [27] is
                           a parent selection method that utilizes semantics to conduct selection by Ô¨Åltering models through ran-
                           domized subsets of cases, which rewards models that perform well on difÔ¨Åcult regions of the training
                           data. EPLEX is also used as the parent selection method in FEAT [40]. Semantic backpropagation
                           (SBP) is another semantic technique to compute, for a given target value and a tree node position,
                           that value which makes the output of the model match the target (i.e., the label) [53‚Äì55]. Here, we
                           evaluate the (SBP-GP) algorithm by Virgolin et al. [46] which improves SBP-based recombination
                           bydynamically adapting intermediate outputs using afÔ¨Åne transformations.
                           Backpropagation-based gradient descent was proposed for GP-SR by Topchy and Punch [56], but
                           tends to appear less often than stochastic hill climbing (e.g. [3, 57]). More recent studies [45, 58]
                           have made a strong case for the use of gradient-based constant optimization as an improvement over
                           stochastic and evolutionary approaches. The aforementioned studies are embodied by Operon, a
                           GPmethod that incorporates non-linear least squares constant optimization using the Levenberg-
                           Marquadt algorithm [59].
                           In addition to the question of how to best optimize constants, a line of research has proposed different
                           waysofdeÔ¨Åningprogramand/or model encodings. The methods FEAT, MRGP, ITEA, and FFX each
                           impose additional structural assumptions on the models being evolved. In FEAT, each model is a
                           linear combination of a set of evolved features, the parameters of which are encoded as edges and
                           optimized via gradient descent. In MRGP [44], the entire program trace (i.e., each subfunction of
                           the model) is decomposed into features and used to train a Lasso model. In ITEA, each model is
                           an afÔ¨Åne combination of interaction-transformation expressions, which compose a unary function
                           (the transformation) and a polynomial function (the interaction) [43, 60]. Finally, FFX [41] simply
                           initializes a population of equations, selects the Pareto optimal set, and returns a single linear model
                           bytreating the population of equations as features.
                           GP-GOMEAisaGPalgorithmwhererecombinationisadaptedovertime[42,61]. Everygeneration,
                           GP-GOMEAbuilds a statistical model of interdependencies within the encoding of the evolving
                           programs, and then uses this information to recombine interdependent blocks of components, as to
                           preserve their concerted action.
                           Jin et al. [16] recently proposed Bayesian Symbolic Regression (BSR), in which a prior is placed
                           on tree structures and the posterior distributions are sampled using a Markov Chain Monte Carlo
                                                                          6
