             [75] Robert E. Schapire. The boosting approach to machine learning: An overview. In Nonlinear Estimation
               and Classiﬁcation, pages 149–171. Springer, 2003.
             [76] Michael D Schmidt, Ravishankar R Vallabhajosyula, Jerry W Jenkins, Jonathan E Hood, Abhishek S Soni,
               John P Wikswo, and Hod Lipson. Automated reﬁnement and inference of analytical models for metabolic
               networks. Physical Biology, 8(5):055011, October 2011. ISSN 1478-3975. doi: 10.1088/1478-3975/8/5/
               055011.
             [77] Michael Schmidt and Hod Lipson. Comparison of Tree and Graph Encodings As Function of Problem
               Complexity. In Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation,
               GECCO’07, pages 1674–1679, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-697-4. doi:
               10.1145/1276958.1277288.
             [78] Grant Dick, Caitlin A. Owen, and Peter A. Whigham. Feature standardisation and coefﬁcient optimisation
               for effective symbolic regression. In Proceedings of the 2020 Genetic and Evolutionary Computation
               Conference, GECCO ’20, pages 306–314, Cancún, Mexico, June 2020. Association for Computing
               Machinery. ISBN 978-1-4503-7128-5. doi: 10.1145/3377930.3390237.
             [79] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing Fairness Gerrymandering:
               Auditing and Learning for Subgroup Fairness. arXiv:1711.05144 [cs], December 2018.
             [80] William La Cava and Jason H. Moore. Genetic programming approaches to learning fair classiﬁers. In
               Proceedings of the 2020 Genetic and Evolutionary Computation Conference, GECCO ’20, 2020. doi:
               10.1145/3377930.3390157.
             [81] Jerome H Friedman. Greedy function approximation: A gradient boosting machine. Annals of statistics,
               pages 1189–1232, 2001.
             [82] JanezDemšar. Statistical Comparisons of Classiﬁers over Multiple Data Sets. Journal of Machine Learning
               Research, 7(Jan):1–30, 2006. ISSN ISSN 1533-7928.
             Checklist
               1. For all authors...
                 (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
                   contributions and scope? [Yes] The results can be veriﬁed by visiting our repository.
                   Speciﬁc claims are supported by statistical tests.
                 (b) Did you describe the limitations of your work? [Yes] See discussion and conclusions.
                 (c) Did you discuss any potential negative societal impacts of your work? [Yes] See
                   Sec. A.4.
                 (d) Have you read the ethics review guidelines and ensured that your paper conforms to
                   them? [Yes] In addition to releasing the benchmark in a transparent way, we discuss
                   ethics in Sec. A.4.
               2. If you ran experiments (e.g. for benchmarks)...
                 (a) Did you include the code, data, and instructions needed to reproduce the main ex-
                   perimental results (either in the supplemental material or as a URL)? [Yes] See
                   https://github.com/EpistasisLab/srbench.
                 (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
                   were chosen)? [Yes] See Table 2.
                 (c) Did you report error bars (e.g., with respect to the random seed after running experi-
                   ments multiple times)? [Yes] See Figs. 1-3 for example.
                                  16
