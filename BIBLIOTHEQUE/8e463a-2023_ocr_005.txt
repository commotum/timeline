its well-organized format, renders EOL the most
suitable data source for constructing ALCUNA.

3.2 Artificial Entity Construction Details

Each time we select a class C from the taxonomic
tree of EOL and consider its members as entities.
We then divide them into one parent entity e? and
its siblings sib(e?) from which we construct the
artificial entity. Since a high-level taxon is usually
not a specific organism, the properties it possesses
may be too homogeneous, so we screen out all
taxons belonging to kingdom, phylum and domain.
In the real world, the naming scheme of a newly
found creature usually incorporates the same prefix
or suffix of other creatures of the same species. In
order to mimic the real world scenario and consid-
ering the tokenization algorithms used in LLMs,
we firstly split the names of related existing enti-
ties (ie. the parent entity and sibling entities of
parent) into subwords *. Then we randomly select
names of related entities, and for the i-th selected
entity we choose its i-th subword. For example,
"ALCUNA" is created from Alpaca and Vicuna.

3.3. Template-based Question Generation

Given a property triplet (€, a, v) (one-hop setting)
or a chain of property triplets 7¢ = (€,r,e1) >
(e1,71,€2) >... > (En-1, TN-1, EN) (multi-hop
setting), we aim to generate natural language ques-
tion asking about the tail object.

We leverage ChatGPT in the process of question
generation to avoid expensive labor costs following
Petroni et al. (2019). Specifically, we use ChatGPT
to generate a question template with a placeholder
[T] given only the relevant properties to avoid intro-
ducing too much model’s knowledge of a specific
entity. Then we generate questions from the ques-
tion template by replacing [T] with the name of
head subject. We generate five question templates
for each property group in form of multiple choice,
fill-in-the-blank and Boolean questions. The details
about the prompts used for question generation and
examples are shown in Appendix B.2. To ensure
the quality of automatic question generation by this
method, we randomly sample 100 questions each
for one-hop and multi-hop questions for human
checking. It turns out that for the generated one-
hop questions, 98% are correct; for the multi-hop
questions, 95% are correct. It shows that this way
of constructing questions is acceptable.

4We utilize the tokenizer of GPT-2 for tokenization.

Cn ee 203 4 Ss
#Attributes #Relations

(a) (b)

20 >=25 o 1

Figure 2: (a) The number of entities with different
counts of attributes. (b) The number of entities with
different counts of relations.

3.4 Dataset Summary

With the previous steps, we constructed a dataset,
ALCUNA, for evaluating the ability of LLMs in
face of new knowledge. The ALCUNA dataset
consists of a total of 84351 questions about 3554
artificial entities. We ensure that the constructed
artificial entities contain rich and unique attributes
and relationships by filtering out parent entities
with less than three properties. Specifically, each
artificial entity contains 11.75 property triples and
25.39 siblings on average. The distribution of the
number of property triplets is shown in Figure 2.

We organize the dataset in terms of questions,
and for each question we collect the correspond-
ing property triplets as evidence and the relevant
artificial entities’ information as new knowledge.
We divide all questions into three subsets, KU,
KD and KA, as mentioned in Section 2.4 to mea-
sure the corresponding capabilities of LLMs in a
fine-grained manner. In specific, KU, KD and KA
contain 11316, 27186 and 15353 questions respec-
tively. The details about question forms in the three
subsets are shown in Appendix B.1.

4 Evaluation of LLMs
4.1 LLMs Selected for Evaluation

We select several popular LLMs for evaluation and
analysis on our benchmarks, including ChatGPT,
Alpaca-7B, Vicuna-13B and ChatGLM-6B. The
detailed description of our selected models can be
found in Appendix C.

4.2 Evaluation Methods

In order to adapt the approach in the era of large
models and to match the application scenarios in
practice, we introduce two types of evaluation
methods: zero-shot and few-shot. We implement
both the vanilla and "Chain-of-Thought" (CoT) rea-
soning forms for zero-shot and few-shot setting.
