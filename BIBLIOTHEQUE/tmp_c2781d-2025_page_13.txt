                         Preprint, Under Review.
                         John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
                           optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
                         Sahil Sharma, Aravind S. Lakshminarayanan, and Balaraman Ravindran. Learning to repeat: Fine
                           grained action repetition for deep reinforcement learning. In International Conference on Learning
                           Representations, 2017. URL https://openreview.net/forum?id=B1GOWV5eg.
                         NoahShinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:
                           Language agents with verbal reinforcement learning. Advances in Neural Information Processing
                           Systems, 36:8634–8652, 2023.
                         David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
                           MarcLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi
                           byself-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,
                           2017a.
                         David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
                           ThomasHubert,LucasBaker,MatthewLai,AdrianBolton,etal. Masteringthegameofgowithout
                           humanknowledge. nature, 550(7676):354–359, 2017b.
                         Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute
                           optimally can be more effective than scaling parameters for reasoning. In The Thirteenth Inter-
                           national Conference on Learning Representations, 2025. URL https://openreview.net/
                           forum?id=4FWAwZtd2n.
                         Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati.           Chain of thoughtless-
                           ness?   an analysis of cot in planning.   In Amir Globersons, Lester Mackey, Danielle
                           Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Ad-
                           vances in Neural Information Processing Systems 38: Annual Conference on Neural Infor-
                           mation Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 -
                           15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/
                           3365d974ce309623bd8151082d78206c-Abstract-Conference.html.
                         YangSui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu,
                           AndrewWen,HanjieChen,XiaHu,etal. Stopoverthinking: A survey on efficient reasoning for
                           large language models. arXiv preprint arXiv:2503.16419, 2025.
                         Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha
                           Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language
                           models. In The Eleventh International Conference on Learning Representations, 2023. URL
                           https://openreview.net/forum?id=1PL1NIMMrw.
                         JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,brianichter,FeiXia,EdH.Chi,QuocV
                           Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models.
                           In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in
                           Neural Information Processing Systems, 2022. URL https://openreview.net/forum?
                           id=_VjQlMeSB_J.
                         Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy
                                                                                                        ¨
                           Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, Louis Castricato, Jan-Philipp Franken, Nick
                           Haber, and Chelsea Finn. Towards system 2 reasoning in llms: Learning how to think with
                           meta chain-of-thought. CoRR, abs/2501.04682, 2025. doi: 10.48550/ARXIV.2501.04682. URL
                           https://doi.org/10.48550/arXiv.2501.04682.
                         Mengjiao Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. Chain of thought imitation with
                           procedure cloning. CoRR, abs/2205.10816, 2022.
                         ShunyuYao,DianYu,Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R
                           Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In
                           Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https:
                           //openreview.net/forum?id=5Xc1ecxO1h.
                                                                     13
