                 be triggered if at least one person appeared in the     ambiguities, typos, and incorrect outputs. The main
                 room, thus both Billy and Suzy could be assigned        ambiguities that were identified relates to the use of
                 the responsibility for the detector going off. Both    {Actor}, which was substituted with the associated
                 explanation could be considered valid. For the re-      action. Each example was discussed to reach an
                 maining questions, the label was kept unchanged         agreementonthechanges,thisresultedinchanging
                 to Yes/No.                                              the outputs of six examples (see Table 6).
                    With the above re-definition of ground truth la-
                 bels, models were then asked to correctly identify      ModelOutputsAnalysis Analysingtheperfor-
                 the way humans, as a group, would answer the            mancethatGemini2.0Flashachievesonthecausal
                 question, thus testing alignment with human causal      understanding task reveals that this model answers
                 intuitions. To reflect this, we added the following     correctly to 45% of the causal judgment queries
                 instructions to each query:                            (random performance is 33%) and 71% of the
                                                                         queries about necessary and sufficient causes (ran-
                     Prompt Instructions                                 domperformanceis 50%).
                     Reply Yes or No based on the answer the               Focusing on the causal judgment queries, most
                     majority of people would give.                      of the errors are in questions for which the ground
                     If you think people would be split roughly          truth label is Ambiguous (44 mistakes out of 45
                     50-50 between Yes and No then reply Am-             examples) or No (24 mistakes out of 49 examples),
                     biguous.                                           with only 10 mistakes out of 48 examples for the
                                                                        Yes label. This reveals the difficulty the model has
                    Finally, to ensure consistency in terms of           in determining an absence of causal relationships,
                 number of queries included for the other tasks,         andindealingwithambiguitiesandtheexistenceof
                 we removed 2 stories included in the origi-             different possible answers. Thisbehaviourhasbeen
                 nal set (these correspond to question 17 and            previously observed across other causal reason-
                 19 at https://moca-llm.github.io/causal_                ing benchmarks (see e.g. (Romanou et al., 2023))
                 stories/1/). The final set includes 45 Ambigu-         where models have been found to infer stronger
                 ous questions and 48 and 49 questions with Yes          causal relationships than those that humans per-
                 and No labels respectively.                             ceive. Another interesting failure mode is that the
                                                                         model sometimes exhibits a lack of understanding
                 Necessary and Sufficient Causes        Wecomple-        of the normative aspect of causal judgements as
                 mented the causal judgement stories with 58 ex-         madebyhumans: humanstendtoascribe causality
                 amples testing reasoning about necessary and suffi-     moreeasily when the causal factor is unusual in a
                 cient causes given a description of a set of events     statistical sense, or when it violates an established
                 (example scenario). These examples were obtained        rule or behavioral norm (Halpern, 2008; Phillips
                 by modifying those in (Kıcıman et al., 2023) to         et al., 2015; Kominsky et al., 2015; Halpern and
                 correct ambiguities and incorrect outputs.              Hitchcock, 2015; Icard et al., 2017). For instance,
                    Thefirst 30 examples in (Kıcıman et al., 2023)       in example 136 of Table 5, the event can only oc-
                 were constructed from 15 scenarios introduced in        cur if both Alice and Zoe log on simultaneously
                 different studies over the years to critique actual     to a computer – which is indeed what happened.
                 causality (Halpern, 2016) definitions from the liter-   However, Alice was allowed to log on while Zoe
                 ature (Kueffner, 2021). Each scenario is associated    violated an established rule when she logged in. In
                 with a question about necessary cause and a ques-       such situations, humans tend to say that Alice did
                 tion about sufficient cause, specifically “Is {Actor}   not cause the event, while Zoe did. The model fails
                 a necessary cause of {Event}?” and “Is {Actor} a        to capture this nuance and instead it applies straight-
                 sufficient cause of {Event}?”. To test LLM memo-        forward causal reasoning and concludes that Alice
                 rization issues, (Kıcıman et al., 2023) constructed     is a cause.
                 28 additional examples from 14 new scenarios ob-          Looking at the reasoning traces for Gemini 2.0
                 tained by adapting the original ones to a chemistry     Flash responses to questions on sufficient and nec-
                 lab setting involving reagents, mixtures and crys-      essarycausesrevealsthat,whilethemodelachieves
                 tals.                                                  very close performances in terms of precision and
                    Three experts of causal reasoning analysed each      recall, it fails on identifying sufficient causes (11
                 exampleandsuggestedminimalchangestoresolve              errors out of 28 examples) more often than identify-
                                                                    26488
