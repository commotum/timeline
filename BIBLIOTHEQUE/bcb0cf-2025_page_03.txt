                    Published as a conference paper at ICLR 2025
                    FPI are designed to find a fixed point z that satisfies z = f(z,x). Here, f is the neural network layer’s
                    function, x is the input to the layer, and z is the fixed-point we compute by iteration, z(k+1) = f(z(k),x),
                    where z(k) is the estimate at iteration k. The process begins with an initial guess z(0) and iterates until
                    convergence, ∥z(k+1) − z(k)∥ < ϵ where ϵ is a small tolerance. Furthermore, Liao et al. (2018) shows a
                    crucial theorem:                                    
                                                    z = J⊤,h∗z +  ∂L ∂y    ⊤.                              (1)
                                                         F              ∗
                                                                   ∂y ∂h
                    The equation determines the new state z by applying a transformation to the current state, adjusted by
                    transposed gradients of the loss with respect to outputs and parameters reduces memory usage during
                    backpropagation by not storing all intermediate vectors. The theoretical underpinnings of FPI in neural
                    networks have been further solidified by works such as Barrett & Bolt (2024), who developed frameworks for
                    differentiating through nonsmooth iterative algorithms.
                    2.2  ADAPTIVE COMPUTATION AND INTROSPECTIVE NEURAL ARCHITECTURES
                    The idea of dynamically adjusting computational effort based on input complexity has gained significant
                    traction in recent years. Granas & Dugundji (2003) introduced Adaptive Computation Time (ACT) for
                    recurrent neural networks, enabling models to learn the optimal number of computational steps. Building on
                    this, Figurnov et al. (2017) extended the concept to spatially adaptive computation time for image recognition
                    tasks. Subsequent approaches have explored various mechanisms for adaptive computation. Huang et al.
                    (2016) proposed stochastic depth networks that randomly drop layers during training, implicitly creating
                    an ensemble of networks with varying depths. Similarly, Huang et al. (2017a) introduced dense networks
                    with early-exit branches, allowing for dynamic inference paths. Banino et al. (2021) developed PonderNet,
                    which learns to adapt the number of computational steps dynamically, while Elbayad et al. (2019) introduced
                    depth-adaptive transformers for efficient language processing. Recent advances in dynamic layer selection,
                    such as DynaLay, which introduces an introspective approach for selecting layers in deep networks Mathur &
                    Plis (2023) and Mixture of Depth (MOD) Raposo et al. (2024) dynamically adjusts processing depth based
                    oninput complexity, optimizing computation in CNNs and Transformers. While MoD selects key channels or
                    token routes, MIND model extends this concept by employing fixed-point iteration (FPI) and introspection to
                    dynamically adapt both layer depth and computation. Unlike MoD’s static depth adjustments, MIND model’s
                    iterative refinement enables more granular control, enhancing efficiency and performance across varying
                    input complexities.
                    OurMINDmodelbuildsupontheseideasbyincorporatingadedicatedintrospection network that analyzes
                    activation patterns to make informed decisions about computational paths. This approach conceptually relates
                    to the meta-learning framework proposed by Andrychowicz et al. (2016), where a separate network learns to
                    optimize the prediction network.
                    3   MINDMODEL: DEEPLEARNINGMODELWITHINTROSPECTION
                    WepresenttheMINDmodelframework,illustratedinFigure1,whichintroducesanintrospective mechanism
                    in a combination with FPIs into deep learning models to achieve adaptive computation. The MIND model
                    consists of two main components: the introspection network and the prediction network. The introspection
                    network serves as the central unit that assesses the complexity of the input and the current activation states,
                    dynamically adjusting the computational graph of the prediction network to optimize resource allocation and
                    computational efficiency for specified tasks.
                    3.1  INTROSPECTION NETWORK ARCHITECTURE
                    Theintrospection network I is the core contribution of our framework, responsible for analyzing intermediate
                    activations and determining the computational pathway within the prediction network P. By dynamically
                    adjusting the computational graph based on input complexity, the introspection network enables the MIND
                                                             3
