                                                                  Dyna,  an  Integrated                                Architecture 
                                                              for  Learning,  Planning,                                   and  Reacting 
                                                                                               Richard      S.  Sutton 
                                                                                    GTE  Laboratories            Incorporated 
                                                                                            Waltham,        MA  02254 
                                                                                                 gutton~gte.com 
                                                       Abstract 
                            Dyna  is an  AI  architecture        that  integrates     learning,                                  Situation/ 
                            planning,     and  reactive      execution.      Learning  meth-                                       State     L----~J                     Action 
                            ods  are  used  in  Dyna  both  for  compiling            planning 
                            results    and  for  updating       a  model  of  the  effects  of 
                            the  agent's  actions  on  the  world.  Planning            is  incre- 
                            mental  and  can  use  the  probabilistic            and  ofttimes                  Figure    1:  The  Problem         Formulation        Used  in  Dyna.         The 
                            incorrect    world  models  generated           by  learning     pro-               agent's  object  is to  maximize  the  total  reward  it  receives  over 
                            cesses.  Execution       is  fully  reactive   in  the  sense  that                 time.  1 
                            no  planning     intervenes     between  perception         and  ac- 
                            tion.   Dyna  relies  on  machine  learning  methods  for                           REPEAT  FOREVER: 
                            learning    from  examples--these           are  among  the  ba- 
                            sic building  blocks  making  up  the  architecture--yet                            1.   Observe      the    world's    state    and  reactively        choose  an 
                            is  not  tied  to  any  particular        method.      This  paper                            action  based  on  it; 
                            briefly  introduces      Dyna  and  discusses  its  strengths                       2.   Observe  resultant        reward  and  new  state; 
                            and  weaknesses  with  respect  to  other  architectures. 
                                                                                                                3.   Apply  reinforcement          learning    to  this  experience; 
                                        1     Introduction              to   Dyna                               4.   Update  action  model  based  on  this  experience; 
                     The  Dyna  architecture          attempts      to  integrate                               5.   Repeat     K  times: 
                         •  Trial-and-error       learning    of  an  optimal     reactive  policy,  a                 5.1  Choose  a hypothetical           world  state  and  action; 
                            mapping  from  situations           to  actions;                                           5.2   Predict  resultant      reward  and  new  state  using  action 
                                                                                                                             model; 
                            Learning  of  domain  knowledge  in  the  form  of  an  action                             5.3  Apply  reinforcement           learning     to  this  hypothetical 
                            model,  a  black  box  that  takes  as  input  a  situation             and                      experience. 
                            action  and  outputs        a  prediction     of  the  immediate       next 
                            situation;                                                                                         Figure  2:  A  Generic  Dyna  Algorithm. 
                         •  Planning:      finding  the  optimal       reactive    policy  given  do- 
                            main  knowledge  (the  action  model); 
                         •  Reactive      execution:       No  planning        intervenes     between           The  main  idea  of  Dyna  is  the  old,  commonsense                  idea  that 
                            perceiving      a  situation    and  responding       to  it.                       planning     is  'trying    things    in  your  head,'      using  an  internal 
                                                                                                                model  of  the  world  (Craik,          1943;  Dennett,       1978;  Sutton       & 
                                                                                                                Barto,  1981).  This  suggests  the  existence  of a more  primitive 
                     In  addition,      the  Dyna  architecture          is  specifically     designed          process  for  trying  things  not  in  your  head,  but  through            direct 
                     for  the  case  in  which  the  agent  does  not  have  complete                and        interaction     with  the  world.         Reinforcement        learning  is  the 
                     accurate     knowledge  of the  effects  of  its  actions  on  the  world                  name  we  use  for  this  more  primitive,            direct    kind  of  trying, 
                     and  in  which  those  effects  may  be  nondeterministic.                                 and  Dyna  is the  extension  of reinforcement             learning  to  include 
                     Dyna  assumes  the  agent's  task  can  be  formulated               as  a  reward         a learned  world  model. 
                      maximization       problem  (Figure         1).  At  each  discrete      time  in-        The  essence  of Dyna  is given  by  the  generic  algorithm               in  Fig- 
                     terval,   the  agent  observes  a  situation,          takes  an  action  based            ure  2.  In  this  algorithm,       an  "experience"       is  a  single  unit  of 
                     on  it,  and  then,  after  one  clock  tick,  observes  a  resultant            re-       experience  consisting  of a starting          state,  an  action,  a resulting 
                     ward  and  new  situation.           The  agent's      objective    is  to  choose         state,   and  a  resulting    reward.     The  first  step  of  the  algorithm 
                      actions  so  as  to  maximizethe          total  reward  it  receives  in  the            is  simply    that   of  a  reactive     system;     the  agent  reads  off  of 
                     long-term.  1 This  problem  formulation              has  been  used  in  stud-           its  reactive    policy  what  to  do  in  the  current         situation.     The 
                     ies  of  reinforcement       learning     for  many  years  and  is  also  be-             first  three  steps  together       comprise     a  standard     reinforcement 
                     ing  used  in  studies       of  planning      and  reactive      systems     (e.g.,       learning    agent.    Given  enough  experience,           such  an  agent  can 
                      Russell,    1989).    Although       somewhat       unfamiliar,      the  reward          learn  the  optimal  reactive  mapping  from  situations               to  action. 
                      maximization        problem  is  easily  mapped  onto  most  problems                     The  fourth  step  is  the  learning         of  domain  knowledge  in  the 
                      of  interest.                                                                             form  of  an  action  model  (Lin,  1991)  that  can  be  used  to  pre- 
                                                                                                                dict  the  results    of  actions.    The  fifth  step  of  the  algorithm        is 
                          1Most  systems        actually    slightly    discount     delayed  reward            essentially    reinforcement       learning     from  hypothetical,        model- 
                      relative   to  immediate       reward.                                                    generated      experiences;     this  is  in  effect  a  planning     process. 
                      SIGART  Bulletin,          Vol.  2,  No.  4                                         160 
                       The  theory  of  Dyna  is  based  on  the  theory  of  dynamic                  pro-         may  be  essentially       the  same  as  traditional        kinds  of planning, 
                       gramming       (e.g.,  Bertsekas,       1987)  and  on  the  relationship          of        but  for  others     it  is  clearly    different.     The  following  section 
                       dynamic      programming          to  reinforcement        learning     (Watkins,           discusses  planning  in  Dyna  further. 
                       1989;  Barto,  Sutton  & Watkins,  1990),  to  temporal-difference                           Among  the  reinforcement             learning      algorithms       that   can  be 
                       learning    (Sutton,     1988),  and  to  AI  methods          for  planning     and         used  in  Steps  3  and  5.3  of  the  Dyna  algorithm                 (Figure     2) 
                       search  (Korf,  1990).          Werbos  (1987)  has  previously              argued          are  the  adaptive       heuristic    critic   (Sutton,      1984),  the  bucket 
                       for  the  general      idea  of  building        AI  systems       that   approxi-           brigade    (Holland,      1986),  and  other  genetic  algorithm              meth- 
                       mate  dynamic  programming,               and  Whitehead        (1989)  and  oth-           ods  (e.g.,  Grefenstette         et  al.,  1990).     For  concreteness,        con- 
                       ers  have  presented       results  for  reinforcement         learning  systems            sider  the  simplest,      most  recent,  and  perhaps           most  promising 
                       augmented        with  with  an  action          model  used  for  planning.                method,       Q-learning (Watkins,            1989).    The  basic  idea  in  Q- 
                       More  recently,       Riolo  (1991)  and  Grefenstette              et  al.  (1990)         learning  is to  learn  an  evaluation         function  that  gives  the  value 
                       have  explored  in  different  ways  the  use  of  action  models  to-                      of performing       each  action  in  each  state.        This  function  is usu- 
                       gether    with  reinforcement          learning     methods      based  on  clas-            ally  denoted      Q(x,  a),  where  x  is  a  state       and  a  is  an  action 
                       sifter  systems.      Mahadevan        and  Connell  (1990)  have  applied                   (the  name  "Q-learning"          comes  from  this  choice  of  notation). 
                       reinforcement       learning  methods  together  with  ideas  from  sub-                    When  using  Q-learning,           the  action  chosen  in  a state  x is usu- 
                       sumption       architectures       to  a  real  robotic     box-pushing        task.        ally  simply  the  action  a  for  which  Q(z,  a) is  maximal. 
                       Lin  has  explored        Dyna  architectures           and  related      ideas  on 
                       both  simulated        (Lin,  1991)  and  real  robot  tasks  (Lin,  per-                   The  update       algorithm      for  Q-learning       can  be  expressed        in  a 
                       sonal  communication).                                                                      general  form  as  a  way  of moving  from  a unit  of experience                   to 
                                                                                                                   a  training     example  for  the  evaluation           function.      This  train- 
                                          2      Components               of  Dyna                                 ing  example  is then  input  to  a supervised             learning  algorithm. 
                                                                                                                    Just  as  in  learning    the  action  model,  the  choice  of supervised 
                       Instantiating       the  Dyna  architecture          involves  selecting       three        learning     algorithm       will  have  a  strong       effect  on  the  perfor- 
                       major  components:                                                                          mance  of  the  Dyna  architecture,               but  is  not  a  part  of  the 
                                                                                                                   architecture      itself.   Recall  that  a  unit  of  experience          consists 
                           •  The  structure      of  the  action  model  and  its  learning  algo-                of  a  starting    state   (z),  an  action  (a),  a  next  state  (y),  and  a 
                              rithms;                                                                              reward  (r).  From  this  one  forms  the  training               example: 
                           •  An  algorithm        for  selecting     hypothetical       states   and  ac-                        Q(z,  a)      should  be        r+7m~xQ(y,b), 
                              tions  (Step  5.1,  search  control).                                                where  % 0 <  3' <  1, is a constant           that  determines        the  relative 
                           •  A  reinforcement        learning     method,     including      a  learning-         value  of  short-term        versus  long-term        reward.      Strong  formal 
                              from-examples         algorithm      and  a  way  of  generating         vari-       results    are  available  for  the  case  in  which  the  Q  function  is 
                              ety  in  behavior.                                                                   implemented         as  a  table.    For  that  case,  Watkins          (1989)  has 
                                                                                                                   shown  that  Q-learning           from  real  experiences--direct             agent- 
                       The  structure       and  learning       of  the  action      model  lie  mostly            environment        interaction     without  using  an  action  model--will 
                       outside  the  the  scope  of  the  Dyna  architecture.                Recall  that          converge  to  the  optimal  behavior  under  weak  conditions. 
                       the  action  model  is meant  to  be  simply  a mimic  of the  world;                                3      Planning           and     Reacting           in   Dyna 
                       it  takes  in  a  description       of  a  state   and  an  action  and  emits 
                       a  prediction      of  the  immediate         resulting     state    and  reward.           Just  as  reinforcement        learning  with  real  experience          (Steps  1- 
                       Actual  experience         with  the  world  continually            produces      ex-       3)  is  meant  to  learn  the  optimal         way  of behaving  for  the  real 
                       amples  of  desired  behavior           for  such  a  model.       These  can  be           world,  reinforcement           learning    with  hypothetical          experience 
                       used  in  conjunction        with  any  of  a  large  number  of  learning                  (Steps  5.1-5.3)  is meant  to  learn  the  optimal  way  of behaving 
                       algorithms      for  supervised      learning     (learning    from  examples).             given  the  action  model.          Reinforcement         learning     with  hypo- 
                       The  design  of  that  algorithm,            its  knowledge       representation            thetical    experience  is in  fact  an  incremental           form  of planning 
                       and  generalization        capabilities     will  of  course  have  a large  ef-            that    is  closely  related      to  dynamic       programming.           Here  we 
                       fect  on  the  quality      of  the  learned      model,  on  how  efficiently              will  call  it  incremental  dynamic  programming, after  Watkins 
                       it  is  learned,   and  on  how  easily  it  can  be  primed  with  prior                   (1989),  or  IDP planning  for  short.              Assuming  IDP  planning 
                       domain  knowledge.          Nevertheless,       we consider  those  issues  to              steps  can  be  done  relatively          quickly  and  cheaply  compared 
                       be  outside  the  scope  of the  Dyna  architecture             per  se.  Because           to  real  steps  (i.e.,  K  >>        1)  and  that     the  model  is  correct, 
                       Dyna  makes  no  strong  assumptions                about  the  action  model,              IDP  planning        will  greatly     speed  the  finding  of  the  optimal 
                       it  can  use  a wide  variety  of methods          now  existent  or  yet  to  be           policy.     In  small  tasks  this  has  been  shown  to  be  true  even 
                       developed.      One  assumption         Dyna  does  make  that  is not  true                if  the  model  must  be  learned          as  well  or  if  the  world  changes 
                       of some  supervised        learning  methods  is that  they  can  operate                   (Sutton,     1990). 
                       incrementally,  that  is,  processing          examples  one  by  one  rather 
                       than  saving  them  up  and  making  multiple  passes.                                      Results  from  dynamic  programming                  (Bertsekas      & Tsitsiklis, 
                       At  this  time  little  can  be  said  about  how  hypothetical            starting         1989)  can  be  adapted          to  show  that  IDP  planning            based  on 
                       states   and  actions  should  be  selected.             It  can  be  done  in  a           the  tabular      version  of  Q-learning          converges      onto  the  opti- 
                       large  variety     of  ways,  but  there  has  been  little  experience                     mal  behavior  given  the  action  model.  This  is  a  strong  result 
                       with  any  but  the  simplest.        For  example,  in  my  previous  work                 because  it  applies  to  nondeterministic              environments         and  no 
                       I  have  selected  among  previously            observed  states  at  random,               matter     how  deep  a search  is  required          to  find  the  optimal      ac- 
                       either  uniformly       or  in  proportion      to  their  frequency       of  prior        tions.    Strictly,    it  applies  only  to  the  tabular         case,  but  the 
                       occurrence.       This  is  essentially      the  issue  of  search control--               results  should  be  similar  for  supervised             learning    methods  to 
                       what  part  of  the  state  space  shall  be  worked  on  (planned                          the  extent     that  they  can  accurately         approximate        the  desired 
                       about)     next?      Larger  problems         will  of  course  require        that        functions. 
                       the  search  be  controlled        more  carefully.       For  some  choices  of            Dyna  is fully  reactive in  the  sense  that  no planning  intervenes 
                      search  control  method,           the  form  of  planning         done  in  Dyna            between  observing          a  state    and  taking      an  action  dependent 
                                                                                                           161                                       SIGART  Bulletin,            Vol.  2,  No.  4 
                                                                                                           4.1      Reliance       on  Supervised          Learning 
                   A)       Situation        ~[    Planner  ]        ~   Action 
                                               t               I                                   On  realistic  problems,       the  state  space  is  obviously  far  too 
                                                                                                   large  for table-based     approaches,  and  thus  Dyna  must  rely on 
                                                                                                   methods  for  learning  and  generalizing  from  examples.              How- 
                                                                                                   ever,  despite  enormous  amounts  of work in several  disciplines, 
                                             _ (Reactive                                           fully  satisfactory    methods  for  supervised  learning  remain  to 
                   B)       Situation        v k,~ Policy j/         ~   Action                    be found.  For  example,  there  remain  difficult  issues  in gener- 
                                                                                                   alization  and  knowledge  representation           that  must  be  solved. 
                                                                                                   Nevertheless,  I do  not  feel  it  is  inappropriate        to  base  an  in- 
                                                                                                   tegrated    architecture     on  a  capability    for  effectively  learning 
                                                                                                   from  examples.       Would  not  any  integrated         architecture    rely 
                   C)       Situation        ~/WReactive~            ~   Action                    on  such'a  capability      at  some  level?  Any  architecture          using 
                                                                                                   analogy,  compilation,       reminding,  or  even  similarity  would  do 
                                                                                                   so.  If the  answer  is  clearly  'yes,'  then  why  not  build  this  in 
                                                [              ]                                   as  a  basic  part  of the  architecture? 
              Figure  3:  Simplistic  Comparison           of  Architectures:     A)  Con-                           4.2     Hierarchical         Planning 
              ventional  Planning,       B)  Reactive  Systems,  C)  IDP  Planning 
              (incremental     compiling  into  reactions).                                        Dyna  as described  is a very  flat  system.  It  plans  at  the  level of 
                                                                                                   individual  actions.  If those  actions  are  muscle  twitches,  then 
                                                                                                   Dyna  will be  of no help  planning  a trip  across  the  country-- 
              on  that  state.   In  the  Dyna  algorithm  given  in  Figure  2, IDP               and  neither  will  any  other  planner  that  operates  at  a  single 
              planning  takes  place  after  action  selection,  but  conceptually                 level.  Planning  must  be  done  at  several  levels  and  the  results 
              these  processes  proceed  in  parallel.  2 The  critical  issue  is that            combined  in some  way.  We have  had  lots  of experience  doing 
              planning  and  reacting  processes  are  not  strongly  coupled:  the                this  with  conventional  planners,  but  it has  not  been  tried  with 
              agent  never  delays  responding  to  a situation  in  order  to  plan               Dyna.  To  my  knowledge  there  is  no  reason  as  yet  to  think 
              a  response  to  it.  Although  the  agent  always  responds  reac-                  that  hierarchical     planning  will  be  either  easier  or  harder  in 
              tively  and  instantly,    this  does  not  mean  it  must  immediately              Dyna  than  it  is  in  conventional      planners. 
              respond  decisively;  for  example,  it  may  choose  the  response 
              of sitting  still.  Figure  3 contrasts     this  approach  to  combining 
              planning  and  execution  with  that  of  conventional              planning                     4.3     Ambiguous           and    Hidden       State 
              systems  and  of reactive  systems. 
              IDP  planning  has  both  advantages            and  disadvantages       com-        So far  we have  assumed  that  the  agent  can  observe  the  rele- 
              pared  to  other  planning  methods.            The  primary  advantage              vant  aspects  of the  world's  state  at  no cost  and  on  every  time 
              is  that  it  is  totally  incremental;    any  time  spent  planning  re-           step,  assumptions       that  are  clearly  violated  in  many  tasks  of 
              sults  in  an  improvement        in  the  agent's  immediate       reactions        interest.     This  is  a  limitation    that   Dyna  shares  with  most 
              or  evaluation     function     for  some  state.     Thus,  performance             other  planning       and  problem  solving  systems--they             are  all 
              continually  improves,  and  arbitrarily         long  optimal  sequences             based  on  state.    For  example,  a robot  may  not  be  able  to  de- 
              of  actions  can  be  found.  In  addition,  it  readily  handles  non-               termine  from  its immediate  surroundings           which  of two similar 
              deterministic     tasks  and  is extremely  general  in  that  it  makes              rooms  it  is  in,  or  whether  a door  is locked,  or  whether  there 
              no  assumptions       about  the  world  other  than  that  is can  be  at           is a person  in the  room  on  the  other  side  of the  door.  In these 
              least  partially    predicted.                                                        cases  the  robot  cannot  unambiguously           determine  the  world's 
              The  primary  disadvantage           of  IDP  planning  is  that  it  may             state,  as  much  of it  is  hidden  from  him. 
              require  large  amounts  of memory.  Whereas  traditional                plan-        There  are  a number  of techniques  for dealing  with  this  prob- 
              ning  methods       are  based  on  constructing         search  trees  and           lem,  though  none  is clearly  a general  solution.  In some  cases, 
              backing-up  evaluations  on demand,  IDP  planning  is based  on                      uncertainty     about  the  true  state  on  the  world  can  be  mod- 
              storing  backed-up  evaluations  (and  possibly  reactions)  associ-                  eled  as  probabilistic    state  transitions    (Kaelbling,     1990).  Ap- 
               ated  with  each  state  or  state-action      pair.  Even  if supervised            proaches  such  as  Dyna  that  can  handle  stochastic            tasks  can 
              learning  methods        are  used  instead  of  tables,  this  is  still  a          then  be  used  without  change.         In  other  cases,  the  state  de- 
               memory-intensive        approach.     It  will  require  far  more  memory           scription  can  be  augmented  with  past  inputs  to disambiguate 
               than  depth-first    search,  for  example.                                          state.  For  example,  a robot  may  not  be  able  to  sense  a  wall 
                                                                                                    in  front  of  it,  but  if  it  remembers    that  it  just  bumped  into 
                        4     Potential         Problems          with     Dyna                     it  and  backed  off,  and  makes  that  memory  part  of  the  cur- 
                                                                                                    rent  state  description,     then  the  situation     can  be  handled  by 
               In  the  rest  of  this  paper  we  briefly  discuss  a  number  of  po-             state-based    methods. 
               tential  problems  with  the  Dyna  architecture.                                    Whitehead      and  Ballard  (1991)  have  proposed  learning  per- 
                   ~The  Dyna  algorithm  given  in  Figure  2 also  sacrifices  re-                ceptual  strategies     for  disambiguating       state  descriptions     cre- 
               activity  somewhat  for  the  sake  of pedagogy.  A more  fully  re-                 ated  by  a  marker-based          visual  system.        Ming  Tan  (per- 
               active  version  of the  algorithm  would  move  Step  5 inbetween                   sonal  communication)          has  also  explored       the  use  of  Cost- 
               Steps  1 and  2.  More  generally,  the  four  main  functions  of the               Sensitive  learning  in  reinforcement        learning  for a similar  pur- 
               algorithm--reacting,        reinforcement      learning,  model  learning,           pose.    There  is  considerable       relevant    work  in  the  dynamic 
               and  IDP  planning--should           be  thought  of  as  running  simul-            programming  literature,         but  that  direction  has  not  been  ex- 
               taneously  and  independently.                                                       plored  yet. 
               SIGART  Bulletin,         Vol.  2, No.  4                                       162 
                                   4.4       Ensuring           Variety        in  Behavior                               Computational            Neuroscience,          M.  Gabriel        and  J.W.  Moore 
                      In  order  to  maintain            an  accurate        action  model,  the  agent                   (Eds.),  539-602,  MIT  Press. 
                      must  try  actions  that  it  believes  to  be  inferior.                     If  it  only          Bertsekas,  D.  P.  (1987)  Dynamic  Programming:                          Determinis- 
                      tries  those  that  it  believes  are  best,  and  the  world  changes,                             tic  and  Stochastic  Models,  Prentice-Hall. 
                      it  may  never  discover  the  change  and  never  discover  what-                                  Bertsekas,  D. P. & Tsitsiklis,  J.  N. (1989)  Parallel  Distributed 
                      ever  new  actions  are  really  best.  The  simplest  way  to  ensure                              Processing:  Numerical  Methods,  Prentice-Hall. 
                      behavioral       variety  is  to  require  the  agent  to  choose  an  ac- 
                      tion  at  random  a  small  percentage                  of  the  time.      This  crude             Cralk,  K.  J.  W.  (1943)  The  Nature  of  Explanation.                            Cam- 
                      strategy      has  many  disadvantages,               but  is  adequate        for  many            bridge  University  Press,  Cambridge,                   UK. 
                      problems.        Another       approach  is  to  choose  actions  based  on                         Dennett,  D.  C.  (1978)  Why  the  law  of effect  will not  go away. 
                      a probability        distribution,       such  as  a Boltzmann  distribution,                       In  Brainstorms,  by  D.  C.  Dennett,                71-89,  Bradford  Books. 
                      that  favors  the  apparently             best  actions,  but  does  not  select 
                      them  100%  of the  time.  If desired,  the  'temperature'                          of  the         Grefenstette,        J.  J.,  Ramsey,  C.  L.,  &  Schultz,  A.  C.  (1990) 
                      distribution       can  be  reduced  over  time  to  increase  the  prefer-                         Learning  sequential            decision  rules  using  simulation                models 
                      ence  for  the  apparent          best  actions  (Watkins,            1989),  but  this             and  competition.          Machine  Learning  5, 355-382. 
                      creates  agMn  the  inability  to  handle  long-term  changes  in the                               Holland,      J.  H.  (1986).         Escaping  brittleness:            The  possibil- 
                      world.      The  'adaptive         heuristic      critic'   architecture        (Sutton,            ities  of  general-purpose           learning      algorithms        applied  to  par- 
                      1984)  also  has  this  problem.            Perhaps  the  best  solution  devel-                    allel  rule-based        systems.        In  R.  Michalski,          J.  Carbonell        & 
                      oped  so  far,  though  still  far  from  perfect,  is  the  exploration                            T.  Mitchell,  Eds.,  Machine  learning  II,  Morgan  Kanfmann. 
                      bonus  proposed  by  Sutton  (1990). 
                                                                                                                          Ka~lbling,        L.  P.  (1990)  Learning  in  Embedded                       Systems. 
                                                     4.5       Taskability                                                Ph.D.  thesis,  Stanford  University. 
                      Superficially,       the  Dyna  architecture            is  not  taskable.       Dyna  is           Korf,  R.  E.  (1990)  Real-Time                 Heuristic       Search.       Artificial 
                      based  on  the  reward  maximization                  problem  (Figure  1) which                    Intelligence  42:  189-211. 
                      recognizes  only  one  goal,  the  maximization                      of  total  reward              Lin,  Long-Ji.         (1991)  Self-improving              reactive     agents:      Case 
                      over  time.  In  addition,          the  object  of the  planning  and  learn-                     studies  of  reinforcement             learning  frameworks.              In:  Proceed- 
                     ing  processes  are  to learn  one  policy  function  that  maps  states                            ings  of  the  International              Conference  on  the  Simulation  of 
                      to  actions  with  no  explicit  'goal'  input.                 However,  this  may                Adaptive  Behavior,  297-305,  MIT  Press. 
                      merely  mean  that  the  goal  specification                  must  be  part  of the                Mahadevan,  S. & Connell,  J.  (1990)  Automatic                        programming 
                     state    description.         For  example,  consider  a  Dyna  robot  re-                          of  behavior-based           robots  using  reinforcement              learning.      IBM 
                      warded  for  picking  up  trash,  but  which  must  recharge  its                                  technical  report. 
                      battery     occassionally.         When  its  battery           is  running  low  the 
                     optimal  behavior  will  be  to  search  out  the  recharger,  whereas                              Riolo,  R.  (1991)  Lookahead  planning  and  latent  learning  in  a 
                     when  it  has  plenty  of  power  the  optimal  behavior  will  be  to                              classifier  system.         In:  Proceedings  of  the  International                  Con- 
                     search  out  more  trash.  If the  charge  on  the  battery  is  part  of                           ference  on  the  Simulation  of Adaptive  Behavior,  MIT  Press. 
                     the  state  description         then  these  two  apparent  goals  can  easily 
                     be  part  of a single  policy.                                                                      Russell,  S. J.  (1989)  Execution  architectures                   and  compilation. 
                     Similarly,  to  train  a dog,  e.g.,  to  heel  or  to  roll  over,  one  pro-                      Proceedings  of IJCAI-89,  15-20. 
                     vides  distinctive        cues,  e.g.,  movements  or  sounds,  that  signal                        Sutton,  R.  S. (1984)  Temporal  credit  assignment  in reinforce- 
                     to  the  animal  which  of its  actions  will be  rewarded  now.  It can                            ment  learning.          PhD  thesis,  COINS  Dept.,  Univ.  of  Mass., 
                     be  time-consuming             to  teach  animals  new  behaviors  because                          Amherst,  MA  01003. 
                     of  the  absence  of  a  common  language.                      It  may  be  possible               Sutton,      R.S.  (1988)  Learning  to  predict  by  the  methods  of 
                     to  task  Dyna  agents  more  directly  than  that.  If one  directly                               temporal  differences.  Machine  Learning  3: 9-44. 
                     modifies  the  part  of  the  action  model  that  predicts  reward, 
                     that  could  in  turn  cause  the  policy  to  change  substantially                                Sutton,      R.  S.  (1990)  Integrated              architectures        for  learning, 
                     through  IDP  planning.                                                                             planning,  and  reacting  based  on  approximating                       dynamic  pro- 
                                                                                                                         gramming.         Proceedings  of  the  SevenLh  International                    Confer- 
                              4.6       Incorporation               of  Prior       Knowledge                            ence  on Machine  Learning,  216-224. 
                     Prior      knowledge         can  be  incorporated               in    Dyna  systems                Sutton,      R.S.,  Barto,  A.G.  (1981)  An  adaptive  network  that 
                     through       the  initial     values  of  the  policy  and  internal               evalu-          constructs       and  uses  an  internal            model  of  its  environment. 
                     ation  functions         such  as  the  Q  function.               In  principle       this         Cognition  and  Brain  Theory  Quarterly  4: 217-246. 
                     could  be  a  very  flexible  and  efficient  method,                     but  there  is            Watkins,  C.  J.  C.  H.  (1989)  Learning  with  Delayed  Rewards. 
                     little   work  with  it  yet.          Lin  (personal         communication)            has         PhD  thesis,  Cambridge              University       Psychology  Department. 
                     demonstrated          in  preliminary         results  a  very  effective  method                   Werbos,  P.  J.  (1987)  Building  and  understanding                           adaptive 
                     that  he  calls  'teaching'          in  which  an  outside  agent,  say  a  hu-                    systems:        A  statistical/numerical              approach        to  factory       au- 
                     man  expert,  takes  control  over  the  agent  and  demonstrates                          a        tomation  and  brain  research.  IEEE  Transactions  on Systems, 
                     correct  solution  to  the  problem.              This  experience  is processed                    Man,  and  Cybernetics,  SMC-17,  No.  1, 7-20. 
                     by  the  Dyna  system  (or,  in  Lin's  case,  Dyna-like  system)  in 
                     the  normal  way,  and  greatly  speeds  subsequent                       learning.                 Whitehead,         S.  D.,  Ballard,  D.H.  (1991)  Learning  to  perceive 
                                                                                                                         and  act  by  trial  and  error.  Machine  Learning  7:, 45-83. 
                                                        References                                                       Whitehead,         S.  D.  (1989)  Scaling  reinforcement                learning  sys- 
                     Barto,  A.  G.,  Sutton,           R.  S.,  &  Watkins,         C.  J.  C.  H.  (1990)              tems.  Technical  Report  304,  Dept.  of Computer  Science,  Uni- 
                     Learning  and  sequential              decision  making.           In  Learning  and                versity  of Rochester,           Rochester,       NY  14627. 
                                                                                                                 163                                          SIGART  Bulletin,              Vol.  2,  No.  4 
