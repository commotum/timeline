                         A Miscellaneous
                         There were several optimizations and ideas that slightly boosted our score
                         but did not fit in the main text.
                             • We found that we could use the logsoftmax probabilities obtained in
                               the DFS Candidate generation in scoring, providing a slight boost at
                               no additional compute cost.
                             • Before using scores from various augmentations, we used a selection
                               algorithm based on the similarity of generated outputs, choosing the
                               candidate that had the most pixelwise similarity to all other candi-
                               dates.
                             • Due to the nature of the contest we had a very limited time window
                               for generating our solutions. In the end we were able to completely
                               parallelize our solution on both T4 GPUs provided.
                             • There is a trade-off between Generation and Selection, that we were
                               unable to optimize for the hidden test set. Reducing the DFS cutoff
                               value provides more potentially correct solutions, but makes it harder
                               to select them. While the best values for evaluation were rather low, we
                               found higher cutoffs of up to 20% to work better on Kaggle, which had
                               the added benefit of reducing run-time. But even for low percentage
                               values the DFSreturnssurprisinglyfewresultsonaverage, seeFigure7.
                             • As can be seen in our results, increasing the size of the base model
                               substantially increases the score at every part of the pipeline.  We
                               invested a lot of time to make sure that we are able train and use the
                               8B Nemo Minitron model in a reasonable timeframe on Kaggle. We
                               believe that even larger models could elevate performance again.
                         Figure 7: Number of potential candidates returned by the DFS using a 5% cutoff value
                         on the evaluation dataset. In most cases the number of candidates is far lower than the
                         possible theoretical maximum of 20.
                                                              18
