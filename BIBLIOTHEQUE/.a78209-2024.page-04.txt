                                                                                                                                                                       Object      Attribute     Scene-text      Fact
                                                                                                                                                   Text-to-Image
                                                                                                                                                   Image-to-Text
                                                                                                                                                                  0         20       40        60        80       100
                                                                                                                                                                                     Proportion (%)
                                                                                                                                      Figure 4: Distribution of hallucination categories within
                                                                                                                                      hallucination-labeled claims of MHaluBench.
                               Figure 3: Claim-Level data statistics of MHaluBench.                                                   into individual claims) and text-to-image models
                               The claims are fine-grained atoms extracted from the                                                   (deconstructing user queries into distinct claims) 2.
                               complete “Query-Response” pairs.
                                                                                                                                      4.2        AutonomousToolSelectionViaQuery
                               for text-to-imagecases, webreakdownuserqueries                                                                    Formulation
                               into fundamental intent concepts, which are subse-                                                     After extracting essential claims from the input
                               quently regarded as claims.                                                                            image-text pair a = {v,x}, the challenge of
                                                                                                                                      hallucination detection is to aptly match each
                               3.3        HumanAnnotationandAgreement.                                                                claim with appropriate aspect-oriented tools. We
                               Our annotation criteria evaluate whether image-                                                        approach this issue by assessing whether the
                               to-text output conflicts with the input image or                                                       underlying MLLMs can generate pertinent queries
                               world knowledge and whether text-to-image visu-                                                        for a given set of claims {ci}i=1···n to provide
                               als conflict with claims or world knowledge. Ex-                                                       relevant input to the specific aspect-oriented tool.
                               tracted claims are labeled as hallucinatory or non-                                                    To facilitate this, we prompt underlying MLLMs
                               hallucinatory, with a segment deemed hallucinatory                                                     like GPT-4V/Gemini to autonomously formulate
                               if it contains any such claim; otherwise, it is labeled                                                meaningful queries. Demonstrated in Figure 5, this
                               non-hallucinatory. An entire response is labeled                                                       module yields custom queries for each claim, or
                               hallucinatory if it includes even one hallucinatory                                                    “none”whenatoolisunnecessary. For example,
                               segment. We allocate the dataset uniformly across                                                      the framework determines that claim1 calls for the
                               three annotators with graduate-level qualifications                                                    attribute-oriented question “What color is
                               for independent categorization. Decisions in un-                                                       the uniform of the athlete on the
                               certain cases were initially held by individual an-                                                    right side?” andtheobject-oriented inquiry
                               notators and later resolved by majority rule. Inter-                                                   “[‘athlete’, ‘uniform’]”, bypassing the need
                               annotator reliability, measured by Fleiss’s Kappa                                                      for scene-text and fact-oriented tools.
                               (κ), shows significant agreement (κ = 0.822) over                                                      4.3        Parallel Tool Execution
                               the full annotated dataset, indicating a high level of                                                 Leveraging queries autonomously generated from
                               concordance within the range 0.80 ≤ κ ≤ 1.00.                                                          various perspectives, we simultaneously deploy
                               4       UNIHD:UnifiedHallucination                                                                     these tools in response to the queries, gathering
                                       Detection Framework for MLLMs                                                                  a comprehensive array of insights to underpin the
                                                                                                                                      verification of hallucinations. The specific tools
                               Wepresent UNIHD in Figure 5 and follow. The                                                            employed in our framework are detailed below, se-
                               specific prompts are listed in Appendix A                                                              lected for their ability to effectively address a wide
                               4.1        Essential Claim Extraction                                                                  range of multimodal hallucination scenarios:
                                                                                                                                      •    Object-oriented tool: We employ the open-set
                               Toidentify fine-grained hallucinations within the                                                          object detection model Grounding DINO (Liu
                               response, claim extraction is a prerequisite. Fol-                                                         et al., 2023d) for capturing visual object infor-
                               lowing the procedure in §3.2, we employ the ad-                                                            mation, crucial for detecting object-level hallu-
                               vanced instruction-following abilities of MLLMs                                                            cinations. For instance, inputting “[‘athlete’,
                               for efficient claim extraction. Specifically, GPT-                                                        ‘uniform’]” prompts the model to return two
                               4V/Gemini is adopted as the base LLM to effi-                                                                2In subsequent experiments, our framework builds upon
                               ciently derive verifiable claims from the outputs                                                      the pre-annotated claims available in MHaluBench, and the
                               of image-to-text models (extracting each response                                                      claim extraction is only necessary in the open-domain setting.
                                                                                                                              3238
