                                                       LiDARbackbone            Decoder (Detector)               Total
                                                    params          cost       params       cost      mAP NDS           cost∗
                                 CMT[39]              8.5          155.1         4.8       163.6      70.3   72.9       318.7
                           SparseVoxFormer-base   2.5(−71%)     50.0(−68%)       4.5    61.3(−63%)    70.8   73.2   111.3(−65%)
                              CMTw/DSVT               18.0         347.5         4.8       163.6      70.8   73.2       511.1
                             SparseVoxFormer      12.1(−33%)    242.4(−30%)      4.6    39.9(−76%)    72.2   74.4   282.3(−45%)
               Table 4. Module-wise computational cost analysis. Params and cost means the number of parameters (M) and the computational cost
               (GFLOPs), respectively. Since the camera backbone part (VoVNet [15]) is identical across all variants, the details are omitted. ∗For the
               total cost, the common cost for camera backbone is excluded for clearer comparison.
               while significantly reducing the number of transformer to-        enhances the practical usefulness of our approach by regu-
               kens. These experimental results demonstrate that directly        larizing a consistent computational cost.
               using sparse voxel features with a higher resolution is not
               only a feasible approach for efficiently handling 3D object       Input voxel resolutions    According to input voxel resolu-
               detection, but it can also be a more effective method than        tions, our model performance can be further improved (Ta-
               previous BEV feature-based approaches.                            ble 3 right). A smaller voxel size means a larger input voxel
                                                                                 resolution for a LiDAR modality.
               Effect of sparse feature refinement      In this section, we      4.2. Computational Cost Analysis
               show how the additional sparse feature refinement effec-
               tively improves the detection accuracy of our models. Al-         An important benefit of our approach is its ability to ex-
               though our approach uses fewer computations for the Li-           ploit rich geometric information in high-resolution 3D fea-
               DARbackbone compared to previous methods, one might               tures while consuming a relatively small computational
               still question if the benefits of using additional computa-       cost. Consequently, the computational cost analysis would
               tions of DSVT can be applied to other approaches. To ad-          be essential to show the effectiveness of our architecture.
               dress this question, we compare CMT models with our               For highlighting the contrast, we conduct a module-wise
               model variants.                                                   comparisonofourmodelwithCMT[39],which,tothebest
                  In the case of CMT, the CNNs for BEV feature refine-           of our knowledge, is one of the fastest among state-of-the-
               ment have already processed sufficient geometric informa-         art models. CMTisalsosuitable for an apple-to-apple com-
               tion, resulting in a relatively smaller gain when employing       parison with our method due to its similar module compo-
               the additional DSVT compared to our model (Table 2). On           sition. We note that, for simplicity, we include the computa-
               the other hand, as we utilize only a minor part of the pre-       tional cost of DSVT modules for sparse feature refinement
               vious LiDAR backbone used in CMT, our sparse features             to a LiDAR backbone.
               may not be sufficiently encoded. Therefore, sparse feature           Table 4 shows that our architecture substantially reduces
               refinement via DSVT brings significant performance im-            computational costs while even enhancing detection perfor-
               provement. It is noteworthy that the performance gap com-         mance. We emphasize again that merely incorporating the
               pared to CMT becomes even larger thanks to our deep fu-           DSVTmoduleinto the existing model (CMT) is not as ef-
               sion approach that exploits the sparse nature of our multi-       fective. The CMT model with DSVT has a cost for a Li-
               modal fused features, realized by our explicit multi-modal        DARbackbone and a detector that is five times larger than
               fusion. Interestingly, the deep fusion module simply relo-        our base model, yet it only shows comparable performance.
               cates the DSVT blocks without any computational over-             Ourfinal model surpasses both CMT models and does so at
               head, implying that our refinement of sparse features not         a lower cost. It is also noteworthy that the effect of this cost
               only encodes rich geometric information but also facilitates      saving will be more effective in the embedded environment,
               the multi-modal fusion.                                           which is limited to use smaller backbones.
               Effect of feature elimination      Our feature elimination        4.3. Comparison
               schemecaneffectivelyreducethenumberoftransformerto-               Finally, we present a comprehensive comparison (Table 5)
               kens (from 18,000 in average) while almost preserving the         with the state-of-the-art multi-modal 3D object detection
               original performance (Table 3 left). Specifically, the perfor-    models[1,5,7,8,17,22,24,32,34,39,42,44].Allmodels
               mance almost never drops when we use 50% of tokens or             donotuseanytest-timeaugmentationandmodelensembles
               the fixed number of 10,000 tokens, and we use 10,000 to-          for this evaluation.
               kensinlaterexperiments.Wewanttonotethattheoverhead                   Although our approach pursues an efficiency-oriented
               of an auxiliary head for the feature elimination is marginal.     design, whichfocustoreducethenumberoftransformerto-
               It is noteworthy that the use of the fixed number of tokens       kensbasedonthesparsityofLiDARdata,ourmodelshows
