                                        Setting                    Ctx Len.   Coursera   GSM QuALITY TOEFL CodeU SFiction                 Avg.
                                                                           LLaMA2-Chat7B
                                     DynamicNTK-Aware(noFT)           32K       31.98    32.00     34.65      59.11     1.11     36.72    32.59
                                        NTK-Aware(s = 8,noFT)         32K       36.77    3.00      26.73      34.2      1.11     50.78    25.43
                                 YaRN(s=8,FT@32K,50epcs.)             32K       36.05    19.00     33.17      50.56     4.44     56.25    33.24
                       Resonance YaRN(s = 8, FT@32K, 50 epcs.)        32K       36.48    22.00     34.16      55.76     0.00     57.03    34.24
                                 YaRN(s=8,FT@4K,400epcs.)             32K       35.03    24.00     37.62      57.62     4.44     60.94    36.61
                       Resonance YaRN(s = 8, FT@4K, 400 epcs.)        32K       36.34    27.00     40.59      56.51     3.33     61.72    37.58
                                                                          LLaMA2-Chat13B
                                     DynamicNTK-Aware(noFT)           16K       29.22    39.00     40.59      63.94     1.11     39.84    35.62
                                        NTK-Aware(s = 4,noFT)         16K       40.26    21.00     38.12      65.43     1.11     46.88    35.47
                                YaRN(s=4,FT@16K,100epcs.)             16K       38.08    39.00     43.07      65.43     0.00     63.28    41.48
                      Resonance YaRN(s = 4, FT@16K, 100 epcs.)        16K       38.66    39.00     43.56      65.06     1.11     62.50    41.65
                                 YaRN(s=4,FT@4K,400epcs.)             16K       41.72    34.00     41.09      66.91     2.22     48.44    39.06
                       Resonance YaRN(s = 4, FT@4K, 400 epcs.)        16K       41.86    35.00     42.57      65.80     5.56     48.44    39.87
                    Table 2: Long text evaluations on some closed-ended tasks in L-Eval. “Ctx Len” means the target context length of
                    the model after scaling its PE. “FT@32K, 50 epcs” means the model is fine-tuned on 32K sequence length for 50
                    epochs. The settings with “no FT” are not fine-tuned after modifying its position embedding. We highlight the best
                    and second-best performance for each base model in Bold and Underline, respectively.
                                GovReport                     Proofpile             length. We report the results in Figure 4. Of the
                       7
                                                    6                               tested methods, RESONANCE YARN achieves the
                       6                                                            lowest perplexity across all context lengths. Espe-
                                                    4                               cially, RESONANCE YARN achieves a lower per-
                       5                                                            plexity compared to YaRN with the same set of
                      Perplexity (PPL)4             2                               hyperparameters optimized for YaRN, demonstrat-
                             10000   20000   30000        10000   20000   30000     ingthebenefitofapplyingtheResonancetechnique
                              Context Length              Context Length            to existing RoPE scaling methods.
                           Dynamic NTK            NTK-Aware Scaling (s=8)
                           YaRN (s=8)             Resonance YaRN (s=8) (Ours)
                    Figure4: TheperplexityofLLaMA-Chat7Bwithdiffer-                 6.2.3    Real-world Task Evaluation
                    ent position embeddings on GovReport and Proofpile.
                       For YaRN and RESONANCE YARN, We use a                        Lastly, we test the real-world task performance of
                    scaling factor of 8 and 4 for LLaMA2 7B and 13B                 LLaMA2-Chat7Band13B’sperformancewithdif-
                    to extend their context window from 4K to 32K                   ferent RoPE scaling strategies on L-Eval (An et al.,
                    and 16K, respectively. For the configurations that              2023)’s close ended task suite, a long-text LLM
                    require fine-tuning, we fine-tune the LLM with the              benchmark covering a wide range of domains such
                    scaled position embedding on the training set of                as school lectures, long conversations and novels.
                    PG19 (Rae et al., 2020) with the fine-tuning set-               Wefine-tunethemodelwithdifferentRoPEscaling
                    ting and hyperparameters adopted directly from                  strategies using two different strategies: training on
                    YaRN(Pengetal., 2024), with the only difference                 shorter sequences (4K length) for more epochs, and
                    being that we control the total training token count            training on longer sequences (32K or 16K length)
                    to be approximately 100M. A more detailed fine-                 for less epochs. All settings requiring fine-tuning
                    tuning setting can be found in Appendix C.2. We                 keep the training token count to be approximately
                    test the model’s performance on two TSTL sce-                   100M.Theresults are listed in Table 2.
                    narios: language modeling evaluation on long-text
                    sequences and long-text downstream application                     Although no single setting in the experiment
                    performance.                                                    achieves the best result on all subtasks, we observe
                                                                                    that applying RESONANCE YARN achieves better
                    6.2.2    Perplexity on Long Sequence                            average performance in different training settings
                    Weevaluate the model’s language modeling per-                   and model sizes compared to its counterpart YaRN
                    formance on GovReport (Huang et al., 2021) and                  setting. This further proves the compatibility of the
                    Proofpile (Azerbayev, 2022). We randomly select                 Resonance technique and RoPE scaling methods,
                    50 samples from each dataset and report the final               and the better length extrapolation performance
                    perplexity in text fragments of gradually increased             brought by our proposed method.
                                                                                593
