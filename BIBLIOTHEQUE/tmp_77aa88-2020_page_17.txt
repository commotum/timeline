                           Published as a conference paper at ICLR 2021
                                         Table 6: Hyper-parameter used to produce the CIFAR-{10,100} results
                                          CIFARDataset                LR      WD      œÅ(CIFAR-10)      œÅ(CIFAR-100)
                                 WRN28-10(200epochs)                  0.1    0.0005        0.05              0.1
                                 WRN28-10(1800epochs)                0.05    0.001         0.05              0.1
                                 WRN26-2x6ShakeShake                 0.02    0.0010        0.02             0.05
                                 Pyramid vanilla                     0.05    0.0005        0.05              0.2
                                 Pyramid ShakeDrop (CIFAR-10)        0.02    0.0005        0.05               -
                                 Pyramid ShakeDrop (CIFAR-100)       0.05    0.0005          -              0.05
                                   Table 7: Hyper-parameter used to produce the SVHN and Fashion-MNIST results
                                                                              LR      WD        œÅ
                                                    SVHN       WRN            0.01   0.0005    0.01
                                                               ShakeShake     0.01   0.0005    0.01
                                                    Fashion    WRN            0.1    0.0005    0.05
                                                               ShakeShake     0.1    0.0005    0.02
                           C.2   FINETUNING DETAILS
                           Weights are initialized to the values provided by the publicly available checkpoints, except the last
                           dense layer, which change size to accomodate the new number of classes, that is randomly initial-
                                                                         ‚àí5
                           ized. We train all models with weight decay 1e   as suggested in (Tan & Le, 2019), but we reduce
                           the learning rate to 0.016 as the models tend to diverge for higher values. We use a batch size of
                           1024 on Google Cloud TPUv3 64 cores and cosine learning rate decay. Because other works train
                           with batch size of 256, we train for 5k steps instead of 20k. We freeze the batch norm statistics and
                           use them for normalization, effectively using the batch norm as we would at test time 11. We train
                           the models using SGD with momentum 0.9 and cosine learning rate decay. For EfÔ¨Åcientnet-L2, we
                           use this time a batch size 512 to save memory and adjusted the number of training steps accord-
                           ingly. For CIFAR, we use the same autoaugment policy as in the previous experiments. We do not
                           use data augmentation for the other datasets, applying the same preprocessing as for the Imagenet
                           experiments. We also scale down the learning rate to 0.008 as the batch size is now twice as small.
                           Weused Google Cloud TPUv3 128 cores. All other parameters stay the same. For Imagenet, we
                           trained both models from checkpoint for 10 epochs using a learning rate of 0.1 and œÅ = 0.05. We
                           do not randomly initialize the last layer as we did for the other datasets, but instead use the weights
                           included in the checkpoint.
                           C.3   EXPERIMENTAL RESULTS WITH œÅ = 0.05
                           A big sensitivity to the choice of hyper-parameters would make a method less easy to use. To
                           demonstrate that SAM performs even when œÅ is not Ô¨Ånely tuned, we compiled the table for the
                           CIFARandtheÔ¨Ånetuning experiments using œÅ = 0.05. Please note that we already used œÅ = 0.05
                           for all Imagenet experiments. We report those scores in table 9 and 10.
                           C.4   ABLATION OF THE SECOND ORDER TERMS
                           As described in section 2, computing the gradient of the sharpness aware objective yield some
                           second order terms that are more expensive to compute. To analyze this ablation more in depth,
                           we trained a WideResNet-40x2 on CIFAR-10 using SAM with and without discarding the second
                           order terms during training. We report the cosine similarity of the two updates in Ô¨Ågure 5, along the
                                                                                                                      ÀÜ
                           training trajectory of both experiments. We also report the training error rate (evaluated at w+(w))
                           and the test error rate (evaluated at w).
                           Weobserve that during the Ô¨Årst half of the training, discarding the second order terms does not im-
                           pact the general direction of the training, as the cosine similarity between the Ô¨Årst and second order
                           updates are very close to 1. However, when the model nears convergence, the similarity between
                             11Wefoundanecdotal evidence that this makes the Ô¨Ånetuning more robust to overtraining.
                                                                          17
