                 and LLaMA2underTSTLscenarios. Furthermore,                   YaRN, RESONANCE ROPEfurtherimproves
                 our approach is compatible with RoPE and any                 LLM’s length extrapolation ability, as evi-
                 RoPE-based scaling techniques, enhancing their               denced by lower perplexity in upstream TSTL
                 performance in TSTL situations without the need              language modeling and enhanced outcomes in
                 for additional computational resources during train-         downstream tasks involving lengthy contexts.
                 ing or inference.
                    Additionally, to facilitate further research on      2 RelatedWork
                 position embeddings, we present a new syn-              2.1   Scaling of RoPE Position Encoding
                 thetic benchmark tailored for TSTL scenarios,
                 named POSGEN. Improving position embeddings             Recent efforts in extending LLMs’ context window
                 for TSTL requires a detailed analysis of the cause      focus on manipulating position embedding (PE),
                 of failures in handling longer contexts. However,       particularly RoPE (Su et al., 2024), which is used
                 current benchmarks, such as those measuring per-        in LLMslikeLLaMA(Touvronetal.,2023a,b)and
                 plexity in long context (Rae et al., 2020; Huang        Mistral (Jiang et al., 2023). Main strategies include
                 et al., 2021; Wu et al., 2022) and most synthetic       embedding scaling (Chen et al., 2023; Liu et al.,
                 TSTL tasks (Liu et al., 2023; Kazemnejad et al.,        2024; Peng et al., 2024) and randomizing token
                 2023) face a common issue: the difficulty of gener-     positions (Ruoss et al., 2023; Zhu et al., 2024). Our
                 ating the next token increases with context length.     emphasis is on the embedding scaling strategies.
                 This makes it difficult to determine whether a             Existing embedding scaling strategies adjust po-
                 model’s failure is due to its inability to generate     sition embedding for longer sequences to match
                 more complex tokens or its failure to recognize         the pre-training range, avoiding feature extrapola-
                 out-of-distribution (OOD) positions. POSGEN ad-         tion. For instance, Chen et al. (2023) compresses
                 dresses this limitation by standardizing the diffi-     position indices to fit the pre-training range, ex-
                 culty level of token generation across all positions.   tending LLaMA’s (Touvron et al., 2023a) context
                 This ensures that any observed shortcomings are         to 16K with 1,000 steps of fine-tuning. Alterna-
                 directly related to the model’s inability to identify   tively, Liu et al. (2024); Rozière et al. (2023);
                 and handle new token positions effectively.             Xiong et al. (2023) modify RoPE’s rotary base
                    Ourcontributions in this study are threefold:        and employ fine-tuning on extended sequences,
                                                                         termed Adjusted Base Frequency (ABF) or "NTK-
                   1. We propose RESONANCE ROPE,aninnova-                aware"scaling. CodeLLaMA(Rozièreetal.,2023)
                      tive modification to RoPE based on an in-          achieved 16K context length with this method af-
                      depth analysis of the wavelengths of RoPE          ter 10,000 fine-tuning steps. YaRN (Peng et al.,
                      features, aiming to narrow the generalization      2024) improved NTK-aware scaling by segment-
                      gap in TSTL scenarios across RoPE and sim-         ing RoPE features and applying tailored extrapo-
                      ilar RoPE-based scaling techniques, without        lation strategies, achieving 64K context length for
                      necessitating extra computational resources        LLaMA2 (Touvron et al., 2023b) with 400 fine-
                      during runtime.                                    tuning steps. Distinguishingly, our RESONANCE
                   2. We present POSGEN, a newly developed syn-          ROPEfocusonreducingfeatureinterpolation on
                      thetic benchmark tailored for TSTL scenarios.      OODpositions, which we argue is another impor-
                      This benchmark is specifically designed to         tant factor in improving the length extrapolation
                      disentangle the complexities associated with       capability of Transformer.
                      generating tokens in longer contexts from the      2.2   LongContextEvaluations
                      challenges posed by recognizing new posi-          Evaluations of Transformer-based LLMs’ long-
                      tions or position embedding values.                context capabilities are twofold: synthetic task as-
                   3. Through rigorous testing of RESONANCE              sessments for length extrapolation strategies and
                       ROPE on both RoPE and YaRN within                 real-world task evaluations at the LLM scale. Syn-
                      the POSGEN benchmark, we demonstrate               thetic evaluations target simple tasks such as long
                      its ability to enhance performance on out-         sequence classification (Tay et al., 2021) and arith-
                      of-distribution (OOD) positions, surpassing        metic language modeling (Liu et al., 2023; Kazem-
                      existing methods that do not include RESO-         nejad et al., 2023). LLM scale evaluations mea-
                       NANCE ROPE. Moreover, when applied to             sure metrics such as perplexity (PPL) in extensive
                                                                     587
