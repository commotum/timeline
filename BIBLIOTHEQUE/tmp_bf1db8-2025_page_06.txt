                                                                                                                                      Average output length
                                         104                                                                                                                                                                                                     Benchmark
                                                                                                                                                                                                                                                       BBEH
                                                                                                                                                                                                                                                       BBH
                                         103
                                        ength in characters
                                        L
                                                                 ables                                                                              CC
                                                      essions                                                       Linguini                      NY             operties    riples                  SportQA                                        uzzles
                                          dgameQA                                                                                                               r         C T              easoning                            eb of liesd Sorting
                                                                                                       Hyperbaton                                                     SAR                                                    W        or
                                      Boar               Buggy T                                                                                                                                                ime Arithmetic      W       Zebra P
                                                                                 Dyck Languages                      ecommendation              Object Counting            Shuffled Objects                    T
                                                                                        Geometric Shapes                                                Object P                  Spatial R
                                                                      DisambiguationQA                                                                                                              emporal Sequence
                                         Boolean Expr    Causal Understanding                                             Multistep Arithmetic                                                     T
                                                                                                             Movie R
                                   Figure 3: A comparison of the average output lengths of the Gemini 2.0 Flash responses for each of the tasks in
                                   BBEHandtheircounterparts in BBH, as a proxy for the required amount of thinking.
                                   thinking. The macro average output length of the                                                                   are susceptible to distortion by outlier performance,
                                   responses for tasks in BBEH is about seven times                                                                   potentially presenting a misleadingly optimistic
                                   bigger than that of BBH.                                                                                           assessment when a model excels in a limited sub-
                                        Input/Context Length: As mentioned in Sec-                                                                    set of tasks while faltering in others. To address
                                                                                                                                                                                                                                                              3
                                   tion 2, the problems in the original BBH dataset                                                                   this, we employ the (adjusted) harmonic mean
                                   are mostly short. On the contrary, the problems                                                                    as our primary evaluation metric for BBEH. The
                                   in BBEHtendtobequitelongandrequire a great                                                                         harmonic mean provides a more conservative and
                                   amountofinputprocessingbythemodels. Figure6                                                                        balanced representation of overall performance, ef-
                                   (in Appendix) compares the average input lengths                                                                   fectively penalizing models with significant per-
                                   of each of the tasks in BBEH with their counterpart                                                                formance disparities across different tasks, thereby
                                   from BBH.Fromthefigure, one can observe how                                                                        aligning more closely with the requirement for con-
                                   input lengths have increased for almost all the tasks                                                              sistent, general reasoning capabilities. We also
                                   (except two), sometimes quite significantly. The                                                                   report micro average accuracies for completeness.
                                   macroaveragecontext length of the tasks in BBEH                                                                    For BBEHMini,duetothesmallnumberofexam-
                                   is about six times bigger than that of BBH.                                                                        ples per task, harmonic mean may be too noisy, so
                                                                                                                                                      weonlyreport micro average.
                                   4.2         ModelEvaluations                                                                                            Theresultsforeachtaskandontheentiredataset
                                   Models: We evaluate various models on BBEH                                                                         for each model is presented in Table 2 and in Ta-
                                   and compare their performance across individual                                                                    ble 3 for BBEH Mini. According to the results,
                                   tasks and on the entire dataset. Specifically, we                                                                  wemakeseveral interesting observations. Firstly,
                                   experiment with models from the following fam-                                                                     weobserve a large headroom not only for the in-
                                   ilies: Llama 3.1 (Dubey et al., 2024), Qwen 2.5                                                                    dividual tasks, but also for BBEH overall. The
                                   (Yang et al., 2024a), Gemma2 (Team et al., 2024b),                                                                 best performance for the general-purpose models
                                   Gemma3(Teametal.,2025),Gemini2.0,GPT4o                                                                             is at 23.9% micro average accuracy. The reasoning-
                                   (the latest version, 2024-11-20, at the time of the                                                                specialized models are expectedly performing bet-
                                   experiments) (Achiam et al., 2023), DeepSeek R1                                                                    ter than the general-purpose models on the bench-
                                   and the Distilled model in Qwen 32b (Guo et al.,                                                                   mark, but the best performance for these models
                                   2025), and o3-mini (high)2.                                                                                        is still at 54.2% on BBEH. Note that while we
                                        Metric: Given the highly versatile use-cases of                                                               calibrated the difficulty with respect to two refer-
                                   the current LLM reasoners, they should be capable                                                                  encemodelssotheiraccuraciesfallbelow70%,the
                                   across the board to excel at real-world problems                                                                   difficulty mostly carries to other models too with
                                   and be robust general reasoners. However, we find                                                                  o3-mini (high) exceeding 70% accuracy only on 4
                                   that micro and macro averages (which are often                                                                     out of 23 tasks, DeepSeek R1 exceeding it only on
                                   used for benchmarks composed of multiple tasks),                                                                   3outof23tasks,andothermodelsneverexceeding
                                   fail to capture this crucial aspect. These metrics                                                                 it. Despite the adversarial construction, the refer-
                                                                                                                                                            3To deal with zero values, we add a value of 1 to all accu-
                                         2https://openai.com/index/openai-o3-mini/                                                                    racy numbers.
                                                                                                                                           26478
