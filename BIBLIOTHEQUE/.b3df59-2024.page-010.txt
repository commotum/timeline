                                       TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
              Hardt, M. and Sun, Y. Test-time training on nearest neigh-    Loshchilov, I. and Hutter, F. Fixing weight decay regular-
                 bors for large language models. In The Twelfth Interna-       ization in Adam, 2018. URL https://openreview.n
                 tional Conference on Learning Representations, 2024.          et/forum?id=rk6qdGgCZ.
                 URLhttps://openreview.net/forum?id=CNL2bku4
                 ra.                                                        McCoy, R. T., Yao, S., Friedman, D., Hardy, M. D., and
                                                                               Griffiths, T. L. Embers of autoregression show how large
              Hodel, M. Addressing the Abstraction and Reasoning               language models are shaped by the problem they are
                 Corpus via procedural example generation, 2024. URL           trained to solve. Proceedings of the National Academy
                 https://arxiv.org/abs/2404.07353.                             of Sciences, 2024. doi: 10.1073/pnas.2322420121. URL
                                                                               https://www.pnas.org/doi/abs/10.1073/pnas.23
              Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,    22420121.
                 S., Wang, L., and Chen, W. LoRA: Low-rank adaptation
                 of large language models. In The Tenth International       Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H.
                 Conference on Learning Representations, 2022. URL             MetaICL: Learning to learn in context. In Proceedings of
                 https://openreview.net/forum?id=nZeVKeeFYf9.                  the 2022 Conference of the North American Chapter of
                                                                               the Association for Computational Linguistics: Human
                ¨
              Hubotter, J., Bongni, S., Hakimi, I., and Krause, A. Effi-       LanguageTechnologies. Association for Computational
                 ciently learning at test-time: Active fine-tuning of LLMs.    Linguistics, 2022a. doi: 10.18653/v1/2022.naacl-mai
                 In The Thirteenth International Conference on Learning        n.201. URL https://aclanthology.org/2022.naac
                 Representations, 2025. URL https://openreview.net             l-main.201.
                 /forum?id=NS1G1Uhny3.
                                                                            Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,
              Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh,      Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of
                 A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A.,          demonstrations: What makes in-context learning work?
                 Radford, A., et al. GPT-4o system card. ArXiv preprint,       In Proceedings of the 2022 Conference on Empirical
                 2024. URLhttps://arxiv.org/abs/2410.21276.                    Methods in Natural Language Processing. Association
              Joachims, T. Transductive inference for text classification      for Computational Linguistics, 2022b. doi: 10.18653/v
                 usingsupportvectormachines. InProceedingsofthe16th            1/2022.emnlp-main.759. URL https://aclanthology
                 International Conference on Machine Learning. Morgan          .org/2022.emnlp-main.759.
                 KaufmannPublishers Inc., 1999. ISBN 1558606122.            OpenAI. GPT-4technical report, 2024. URL https://ar
              Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,          xiv.org/abs/2303.08774.
                 C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient   Opiełka, G., Rosenbusch, H., Vijverberg, V., and Steven-
                 memory management for large language model serv-              son, C. E. Do large language models solve ARC vi-
                 ing with PagedAttention. In Proceedings of the 29th           sual analogies like people do?, 2024.    URL https:
                 SymposiumonOperatingSystemsPrinciples, SOSP ’23.              //arxiv.org/abs/2403.09734.
                 Association for Computing Machinery, 2023.       ISBN
                 9798400702297. doi: 10.1145/3600006.3613165. URL           Ravi, S. and Larochelle, H. Optimization as a model for
                 https://doi.org/10.1145/3600006.3613165.                      few-shot learning. In The Fifth International Conference
                                                                               on Learning Representations, 2017. URL https://op
              LeGris, S., Vong, W. K., Lake, B. M., and Gureckis, T. M.        enreview.net/forum?id=rJY0-Kcll.
                 H-ARC:Arobustestimateofhumanperformanceonthe
                 Abstraction and Reasoning Corpus benchmark. ArXiv          Snell, C. V., Lee, J., Xu, K., and Kumar, A. Scaling test-
                 preprint, 2024. URL https://arxiv.org/abs/2409.0              time compute optimally can be more effective than scal-
                 1374.                                                         ing LLM parameters. In The Thirteenth International
                                                                               Conference on Learning Representations, 2025. URL
              Li, W.-D., Hu, K., Larsen, C., Wu, Y., Alford, S., Woo, C.,      https://openreview.net/forum?id=4FWAwZtd2n.
                 Dunn, S. M., Tang, H., Zheng, W.-L., Pu, Y., and Ellis,
                 K. Combining induction and transduction for abstract       Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,
                 reasoning. In The Thirteenth International Conference         A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,
                 on Learning Representations, 2025. URL https://op             Garriga-Alonso, A., et al. Beyond the imitation game:
                 enreview.net/forum?id=UmdotAAVDe.                             Quantifyingandextrapolatingthecapabilitiesoflanguage
                                                                               models. Transactions on Machine Learning Research,
              Llama Team. The Llama 3 herd of models, 2024. URL                2023. ISSN 2835-8856. URL https://openreview.n
                 https://arxiv.org/abs/2407.21783.                             et/forum?id=uyTL5Bvosj.
                                                                         10
