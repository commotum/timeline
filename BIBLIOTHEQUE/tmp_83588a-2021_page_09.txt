                                                                                Stabilizing Equilibrium Models by Jacobian Regularization
                                              WikiText-103 DEQ (Post-LN) Forward (T=16 steps)
                                  0                                                                                                  does quickly converge, but the constraint imposed on the
                          |
                                10
                          |
                             |
                          z
                             |
                             )
                          −
                             x                                                                                                       model class is too strong and eventually hurts performance
                          )
                             ;
                          x
                             z
                             (
                          ;
                              θ
                          z
                             f
                                 −1
                          (
                             |
                               10
                           θ                                                                                                        (e.g., since the training loss on CIFAR-10 usually overﬁts
                             |
                          f
                          |
                          |
                             
                            l
                                                                                   Tran. DEQ
                            a                                                                                                        to almost 0 towards the end of training, which makes the
                            u
                                 −2
                                                                                   Tran. DEQ + reg. (ours)
                               10
                            d
                            i                                                                                                        Jacobian loss dominant instead).
                            s
                                                                                   Trans. DEQ + weight decay 1e-6
                            e
                            r
                             
                            d
                                 −3
                            r
                               10                                                                                                   Wealso highlight two limitations of this approach. First,
                            a
                            w
                            r                                                                                                        the addition of Jacobian regularization term does not fun-
                            o
                            F
                                 −4
                               10                                                                                                    damentally solve the growing instability problem, but only
                                    0           10           20          30           40          50          60
                                                        Training Iterations (thousand steps)                                         empirically alleviates it. This means that we have to be
                                                                                                    −6
                        Figure 8. Adding weight decay of magnitude 10                                     to the DEQ-                careful about balancing the main loss objective and this aux-
                        Transformer doesn’t help stabilize the forward convergence.                                                  iliary objective (see Table 3). Second, while Jacobian reg-
                                                                                                                                     ularization facilitates faster convergence, there are certain
                        Table 3. Controlled experiments on the strength γ of the Jacobian                                           “physical laws” that we simply cannot bypass. For example,
                        regularization. The NFE value represents the “hard stop” threshold                                           if we apply a shallow convolutional DEQ whose layer has
                        wesetfor the corresponding DEQ models at inference.                                                          receptive ﬁeld 5 × 5 on a large image (e.g., 1024 × 1024),
                                          NFE=1         NFE=2         NFE=3         NFE=4          NFE=5         NFE=6               it is hard to be able to reach the ﬁxed point with just 6 iter-
                           γ = 0:1         82.4%         89.7%         91.9%         92.3%         92.7%         92.9%               ations simply because the model’s receptive ﬁeld may not
                           γ = 0:6         85.8%         91.5%         92.7%         93.0%         93.0%         93.1%               broaden sufﬁciently to cover valuable context. Although
                           γ = 1:2         84.4%         89.6%         92.2%         92.6%         92.7%         92.7%               one can possibly still force convergence with a large γ, it
                                                                                                                                    wouldundoubtedlyhurttheperformance. Thisexplainswhy
                        are large (e.g., [(B·110K) × (B·110K)] in WikiText-103,                                                     weneedmoreNFEsonImageNetthanonCIFAR-10(see
                        and [(B·198K) × (B·198K)] in ImageNet with MDEQ-                                                            Table 2); it also indicates that while our approach alleviates
                        small) and checking their full spectrum would be infeasible.                                                 the brittleness to architectural choices, its effectiveness can
                        Therefore, we conduct a study that monitors the average                                                      still depend on the architecture. This makes global-context
                        spectral radius ρ(J                  (z?)) (i.e., the largest absolute eigen-                                alternatives to ConvNets, such as self-attention-based vision
                                                         fθ                                                                          layers (e.g.,ViT (Dosovitskiy et al., 2020)) likely more ap-
                        value) on the validation set, over the ﬁrst 100K steps of                                                    pealing in the implicit model setting, which we leave for
                        DEQtrainingonWikiText-103usingthepowermethod(von                                                             future work.
                        Mises&Pollaczek-Geiringer,1929);seeFig.7. Importantly,
                        although kJ k onlyupper-boundsthespectral radius (see
                                             f    F
                                              θ
                        Sec. 4.2), we verify that our proposed regularization does ef-                                               6. Conclusion
                        fectively constrain ρ(Jf ) (see                         /    paths in Fig. 7), thereby
                                                                 θ                                                                  Wesummarized the weaknesses of existing DEQ models,
                        making DEQs more stable. In contrast, the unregularized
                        DEQwiththesamefewNFEsexplodesinbotheigenvalue                                                                including instability & inefﬁciency, architectural brittleness,
                        and shortly after also in perplexity (see                            /    paths), and only                   and hidden memory costs. We speciﬁcally discussed the
                        works if we increase NFE to 30 (see ×/× paths). In general,                                                  relationship between the spectral radius of the Jacobian
                        weempirically observe that training an unregularized DEQ                                                     and the stability of forward non-linear and backward linear
                        with insufﬁcient NFEs generally begets extremely noisy                                                       systems of DEQ models, and provided empirical evidence
                        gradients, thus leading to faster destabilization and even                                                   of the poor conditioning of the Jacobian. This motivates our
                        divergence.                                                                                                  introduction of Jacobian regularization. Our experiments
                                                                                                                                     showthatourmethodsigniﬁcantlyalleviatestheweaknesses
                        5.5. Ablative Analysis and Limitations of the Approach                                                       of DEQs, yielding a > 2:5× acceleration. This is a major
                                                                                                                                     step towards making implicit models more practical and
                        Wecontinue our discussion with some empirical ablative                                                       suitable for large-scale real-world applications. We hope
                        studies. First, while Grathwohl et al. (2019) found weight                                                   that our work will motivate further research that advances
                        decayuseful for regularizing ODE-based models’ NFEs, we                                                      our understanding and application of this class of models.
                        found weight decay generally not effective in stabilizing
                        DEQsandsometimesevencounter-productive. This is illus-                                                       References
                        trated in Figure 8, where after 50K steps the model started
                        to diverge to > 500 perplexity and stopped improving. In                                                     Amos, B. and Kolter, J. Z. OptNet: Differentiable opti-
                        addition, we also conduct an ablative experiment on how the                                                      mization as a layer in neural networks. In International
                        Jacobian regularization strength γ affects the performance                                                       Conference on Machine Learning (ICML), 2017.
                        when we constrain NFEs to ≤ 6 at inference time, with
                        results shown in Table 3 (CIFAR-10 dataset). In general, we                                                  Anderson, D. G. Iterative procedures for nonlinear integral
                        ﬁndthatifγ istoosmall, the ﬁnal performance maybegood                                                            equations. Journal of the ACM (JACM), 12(4):547–560,
                        but entails more NFEs. When γ is too large, the accuracy                                                         1965.
