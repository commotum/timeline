                               Method                 Information        mAP NDS Car Truck C.V. Bus Trailer Barrier Motor. Bicycle                                                   Ped.     T.C.
                          PointPillars [12]                  L           30.5       45.3     68.4      23.0       4.1     28.2       23.4        38.9        27.4          1.1       59.7     30.8
                            3DVID[42]                      L+T           45.4         -      79.7      33.6      18.1     47.1       43.0        48.8        40.7          7.9       76.5     58.8
                         PointPainting [31]                L+I           46.4       58.1     77.9      35.8      15.8     36.2       37.3        60.2        41.5         24.1       73.3     62.4
                              TCT[46]                      L+T           50.5         -      83.2      51.5      15.6     63.7       33.0        53.8        54.0         53.8       74.9     52.5
                                           ∗
                     PointAugmenting [32]                  L+I           61.5       67.2     86.0      50.9      26.4     58.9       55.8        68.9        64.4         40.7       83.9     79.0
                            LIFT(Ours)                    L+I+T          65.1       70.2     87.7      55.1      29.4     62.4       59.3        69.3        70.8         47.7       86.1     83.2
                   Table 1. Performance comparisons on the nuScenes test set. We report the overall mAP, NDS and mAP for each detection category, where
                   Ldenotes Lidar modality, I denotes Image modality and T denotes Temporal input. ∗: reproduced results based on PointPillars.
                             Method              Vehicle      Pedestrian      Cyclist        Overall             of non-empty pillars is limited to 32000. Following Cen-
                                                L 1    L 2    L 1    L 2    L 1     L 2    L 1    L 2            terPoint [43], we use the adamW [17] optimizer with the
                           PointPillars        66.0   61.3    67.4   62.3   62.8   62.4    65.4   62.0
                        PointPainting [31]     66.6   61.9    63.5   61.2   63.5   61.2    64.5   61.4           one-cycle policy [6]. During training, additional to our pro-
                     PointAugmenting∗ [32]     68.1   63.3    66.9   62.1   65.4   63.0    66.8   62.8           posed sensor-time data augmentation, we use random flip-
                           LIFT(Ours)          69.0   64.2    69.9   65.3   69.2   66.5    69.4   65.3
                                                                                                                 ping, global scaling, global rotation and global translation.
                   Table 2. Performance comparisons on the Waymo validation set.                                 Models are trained for 20 epochs on 8 V100 GPUs.
                   We report LEVEL 1 and LEVEL 2 mAP(%) for all categories                                       4.2. Main Results
                   (L 1 and L 2). All models are built on the PointPillars backbone.                             nuScenes Results.                  We compare our algorithm with
                                                                                                                 the state-of-the-art approaches as illustrated in Table 1.
                   studies to validate our design choices.                                                       For fair comparison, all the presented methods are pillar-
                   4.1. Experimental Setup                                                                       based detectors. In particular, PointPillars [12] is a single-
                   Datasets. We apply two widely used auto-driving datasets                                      frame point cloud detector that is used as the baseline of
                   including nuScenes [2] and Waymo [28]. The nuScenes                                           our model. 3DVID [42] uses a ConvGRU module to ex-
                   dataset is collected by six cameras and a 32-beam LiDAR,                                      ploit the temporal information from sequential point clouds.
                   consisting of 700, 150 and 150 scenes for training, valida-                                   TCT [46] applies a channel-wise transformer network to
                   tion and test respectively. Each scene is 20 seconds long                                     integrate the information of multiple point cloud frames.
                   with 20 Hz frequency. 3D bounding boxes are annotated                                         PointPainting [31] and PointAugmenting [32] are typical
                   at 2 Hz with 10 categories in 360 degree field of view. We                                    methods that fuse camera features with LiDAR points. Our
                   follow the official evaluation protocol [2] and use mAP and                                   method outperforms these approaches by large margins,
                   NDS as the evaluation metrics on nuScenes. The Waymo                                          boosting the original PointPillars by 34.6% and the cur-
                   dataset uses five cameras and five 64-beam LiDAR and con-                                     rent best PointAugmenting method by 3.6%. Table 1 shows
                   tains 798 training scenes and 202 validation scenes. Data                                     that, although 3D object detectors generally benefit from
                   collection and 3D annotation are both at 10 Hz frequency.                                     cross-sensor or cross-time information fusion, our proposed
                   We follow the official evaluation metrics mAP and report                                      method makes the best of all available data across sen-
                   two difficulty levels: LEVEL 1 and LEVEL 2.                                                   sors and time by modeling the mutual correlation, and thus
                                                                                                                 achieves state-of-the-art performance.
                   Network Architecture and Training Details. For the se-                                        Waymo Results.                  We also make comparisons on the
                   quential cross-sensor input, we use T = 2 different key                                       Waymodataset in Table 2. We reproduce all models based
                   frames and m = 2 different modalities.                             For network                on PointPillars as well.               Note that the camera configu-
                   design, we use Hw = Ww = 4 as the window size                                                 rations in Waymo are different from nuScenes, covering
                   and each window takes as input N                          = 64 tokens with                    only around 250 degree field. In contrast to applying two
                                                                        F                                        models on camera FOV and LiDAR FOV separately as in
                   feature dimension f = 64.                     We apply N               = 3 dif-
                                                                                     M                           PointAugmenting [32], we apply a unified model on full
                   ferent scales and set the number of attention heads to 2
                   in all experiments.            We limit the max number of points                              view as adaption to real application. Results show that pre-
                   within each pillar to 20. For nuScenes data, we set the                                       vious cross-modal detectors fail to achieve consistent im-
                   detection range to [−51.2m,51.2m] for X and Y axis,                                           provements on pedestrian and cyclist categories. However,
                   and [−5m,3m] for the Z axis, which is voxelized with                                          our method generalizes and scales well, which consistently
                   (0.2m,0.2m,8m) grid size. We utilize 10 sweeps for Li-                                        outperforms previous methods, especially boosts the origi-
                   DARenhancementandlimitthemaxnumberofnon-empty                                                 nal LiDAR-only detector on the challenging pedestrian and
                   pillars to 30000. For Waymo data, the detection range is set                                  cyclist categories by large margins.
                   to [−71.68m,71.68m] for X and Y axis, [−2m,4m] for Z                                          QualitativeResults. WequaliatitvelycomparewithPoint-
                   axis, with (0.32m,0.32m,6m) grid size. The max number                                         Pillars and PointAugmenting on the nuScenes dataset in
                                                                                                           17177
