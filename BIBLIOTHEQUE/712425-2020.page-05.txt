                                                a) Attention-Map (Head-1)        b) Attention-Map (Head-2)        c) Attention-Map (Head-3)        d) Attention-Map (Head-4)
                                            T                                T                               T                                T
                                            (                                (                                (                                (                                    1.0
                                            [                                [                                [                                [
                                            (                                (                                (                                (
                                            [                                [                                [                                [
                                            ]                                ]                                ]                                ]                                    0.8
                                            )                                )                                )                                )
                                            ]                                ]                                ]                                ]
                                            )                                )                                )                                )                                    0.6
                                                T ( [ ( [ ] ) ] )               T ( [ ( [ ] ) ] )                T ( [ ( [ ] ) ] )                T ( [ ( [ ] ) ] )
                                             (                               (                                (                                (                                    0.4
                                             [                               [                                [                                [
                                             (                               (                                (                                (
                                             [                               [                                [                                [                                    0.2
                                             ]                               ]                                ]                                ]
                                             )                               )                                )                                )
                                             ]                               ]                                ]                                ]                                    0.0
                                             )                               )                                )                                )
                                                (   [  (   [  ]   )  ]   )       (  [   (  [   ]   )  ]   )      (   [   (  [   ]  )   ]  )       (   [  (   [  ]   )   ]  )
                                                               +
                           Figure 4: Comparing SA (top) and SA (bottom), based on their attention maps on a D sequence. The third head
                                                                                                                                                          2
                           of SA+ has produced weights that are compatible with the operations of a stack-based recognizer.
                                                                             4 (SA+)                            of the whole sequence.
                                           1
                                         0.8                                                                     5      Conclusion and Future Work
                                        y         4 (SA−)
                                        t
                                        i
                                        l
                                        i
                                        b0.6
                                        i                                                                        We provide empirical evidence on the ability of
                                        t                                            +
                                        a                                    2 (SA )
                                        p                                                                        self-attention (SA) networks to learn generalized
                                        m0.4
                                        o
                                        c                                                                        D languages. Wecomparetheperformanceoftwo
                                                                                                                    n
                                                  2 (SA−)                                                                                   +               −
                                         0.2                                                                     SAnetworks, SA andSA ,whichdifferonlyin
                                                                                                                 the inclusion of a starting symbol in their vocabu-
                                           0                                                                     lary. We demonstrate that a simple addition of the
                                                 20         40        60         80        100                                                           +
                                                                  length                                         starting symbol helps SA                   generalize to sequences
                                                                                                  +              that are longer and have higher depths. The com-
                           Figure 5: Compatibility versus length for SA                               and        petitive performance of SA (no-recurrence) against
                                −
                           SA onD andD languages.
                                          2           4                                                          LSTMsmightseemsurprising,consideringthatthe
                                                                                                                 recognition of D languages is an inherently hier-
                                                                                                                                            n
                           versus sequence length. We ﬁnd that SA− on D                                          archical task. From our experiments, we conclude
                                                                                                         2
                           has almost zero compatibility, even for sequence                                      that recognizing Dycklanguagesisnottiedtorecur-
                           lengths seen during training (40-50), on which it                                     sion, but rather learning the right representations
                           achieves close-to-perfect accuracy. In comparison,                                    to look up the head token. Further, we ﬁnd that
                           SA+hasperfectcompatibilityforsequencelengths                                                                                                 +
                                                                                                                 the representations learned by SA                          are highly in-
                           seenduringtraining,andmaintainsahighdegreeof                                          terpretable and the network performs computations
                           compatibility for longer ones. Further, perhaps not                                   similar to a stack automaton. Our results suggest
                           surprisingly, the Pearson correlation between the                                     formal languages could be an interesting avenue
                           distribution of accuracy and compatibility across                                     to explore the interplay between performance and
                                                                                    +
                           lengths 50-100 is & 90% for all SA                          models.                   interpretability for SA. Comparisons between SA
                               Figure 4 shows the attention maps of all four                                     and LSTMrevealinteresting contrast between the
                                                 +                 −
                           heads of SA               and SA            for the D            sequence             two architectures which calls for further investi-
                                                                                        2
                                                                                                         +
                          “([([])])”. We observe that the third head of SA                                       gation. Recent work (Katharopoulos et al., 2020)
                           matches our expectation of a stack-based recog-                                       shows how to express the Transformer as an RNN
                           nizer. An important feature of the third head is that                                 through linearization of the attention mechanism,
                           the last symbol attends to the starting symbol T.                                     which could lay grounds for more theoretical anal-
                           Thestarting symbol has enabled the model to learn                                     ysis of these neural architectures (e.g., inductive
                           the occurrence of the end of a clause and the end                                     biases and complexity.)
                                                                                                          4305
