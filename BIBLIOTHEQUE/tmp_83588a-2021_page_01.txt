                              Stabilizing Equilibrium Models by Jacobian Regularization
                                                   Shaojie Bai1 Vladlen Koltun2 J. Zico Kolter1
                                       Abstract                                equilibrium z? by the implicit function theorem (Krantz &
                    Deep equilibrium networks (DEQs) are a new                 Parks, 2012), irrespective of the method used to solve for
                    class of models that eschews traditional depth in          this equilibrium in the forward pass. Therefore, like other
                    favor of ﬁnding the ﬁxed point of a single non-            implicit-depth architectures such as Neural ODEs (Chen
                    linear layer. These models have been shown to              et al., 2018), DEQs have the notable advantages that their
                    achieve performance competitive with the state-            forward passes can rely on any black-box root solvers (e.g.,
                    of-the-art deep networks while using signiﬁcantly          Newton, quasi-Newton, simplest forward iterations), and
                    less memory. Yet they are also slower, brittle to          that their training only consumes O(1) memory. With this
                    architectural choices, and introduce potential in-         formulation, prior works have managed to extend the DEQ
                    stability to the model. In this paper, we propose          framework for multiple large-scale applications, such as
                    a regularization scheme for DEQ models that ex-            language modeling (Bai et al., 2019) and large-scale image
                    plicitly regularizes the Jacobian of the ﬁxed-point        classiﬁcation or segmentation (Bai et al., 2020).
                    update equations to stabilize the learning of equi-        However, these models suffer from a few issues. First,
                    librium models. We show that this regularization           despite their memory efﬁciency, DEQs are also slower than
                    adds only minimal computational cost, signiﬁ-              conventional deep networks that achieve the same level
                    cantly stabilizes the ﬁxed-point convergence in            of accuracy. Second, the number of iterations required to
                    both forward and backward passes, and scales               solve for the equilibrium quickly grows over the course
                    well to high-dimensional, realistic domains (e.g.,         of training, indicating a trend for approaching instability.
                    WikiText-103 language modeling and ImageNet                Third, the DEQ model is sensitive to architectural choices,
                    classiﬁcation). Using this method, we demon-               and sometimes even small modiﬁcations could break the
                    strate, for the ﬁrst time, an implicit-depth model         model’s stability of convergence. Some recent works have
                    that runs with approximately the same speed and            tackled this third issue by exploiting provably convergent
                    level of performanceaspopularconventionaldeep              layers via monotone operator splitting theories (Winston
                    networks such as ResNet-101, while still main-             &Kolter, 2020) and Lipschitz boundedness (Revay et al.,
                    taining the constant memory footprint and archi-           2020). However, these structural solutions rely extensively
                    tectural simplicity of DEQs. Code is available             on speciﬁc layer parameterizations, rendering DEQ models
                    here.                                                      unscalable and even more inﬂexible.
                                                                               In this paper, we ﬁrst summarize and provide empirical evi-
               1. Introduction                                                 dence on all of these downsides of the equilibrium networks
                                                                               that have so far thwarted many from extending DEQs to
               While conventional deep networks like ResNets (He et al.,       both broader applications and more architectural variants.
         arXiv:2106.14342v1  [cs.LG]  28 Jun 20212016) and Transformers (Vaswani et al., 2017) rely on hi-Toaddress these issues, we further propose a regularization
               erarchical layer stacking, the recently-proposed deep equi-     solution to improve on DEQ models’ stability, efﬁciency
               librium networks (DEQs) (Bai et al., 2019) directly model       and ﬂexibility. Importantly, while prior DEQs adopted regu-
               the “inﬁnite-depth” representation of a single layer fθ by      larization methods direcly borrowed from explicit deep net-
               solving for its ﬁxed point (i.e., “equilibrium”) z?:            works (e.g., recurrent dropout (Gal & Ghahramani, 2016)),
                                     z? = f (z?;x),                            weintroduce a simple and theoretically-motivated Jacobian
                                            θ                                  regularization pursuant to DEQ models’ implicitness. We
               where x is the original input. Importantly, to train these      will discuss in detail how this Jacobian regularization relates
               models, one could directly differentiate through the ﬁnal       to the contractivity of DEQ’s forward non-linear system and
                                                                               backward linear system, and is thus able to effectively stabi-
                  1Carnegie Mellon University, Pittsburgh PA, USA 2Intel Labs, lize not only forward but also backward dynamics of DEQ
               USA.Correspondenceto: Shaojie Bai <shaojieb@cs.cmu.edu>.        networks. There are two immediate beneﬁts of the resulting
               Proceedings of the 38th International Conference on Machine     stability in the dynamics. First, solving a DEQ requires
               Learning, PMLR 139, 2021. Copyright 2021 by the author(s).      far fewer iterations than before, which makes regularized
