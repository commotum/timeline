                                   SparseVoxFormer:SparseVoxel-basedTransformerfor
                                                    Multi-modal 3D Object Detection
                                        1            2                     1                2                      2                    1
                    Hyeongseok Son           Jia He       Seung-In Park         Ying Min         YunhaoZhang            ByungIn Yoo
                                                  1SamsungElectronics, AI Center, South Korea
                                              2SamsungR&DInstitute China Xi’an (SRCX), China
                         {hs1.son, jia01.he, si14.park ying.min, yunhao.zhang, byungin.yoo}@samsung.com
                                        Abstract                                 to their ability to provide accurate localization. However,
                                                                                 LiDAR sensors have clear limitations; the density of the
               Most previous 3D object detection methods that leverage           point cloud significantly decreases as the distance from the
               the multi-modality of LiDAR and cameras utilize the Bird’s        sensor increases, leading to a considerable drop in accuracy
               Eye View (BEV) space for intermediate feature represen-           for objects at long range [13]. Given that employing high-
               tation. However, this space uses a low x, y-resolution and        specification LiDAR sensors is cost-inefficient, a plausible
               sacrifices z-axis information to reduce the overall feature       solution would be to incorporate a camera modality.
               resolution, which may result in declined accuracy. To tackle         Recent multi-modal approaches [1, 8, 22, 24, 30, 42, 43]
               the problem of using low-resolution features, this paper fo-      combining LiDAR and camera data have achieved new
               cuses on the sparse nature of LiDAR point cloud data. From        state-of-the-art performances in 3D object detection for au-
               ourobservation,thenumberofoccupiedcellsinthe3Dvox-                tonomous driving. These approaches typically use a BEV
               els constructed from a LiDAR data can be even fewer than          space to fuse multi-modal features from LiDAR and cam-
               the number of total cells in the BEV map, despite the vox-        era data, primarily due to the significant computational de-
               els’ significantly higher resolution. Based on this, we intro-    mandsofdirectly utilizing high-resolution 3D features. De-
               duce a novel sparse voxel-based transformer network for           spite their practical achievements, these methodspotentially
               3D object detection, dubbed as SparseVoxFormer. Instead           lose 3D geometric information due to lower resolution and
               of performing BEV feature extraction, we directly lever-          suppressedz-axisinformation.Webelievethatthereisroom
               age sparse voxel features as the input for a transformer-         for improvement by exploiting the rich geometric informa-
               baseddetector. Moreover, with regard to the camera modal-         tion present in 3D features.
               ity, we introduce an explicit modality fusion approach that          We observe that a point cloud of LiDAR data is in-
               involves projecting 3D voxel coordinates onto 2D images           herently sparse, as are the voxels constructed from this
               and collecting the corresponding image features. Thanks           data. Consequently, while the raw voxels occupy a high-
               to these components, our approach can leverage geomet-            resolution 3D space, the number of valid cells is not ex-
               rically richer multi-modal features while even reducing the       tensive. For instance, voxel features with a resolution of
               computational cost. Beyond the proof-of-concept level, we         360×360×11haveacomparablenumberofcellstothatof
         arXiv:2503.08092v1  [cs.CV]  11 Mar 2025further focus on facilitating better multi-modal fusion andBEVfeatures with a lower resolution of 180 × 180, result-
               flexible control over the number of sparse features. Finally,     ing in 32,400 cells. This is surprising given that the voxel
               thorough experimental results demonstrate that utilizing a        features originally have a total cell count 41 times greater
               significantly smaller number of sparse features drastically       than that of the BEV features. This suggests that by utiliz-
               reduces computational costs in a 3D object detector while         ing the sparsity of data, we can fully leverage the benefits of
               enhancing both overall and long-range performance.                3Dvoxelfeaturesandrichergeometricinformation,achiev-
                                                                                 ing better performance while using fewer computational re-
                                                                                 sources than when using BEV features.
               1. Introduction                                                      Basedonthisimportantobservation, we propose a novel
                                                                                 multi-modal 3D object detection framework directly us-
               3Dobject detection is a critical task in real-world applica-      ing sparse voxel features, dubbed as SparseVoxFormer. To
               tions such as autonomous driving. A prominent approach to         this end, we employ a transformer decoder architecture like
               3D object detection in autonomous driving involves using          DETR[4]forourdetectorbecausethisstructure can accept
               LiDAR sensors [14, 19, 27, 28, 36, 40], primarily thanks          the sparse 3D voxel features intactly thanks to the nature of
