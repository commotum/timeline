# GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (2019)
Source: 53de23-2018.pdf

## Core reasons
- Introduces GPipe as a pipeline-parallel training library for scaling model training across accelerators.
- Centers on micro-batch pipeline parallelism as the training efficiency mechanism, not new architectures or datasets.

## Evidence extracts
- "pipeline parallelism library that allows scaling any network that can be expressed" (p. 1)
- "GPipeintroduces a new brand of pipeline parallelism that pipelines the execution of micro-batches" (p. 8)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
