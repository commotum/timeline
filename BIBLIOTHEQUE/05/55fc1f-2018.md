# Improving Language Understanding by Generative Pre-Training (2018)
Source: 55fc1f-2018.pdf

## Core reasons
- Introduces a two-stage framework of generative language model pre-training followed by discriminative fine-tuning for natural language understanding tasks.
- Presents a task-agnostic Transformer-based training approach aimed at transfer across multiple NLU benchmarks, rather than a positional encoding or dimensionality adaptation.

## Evidence extracts
- "We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model onadiversecorpusofunlabeledtext,followedbydiscriminativeﬁne-tuningoneach speciﬁc task." (p. 1)
- "We introduced a framework for achieving strong natural language understanding with a single task-agnostic model through generative pre-training and discriminative ﬁne-tuning." (p. 8)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
