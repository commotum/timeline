# Approximately Optimal Approximate Reinforcement Learning (Not specified in the paper.)
Source: 1f259f-2002.pdf

## Core reasons
- The paper proposes and analyzes a reinforcement learning algorithm (conservative policy iteration) rather than a new positional encoding, transformer dimensioning, or dataset.
- The contribution centers on approximate RL methodology and theoretical guarantees for policy improvement, fitting ML foundations and principles.

## Evidence extracts
- "In this paper, we present the conser-
vative policy iteration algorithm which
finds an “approximately” optimal pol-
icy, given access to a restart distri-
bution (which draws the next state
from a particular distribution) and
an approximate greedy policy chooser." (p. 1)
- "For simplicity, we assume knowledge of ¢. The conser-
vative policy iteration algorithm is:

(1) Call G.(z, u) to obtain some 7’" (p. 6)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
