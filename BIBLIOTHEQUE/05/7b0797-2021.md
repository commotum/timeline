# Training data-efÔ¨Åcient image transformers & distillation through attention (2021)
Source: 7b0797-2021.pdf

## Core reasons
- The contribution centers on training methodology for vision transformers, explicitly highlighting training strategies and a distillation token rather than a new architecture or positional encoding.
- The paper's core mechanism is a transformer-specific distillation token used during training, indicating a training/optimization focus consistent with ML foundations.

## Evidence extracts
- "Our only differences are the training strategies, and the distillation token." (p. 4)
- "We add a new token, the distillation token, to the initial embeddings (patches and class token)." (p. 4)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
