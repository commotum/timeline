# Scaling Transformer-Based Novel View Synthesis with Models Token Disentanglement and Synthetic Data (Not specified in the paper.)
Source: cc44ca-2025.pdf

## Core reasons
- Proposes a Token-Disentangled (Tok-D) transformer block specialized for NVS, making the contribution primarily an architectural change to a transformer model.
- Introduces a synthetic-data training scheme to address multi-view data scarcity and artifact robustness, centering the contribution on training methodology rather than benchmarks or positional encoding.

## Evidence extracts
- "First, our Token-Disentangled (Tok-D) transformer block is specialized for NVS and distinguishes information from the source and target views, leading to more efficient allo- cation of representation capacity." (Section 4. Method)
- "Second, to address the scarcity of multi-view data, we generate synthetic data us- ing CAT3D [10] and propose a model training scheme that is robust to artifacts in this synthetic data." (Section 4. Method)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
