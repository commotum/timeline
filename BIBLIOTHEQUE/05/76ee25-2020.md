# Scaling Laws for Neural Language Models (2020)
Source: 76ee25-2020.pdf

## Core reasons
- The paper's main contribution is empirical scaling laws connecting language model loss to model size, dataset size, and compute, which is a foundational performance analysis rather than a new architecture or encoding.
- It centers on how performance scales and how to allocate compute for training efficiency, aligning with training/optimization principles.

## Evidence extracts
- "The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude." (p. 1)
- "In this work we will empirically investigate the dependence of language modeling loss on all of these factors, focusing on the Transformer architecture [VSP 17, LSP 18]." (p. 2)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
