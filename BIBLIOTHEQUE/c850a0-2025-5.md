# R-PRM:Reasoning-DrivenProcessRewardModeling (2025)
Source: c850a0-2025.pdf

## Core reasons
- Proposes a reasoning-driven process reward modeling framework for process-level evaluation of LLM reasoning steps, which is a training/evaluation contribution rather than positional encoding or dimensional lifting.
- Focuses on model learning and inference strategies (seed data, preference optimization, inference-time scaling) to improve reward modeling quality, aligning with ML training/optimization principles.

## Evidence extracts
- "Toaddresstheseissues,weproposeaReasoning-
  Driven Process Reward Modeling (R-PRM) frame-
  work that leverages the inherent reasoning capa-
  bilities of LLMs to conduct process-level eval-
  uation." (p. 1)
- "In this section, we propose a novel reasoning-
  driven process-level reward modeling framework.
  Its core objective is to fully leverage the inher-
  ent reasoning capabilities of LLMs to evaluate
  the given reasoning steps, achieved through three
  stages: cold start with limited labeled data, self-
  evolution via preference optimization, and infer-
  ence time scaling." (p. 2)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
