               of this forward flow. The process continues until the dis-       wedividethesPQintofourcomponents: sPQd fordynamic
                                                        ′
               tance between current query position p and the point p           objects, sPQ for static objects, sPQ   for thing classes, and
                                                                                             s                      th
               closely approximates the inverse of the forward flow. The        sPQst for stuff classes. In the streaming setting, evaluation
               pseudo-code for this process is as follows:                      of each frame must occur at every input timestamp, accord-
                                                                                ing to the dataset’s frame rate. If the computation for the
               Algorithm 1 Iterative Inverse Forward Flow Method                current frame is not completed in time, we use the features
               Require: forecast forward flow query Q, stop threshold ϵ,        from the last completed frame to query the results and per-
                   maximumiterations N                                          form the evaluation.
                                          max                                   Implementation details. We choose P3Former [42] and
                 1: for each point p in the non-key frame do
                 2:    Initialize current query position p′ ← p                 Mask4Former [45] as our backbone model, which is origi-
                 3:    Initialize iteration counter n ← 0                       nallyaSOTAmethodfor3Dand4Dpanopticsegmentation.
                 4:    Inverse(f) ← −f                                          Byincorporatingtheegoposeandflowalignmentstrategies
                                 ′            ′                                 we proposed, along with memory construction, they can
                 5:    while ∥(p −p)+Q(p )∥ ≥ ϵandn < N                do
                                                                  max
                 6:        Query local forecast forward flow f ← Q(p′)          also achieve good performance in 4D streaming panoptic
                                                   ′                            segmentation. We first train the model on each dataset, then
                 7:        Update track position: p ← p + Inverse(f)            freeze it for feature extraction. The remaining components,
                 8:        Increment iteration counter: n ← n + 1               including ego-pose forecasting, forward flow forecasting,
                 9:    endwhile                                                 and history memory aggregation, are trained subsequently.
                10: end for                                                     For the inverse flow iteration, the maximum iterations pa-
                                                                                tience is set to 10. All models are trained on 4 NVIDIA
               5. Experiments                                                   GTX3090GPUsandevaluated on a single NVIDIA GTX
                                                                                3090GPU.
                  We present the experimental setup and benchmark re-           5.2. Streaming 4D Panoptic Segmentation in Out-
               sults on two widely used outdoor LiDAR-based panoptic                   doordatasets
               segmentationdatasets, SemanticKITTI[4]andnuScenes[5],
               as well as the indoor dataset HOI4D[30].                         SemanticKITTI [4]. Tab. 1 and 2 compare streaming 4D
               5.1. Settings                                                    panoptic segmentation on the SemanticKITTI validation
                                                                                split in the unknown and known pose settings. We compare
               SemanticKITTI [4].        SemanticKITTI is a large-scale         our method with StreamYOLO [44], LongShortNet [23],
               dataset for LiDAR-based panoptic segmentation, contain-          DAMO-StreamNet [17], Mask4Former [45], Eq-4D-StOP
               ing 23,201 outdoor scene frames at 10 fps. Unlike tradi-         [48] and PTv3 [41]. Originally designed for 2D streaming
               tional 4D panoptic segmentation, streaming 4D panoptic           object detection via temporal feature fusion, the first three
               segmentation also involves distinguishing between moving         modelsareadaptedto4Dstreamingbyreplacingtheirback-
               and static objects, since the ability to perceive moving ob-     bones with P3Former [42]. Mask4Former and Eq-4D-StOP
               jects is significant in streaming perception.  This adds 6       are designed for 4D panoptic segmentation but are not op-
               additional classes for moving objects (e.g., ”moving car”)       timized for streaming. PTv3 is a state-of-the-art method
               to the standard 19 semantic classes. In total, there are 25      designed for 3D perception. We adapt it to 4D panoptic
               classes, including 14 thing classes and 11 stuff classes.        segmentation with flow propagation according to [2].
               nuScenes[5]. nuScenes is a publicly available autonomous            Frombothtables,weobservethat2Dstreamingmethods
               driving dataset with 1,000 scenes captured at 2 fps. We ex-      performpoorlyduetotheirrelianceonreal-timebackbones,
               tend the per-point semantic labels to distinguish between        which are difficult to achieve in such a high-granularity
               moving and non-moving objects using ground truth 3D              task.  Similarly, 4D panoptic segmentation methods also
               bounding box attributes. This extension includes 8 mov-          suffer significant performance degradation due to computa-
               ing object classes and 16 static object classes, totaling 18     tional latency. PTv3 performs better than 4D methods due
               thing classes and 6 stuff classes.                               to its high efficiency, but it still suffers from performance
               HOI4D [30]. HOI4D is a large-scale egocentric dataset            drop. In contrast, our method outperforms all baseline mod-
               focused on indoor human-object interactions. It contains         els by a large margin in the streaming setting. Notably,
               3,865 point cloud sequences, with 2,971 for training and         in the unknown pose setting, our method achieves signifi-
               892 for testing. Each sequence has 300 frames captured at        cantimprovementsof7.7%and15.2%insLSTQoverPTv3
               15fps.                                                           [41]when integrated with P3Former and Mask4Former re-
               Evaluation metrics. We use PQ and LSTQ in streaming              spectively, demonstrating the effectiveness of our alignment
               setting (denoted as sPQ and sLSTQ) as our main metrics to        strategies across both dynamic and static classes. When
               evaluate panoptic segmentation performance. Furthermore,         combined with Mask4Former, our method outperforms its
                                                                             6
