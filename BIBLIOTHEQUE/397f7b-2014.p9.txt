                         Published as a conference paper at ICLR 2015
                         6.2  NEURALNETWORKSFORMACHINETRANSLATION
                         SinceBengioetal.(2003)introducedaneuralprobabilisticlanguagemodelwhichusesaneuralnet-
                         work to model the conditional probability of a word given a ﬁxed number of the preceding words,
                         neural networks have widely been used in machine translation. However, the role of neural net-
                         workshasbeenlargelylimitedtosimplyprovidingasinglefeaturetoanexistingstatistical machine
                         translation system or to re-rank a list of candidate translations provided by an existing system.
                         For instance, Schwenk (2012) proposed using a feedforward neural network to compute the score of
                         a pair of source and target phrases and to use the score as an additional feature in the phrase-based
                         statistical machine translation system. More recently, Kalchbrenner and Blunsom (2013) and Devlin
                         et al. (2014) reported the successful use of the neural networks as a sub-component of the existing
                         translation system. Traditionally, a neural network trained as a target-side language model has been
                         used to rescore or rerank a list of candidate translations (see, e.g., Schwenk et al., 2006).
                         Although the above approaches were shown to improve the translation performance over the state-
                         of-the-art machine translation systems, we are more interested in a more ambitious objective of
                         designing a completely new translation system based on neural networks. The neural machine trans-
                         lation approach we consider in this paper is therefore a radical departure from these earlier works.
                         Rather than using a neural network as a part of the existing system, our model works on its own and
                         generates a translation from a source sentence directly.
                         7   CONCLUSION
                         The conventional approach to neural machine translation, called an encoder–decoder approach, en-
                         codes a whole input sentence into a ﬁxed-length vector from which a translation will be decoded.
                         Weconjectured that the use of a ﬁxed-length context vector is problematic for translating long sen-
                         tences, based on a recent empirical study reported by Cho et al. (2014b) and Pouget-Abadie et al.
                         (2014).
                         In this paper, we proposed a novel architecture that addresses this issue. We extended the basic
                         encoder–decoder by letting a model (soft-)search for a set of input words, or their annotations com-
                         puted by an encoder, when generating each target word. This frees the model from having to encode
                         awholesourcesentenceintoaﬁxed-lengthvector,andalsoletsthemodelfocusonlyoninformation
                         relevant to the generation of the next target word. This has a major positive impact on the ability
                         of the neural machine translation system to yield good results on longer sentences. Unlike with
                         the traditional machine translation systems, all of the pieces of the translation system, including
                         the alignment mechanism, are jointly trained towards a better log-probability of producing correct
                         translations.
                         Wetestedtheproposedmodel,called RNNsearch, on the task of English-to-French translation. The
                         experiment revealed that the proposed RNNsearch outperforms the conventional encoder–decoder
                         model (RNNencdec) signiﬁcantly, regardless of the sentence length and that it is much more ro-
                         bust to the length of a source sentence. From the qualitative analysis where we investigated the
                         (soft-)alignment generated by the RNNsearch, we were able to conclude that the model can cor-
                         rectly align each target word with the relevant words, or their annotations, in the source sentence as
                         it generated a correct translation.
                         Perhapsmoreimportantly,theproposedapproachachievedatranslationperformancecomparableto
                         the existing phrase-based statistical machine translation. It is a striking result, considering that the
                         proposed architecture, or the whole family of neural machine translation, has only been proposed
                         as recently as this year. We believe the architecture proposed here is a promising step toward better
                         machine translation and a better understanding of natural languages in general.
                         Oneofchallengesleftforthefutureistobetterhandleunknown,orrarewords. Thiswillberequired
                         for the model to be more widely used and to match the performance of current state-of-the-art
                         machine translation systems in all contexts.
                                                                    9
