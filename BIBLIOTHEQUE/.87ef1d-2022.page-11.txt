           Published as a conference paper at ICLR 2022
           Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher
            Hesse,andJohnSchulman. Trainingveriﬁerstosolvemathwordproblems. CoRR,abs/2110.14168,
            2021. URLhttps://arxiv.org/abs/2110.14168.
           Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov.
            Transformer-XL: Attentive language models beyond a ﬁxed-length context. In ACL, 2019.
           Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
            bidirectional transformers for language understanding. In ACL, 2019.
           Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing
            somelimitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020.
           Angela Fan, Claire Gardent, Chloé Braud, and Antoine Bordes. Augmenting transformers with
            KNN-based composite memory for dialog. Transactions of the Association for Computational
            Linguistics, 9:82–99, 2021.
           Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a
            continuous cache. In ICLR, 2017.
           Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar.
            Accelerating large-scale inference with anisotropic vector quantization. In ICML, 2020.
           Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efﬁcient
            transformers via top-k attention. CoRR, abs/2106.06899, 2021. URL https://arxiv.org/
            abs/2106.06899.
           Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval augmented
            language model pre-training. In ICML, 2020.
           Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, and Bernd Finkbeiner.
            Teaching temporal logics to neural networks. In ICLR, 2021.
           Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
            Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL
            http://github.com/google/flax.
           Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. Query-key
            normalization for transformers. In EMNLP, 2020.
           Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE
            Transactions on Big Data, 2021.
           Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization
            through memorization: Nearest neighbor language models. In ICLR, 2020.
           Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In ICLR,
            2020.
           Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
            tokenizer and detokenizer for neural text processing. In EMNLP, 2018.
           Guillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé
            Jégou. Large memory layers with product keys. In NeurIPS, 2019.
           Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke
            Zettlemoyer. Pre-training via paraphrasing. In NeurIPS, 2020a.
           Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
            Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-
            tion for knowledge-intensive NLP tasks. In NeurIPS, 2020b.
           Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C. Paulson. Isarstep: a benchmark for high-level
            mathematical reasoning. In ICLR, 2021.
                               11
