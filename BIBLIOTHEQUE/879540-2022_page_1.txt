                   LIFT:Learning4DLiDARImageFusionTransformerfor3DObjectDetection
                                                                   1                      2                              1                             2
                                                Yihan Zeng                DaZhang               ChunweiWang                     Zhenwei Miao
                                                                        2                     2                          2                    1*
                                                          Ting Liu            Xin Zhan              DayangHao                  ChaoMa
                                1 MoEKeyLabofArtificial Intelligence, AI Institute, Shanghai Jiao Tong University
                                                                              2 Alibaba DAMOAcademy
                                 {zengyihan, weiwei0224, chaoma}@sjtu.edu.cn, zhangda.zhang@alibaba-inc.com
                                                  Abstract                                                    s
                                                                                                              t                                                                      L
                                                                                                              n                                                                      i
                     LiDAR and camera are two common sensors to collect                                       i                                                                      DAR P
                 data in time for 3D object detection under the autonomous                                                                                                           o
                                                                                                                                                                                     i
                                                                                                                                                                                     n
                                                                                                              DAR Po                                                                 t
                 driving context.         Though the complementary information                                i
                 across sensors and time has great potential of benefiting                                 )  L                                      MissingPoints
                 3D perception, taking full advantage of sequential cross-                                 (a
                                                                                                                                                                                     GT
                 sensor data still remains challenging. In this paper, we pro-                                                                                          Occlusion     B
                                                                                                             age                                                                     o
                                                                                                                                                                                     x
                                                                                                                                                                                     e
                 pose a novel LiDAR Image Fusion Transformer (LIFT) to                                       a Im                                                                    s
                 model the mutual interaction relationship of cross-sensor                                   r
                                                                                                             e                                                                       L
                                                                                                                                                                                     i
                 dataovertime. LIFTlearnstoaligntheinput4Dsequential                                         am                                                                      DAR F
                 cross-sensor data to achieve multi-frame multi-modal infor-                                 C
                                                                                                                                                                                     e
                                                                                                                                                                                     a
                                                                                                                                                                                     t
                 mation aggregation. To alleviate computational load, we                                                                                                             u
                                                                                                            Fusion Scheme                                                            r
                 project both point clouds and images into the bird-eye-view                                                                                                         e
                 mapstocomputesparsegrid-wise self-attention. LIFT also                                                                                                              C
                                                                                                           )                                                                         a
                                                                                                           b                                                                         m
                 benefits from a cross-sensor and cross-time data augmen-                                  (                                                                         era
                                                                                                                                                                                      F
                 tation scheme. We evaluate the proposed approach on the                                                                                                             ea
                                                                                                                                                                                     t
                                                                                                                                                                                     u
                 challengingnuScenesandWaymodatasets,whereourLIFT                                               (i) Separate Interaction             (ii) Mutual Interaction         re
                 performswelloverthestate-of-the-artandstrongbaselines.
                 1. Introduction                                                                        Figure 1. Illustration of the information interaction between se-
                                                                                                        quential cross-sensor data. (a) The misaligned complementary in-
                     3D object detection plays the primary role in scene un-                            formation cross sensors over time. (b) Information fusion scheme:
                 derstanding for autonomous driving, where cameras and                                  (i) Integrating the cross-sensor data at the corresponding times-
                 LiDAR are two standard complementary sensors for au-                                   tamp, then combining the sequential information within sensor
                 tonomous vehicles to perceive environments in time. Cam-                               streams.     (ii) Aggregating information from all timestamps in
                 eras provide sequential 2D images with rich texture and                                cross-sensor data streams. Mutual interaction can better connect
                                                                                                        misaligned complementary information across sensors and time.
                 color cues, while LiDAR specializes in distance sensing via
                 continuous sparse 3D points. Successfully detecting 3D ob-
                 jects in the environments hinges on the best exploitation                              synchronized images and point clouds.
                 of all available data across sensors and time to cooperate                                 Due to the challenges of jointly processing sequen-
                 complementary information. However, we observe that the                                tial cross-sensor data, existing 3D object detection algo-
                 cross-sensor information may be misaligned over time, as                               rithms independently perform information fusion over time
                 illustrated in Figure 1(a). The reasons lie in two aspects.                            or across sensors.          On one hand, a large portion of ap-
                 First, there may exist asynchronous timelines between Li-                              proaches attempt to exploit the valuable temporal infor-
                 DARandcameras. Second,thedifferentcoordinatesystems                                    mation from multiple frames or a longer sequential in-
                 across sensors introduce spatial misalignment even between                             put [18,35,41,42]. In addition to the straight-forward point
                     * Corresponding author.                                                            concatenation [20,35] to produce denser point cloud, con-
                                                                                                    17172
