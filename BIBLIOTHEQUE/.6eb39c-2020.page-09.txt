                                                    Agent57: Outperforming the Atari Human Benchmark
               Acknowledgments                                                    Burda, Y., Edwards, H., Storkey, A., and Klimov, O. Ex-
               We thank Daan Wierstra, Koray Kavukcuoglu, Vlad                       ploration by random network distillation. arXiv preprint
               Mnih, Vali Irimia, Georg Ostrovski, Mohammad Ghesh-                   arXiv:1810.12894, 2018.
               laghi Azar, Rémi Munos, Bernardo Avila Pires, Florent              Choi, J., Guo, Y., Moczulski, M., Oh, J., Wu, N., Norouzi,
               Altché, Steph Hughes-Fitt, Rory Fitzpatrick, Andrea Ban-              M., and Lee, H. Contingency-aware exploration in re-
               ino, Meire Fortunato, Melissa Tan, Benigno Uria, Borja                inforcement learning. arXiv preprint arXiv:1811.01483,
               Ibarz, Andre Barreto, Diana Borsa, Simon Osindero, Tom                2018.
               Schaul,andmanyothercolleaguesatDeepMindforhelpful                  Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O.,
               discussions and comments on the manuscript.                           and Clune, J. Go-explore: a new approach for hard-
               References                                                            exploration problems. arXiv preprint arXiv:1901.10995,
                                                                                     2019.
               Achiam, J., Edwards, H., Amodei, D., and Abbeel, P.                Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Di-
                  Variational option discovery algorithms. arXiv preprint            versity is all you need: Learning skills without a reward
                  arXiv:1807.10299, 2018.                                            function. arXiv preprint arXiv:1802.06070, 2018.
               Andrychowicz,M.,Baker,B.,Chociej,M.,Jozefowicz,R.,                 Ferret, J., Marinier, R., Geist, M., and Pietquin, O. Self-
                  McGrew,B.,Pachocki,J.,Petron,A.,Plappert,M.,Pow-                   attentional credit assignment for transfer in reinforce-
                  ell, G., Ray, A., et al. Learning dexterous in-hand ma-            mentlearning. IJCAI, 2020.
                  nipulation. arXiv preprint arXiv:1808.00177, 2018.              Fortunato, M., Azar, M.G., Piot, B., Menick, J., Osband, I.,
               Arjona-Medina, J. A., Gillhofer, M., Widrich, M., Un-                 Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin,
                  terthiner, T., Brandstetter, J., and Hochreiter, S. Rudder:        O., et al. Noisy networks for exploration. arXiv preprint
                  Return decomposition for delayed rewards. In Advances              arXiv:1706.10295, 2017.
                  in Neural Information Processing Systems, pp. 13544–            Fortunato, M., Tan, M., Faulkner, R., Hansen, S., Badia,
                  13555, 2019.                                                       A.P.,Buttimore,G.,Deck,C.,Leibo,J.Z.,andBlundell,
               Aytar, Y., Pfaff, T., Budden, D., Paine, T., Wang, Z., and            C. Generalization of reinforcement learners with work-
                  deFreitas, N. Playing hard exploration games by watch-             ing and episodic memory. In Advances in Neural Infor-
                  ing youtube. In Advances in Neural Information Pro-                mation Processing Systems, pp. 12448–12457, 2019.
                  cessing Systems, pp. 2930–2941, 2018.                           Fu, J., Co-Reyes, J., and Levine, S. Ex2: Exploration with
               Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T.,          exemplar models for deep reinforcement learning. In
                  van Hasselt, H. P., and Silver, D. Successor features for          Advances in neural information processing systems, pp.
                  transfer in reinforcement learning. In Advances in neural          2577–2587, 2017.
                  information processing systems, pp. 4055–4065, 2017.            Garivier, A. and Moulines, E. On upper-conﬁdence bound
                                                                                     policies for non-stationary bandit problems, 2008.
               Barto, A. G. Intrinsic motivation and reinforcement learn-         Gregor, K., Rezende, D. J., and Wierstra, D. Variational in-
                  ing. In Intrinsically motivated learning in natural and            trinsic control. arXiv preprint arXiv:1611.07507, 2016.
                  artiﬁcial systems, pp. 17–47. Springer, 2013.
               Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T.,          Harutyunyan, A., Dabney, W., Mesnard, T., Azar, M. G.,
                  Saxton, D., and Munos, R. Unifying count-based explo-              Piot, B., Heess, N., van Hasselt, H. P., Wayne, G., Singh,
                  ration and intrinsic motivation. In Advances in Neural             S., Precup, D., et al. Hindsight credit assignment. In
                  Information Processing Systems, pp. 1471–1479, 2016.               Advances in neural information processing systems, pp.
                                                                                     12467–12476, 2019.
               Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.          Horgan, D., Quan, J., Budden, D., Barth-Maron, G.,
                  The arcade learning environment: An evaluation plat-               Hessel, M., Van Hasselt, H., and Silver, D.           Dis-
                  formforgeneralagents. JournalofArtiﬁcialIntelligence               tributed prioritized experience replay.    arXiv preprint
                  Research, 47:253–279, 06 2013.                                     arXiv:1803.00933, 2018.
               Blundell, C., Uria, B., Pritzel, A., Li, Y., Ruderman,             Hung, C.-C., Lillicrap, T., Abramson, J., Wu, Y., Mirza,
                  A., Leibo, J. Z., Rae, J., Wierstra, D., and Hass-                 M., Carnevale, F., Ahuja, A., and Wayne, G. Optimiz-
                  abis, D.   Model-free episodic control. arXiv preprint             ing agent behavior over long time scales by transporting
                  arXiv:1606.04460, 2016.                                            value. Nature communications, 10(1):1–12, 2019.
