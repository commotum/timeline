# Distributed Representations of Words and Phrases and their Compositionality (2013)
Source: 33f01b-2013.pdf

## Core reasons
- The paper focuses on learning distributed word and phrase representations with the Skip-gram model, a foundational modeling contribution rather than positional encoding or transformer adaptation.
- It introduces training-method improvements such as negative sampling to improve the learned representations, which fits ML training/optimization principles.

## Evidence extracts
- "alsolearnmoreregularwordrepresentations.Wealsodescr
ibeasimplealterna-
tivetothehierarchicalsoftmaxcallednegativesampling." (p. 1)
- "Thisworkhasseveralkeycontributions.Weshowhowtotrain
distributedrepresentationsofwords
andphraseswiththeSkip-grammodelanddemonstratethatth
eserepresentationsexhibitlinear
structurethatmakespreciseanalogicalreasoningpossibl
e.Thetechniquesintroducedinthispaper" (p. 8)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
