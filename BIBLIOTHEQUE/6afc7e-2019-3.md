# Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019)
Source: 6afc7e-2019.pdf

## Core reasons
- The paper frames fixed-length context as a core limitation for Transformers in language modeling and targets longer-term dependency modeling.
- It introduces a segment-level recurrence/state-reuse mechanism that changes computation by caching past hidden states as extended context.

## Evidence extracts
- "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling." (Abstract)
- "During training, the hidden state sequence computed for the previous segment is fixed and cached to be reused as an extended context when the model processes the next new segment, as shown in Fig. 2a." (Section 3.2 Segment-Level Recurrence with State Reuse)

## Classification
Class name: Computation & Reasoning Mechanism Proposal
Class code: 3

$$
\boxed{3}
$$
