                                             ANEURALPROBABILISTIC LANGUAGEMODEL
                    and McCallum, 1998): each word is associated deterministically or probabilistically with a discrete
                    class, and words in the same class are similar in some respect. In the model proposed here, instead
                    of characterizing the similarity with a discrete random or deterministic variable (which corresponds
                    to a soft or hard partition of the set of words), we use a continuous real-vector for each word, i.e.
                    a learned distributed feature vector, to represent similarity between words. The experimental
                    comparisons in this paper include results obtained with class-based n-grams (Brown et al., 1992,
                    NeyandKneser, 1993, Niesler et al., 1998).
                        Theidea of using a vector-space representation for words has been well exploited in the area of
                    information retrieval (for example see work by Schutze, 1993), where feature vectors for words are
                    learned on the basis of their probability of co-occurring in the same documents (Latent Semantic
                    Indexing, see Deerwester et al., 1990). An important difference is that here we look for a repre-
                    sentation for words that is helpful in representing compactly the probability distribution of word
                    sequences from natural language text. Experiments suggest that learning jointly the representation
                    (word features) and the model is very useful. We tried (unsuccessfully) using as ﬁxed word features
                    for each word w the ﬁrst principal components of the co-occurrence frequencies of w with the words
                    occurring in text around the occurrence of w. This is similar to what has been done with documents
                    for information retrieval with LSI. The idea of using a continuous representation for words has how-
                    ever been exploited successfully by Bellegarda (1997) in the context of an n-gram based statistical
                    language model, using LSI to dynamically identify the topic of discourse.
                        Theideaofavector-space representation for symbols in the context of neural networks has also
                    previously been framed in terms of a parameter sharing layer, (e.g.   Riis and Krogh, 1996) for
                    secondary structure prediction, and for text-to-speech mapping (Jensen and Riis, 2000).
                    2. A Neural Model
                    The training set is a sequence w ···w   of words w ∈V, where the vocabulary V is a large but
                                                    1     T            t                 ˆ     t−1
                    ﬁnite set. The objective is to learn a good model f(w ,···,w    )=P(w|w ),inthesense that
                                                                        t      t−n+1        t  1         ˆ     t−1
                    it gives high out-of-sample likelihood. Below, we report the geometric average of 1/P(w |w    ),
                                                                                                            t  1
                    also known as perplexity, which is also the exponential of the average negative log-likelihood. The
                                                                           t−1    |V|
                    only constraint on the model is that for any choice of w  , ∑    f (i,w  ,···,w      )=1, with
                                                                            1     i=1     t−1       t−n+1
                     f >0. Bythe product of these conditional probabilities, one obtains a model of the joint probability
                    of sequences of words.
                                                                        ˆ      t−1
                        Wedecompose the function f(w ,···,w         )=P(w|w )intwoparts:
                                                       t       t−n+1        t  1
                       1. A mappingC from any element i ofV to a real vectorC(i) ∈Rm. It represents the distributed
                          feature vectors associated with each word in the vocabulary. In practice, C is represented by
                          a |V|×mmatrix of free parameters.
                       2. The probability function over words, expressed with C: a function g maps an input sequence
                          of feature vectors for words in context, (C(w   ),···,C(w    )), to a conditional probability
                                                                     t−n+1          t−1
                          distribution over words in V for the next word w . The output of g is a vector whose i-th
                                                           ˆ         t−1   t
                          element estimates the probability P(w = i|w   ) as in Figure 1.
                                                               t     1
                                             f (i,w   ,···,w     )=g(i,C(w      ),···,C(w      ))
                                                   t−1      t−n+1            t−1          t−n+1
                    The function f is a composition of these two mappings (C and g), with C being shared across
                    all the words in the context. With each of these two parts are associated some parameters. The
                                                                  1141
