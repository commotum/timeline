                                           highly depend on the positional vectors within the context window, we speculate that OOD positional
                                           vectors disrupt the original attention distribution. Besides, we feed the positional vectors of
                                           the last layer into the linear projection of the softmax layer to get the logits at different positions.
                                           Figure 5 (Right) presents that the logits within the context window are tightly similar while others
                                           showdifferent distributions. Thus, the OOD positional vectors can damage the token prediction
                                           probability distribution, thereby leading to performance degradation.
                                                             0    Logarithmic Attention Map           30       Attention Scores on Initial Token      0        Similarity of Logits
                                                                                                            1.0
                                                            500                                                                                     500
                                                                                                      25
                                                           1000                                            e0.8                                    1000
                                                           1500                                       20                                           1500
                                                                                                            0.6
                                                           2000                                       15                                           2000
                                                          osition id2500                                    0.4                                    2500
                                                          P                                           10   ttention Scor                          position id
                                                           3000                                            A                                       3000
                                                                                                            0.2
                                                           3500                                       5                                            3500
                                                           4000                                      0      0.0                                    4000
                                                               0      1000    2000    3000    4000              0     1000   2000    3000   4000       0      1000    2000    3000    4000
                                                                           Position id                                     Position id                             position id
                                           Figure 5: Left: Attention map of TL-NoPE. Middle: Attention Scores between initial token and
                                           others in TL-NoPE. Right: Similarity of logits of positional vectors across positions in TL-NoPE.
                                           Table 3: The interpolation results of positional vectors, where Factor (= Target Length/C) is the
                                           expansion factor of the context window, Ratio is the effective interpolation ratio of positional vectors
                                           (detailed in Appendix C), and Similarity is the average cosine similarity between the scaled positional
                                           vector and the original most similar positional vector by averaging all layers.
                                              Model              Method                                       Target Length           Factor        Ratio       Similarity           PPL/∆PPL
                                                                 Attention Scaling (λ = 1.2)                        4096                  2          2.56           0.98             8.95/+1.42
                                              TL-NoPE            Attention Scaling (λ = 1.3)                        8192                  4          4.30           0.94           17.87/+10.34
                                                                 Initial Scaling (λ = 1.2)                          4096                  2          2.38           0.97             9.82/+2.29
                                                                 Initial Scaling (λ = 1.3)                          8192                  4          4.10           0.91           32.78/+25.25
                                              TL-RoPE            DynamicNTK                                         4096                  2          2.05           0.99             6.00/-0.02
                                                                 DynamicNTK                                         8192                  4          3.75           0.96             6.78/+0.76
                                           3.3.2       Context WindowExtension
                                           ChangeofPositional Vectors When Extending Context Windows                                                          Toinvestigate why context
                                           windowextension can prevent performance degradation, we analyze the change of positional vectors
                                           in two training-free context window extension methods, including dynamic-NTK [11] for TL-RoPE
                                           and attention scaling (qikj multiplied by a scaling factor λ) [20] for TL-NoPE. From Figure 6,
                                           we can see that after context window extension, positional vectors have undergone interpolation
                                           compared to the original ones. Comparing the Factor and Ratio metrics in Table 3, we conclude that
                                           the effective interpolation ratio is close to the expansion factor (e.g., 2 vs 2.56). Besides, as the
                                           expansion factor increases, there is a decrease in Similarity and an increase in PPL. Therefore, we
                                           suspect that imperfect interpolation may be a major reason for the decline in model performance.
                                                   0                  TL-RoPE Dynamic NTK                                          0               TL-NoPE Attention Scaling
                                                 500                                                                             500
                                                1000                                                                           1000
                                                1500                                                                           1500
                                               position id (Original)2000 01000200030004000 5000    6000    7000    8000      position id (Original)2000 01000200030004000 5000    6000    7000    8000
                                                                            position id (Dynamic NTK)                                                     position id (Attention Scaling)
                                                          0.800 0.825  0.850  0.875 0.900  0.925 0.950  0.975                               0.2     0.0      0.2     0.4      0.6      0.8
                                                   Figure 6: The average cosine similarity between the scaled and original positional vectors.
                                                                                                                         7
