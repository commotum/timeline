                   Published as a conference paper at ICLR 2022
                                 MEMORIZING TRANSFORMERS
                             Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins, Christian Szegedy
                               {yuhuai,mrabe,delesley,szegedy}@google.com
                                               ABSTRACT
                         Language models typically need to be trained or ﬁnetuned in order to acquire
                         new knowledge, which involves updating their weights. We instead envision
                         language models that can simply read and memorize new data at inference time,
                         thus acquiring new knowledge immediately. In this work, we extend language
                         models with the ability to memorize the internal representations of past inputs. We
                         demonstrate that an approximate kNN lookup into a non-differentiable memory of
                         recent (key, value) pairs improves language modeling across various benchmarks
                         and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19),
                         code(Github), as well as formal theorems (Isabelle). We show that the performance
                         steadily improves when we increase the size of memory up to 262K tokens. On
                         benchmarks including code and mathematics, we ﬁnd that the model is capable of
                         making use of newly deﬁned functions and theorems during test time.
                   1  INTRODUCTION
                   Transformers (Vaswani et al., 2017) have led to remarkable progress in natural language process-
                   ing (Devlin et al., 2019; Brown et al., 2020), mathematical reasoning (Polu & Sutskever, 2020; Wang
                   et al., 2020a; Rabe et al., 2021; Li et al., 2021; Hahn et al., 2021; Cobbe et al., 2021), and program
                   synthesis (Austin et al., 2021; Chen et al., 2021; Li et al., 2022). However, transformer performance
                   on many of these tasks is limited by the context length of attention, which is typically short. The
                   ability to attend to far-away tokens is important in many situations. In novels, characters and events
                   are referenced across multiple chapters. In source code, references to classes and functions may
                   occur quite far from the places in which they are deﬁned. In theorem proving, proofs make use of
                   previously deﬁned lemmas.
                   Attention over long sequences is also useful as a form of rapid learning. Facts and information
                   which are stored in the form of weight matrices must be slowly trained over hundreds of thousands
                   of training steps. By using attention, however, a model can simply memorize facts (e.g. function
                   deﬁnitions) by storing them as (key, value) pairs in long-term memory, and then retrieve those facts
      arXiv:2203.08913v1  [cs.LG]  16 Mar 2022later by creating a query that attends to them. In this case, attention acts as a form of information
                   retrieval, allowing the model to look up facts that it has seen previously.
                   Wedemonstrate that a simple and effective way to increase the size of the attention context is to use
                   approximate k-nearest-neighbor (kNN) lookup, which is widely used in information retrieval. A
                   numberofextremelyscalable implementations of kNN lookup are available, such as ScaNN (Guo
                   et al., 2020) and Faiss (Johnson et al., 2021).
                   There are two things which distinguish our approach from previous work on long-range attention (c.f.
                   Section 2). First, unlike some other approaches, kNN lookup does not do averaging or summarization
                   of tokens at long distances, but retrieves exact values even from the distant context.
                   Second, gradients are not backpropagated into the external memory, which is critical to the scalability
                   of our technique. The keys and values are a function of model parameters, so attempting to backprop-
                   agate gradients into external memory would necessarily involve computing all of the keys and values
                   with the current model parameters on every training step. However, if the external memory is not
                   differentiable, then we can instead instead reuse keys and values that were previously computed on
                   prior training steps, which drastically reduces the amount of computation for large memories. With
                                                    1
