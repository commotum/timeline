               the liquids are mixed in some areas but not in others—it seems more diﬃcult to describe what the
               contents of the cup look like.
                   Thus, it appears that the coﬀee cup system starts out at a state of low complexity, and that
               the complexity ﬁrst increases and then decreases over time.   In fact, this rising-falling pattern of
               complexity seems to hold true for many closed systems. One example is the universe itself. The
               universe began near the Big Bang in a low-entropy, low-complexity state, characterized macro-
               scopically as a smooth, hot, rapidly expanding plasma. It is predicted to end in the high-entropy,
               low-complexity state of heat death, after black holes have evaporated and the acceleration of the
               universe has dispersed all of the particles (about 10100 years from now). But in between, complex
               structures such as planets, stars, and galaxies have developed. There is no general principle that
               quantiﬁes and explains the existence of high-complexity states at intermediate times in closed sys-
               tems.   It is the aim of this work to explore such a principle, both by developing a more formal
               deﬁnition of “complexity,” and by running numerical experiments to measure the complexity of a
               simulated coﬀee cup system.     The idea that complexity ﬁrst increases and then decreases in as
               entropy increases in closed system has been suggested informally [4,7], but as far as we know this
               is the ﬁrst quantitative exploration of the phenomenon.
               2    Background
               Before discussing how to deﬁne “complexity,” let’s start with the simpler question of how to deﬁne
               entropy in a discrete dynamical system. There are various deﬁnitions of entropy that are useful in
               diﬀerent contexts. Physicists distinguish between the Boltzmann and Gibbs entropies of physical
               systems. (Thereisalsothephenomenologicalthermodynamicentropyandthequantum-mechanical
               von Neumann entropy, neither of which are relevant here.) The Boltzmann entropy is an objective
               feature of a microstate, but depends on a choice of coarse-graining. We imagine coarse-graining the
               space of microstates into equivalence classes, so that each microstate x is an element of a unique
                                                                                      a
               macrostate X . The volume W of the macrostate is just the number of associated microstates
                             A                  A
               x ∈ X . Then the Boltzmann entropy of a microstate x is the normalized logarithm of the
                 a     A                                                    a
               volume of the associated macrostate:
                                                  S         (x ) := k logW ,                                  (1)
                                                   Boltzmann  a      B      A
               where kB is Boltzmann’s constant (which we can set equal to 1).        The Boltzmann entropy is
               independent of our knowledge of the system; in particular, it can be nonzero even when we know
               the exact microstate.   The Gibbs entropy (which was also studied by Boltzmann), in contrast,
               refers to a distribution function ρ(x) over the space of microstates, which can be thought of as
               characterizing our ignorance of the exact state of the system. It is given by
                                                 S     [ρ] := −Xρ(x)logρ(x).                                  (2)
                                                  Gibbs
                                                                x
               In probability theory, communications, information theory, and other areas, the Shannon entropy
               of a probability distribution D = (p ) is the expected number of random bits needed to output a
                                                   x x
               sample from the distribution:                    X
                                                    H(D):=−        p logp .                                   (3)
                                                                    x     x
                                                                 x
                                                               2
