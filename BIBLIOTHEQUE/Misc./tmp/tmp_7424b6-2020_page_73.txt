               [PSM14] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word
                      representation. In Proceedings of the 2014 conference on empirical methods in natural language
                      processing (EMNLP), 2014.
               [QIA20] QIANXIN. Sa-net on albert (ensemble), April 2020.
              [QMZH19] YusuQian,UrwaMuaz,BenZhang,andJaeWonHyun. Reducinggenderbiasinword-levellanguage
                      models with a gender-equalizing loss function. arXiv preprint arXiv:1905.12801, 2019.
               [RBG11] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An
                      evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.
               [RCM19] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering
                      challenge. Transactions of the Association for Computational Linguistics, 7:249–266, 2019.
                  +                                 ¨
              [RCP 17] Scott Reed, Yutian Chen, Thomas Paine, Aaron van den Oord, SM Eslami, Danilo Rezende, Oriol
                      Vinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn
                      distributions. arXiv preprint arXiv:1710.10304, 2017.
                [RJL18] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for
                      squad. arXiv preprint arXiv:1806.03822, 2018.
                [RL16] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR 2017 (oral),
                      2016.
                  +
              [RLL 19] QiuRan,YankaiLin,PengLi,JieZhou,andZhiyuanLiu. NumNet: Machinereadingcomprehension
                      with numerical reasoning. In Proceedings of EMNLP, 2019.
             [RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in
                      coreference resolution. arXiv preprint arXiv:1804.09301, 2018.
              [RNSS18] AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever. Improvinglanguageunderstanding
                      bygenerative pre-training, 2018.
                [Ros12] R.S. Ross. Guide for conducting risk assessments. NIST Special Publication, 2012.
              [RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of
                      the generalization error across scales, 2019.
               [RRS20] AdamRoberts,ColinRaffel,andNoamShazeer. Howmuchknowledgecanyoupackintotheparameters
                      of a language model? arXiv preprint arXiv:2002.08910, 2020.
              [RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
                      Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
                      transformer, 2019.
              [RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
                      models are unsupervised multitask learners, 2019.
              [SBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial
                      winograd schema challenge at scale, 2019.
              [SBC+19] IreneSolaiman,MilesBrundage,JackClark,AmandaAskell,ArielHerbert-Voss,JeffWu,AlecRadford,
                      Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris
                      McGufﬁe,andJasmineWang. Releasestrategies and the social impacts of language models, 2019.
              [SCNP19] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a
                      babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326, 2019.
              [SDCW19] VictorSanh,LysandreDebut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of
                      BERT:smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
              [SDSE19] RoySchwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. CoRR, abs/1907.10597, 2019.
               [SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with
                      monolingual data. arXiv preprint arXiv:1511.06709, 2015.
                                                  73
