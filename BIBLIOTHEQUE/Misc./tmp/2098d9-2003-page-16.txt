                                    BENGIO,DUCHARME,VINCENTANDJAUVIN
                  4. Introducing a-priori knowledge. Several forms of such knowledge could be introduced, such
                    as: semantic information (e.g., from WordNet, see Fellbaum, 1998), low-level grammatical
                    information (e.g., using parts-of-speech), and high-level grammatical information, e.g., cou-
                    pling the model to a stochastic grammar, as suggested in Bengio (2002). The effect of longer
                    term context could be captured by introducing more structure and parameter sharing in the
                    neural network, e.g. using time-delay or recurrent neural networks. In such a multi-layered
                    network the computation that has been performed for small groups of consecutive words does
                    not need to be redone when the network input window is shifted. Similarly, one could use a
                    recurrent network to capture potentially even longer term information about the subject of the
                    text.
                  5. Interpreting (and possibly using) the word feature representation learned by the neural net-
                    work. A simple ﬁrst step would start with m = 2 features, which can be more easily dis-
                    played. We believe that more meaningful representations will require large training corpora,
                    especially for larger values of m.
                  6. Polysemous words are probably not well served by the model presented here, which assigns
                    to each word a single point in a continuous semantic space. We are investigating extensions of
                    this model in which each word is associated with multiple points in that space, each associated
                    with the different senses of the word.
                6. Conclusion
                The experiments on two corpora, one with more than a million examples, and a larger one with
                above 15 million words, have shown that the proposed approach yields much better perplexity than
                astate-of-the-art method, the smoothed trigram, with differences between 10 and 20% in perplexity.
                  Webelieve that the main reason for these improvements is that the proposed approach allows
                to take advantage of the learned distributed representation to ﬁght the curse of dimensionality with
                its own weapons: each training sentence informs the model about a combinatorial number of other
                sentences.
                  There is probably much more to be done to improve the model, at the level of architecture,
                computational efﬁciency, and taking advantage of prior knowledge. An important priority of future
                                                      5
                research should be to improve speed-up techniques as well as ways to increase capacity without
                increasing training time too much (to deal with corpora withhundreds of millions of words or more).
                Asimple idea to take advantage of temporal structure and extend the size of the input window to
                include possibly a whole paragraph (without increasing too much the number of parameters or
                computation time) is to use a time-delay and possibly recurrent neural networks. Evaluations of the
                type of models presented here in applicative contexts would also be useful, but see work already
                done by Schwenk and Gauvain (2002) for improvements in speech recognition word error rate.
                  Moregenerally, the work presented here opens the door to improvements in statistical language
                models brought by replacing “tables of conditional probabilities” by more compact and smoother
                representations based on distributed representations that can accommodate far more conditioning
                variables. Whereas much effort has been spent in statistical language models (e.g. stochastic gram-
                mars) to restrict or summarize the conditioning variables in order to avoid overﬁtting, the type of
                 5. See work by Bengio and Senécal (2003) for a 100-fold speed-up technique.
                                                  1152
