      basic programming tasks, like those used in introductory computer science classes. Crucially the initial
      language includes primitive recursion (the Y combinator), which in principle allows learning to express
      any recursive function, but no other recursive function is given to start; previously we had sequestered
      recursion within higher-order functions (map, fold, ...) given to the learner as primitives. With
      enough compute time (roughly ﬁve days on 64 CPUs), DreamCoder learns to solve all 20 problems,
      and in so doing assembles a library equivalent to the modern repertoire of functional programming
      idioms, including map, fold, zip, length, and arithmetic operations such as building lists of natural
      numbers between an interval (see Fig. 7B). All these library functions are expressible in terms of
      the higher-order function fold and its dual unfold, which, in a precise formal manner, are the two
      most elemental operations over recursive data – a discovery termed “origami programming” (42).
      DreamCoderretraced the discovery of origami programming: ﬁrst reinventing fold, then unfold, and
      then deﬁning all other recursive functions in terms of folding and unfolding.
      Discussion
      Ourworkshowsthatitispossible and practical to build a single general-purpose program induction
      system that learns the expertise needed to represent and solve new learning tasks in many qualitatively
      different domains, and that improves its expertise with experience. Optimal expertise in Dream-
      Coder hinges on learning explicit declarative knowledge together with the implicit procedural skill to
      use it. More generally, DreamCoder’s ability to learn deep explicit representations of a domain’s con-
      ceptual structure shows the power of combining symbolic, probabilistic and neural learning approaches:
      Hierarchical representation learning algorithms can create knowledge understandable to humans, in
      contrast to conventional deep learning with neural networks, yielding symbolic representations of
      expertise that ﬂexibly adapt and grow with experience, in contrast to traditional AI expert systems.
       Wefocusedhereonproblemswherethesolutionspaceis well captured by crisp symbolic forms,
      even in domains that admit other complexities such as pixel image inputs, or exceptions and irreg-
      ularities in generative text patterns, or continuous parameters in our symbolic regression examples.
      Nonetheless, much real-world data is far messier. A key challenge for program induction going
      forward is to handle more pervasive noise and uncertainty, by leaning more heavily on probabilistic and
      neural AI approaches (5,43,44). Recent research has explored program induction with various hybrid
      neuro-symbolic representations (45–49), and integrating these approaches with the library learning and
      bootstrapping capacities of DreamCoder could be especially valuable going forward.
       Scaling up program induction to the full AI landscape — to commonsense reasoning, natural
      language understanding, or causal inference, for instance — will demand much more innovation but
      holds great promise. As a substrate for learning, programs uniquely combine universal expressiveness,
      data-efﬁcient generalization, and the potential for interpretable, compositional reuse. Now that we
      can start to learn not just individual programs, but whole domain-speciﬁc languages for programming,
      a further property takes on heightened importance: Programs represent knowledge in a way that is
      mutually understandable by both humans and machines. Recognizing that every AI system is in reality
      the joint product of human and machine intelligence, we see the toolkit presented here as helping to
      lay the foundation for a scaling path to AI that people and machines can truly build together.
       In the rest of this discussion, we consider the broader implications of our work for building better
      models of human learning, and more human-like forms of machine learning.
                          16
