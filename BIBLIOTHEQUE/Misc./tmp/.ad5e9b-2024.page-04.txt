                                                0.9  31     34     36     31     43     42     38    39     47     54     55     54
                                               0.75  48     49     55     55     55     56     47    54     45     57     58     37
                                                0.5  63     63     65     66     64     62     58    59     60     60     48     31
                                               0.25  69     67     65     62     63     64     64    60     53     36     29     32
                                              Repeated set probability0.15857576059     48     55    40     30     25     24     31
                                                      50     75    100    150    200    250   500    1000 2500 5000 10000 25000
                                                                             Repeated set size (thousands)
                                Figure 2: Two-set training for the GCD problem: Number of correctly predicted GCD as a function of S
                                and p. Each measurement is the average of 6 models. Data budget 100M, training budget 600M. Note the high
                                performance for very small sets S of sizes 50, 75, 100, 150 and 200 thousand, with p = 0.25 and p = 0.5.
                                Table 2: Two-set training on modular multiplication. Percentage of models (different random initializa-
                                tions) learning to compute modular multiplication with 50 and 99% accuracy. Training budget: 600M. For DB
                                25Mand50M,10modelswithtwo-settraining, and 25 with single set training. For DB 100M and unlimited,
                                26modelswithtwo-set training, and 30 with single set training.
                                                                                         Twosets               Single set
                                                     Data budget        p / S       >50% >99% >50% >99%
                                                     25M              0.1 / 1M         50         50         52         24
                                                     50M             0.25 / 2.5M       90         50         88         28
                                                     100M            0.5 / 10M         88         54          0          0
                                                     Unlimited       0.25 / 2.5M       92         58          0          0
                                Ablation studies (Appendix D) indicate that curating the small sample – selecting easier, or partic-
                                ular examples for the repeated set (an obvious strategy for improving two-set training), brings at
                                best a marginal increase in performance. They also indicate that mixing repeated and non-repeated
                                examples in the same mini-batches is an essential element of two-set training. Models trained on
                                batches exclusively selected from the small and large sets do not learn.
                                5    Conclusion
                                Ourfindingsindicate that the performance of math transformers can be greatly improved by training
                                them on datasets which include repeated examples. This can be done by using smaller train set, or
                                randomlyselectingarepeatedsubsetfromalargercorpus. OntheGCDproblem,repeatedexamples
                                allow for faster learning and better performance. For modular arithmetic, they are necessary for the
                                model to learn. This suggests that abandoning the customary practice of training models on the
                                largest possible set of single-use example may be beneficial.
                                Our observations on two-set training, in particular the fact that the repeated set can be selected at
                                random(andthat curation does not help) are thought-provoking. All that seems to matter is that the
                                exact same examples are repeated, i.e. not their informational content. This is all the more shocking
                                as repetition occurs at a very low frequency. In the GCD experiments, examples in the small set are
                                repeated 3000 times over a TB of 600 millions: once in 200,000 examples on average. For modular
                                multiplication, the frequency is even lower. Besides, the repeated examples need to be mixed with
                                non-repeated examples into mini-batches for the two-set effect to appear.
                                This raises several tantalizing questions: how does the transformer “figure” that a given example,
                                lost in a minibatch, has been seen, hundreds of thousands of examples before? Our research suggests
                                                                                           ´ `
                                that there exists a qualitative difference between “deja vu” and “jamais vu” examples – data points
                                the model has already seen, or never seen. How do transformers, and perhaps other architectures,
                                                                  ´ `
                                identify, and then process, “deja vu” examples? To our knowledge, this aspect was overlooked in
                                manyprior works on model interpretation. It is an intriguing subject for further study.
                                                                                        4
