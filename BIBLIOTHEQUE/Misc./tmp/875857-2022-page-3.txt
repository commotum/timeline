              Y via a ﬁnal mapping SΘ : U → Y. This enables us to               schemewhencomputingu˜(i.e.Algorithm1).Inthecontem-
              deﬁne an implicit network NΘ by                                   poraneous (Gilton, Ongie, and Willett 2021), it is reported
                     N (d) , S (u?) where u? = T (u?;d).              (7)       that using ﬁxed point iteration in conjunction with Anderson
                       Θ         Θ d              d     Θ d                     acceleration ﬁnds u˜ faster than both vanilla ﬁxed point itera-
                                                                                tion and Broyden’s method. Combining JFB with Anderson
              Algorithm 1: Implicit Network with Fixed Point Iteration          accelerated ﬁxed point iteration is a promising research di-
                                                                                rection we leave for future work.
               1: NΘ(d):                         CInputdatais d
               2:   u1 ←uˆ                       CAssignlatent term             Other Implicit Formulations      Arelated implicit learning
               3:   while kuk −T (uk;d)k > εCLooptilconverge                    formulation is the well-known neural ODE model (Chen
                                   Θ                                            et al. 2018; Dupont, Doucet, and Teh 2019; Ruthotto and
               4:    uk+1 ←TΘ(uk;d)              CReﬁnelatentterm               Haber2021).NeuralODEsleverageknownconnectionsbe-
               5:    k ←k+1                      CIncrementcounter              tween deep residual models and discretizations of differ-
               6: return SΘ(uk)                  COutputestimate                ential equations (Haber and Ruthotto 2017; Weinan 2017;
                                                                                Ruthotto and Haber 2019; Chang et al. 2018; Finlay et al.
                Implementation considerations for TΘ are discussed be-          2020; Lu et al. 2018), and replace these discretizations by
              low. We also introduce assumptions on TΘ that yield sufﬁ-         black-box ODE solvers in forward and backward passes.
              cient conditions to use the simple procedure in Algorithm         The implicit property of these models arise from their
              1 to approximate NΘ(d). In this algorithm, the latent vari-       method for computing gradients. Rather than backpropa-
              able initialization uˆ can be any ﬁxed quantity (e.g. the zero    gate through each layer, backpropagation is instead done by
              vector). The inequality in Step 3 gives a ﬁxed point residual     solving the adjoint equation (Jameson 1988) using a black-
              condition that measures convergence. Step 4 implements a          box ODE solver as well. This is analogous to solving the
              ﬁxed point update. The estimate of the inference NΘ(d) is         Jacobian-based equation when performing backpropagation
              computedbyapplyingSΘ tothelatentvariableuk inStep6.               for implicit networks (see (13)) and allows the user to al-
              ThebluepathinFigure1visually summarizes Algorithm 1.              leviate the memory costs of backpropagation through deep
              Convergence     Finitely many loops in Steps 3 and 4 of Al-       neural models by solving the adjoint equation at additional
              gorithm 1 is guaranteed by a classic functional analysis re-      computationalcosts. A drawbackis that the adjoint equation
              sult (Banach1922).Thisapproachisusedbyseveralimplicit             mustbesolvedtohigh-accuracy;otherwise,adescentdirec-
              networks (Ghaoui et al. 2019; Winston and Kolter 2020;            tion is not necessarily guaranteed (Gholami, Keutzer, and
              Jeon, Lee, and Choi 2021). Below we present a variation           Biros 2019; Onken and Ruthotto 2020; Onken et al. 2021).
              of Banach’s result for our setting.                                                  Backpropagation
              Assumption 0.1. The mapping TΘ is L-Lipschitz with re-
              spect to its inputs (u,d), i.e. ,                                 We present a simple way to backpropagate with implicit
                                                                                networks, called Jacobian-free backprop (JFB). Traditional
                  kTΘ(u; d)−TΘ(v; w)k ≤ Lk(u,d)−(v,w)k,               (8)       backpropagation will not work effectively for implicit net-
              for all (u,d),(v,w) ∈ U ×D. Holding d ﬁxed, the operator          workssinceforwardpropagationduringtrainingcouldentail
              T (·;d)isacontraction,i.e.thereexists γ ∈ [0,1) such that         hundreds or thousands of iterations, requiring ever growing
               Θ                                                                memory to store computational graphs. On the other hand,
               kTΘ(u;d)−TΘ(v;d)k ≤ γku−vk, forallu,v ∈ U. (9)                   implicit models maintain ﬁxed memory costs by backprop-
              Remark0.1. TheL-LipschitzconditiononT isusedsince                 agating “through the ﬁxed point” and solving a Jacobian-
                                                           Θ                    based equation (at potentially substantial added computa-
              recentworksshowLipschitzcontinuitywithrespecttoinputs             tional costs). The key step to circumvent this Jacobian-based
              improves generalization (Sokolic et al. 2017; Gouk et al.
                                              ´                                 equation with JFB is to tune weights by using a precondi-
              2021; Finlay et al. 2018) and adversarial robustness (Cisse       tioned gradient. Let ` : Y × Y → R be a smooth loss func-
              et al. 2017; Anil, Lucas, and Grosse 2019).                       tion, denoted by `(x,y), and consider the training problem
              Theorem 0.1. (BANACH) For any u1 ∈ U, if the sequence
              {uk}is generated via the update relation                                          minEd∼D`(yd,NΘ(d)),                  (11)
                          uk+1 = T (uk; d), for all k ∈ N,           (10)                        Θ
                                    Θ                                           where we abusively write D to also mean a distribution. For
              and if Assumption 0.1 holds, then {uk} converges linearly         clarity of presentation, in the remainder of this section we
              to the unique ﬁxed point u? of TΘ(·;d).
                                        d                                       notationally suppress the dependencies on weights Θ by let-
              Alternative Approaches      In (Bai, Kolter, and Koltun           ting u? denote the ﬁxed point in (7). Unless noted otherwise,
                                                                                      d
              2019; Bai, Koltun, and Kolter 2020) Broyden’s method is           mapping arguments are implicit in this section; in each im-
              used for ﬁnding u?. Broyden’s method is a quasi-Newton            plicit case, this will correspond to entries in (7). We begin
                                 d                                              with standard assumptions enabling us to differentiate N .
              scheme and so at each iteration it updates a stored approxi-                                                              Θ
              mation to the Jacobian Jk and then solves a linear system in      Assumption 0.2. The mappings S        and T    are continu-
              J . Since in this work our goal is to explore truly Jacobian-                                        Θ        Θ
               k                                                                ously differentiable with respect to u and Θ.
              free approaches, we stick to the simpler ﬁxed point iteration
                                                                          6650
