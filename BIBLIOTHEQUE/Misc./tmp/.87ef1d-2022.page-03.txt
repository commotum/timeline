                                                                                 Published as a conference paper at ICLR 2022
                                                                                                                                                                                                                            output predictions                       the           ccaatt        iinn        tthehe       hhaatt         to
                                                                                                                                                                                                                                                                                                  softmax
                                                                                                                                                                                                                                                                                    dloecnseal  aatttteentntiioon +n + F FFFNN
                                                                                                                                                                                                                                  kNN attention
                                                                                                                                             k nearest neighbor lookup.                                                                                                     kNNd &ense loc atatle antttieontn +ion F +FN FFN
                                                                                                                             external memory: cached (key, value) pairs                                                                                                                      local context
                                                                                                                                                                                                                                                                                       … more layers … 
                                                                                                                                                                                                                         Will be added to
                                                                                                                                                                                                                         external memory
                                                                                                                                                                                                                          after the current
                                                                                                                                                                                                                            training step. 
                                                                                                                                                                                                                                                                                    local attention + FFN
                                                                                                                                                                                                                                                                                         embedding layer
                                                                                                                                                                                                                                        input tokens said                          the          ccaatt        iinn        tthehe        hahatt
                                                                                 Figure 2: We extend Transformers with access to (key, value) pairs of previously seen subsequences.
                                                                                 cache. In contrast, we use a very large cache without compression, combined with an approximate
                                                                                 kNNattention mechanism over it.
                                                                                 Sukhbaatar et al. (2019) make the observation that the feed-forward portion of a transformer layer
                                                                                 functions very much like attention if one replaces the ReLU activation with softmax. They implement
                                                                                 a combined attention over both tokens from the input sequence and a learned (and differentiable)
                                                                                “memory”. Lample et al. (2019) exploit this observation to replace the feed-forward layers (FFNs)
                                                                                 with a fast kNN lookup over a much larger “memory”, and achieve large gains in model accuracy
                                                                                 without signiﬁcant computation overhead. (We use kNN lookup to approximate attention to previous
                                                                                 tokens, not to replace the FFN.)
                                                                                 Non-differentiable external memory has been used in different ways by Khandelwal et al. (2020), who
                                                                                 run a pre-trained model over an entire corpus, and construct a large table of (key, token) pairs. They
                                                                                 then use that table to replace the ﬁnal softmax layer for token selection in the model, which results in
                                                                                 signiﬁcant improvements in language modeling. Yogatama et al. (2021) extend this approach by a
                                                                                 gating mechanism and a process to compress the context into keys for retrieval.
                                                                                 There are several works that combine retrieval with transformers. REALM (Guu et al., 2020),
                                                                                 MARGE(Lewisetal.,2020a),RAG(Lewisetal.,2020b),andcompositememoryfordialog(Fan
                                                                                 et al., 2021) retrieve documents from a knowledge base to improve question answering or dialogue.
                                                                                 Theknowledgebaseconsists of text snippets and is static and typically separate from the inputs and
                                                                                 outputs of the models. Instead, we focus on language modeling using a decoder-only model, and
                                                                                 propose a simple model that uniﬁes attention and retrieval.
                                                                                 k-nearest-neighbor lookup is a general-purpose technique that is used for a wide variety of machine
                                                                                 learning and retrieval tasks, and high-performance implementations are available for various architec-
                                                                                 tures (Johnson et al., 2021; Guo et al., 2020). Memory-efﬁcient Transformers (Gupta et al., 2021)
                                                                                 replace dense attention with a kNN lookup to increase speed and reduce memory usage.
                                                                                 3             METHOD
                                                                                 Thearchitecture of our kNN-augmented transformer is shown in Figure 2. The bulk of the model is a
                                                                                 vanilla, decoder-only transformer (Vaswani et al., 2017). The input text is tokenized, and the tokens
                                                                                 are embedded into vector space. The embedding vectors are passed through a series of transformer
                                                                                 layers, each of which does dense self-attention, followed by a feed-forward network (FFN). Since
                                                                                 this is a decoder-only language model, we use a causal attention mask and the token embeddings of
                                                                                 the last layer are used to predict the next token.
                                                                                 Longdocumentsaresplit into subsequences of 512 tokens, and each subsequence is used as the input
                                                                                 for one training step. In contrast to standard practice, we do not shufﬂe the subsequences; instead,
                                                                                 each long document is fed into the transformer sequentially, from beginning to end, as is done with
                                                                                 Transformer-XL (Dai et al., 2019).
                                                                                                                                                                                                                                   3
