                            References
                                                                   ´
                            Peter L. Bartlett, Philip M. Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in
                               linear regression.  Proceedings of the National Academy of Sciences, 117(48):30063–30070,
                               2020. doi: 10.1073/pnas.1907378117. URL https://www.pnas.org/doi/abs/10.1073/
                               pnas.1907378117.
                            Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint.
                               Acta Numerica, 30:87–201, 2021.
                            MikhailBelkin. Fit without fear: remarkable mathematical phenomena of deep learning through the
                               prism of interpolation. Acta Numerica, 30:203–248, 2021.
                                              ´ ˆ
                            Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
                               Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pp.
                               41–48, NewYork,NY,USA,2009.AssociationforComputingMachinery. URLhttps://doi.
                               org/10.1145/1553374.1553380.
                            Franc¸ois Charton.    Learning the greatest common divisor: explaining transformer predictions.
                               In The Twelfth International Conference on Learning Representations, 2024.           URL https:
                               //openreview.net/forum?id=cmcD05NPKa.
                            Elvis Dohmatob, Yunzhen Feng, Pu Yang, Franc¸ois Charton, and Julia Kempe. A tale of tails:
                               Model collapse as a change of scaling laws. In Forty-first International Conference on Machine
                               Learning, 2024. URL https://openreview.net/forum?id=KVvku47shW.
                            Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant.                  What can trans-
                               formers learn in-context?       a case study of simple function classes.           In S. Koyejo,
                               S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neu-
                               ral Information Processing Systems, volume 35, pp. 30583–30598. Curran Associates,
                               Inc., 2022.  URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
                               c529dba08a146ea8d6cf715ae8930cbe-Paper-Conference.pdf.
                            Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International
                               Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=
                               lQdXeXDoWtI.
                            Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
                               arXiv:1412.6980, 2014.
                            Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric J Michaud, Max Tegmark, and Mike Williams. To-
                               wards understanding grokking: An effective theory of representation learning. In Alice H. Oh,
                               Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information
                               Processing Systems, 2022a. URL https://openreview.net/forum?id=6at6rB3IZm.
                            Ziming Liu, Ouail Kitouni, Niklas S Nolte, Eric Michaud, Max Tegmark, and Mike Williams.
                               Towards understanding grokking:        An effective theory of representation learning.           In
                               S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances
                               in Neural Information Processing Systems, volume 35, pp. 34651–34663. Curran Associates,
                               Inc., 2022b. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
                               dfc310e81992d2e4cedc09ac47eff13e-Paper-Conference.pdf.
                            Ziming Liu, Eric J Michaud, and Max Tegmark. Omnigrok: Grokking beyond algorithmic data.
                               In The Eleventh International Conference on Learning Representations, 2023. URL https:
                               //openreview.net/forum?id=zDiHoIWa0q1.
                            David Lopez-Paz. The Invariance Principle. MIT Press, 2025.
                            Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
                               MITPress, 2018.
                            Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksan-
                               dra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language
                               models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL
                               https://openreview.net/forum?id=j5BuTrEj35.
                                                                              5
