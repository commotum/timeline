                             Published as a conference paper at ICLR 2024
                             from a repository’s webpage. We create executable contexts for every version of a repository, as
                             discussed in greater detail in § A.3.
                             A.3    EXECUTION-BASED VALIDATION
                             After filtering through all the PRs from a repository and converting those that satisfy the aforemen-
                             tioned criteria into candidate task instances, the next step is to validate the usability of each task
                             instance via execution. This procedure is broken down into three steps. First, we create executable
                             contexts for each release version of a repository. Next, we check whether the solution δ and tests T
                             can be applied, installed, and run successfully on top of codebase C. Finally, we examine each task
                             instance’s execution log to verify a specific set of behaviors to ensure that the task is usable and fair
                             for model evaluation.
                             Executable Contexts. We choose to create executable contexts per release version after experi-
                             menting with various degrees of granularity with regards to what definition level to define virtual
                             environments for. Defining task instance-specific contexts is most conducive to ensuring end-to-end
                             installation success, but comes at the cost of laborious manual handcrafting. On the other hand, a
                             repository-specific context based on the latest version of a repository is typically too coarse of a
                             definition that is not compatible with older versions’ requirements. We find that release versions are
                             a good proxy for capturing the dependency requirements across a subset of task instances, striking a
                             manageable balance between installation success and manual effort. We manually create each exe-
                             cutable context by examining the codebase of the latest task instance for each version. Based on the
                             source code and documentation typically found in the repository’s README and CONTRIBUTING
                             guides, we find out the Python version, necessary dependencies, and installation command.
                             Validation Engine. The purpose of the validation engine is to verify candidate task instances.
                             Specifically, this step checks first, that the solution δ and tests T can be applied to codebase C,
                             and second, that the codebase can be properly installed and run within the corresponding virtual
                             environment. Todothis,weperformvalidationrepository-by-repository,whereforeachrepository’s
                             set of task instances, we perform the following procedure:
                                   1. Create executable contexts as conda envs. based on latest task instance per version.
                                   2. Group task instances by version.
                                   3. Iterate across each task instances group, where for each task instance, we perform the
                                       following within the corresponding conda env.
                                        (a) Remove any file changes and checkout the task instance’s base commit. This sets
                                            the repository to codebase C.
                                       (b) Run the installation command to instantiate codebase C.
                                        (c) Apply the test patch T to codebase C.
                                       (d) Runthetestingscript, determinedfromtestpatchT,togeneratetestresultlogslog                 .
                                                                                                                                  pre
                                        (e) Apply the solution δ patch to the codebase C with tests T.
                                        (f) Run the testing script from part (d) again to generate test result logs log    .
                                                                                                                       post
                             Thetesting command consists of the testing framework used by the repository (e.g. pytest, tox)
                             with paths specified in T appended. The testing command would run any and all tests that are speci-
                             fied within the contents of each file path. If any of the steps (a) through (f) fails, the candidate task
                             instance is discarded from consideration. With moderate variation across repositories, we observe
                             that this step generally removes half of the candidate task instances.
                             ExaminingValidationLogs. Lastbutnotleast, wecheckthelogslog                   andlog      created by the
                                                                                                       pre         post
                             validation engine for specific properties. First, to guard against arbitrary naming choices, we check
                             log     for ImportError and AttributeError occurrences, which are potentially indicative
                                pre
                             of dependency naming related errors that would trivial and near-impossible to address correctly. To
                             this end, we remove all task instances with such errors in their log      from consideration. Next, we
                                                                                                   pre
                             compare the test results to check that the task instance is non-trivial, indicated by at least one or
                             moretests having a fail status before the solution δ is applied, then a pass status after. To check
                             this, we first define several repository-specific parsers to convert log     and log      into mappings
                                                                                                      pre         post
                             of test ti ∈ T to a status s ∈ [fail,pass]. Given these two data structures, we then check that there
                                                                                18
