                                    2        C. He et al.
                                                                                MHSA
                                                                               MHSA
                                                                             SST, SWFormer, 
                                                                             SST, SWFormer, 
                                                                               DSVT, etc.
                                                                               DSVT, etc.
                                                           (B, M, D)                              (B, M, D)
                                                           (B, M, D)             (a)              (B, M, D)
                                                                                 SLA
                                                                                 SLA
                                                                             ScatterFormer
                                                                             ScatterFormer
                                                             (N, D)                                 (N, D)
                                                                                (ours)
                                                            (N, D)                                  (N, D)
                                                                                (ours)
                                                                                (b)
                                    Fig.1: Illustration of (a) group-based attention and (b) our scattered linear attention
                                    on variable-length sequences. The square in different colors represent voxels in different
                                    windows and the square with a cross represents the padding voxel.
                                    like PointNet++ [31] for feature extraction in continuous space [37]. Some ap-
                                    proaches [5,9,16,18,49,50,52,60] have transformed point clouds into voxel grids,
                                    which are then efÏciently processed by a sparse convolutional neural network
                                    (SpCNN) [49].
                                        Recently, the success of vision transformers [7,21] has motivated a number
                                    of attention-based methods to process indoor point clouds [12,22,24,25,27,57].
                                    Inspired by SwinTransformer [21], various studies [8,14,23,47] have advanced
                                    the application of window-based Transformers in large-scale 3D detection tasks
                                    within outdoor environments, achieving outstanding performance and highlight-
                                    ing their potential as alternatives to SpCNN. However, due to the inherent spar-
                                    sity of point clouds, the number of features grouped by windows can vary sig-
                                    nificantly, hampering the parallelism in attention computation. To resolve this
                                    issue, SST [8] and SWFormer [40] group the voxels within a window into different
                                    batches and manages the attention computation in a hybrid serial-parallel man-
                                    ner. More effectively, DSVT [47] alternatively sort the voxels within a window
                                    fromdifferent axes and partition them into sequences of fixed length, which allow
                                    parallel computation on sparse voxels. Albeit effective, these group-based meth-
                                    ods, as shown in Figure 1(a), require extensive sorting and padding operations,
                                    incurring substantial memory and computational overhead.
                                        In this paper, we delve into the window-based voxel transformer where the
                                    voxels grouped by windows form variable-length sequences {X ∈ Rn1×d,X ∈
                                                                                                             1               2
                                    Rn2×d,...,Xk ∈ Rnk×d}. Their corresponding self-attention matrices {A1 ∈
                                      n ×n             n ×n                n ×n
                                    R 1     1, A ∈ R 2      2,..., A  ∈R k k} occupy irregular memory spaces, mak-
                                                2                   k
                                    ing it a challenge to perform attention in parallel for the voxels of the entire
                                    scene. Recent efforts on linear attention [1,13,17,48] have emerged a promising
                                    alternative to traditional attention. By approximating softmax operation with
                                    the kernel function, i.e., ϕ(Q)·ϕ(K)T ≈ softmax(QKT), we can change the com-
                                    putation order from (Q·KT)·V to Q·(KT ·V). This not only results in linear
                                    complexity but also yields a compressed hidden state S ∈ Rd×d that is invariant
                                    to the sequence length. Another attractive property of linear attention is that it
