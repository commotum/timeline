                                            The overall input it is concatenated with previous read rt−1 and passed to the RNN controller as
                                            input along with the previous controller state h                                   .  The controller outputs its next state h and a
                                                                          0                                              t−1                                                                    t
                                            controller output o , from which we obtain the push and pop scalars d and u and the value vector
                                                                          t                                                                                  t          t
                                            vt, which are passed to the stack, as well as the network output ot:
                                                                   d =sigmoid(W o0 +b )                                                u =sigmoid(W o0 +b )
                                                                     t                          d t         d                            t                          u t         u
                                                                                              0                                                                        0
                                                                   v =tanh(W o +b )                                                          o =tanh(W o +b )
                                                                      t                   v t          v                                       t                   o t          o
                                            where W and W are vector-to-scalar projection matrices, and b and b are their scalar biases;
                                                           d             u                                                                             d           u
                                            W and W are vector-to-vector projections, and b and b are their vector biases, all randomly
                                                v             o                                                                 d            u
                                            intialised and then tuned during training. Along with the previous stack state (Vt−1, st−1), the stack
                                            operations d and u and the value v are passed to the neural stack to obtain the next read r and
                                                                t           t                           t                                                                                          t
                                            next stack state (V , s ), which are packed into a tuple with the controller state h to form the next
                                                                         t     t                                                                                               t
                                            state H of the overall recurrent layer. The output vector o serves as the overall output of the
                                                        t                                                                                      t
                                            recurrent layer. The structure described here can be adapted to control a neural Queue instead of a
                                            stack by substituting one memory module for the other.
                                            The only additional trainable parameters in either conﬁguration, relative to a non-enhanced RNN,
                                            are the projections for the input concatenated with the previous read into the RNN controller, and the
                                            projections from the controller output into the various Stack/Queue inputs, described above. In the
                                            case of a DeQue, both the top read rtop and bottom read rbot must be preserved in the overall state.
                                            TheyarebothconcatenatedwiththeinputtoformtheinputtotheRNNcontroller. Theoutputofthe
                                            controller must have additional projections to output push/pop operations and values for the bottom
                                            of the DeQue. This roughly doubles the number of additional tunable parameters “wrapping” the
                                            RNNcontroller, compared to the Stack/Queue case.
                                            4      Experiments
                                            In every experiment, integer-encoded source and target sequence pairs are presented to the candidate
                                            modelasabatchofsinglejoint sequences. The joint sequence starts with a start-of-sequence (SOS)
                                            symbol, and ends with an end-of-sequence (EOS) symbol, with a separator symbol separating the
                                            source and target sequences. Integer-encoded symbols are converted to 64-dimensional embeddings
                                            via an embedding matrix, which is randomly initialised and tuned during training. Separate word-
                                            to-index mappings are used for source and target vocabularies. Separate embedding matrices are
                                            used to encode input and output (predicted) embeddings.
                                            4.1      Synthetic Transduction Tasks
                                            Theaimofeachofthefollowingtasksistoreadaninputsequence,andgenerateastargetsequencea
                                            transformed version of the source sequence, followed by an EOS symbol. Source sequences are ran-
                                            domlygeneratedfromavocabularyof128meaninglesssymbols. Thelengthofeachtrainingsource
                                            sequence is uniformly sampled from unif{8,64}, and each symbol in the sequence is drawn with
                                            replacement from a uniform distribution over the source vocabulary (ignoring SOS, and separator).
                                            Adeterministic task-speciﬁc transformation, described for each task below, is applied to the source
                                            sequence to yield the target sequence. As the training sequences are entirely determined by the
                                            source sequence, there are close to 10135 training sequences for each task, and training examples
                                            are sampled from this space due to the random generation of source sequences. The following steps
                                            are followed before each training and test sequence are presented to the models, the SOS symbol
                                            (hsi) is prepended to the source sequence, which is concatenated with a separator symbol (|||) and
                                            the target sequences, to which the EOS symbol (h/si) is appended.
                                            Sequence Copying                    The source sequence is copied to form the target sequence. Sequences have
                                            the form:
                                                                                                      hsia ...a |||a ...a h/si
                                                                                                             1         k      1         k
                                            Sequence Reversal                   The source sequence is deterministically reversed to produce the target se-
                                            quence. Sequences have the form:
                                                                                                  hsia a ...a |||a ...a a h/si
                                                                                                         1 2           k      k         2 1
                                                                                                                          6
