                                                                                                                                                                            High Rank
                                     BLIP-2
                                InstructBLIP
                         InstructBLIP Vicuna
                                     LLaVA
                                 MiniGPT-4
                                  VPGTrans
                            MultiModal-GPT
                                      Otter
                             OpenFlamingo
                          LLaMA-Adapter V2
                                       GVT
                   Model        mPLUG-Owl
                                  Kosmos-2
                              Qwen-VL-Chat
                                  LLaVA-1.5
                         IDEFICS-9B-Instruct
                     InternLM-Xcomposer-VL
                                 VideoChat
                             Video-ChatGPT
                                     Valley
                                       Emu
                                  NExt-GPT
                                                                                                                                                                             Low Rank
                                                   InstanceIdentityInstancettributesInstanceLocationInstanceCountingSpatialRelationsInstanceVisualTextVisualReferringSciencenowledgeVisualSpottingActionActionAnalysisNextImageCreation
                                                         A                      Reasoning         Chart         Emotion            Video     Prediction              Prediction
                                              Scene                         InteractionRecognitionCelebrityRecognitionLandmarkRecognitionExperssionKRecognitionMathematicsDifferenceMemeGlobalRecognitionProcedureIn-ContextCaptioningInterleavedImage-TextGenerationText-Image
                                                Understanding                                      Understanding               ComprehensionUnderstandingUnderstandingText-to-Image
                                                                                              Evaluation Dimension
                 Figure 5. Illustration of each modelâ€™s performance across different evaluation dimensions, where darker colors represent higher ranks.
                 Grayindicates that the model has not yet reached the capability level required for evaluating that dimension.
                 capability level required for evaluating that dimension. The                          and generation simultaneously.                    Although NExt-GPT
                 championMLLMInternLM-Xcomposer-VLachievescom-                                         reaches the capability level L , which can generate both
                                                                                                                                                3
                 petitive results in a large number of evaluation dimensions                           texts and images, it shows poor performance in capabil-
                 of capability level L and L . Although NExt-GPT reaches                               ity L for multimodal comprehension. Equipping MLLMs
                                            1         2                                                       1
                 the capability level L3, it performs poorly in multiple eval-                         with image generation ability without compromising their
                 uation dimensions at levels L1 and L2.                                                inherent text output performance remains to be addressed.
                 4.3. Observations                                                                     5. Conclusion
                 Throughthecomprehensionandobjectiveevaluationofvar-                                   In this work, we introduce SEED-Bench, a large-scale
                 ious MLLMsindifferent capability levels of SEED-Bench,                                benchmark for evaluating Multimodal Large Language
                 wehaveuncoveredinsights that can inform future work.                                  Models (MLLMs) in terms of hierarchical capabilities, in-
                 Existing MLLMs have yet to reach the ceiling level of                                 cluding the generation of both texts and images. SEED-
                 capability L . Even the top-ranked MLLM achieves only
                                  1                                                                    Benchconsistsof24Kmultiple-choicequestionswithaccu-
                 a 60% averaged accuracy in capability L , which evaluates
                                                                       1                               rate human annotations, which cover 27 evaluation dimen-
                 the comprehension of multimodal inputs in a fixed format,                             sions. We conduct a thorough evaluation of 22 prominent
                 i.e., images or multiple images (videos) and then texts.                              open-source MLLMs, analyzing and comparing their per-
                 The comprehension of Interleaved Image-Text data is                                   formances to provide insights for future research. We plan
                 moredifficult. The majority of MLLMs achieve worse re-                                to launch and maintain a leaderboard, offering a platform
                 sults on part 2, which consists of multiple-choice questions                          for the community to assess model performance.
                 with interleaved image-text inputs, than on L1 with fixed-                                Ackoweledge           The      work      is    partially     supported
                 form image and text as inputs.                                                        by the Young Scientists Fund of the National Nat-
                 OnlyasmallnumberofMLLMscanreachthecapabil-                                            ural    Science Foundation of China under grant No.
                 ity L . Only two open-source MLLMs possess the ability                                62106154, by the Natural Science Foundation of Guang-
                        3
                 to generate images, besides the inherent ability of LLMs to                           dong Province, China (General Program) under grant
                 output texts. A universal MLLM that unifies the generation                            No.2022A1515011524, and by Shenzhen Science and
                 of images and texts is currently underexplored.                                       Technology         Program        JCYJ20220818103001002                   and
                 It is challenging to address multimodal comprehension                                 ZDSYS20211021111415025
                                                                                                  13306
