                           5.3  Mini-Pacman
                           In Mini Pacman with viewport the RMC achieved approximately 100 points more than an LSTM
                           (677 vs. 550), and when trained with the full observation the RMC nearly doubled the performance
                           of an LSTM (1159 vs. 598, ﬁgure 10).
                           5.4  LanguageModeling
                           For all three language modeling tasks we observe lower perplexity when using the relational memory
                           core, with a drop of 1.4 − 5.4 perplexity over the best published results. Although small, this
                           constitutes a 5 − 12% relative improvement and appears to be consistent across tasks of varying
                           size and style. For WikiText-103, we see this can be compared to LSTM architectures [5, 32],
                           convolutional models [42] and hybrid recurrent-convolutional models [43].
                           Themodellearns with a slightly better data efﬁciency than an LSTM (appendix ﬁgure 11). The RMC
                           scored highly when the number of context words provided during evaluation were relatively few,
                           comparedtoanLSTMwhichproﬁtedmuchmorefromalargercontext(supplementaryﬁgure12).
                           This could be because RMC better captures short-term relations, and hence only needs a relatively
                           small context for accurate modeling. Inspecting the perplexity broken down by word frequency in
                           supplementary table 3, we see the RMC improved the modeling of frequent words, and this is where
                           the drop in overall perplexity is obtained.
                           6   Discussion
                           Anumberofotherapproacheshaveshownsuccessinmodelingsequentialinformation by using a
                           growing buffer of previous states [21, 22]. These models better capture long-distance interactions,
                           since their computations are not biased by temporally local proximity. However, there are serious
                           scaling issues for these models when the number of timesteps is large, or even unbounded, such as
                           in online reinforcement learning (e.g., in the real world). Thus, some decisions need to be made
                           regarding the size of the past-embedding buffer that should be stored, whether it should be a rolling
                           window, how computations should be cached and propagated across time, etc. These considerations
                           make it difﬁcult to directly compare these approaches in these online settings. Nonetheless, we
                           believe that a blend of purely recurrent approaches with those that scale with time could be a fruitful
                           pursuit: perhaps the model accumulates memories losslessly for some chunk of time, then learns to
                           compress it in a recurrent core before moving onto processing a subsequent chunk.
                           Weproposed intuitions for the mechanisms that may better equip a model for complex relational
                           reasoning. Namely, by explicitly allowing memories to interact either with each other, with the
                           input, or both via MHDPA, we demonstrated improved performance on tasks demanding relational
                           reasoning across time. We would like to emphasize, however, that while these intuitions guided our
                           design of the model, and while the analysis of the model in the Nth farthest task aligned with our
                           intuitions, we cannot necessarily make any concrete claims as to the causal inﬂuence of our design
                           choices on the model’s capacity for relational reasoning, or as to the computations taking place within
                           the model and how they may map to traditional approaches for thinking about relational reasoning.
                           Thus, we consider our results primarily as evidence of improved function – if a model can better
                           solve tasks that require relational reasoning, then it must have an increased capacity for relational
                           reasoning, even if we do not precisely know why it may have this increased capacity. In this light the
                           RMCmaybeusefullyviewedfrommultiplevantages, and these vantages may offer ideas for further
                           improvements.
                           Our model has multiple mechanisms for forming and allowing for interactions between memory
                           vectors: slicing the memory matrix row-wise into slots, and column-wise into heads. Each has its
                           own advantages (computations on slots share parameters, while having more heads and a larger
                           memorysize takes advantage of more parameters). We don’t yet understand the interplay, but we
                           note some empirical ﬁndings. First, in the the Nth farthest task a model with a single memory slot
                           performed better when it had more attention heads, though in all cases it performed worse than a
                           model with many memory slots. Second, in language modeling, our model used a single memory
                           slot. The reasons for choosing a single memory here were mainly due to the need for a large number
                           of parameters for LM in general (hence the large size for the single memory slot), and the inability to
                           quickly run a model with both a large number of parameters and multiple memory slots. Thus, we do
                                                                          8
