--- Page 1 ---
End-To-EndMemoryNetworksSainbayarSukhbaatarDept.ofComputerScienceCourantInstitute,NewYorkUniversitysainbar@cs.nyu.eduArthurSzlamJasonWestonRobFergusFacebookAIResearchNewYorkfaszlam,jase,robfergusg@fb.comAbstractWeintroduceaneuralnetworkwitharecurrentattentionmodeloverapossiblylargeexternalmemory.ThearchitectureisaformofMemoryNetwork[23]butunlikethemodelinthatwork,itistrainedend-to-end,andhencerequiressignicantlylesssupervisionduringtraining,makingitmoregenerallyapplicableinrealisticsettings.ItcanalsobeseenasanextensionofRNNsearch[2]tothecasewheremultiplecomputationalsteps(hops)areperformedperoutputsymbol.Theexibilityofthemodelallowsustoapplyittotasksasdiverseas(synthetic)questionanswering[22]andtolanguagemodeling.FortheformerourapproachiscompetitivewithMemoryNetworks,butwithlesssupervision.Forthelatter,onthePennTreeBankandText8datasetsourapproachdemonstratescomparableperformancetoRNNsandLSTMs.Inbothcasesweshowthatthekeyconceptofmultiplecomputationalhopsyieldsimprovedresults.1IntroductionTwograndchallengesinarticialintelligenceresearchhavebeentobuildmodelsthatcanmakemultiplecomputationalstepsintheserviceofansweringaquestionorcompletingatask,andmodelsthatcandescribelongtermdependenciesinsequentialdata.Recentlytherehasbeenaresurgenceinmodelsofcomputationusingexplicitstorageandanotionofattention[23,8,2];manipulatingsuchastorageoffersanapproachtobothofthesechallenges.In[23,8,2],thestorageisendowedwithacontinuousrepresentation;readsfromandwritestothestorage,aswellasotherprocessingsteps,aremodeledbytheactionsofneuralnetworks.Inthiswork,wepresentanovelrecurrentneuralnetwork(RNN)architecturewheretherecurrencereadsfromapossiblylargeexternalmemorymultipletimesbeforeoutputtingasymbol.OurmodelcanbeconsideredacontinuousformoftheMemoryNetworkimplementedin[23].Themodelinthatworkwasnoteasytotrainviabackpropagation,andrequiredsupervisionateachlayerofthenetwork.Thecontinuityofthemodelwepresentheremeansthatitcanbetrainedend-to-endfrominput-outputpairs,andsoisapplicabletomoretasks,i.e.taskswheresuchsupervisionisnotavail-able,suchasinlanguagemodelingorrealisticallysupervisedquestionansweringtasks.OurmodelcanalsobeseenasaversionofRNNsearch[2]withmultiplecomputationalsteps(whichwetermÂ“hopsÂ”)peroutputsymbol.Wewillshowexperimentallythatthemultiplehopsoverthelong-termmemoryarecrucialtogoodperformanceofourmodelonthesetasks,andthattrainingthememoryrepresentationcanbeintegratedinascalablemannerintoourend-to-endneuralnetworkmodel.2ApproachOurmodeltakesadiscretesetofinputsx1;:::;xnthataretobestoredinthememory,aqueryq,andoutputsananswera.Eachofthexi,q,andacontainssymbolscomingfromadictionarywithVwords.Themodelwritesallxtothememoryuptoaxedbuffersize,andthenndsacontinuousrepresentationforthexandq.Thecontinuousrepresentationisthenprocessedviamultiplehopstooutputa.Thisallowsbackpropagationoftheerrorsignalthroughmultiplememoryaccessesbacktotheinputduringtraining.1

--- Page 2 ---
2.1SingleLayerWestartbydescribingourmodelinthesinglelayercase,whichimplementsasinglememoryhopoperation.Wethenshowitcanbestackedtogivemultiplehopsinmemory.Inputmemoryrepresentation:Supposewearegivenaninputsetx1;::;xitobestoredinmemory.Theentiresetoffxigareconvertedintomemoryvectorsfmigofdimensiondcomputedbyembeddingeachxiinacontinuousspace,inthesimplestcase,usinganembeddingmatrixA(ofsizedV).Thequeryqisalsoembedded(again,inthesimplestcaseviaanotherembeddingmatrixBwiththesamedimensionsasA)toobtainaninternalstateu.Intheembeddingspace,wecomputethematchbetweenuandeachmemorymibytakingtheinnerproductfollowedbyasoftmax:pi=Softmax(uTmi):(1)whereSoftmax(zi)=ezi=Pjezj.Denedinthiswaypisaprobabilityvectorovertheinputs.Outputmemoryrepresentation:Eachxihasacorrespondingoutputvectorci(giveninthesimplestcasebyanotherembeddingmatrixC).Theresponsevectorfromthememoryoisthenasumoverthetransformedinputsci,weightedbytheprobabilityvectorfromtheinput:o=Xipici:(2)Becausethefunctionfrominputtooutputissmooth,wecaneasilycomputegradientsandback-propagatethroughit.Otherrecentlyproposedformsofmemoryorattentiontakethisapproach,notablyBahdanauetal.[2]andGravesetal.[8],seealso[9].Generatingthenalprediction:Inthesinglelayercase,thesumoftheoutputvectoroandtheinputembeddinguisthenpassedthroughanalweightmatrixW(ofsizeVd)andasoftmaxtoproducethepredictedlabel:^a=Softmax(W(o+u))(3)TheoverallmodelisshowninFig.1(a).Duringtraining,allthreeembeddingmatricesA,BandC,aswellasWarejointlylearnedbyminimizingastandardcross-entropylossbetween^aandthetruelabela.Trainingisperformedusingstochasticgradientdescent(seeSection4.2formoredetails).Figure1:(a):Asinglelayerversionofourmodel.(b):Athreelayerversionofourmodel.Inpractice,wecanconstrainseveraloftheembeddingmatricestobethesame(seeSection2.2).2.2MultipleLayersWenowextendourmodeltohandleKhopoperations.Thememorylayersarestackedinthefollowingway:Theinputtolayersabovetherstisthesumoftheoutputokandtheinputukfromlayerk(differentwaystocombineokandukareproposedlater):uk+1=uk+ok:(4)2

--- Page 3 ---
EachlayerhasitsownembeddingmatricesAk;Ck,usedtoembedtheinputsfxig.However,asdiscussedbelow,theyareconstrainedtoeasetrainingandreducethenumberofparameters.Atthetopofthenetwork,theinputtoWalsocombinestheinputandtheoutputofthetopmemorylayer:^a=Softmax(WuK+1)=Softmax(W(oK+uK)).Weexploretwotypesofweighttyingwithinthemodel:1.Adjacent:theoutputembeddingforonelayeristheinputembeddingfortheoneabove,i.e.Ak+1=Ck.Wealsoconstrain(a)theanswerpredictionmatrixtobethesameasthenaloutputembedding,i.eWT=CK,and(b)thequestionembeddingtomatchtheinputembeddingoftherstlayer,i.e.B=A1.2.Layer-wise(RNN-like):theinputandoutputembeddingsarethesameacrossdifferentlayers,i.e.A1=A2=:::=AKandC1=C2=:::=CK.WehavefounditusefultoaddalinearmappingHtotheupdateofubetweenhops;thatis,uk+1=Huk+ok.Thismappingislearntalongwiththerestoftheparametersandusedthroughoutourexperimentsforlayer-wiseweighttying.Athree-layerversionofourmemorymodelisshowninFig.1(b).Overall,itissimilartotheMemoryNetworkmodelin[23],exceptthatthehardmaxoperationswithineachlayerhavebeenreplacedwithacontinuousweightingfromthesoftmax.Notethatifweusethelayer-wiseweighttyingscheme,ourmodelcanbecastasatraditionalRNNwherewedividetheoutputsoftheRNNintointernalandexternaloutputs.Emittinganinternaloutputcorrespondstoconsideringamemory,andemittinganexternaloutputcorrespondstopredictingalabel.FromtheRNNpointofview,uinFig.1(b)andEqn.4isahiddenstate,andthemodelgeneratesaninternaloutputp(attentionweightsinFig.1(a))usingA.ThemodeltheningestspusingC,updatesthehiddenstate,andsoon1.Here,unlikeastandardRNN,weexplicitlyconditionontheoutputsstoredinmemoryduringtheKhops,andwekeeptheseoutputssoft,ratherthansamplingthem.ThusourmodelmakesseveralcomputationalstepsbeforeproducinganoutputmeanttobeseenbytheÂ“outsideworldÂ”.3RelatedWorkAnumberofrecenteffortshaveexploredwaystocapturelong-termstructurewithinsequencesusingRNNsorLSTM-basedmodels[4,7,12,15,10,1].Thememoryinthesemodelsisthestateofthenetwork,whichislatentandinherentlyunstableoverlongtimescales.TheLSTM-basedmodelsaddressthisthroughlocalmemorycellswhichlockinthenetworkstatefromthepast.Inpractice,theperformancegainsovercarefullytrainedRNNsaremodest(seeMikolovetal.[15]).Ourmodeldiffersfromtheseinthatitusesaglobalmemory,withsharedreadandwritefunctions.However,withlayer-wiseweighttyingourmodelcanbeviewedasaformofRNNwhichonlyproducesanoutputafteraxednumberoftimesteps(correspondingtothenumberofhops),withtheintermediarystepsinvolvingmemoryinput/outputoperationsthatupdatetheinternalstate.SomeoftheveryearlyworkonneuralnetworksbySteinbuchandPiske[19]andTaylor[21]con-sideredamemorythatperformednearest-neighboroperationsonstoredinputvectorsandthentparametricmodelstotheretrievedsets.Thishassimilaritiestoasinglelayerversionofourmodel.Subsequentworkinthe1990'sexploredothertypesofmemory[18,5,16].Forexample,Dasetal.[5]andMozeretal.[16]introducedanexplicitstackwithpushandpopoperationswhichhasbeenrevisitedrecentlyby[11]inthecontextofanRNNmodel.CloselyrelatedtoourmodelistheNeuralTuringMachineofGravesetal.[8],whichalsousesacontinuousmemoryrepresentation.TheNTMmemoryusesbothcontentandaddress-basedaccess,unlikeourswhichonlyexplicitlyallowstheformer,althoughthetemporalfeaturesthatwewillintroduceinSection4.1allowakindofaddress-basedaccess.However,inpartbecausewealwayswriteeachmemorysequentially,ourmodelissomewhatsimpler,notrequiringoperationslikesharpening.Furthermore,weapplyourmemorymodeltotextualreasoningtasks,whichqualitativelydifferfromthemoreabstractoperationsofsortingandrecalltackledbytheNTM.1Notethatinthisview,theterminologyofinputandoutputfromFig.1isipped-whenviewedasatraditionalRNNwiththisspecialconditioningofoutputs,AbecomespartoftheoutputembeddingoftheRNNandCbecomestheinputembedding.3

--- Page 4 ---
OurmodelisalsorelatedtoBahdanauetal.[2].Inthatwork,abidirectionalRNNbasedencoderandgatedRNNbaseddecoderwereusedformachinetranslation.Thedecoderusesanattentionmodelthatndswhichhiddenstatesfromtheencodingaremostusefulforoutputtingthenexttranslatedword;theattentionmodelusesasmallneuralnetworkthattakesasinputaconcatenationofthecurrenthiddenstateofthedecoderandeachoftheencodershiddenstates.AsimilarattentionmodelisalsousedinXuetal.[24]forgeneratingimagecaptions.OurÂ“memoryÂ”isanalogoustotheirattentionmechanism,although[2]isonlyoverasinglesentenceratherthanmany,asinourcase.Furthermore,ourmodelmakesseveralhopsonthememorybeforemakinganoutput;wewillseebelowthatthisisimportantforgoodperformance.Therearealsodifferencesinthearchitectureofthesmallnetworkusedtoscorethememoriescomparedtoourscoringapproach;weuseasimplelinearlayer,whereastheyuseamoresophisticatedgatedarchitecture.Wewillapplyourmodeltolanguagemodeling,anextensivelystudiedtask.Goodman[6]showedsimplebuteffectiveapproacheswhichcombinen-gramswithacache.Bengioetal.[3]ignitedinterestinusingneuralnetworkbasedmodelsforthetask,withRNNs[14]andLSTMs[10,20]showingclearperformancegainsovertraditionalmethods.Indeed,thecurrentstate-of-the-artisheldbyvariantsofthesemodels,forexampleverylargeLSTMswithDropout[25]orRNNswithdiagonalconstraintsontheweightmatrix[15].Withappropriateweighttying,ourmodelcanberegardedasamodiedformofRNN,wheretherecurrenceisindexedbymemorylookupstothewordsequenceratherthanindexedbythesequenceitself.4SyntheticQuestionandAnsweringExperimentsWeperformexperimentsonthesyntheticQAtasksdenedin[22](usingversion1.1ofthedataset).AgivenQAtaskconsistsofasetofstatements,followedbyaquestionwhoseansweristypicallyasingleword(inafewtasks,answersareasetofwords).Theanswerisavailabletothemodelattrainingtime,butmustbepredictedattesttime.Thereareatotalof20differenttypesoftasksthatprobedifferentformsofreasoninganddeduction.Herearesamplesofthreeofthetasks:Samwalksintothekitchen.Brianisalion.Maryjourneyedtotheden.Sampicksupanapple.Juliusisalion.Marywentbacktothekitchen.Samwalksintothebedroom.Juliusiswhite.Johnjourneyedtothebedroom.Samdropstheapple.Bernhardisgreen.Marydiscardedthemilk.Q:Whereistheapple?Q:WhatcolorisBrian?Q:Wherewasthemilkbeforetheden?A.BedroomA.WhiteA.HallwayNotethatforeachquestion,onlysomesubsetofthestatementscontaininformationneededfortheanswer,andtheothersareessentiallyirrelevantdistractors(e.g.therstsentenceintherstexample).IntheMemoryNetworksofWestonetal.[22],thissupportingsubsetwasexplicitlyindicatedtothemodelduringtrainingandthekeydifferencebetweenthatworkandthisoneisthatthisinformationisnolongerprovided.Hence,themodelmustdeduceforitselfattrainingandtesttimewhichsentencesarerelevantandwhicharenot.Formally,foroneofthe20QAtasks,wearegivenexampleproblems,eachhavingasetofIsentencesfxigwhereI320;aquestionsentenceqandanswera.Letthejthwordofsentenceibexij,representedbyaone-hotvectoroflengthV(wherethevocabularyisofsizeV=177,reectingthesimplisticnatureoftheQAlanguage).Thesamerepresentationisusedforthequestionqandanswera.Twoversionsofthedataareused,onethathas1000trainingproblemspertaskandasecondlargeronewith10,000pertask.4.1ModelDetailsUnlessotherwisestated,allexperimentsusedaK=3hopsmodelwiththeadjacentweightsharingscheme.Foralltasksthatoutputlists(i.e.theanswersaremultiplewords),wetakeeachpossiblecombinationofpossibleoutputsandrecordthemasaseparateanswervocabularyword.SentenceRepresentation:Inourexperimentsweexploretwodifferentrepresentationsforthesentences.Therstisthebag-of-words(BoW)representationthattakesthesentencexi=fxi1;xi2;:::;xing,embedseachwordandsumstheresultingvectors:e.gmi=PjAxijandci=PjCxij.Theinputvectorurepresentingthequestionisalsoembeddedasabagofwords:u=PjBqj.Thishasthedrawbackthatitcannotcapturetheorderofthewordsinthesentence,whichisimportantforsometasks.Wethereforeproposeasecondrepresentationthatencodesthepositionofwordswithinthesentence.Thistakestheform:mi=PjljAxij,whereisanelement-wisemultiplication.ljisa4

--- Page 5 ---
columnvectorwiththestructurelkj=(1 j=J) (k=d)(1 2j=J)(assuming1-basedindexing),withJbeingthenumberofwordsinthesentence,anddisthedimensionoftheembedding.Thissentencerepresentation,whichwecallpositionencoding(PE),meansthattheorderofthewordsnowaffectsmi.Thesamerepresentationisusedforquestions,memoryinputsandmemoryoutputs.TemporalEncoding:ManyoftheQAtasksrequiresomenotionoftemporalcontext,i.e.intherstexampleofSection2,themodelneedstounderstandthatSamisinthebedroomafterheisinthekitchen.Toenableourmodeltoaddressthem,wemodifythememoryvectorsothatmi=PjAxij+TA(i),whereTA(i)istheithrowofaspecialmatrixTAthatencodestemporalinformation.TheoutputembeddingisaugmentedinthesamewaywithamatrixTc(e.g.ci=PjCxij+TC(i)).BothTAandTCarelearnedduringtraining.TheyarealsosubjecttothesamesharingconstraintsasAandC.Notethatsentencesareindexedinreverseorder,reectingtheirrelativedistancefromthequestionsothatx1isthelastsentenceofthestory.Learningtimeinvariancebyinjectingrandomnoise:wehavefoundithelpfultoaddÂ“dummyÂ”memoriestoregularizeTA.Thatis,attrainingtimewecanrandomlyadd10%ofemptymemoriestothestories.Werefertothisapproachasrandomnoise(RN).4.2TrainingDetails10%ofthebAbItrainingsetwasheld-outtoformavalidationset,whichwasusedtoselecttheoptimalmodelarchitectureandhyperparameters.Ourmodelsweretrainedusingalearningrateof=0:01,withannealsevery25epochsby=2until100epochswerereached.Nomomentumorweightdecaywasused.TheweightswereinitializedrandomlyfromaGaussiandistributionwithzeromeanand=0:1.Whentrainedonalltaskssimultaneouslywith1ktrainingsamples(10ktrainingsamples),60epochs(20epochs)wereusedwithlearningrateannealsof=2every15epochs(5epochs).Alltrainingusesabatchsizeof32(butcostisnotaveragedoverabatch),andgradientswithan`2normlargerthan40aredividedbyascalartohavenorm40.Insomeofourexperiments,weexploredcommencingtrainingwiththesoftmaxineachmemorylayerremoved,makingthemodelentirelylinearexceptforthenalsoftmaxforanswerprediction.Whenthevalidationlossstoppeddecreasing,thesoftmaxlayerswerere-insertedandtrainingrecommenced.Werefertothisaslinearstart(LS)training.InLStraining,theinitiallearningrateissetto=0:005.Thecapacityofmemoryisrestrictedtothemostrecent50sentences.Sincethenumberofsentencesandthenumberofwordspersentencevariedbetweenproblems,anullsymbolwasusedtopadthemalltoaxedsize.Theembeddingofthenullsymbolwasconstrainedtobezero.Onsometasks,weobservedalargevarianceintheperformanceofourmodel(i.e.sometimesfailingbadly,othertimesnot,dependingontheinitialization).Toremedythis,werepeatedeachtraining10timeswithdifferentrandominitializations,andpickedtheonewiththelowesttrainingerror.4.3BaselinesWecompareourapproach2(abbreviatedtoMemN2N)toarangeofalternatemodels:MemNN:ThestronglysupervisedAM+NG+NLMemoryNetworksapproach,proposedin[22].Thisisthebestreportedapproachinthatpaper.Itusesamaxoperation(ratherthansoftmax)ateachlayerwhichistraineddirectlywithsupportingfacts(strongsupervision).Itemploysn-grammodeling,nonlinearlayersandanadaptivenumberofhopsperquery.MemNN-WSH:AweaklysupervisedheuristicversionofMemNNwherethesupportingsen-tencelabelsarenotusedintraining.Sinceweareunabletobackpropagatethroughthemaxoperationsineachlayer,weenforcethattherstmemoryhopshouldshareatleastonewordwiththequestion,andthatthesecondmemoryhopshouldshareatleastonewordwiththersthopandatleastonewordwiththeanswer.Allthosememoriesthatconformarecalledvalidmemories,andthegoalduringtrainingistorankthemhigherthaninvalidmemoriesusingthesamerankingcriteriaasduringstronglysupervisedtraining.LSTM:AstandardLSTMmodel,trainedusingquestion/answerpairsonly(i.e.alsoweaklysupervised).Formoredetail,see[22].2MemN2Nsourcecodeisavailableathttps://github.com/facebook/MemNN.5

--- Page 6 ---
4.4ResultsWereportavarietyofdesignchoices:(i)BoWvsPositionEncoding(PE)sentencerepresentation;(ii)trainingonall20tasksindependentlyvsjointlytraining(jointtrainingusedanembeddingdimensionofd=50,whileindependenttrainingusedd=20);(iii)twophasetraining:linearstart(LS)wheresoftmaxesareremovedinitiallyvstrainingwithsoftmaxesfromthestart;(iv)varyingmemoryhopsfrom1to3.Theresultsacrossall20tasksaregiveninTable1forthe1ktrainingset,alongwiththemeanperformancefor10ktrainingset3.Theyshowanumberofinterestingpoints:ThebestMemN2Nmodelsarereasonablyclosetothesupervisedmodels(e.g.1k:6.7%forMemNNvs12.6%forMemN2Nwithpositionencoding+linearstart+randomnoise,jointlytrainedand10k:3.2%forMemNNvs4.2%forMemN2Nwithpositionencoding+linearstart+randomnoise+non-linearity4,althoughthesupervisedmodelsarestillsuperior.Allvariantsofourproposedmodelcomfortablybeattheweaklysupervisedbaselinemethods.Thepositionencoding(PE)representationimprovesoverbag-of-words(BoW),asdemonstratedbyclearimprovementsontasks4,5,15and18,wherewordorderingisparticularlyimportant.Thelinearstart(LS)totrainingseemstohelpavoidlocalminima.Seetask16inTable1,wherePEalonegets53.6%error,whileusingLSreducesitto1.6%.Jitteringthetimeindexwithrandomemptymemories(RN)asdescribedinSection4.1givesasmallbutconsistentboostinperformance,especiallyforthesmaller1ktrainingset.Jointtrainingonalltaskshelps.Importantly,morecomputationalhopsgiveimprovedperformance.Wegiveexamplesofthehopsperformed(viathevaluesofeq.(1))oversomeillustrativeexamplesinFig.2andinAppendixB.BaselineMemN2NStronglyPE1hop2hops3hopsPEPELSSupervisedLSTMMemNNPELSPELSPELSPELSLSRNLWTaskMemNN[22][22]WSHBoWPELSRNjointjointjointjointjoint1:1supportingfact0.050.00.10.60.10.20.00.80.00.10.00.12:2supportingfacts0.080.042.817.621.612.88.362.015.614.011.418.83:3supportingfacts0.080.076.471.064.258.840.376.931.633.121.931.74:2argumentrelations0.039.040.332.03.811.62.822.82.25.713.417.55:3argumentrelations2.030.016.318.314.115.713.111.013.414.814.412.96:yes/noquestions0.052.051.08.77.98.77.67.22.33.32.82.07:counting15.051.036.123.521.620.317.315.925.417.918.310.18:lists/sets9.055.037.811.412.612.710.013.211.710.19.36.19:simplenegation0.036.035.921.123.317.013.25.12.03.11.91.510:indeniteknowledge2.056.068.722.817.418.615.110.65.06.66.52.611:basiccoreference0.038.030.04.14.30.00.98.41.20.90.33.312:conjunction0.026.010.10.30.30.10.20.40.00.30.10.013:compoundcoreference0.06.019.710.59.90.30.46.30.21.40.20.514:timereasoning1.073.018.31.31.82.01.736.98.18.26.92.015:basicdeduction0.079.064.824.30.00.00.046.40.50.00.01.816:basicinduction0.077.050.552.052.11.61.347.451.33.52.751.017:positionalreasoning35.049.050.945.450.149.051.044.441.244.540.442.618:sizereasoning5.048.051.348.113.610.111.19.610.39.29.49.219:pathnding64.092.0100.089.787.485.682.890.789.990.288.090.620:agent'smotivation0.09.03.60.10.00.00.00.00.10.00.00.2Meanerror(%)6.751.340.225.120.316.313.925.815.613.312.415.2Failedtasks(err.>5%)42018151312111711111110On10ktrainingdataMeanerror(%)3.236.439.215.49.47.26.624.510.97.97.511.0Failedtasks(err.>5%)216179644167666Table1:Testerrorrates(%)onthe20QAtasksformodelsusing1ktrainingexamples(meantesterrorsfor10ktrainingexamplesareshownatthebottom).Key:BoW=bag-of-wordsrepresentation;PE=positionencodingrepresentation;LS=linearstarttraining;RN=randominjectionoftimeindexnoise;LW=RNN-stylelayer-wiseweighttying(ifnotstated,adjacentweighttyingisused);joint=jointtrainingonalltasks(asopposedtoper-tasktraining).5LanguageModelingExperimentsThegoalinlanguagemodelingistopredictthenextwordinatextsequencegiventhepreviouswordsx.Wenowexplainhowourmodelcaneasilybeappliedtothistask.3Moredetailedresultsforthe10ktrainingsetcanbefoundinAppendixA.4Following[17]wefoundaddingmorenon-linearitysolvestasks17and19,seeAppendixA.6

--- Page 7 ---
Figure2:ExamplepredictionsontheQAtasksof[22].Weshowthelabeledsupportingfacts(support)fromthedatasetwhichMemN2Ndoesnotuseduringtraining,andtheprobabilitiespofeachhopusedbythemodelduringinference.MemN2Nsuccessfullylearnstofocusonthecorrectsupportingsentences.PennTreebankText8#of#ofmemoryValid.Test#of#ofmemoryValid.TestModelhiddenhopssizeperp.perp.hiddenhopssizeperp.perp.RNN[15]300--133129500---184LSTM[15]100--120115500--122154SCRN[15]100--120115500---161MemN2N1502100128121500210015218715031001291225003100142178150410012712050041001291621505100127118500510012315415061001221155006100124155150710012011450071001181471506251251185006251311631506501211145006501321661506751221145006751261581506100122115500610012415515061251201125006125125157150615012111450061501231541507200118111-----Table2:TheperplexityonthetestsetsofPennTreebankandText8corpora.Notethatincreasingthenumberofmemoryhopsimprovesperformance.Figure3:Averageactivationweightofmemorypositionsduring6memoryhops.Whitecolorindicateswherethemodelisattendingduringthekthhop.Forclarity,eachrowisnormalizedtohavemaximumvalueof1.Amodelistrainedon(left)PennTreebankand(right)Text8dataset.Wenowoperateonwordlevel,asopposedtothesentencelevel.ThusthepreviousNwordsinthesequence(includingthecurrent)areembeddedintomemoryseparately.Eachmemorycellholdsonlyasingleword,sothereisnoneedfortheBoWorlinearmappingrepresentationsusedintheQAtasks.WeemploythetemporalembeddingapproachofSection4.1.Sincethereisnolongeranyquestion,qinFig.1isxedtoaconstantvector0.1(withoutembedding).Theoutputsoftmaxpredictswhichwordinthevocabulary(ofsizeV)isnextinthesequence.Across-entropylossisusedtotrainmodelbybackpropagatingtheerrorthroughmultiplememorylayers,inthesamemannerastheQAtasks.Toaidtraining,weapplyReLUoperationstohalfoftheunitsineachlayer.Weuselayer-wise(RNN-like)weightsharing,i.e.thequeryweightsofeachlayerarethesame;theoutputweightsofeachlayerarethesame.AsnotedinSection2.2,thismakesourarchitecturecloselyrelatedtoanRNNwhichistraditionallyusedforlanguage7

--- Page 8 ---
modelingtasks;howeverheretheÂ“sequenceÂ”overwhichthenetworkisrecurrentisnotinthetext,butinthememoryhops.Furthermore,theweighttyingrestrictsthenumberofparametersinthemodel,helpinggeneralizationforthedeepermodelswhichwendtobeeffectiveforthistask.Weusetwodifferentdatasets:PennTreeBank[13]:Thisconsistsof929k/73k/82ktrain/validation/testwords,distributedoveravocabularyof10kwords.Thesamepreprocessingas[25]wasused.Text8[15]:Thisisaapre-processedversionoftherst100Mmillioncharacters,dumpedfromWikipedia.Thisissplitinto93.3M/5.7M/1Mcharactertrain/validation/testsets.Allwordoccurringlessthan5timesarereplacedwiththe<UNK>token,resultinginavocabularysizeof44k.5.1TrainingDetailsThetrainingprocedureweuseisthesameastheQAtasks,exceptforthefollowing.Foreachmini-batchupdate,the`2normofthewholegradientofallparametersismeasured5andiflargerthanL=50,thenitisscaleddowntohavenormL.Thiswascrucialforgoodperformance.Weusethelearningrateannealingschedulefrom[15],namely,ifthevalidationcosthasnotdecreasedafteroneepoch,thenthelearningrateisscaleddownbyafactor1.5.Trainingterminateswhenthelearningratedropsbelow10 5,i.e.after50epochsorso.WeightsareinitializedusingN(0;0:05)andbatchsizeissetto128.OnthePenntreedataset,werepeateachtraining10timeswithdifferentrandominitializationsandpicktheonewithsmallestvalidationcost.However,wehavedoneonlyasingletrainingrunonText8datasetduetolimitedtimeconstraints.5.2ResultsTable2comparesourmodeltoRNN,LSTMandStructurallyConstrainedRecurrentNets(SCRN)[15]baselinesonthetwobenchmarkdatasets.Notethatthebaselinearchitecturesweretunedin[15]togiveoptimalperplexity6.OurMemN2Napproachachieveslowerperplexityonbothdatasets(111vs115forRNN/SCRNonPennand147vs154forLSTMonText8).NotethatMemN2Nhas1.5xmoreparametersthanRNNswiththesamenumberofhiddenunits,whileLSTMhas4xmoreparameters.WealsovarythenumberofhopsandmemorysizeofourMemN2N,showingthecontributionofbothtoperformance;noteinparticularthatincreasingthenumberofhopshelps.InFig.3,weshowhowMemN2Noperatesonmemorywithmultiplehops.Itshowstheaverageweightoftheactivationofeachmemorypositionoverthetestset.Wecanseethatsomehopsconcentrateonlyonrecentwords,whileotherhopshavemorebroadattentionoverallmemorylocations,whichisconsistentwiththeideathatsuccesfullanguagemodelsconsistofasmoothedn-grammodelandacache[15].Interestingly,itseemsthatthosetwotypesofhopstendtoalternate.AlsonotethatunlikeatraditionalRNN,thecachedoesnotdecayexponentially:ithasroughlythesameaverageactivationacrosstheentirememory.Thismaybethesourceoftheobservedimprovementinlanguagemodeling.6ConclusionsandFutureWorkInthisworkweshowedthataneuralnetworkwithanexplicitmemoryandarecurrentattentionmechanismforreadingthememorycanbesuccessfullytrainedviabackpropagationondiversetasksfromquestionansweringtolanguagemodeling.ComparedtotheMemoryNetworkimplementationof[23]thereisnosupervisionofsupportingfactsandsoourmodelcanbeusedinawiderrangeofsettings.Ourmodelapproachesthesameperformanceofthatmodel,andissignicantlybetterthanotherbaselineswiththesamelevelofsupervision.Onlanguagemodelingtasks,itslightlyoutperformstunedRNNsandLSTMsofcomparablecomplexity.Onbothtaskswecanseethatincreasingthenumberofmemoryhopsimprovesperformance.However,thereisstillmuchtodo.Ourmodelisstillunabletoexactlymatchtheperformanceofthememorynetworkstrainedwithstrongsupervision,andbothfailonseveralofthe1kQAtasks.Furthermore,smoothlookupsmaynotscalewelltothecasewherealargermemoryisrequired.Forthesesettings,weplantoexploremultiscalenotionsofattentionorhashing,asproposedin[23].AcknowledgmentsTheauthorswouldliketothankArmandJoulin,TomasMikolov,AntoineBordesandSumitChopraforusefulcommentsandvaluablediscussions,andalsotheFAIRInfrastructureteamfortheirhelpandsupport.5IntheQAtasks,thegradientofeachweightmatrixismeasuredseparately.6Theytunedthehyper-parametersonPennTreebankandusedthemonText8withoutadditionaltuning,exceptforthenumberofhiddenunits.See[15]formoredetail.8

--- Page 9 ---
References[1]C.G.AtkesonandS.Schaal.Memory-basedneuralnetworksforrobotlearning.Neurocom-puting,9:243Â–269,1995.[2]D.Bahdanau,K.Cho,andY.Bengio.Neuralmachinetranslationbyjointlylearningtoalignandtranslate.InInternationalConferenceonLearningRepresentations(ICLR),2015.[3]Y.Bengio,R.Ducharme,P.Vincent,andC.Janvin.Aneuralprobabilisticlanguagemodel.J.Mach.Learn.Res.,3:1137Â–1155,Mar.2003.[4]J.Chung,CÂ¸.GÂ¨ulcÂ¸ehre,K.Cho,andY.Bengio.Empiricalevaluationofgatedrecurrentneuralnetworksonsequencemodeling.arXivpreprint:1412.3555,2014.[5]S.Das,C.L.Giles,andG.-Z.Sun.Learningcontext-freegrammars:Capabilitiesandlimitationsofarecurrentneuralnetworkwithanexternalstackmemory.InInProceedingsofTheFourteenthAnnualConferenceofCognitiveScienceSociety,1992.[6]J.Goodman.Abitofprogressinlanguagemodeling.CoRR,cs.CL/0108005,2001.[7]A.Graves.Generatingsequenceswithrecurrentneuralnetworks.arXivpreprint:1308.0850,2013.[8]A.Graves,G.Wayne,andI.Danihelka.Neuralturingmachines.arXivpreprint:1410.5401,2014.[9]K.Gregor,I.Danihelka,A.Graves,andD.Wierstra.DRAW:Arecurrentneuralnetworkforimagegeneration.CoRR,abs/1502.04623,2015.[10]S.HochreiterandJ.Schmidhuber.Longshort-termmemory.Neuralcomputation,9(8):1735Â–1780,1997.[11]A.JoulinandT.Mikolov.Inferringalgorithmicpatternswithstack-augmentedrecurrentnets.NIPS,2015.[12]J.KoutnÂ´k,K.Greff,F.J.Gomez,andJ.Schmidhuber.AclockworkRNN.InICML,2014.[13]M.P.Marcus,M.A.Marcinkiewicz,andB.Santorini.Buildingalargeannotatedcorpusofenglish:ThePennTreebank.Comput.Linguist.,19(2):313Â–330,June1993.[14]T.Mikolov.Statisticallanguagemodelsbasedonneuralnetworks.Ph.D.thesis,BrnoUniversityofTechnology,2012.[15]T.Mikolov,A.Joulin,S.Chopra,M.Mathieu,andM.Ranzato.Learninglongermemoryinrecurrentneuralnetworks.arXivpreprint:1412.7753,2014.[16]M.C.MozerandS.Das.Aconnectionistsymbolmanipulatorthatdiscoversthestructureofcontext-freelanguages.NIPS,pages863Â–863,1993.[17]B.Peng,Z.Lu,H.Li,andK.Wong.TowardsNeuralNetwork-basedReasoning.ArXivpreprint:1508.05508,2015.[18]J.Pollack.Theinductionofdynamicalrecognizers.MachineLearning,7(2-3):227Â–252,1991.[19]K.SteinbuchandU.Piske.Learningmatricesandtheirapplications.IEEETransactionsonElectronicComputers,12:846Â–862,1963.[20]M.Sundermeyer,R.SchlÂ¨uter,andH.Ney.LSTMneuralnetworksforlanguagemodeling.InInterspeech,pages194Â–197,2012.[21]W.K.Taylor.Patternrecognitionbymeansofautomaticanalogueapparatus.ProceedingsofTheInstitutionofElectricalEngineers,106:198Â–209,1959.[22]J.Weston,A.Bordes,S.Chopra,andT.Mikolov.TowardsAI-completequestionanswering:Asetofprerequisitetoytasks.arXivpreprint:1502.05698,2015.[23]J.Weston,S.Chopra,andA.Bordes.Memorynetworks.InInternationalConferenceonLearningRepresentations(ICLR),2015.[24]K.Xu,J.Ba,R.Kiros,K.Cho,A.Courville,R.Salakhutdinov,R.Zemel,andY.Bengio.Show,AttendandTell:NeuralImageCaptionGenerationwithVisualAttention.ArXivpreprint:1502.03044,2015.[25]W.Zaremba,I.Sutskever,andO.Vinyals.Recurrentneuralnetworkregularization.arXivpreprintarXiv:1409.2329,2014.9

--- Page 10 ---
AppendixAResultson10kQAdatasetBaselineMemN2NStronglyPEPELS1hop2hops3hopsPEPELSSupervisedMemNNPELSLWPELSPELSPELSLSRNLWTaskMemNNLSTMWSHBoWPELSRNRNjointjointjointjointjoint1:1supportingfact0.00.00.10.00.00.00.00.00.00.00.00.00.02:2supportingfacts0.081.939.60.60.40.50.30.362.01.32.31.00.83:3supportingfacts0.083.179.517.812.615.09.32.180.015.814.06.818.34:2argumentrelations0.00.236.631.80.00.00.00.021.40.00.00.00.05:3argumentrelations0.31.221.114.20.80.60.80.88.77.27.56.10.86:yes/noquestions0.051.849.90.10.20.10.00.16.10.70.20.10.17:counting3.324.935.110.75.73.23.72.014.810.56.16.68.48:lists/sets1.034.142.71.42.42.20.80.98.94.74.02.71.49:simplenegation0.020.236.41.81.32.00.80.33.70.40.00.00.210:indeniteknowledge0.030.176.01.91.73.32.40.010.30.60.40.50.011:basiccoreference0.010.325.30.00.00.00.00.18.30.00.00.00.412:conjunction0.023.40.00.00.00.00.00.00.00.00.00.10.013:compoundcoreference0.06.112.30.00.10.00.00.05.60.00.00.00.014:timereasoning0.081.08.70.00.20.00.00.130.90.20.20.01.715:basicdeduction0.078.768.812.50.00.00.00.042.60.00.00.20.016:basicinduction0.051.950.950.948.60.10.451.847.346.40.40.249.217:positionalreasoning24.650.151.147.440.341.140.718,640.039.741.741.840.018:sizereasoning2.16.845.841.37.48.66.75.39.210.18.68.08.419:pathnding31.990.3100.075.466.666.766.52.391.080.873.375.789.520:agent'smotivation0.02.14.10.00.00.00.00.00.00.00.00.00.0Meanerror(%)3.236.439.215.49.47.26.64.224.510.97.97.511.0Failedtasks(err.>5%)2161796443167666Table3:Testerrorrates(%)onthe20bAbIQAtasksformodelsusing10ktrainingexamples.Key:BoW=bag-of-wordsrepresentation;PE=positionencodingrepresentation;LS=linearstarttraining;RN=randominjectionoftimeindexnoise;LW=RNN-stylelayer-wiseweighttying(ifnotstated,adjacentweighttyingisused);joint=jointtrainingonalltasks(asopposedtoper-tasktraining);=thisisalargermodelwithnon-linearity(embeddingdimensionisd=100andReLUappliedtotheinternalstateaftereachhop.Thiswasinspiredby[17]andcrucialforgettingbetterperformanceontasks17and19).10

--- Page 11 ---
AppendixBVisualizationofattentionweightsinQAproblemsFigure4:ExamplesofattentionweightsduringdifferentmemoryhopsforthebAbitasks.ThemodelisPE+LS+RNwith3memoryhopsthatistrainedseparatelyoneachtaskwith10ktrainingdata.Thesupportcolumnshowswhichsentencesarenecessaryforansweringquestions.Althoughthisinformationisnotused,themodelsuccesfullylearnstofocusonthecorrectsupportsentencesonmostofthetasks.Thehopcolumnsshowwherethemodelputmoreweight(indicatedbyvaluesandbluecolor)duringitsthreehops.Themistakesmadebythemodelarehighlightedbyredcolor.11

