leads to remarkably strong performance on many reasoning benchmarks, no-
tably without requiring any additional finetuning. Wei et al. (2022) and Nye
et al. (2021) demonstrate the importance of explicitly performing intermediate
reasoning steps via a chain of thought or a scratchpad in order to solve tasks
that require multi-step reasoning. Kojima et al. (2022) show that models are
able to perform this behavior zero-shot, conditioned only on a simple prompt.

8 Conclusion

We have shown that process supervision can be used to train much more reliable
reward models than outcome supervision in the domain of mathematical rea-
soning. We have also shown that active learning can be used to lower the cost of
human data collection by surfacing only the most valuable model completions
for human feedback. We release PRM800K, the full dataset of human feedback
used to train our state-ofthe-art reward model, with the hope that removing
this significant barrier to entry will catalyze related research on the alignment of
large language models. We believe that process supervision is currently under-
explored, and we are excited for future work to more deeply investigate the
extent to which these methods generalize.

Acknowledgements

We thank Joshua Achiam, Mark Chen, Jonathan Gordon, Dan Hendrycks,
Lukasz Kaiser, Oleg Murk, Ben Sokolowsky, Francis Song, and Jonathan Uesato
for valuable feedback and thoughtful discussions; Giambattista Parascandolo
and Daniel Selsam for their contributions to the MathMix dataset; Jonathan
Ward for contributing to the data collection interface; Wojciech Zaremba for en-
couraging us to scale up data collection; Peter Hoeschele and Aris Kostantinidis
for supporting our data collection; the research acceleration and supercomput-
ing teams at OpenAI for providing infrastructure support; and the team at Scale
and the many data-labelers who created PRM800K.

References

A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones,
N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a
laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.

S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar,
P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general
intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712,
2023.
