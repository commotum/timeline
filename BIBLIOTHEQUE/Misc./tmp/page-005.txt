                                   Table 2: Results of removing different components in attention. Sim denotes the cosine similarity
                                   between original and new positional vectors, and PPL denotes perplexity on RedPajama.
                                                          original       w/o value        w/o positional vector    w/o positional basis    w/o semantic vector
                                                              -        1∼4      4∼256       1∼4        4∼256         1∼4       4∼256        1∼4        4∼256
                                     TL-NoPE      Sim        1       -0.1558    0.9797    -0.1810      0.9086      -0.1817     0.9046      0.9985      0.9514
                                                  PPL       7.56      >1000      8.97      >1000        13.36       >1000       10.23       8.20       10.55
                                     TL-RoPE      Sim        1       0.8394     0.9902     0.8505      0.9874       0.1711     0.9944      0.9970      0.9596
                                                  PPL       6.06      11.98      6.44      12.28        6.24        >1000        6.11       6.63        6.85
                                                        Figure 3: Logarithmic attention maps of TL-RoPE, and TL-NoPE.
                                   3.2.2    FormationofPositional Vectors with Window Attention
                                   Unlike full attention, LLMs employing window attention restrict each token to attend only to tokens
                                   within a window size. Previous work has shown that the maximum theoretical receptive field (TRF)
                                   in window attention is equal to the product of the window size W and the layer index l [18].
                                   Toanalyze how positional vectors change across layers, we compute the number of distinct positional
                                   vectors within the maximum TRF. Notably, those tokens beyond the maximum TRF share the same
                                   positional vectors due to translation invariance [18]. Specifically, we first randomly select a positional
                                   vector outside the maximum TRF and then compute the cosine similarity between positional vectors
                                   within the maximum TRF and the selected vector. We consider the positional vector with a similarity
                                   score lower than a threshold (i.e., 0.99) as distinct. Figure 2 presents the number of distinct positional
                                   vectors and TRF at each layer. We can see that after the first layer, only initial tokens show distinct
                                   positional vectors, further verifying the findings in Section 3.2.1. As the layer increases, more tokens
                                   display different positional information and the number of distinct positional vectors increases by
                                   512 (a window size W) with each additional layer. The reason is that due to the constraint of window
                                   attention, each token at the preceding layer can only influence tokens within the window size at the
                                   next layer. As being distinct indicates the formation of position information, similar positional
                                   information flow from initial tokens to subsequent tokens also occurs for window attention, but
                                   gradually propagating across both windows and layers.
                                   3.2.3    Effect of Positional Vectors on Attention
                                   After discussing the formation of positional vectors, we explore their impact on the attention module,
                                   mainly focusing on the attention scores. We first extract queries and keys from each head in all layers,
                                   and then compute the average attention scores in the following four settings, including (1) original:
                                   the original model, (2) w/o semantic vector: removing the semantic vectors of keys and queries, (3)
                                   w/o positional vector: removing the positional vectors of keys and queries, (4) w/o positional basis:
                                   removing the positional basis of keys and queries. Figure 3 presents the logarithmic attention scores
                                   for the first 100 tokens in the first head and fifth layer (similar results in many other heads and layers).
                                                                                                 5
