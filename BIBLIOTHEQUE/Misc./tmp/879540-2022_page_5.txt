                                                                                 í µãŒµ!          í µãŒµ!                                                    R(2Hwâˆ’1)Ã—(2Wwâˆ’1)Ã—(2Tâˆ’1)Ã—(2mâˆ’1) andthevaluesinB are
                                                                                                                                                                             Ë†
                                                 Blank grid                                                                                          taken from B. Specifically, the relative position along the
                                                 Non-blank grid                                                                                      spatial dimension lies in the range of [âˆ’Hw + 1,Hw âˆ’ 1]
                                                 Valid window of non-blank area                                                                      and [âˆ’Ww +1,Ww âˆ’1]. The temporal dimension range
                                                 Invalid window of blank area      í µãŒµ
                                                                                     !                                                               and cross-sensor dimension range are respectively [âˆ’T +
                                                                                                                                                     1,T âˆ’1]and[âˆ’m+1,mâˆ’1]. Thusthelearnableposition
                                                             í µãŒµ                                                                                      encoder contributes to locating each token with a position
                                        í µãŒµ                     !/#                                                                                   embedding, which takes the 4D relative relationship of in-
                                          !/$                    í µãŒµ                                                                                  formation into account.
                                          í µãŒµ                       !/#
                                            !/$                                               í µãŒµ                                                     3.3. Sensor-Time Data Augmentation
                                                                                                 !
                                                                                                                                                           GT-Paste [35] currently serves as a popular augmenta-
                                                                                                                                                     tion techniqueforsingle-framepointclouddetection,which
                         Figure 4. An illustration of pyramid context structure based on                                                             pastes virtual 3D objects in the forms of point cloud and
                         sparse windows with NM = 3. We sparsify the attention win-                                                                  its corresponding ground-truth box from other scenes to the
                         dowonBEVmapsaccordingtowhetherthe partitioned windows                                                                       current training frame. This operation largely improves the
                         include only blank areas. Besides, we adapt the map scale in a                                                              performancebyalleviatingtheclassimbalanceproblemand
                         pyramidstructure, where the down-sampled map provides a larger                                                              accelerating convergence. However, the naive GT-paste is
                         receptive field.                                                                                                            not applicable in our work due to the destruction of data
                                                                                                                                                     consistency across sensors and time. To address this issue,
                         Pyramid Context. Another issue with the window-based                                                                        we propose a sensor-time data augmentation scheme that
                         attention is that the limited local regions may not be suffi-                                                               extends the vanilla augmentation pipeline to preserve both
                         cient to cover dynamic objects with large motions in adja-                                                                  cross-sensor and cross-time consistency.
                         cent frames. An intuitive solution is to enlarge the size of                                                                      AsthenaiveGT-pasteschemerandomlypicksupthevir-
                         localwindows. However,thiswouldlargelyincreasethepa-                                                                        tual LiDARobjectpatternO â€² fromitsoriginalsourcescene
                         rameter volume of attention QKT, yielding heavy compu-                                                                                                                            t
                                                                                                                                                     Sâ€² and then paste into current training scene S , it treats
                                                                                                                                                        t                                                                                           t
                         tational load. To enlarge the receptive field with light com-                                                               the selected object as independent individuals.                                                   By con-
                         putation, we consider to resize BEV maps instead, where                                                                     trast, we extend those candidates as a temporal consistent
                         smaller resolution corresponds to larger receptive regions                                                                  sequence to maintain cross-time consistency for sequential
                         with fixed window size as demonstrated in Figure 4. In par-                                                                 input.          Concretely, with the training sequence of scenes
                         ticular, we downsample the original BEV map {M} with                                                                        {S             }                            , we expand the virtual LiDAR pat-
                                                                                                                                                           tâˆ’âˆ†t âˆ†t=0,1,...,Tâˆ’1
                         the factor j, and then apply the aforementioned window-                                                                     tern candidate as a sequence {O â€²                                          } by searching from
                                                                                                                   jâˆˆ{ 1 }NM                                                                                         t âˆ’âˆ†t
                         based attention on the packed input {{M} }                                                       2iâˆ’1 i=1                   the past scenes {Stâ€²âˆ’âˆ†t}. Notably, it is necessary to main-
                                                                                                               j                                     tain the relative motion relationship within sequence, which
                         with shared parameters, where N                                      is the number of scales.
                                                                                        M                                                            serves as part of supervisory signal for training. Since the
                         Consequently, the attention computation is adapted to:
                                                                                                                                                     ego-motion between adjacent frames are different in source
                                                                                                  Q KT                                               scenes and training scenes, we first transfer the virtual pat-
                                                                                                      j     j
                                           A (Q ,K ,V ) = softmax( âˆš                                            )V ,
                                               j       j       j      j                                              j                               terns in history source scene S â€²                                     into the original source
                                                         X                                               d                           (3)                                                                       t âˆ’âˆ†t
                                                                                                                                                     scene S â€² with homogeneous transformation K â€²                                                                  â€² ,
                                           F =                   Up(MLP(A ))+F ,                                                                                     t                                                                           (t âˆ’âˆ†t)â†’t
                                              out                                       j              in                                            and then transform them into corresponding history train-
                                                            j                                                                                        ing scene S                         with transformation K                                          .     Thus
                                                                                                                                                                              tâˆ’âˆ†t                                                    tâ†’(tâˆ’âˆ†t)
                         where the upsample operation Up(Â·) is used to recover the                                                                   the pasted sequential patterns preserve its original motion
                         original resolution. With linear computing complexity, the                                                                  states. To further maintain the cross-sensor consistency, we
                         proposed pyramid context is scalable.                                                                                       paste the corresponding image patches {IO â€²                                                    } into the
                                                                                                                                                                                                                                          t âˆ’âˆ†t
                                                                                                                                                     training image frames {I                                    }. Following [32], we cal-
                         4D Positional Encoding.                               As vanilla self-attention is un-                                                                                        tâˆ’âˆ†t
                         ordered, it is crucial to encode the locations of tokens in                                                                 culate the occlusion perspective to filter out the occluded
                         the input sequence. A common practice of positional en-                                                                     point. Leveraging the above designs, we propose a general-
                         coding is to supplement the feature vector with positional                                                                  use augmentation scheme that is feasible to any sequential
                         priors. In this work, the candidate tokens in the input se-                                                                 cross-sensor training data input.
                         quences are across both sensors and time, which requires                                                                    4. Experiments
                         4-Dimensional positional encoding. Thus we introduce a
                         4D relative position encoder B âˆˆ R(Hw)2Ã—(Ww)2Ã—T2Ã—m2,                                                                              Weevaluate the proposed method on both the nuScenes
                                                                                                                                Ë†
                         where the positional matrix is parameterized as B                                                             âˆˆ             dataset and Waymodatasets,andconductextensiveablation
                                                                                                                                               17176
