                          Table 3: L denotes the number of blocks and D denotes the hidden dimension (#channels). For all
                          ConvandMBConvblocks,wealwaysusethekernelsize3. ForallTransformerblocks, we set the
                          size of each attention head to 32, following [22]. The expansion rate for the inverted bottleneck is
                          always 4 and the expansion (shrink) rate for the SE is always 0.25.
                           Stages        Size   CoAtNet-0    CoAtNet-1      CoAtNet-2       CoAtNet-3      CoAtNet-4
                                          1
                           S0-Conv        /2   L=2 D=64      L=2  D=64     L=2   D=128    L=2   D=192     L=2   D=192
                                          1
                           S1-MbConv      /4   L=2 D=96      L=2  D=96     L=2   D=128    L=2   D=192     L=2   D=192
                                          1
                           S2-MBConv      /8   L=3 D=192     L=6  D=192    L=6   D=256    L=6   D=384     L=12 D=384
                                         1
                           S3-TFM         /16  L=5 D=384     L=14 D=384    L=14 D=512     L=14 D=768      L=28 D=768
                                   Rel
                                         1
                           S4-TFM         /32  L=2 D=768     L=2  D=768    L=2   D=1024   L=2   D=1536    L=2   D=1536
                                   Rel
                          resolutions for 30 epochs and obtain the corresponding evaluation accuracy. One exception is the
                          ImageNet-1Kperformanceatresolution224,whichcanbedirectlyobtainedattheendofpre-training.
                          Note that similar to other models utilizing Transformer blocks, directly evaluating models pre-trained
                          on ImageNet-1K at a larger resolution without ﬁnetuning usually leads to performance drop. Hence,
                          ﬁnetuning is always employed whenever input resolution changes.
                          Data Augmentation & Regularization.      In this work, we only consider two widely used data
                          augmentations, namely RandAugment[45]andMixUp[46],andthreecommontechniques,including
                          stochastic depth [47], label smoothing [48] and weight decay [49], to regularize the model. Intuitively,
                          the speciﬁc hyper-parameters of the augmentation and regularization methods depend on model size
                          and data scale, where strong regularization is usually applied for larger models and smaller dataset.
                          Under the general principle, a complication under the current paradigm is how to adjust the regular-
                          ization for pre-training and ﬁnetuning as data size can change. Speciﬁcally, we have an interesting
                          observation that if a certain type of augmentation is entirely disabled during pre-training, simply
                          turning it on during ﬁne-tuning would most likely harm the performance rather than improving.
                          Weconjecture this could be related to data distribution shift. As a result, for certain runs of the
                          proposed model, we deliberately apply RandAugment and stochastic depth of a small degree when
                          pre-training on the two larger datasets, ImageNet21-K and JFT. Although such regularization can
                          harm the pre-training metrics, this allows more versatile regularization and augmentation during
                          ﬁnetuning, leading to improved down-stream performances.
                          4.2  MainResults
                          Figure 2: Accuracy-to-FLOPs scaling curve un-   Figure 3: Accuracy-to-Params scaling curve un-
                          der ImageNet-1K only setting at 224x224.        der ImageNet-21K ) ImageNet-1K setting.
                          ImageNet-1K Theexperiment results with only the ImageNet-1K dataset are shown in Table 4.
                          Under similar conditions, the proposed CoAtNet models not only outperform ViT variants, but also
                          match the best convolution-only architectures, i.e., EfﬁcientNet-V2 and NFNets. Additionally, we
                          also visualize the all results at resolution 224x224 in Fig. 2. As we can see, CoAtNet scales much
                          better than previous model with attention modules.
                                                                        7
