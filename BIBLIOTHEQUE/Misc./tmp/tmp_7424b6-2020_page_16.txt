                                          Setting          Winograd  Winogrande (XL)
                                                                a             b
                                          Fine-tuned SOTA    90.1         84.6
                                          GPT-3Zero-Shot     88.3*         70.2
                                          GPT-3One-Shot      89.7*         73.2
                                          GPT-3Few-Shot      88.6*         77.7
               Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section
               4 for details on potential contamination of the Winograd test set. a[SBBC19] b[LYN+20]
               Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales.
               Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B
               is competitive with a ﬁne-tuned RoBERTA-large.
               each translation task improves performance by over 7 BLEU and nears competitive performance with prior work.
               GPT-3inthefull few-shot setting further improves another 4 BLEU resulting in similar average performance to prior
               unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the
               three input languages studied, GPT-3 signiﬁcantly outperforms prior unsupervised NMT work when translating into
               English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at
               over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE
               tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En,
               few shot GPT-3 outperforms the best supervised result we could ﬁnd but due to our unfamiliarity with the literature and
               the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art.
               For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of
               unsupervised pretraining, supervised ﬁnetuning on 608K labeled examples, and backtranslation [LHCG19b].
               Finally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of
               improvement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three
               settings is shown in Appendix H.
               3.4  Winograd-Style Tasks
               TheWinogradSchemasChallenge[LDM12]isaclassicaltaskinNLPthatinvolvesdetermining which word a pronoun
               refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently ﬁne-tuned
               language models have achieved near-human performance on the original Winograd dataset, but more difﬁcult versions
                                                              16
