                            (a) Two State Problem                         (b) Single Chain Problem                        (c) Double Chain Problem
                        (Bagnell and Schneider 2003)                      (Furmston & Barbar 2010)                        (Furmston & Barbar 2010)
                Figure 1: Three different methods are compared on three toy examples. The vanilla policy gradients are signiﬁcantly outper-
                formed due to their slow convergence as already discussed by Bagnell and Schneider (2003) for the Two State Problem. Policy
                iteration based on Relative Entropy Policy Search (REPS) exhibited the best performance.
                ﬁrst several example problems from the literature and, sub-               inspired by Furmston & Barbar (2010). See Fig. 1 (b) for
                sequently, on the Mountain Car standard evaluation. Sub-                  moreinformation.
                sequently, we show ﬁrst steps towards a robot application
                currently under development.                                              DoubleChainProblem. TheDoubleChainProblemcon-
                ExampleProblems                                                           catinates two single chain problems into one big one were
                Wecompare our approach both to ‘vanilla’ policy gradient                  state 1 is shared. As before, returning to state 1 will yield
                methods and natural policy gradients (Bagnell and Schnei-                 a reward 2 and requires taking action 2. If in state 1, action
                der 2003; Peters and Schaal 2008) using several toy prob-                 2 will lead to state 6 and also yield a reward of 2. An ac-
                lems. As such, we have chosen (i) the Two-State Problem                   tion 1 yields a reward 5 in state 9 and a reward 10 in state
                (Bagnell and Schneider 2003), (ii) the Single Chain Prob-                 5. In all other states, action 1 will yield 0 reward. Note
                lem(FurmstonandBarber2010),and(iii)theDoubleChain                         that this problem differs from (Furmston and Barber 2010)
                Problem (Furmston and Barber 2010). In all of these prob-                 signiﬁcantly. We have made it purposefully harder for any
                lems, the optimal policy can be observed straightforwardly                incrementalmethodinordertohighlightheadvantageofthe
                by a human observer but they pose a major challenge for                   presented approach. See Fig. 1 (c) for more information.
                ‘vanilla’ policy gradient approaches.                                        Weusedunitfeatures for all methods. For the two policy
                TwoStateProblem. Thetwostateproblemhastwostates                           gradient approaches a Gibbs policy was employed (Sutton et
                andtwoactions. If it takes the action that has the same num-              al. 2000; Bagnell and Schneider 2003). On all three prob-
                ber as its current state, it will remain in this state. If it takes       lems, we let our policy run until the state distribution has
                the action that has the others state’s number, it will trans-             converged to the stationary distribution. For small problems
                fer to that one. State transfers are punished while staying               like the presented ones, this usually takes less than 200 steps.
                in ones’ state will give an immidiate reward that equals the              Subsequently, we update the policy and resample. We take
                number of the state. This problem is a derivate of the one                highly optimized vanilla policy gradients with minimum-
                in (Bagnell and Schneider 2003). The optimal policy can be                variance baselines (Peters and Schaal 2008) and the Natu-
                observed straightforwardly: always take action 2. See Fig. 1              ral Actor-Critic with unit basis functions as additional func-
                (a) for more information.                                                 tion approximation (Peters and Schaal 2008). Instead of
                                                                                          a small ﬁxed learning rate, we use an additional momen-
                Single Chain Problem.        TheSingleChainProblemcanbe                   tum term in order to improve the performance. We tuned
                seen as an extension of the Two State Problem. Here, the                  all meta-parameters of the gradient methods to maximum
                actor may return to state 1 at any point in time by taking                performance. We start with the same random initial poli-
                action 2 and receiving a reward of 2. However, if he keeps                cies for all algorithms and average over 150 learning runs.
                using action 1 all the time, he will not receive any rewards              Nevertheless, similar as in (Bagnell and Schneider 2003;
                until he reaches state 5 where he obtains the reward of 10                Peters and Schaal 2008), we directly observe that natu-
                and may remain in state 5. The version presented here was                 ral gradient outperforms the vanilla policy gradient. Fur-
                                                                                    1610
