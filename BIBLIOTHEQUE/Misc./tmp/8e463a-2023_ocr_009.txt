et al., 2015), GLUE(Wang et al., 2018), Super-
GLUE (Wang et al., 2020), LAMBADA (Paperno
et al., 2016), etc., is essential for setting evaluation
criteria and tracking model performance. While
some traditional benchmarks like SQUAD and
SNLI assess single-task abilities, GLUE and Su-
perGLUE evaluate general language models across
various NLP tasks. More recently, with rapid de-
velopment of LLMs, new benchmarks have been
proposed to evaluate on more complex tasks such
as college entrance exams, law school admission
tests and so on (Hendrycks et al., 2021; Guo et al.,
2023; Zhong et al., 2023). However, these bench-
marks are still based on existing knowledge, which
is abundant in the training data of LLM.

Some knowledge benchmarks (Onoe et al., 2021;
Mallen et al., 2023; Arodi et al., 2023) evaluate
model’s ability of knowledge integration while they
either only evaluate on small models which are very
different with LLMs (training data, ability, etc.) or
simply use knowledge that already exists. There-
fore, benchmarks that assess LLMs’ ability with
new knowledge rather than existing knowledge are
urgently needed.

Source of New Knowledge Many works use tem-
poral knowledge as a source of new knowledge to
evaluate new knowledge behavior of LLMs (Lazari-
dou et al., 2021; Agarwal and Nenkova, 2022; Jang
et al., 2023; Zaporojets et al., 2023). There is also
some work that uses entity or attribute substitution
to create new knowledge (Longpre et al., 2022;
Zhou et al., 2023). A discussion of the differences
and strengths and weaknesses of our work versus
prior work is in Appendix I.

8 Conclusion

We propose a new approach KnowGen to construct
new knowledge and build an artificial biological en-
tity benchmark ALCUNA for evaluating the ability
of LLMs faced with new knowledge. We test and
analyze several popular models with commonly
used methods, and find some useful conclusions.
Our proposed approach and benchmark can help
the development of more powerful LLMs that can
understand, differentiate and reason across new and
existing knowledge. In the future, we expect that
more LLMs will be evaluated on our benchmark.
More benchmarks in other disciplines also can be
constructed based our method.

Limitations

Although the method we design can be used for
the construction of any knowledge that satisfies the
ontological representation, we have implemented
it only on biological data. It is absolutely possible
to use this approach to create new knowledge in
other domains for evaluating LLMs. It is because
KnowGen method for creating new knowledge only
requires some defined classes, and some entities in
the classes, entities with their own attributes, and
connections between the entities. Any knowledge
base with such a structure can be used to create
new knowledge with our KnowGen method. In the
future, we hope that new knowledge datasets from
more domains will be available.

We evaluate only a few powerful models due to
the fact that some models are closed source or the
number of parameters is too large. We expect that
more models can be measured using our bench-
mark.

Ethics Statement

The knowledge of biological entities in our bench-
mark is artificially constructed, so it does not exist
in the real world. Therefore, for humans, it is neces-
sary to be careful not to be misled by the knowledge
inside; for models, the hazard of using our dataset
for training on model knowledge is unknown. This
is in line with our expectation that we want AL-
CUNA to be used as a new knowledge benchmark
only for the evaluation of model capabilities in face
of new knowledge.

Acknowledgements

This work was supported by National Key R&D
Program of China (2021 YFF0901502), National
Science Foundation of China (No. 62161160339),
State Key Laboratory of Media Convergence Pro-
duction Technology and Systems and Key Lab-
oratory of Science, Technology and Standard in
Press Industry (Key Laboratory of Intelligent Press
Media Technology). We appreciate the anonymous
reviewers for their helpful comments. Xiaojun Wan
is the corresponding author.

References

Oshin Agarwal and Ani Nenkova. 2022. Temporal ef-
fects on pre-trained models for language processing
tasks.
