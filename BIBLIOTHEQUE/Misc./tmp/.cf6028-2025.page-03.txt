                                                    ping: it runs k reasoning chains in parallel and halts early based on the proportion of traces
                                                    completed.
                                                    Sequential                   scaling             strategies                extend
                                                                                                                                                    Sequential Scaling               Parallel Scaling                  Hybrid Scaling
                                                    reasoning depth by iteratively refining,
                                                                                                                                                                                         Question                         Question
                                                    restarting,              or backtracking.                            Chain-of-                       Question
                                                    Thought(CoT)prompting(Weietal.,2023)                                                                   Thought
                                                                                                                                                             1
                                                                                                                                                                                                                            Thought
                                                                                                                                                                                                                                       Thought
                                                    is a fundamental idea, and subsequent                                                                                                                                     2
                                                                                                                                                                                                                                         3
                                                                                                                                                                                Thought              Thought
                                                                                                                                                                                          Thought
                                                                                                                                                                                                                 Thought
                                                                                                                                                                                  1                    3
                                                                                                                                                                                            2
                                                    work like STaR (Zelikman et al., 2022)                                                                                                                         1
                                                                                                                                                           Thought
                                                                                                                                                             2
                                                    and Reflexion (Shinn et al., 2023) explore                                                                                                                              Thought
                                                    revision through trial-and-error or verbal                                                                                                                                4
                                                                                                                                                           Thought
                                                    self-reflection. Tree-of-Thought (ToT) (Yao                                                              3
                                                    et al., 2023) and Graph-of-Thoughts (Besta
                                                                                                                                                         Response                       Response
                                                    et al., 2024) scale this further via struc-                                                                                                                           Response
                                                    tured breadth-first or DAG-style search.                                                               (a)                             (b)                               (c)
                                                    AlphaGeometry(Chervonyietal.,2025)                                                                       Figure 2: Different TTS paradigms
                                                    integrates symbolic proof search with
                                                    LLMsforstep-level sequential control. S1 (Muennighoff et al., 2025) fine-tunes models for
                                                    teaching self-correction strategies, utilizing higher test-time compute.
                                                    More recent efforts like hybrid scaling strategies blend both axes. Meta-Reasoner (Sui
                                                    et al., 2025) uses contextual bandits to dynamically select TTS strategies based on per-
                                                    ceived task difficulty. AgentTTS (Wang et al., 2025a) and START (Li et al., 2025) deploy
                                                    agents (LLMs with tool-calling capabilities) to switch between direct generation or more
                                                    elaborate reasoning. PEARL (Liu et al., 2025) interleaves draft generation with refinement,
                                                    simulating self-improvement loops. These meta-schedulers recognize that neither deep
                                                    nor parallel scaling alone is enough, and aim to adapt the strategy based on model be-
                                                    havior and prompt dynamics. Internal scaling strategies, in contrast, modify how much
                                                    computation the model performs internally during inference, without explicitly adjusting
                                                    the number of external samples or reasoning steps. HALT-CoT (Laaouach, 2025) and Soft-
                                                    CoT++ (Xu et al., 2025) estimate answer uncertainty and terminate early if confidence is
                                                    high.
                                                    Nostrategyisuniversallybest.                                           MultipleempiricalstudiesreinforcethatnoTTSstrategy
                                                    consistently dominates. Zhang et al. (2025) emphasize tradeoffs across accuracy, consis-
                                                    tency, and efficiency—the “TTS trilemma.” Snell et al. (2024) show that compute-optimal
                                                    allocation (e.g., short inference on easy questions, deeper inference on hard ones) out-
                                                    performs scaling model size alone.                                             Ghosal et al. (2025) and Hassid et al. (2025) show
                                                    that longer CoT chains often degrade accuracy. Inverse-scaling effects (Gema et al., 2025)
                                                    demonstrate that larger models or longer prompts may hurt, especially when uncertainty
                                                    is high or symbolic reasoning is required. This underscores our central thesis: optimal TTS
                                                    is highly contextual and must consider model training (e.g., type of post-training), task
                                                    type, and difficulty.
                                                    In this work, we consider first finish search (FFS, Algorithm 1), last finish search (LFS, Al-
                                                    gorithm 2) and beam search for our analyses, the first two of which are parametrized by
                                                    variables k and N, while the last is parametrized by N alone. FFS-k@N means sampling N
                                                    outputs and performing MV among the shortest k samples to determine the majority vote
                                                    while LFS-k@N simply involves choosing the longest k samples instead of shortest, fol-
                                                    lowedbymajorityvotingonthese. Beamsearchinvolvesmaintainingabeamofhighprob-
                                                                                                                                                                                                                                            2
                                                    ability partial hypotheses, continuously updating these prefixes as decoding progresses.
                                                    2.2        Models
                                                    Weevaluatebothreasoningandnon-reasoningmodelstoanalyzetheeffectsofTTSstrate-
                                                    gies across diverse training paradigms.
                                                    Reasoning models. DeepSeek-R1 is a reasoning-tuned LLM optimized for mathemat-
                                                    ical and logical tasks using GRPO—an RL algorithm that improves efficiency over PPO
                                                          2SinceweuseAPI-basedmodelinference(deepinfra.com),werestrictouranalysistoAPI-friendly
                                                    TTSstrategies.
                                                                                                                                                 3
