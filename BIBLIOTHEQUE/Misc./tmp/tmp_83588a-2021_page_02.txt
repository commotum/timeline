                                                  Stabilizing Equilibrium Models by Jacobian Regularization
               DEQssigniﬁcantly faster than their unregularized counter-          f (z?;x)−z? = 0; later works (Winston & Kolter, 2020;
                                                                                    θ
               parts. Second, this model class becomes much less brittle to       Revay et al., 2020) and a recent tutorial (Duvenaud et al.,
               architectural variants that would otherwise break the DEQ.         2020) have applied other algorithms, such as Peaceman-
               Wevalidate the proposed regularization by experiments on           Rachford splitting (Peaceman & Rachford, 1955) and An-
               both toy-scale synthetic tasks and large-scale real datasets       derson acceleration (Anderson, 1965).
               acrossdomains: word-levellanguagemodelingonWikiText-               Compared to Neural ODEs, deep equilibrium networks
               103(Merity et al., 2017) and high-resolutional image clas-         have been demonstrated to scale well to large and high-
               siﬁcation on the full ImageNet dataset (Deng et al., 2009).        dimensional tasks, such as language modeling, ImageNet
               Empirically, our regularized DEQs are generally 2x to 3x           classiﬁcation, and semantic segmentation (Bai et al., 2019;
               faster than prior DEQs, and can be accelerated to be as fast       2020), and are thus more applicable to domains where deep
               as explicit deep networks (e.g., ResNets and DenseNets).           learning has been traditionally successful. However, unlike
               This is the ﬁrst time that implicit models are accelerated to      ODEﬂows,DEQnetworksdonothaveauniquetrajectory,
               this level without sacriﬁcing scalability, accuracy, or struc-     and are not guaranteed to converge. Thus recent works have
               tural ﬂexibility. With their O(1) memory footprint, this           also begun to examine the stability and other theoretical
               further establishes implicit models as a strong competitor to      properties of DEQs. Winston & Kolter (2020) propose a
               explicit deep architectures.                                       monotone DEQ that has a unique ﬁxed point. Pabbaraju
                                                                                  et al. (2021); Revay et al. (2020) further study the Lipschitz
               2. Background & Related Work                                       boundedness of monotone DEQs. Kawaguchi (2021) ana-
                                                                                  lyze the gradient dynamics of a linearized version of DEQs.
               While explicit deep networks hierarchically stack layers to        Lu et al. (2021) apply an invertible equilibrium model to
               build a computation graph for their forward and backward           generative modeling via normalizing ﬂows.
               propagations, implicit models (Amos & Kolter, 2017; Chen
               et al., 2018; El Ghaoui et al., 2019; Gould et al., 2019; Bai      2.2. Regularizing Implicit Models
               et al., 2019) do not have a prescribed computation graph.          Just like explicit deep networks, implicit networks can over-
               Instead these models solve for a non-linear system. One            ﬁt to the dataset; but additionally, they can also become
               exampleistheNeuralODE(Chenetal.,2018),whichsolves                  unstable. For instance, Neural ODEs are essentially mod-
               an initial-value ODE problem that involves a residual layer.       eling inﬁntesimal steps of a residual block (He et al., 2016;
               Another example, which is the primary focus of our work,           Chang et al., 2018), and Grathwohl et al. (2019) found
               is the deep equilibrium network (DEQ) (Bai et al., 2019),          that weight decay & spectral normalization (Miyato et al.,
               which reduces the forward pass to a root-solving problem.          2018) are useful (though expensive) in reducing the rapidly
               In this section, we introduce the basics of DEQ models and         growing number of functional evaluations (NFEs) needed
               the relevant threads of work, followed by a discussion of          to solve for the ODE endpoint. On the other hand, large-
               prior approaches to regularizing implicit models.                  scale DEQ networks (Bai et al., 2019; 2020) have adopted
               2.1. Deep Equilibrium Models                                       weightnormalization(Salimans&Kingma,2016),recurrent
                                                                                  dropout (Gal & Ghahramani, 2016), and group normaliza-
               Given a layer/block f (which may contain a few shallow             tion (Wu & He, 2018) for preventing overﬁtting and diver-
                                       θ
               sublayers) and an input x, a deep equilibrium model aims           gence. Nonetheless, all these methods are borrowed from
               to approximate an “inﬁnite-level” layer stacking of the form       explicit deep networks, where they have long been known to
                 [i+1]        [i]                                                 workwell. They do not exploit the implicitness of implicit
               z      =fθ(z ;x)(wherei = 1,...,L,withL → ∞)by                     models.
               directly solving for its ﬁxed-point representation:
                                      z? = f (z?;x).                              Morerecently, a few different regularization methods have
                                             θ                                    been introduced to speciﬁcally ﬁx the numerous issues of
               Oneofthe appealing properties of this ﬁxed-point formu-            the vanilla Neural ODE and continuous normalizing ﬂow
               lation is that one can implicitly differentiate through the        models, such as augmenting the hidden state (Dupont et al.,
               equilibrium feature, without dependency on any intermedi-          2019), using hyper ODE solvers (Poli et al., 2020), and reg-
               ate activations in the forward pass. Formally, given a loss `,     ularizing higher-order time derivatives (Kelly et al., 2020).
               one can directly compute the gradient using the ﬁnal output:       These methods directly leverage the dynamical system view
                                                               ?                  of Neural ODEs. However, due to the inherent challenge of
                         ∂`       ∂`                    ∂f (z ;x)
                                                 ?   −1    θ                      solving high-dimensional ODEs, these accelerated Neural
                              =       (I −Jf (z ))                  ,
                        ∂(·)     ∂z?          θ             ∂(·)                  ODEmodelscanstilleasily take > 100 forward iterations
               where J (z?) is the Jacobian matrix at equilibrium z?.             even on MNIST classiﬁcation (Kelly et al., 2020), and even
                        fθ                                                        morefortheir backward pass. In comparison, DEQs scale
               Tosolve for the equilibrium, Bai et al. (2019) proposed to         better to high-dimensional tasks (e.g., 25-30 iterations on
               use Broyden’s method (Broyden, 1965) to ﬁnd the root of
