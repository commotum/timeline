Published as a conference paper at ICLR 2021

Table 1: GLUE scores on dev set. All settings are pre-trained by BERT-Base (110M) model with
16GB data. TUPE-A™ (TUPE-R™“) is the intermediate 300k-step checkpoint of TUPE-A (TUPE-R).
TUPE-A'*** removes the reset function from TUPE-A. BERT-A“ uses different projection matrices
for words and positions, based on BERT-A.

Steps MNLI-m/mm QNLI QQP SST CoLA MRPC RTE _ STS_ Avg.
BERT-A 1M 84.93/84.91 91.34 91.04 92.88 55.19 88.29 68.61 89.43 82.96
BERT-R 1M 85.81/85.84 92.16 91.12 92.90 55.43 89.26 71.46 88.94 83.66
TUPE-A 1M 86.05/85.99 91.92 91.16 93.19 63.09 88.37 71.61 88.88 84.47
TUPE-R 1M  86.21/86.19 92.17 91.30 93.26 63.56 89.89 73.56 89.23 85.04
TUPE-A™ 300k = 84.76/84.83 90.96 91.00 92.25 62.13 87.1 68.79 88.16 83.33
TUPE-R™ 300k 84.86/85.21 91.23 91.14 92.41 62.47 87.29 69.85 88.63 83.68
TUPE-A™*S 1M 85.91/85.73 91.90 91.05 93.17 59.46 88.53 69.54 88.97 83.81
BERT-A¢ 1M 85.26/85.28 91.56 91.02 92.70 59.73 88.46 71.31 87.47 83.64

4 EXPERIMENT

To verify the performance of the proposed TUPE, we conduct extensive experiments and demonstrate
the results in this section. In the main body of the paper, we study TUPE under the BERT-Base setting
(Devlin et al., 2018). We provide all the experimental details and more results on applying TUPE
under the BERT-Large setting and the ELECTRA setting (Clark et al., 2019b) in Appendix B and C.

4.1 EXPERIMENTAL DESIGN

We use BERT-Base (110M parameters) architecture for all experiments. Specifically, BERT-Base
is consist of 12 Transformer layers. For each layer, the hidden size is set to 768 and the number
of attention head is set to 12. To compare with TUPE-A and TUPE-R, we set up two baselines
correspondingly: BERT-A, which is the standard BERT-Base with absolute positional encoding
(Devlin et al., 2018); BERT-R, which uses both absolute positional encoding and relative positional
encoding (Raffel et al., 2019) (Eq. (5)).

Following Devlin et al. (2018), we use the English Wikipedia corpus and BookCorpus (Zhu et al.,
2015) for pre-training. By concatenating these two datasets, we obtain a corpus with roughly 16GB
in size. We set the vocabulary size (sub-word tokens) as 32,768. We use the GLUE (General
Language Understanding Evaluation) dataset (Wang et al., 2018) as the downstream tasks to evaluate
the performance of the pre-trained models. All codes are implemented based on fairseg (Ott et al.,
2019) in PyTorch (Paszke et al., 2017). All models are run on 16 NVIDIA Tesla V100 GPUs with
mixed-precision (Micikevicius et al., 2017).

4.2 OVERALL COMPARISON

V7 » 4
7 === BERT-A 84 2
SFR BERT-R + 4
P —-- TUPE-A o 4 a
2 ° o 4
a ®
x4
© o a4
a 8
* 24 --- BERT-A a4 -o> BERT-A
NP Nese ees BERT-R hn er a Corr BERT-R
x | / —-— TUPE-A sj / —-- TUPE-A
N / —— TUPE-R / — TUPE-R
2 —1—1— Fy —1—1—t 2 ——1—1—
0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000
Pre-Train Steps (k) Pre-Train Steps (k) Pre-Train Steps (k)
(a) Validation loss in pre-training. (b) MNLI-m score. (c) GLUE average score.

Figure 5: Both TUPE-A and TUPE-R converge much faster than the baselines, and achieve better
performance in downstream tasks while using much fewer pre-training steps.
