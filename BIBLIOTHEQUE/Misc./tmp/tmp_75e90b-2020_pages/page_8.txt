Published as a conference paper at ICLR 2021

The overall comparison results are shown in Table 1. Firstly, it is easy to find that both TUPE-A and
TUPE-R outperform baselines significantly. In particular, TUPE-R outperforms the best baseline
BERT-R by 1.38 points in terms of GLUE average score and is consistently better on almost all tasks,
especially on MNLI-m/mm, CoLA and MRPC. We can also see that TUPE-R outperforms TUPE-A
by 0.57 points. As discussed in Sec.3.3, although using the absolute/relative positional encoding
together seems to be redundant, they capture complement information to each other.

Besides the final performance, we also examine the efficiency of different methods. As shown in
Figure 5a, TUPE-A (TUPE-R) achieves smaller validation loss than the baselines during pre-training.
As shown in Table | and Figure Sc, TUPE-A (TUPE-R) can even achieve a better GLUE average
score than the baselines while only using 30% pre-training steps. Similar improvements can be found
in BERT-Large and ELECTRA settings in Table. 3 and Table. 4 (in Appendix C).

Since the correlations between words and positions are removed in TUPE, we can easily visualize the
attention patterns over positions, without considering the variability of input sentences. In TUPE-A
(see Figure 6), we find there are mainly five patterns (from 12 heads): (1) attending globally; (2)
attending locally; (3) attending broadly; (4) attending to the previous positions; (5) attending to
the next positions. Interestingly, the model can automatically extract these patterns from random
initialization. As there are some attention patterns indeed have strong local dependencies, our
proposed method to untie [CLS] is necessary. Similar patterns could be found in TUPE-R as well.

To summarize, the comparisons show the effectiveness and efficiency of the proposed TUPE. As
the only difference between TUPE and baselines is the positional encoding, these results indicate
TUPE can better utilize the positional information in sequence. In the following subsection, we will
examine each modification in TUPE to check whether it is useful.

4.3 ABLATION STUDY

Untie the [CLS] symbol from other positions. To study the improvement brought by untying
[CLS], we evaluate a positional encoding method which removes the reset function in Eq. 10. We
call it TUPE-A"*** and train this model using the same configuration. We also list the performance
of TUPE-A"*** in Table 1. From the table, we can see that TUPE-A works consistently better than
TUPE-A"***, especially for low-resource tasks, such as CoLA and RTE.

Untie the correlations between positions and words. Firstly, from Table 1, it is easy to
find that TUPE-A“*"’ outperforms BERT-A. Since the only difference between TUPE-A‘***
and BERT-A is the way of dealing with the absolution positional encoding, we can get a
conclusion that untying the correlations between positions and words helps the model training.
To further investigate this, we design another encoding method, BERT-A“, which is based
on BERT-A and uses different projection matrices for words and positions. Formally, aij =

Tu (eh We) (ae WEP + (al W2!)(pjU)? + (piU) (a WEP + (piU®) (pjU*)*) in

BERT-A‘. Therefore, we can check whether using different projection matrices (BERT-A vs.
BRET-A“) can improve the model and whether removing word-to-position and position-to-word
correlations (BRET-A¢ vs. TUPE-A‘**") hurts the final performance. From the summarized results
in Table 1, we find that TUPE-A"*** is even slightly better (0.17) than BERT-A¢ and is more
computationally efficient*. BERT-A is the worst one. These results indicate that using different
projection matrices improves the model, and removing correlations between words and positions
does not affect the performance.

Summary. From the above analysis, we find that untying [CLS] helps a great deal for the low-
resource tasks, such as CoLA and RTE. Untying the positional correlation and word correlation helps
high-resource tasks, like MNLI-m/-mm. By combining them, TUPE can consistently perform better
on all GLUE tasks. We also have several failed attempts regarding modifying the positional encoding
strategies, see Appendix D.

4BERT-A? is inefficient compared to TUPE as it needs to additionally compute middle two correlation terms
in all layers.
