Published as a conference paper at ICLR 2021

2

1

0

-1
--2

Figure 2: Visualizations of the four correlations (Eq. (6)) on a pre-trained BERT model for a sampled
batch of sentences. From left to right: word-to-word, word-to-position, position-to-word, and
position-to-position correlation matrices. In each matrix, the (i-th, j-th) element is the correlation
between i-th word/position and j-th word/position. We can find that the correlations between a word
and a position are not strong since the values in the second and third matrices look uniform.

3. TRANSFORMER WITH UNTIED POSITIONAL ENCODING

3.1 UNTIE THE CORRELATIONS BETWEEN POSITIONS AND WORDS

In absolute positional encoding, the positional embedding is added together with the word embedding
to serves as the input to the neural networks. However, these two kinds of information are hetero-
geneous. The word embedding encodes the semantic meanings of words and word analogy tasks
can be solved using simple linear arithmetic on word embeddings (Mikolov et al., 2013; Pennington
et al., 2014; Joulin et al., 2016). On the other hand, the absolute positional embedding encodes the
indices in a sequence, which is not semantic and far different from word meanings. We question the
rationality of the linear arithmetic between the word embedding and the positional embedding. To
check clearly, we take a look at the expansion of Eq. (3).

Avs ((wi + ps)WE*)((wj + p)WE)T

ij = Va

(wW2!) (ww)? + (wiW2) (ppWHA)T
vd vd

(DWE) (Wj WEYT | (rWE)(DAWEY)T
vd vd

The above expansion shows how the word embedding and the positional embedding are projected

and queried in the attention module. We can see that there are four terms after the expansion:
word-to-word, word-to-position, position-to-word, and position-to-position correlations.

ae

+

(6)

We have several concerns regarding this formulation. First, it is easy to see that the first and the
last term characterize the word-word and position-position relationships respectively. However, the
projection matrices W@! and W*"! are shared in both terms. As the positional embedding and
the word embedding encode significantly different concepts, it is not reasonable to apply the same
projection to such different information.

Furthermore, we also notice that the second and the third term use the position (word) as the query
to get keys composed of words (positions). As far as we know, there is little evidence suggesting
that the word and its location in a sentence have a strong correlation. Furthermore, in BERT and
recently developed advanced methods such as RoOBERTa (Liu et al., 2010), sentences are patched
in a random way. For example, in BERT, each input contains multiple sentences and some of the
sentences are negatively sampled from other documents to form the next sentence prediction task.
Due to the random process of batching, it is possible that a word can even appear at any positions and
the correlations between words and positions could be weak. To further investigate this, we visualize
the four correlations in Eq. (6) on a pre-trained BERT model. We find that the second and the third
term looks uniform across positions, as shown in Figure 2. This phenomenon suggests that there are
no strong correlationsÂ® between the word and the absolute position and using such noisy correlation
may be inefficient for model training.

3Some recent works (Yang et al., 2019; He et al., 2020) show that correlations between relative positions
and words can improve the performance. Our results have no contradiction with theirs as our study is on the
correlations between word embeddings and absolute positional embeddings.
