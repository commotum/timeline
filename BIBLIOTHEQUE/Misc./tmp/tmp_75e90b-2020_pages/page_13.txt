Published as a conference paper at ICLR 2021

Table 2: Hyperparameters for the pre-training and fine-tuning.

Pre-training Fine-tuning
Max Steps 1M -
Max Epochs - Sor 10?
Learning Rate le-4 {2e-5, 3e-5, 4e-5, Se-5}
Batch Size 256 32
Warm-up Ratio 0.01 0.06
Sequence Length 512 512
Learning Rate Decay Linear Linear
Adam € le-6 le-6
Adam (1, 82) (0.9, 0.999) (0.9, 0.999)
Clip Norm 1.0 1.0
Dropout 0.1 0.1
Weight Decay 0.01 0.01

a

we use five for the top four high-resource tasks, MNLI-m/-mm, QQP,
and QNLL, to save the fine-tuning costs. Ten is used for other tasks.

Table 3: GLUE scores on dev set. Different models are pre-trained in the BERT-Large setting (330M)
with 16GB data. TUPE-Large™ is the intermediate 300k-step checkpoint of TUPE-Large.

Steps MNLI-m/mm QNLI QQP SST CoLA MRPC RTE STS Avg.
BERT-Large 1M 88.21/88.18 93.56 91.66 94.08 58.42 90.46 77.63 90.15 85.82
TUPE-Large 1M 88.22/88.21 93.55 91.69 94.98 67.46 90.06 81.66 90.67 87.39
TUPE-Large™ 300k 86.92/86.80 92.61 91.48 93.97 63.88 89.26 75.82 89.34 85.56

Normalization and rescaling. Layer normalization (Ba et al., 2016; Xiong et al., 2020) is a key
component in Transformer. In TUPE, we also apply layer normalization on p; whenever it is used.
The vi in Eq. (2) is used in the Transformer to rescale the dot product outputs into a standard range.
In a similarly way, we use a in the Eq. (7) to both terms to keep the scale after the summation.
Furthermore, in order to directly obtain similar scales for every term, we parameterize 6; and 62
by using 6, = Jaq (Pe U®)(po,U*)? and 02 = Jaq (Poa U®)(po,U* )*, where po, , po, € R% are

learnable vectors.

C MORE RESULTS IN THE BERT-LARGE/ELECTRA-BASE SETTINGS

In the main body of the paper, we provide empirical studies on TUPE under the BERT-Base setting.
Since TUPE only modifies the positional encoding, one expects it to improve all language pre-training
methods and deeper models. To demonstrate this, we integrate TUPE-R (with relative position)
in BERT-Large and ELECTRA-Base models, and conduct experiments for comparison. For the
BERT-Large setting, we directly apply TUPE-R to the 24-layer Transformer model and obtain the
TUPE-Large model. For ELECTRA-Base, we apply our positional encoding to both generator and
discriminator and obtain the ELKECTRA-TUPE model. We use the same data introduced previously
and use the same hyper-parameters as in Liu et al. (2019); Clark et al. (2019b). The experimental
results are listed in the Table. 3 and Table. 4 respectively. From the results, we can that TUPE is
better than the corresponding baseline methods. These additional experiments further demonstrate
that TUPE is a better positional encoding solution.

D_ FAILED ATTEMPTS

We tried to replace the parametric form of the positional correlation (Faq (PU )(0; U*)*) to

the non-parametric form. However, empirically we found that the training of this setting con-
verges much slower than the baselines. We also tried to parameterize relative position bias bj; by
(7j;-iF)(rj-iF*)*. But the improvement is just marginal.

13
