                       Overall, these experiments suggest that in two-set training, random selection of the small set may be
                       optimal. Selecting a small set of easy cases (GCD multiple of 2 and 5), and examples that are known
                       to help training (log-uniform inputs) does not help, and limiting the small set to edge cases from the
                       tail of the outcome distribution brings no improvement to performance. This is a counter-intuitive,
                       but significant result.
                       D.2  Batching in two-set training: mixed batches are needed
                       In all experiments, during training, the model computes gradients over minibatches of 64 examples.
                       In two-set training, minibatches mix examples from the small and large set. We experimented with
                       using “mono-batches” that use samples from one set at a time. For instance, when training with
                       p = 0.25, 25% of minibatches would use examples from S only, and 75% would only use those
                       from S.
                       Onthe GCDproblem, we rerun the most successful two-set experiments (Section 4) with “mono-
                       batches” for S = 50K, 100K and 250K, and p = 0.25 and 0.5. For training budgets of 600M
                       and data budget of 100M examples, the models trained on mixed batches predicted 62 to 69 GCD
                       (Section 4). With “mono-batches”, the number of correctly predicted GCD never rises above 15.
                       For modular multiplication, we experimented with the following (S,p) pairs (S in millions):
                       (0.5,0.1),(2.5,0.25) and (10,0.5) with data budget 100M and training budget 600M. With these
                       settings, mixed-batch models achieve an average accuracy of 67% or more (Section 4). With “mono-
                       batches”, none of the models manages to learn (accuracy around 4%). This indicates that mixed
                       batching of samples from each of the two sets plays a central role for the two-set effect.
                       D.3  Shifting the small set
                       In these experiments, we study, in two-set training, the possible impact of overfitting on the small
                       set, by refreshing the small set with fresh examples periodically. This mimics certain aspects of
                       curriculum learning, where the training set is changed over time. On the GCD experiments, with
                       a data budget of 100 million, a training budget of 600 million, we shift the small set as training
                       proceeds, so that examples in the small set are seen k times on average. At the beginning of training,
                       the small set is the S first elements in the train set. After training on kS/p examples, examples in
                       the small set have been seen k times, and the small set is shifted to elements S + 1 to 2S of the
                       training set.
                       Table6providesperformancesfortwo-settrainingwithshift,fordifferentvaluesofp,S andk,fora
                       data budget of 100 million, and a training budget of 600 million. It is interesting to note that shifting
                       brings no improvement to 2-set training.
                       Table 6: Shifted two-set training. GCD predicted, average of 3 models, trained on a budget of 600 millions,
                       and a data budget of 100 million, for different values of S, p and k.
                                 S             250,000            500,000          1,000,000
                                 k        10   25  50  100   10  25   50  100  10   25  50  100
                                 p = 1.0  37   22  21   22   37  38   30  31   55   45  37   30
                                 p = 0.9  47   38  38   38   55  47   43  39   55   48  47   47
                                 p = 0.75 56   38  54   48   56  55   49  55   60   56  55   56
                                 p = 0.5  61   56  56   58   61  60   56  58   64   63  63   61
                                 p = 0.25 56   62  61   63   49  63   63  61   49   63  62   63
                       D.4  Fromtwo-settomany-settraining
                       Two-settrainingwithasmallrandomlyselectedsubsetS amountstoassigningdifferentprobabilities
                       to elements in the training set. For a randomly shuffled training set of size N, two-set training
                       amountstoselectingthefirstS elementswithprobabilityp/S (withreplacement)andtheN−S last
                       with probability (1−p)/(N −S), a step-function distribution over {1,...,N}. We now generalize
                       this approach by introducing a probability law P such that P(i) is the probability of selecting the
                       i-th example in the training set. Our motivation is to obtain a smooth, possibly more principled,
                       distribution than the step-function induced by the two-set approach. Pragmatically, a one-parameter
                                                               11
