                                      Published as a conference paper at ICLR 2025
                                      against computational efficiency:
                                                                                                      L        =L +λL                            ,                                                            (4)
                                                                                                         total        pred           introspect
                                     where, L              is the prediction loss from the prediction network, L                                             is the introspection loss from the
                                                    pred                                                                                        introspect
                                      introspection network, penalizing computational cost, λ is a hyperparameter balancing the influence of the
                                      introspection loss. The prediction loss L                              measures the discrepancy between the model’s predictions and
                                                                                                       pred
                                      the ground truth labels (e.g. cross entropy). The introspection loss L                                                     encourages the efficient use of
                                                                                                                                                     introspect
                                      computational resources by penalizing the use of additional layers and iterations,
                                                                                            M                                           N                             N           !
                                                                 L              = 1 X β·CompCost +γ·Xw ·m +δ·Xm                                                                       ,                       (5)
                                                                    introspect       M                                     i                    l      i,l                    i,l
                                                                                           i=1                                         l=1                           l=1
                                     where CompCost is the computational cost for input x , calculated based on the number of layers and
                                                                  i                                                               i
                                                                                                                                                                                 c
                                      iterations used, m               ∈{0,1}indicates whether layer l is selected for input x , w = Pl                                                is the importance
                                                                   i,l                                                                                           i      l          cl
                                     weight for layer l, reflecting its computational cost, β, γ, and δ are hyperparameters controlling the trade-
                                      off between accuracy and efficiency. Ablation of these hyperparameters in explored in Appendix F.3.
                                      The layer selection variables mi,l are discrete, introducing non-
                                      differentiability into the optimization process. To enable gradient-
                                      based optimization, we employ the Gumbel-Softmax trick (Jang
                                      et al., 2017; Paulus et al., 2020) to approximate the discrete sam-
                                      pling with a differentiable operation. For each input xi, the intro-
                                      spection network outputs logits z = [z                             , . . . , z     ], from which
                                                                                            i         i,1           i,N
                                     wecomputetheselection probabilities:
                                                                               exp((z          +g )/τ)
                                                              p     = P                    i,l       i,l              ,                    (6)
                                                                i,l          N exp((z               +g )/τ)
                                                                             j=1                i,j       i,j
                                     where g           are samples from the Gumbel(0,1) distribution, and τ
                                                   i,l
                                      is the temperature parameter controlling the smoothness of the
                                      approximation. This continuous relaxation of m                                    is then used in
                                                                                                                    i,l
                                      place of the discrete variables during optimization. The prediction                                           Figure 2: MIND model performance
                                      network applies fixed-point iterations in selected layers to refine                                           on the random dot movement across
                                      the activations. Backpropagating through these iterations poses                                              various consolidation difficulty in test
                                      challenges due to their implicit nature.                                                                      data. Harder problems are adaptively
                                                                                                                                                    addressed with more layers in the FPI.
                                      4      EXPERIMENTS
                                     Weevaluated MIND against existing deep learning models using Vision and NLP benchmarks covering
                                      more than 10 datasets. For robustness, all experiments are conducted using 10 different random seeds
                                      and 9-fold cross-validation. Additional training specifications and extensive details on the datasets are
                                      given in Appendix C. We want to provide a rich picture of how MIND performs in different scenarios.
                                     The primary goals of these experiments are twofold: 1) to evaluate the classification accuracy of our
                                      model in comparison with existing state-of-the-art architectures, and 2) to assess the computational efficiency
                                      improvementsachievedthroughouradaptivelayerselectionmechanism. Wealsoshareadditionalexperiments
                                      in Appendix E including Ablations.
                                      4.1      TOYEXAMPLE: LEVELS OF PROBLEM COMPLEXITY
                                     Toevaluate and demonstrate how the MIND model handles inputs of varying complexities, we used a problem
                                     where the degree of difficulty is apparent to a human and also closely aligns with the performance of a deep
                                      learning network. This example was inspired by the motion coherence task, also known as the random dot
                                      motion task (Newsome et al., 1989).
                                                                                                                    6
