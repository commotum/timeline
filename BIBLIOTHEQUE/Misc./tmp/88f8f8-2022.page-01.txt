                                Point Primitive Transformer for Long-Term 4D
                                          Point Cloud Video Understanding
                                              1∗           1∗                2          2           1,3
                                     Hao Wen , Yunze Liu , Jingwei Huang , Bo Duan , and Li Yi
                                                           1 Tsinghua University
                                           wenh19@mails.tsinghua.edu.cn, liuyzchina@gmail.com
                                         2 Huawei Technologies {huangjingwei6,duanbo5}@huawei.com
                                              3 Shanghai Qi Zhi Institute ericyi0124@gmail.com
                                     Abstract. Thispaperproposesa4Dbackboneforlong-termpointcloud
                                     video understanding. A typical way to capture spatial-temporal context
                                     is using 4Dconv or transformer without hierarchy. However, those meth-
                                     ods are neither effective nor efÏcient enough due to camera motion, scene
                                     changes, sampling patterns, and complexity of 4D data. To address those
                                     issues, we leverage the primitive plane as mid-level representation to cap-
                                     ture the long-term spatial-temporal context in 4D point cloud videos,
                                     andpropose a novel hierarchical backbone named Point Primitive Trans-
                                     former(PPTr), which is mainly composed of intra-primitive point trans-
                                     formers and primitive transformers. Extensive experiments show that
                                     PPTr outperforms the previous state of the arts on different tasks.
                                     Keywords: Transformer; Primitive; Long-term Point Cloud Video
                               1   Introduction
                                   Point cloud videos are ubiquitous in robots and AR systems that act as a
                               windowintoourdynamicallychanging3Dworld.Beingabletorecordmovements
                               in the physical space, point cloud sequences play a key role in comprehending
                               environmental changes and supporting interactions with the world, which can be
                               hardly described by 2D images or static 3D point clouds. Therefore, an intelligent
                               agent must process such a form of data precisely to better model the real world,
                               adapt to environmental changes, and interact with them.
                                   Despite its importance, processing point cloud sequences is a quite challeng-
                               ing task for machines that are largely determined by two aspects: effectiveness
                               and efÏciency. Effectiveness refers to the ability to capture long-term spatial-
                               temporal structures. Due to camera motion, scene changes, occlusion changes,
                               and sampling patterns, points between different frames are unstructured and
                               inconsistent, making it difÏcult to effectively integrate different frames into the
                               underlying spatio-temporal structure. EfÏciency refers to how to efÏciently pro-
                               cess long point cloud videos with limited computing resources. The complexity
                                 ∗Equal contribution.
