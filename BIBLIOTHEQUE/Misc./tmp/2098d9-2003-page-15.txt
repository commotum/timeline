                                                ANEURALPROBABILISTIC LANGUAGEMODEL
                      the computation of the gradient is tractable. This is not the case for example with products-of-
                      HMMs(Brown and Hinton, 2000), in which the product is over experts that view the whole se-
                      quence, and which can be trained with approximate gradient algorithms such as the contrastive
                      divergence algorithm (Brown and Hinton, 2000). Note also that this architecture and the products-
                      of-experts formulation can be seen as extensions of the very successful Maximum Entropy mod-
                      els (Berger et al., 1996), but where the basis functions (or “features”, here the hidden units acti-
                      vations) are learned by penalized maximum likelihood at the same time as the parameters of the
                      features linear combination, instead of being learned in an outer loop, with greedy feature subset
                      selection methods.
                         We have implemented and experimented with the above architecture, and have developed a
                      speed-up technique for the neural network training, based on importance sampling and yielding a
                      100-fold speed-up (Bengio and Senécal, 2003).
                         Out-of-vocabulary words. Anadvantage ofthis architecture over the previous one is that it can
                      easily deal with out-of-vocabulary words (and even assign them a probability!). The main idea is to
                      ﬁrst guess an initial feature vector for such a word, by taking a weighted convex combination of the
                      feature vectors of other words that could have occurred in the same context, with weights propor-
                                                                                                               ˆ    t−1
                      tional to their conditional probability. Suppose that the network assigned a probability P(i|w     )
                                                 t−1                                                                t−n+1
                      to words i∈V incontext w        , and that in this context we observe a new word j 6∈V. We initialize
                                                 t−n+1                            ˆ    t−1
                      the feature vector C(j) for j as follows: C(j) ← ∑     C(i)P(i|w       ). We can then incorporate j
                                                                          i∈V          t−n+1
                      in V and re-compute probabilities for this slightly larger set (which only requires a renormalization
                      for all the words, except for word i, which requires a pass through the neural network). This feature
                      vector C(i) can then be used in the input context part when we try to predict the probabilities of
                      words that follow word i.
                      5.2 Other Future Work
                      There are still many challenges ahead to follow-up on this work. In the short term, methods to
                      speed-up training and recognition need to be designed and evaluated. In the longer term, more ways
                      to generalize should be introduced, in addition to the two main ways exploited here. Here are some
                      ideas that we intend to explore:
                         1. Decomposing the network in sub-networks, for example using a clustering of the words.
                            Training many smaller networks should be easier and faster.
                         2. Representing the conditional probability with a tree structure where a neural network is ap-
                            plied at each node, and each node represents the probability of a word class given the context
                            andtheleavesrepresent theprobability ofwordsgiventhecontext. Thistypeofrepresentation
                            has the potential to reduce computation time by a factor |V|/log|V| (see Bengio, 2002).
                         3. Propagating gradients only from a subset of the output words. It could be the words that
                            are conditionally most likely (based on a faster model such as a trigram, see Schwenk and
                            Gauvain, 2002, for an application of this idea), or it could be a subset of the words for which
                            the trigram has been found to perform poorly. If the language model is coupled to a speech
                            recognizer, then only the scores (unnormalized probabilities) of the acoustically ambiguous
                            words need to be computed. See also Bengio and Senécal (2003) for a new accelerated
                            training method using importance sampling to select the words.
                                                                      1151
