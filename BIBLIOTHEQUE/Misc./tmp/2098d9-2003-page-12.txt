                                               BENGIO,DUCHARME,VINCENTANDJAUVIN
                    the EM algorithm in about 5 iterations, on a set of data (the validation set) not used for estimating
                    the unigram, bigram and trigram relative frequencies. The interpolated n-gram was used to form a
                    mixture with the MLPs since they appear to make “errors” in very different ways.
                        Comparisons were also made with other state-of-the-art n-gram models: back-off n-gram mod-
                    els with the Modiﬁed Kneser-Ney algorithm (Kneser and Ney, 1995, Chen and Goodman., 1999), as
                    well as class-based n-gram models (Brown et al., 1992, Ney and Kneser, 1993, Niesler et al., 1998).
                    The validation set was used to choose the order of the n-gram and the number of word classes for
                    the class-based models. We used the implementation of these algorithms in the SRI Language Mod-
                    eling toolkit, described by Stolcke (2002) and in www.speech.sri.com/projects/srilm/.They
                     were used for computing the back-off models perplexities reported below, noting that we did not
                     give a special status to end-of-sentence tokens in the accounting of the log-likelihood, just as for our
                     neural network perplexity. All tokens (words and punctuation) were treated the same in averaging
                     the log-likelihood (hence in obtaining the perplexity).
                     4.2 Results
                                                                                     ˆ      t−1
                     Beloware measures of test set perplexity (geometric average of 1/P(w |w   )) for different models
                     ˆ                                                                   t  1
                     P. Apparent convergence of the stochastic gradient ascent procedure was obtained after around 10
                     to 20 epochs for the Brown corpus. On the AP News corpus we were not able to see signs of over-
                     ﬁtting (on the validation set), possibly because we ran only 5 epochs (over 3 weeks using 40 CPUs).
                     Early stopping on the validation set was used, but was necessary only in our Brown experiments.
                                                  −4                                                              −5
                     Aweight decay penalty of 10     was used in the Brown experiments and a weight decay of 10
                    was used in the APNews experiments (selected by a few trials, based on validation set perplexity).
                    Table 1 summarizes the results obtained on the Brown corpus. All the back-off models of the
                    table are modiﬁed Kneser-Ney n-grams, which worked signiﬁcantly better than standard back-off
                    models. When m is speciﬁed for a back-off model in the table, a class-based n-gram is used (m
                    is the number of word classes). Random initialization of the word features was done (similarly to
                    initialization of neural network weights), but we suspect that better results might be obtained with a
                    knowledge-based initialization.
                        The main result is that signiﬁcantly better results can be obtained when using the neural net-
                    work, in comparison with the best of the n-grams, with a test perplexity difference of about 24% on
                    Brownandabout8%onAPNews,whentakingtheMLPversusthen-gramthatworkedbestonthe
                    validation set. The table also suggests that the neural network was able to take advantage of more
                    context (on Brown, going from 2 words of context to 4 words brought improvements to the neural
                    network, not to the n-grams). It also shows that the hidden units are useful (MLP3 vs MLP1 and
                    MLP4vsMLP2), and that mixing the output probabilities of the neural network with the interpo-
                    lated trigram always helps to reduce perplexity. The fact that simple averaging helps suggests that
                    the neural network and the trigram make errors (i.e. low probability given to an observed word) in
                    different places. The results do not allow to say whether the direct connections from input to output
                    are useful or not, but suggest that on a smaller corpus at least, better generalization can be obtained
                    without the direct input-to-output connections, at the cost of longer training: without direct connec-
                    tions the network took twice as much time to converge (20 epochs instead of 10), albeit to a slightly
                    lower perplexity. A reasonable interpretation is that direct input-to-output connections provide a bit
                    more capacity and faster learning of the “linear” part of the mapping from word features to log-
                                                                  1148
