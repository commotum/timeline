                                Inotherwords,DandGplaythefollowingtwo-playerminimaxgamewithvaluefunctionV(G,D):
                                            minmaxV(D,G)=E                        [logD(x)]+E                [log(1 − D(G(z)))].              (1)
                                              G    D                    x∼pdata(x)                  z∼pz(z)
                                In the next section, we present a theoretical analysis of adversarial nets, essentially showing that
                                the training criterion allows one to recover the data generating distribution as G and D are given
                                enough capacity, i.e., in the non-parametric limit. See Figure 1 for a less formal, more pedagogical
                                explanation of the approach. In practice, we must implement the game using an iterative, numerical
                                approach. Optimizing D to completion in the inner loop of training is computationally prohibitive,
                                andonﬁnitedatasetswouldresultinoverﬁtting. Instead, we alternate between k steps of optimizing
                                Dandonestep of optimizing G. This results in D being maintained near its optimal solution, so
                                long as G changes slowly enough. This strategy is analogous to the way that SML/PCD [31, 29]
                                training maintains samples from a Markov chain from one learning step to the next in order to avoid
                                burning in a Markov chain as part of the inner loop of learning. The procedure is formally presented
                                in Algorithm 1.
                                In practice, equation 1 may not provide sufﬁcient gradient for G to learn well. Early in learning,
                                when G is poor, D can reject samples with high conﬁdence because they are clearly different from
                                the training data. In this case, log(1 − D(G(z))) saturates. Rather than training G to minimize
                                log(1 −D(G(z))) we can train G to maximize logD(G(z)). This objective function results in the
                                sameﬁxedpointofthedynamicsofGandDbutprovidesmuchstrongergradientsearlyinlearning.
                                                                                                                     . . .
                                X X X
                                  x
                                Z Z Z
                                  z
                                            (a)                         (b)                         (c)                             (d)
                                Figure 1: Generative adversarial nets are trained by simultaneously updating the discriminative distribution
                                (D, blue, dashed line) so that it discriminates between samples from the data generating distribution (black,
                                dotted line) p  from those of the generative distribution p (G) (green, solid line). The lower horizontal line is
                                             x                                             g
                                the domain from which z is sampled, in this case uniformly. The horizontal line above is part of the domain
                                of x. The upward arrows show how the mapping x = G(z) imposes the non-uniform distribution pg on
                                transformed samples. G contracts in regions of high density and expands in regions of low density of p . (a)
                                                                                                                                            g
                                Consider an adversarial pair near convergence: pg is similar to pdata and D is a partially accurate classiﬁer.
                                (b) In the inner loop of the algorithm D is trained to discriminate samples from data, converging to D∗(x) =
                                   p   (x)
                                    data      . (c) After an update to G, gradient of D has guided G(z) to ﬂow to regions that are more likely
                                p   (x)+p (x)
                                 data    g
                                to be classiﬁed as data. (d) After several steps of training, if G and D have enough capacity, they will reach a
                                point at which both cannot improve because p      =p . Thediscriminator is unable to differentiate between
                                                                                g     data
                                the two distributions, i.e. D(x) = 1.
                                                                    2
                                4    Theoretical Results
                                The generator G implicitly deﬁnes a probability distribution p as the distribution of the samples
                                                                                                        g
                                G(z)obtainedwhenz ∼ p . Therefore,wewouldlikeAlgorithm1toconvergetoagoodestimator
                                                               z
                                of p    , if given enough capacity and training time. The results of this section are done in a non-
                                    data
                                parametric setting, e.g. we represent a model with inﬁnite capacity by studying convergence in the
                                space of probability density functions.
                                Wewill show in section 4.1 that this minimax game has a global optimum for p = p                       . We will
                                                                                                                            g      data
                                then show in section 4.2 that Algorithm 1 optimizes Eq 1, thus obtaining the desired result.
                                                                                        3
