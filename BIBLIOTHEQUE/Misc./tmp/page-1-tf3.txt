                                 Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess
                                                                                          Test-Time Scaling Capabilities?
                                                                 1                                           1                                        1                                       3                                 1,2 *
                                 ZhiyuanZeng , QingyuanChen , ZhangyueYin , YunhuaZhou                                                                                                                XipengQiu
                                                   1Fudan University, 2Shanghai Innovation Institute, 3Shanghai AI Laboratory
                                                                                cengzy23@m.fudan.edu.cn; xpqiu@fudan.edu.cn
                                                                      Abstract                                                                                            Correct                Incorrect
                                                                                                                                                                 MATH                                             AIME
                                       The advent of test-time scaling in large lan-                                                        7500                                           15000
                                       guage models (LLMs), exemplified by Ope-                                                             5000                                           10000
                                       nAI’s o1 series, has advanced reasoning capa-                                                       Length2500                                        5000
                                       bilities by scaling computational resource al-
                                       location during inference. While successors                                                                0                                               0
                                                                                                                                                                            QwQ  LIMO                                       QwQ  LIMO
                                       like QwQ, Deepseek-R1 (R1) and LIMO repli-                                                                 R1-671b                                         R1-671b
                                                                                                                                                    R1-Distill-32bR1-Distill-14b                    R1-Distill-32bR1-Distill-14b
                                       cate these advancements, whether these models                                                                           R1-Distill-1.5b                                  R1-Distill-1.5b
                                       truly possess test-time scaling capabilities re-                                                                           GPQA                     15000             Omin-MATH
                                       mains underexplored. This study found that                                                           7500
                                                                                                                                                                                           10000
                                       longer CoTs of these o1-like models do not                                                           5000
                                       consistently enhance accuracy; in fact, correct                                                     Length2500                                        5000
                                       solutions are often shorter than incorrect ones                                                            0                                               0
                                       for the same questions. Further investigation                                                                                        QwQ  LIMO                                       QwQ  LIMO
                                       shows this phenomenon is closely related to                                                                R1-671b                                         R1-671b
                                                                                                                                                    R1-Distill-32bR1-Distill-14b                    R1-Distill-32bR1-Distill-14b
                                       models’self-revision capabilities - longer CoTs                                                                         R1-Distill-1.5b                                  R1-Distill-1.5b
                                       contain more self-revisions, which often lead                                                    Figure 1: The average length of correct solutions versus
                                       to performance degradation. We then com-                                                         incorrect solutions evaluated on the same questions.For
                                       pare sequential and parallel scaling strategies                                                  eachquestion,solutionlengthswereaveragedseparately
                                       on QwQ, R1 and LIMO, finding that parallel                                                       for correct and incorrect responses, then averaged across
                                       scaling achieves better coverage and scalability.                                                all questions.
                                       Based on these insights, we propose Shortest
                                       Majority Vote, a method that combines parallel
                                       scaling strategies with CoT length characteris-                                                       Following o1’s success, models such as QwQ
                                       tics, significantly improving models’ test-time                                                  (Team, 2024b), Deepseek-R1 (R1) (DeepSeek-AI
                                       scalability compared to conventional majority                                                    et al., 2025) and LIMO (Ye et al., 2025) have
                                       voting approaches.                                                                               emerged as leading open-source successors, repli-
                                                                                                                                        cating o1’s achievements and demonstrating com-
                                1 Introduction                                                                                          parable reasoning abilities. Although both QwQ,
                                                                                                                                        R1 and LIMO demonstrate strong reasoning ca-
                                Therelease of the OpenAI o1 series models (Ope-                                                         pabilities and the ability to generate lengthy CoT
                                nAI, 2024a,b) marked a pivotal advancement in                                                           at test time, the existence of true test-time scal-
                                the reasoning capabilities of Large Language Mod-                                                       ing where performance consistently improves
                                els (LLMs), introducing a novel scaling paradigm,                                                       with longer CoTs remains to be verified for these
                                test-time scaling, which allocates more compute                                                         models.
                                resources during test time. The test-time scaling                                                            Toexplorethisquestion,wesystematicallyinves-
                                havetwodimensions,sequentialandparallel (Zeng                                                           tigate the relationship between CoT length and rea-
                                et al., 2024). Sequential scaling increase test-time                                                    soning performance in QwQ, R1 and LIMO, chal-
                                computebyscalingthelengthofChain-of-Thought                                                             lenging the conventional assumption that extended
                                (CoT) (Wei et al., 2022), while parallel scaling par-                                                   reasoning chains inherently lead to improved ac-
                                allely samples multiple solutions and pick the best                                                     curacy. Contrary to expectations, our analysis re-
                                one.                                                                                                    veals that longer CoTs do not consistently improve
                                      * Corresponding author                                                                            accuracy of these o1-like models. Notably, we
                                                                                                                                4651
                          Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4651–4665
                                                                         July 27 - August 1, 2025 ©2025 Association for Computational Linguistics
