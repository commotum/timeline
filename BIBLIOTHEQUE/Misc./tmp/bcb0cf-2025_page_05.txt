                      Published as a conference paper at ICLR 2025
                      Vision Tasks.  For computer vision applications, we implement the prediction network using a lightweight
                      convolutional neural network (CNN) architecture. Specifically, it consists of three convolutional layers with
                      filter sizes of 64, 128, and 256 channels, respectively. Each layer employs a kernel size of 3 × 3 and is
                      followed by a Rectified Linear Unit (ReLU) activation function (Xu et al., 2015) to introduce non-linearity.
                      Batch normalization is applied after each activation to stabilize the learning process (Ioffe & Szegedy, 2015).
                      This architecture strikes a balance between computational efficiency and representational capacity, containing
                      approximately 5.6 million parameters, which facilitates rapid training and inference while handling complex
                      image data effectively.
                      LanguageTasks. Inthecontextofnatural language processing, the prediction network is instantiated as
                      a dual-layer Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1995). Each LSTM
                      layer consists of 256 hidden units, capturing temporal dependencies in sequential data. Dropout layers are
                      interleaved between the LSTM layers to prevent overfitting, enhancing the model’s generalization capabilities
                      (Srivastava et al., 2014).
                      MIND-Transformer Fornaturallanguageprocessing tasks, we extend the prediction network to a Trans-
                      former architecture, introducing adaptive computation in both the self-attention and feed-forward networks.
                      Theself-attention mechanism with fixed-point iterations fθ is defined as:
                                         QKT                                        Table 1: MIND-Transformer Details
                           A0 =softmax       √d    V                                  Component               Setting
                                                        QKT+f (A )
                         A     =f(A ,x;θ)=softmax               √ θ k      V    (2)   NumberofLayers          6
                           k+1        k                                               Dimension (d     )      512
                                                                  d                                model
                      where Q = W x,K = W x,V =W xarethequery,key,and                 Attention Heads #       8
                                   Q           K          V                           FFNDimension            2048
                      value projections of the input x, and f is a learnable function that
                                                         θ                            MaxSequenceLength       512
                      refines the attention mechanism.                                Fixed Point Iteration   1-6 layers
                      Similarly, we apply fixed-point iterations g in the feed-forward network:
                                                              θ
                                               FFN0(x) = W2 ·ReLU(W1x+b1)+b2
                                            FFN     (x) = W ·ReLU(W x+b +g (FFN (x)))+b
                                                 k+1         2          1     1    θ      k         2
                      Furthermore, we cap the number of iterations in all FPIs based on the input complexity score computed as:
                                  IC(x) = α·(1−max(softmax(f(x))))+β ·H(softmax(x))+γ ·∥∇ f(x)∥ ,                       (3)
                                                                                                       x      2
                      wheref(x)isthemodel’soutputbeforethefinalsoftmaxlayer,H(.)istheentropyfunction,|∇ f(x)| isthe
                                                                                                              x     2
                      L2 norm of the input gradient and α, β, and γ are weighting coefficients set to 0.4, 0.4, and 0.2 respectively.
                      Maximumnumberofiterationsis set to max(10IC(x),50).
                      Forsimplicity, the MIND-Transformer employs the same configurations as a standard Transformer of Vaswani
                      et al. (2017) (see Table 1). The model incorporates fixed point iterations within its self-attention mechanism
                      and transition function block (ϕ) across multiple layers. We utilize relative positional embedding with a
                      sequence length of 120 tokens for both training and inference processes.
                      3.4   TRAINING THE MIND MODEL
                      Training the MIND model involves jointly optimizing the prediction network and the introspection network
                      to achieve high predictive performance while efficiently allocating computational resources. This section
                      details the training objectives, loss functions, optimization strategies, and the methodology for computing
                      gradients through the fixed-point iterations using phantom gradients as given in Algorithm 1 In Appendix A.
                      The overall training objective is to minimize a composite loss function that balances prediction accuracy
                                                                    5
