                          ANEURALPROBABILISTIC LANGUAGEMODEL
            will focus on in this paper. First, it is not taking into account contexts farther than 1 or 2 words,1
            second it is not taking into account the “similarity” between words. For example, having seen the
            sentence “The cat is walking in the bedroom” in the training corpus should help us gener-
            alize to make the sentence “A dog was running in a room” almost as likely, simply because
            “dog”and“cat” (resp. “the”and“a”, “room”and“bedroom”, etc...) have similar semantic and
            grammatical roles.
              There are many approaches that have been proposed to address these two issues, and we will
            brieﬂy explain in Section 1.2 the relations between the approach proposed here and some of these
            earlier approaches. We will ﬁrst discuss what is the basic idea of the proposed approach. A more
            formal presentation will follow in Section 2, using an implementation of these ideas that relies
            on shared-parameter multi-layer neural networks. Another contribution of this paper concerns the
            challenge of training such very large neural networks (with millions of parameters) for very large
            data sets (with millions or tens of millions of examples). Finally, an important contribution of
            this paper is to show that training such large-scale model is expensive but feasible, scales to large
            contexts, and yields good comparative results (Section 4).
              Manyoperations inthis paper are in matrix notation, with lower case v denoting a column vector
               0                              0
            and v its transpose, Aj the j-th row of a matrix A,andx.y = x y.
            1.1 Fighting the Curse of Dimensionality with Distributed Representations
            In a nutshell, the idea of the proposed approach can be summarized as follows:
               1. associate with each word in the vocabulary a distributed word feature vector (a real-
                 valued vector in Rm),
               2. express the joint probability function of word sequences in terms of the feature vectors
                 of these words in the sequence, and
               3. learn simultaneously the word feature vectors and the parameters of that probability
                 function.
              Thefeature vector represents different aspects of the word: each word is associated with a point
            in a vector space. The number of features (e.g. m =30, 60 or 100 in the experiments) is much
            smaller than the size of the vocabulary (e.g. 17,000). The probability function is expressed as a
            product of conditional probabilities of the next word given the previous ones, (e.g. using a multi-
            layer neural network to predict the next word given the previous ones, in the experiments). This
            function has parameters that can be iteratively tuned in order to maximize the log-likelihood of
            the training data or a regularized criterion, e.g. by adding a weight decay penalty.2 The feature
            vectors associated with each word are learned, but they could be initialized using prior knowledge
            of semantic features.
              Why does it work? In the previous example, if we knew that dog and cat played simi-
            lar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), (is,was),
             1. n-grams with n up to 5 (i.e. 4 words of context) have been reported, though, but due to data scarcity, most predictions
              are made with a much shorter context.
             2. Like in ridge regression, the squared norm of the parameters is penalized.
                                      1139
