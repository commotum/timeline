                14                                                                       Transactions of the Institute of Measurement and Control 00(0)
                References                                                               Huang J, Huang G, Zhu Z, et al. (2022b) BEVDet: High-perfor-
                Bijelic M, Gruber T, Mannan F, et al. (2020) Seeing through fog with-        mance multi-camera 3D object detection in bird-eye-view
                    out seeing fog: Deep multimodal sensor fusion in unseen adverse          (arXiv:2112.11790). arXiv. Available at: http://arxiv.org/abs/
                    weather (arXiv:1902.08913). arXiv. Available at: http://arxiv.org/       2112.11790 (accessed 11 September 2023).
                    abs/1902.08913(accessed 14 October 2024).                            Jiang Y, Zhang L, Miao Z, et al. (2023) Polarformer: Multi-camera
                Caesar H, Bankiti V, Lang AH, et al. (2020) nuScenes: A multimodal           3Dobject detection with polar transformer. In: Proceedings of the
                    dataset for autonomous driving (arXiv:1903.11027). arXiv. Avail-         AAAI conference on artificial intelligence, vol. 37, Washington,
                    able at: http://arxiv.org/abs/1903.11027 (accessed 14 October            DC,7–14February,pp.1042–1050. New York: ACM.
                    2024).                                                               Lang AH, Vora S, Caesar H, et al. (2019) PointPillars: Fast encoders
                Chambon L, Zablocki E, Chen M, et al. (2024) PointBeV: A sparse              for object detection from point clouds. In: Proceedings of the
                    approach for BeV predictions. In: Proceedings of the IEEE/CVF            IEEE/CVF conference on computer vision and pattern recognition,
                    conference on computer vision and pattern recognition, pp. 15195–        LongBeach,CA,pp.12697–12705. NewYork:IEEE.
                    15204.   Available    at:  https://openaccess.thecvf.com/content/    Li Y, Bao H, Ge Z, et al. (2023a) BEVStereo: Enhancing depth esti-
                    CVPR2024/papers/Chambon_PointBeV_A_Sparse_Approach_                      mation in multi-view 3D object detection with temporal stereo. In:
                    for_BeV_Predictions_CVPR_2024_paper.pdf                                  Proceedings of the AAAI conference on artificial intelligence, vol.
                Chen Y, Liu J, Zhang X, et al. (2023) LargeKernel3D: Scaling up              37, Washington, DC, 7–14 February, pp. 1486–1494. New York:
                    Kernels in 3D sparse CNNs. In: Proceedings of the IEEE/CVF               ACM.
                    conference on computer vision and pattern recognition, pp. 13488–    Li Y, Ge Z, Yu G, et al. (2023b) BEVDepth: Acquisition of reliable
                    13498.   Available    at:  https://openaccess.thecvf.com/content/        depth for multi-view 3D object detection. In: Proceedings of the
                    CVPR2023/papers/Chen_LargeKernel3D_Scaling_Up_Kernels_                   AAAI conference on artificial intelligence, vol. 37, Washington,
                    in_3D_Sparse_CNNs_CVPR_2023_paper.pdf                                    DC,7–14February,pp.1477–1485. New York: ACM.
                ChuX,TianZ,WangY,etal.(2021a)Twins:Revisitingthedesignof                 Li Z, Wang W, Li H, et al. (2022) BEVFormer: Learning bird’s-eye-
                    spatial attention in vision transformers. Advances in Neural Infor-      view representation from multi-camera images via spatiotemporal
                                                                                                                                          ´
                    mation Processing Systems 34: 9355–9366.                                 transformers. In: Avidan S, Brostow G, Cisse M, et al. (eds) Eur-
                ChuX,TianZ,ZhangB,etal.(2021b)Conditional positional encod-                  opean Conference on Computer Vision. Cham: Springer, pp. 1–18.
                    ings for vision transformers. Epub ahead of print 18 March. DOI:     Li Z, Yu Z, Wang W, et al. (2023c) FB-BEV: BEV representation
                    10.48550/ARXIV.2102.10882.                                               from forward-backward view transformations. In: Proceedings of
                Dai J, Qi H, Xiong Y, et al. (2017) Deformable convolutional net-            the IEEE/CVF international conference on computer vision, Paris,
                    works. In: Proceedings of the IEEE international conference on           1–6 October, pp. 6919–6928. New York: IEEE.
                    computervision, Venice, 22–29 December, pp.764–773.NewYork:          Liu H, Teng Y, Lu T, et al. (2023a) SparseBEV: High-performance
                    IEEE.                                                                    sparse 3D object detection from multi-camera videos. In: Proceed-
                Dosovitskiy A, Beyer L, Kolesnikov A, et al. (2021) An image is              ings of the IEEE/CVF international conference on computer vision,
                    worth 16x16 words: Transformers for image recognition at scale           Paris, 1–6 October, pp. 18580–18590. New York: IEEE.
                    (arXiv:2010.11929). arXiv. Available at: http://arxiv.org/abs/       Liu Y, Wang T, Zhang X, et al. (2022) PETR: Position embedding
                    2010.11929 (accessed 5 March 2024).                                      transformation for multi-view 3D object detection. In: Avidan S,
                                                                                                             ´
                Fan Q, Huang H, Chen M, et al. (2024) RMT: Retentive networks                BrostowG,CisseM,etal.(eds)EuropeanConferenceonComputer
                    meet vision transformers. In: Proceedings of the IEEE/CVF con-           Vision. Cham: Springer, pp. 531–548.
                    ference on computer vision and pattern recognition, Seattle, WA,     Liu Y, Yan J, Jia F, et al. (2023b) PETRv2: A unified framework for
                    16–22 June, pp. 5641–5651. New York: IEEE.                               3D perception from multi-camera images. In: Proceedings of the
                FanQ,HuangH,GuanJ,etal.(2023)Rethinkinglocal perceptionin                    IEEE/CVF international conference on computer vision, Paris, 1–6
                    lightweight vision transformer. Epub ahead of print 12 May. DOI:         October, pp. 3262–3272. New York: IEEE.
                    10.48550/ARXIV.2303.17803.                                           Liu Z, Lin Y, Cao Y, et al. (2021) Swin transformer: Hierarchical
                GuoJ, Han K, Wu H, et al. (2022) CMT: Convolutional neural net-              vision transformer using shifted windows. In: Proceedings of the
                    works meet vision transformers. In: Proceedings of the IEEE/CVF          IEEE/CVF international conference on computer vision, Montreal,
                    conference on computer vision and pattern recognition, New               QC,Canada,10–17October,pp.10012–10022. NewYork:IEEE.
                    Orleans, LA, 18–24 June, pp. 12175–12185. New York: IEEE.            Lu T, Ding X, Liu H, et al. (2023) LinK: Linear Kernel for LiDAR-
                Han C, Yang J, Sun J, et al. (2024) Exploring recurrent long-term            based 3D perception. In: Proceedings of the IEEE/CVF conference
                    temporal fusion for multi-view 3D perception. IEEE Robotics and          on computer vision and pattern recognition, Vancouver, BC,
                    Automation Letters 9: 6544–6551.                                         Canada, 17–24 June, pp. 1105–1115. New York: IEEE.
                Hassani A, Walton S, Li J, et al. (2023) Neighborhood attention          PanZ,CaiJandZhuangB(2022)FastvisiontransformerswithHiLo
                    transformer. In: Proceedings of the IEEE/CVF conference on com-          attention. Advances in Neural Information Processing Systems 35:
                    puter vision and pattern recognition, Vancouver, BC, Canada, 17–         14541–14554.
                    24 June, pp. 6185–6194. New York: IEEE.                              Park D, Ambrus R, Guizilini V, et al. (2021) Is pseudo-LiDAR
                Hu Y, Yang J, Chen L, et al. (2023) Planning-oriented autonomous             needed for monocular 3D object detection? In: Proceedings of the
                    driving. In: Proceedings of the IEEE/CVF conference on computer          IEEE/CVF international conference on computer vision, Montreal,
                    vision and pattern recognition, Vancouver, BC, Canada, 17–24             QC,Canada,10–17October,pp.3142–3152.NewYork:IEEE.
                    June, pp. 17853–17862. New York: IEEE.                               Park J, Xu C, Yang S, et al. (2022) Time will tell: New outlooks and a
                Huang J and Huang G (2022) BEVDet4D: Exploit temporal cues in                baseline for temporal multi-view 3D object detection. In: The ele-
                    multi-camera 3D object detection. Arxiv:2203.17054. Available at:        venth international conference on learning representations. Avail-
                    https://arxiv.org/abs/2203.17054                                         able at: https://arxiv.org/abs/2210.02443
                HuangJ,HuangG,ZhuZ,etal.(2022a)BEVDet:High-performance                   Philion J and Fidler S (2020) Lift, splat, shoot: Encoding images from
                    multi-camera 3D object detection in bird-eye-view (arXiv:                arbitrary camera rigs by implicitly unprojecting to 3D. In: Vedaldi
                    2112.117900). arXiv. Available at: http://arxiv.org/abs/2112.11790       A, Bischof H, Brox T, et al. (eds) Computer Vision–ECCV 2020:
                    (accessed 12 August 2024).                                               16th European Conference Glasgow UK, August 23–28, 2020, Pro-
                                                                                             ceedings Part XIV 16. Cham: Springer, pp. 194–210.
