                                    8       Wen. et al.
                                         In the rest of this section, we will elaborate on the design of PPTr in detail.
                                    We start with how we fit primitive planes and how we pre-compute primitive
                                    features in Section 4.1. Then in Section 4.2 and Section 4.3, we explain how we
                                    extract short-term and long-term spatial-temporal features respectively.
                                    4.1   Primitive Fitting and Feature Pre-Computation
                                         We represent a point cloud sequence as Ψ = {(P ,V )|t = 1,...,L}, where
                                                                                                t   t
                                    P is the point cloud of frame t optionally accompanied with normals V . In
                                     t                                                                                  t
                                    this phase, we detect planes for each frame (P ,V ) and output primitive label
                                                                                         t  t
                                             N×3                                           M×4
                                    Ξ ∈ R          and primitive parameters Θ ∈ R              , where N is the number
                                     t                                             t
                                    of points and M is the number of primitives. We adopt two primitive fitting
                                    methods in our study for different datasets: region grow [31] and RANSAC [13].
                                         We leverage region grow for indoor and outdoor scene segmentation. Re-
                                    gion grow detects planes based on normal estimation. If not provided with nor-
                                    mal Vt, we calculate the normal direction at each point beforehand by linear
                                    least squares fitting of a plane over its nearest k neighbors. Compared with re-
                                    gion grow, RANSAC does not require normal estimation and is more suitable
                                    for low-resolution point clouds such as those for action recognition in MSR-
                                    Action3D [25].
                                         After primitive fitting, we pre-compute the primitive features for efÏcient
                                    long-term context aggregation and form a memory pool F                 as shown in Fig-
                                                                                                     mem
                                    ure 4. Specifically, we pre-train a 3D point feature learner [10] to solve the task of
                                    interest just from every single frame (P ,V ). This allows us to extract per-point
                                                                               t   t
                                    features Ft ∈ RC×N where C denotes the feature dimension. To extract primi-
                                    tive level representations, point-wise max pooling is adopted for each primitive
                                    plane. The final memory pool F           has a shape of RC×M×L.
                                                                        mem
                                    4.2   Short-Term Spatial-Temporal Feature Extraction
                                         This branch mainly consists of a 4D backbone and an intra-primitive point
                                    transformer. The per-point features of each 4D sequence are first extracted using
                                    the 4D backbone. Following that, an intra-primitive point transformer is used
                                    to extract low-level features. Point features can provide the most fine-grained
                                    information, enabling us to better perform dense prediction tasks. The intra-
                                    primitive point transformer can not only align point features of similar geometry
                                    but also save computational overhead and reduce the optimization difÏculty of
                                    the transformer.
                                    4D Backbone. Our 4D backbone is built using a UNet structure. Following
                                    the state-of-the-art P4Transformer [10], the encoder/decoder is made up of four
                                    4D convolution/decovolution layers. Given clip Ψ, the convolution layer can be
                                    described as:
                                                  rt
                                     ′(x,y,z)    X X                                       T              (x+δ ,y+δ ,z+δ )
                                    f        =                        (W ·(δ ,δ ,δ ,δ ) )⊙(W ·f                x    y    z )
                                     t                                   d     x  y   z  t           f    t+δ
                                                                                                             t
                                                δ =−r ∥(δ ,δ ,δ )∥≤r
                                                 t    t   x y z      s
                                                                                                                          (1)
