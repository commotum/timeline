                  question, numbers, or dates, which allows for easy       et al., 2019), tracking entity state changes (Mishra
                  and accurate evaluation metrics.                         et al., 2018; Ostermann et al., 2018) or a particular
                     Wepresent an analysis of the resulting dataset        kind of “multi-step” reasoning over multiple doc-
                  to show what phenomena are present. We ﬁnd               uments (Welbl et al., 2018; Khashabi et al., 2018).
                  that many questions combine complex question se-         Similar facets are explored in medical domain
                                                                                                              ˇ
                  mantics with SQuAD-style argument ﬁnding; e.g.,          datasets (Pampari et al., 2018; Suster and Daele-
                  in the ﬁrst question in Table 1, BiDAF correctly         mans,2018)whichcontainautomaticallygenerated
                  ﬁnds the amount the painting sold for, but does not      queries on medical records based on predeﬁned
                  understand the question semantics and cannot per-        templates. We applaud these efforts, which offer
                  form the numerical reasoning required to answer          good avenues to study these additional phenomena.
                  the question. Other questions, such as the ﬁfth          However, we are concerned with paragraph under-
                  question in Table 1, require ﬁnding all events in the    standing, which on its own is far from solved, so
                  passage that match a description in the question,        DROPhasnoneoftheseadditional complexities.
                  then aggregating them somehow (in this instance,         It consists of single passages of text paired with
                  by counting them and then performing an argmax).         independent questions, with only linguistic facil-
                                                                                                                   1
                  Very often entity coreference is required. Table 1       ity required to answer the questions.     One could
                  gives a number of different phenomena, with their        argue that we are adding numerical reasoning as
                  proportions in the dataset.                              an “additional complexity”, and this is true; how-
                     Weused three types of systems to judge base-          ever, it is only simple reasoning that is relatively
                  line performance on DROP: (1) heuristic baselines,       well-understood in the semantic parsing literature,
                  to check for biases in the data; (2) SQuAD-style         and we use it as a necessary means to force more
                  reading comprehension methods; and (3) semantic          comprehensive passage understanding.
                  parsers operating on a pipelined analysis of the pas-       Many existing algebra word problem datasets
                  sage. Thereadingcomprehensionmethodsperform              also contain similar phenomena to what is in
                  the best, with our best baseline achieving 32.7%         DROP(Koncel-Kedziorski et al., 2015; Kushman
                  F onourgeneralized accuracy metric, while ex-            et al., 2014; Hosseini et al., 2014; Clark et al., 2016;
                    1
                  pert human performance is 96.4%. Finally, we             Ling et al., 2017). Our dataset is different in that it
                  contribute a new model for this task that combines       uses much longer contexts, is more open domain,
                  limited numerical reasoning with standard reading        and requires deeper paragraph understanding.
                  comprehension methods, allowing the model to an-            Semantic parsing The semantic parsing litera-
                  swer questions involving counting, addition and          ture has a long history of trying to understand com-
                  subtraction. This model reaches 47% F , a 14.3%          plex, compositional question semantics in terms of
                                                            1
                  absolute increase over the best baseline system.         somegroundedknowledgebaseorother environ-
                     Thedataset, code for the baseline systems, and        ment (Zelle and Mooney, 1996; Zettlemoyer and
                  a leaderboard with a hidden test set can be found        Collins, 2005; Berant et al., 2013a, inter alia). It
                  at https://allennlp.org/drop.                            is this literature that we modeled our questions on,
                                                                           particularly looking at the questions in the Wik-
                  2   Related Work                                         iTableQuestions dataset (Pasupat and Liang, 2015).
                  QuestionansweringdatasetsWithsystemsreach-               If we had a structured, tabular representation of
                                                                           the content of our paragraphs, DROP would be
                  ing human performance on the Stanford Question           largely the same as WikiTableQuestions, with simi-
                  Answering Dataset (SQuAD) (Rajpurkar et al.,             lar (possibly even simpler) question semantics. Our
                  2016), many follow-on tasks are currently being          noveltyisthatwearetheﬁrsttocombinethesecom-
                  proposed. All of these datasets throw in additional      plex questions with paragraph understanding, with
                  complexities to the reading comprehension chal-          the aim of encouraging systems that can produce
                  lenge, around tracking conversational state (Reddy       comprehensive structural analyses of paragraphs,
                  et al., 2019; Choi et al., 2018), requiring passage      either explicitly or implicitly.
                  retrieval (Joshi et al., 2017; Yang et al., 2018; Tal-      Adversarial dataset construction We continue
                  morandBerant,2018), mismatched passages and
                                                         ´                    1Somequestions in our dataset require limited sports do-
                  questions (Saha et al., 2018; Kocisky et al., 2018;      mainknowledgetoanswer;weexpectthatthereareenough
                  Rajpurkaretal.,2018), integrating knowledgefrom          such questions that systems can reasonably learn this knowl-
                  external sources (Mihaylov et al., 2018; Zhang           edge from the data.
                                                                       2369
