                1544                                      G.Hinton,S.Osindero,andY.-W.Teh
                Table1: ErrorratesofVariousLearningAlgorithmsontheMNISTDigitRecog-
                nition Task.
                Version of MNIST Task                  Learning Algorithm             Test Error %
                Permutationinvariant                  Ourgenerativemodel:                1.25
                                                  784 →500→500↔2000↔10
                Permutationinvariant             Supportvectormachine:degree9            1.4
                                                        polynomialkernel
                Permutationinvariant            Backprop: 784 → 500 → 300 → 10           1.51
                                                  cross-entropy and weight-decay
                Permutationinvariant                Backprop: 784 → 800 → 10             1.53
                                                 cross-entropy and early stopping
                Permutationinvariant            Backprop: 784 → 500 → 150 → 10           2.95
                                                 squarederrorandon-lineupdates
                Permutationinvariant           Nearest neighbor: all 60,000 examples     2.8
                                                          andL3norm
                Permutationinvariant           Nearest neighbor: all 60,000 examples     3.1
                                                          andL2norm
                Permutationinvariant          Nearest neighbor: 20,000 examples and      4.0
                                                            L3norm
                Permutationinvariant          Nearest neighbor: 20,000 examples and      4.4
                                                            L2norm
                Unpermutedimages;extra             Backprop: cross-entropy and           0.4
                  data from elastic           early-stoppingconvolutionalneuralnet
                  deformations
                Unpermutedde-skewed              Virtual SVM: degree 9 polynomial        0.56
                  images; extra data from 2                  kernel
                  pixel translations
                Unpermutedimages                Shape-context features: hand-coded       0.63
                                                            matching
                Unpermutedimages;extra          BackpropinLeNet5:convolutional           0.8
                  data from afﬁne                           neural net
                  transformations
                Unpermutedimages                BackpropinLeNet5:convolutional           0.95
                                                            neural net
                    Abetter method is to ﬁrst ﬁx the binary states of the 500 units in the
                lower layer of the associative memory and to then turn on each of the
                label units in turn and compute the exact free energy of the resulting
                510-component binary vector. Almost all the computation required is in-
                dependent of which label unit is turned on (Teh & Hinton, 2001), and this
                methodcomputestheexactconditionalequilibriumdistributionoverlabels
                instead of approximating it by Gibbs sampling, which is what the previ-
                ous method is doing. This method gives error rates that are about 0.5%
                higher than the ones quoted because of the stochastic decisions made in
                the up-pass. We can remove this noise in two ways. The simpler is to make
                the up-pass deterministic by using probabilities of activation in place of
