                   Table 1. Comparisons between existing MLLM benchmarks. “H/G Evaluation” denotes whether human or GPT is used for evaluation.
                      Benchmark             Visual Modality       Evaluation Level   Customized Question    #AnswerAnnotation      AnswerType      H/GEvaluation     #Models
                   LLaVA-Bench[24]              Image                   L                     ✓                     150              free-form          GPT             4
                                                                          1
                    OCR-Bench[26]               Image                   L                     ✗                       -              free-form          N/A             6
                                                                          1
                       MME[11]                  Image                   L                     ✓                     2194               Y/N              N/A             10
                                                                          1
                     M3Exam[46]                 Image                   L                     ✓                    12317             A/B/C/D            N/A             7
                                                                          1
                      LAMM[42]          Image(s) & Point cloud          L                     ✗                       -              free-form          GPT             4
                                                                          1
                   LVLM-eHub[40]                Image                   L                     ✗                       -              free-form         Human            8
                                                                          1
                     MMBench[25]               Image(s)                 L                     ✓                     2974             free-form          GPT             14
                                                                          1
                    VisIT-Bench [5]             Images                  L                     ✓                     592              free-form      Human/GPT           14
                                                                          1
                     MM-VET[43]                 Image                   L                     ✓                     200              free-form          GPT             9
                                                                          1
                     Touchstone [3]            Image(s)                 L                     ✓                     908              free-form          GPT             7
                                                                          1
                   SciGraphQA[20]               Image                   L                     ✓                      3K              free-form          N/A             4
                                                                          1
                          Ours             Image(s) & Video             L                     ✓                    24371             A/B/C/D            N/A             22
                                                                          3
                 summarize valuable observations. By revealing the limi-                           ligence (AGI) since a human-level AI should be able to ef-
                 tations of existing MLLMs through extensive evaluations,                          fortlessly digest and create multimodal content. In the ca-
                 we aim for SEED-Bench to provide insights that will mo-                           pability pyramid, higher levels inherently include the capa-
                 tivate future research toward the goal of General Artificial                      bilities of lower tiers. This hierarchical categorization not
                 Intelligence. Dataset and evaluation code are available at                        only clearly illustrates the current progress of MLLMs, but
                 https://github.com/AILab-CVC/SEED-Bench.                                          also provides a well-defined roadmap for future research.
                                                                                                       Wepropose SEED-Bench, a comprehensive benchmark
                 1. Introduction                                                                   that evaluates the hierarchical capabilities of MLLMs up
                                                                                                   to L3, including the generation of both texts and images
                 In recent years, Large Language Models (LLMs) [7, 10,                             given interleaved image-text inputs. As shown in Fig. 1,
                 30, 31, 37] have exhibited remarkable capabilities to under-                      SEED-Bench consists of three parts, where part-1 consti-
                                                                                                   tutes capability level L         for images and texts comprehen-
                 stand, reason, and generate texts across a variety of open-                                                      1
                 ended tasks. Leveraging the strong generality of LLMs,                            sion, part-1&2 constitute capability level L2 for interleaved
                 Multimodal Large Language Models (MLLMs) [2, 8, 15–                               image-text comprehension, and part-1&2&3 constitute ca-
                 19, 23, 24, 27, 28, 32, 32, 34, 41, 45, 47] have demon-                           pability level L3 for image and text generation. To the best
                 strated exceptional capabilities in comprehending multi-                          of our knowledge, SEED-Bench is the first benchmark that
                 modal data through predicting open-form texts.                    Recent          provides hierarchical evaluations of MLLMs, which effec-
                 work [9, 13, 14, 21, 36, 39] further empower LLMs with                            tively showcases the range of model capabilities.
                 the ability to generate images beyond texts (acting like                              Specifically, SEED-Bench consists of 24K multiple-
                 a combination of GPT-4V [1] and DALL-E 3 [4]), since                              choice questions with ground truth answers derived from
                 they contend that the premise for the emergence of multi-                         humanannotation(×10largerthanMME[11]and×8larger
                 modalcapabilities is that text and image can be represented                       than MMBench [25] as shown in Tab. 1). SEED-Bench
                 and processed interchangeably in a unified autoregressive                         spans 27 evaluation dimensions, enabling a comprehen-
                 Transformer. However, despite the extensive capabilities of                       sive assessment of MLLMs’ performance across diverse
                 MLLMs, existing MLLM benchmarks [3, 11, 25, 40, 42]                               aspects. We employ three approaches for the generation
                 primarily focus on evaluating single image-text comprehen-                        of multiple-choice questions, including (1) a sophisticated
                 sion, thus failing to fully demonstrate the progress and lim-                     pipeline utilizing foundation models, (2) the adaptation of
                 itations of current MLLMs. The lag of benchmarks behind                           existing datasets, and (3) a combination of human cre-
                 the rapid development of MLLMs hinders the exploration                            ation and GPT assistance. We further incorporate an au-
                 and evolution of models.                                                          tomated filtering mechanism and manual verification pro-
                    In this work, we categorize the capabilities of MLLMs                          cess to ensure the quality of questions and the accuracy
                 into hierarchical levels ranging from L                to L based on              of ground truth answers. Different from existing MLLM
                                                                     0        4
                 the modalities they can accept and generate, as depicted                          benchmarks [3, 5, 24, 25, 40, 42, 43] that employ hu-
                 in Fig. 1. Building upon LLMs, the lowest-tier capability                         man annotators or GPT to evaluate open-form output, re-
                 L involves generating texts given text inputs, while the                          sulting in compromised efficiency, increased subjectivity,
                   0
                 highest-tier capability L4 entails producing open-form in-                        and reduced assessment accuracy, SEED-Bench provides
                 terleaved image and text output given arbitrary interleaved                       multiple-choice questions, which restricts the model’s out-
                 image-text inputs.        Reaching the capability L             is a cru-         puttoA/B/C/Doptions. Thisapproachfacilitatestheconve-
                                                                              4
                 cial milestone on the path towards General Artificial Intel-                      nient computation of accuracy, serving as an objective met-
                                                                                               13300
