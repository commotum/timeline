                           approach uses augmentations at every part of the pipeline. Specifically, we
                           use rotations, transpositions, and permutations of the colors and the order
                           of examples, as these preserve the underlying task structure. We use these
                           augmentations not only to increase the amount of training data, but also to
                           increase the variety in our predictions and to provide a way to filter wrong
                           solutions.  This contributes significantly to the performance, and will be
                           discussed more extensively in Section 3.3.
                           Models:
                           We use the augmented data to fine-tune decoder-only LLMs. Being con-
                           strained by Kaggleâ€™s 2 Nvidia T4 GPUs, the models have to meet two main
                           requirements: a maximal memory usage of 16GB during training and infer-
                           ence, and a minimumcontextsizeof8000tokenstohandleinferenceonlarger
                           problems. We want to point out two models that worked particularly well
                           in our case: Mistral-NeMo-Minitron-8B-Base [5] and an uncensored version
                           of Llama-3.2-3B-instruct [14].
                           Preliminary & Secondary Fine-Tuning:
                           Ourmodelsareinitially trained on Re-ARC and 75% of the ARC-AGI public
                           evaluation dataset, leveraging LoRA adapters [15] as a fine-tuning method.
                           Thefine-tunedmodelissubsequentlyuploadedtoKaggle,whereitundergoes
                           additional fine-tuning on the hidden dataset. For more comprehensive details
                           on training and hyperparameters, please refer to Section 3.4.
                           Candidate Generation:
                           After fine-tuning has completed, the final model is used to generate solution
                           candidates. We found that both greedy and multinomial sampling provided
                           suboptimal performance for the benchmark. Instead, we designed a cus-
                           tom generator that leverages depth-first search (DFS) to extract all possible
                           completions with a sampling probability exceeding a specified cutoff value.
                           This DFS approach is applied to both the original task as well as its "aug-
                           mented" versions, resulting in a list of candidates for each task. This not
                           only improved our score, but also the inference time needed. For an in-depth
                           discussion on inference see Section 3.5.
                           Candidate Selection:
                           Finally, given the generated list of candidates, we use the aggregated log-
                           softmax scores assigned by the fine-tuned model to select two of them for
                           submission. To make sure that these values are informative we compute
                           them over several augmentations of the task. This selection algorithm is
                           highly effective, provided that the correct candidate is among those gener-
                           ated. For a detailed explanation of our candidate selection process, refer to
                           Section 3.6.
                                                                   4
