            Kolmogorov 
            Complexity 
            and Algorithmic
            Randomness
            A.Shen
            V. A. Uspensky
            N. Vereshchagin
            American Mathematical Society
      Kolmogorov 
      Complexity 
      and Algorithmic 
      Randomness
     Mathematical
        Surveys
           and
      Monographs 
      Volume 220
                 Kolmogorov 
                Complexity 
                and Algorithmic 
                Randomness
                A.Shen
                V. A. Uspensky
                N. Vereshchagin
                American Mathematical Society
                Providence, Rhode Island
                                                  EDITORIAL COMMITTEE
                                        Robert Guralnick                       Benjamin Sudakov
                                        Michael A. Singer, Chair               Constantin Teleman
                                                           Michael I. Weinstein
                                      H.  К.  Верещагин,  В.  А.  Успенский,  А.  Шень 
                          Колмогоровская сложность и алгоритмическая случайность 
                                                       МИНМО, Москва, 2013
                     2010 Mathematics Subject Classification.  Primary 68Q30, 03D32, 60A99, 97K70.
                                   For additional information and updates on this book, visit 
                                               www.ams.org/bookpages/surv-220
                Library of Congress Cataloging-in-Publication Data
                Names:  Shen,  A.  (Alexander),  1958-  |  Uspenskiï,  V.  A.  (Vladimir  Andreevich)  |  Vereshchagin, 
                    N. K.,  1928-
                Title:  Kolmogorov complexity and algorithmic randomness / A. Shen, V. A.  Uspensky, N. Vere­
                    shchagin.
                Other titles:  Kolmogorovskaya slozhnost i algoritmieskaya sluchanost.  English
                Description:  Providence, Rhode Island :  American Mathematical Society,  [2017]  |  Series:  Mathe­
                    matical surveys and monographs ; volume 220 | Includes bibliographical references and index.
                Identifiers:  LCCN 2016049293 |  ISBN 9781470431822 (alk.  paper)
                Subjects:  LCSH:  Kolmogorov complexity.  |  Computational complexity.  |  Information theory.  | 
                    AMS: Computer science - Theory of computing - Algorithmic information theory (Kolmogorov 
                    complexity,  etc.),  msc  |  Mathematical  logic  and  foundations - Computability  and recursion 
                    theory  -  Algorithmic  randomness  and  dimension,  msc  |  Probability  theory  and  stochastic 
                    processes - Foundations of probability theory - None of the above, but in this section,  msc  | 
                    Mathematics education - Combinatorics, graph theory,  probability theory, statistics - Foun­
                    dations and methodology of statistics,  msc
                Classification:    LCC  QA267.7  .S52413  2017  |  DDC  511.3/42-dc23  LC  record  available  at
                    https://lccn.loc.gov/2016049293
                Copying and reprinting.  Individual readers of this publication, and nonprofit libraries acting 
                for  them,  are  permitted  to  make  fair  use  of the  material,  such  as  to  copy  select  pages  for  use 
                in  teaching or  research.  Permission  is  granted  to  quote  brief passages  from  this  publication  in 
                reviews, provided the customary acknowledgment of the source is given.
                    Republication, systematic copying, or multiple reproduction of any material in this publication 
                is  permitted only  under  license from the  American Mathematical Society.  Permissions to reuse 
                portions of AMS publication content are handled by Copyright Clearance Center’s RightsLink® 
                service.  For more information, please visit:  http://www.ams.org/rightslink.
                    Send requests for translation rights and licensed reprints to reprint-permission@ams.org.
                    Excluded from these provisions is material for which the author holds copyright.  In such cases, 
                requests for permission to reuse or reprint material should be addressed directly to the author (s). 
                Copyright ownership is indicated on the copyright page, or on the lower right-hand corner of the 
                first  page of each article within proceedings volumes.
                                               ©2017 by the authors.  All rights reserved.
                                                Printed in the United States of America.
                             @ The paper used in this book is acid-free and falls within the guidelines 
                                           established to ensure permanence and durability.
                                          Visit the AMS home page at http://www.ams.org/
                                               10 987654321                22 21 20 19 18 17
                    To the memory of Andrei Muchnik
                                                             Contents
                Preface                                                                                                   xi
                    Acknowledgments                                                                                     xiii
                Basic notions and notation                                                                               xv
                Introduction.  What is this book about?                                                                    1
                    What is Kolmogorov complexity?                                                                         1
                    Optimal description modes                                                                              2
                    Kolmogorov complexity                                                                                  4
                    Complexity and information                                                                             5
                    Complexity and randomness                                                                              8
                    Non-computability of C and Berry’s paradox                                                             9
                    Some applications of Kolmogorov complexity                                                           10
                Chapter 1.  Plain Kolmogorov complexity                                                                  15
                    1.1.   Definition and main properties                                                                15
                    1.2.   Algorithmic properties                                                                        21
                Chapter 2.  Complexity of pairs and conditional complexity                                               31
                    2.1.   Complexity of pairs                                                                           31
                    2.2.   Conditional complexity                                                                        34
                    2.3.   Complexity as the amount of information                                                       45
                Chapter 3.  Martin-Löf randomness                                                                        53
                   3.1.    Measures on Œ                                                                                 53
                   3.2.    The Strong Law of Large Numbers                                                               55
                   3.3.    Effectively null sets                                                                         59
                   3.4.    Properties of Martin-Löf randomness                                                           65
                   3.5.    Randomness deficiencies                                                                       70
                Chapter 4.  A priori probability and prefix complexity                                                   75
                   4.1.    Randomized algorithms and semimeasures on N                                                   75
                   4.2.    Maximal semimeasures                                                                          79
                   4.3.    Prefix machines                                                                               81
                   4.4.    A digression:  Machines with self-delimiting input                                            85
                   4.5.    The main theorem on prefix complexity                                                         91
                   4.6.    Properties of prefix complexity                                                               96
                   4.7.    Conditional prefix complexity and complexity of pairs                                        102
                Chapter 5.  Monotone complexity                                                                         115
                   5.1.    Probabilistic machines and semimeasures on the tree                                          115
                                                                     vii
                viii                                          CONTENTS
                   5.2.    Maximal semimeasure on the binary tree                                                    121
                   5.3.    A priori complexity and its properties                                                    122
                   5.4.    Computable mappings of type £ -» £                                                        126
                   5.5.    Monotone complexity                                                                       129
                   5.6.    Levin-Schnorr theorem                                                                     144
                   5.7.    The random number Q                                                                       157
                   5.8.    Effective Hausdorff dimension                                                             172
                   5.9.    Randomness with respect to different measures                                             176
                Chapter 6.  General scheme for complexities                                                          193
                   6.1.   Decision complexity                                                                        193
                   6.2.    Comparing complexities                                                                    197
                   6.3.   Conditional complexities                                                                  200
                   6.4.   Complexities and oracles                                                                  202
               Chapter 7.  Shannon entropy and Kolmogorov complexity                                                213
                   7.1.   Shannon entropy                                                                           213
                   7.2.   Pairs and conditional entropy                                                             217
                   7.3.   Complexity and entropy                                                                    226
               Chapter 8.  Some applications                                                                        233
                   8.1.   There are infinitely many primes                                                          233
                   8.2.   Moving information along the tape                                                         233
                   8.3.   Finite automata with several heads                                                        236
                   8.4.   Laws of Large Numbers                                                                     238
                   8.5.   Forbidden substrings                                                                      241
                   8.6.   A proof of an inequality                                                                  255
                   8.7.   Lipschitz transformations are not transitive                                              258
               Chapter 9.  Frequency and game approaches to randomness                                              261
                   9.1.   The original idea of von Mises                                                            261
                   9.2.   Set of strings as selection rules                                                         262
                   9.3.   Mises-Church randomness                                                                   264
                   9.4.   Ville’s example                                                                           267
                   9.5.   Martingales                                                                               270
                   9.6.   A digression:  Martingales in probability theory                                          275
                   9.7.   Lower semicomputable martingales                                                          277
                   9.8.   Computable martingales                                                                    279
                   9.9.   Martingales and Schnorr randomness                                                        282
                   9.10.    Martingales and effective dimension                                                     284
                   9.11.    Partial selection rules                                                                 287
                   9.12.    Non-monotonic selection rules                                                           291
                   9.13.    Change in the measure and randomness                                                    297
               Chapter 10.  Inequalities for entropy, complexity, and size                                          313
                   10.1.    Introduction and summary                                                                313
                   10.2.    Uniform sets                                                                            318
                   10.3.    Construction of a uniform set                                                           321
                   10.4.    Uniform sets and orbits                                                                 323
                   10.5.    Almost uniform sets                                                                     324
                                                           CONTENTS                                              ix
                  10.6.    Typization trick                                                                    326
                  10.7.    Combinatorial interpretation:  Examples                                             328
                  10.8.    Combinatorial interpretation:  The general  case                                    330
                  10.9.    One more combinatorial interpretation                                               332
                  10.10.    The inequalities for two and three strings                                        336
                  10.11.    Dimensions and Ingleton’s inequality                                              337
                  10.12.    Conditionally independent random variables                                        342
                  10.13.    Non-Shannon inequalities                                                          343
               Chapter 11.  Common information                                                                351
                  11.1.    Incompressible representations of strings                                          351
                  11.2.    Representing mutual information as a string                                        352
                  11.3.    The combinatorial meaning of common information                                    357
                  11.4.    Conditional independence and common information                                    362
               Chapter 12.  Multisource algorithmic information theory                                        367
                  12.1.    Information transmission requests                                                  367
                  12.2.    Conditional encoding                                                               368
                  12.3.    Conditional codes:  Muchnik’s theorem                                              369
                  12.4.    Combinatorial interpretation of Muchnik’s theorem                                  373
                  12.5.   A digression:  On-line matching                                                     375
                  12.6.   Information distance and simultaneous encoding                                      377
                  12.7.    Conditional codes for two conditions                                               379
                  12.8.   Information flow and network cuts                                                   383
                  12.9.   Networks with one source                                                            384
                  12.10.    Common information as an information request                                      388
                  12.11.    Simplifying a program                                                             389
                  12.12.    Minimal sufficient statistics                                                     390
               Chapter 13.  Information and logic                                                             401
                  13.1.   Problems, operations, complexity                                                    401
                  13.2.   Problem complexity and intuitionistic logic                                         403
                  13.3.   Some formulas and their complexity                                                  405
                  13.4.   More examples and the proof of Theorem 238                                          408
                  13.5.   Proof of a result similar to Theorem 238 using Kripke models                        413
                  13.6.   A problem whose complexity is not expressible in terms of the
                            complexities of tuples                                                            417
               Chapter 14.  Algorithmic statistics                                                            425
                  14.1.   The framework and randomness deficiency                                             425
                  14.2.   Stochastic objects                                                                  428
                  14.3.   Two-part descriptions                                                               431
                  14.4.   Hypotheses of restricted type                                                       438
                  14.5.   Optimality and randomness deficiency                                                447
                  14.6.   Minimal hypotheses                                                                  450
                  14.7.   A bit of philosophy                                                                 452
               Appendix 1.  Complexity and foundations of probability                                         455
                  Probability theory paradox                                                                  455
                  Current best practice                                                                       455
                                                 CONTENTS
               Simple events and events specified in advance                                456
               Frequency approach                                                          458
               Dynamical and statistical laws                                              459
               Are “real-life”  sequences complex?                                         459
               Randomness as ignorance:  Blum-Micali-Yao pseudo-randoinness                460
               A digression:  Thermodynamics                                               461
               Another digression:  Quantum mechanics                                      463
            Appendix 2.  Four algorithmic faces of randomness                              465
               Introduction                                                                465
               Face one:  Frequency stability and stochasticness                           468
               Face two:  Chaoticness                                                      470
               Face three:  Typicalness                                                    475
               Face four:  Unpredictability                                                476
               Generalization for arbitrary computable distributions                       480
               History and bibliography                                                    486
            Bibliography                                                                   491
            Name Index                                                                     501
            Subject Index                                                                  505
                                                            Preface
                     The notion of algorithmic  complexity  (also sometimes  called  algorithmic  en­
                tropy)  appeared in the  1960s in  between the theory of computation,  probability 
               theory, and information theory.
                     The  idea  of A.  N.  Kolmogorov  was  to  measure  the  amount  of information 
               in finite  objects  (and not  in random variables,  as it  is done in classical  Shannon 
               information theory).  His famous paper  [78], published in  1965, explains how this 
               can be done (up to a bounded additive term) using the algorithmic approach.
                     Similar ideas were suggested a few years earlier by R.  Solomonoff (see  [187] 
               and his other papers; the historical account and reference can be found in [103]).1 
               The motivation of Solomonoff was quite different.  He tried to define the notion 
               of  a  priori probability.  Imagine  there  is some  experiment  (random  process)  and 
               we know nothing about its internal structure.  Can we say something about  the 
               probabilities  of different  outcomes  in  this  situation?  One  can  relate  this  to  the 
               complexity measures saying that simple objects have greater a priori probability 
               than complex ones.  (Unfortunately, Solomonoff’s work become popular only after 
               Kolmogorov mentioned it in his paper.)
                    In  1965  G.  Chaitin  (then  an  18-year-old  undergraduate  student)  submitted 
               two papers [28]  and  [29];  they were published in 1966 and  1969,  respectively.  In 
               the  second  paper  he  proposed  the  same  definition  of algorithmic  complexity  as 
               Kolmogorov.
                    The basic properties of Kolmogorov complexity were established in the 1970s. 
               Working independently,  С.  P.  Schnorr and L.  Levin  (who was a student  of Kol­
               mogorov) found a link between complexity and the notion of algorithmic random­
               ness (introduced in 1966 by P. Martin-Löf [115]).  To achieve this, they introduced 
               a slightly different version of complexity, the so-called monotone complexity.  Also 
               Solomonoff’s ideas about a priori probability were formalized in the form of prefix 
               complexity,  introduced by Levin and later by Chaitin.  The notions of complexity 
               turned out to be useful both for theory of computation and probability theory.
                    Kolmogorov complexity became popular (and for a good reason: it is a basic and 
               philosophically important notion of algorithm theory)  after M.  Li and P. Vitânyi 
               published  a book on the subject  [103]  (first  edition  appeared  in  1993).  Almost 
               everything about Kolmogorov complexity that was known at the moment was cov­
               ered in the book or at least mentioned as an exercise.  This book also provided a 
               detailed historical account, references to first publications, etc.  Then the books of
               C. Calude [25] and A. Nies [147] appeared, as well as the book of R. Downey and
               D. Hirschfeldt  [49].  These books cover many interesting results obtained recently
                    1Kolmogorov wrote  in  [79],  “I  came  to  a  similar  notion  not  knowing  about  Solomonoff’s 
               work.”
                                                                  XI
        xii                    PREFACE
        (in  particular,  the  results  that  relate  complexity  and  randomness  with  classical 
        recursion theory).
          Our book does not try to be comprehensive (in particular, we do not say much 
        about the recent  results  mentioned  above).  Instead,  we tried to select  the  most 
        important topics and results (both from the technical and philosophical viewpoints) 
        and to explain them clearly.  We do not say much about the history of the topic: 
        as is usually done in textbooks, we formulate most statements without references, 
        but this does not mean (of course) any authorship claim.
          We start the book with a section  “What is this book about?”  where we try to 
        give a brief overview of the main ideas and topics related to Kolmogorov complexity 
        and algorithmic randomness so the reader can browse this section to decide whether 
        the book is worth reading.
          As an  appendix we reproduce the  (English translation)  of a small  brochure 
        written by one of the authors  (V.U.),  based on his talk for high school students 
        and undergraduates (July 23,  2005)  delivered during the  “Modern Mathematics” 
        Summer  School  (Dubna near  Moscow);  the  brochure  was  published  in  2006  by 
        MCCME publishing house (Moscow).  The lecture was devoted to different notions 
        of algorithmic randomness, and the reader who has no time or incentive to study 
        the corresponding chapters of the book in detail can still get some acquaintance 
        with this topic.
          Unfortunately, the notation and terminology related to Kolmogorov complexity 
        is not very logical (and different people often use different notation).  Even the same 
        authors use different notation in different papers.  For example, Kolmogorov used 
        both the letters К and H in his two basic publications [78,  79].  In [78]  he used 
        the term  “complexity”  and denoted the complexity of a string x by K(x).  Later 
        in  [79]  he used the term  “entropy”  (borrowed from Shannon information theory) 
        for  the  same  notion  that  was  called  “complexity”  in  [78].  Shannon  information 
        theory is based on probability theory; Kolmogorov had an ambitious plan to con­
        struct a parallel theory that does not depend on the notion of probability.  In [79] 
        Kolmogorov wrote, using the same word entropy in this new sense:
              The ordinary definition of entropy uses probability concepts, and 
              thus does not pertain to individual values,  but to random val­
              ues,  i.e.,  to  probability  distributions  within  a group  of values. 
              [...]  By far,  not all applications of information theory fit ratio­
              nally into such an interpretation of its basic concepts.  I believe 
              that the need for attaching definite meanings to the expressions 
              H(x\y) and I(x\y), in the case of individual values x and у that 
              are not  viewed as a result of random tests with a definite law 
              of distribution,  was realized long ago  by many who dealt with 
              information theory.
                As far as I know,  the first paper published on the idea of 
              revising information theory so as to satisfy the above conditions 
              was the article of Solomonoff [187].  I came to similar conclu­
              sions, before becoming aware of Solomonoff’s work in 1963-1964, 
              and published my first article on the subject [78] in early 1965.
                                ACKNOWLEDGMENTS                     xiii
                   The meaning of the new definition is very simple.  Entropy 
                H(x\y) is the minimal [bit] length of a [...] program P that per­
                mits construction of the value of x, the value of y being known,
                               #(я|г/)=  min  1{P).
                                       A(P,y)—x
                This concept is supported by the general theory of “computable”
                (partially  recursive)  functions,  i.e.,  by  theory  of algorithms  in 
                general.
                   [...]  The preceding rather superficial discourse should prove 
                two general theses.
                     1)  Basic  information  theory  concepts  must  and  can  be 
                founded without recourse to the probability theory, and in such 
                a manner that “entropy” and “mutual information” concepts are 
                applicable to individual values.
                     2) Thus introduced, information theory concepts can form 
                the basis of the term random,, which naturally suggests that ran­
                domness is the absence of regularities.2
            And earlier (April 23, 1965), giving a talk “The notion of information and the 
         foundations of the probability theory”  at the Institute of Philosophy of the USSR 
         Academy of Sciences, Kolmogorov said:
                So the two problems arise sequentially:
                     1.  Is  it  possible to free the information theory  (and the 
                notion of the  “amount of information” ) from probabilities?
                     2.  It is possible to develop the intuitive idea of randomness 
                as  incompressibility  (the  law  describing  the  object  cannot  be 
                shortened)?
         (The transcript of his talk was published in [85] on p.  126).
            So Kolmogorov uses the term  “entropy”  for the same notion that was named 
         “complexity”  in his first paper, and denotes it by letter H instead of К .
            Later the same notion was denoted by C (see, e.g.,  [103]) while the letter К 
         is  used  for  prefix  complexity  (denoted  by  KP(x)  in  Levin’s  papers  where  prefix 
         complexity was introduced).
            Unfortunately, attempts to unify the terminology and notation made by differ­
         ent people (including the authors) have lead mostly to increasing confusion.  In the 
         English version of this book we follow the terminology that is most used nowadays, 
         with few exceptions,  and we mention  the other  notation used.  For the reader’s 
         convenience, a list of notation used (p. xv) and index (p. 505) are provided.
                                Acknowledgments
            In the beginning of the 1980s Kolmogorov (with the assistance of A. Semenov) 
         initiated  a  seminar  at  the  Mathematics  and  Mechanics  Department  of Moscow 
         State  (Lomonosov)  University called  “Description and computation complexity”; 
         now the seminar (still active) is known as the “Kolmogorov seminar”.  The authors 
         are deeply grateful to their colleagues working in this seminar, including A. Zvonkin,
            2The published  English version of this paper  says  “random  is the absence of periodicity”, 
         but this evidently is a translation error, and we correct the text following the Russian version.
              XIV                                        PREFACE
              E.  Asarin,  V.  Vovk  (they  were  Kolmogorov’s  students),  S.  Soprunov,  V.  Vyu- 
              gin,  A.  Romashchenko,  M.  Vyalyi,  S.  Tarasov,  A.  Chernov,  M.  Vyugin,  S.  Posit- 
              selsky,  K.  Makarychev,  Yu.  Makarychev,  M.  Ushakov,  M.  Ustinov,  S.  Salnikov, 
              A.  Rumyantsev,  D. Musatov, V. Podolskii, I. Mezhirov, Yu. Pritykin, M. Raskin, 
              A.  Khodyrev,  P.  Karpovich,  A.  Minasyan,  E.  Kalinina,  G.  Chelnokov,  I.  Razen- 
              shteyn,  M.  Andreev,  A.  Savin,  M.  Dektyarev,  A.  Savchik,  A.  Kumok,  V.  Arzu­
              manyan,  A.  Makhlin,  G.  Novikov,  A.  Milovanov;  the book would not  have been 
              possible without them.
                   The authors express their gratitude to all the people who helped them to pre­
              pare the Russian edition, especially Marina Feigelman who made a drawing for its 
              cover,  Olga Lehtonen who performed the cover design,  and Victor Shuvalov who 
              helped a lot with typesetting.
                   The authors were supported by the International Science Foundation  (Soros 
              foundation),  STINT  (Sweden),  Russian  Fund  for  Basic  Research  (01-01-00493- 
              a,  01-01-01028-a,  06-01-00122-a,  09-01-00709-a,  12-01-00864-a),  CNRS,  and ANR 
              (France, ANR-08-EMER-008 NAFIT and ANR-15-CE40-0016-01 RaCAF grants).
                   The book was made possible by the generous support  of our colleagues,  in­
              cluding Bruno Bauwens, Laurent Bienvenu, Harry Buhrman,  Cris Calude, Bruno 
              Durand,  Péter  Gåcs,  Denis  Hirschfeldt,  Rupert  Hölzl,  Mathieu  Hoyrup,  Michal 
              Koucky, Leonid Levin, Wolfgang Merkle, Joseph Miller, Andre Nies,  Christopher 
              Porter, Jan Reimann, Jason Rute, Michael Sipser, Steven Simpson, Paul Vitånyi, 
              Sergey Vorobyov, and many others.
                   We are thankful to American Mathematical Society (in particular, Sergei Gel- 
              fand) for the suggestion to submit the book for publication in their book program 
              and for the kind permission to keep the book available freely in electronic form at 
              our homepages.  We thank the (anonymous) referees for their attention and sugges­
              tions,  and Jennifer Wright Sharp for correcting our (numerous)  English language 
              errors.
                   For many years the authors had the privilege to work in a close professional and 
              personal contact with Andrej Muchnik (1958-2007), an outstanding mathematician, 
              a  deep  thinker,  and  an  admirable  person,  who  participated  in  the  work  of the 
              Kolmogorov seminar and inspired a lot of work done in this seminar.  We devote 
              this book to his memory.
                                                               A.  Shen,  V.  Uspensky,  N.  Vereshchagin 
                                                                                        November 1,  2016
                                     Basic notions and notation
                   This section is intended for people who are already familiar with some notions 
              of Kolmogorov complexity and algorithmic randomness theory and want to take 
              a quick look at the terminology and notation used throughout  this book.  Other 
              readers can (and probably should) skip it and look back only when needed.
                   The set of all integer numbers is denoted by Z,  the notation N refers to the 
              set of all non-negative integers (i.e.,  natural numbers), R stands for the set of all 
              reals.  The set of all rational numbers is denoted by Q.  Dyadic rationale are those 
              rationale having the form m/2n for some integer m and n.
                   The cardinality of a set A is denoted by \A\.
                   When the base of the logarithmic function is omitted, it is assumed that the 
              base equals 2, thus logx means the same as log2 x (as usual, ln:r denotes the natural 
              logarithm).
                   We use the notation  [x\  for the integer part of a real number x  (the largest 
              integer number that is less than or equal to x).  Similarly,  \x]  denotes the smallest 
              integer number that is larger than or equal to x.
                   Orders of magnitude.  The notation /  ^ g+ 0(1), where /  and g are expressions 
              containing variables,  means that for some c the inequality /  ^ g + c holds for all 
              values of variables.  In a similar way we understand the expression /  ^ g + 0(h) 
              (where h is non-negative):  it means that for some c for all values of variables, the 
              inequality /  ^ g + ch holds.  The notation f  — g + 0(h)  (where h is non-negative) 
              means that for some c for all values of variables we have \f — g\ ^ ch.  In particular, 
              /  = 0(h) holds if l/l ^ ch for some constant c; the notation /  = Q(h) means that 
              l/l  ^  ch for some constant  c > 0  (usually /  is positive).  The notation /  = 0(fi) 
              means that c\h ^ |/| ^ C2/1 (again, usually /  is positive).
                   В denotes the set  {0,1}.  Finite sequences of zeros and ones are called binary 
              strings.  The  set  of all  binary  strings  is  denoted  by  E.  If A  is  a  finite  set  (an 
              alphabet),  then An  denotes the set of all  strings  of length n  over the  alphabet A, 
              that is,  the set of all sequences of length n, whose terms belong to A.  We denote 
              by A* the set of all strings over the alphabet A  (including the  empty string A of 
              length 0).  For instance, E — B*.  The length of a string x is denoted by l(x).  The 
              notation ab refers to  the  concatenation  of strings  a  and  b,  that  is,  the  result  of 
              appending b to a.  We say that a string a is a prefix of a string b if b = ax for some 
              string x.  We say that a is a suffix of a string b if b = xa for some string x.  We say 
              that a is a substring of b, if b = xay for some strings x and у (m other words, a is 
              a suffix of a prefix of b or the other way around).
                   We also consider infinite sequences of zeros and ones, and Q denotes the set of 
              all such sequences.  The set of infinite sequences of elements of a set A is denoted 
              by A°°, thus D, = B°°.  For a finite sequence x we use the notation Qx for the set of 
              all infinite sequences that start with x  (i.e., have x as a prefix).  Sets of this form
                                                             XV
              XVI                           BASIC  NOTIONS  AND  NOTATION
              are called  intervals.  The concatenation xoj  of a finite sequence x  and an infinite 
              sequence w is defined in a natural way.
                   In some contexts it is convenient to consider finite and infinite sequences to­
              gether.  We use the notation E for the set of all finite and infinite sequences of zeros 
              and ones, i.e., E = SUfi, and Ex denotes the set of all finite and infinite extensions 
              of a string x.
                   We  consider  computable  functions  whose  arguments  and  values  are  binary 
              strings.  Unless stated otherwise,  functions are partial  (not  necessarily total).  A 
              function  /   is  called  computable  if there  is  a machine  (a program,  an  algorithm) 
              that for all x,  such that  f(x)  is defined,  halts on input x  and outputs the result 
              f(x)  and does not halt on all inputs x outside the domain of /.  We also consider 
              computable  functions  whose  arguments  and values  are finite objects  of different 
              type,  like natural numbers,  integer numbers,  finite graphs,  etc.  We assume that 
              finite objects are encoded by binary strings.  The choice of an encoding is not im­
              portant provided different encodings can be translated to each other.  The latter 
              means that we can algorithmically decide whether a string is an encoding of an 
              object  and,  if this  is  the  case,  we  can  find  an  encoding of the  same object  with 
              respect to the other encoding.
                   Sometimes we consider computable functions of infinite objects, like real num­
              bers or measures.  Such considerations require rigorous definitions of the notion of 
              computability, which are provided when needed (see below).
                  A set  of finite  objects  (binary  strings,  natural  numbers,  etc.)  is  called  com- 
              putably enumerable, or just  enumerable, if there is a machine (a program, an algo­
              rithm) without input that prints all elements from the set (and no other elements) 
              with arbitrary delays between printing consecutive elements.  The algorithm is not 
              required to halt even when the set is finite.  The order in which the elements are 
              printed can be arbitrary.
                  A  real  number  a  is  computable  if there  exists  an  algorithm  that  computes 
              a  with  any  given  precision:  for  any  given  rational  e  >  0,  the  algorithm  must 
              produce a rational number at distance at most £ from a (in this case we say that 
              the algorithm  computes  the number).  A real number a is  lower semicomputable 
              if  it  can  be  represented  as  a  limit  of a  non-decreasing  computable  sequence  of 
              rational numbers.  An equivalent definition:  a is lower semicomputable if the set of 
              rational numbers that are less than a is enumerable.  A sequence of real numbers 
              is  computable if all its terms are computable, and given any n we are able to find 
              an algorithm computing the nth number in the sequence.  The notion of a lower 
              semicomputable sequence of reals is defined in an entirely similar way (for any given 
              n we have to find an algorithm that lower semicomputes the nth number).
                  We consider measures  (more specifically, probability measures,  or probability 
              distributions) on Q.  Every measure can be identified by its values on intervals flx. 
              So measures are identified with non-negative functions p on strings which satisfy 
              the following two conditions:  p{A)  =  1  and p(x)  = p(x0) + p(x 1)  for all x.  Such 
              measures are called  measures  on the  binary tree.  We consider also  semimeasures 
              on the binary tree, which are probability measures on the space E of all finite and 
              infinite binary sequences.  They correspond to functions p such that p(A) = 1 and 
              p(x) ^ p(x0) +p(xl).  We consider also semimeasures on natural numbers, which are 
              defined as sequences {pi} of non-negative reals with YlienPi ^ 1-  ^              natural to
                                           BASIC  NOTIONS  AND  NOTATION                              XVII
              identify such sequences with probability distributions on the set Nj_, which consists 
              of natural numbers and of the special symbol _L (undefined value).
                  Among all semimeasures  (on the tree or on natural numbers)  we distinguish 
              lower semicornputable ones.  Both the class of lower seinicomputable serniineasures 
              on the tree and the class of lower semicornputable semimeasures on natural numbers 
              have a maximal semimeasure (up to a multiplicative constant).  Any maximal lower 
              semicornputable semimeasure is called an  a priori probability  (on  the  tree or  on 
              natural numbers).  The a priori probability of a natural number n is denoted by 
              m(n); the a priori probability of a node x in the binary tree (that is, of the string x) 
              is denoted by a(x).  We use also the notation m{x) for a binary string x, which means 
              an a priori probability of the number of x with respect to some fixed computable 
              one-to-one correspondence between strings and natural numbers.
                  The plain Kolmogorov complexity is denoted by C(x), the prefix Kolmogorov 
              complexity is denoted by K(x) (and by K'{x) when we want to stress that we are us­
              ing prefix-free description modes).  The same letters are used to denote complexities 
              of pairs,  triples,  etc.,  and to denote conditional complexity.  For instance,  C(x\y) 
              stands for the plain conditional complexity of x when у is known, and m(x,y\z) 
              denotes  the  a priori  probability  of the  pair  (x,y)  (that  is,  of the  corresponding 
              number) when z is known.  The monotone Kolmogorov complexity is denoted by 
              AM, and the a priori complexity (negative logarithm of the a priori probability on 
              the tree)  is denoted by KA.  (In the literature monotone complexity is sometimes 
              denoted by Km and Km and the a priori complexity is denoted by KM.) Finally, 
              the decision complexity is denoted by KR.
                  В В (n) denotes the maximal halting time of the optimal decompressor on inputs 
              of length at most n (if the optimal prefix decompressor is meant, then we use the 
              notation  BP(n)).  The  function  BB{n)  is  closely  related  to  the  function  B(n) 
              defined as the maximal natural number of Kolmogorov complexity at most n.
                  We  use  also  several  topological  notions.  The  space  Nj_  consists  of natural 
              numbers  and  of a special  element  _L  (undefined  value);  the  family  of open  sets 
              consists of the whole space and of all sets that do not contain _L.  This topological 
              space, as well as the space £  (where the family of open sets consists of all unions 
              of sets of the form £x),  is used for the general classification of complexities.  For 
              the spaces 0. and £ and for the space of real numbers, we call a set effectively open 
              if it is a union of a computably enumerable family of intervals (sets of the form £x 
              for the second space and intervals with rational endpoints for the space of reals).
                  Most notions of computability theory (including Kolmogorov complexity) can 
              be relativized, which means that all involved algorithms are supplied by an external 
              procedure, called an oracle.  That procedure can be asked whether any given number 
              belongs to a set A.  That set is also called an oracle.  Thus we get the notions of 
              “decidability relative to  an oracle A”,  “computability relative to A”,  etc.  In the 
              corresponding notation we use the superscript A, for example, CA(x).
                  In the chapter on classical information theory, we use the notion of Shannon en­
              tropy of a random variable £.  If the variable has к possible outcomes and pi,... ,Pk 
              are their probabilities, then its Shannon entropy H(£) is defined as — YlkPk l°gVk- 
              This definition makes sense also for pairs of jointly distributed random variables. 
              For such a pair the conditional entropy of a random variable £ when i] is known is 
              defined as H{£, rf) —H(r)).  The difference if(£) + H(rj) — H(£, 77) is called the mutual
        xviii           BASIC NOTIONS AND NOTATION
        information in random variables £ and rj and is denoted by I(£:rj).  A similar no­
        tation I(x:y) is used in algorithmic information theory.  As I(x:y) is commutative 
        only up to a small error term, we usually say  “the information in x about y”  and 
        define this notion as C(y) — C(y\x).
                   INTRODUCTION
              What is this book about?
               What is Kolmogorov complexity?
       Roughly  speaking,  Kolmogorov  complexity  means  “compressed  size”.  Pro­
     grams like zip, gzip, bzip2, compress, rar, arj, etc., compress a file (text, image, 
     or some other data) into a presumably shorter one.  The original file can then be 
     restored by a “decompressing”  program (sometimes both compression and decom­
     pression  are performed by the same program).  Note that we consider here only 
     lossless compression.
       A file that  has a regular structure can be compressed significantly.  Its com­
     pressed size  is  small  compared  to  its  length.  On  the other  hand,  a file without 
     regularities can hardly be compressed, and its compressed size is close to its origi­
     nal size.
       This explanation is very informal and contains several inaccuracies—both tech­
     nical and more essential.  First, instead of files (sequences of bytes) we will consider 
     binary strings  (finite sequences of bits,  that is,  of zeros and ones).  The length of 
     such  a string is  the number of symbols in it.  (For example,  the string  1001  has 
     length 4, and the empty string has length 0.)
       Here are the more essential points:
        •  We consider only decompressing programs; we do not worry at all about 
         compression.  More specifically,  a decompressor is any algorithm  (a pro­
         gram) that receives a binary string as an input and returns a binary string 
         as  an  output.  If a decompressor  D  on  input  x  terminates  and  returns 
         string y,  we  write  D(x)  —  y  and  say  that  x  is  a  description  of y  with 
         respect to D.  Decompressors are also called description modes.
        •  A description mode is not required to be total.  For some x, the compu­
         tation D(x) may never terminate and therefore produces no result.  Also 
         we do not put any constraints on the computation time of D:  on some 
         inputs the program D may halt only after an extremely long time.
       Using recursion theory terminology, we say that a description mode is a partial 
     computable (=partial recursive) function from E to E, where E = {0,1}* stands for 
     the set of all binary strings.  Let us recall that we associate with every algorithm D 
     (whose inputs and outputs are binary strings) a function d computed by D; namely, 
     d(x) is defined for a string x if and only if D halts on x, and d(x) is the output of 
     D on x.  A partial function from E to E is called computable if it is associated with 
     (=computed by) some algorithm D.  Usually we use the same letter to denote the 
     algorithm and the function it computes.  So we write D(x) instead of d(x) unless it 
     causes a confusion.
       Assume that a description mode (a decompressor) D is fixed.  (Recall that D is 
     computable according to our definitions.)  For a string x consider all its descriptions,
                      i
                          INTRODUCTION.  WHAT IS THIS  BOOK  ABOUT?
          2
          that is,  all  y  such  that  D(y)  is defined  and  equals x.  The  length of the shortest 
          string y among them is called the Kolmogorov complexity of x with respect to D:
                               CD(x) = min{ l(y) I D(y) = .r}.
          Here l(y) denotes the length of the string y;  we use this notation throughout the 
          book.  The subscript D indicates that the definition depends on the choice of the 
          description mode D.  The minimum of the empty set is defined as +oo, thus Co(x) 
          is  infinite for  all  the strings x  outside the range of the function D  (they have no 
          descriptions).
             At first glance this definition seems to be meaningless, as for different D we ob­
          tain quite different notions, including ridiculous ones.  For instance, if D is nowhere 
          defined,  then Co  is infinite everywhere.  If D(y) — Л (the empty string) for all y, 
          then the complexity of the empty string is 0 (since D(Л) = Л and 1(A) = 0), and 
          the complexity of all the other strings is infinite.
             Here is a more reasonable example:  consider a decompressor D that just copies 
          its input to output, that is, D(x) — x for all x.  In this case every string is its own 
          description and Co(x) = l(x).
             Of course,  for  any  given  string  x  we  can  find  a description  mode  D  that  is 
          tailored to x and with respect to which x has small complexity.  Indeed, let D(A) — 
          x.  This implies Co(x) = 0.
             More generally, if we have some class of strings, we may look for a description 
          mode that favors all the strings in this class.  For example, for the class of strings 
          consisting of zeros only we may consider the following decompressor:
                             D(bin(n)) = 000... 000  (n zeros),
          where bin(n) stands for the binary notation of natural number n.  The length of 
          the  string  bin(n)  is  about  log2 n  (does  not  exceed  log2 n + 1).  With  respect  to 
          this  description  mode,  the complexity of the string consisting of n zeros is close 
          to log2 n.  This is much less that the length of the string (n).  On the other hand, 
          all strings containing symbol 1 have infinite complexity Co­
             lt  may seem that  the dependence of complexity on the choice of the decom­
          pressor makes impossible any general theory of complexity.  However, that is not 
          the case.
                               Optimal description modes
             A description mode is better when descriptions are shorter.  According to this, 
          we say that a description mode (decompressor) D\ is not worse than a description 
          mode Z)2 if
                                   CDl(x) ^ Co2(x) + c
          for some constant c and for all strings x.
             Let us comment on the role of the constant c in this definition.  We consider a 
          change in the complexity bounded by a constant as “negligible”.  One could say that 
          such a tolerance makes the complexity notion practically useless, as the constant c 
          can be very large.  However,  nobody managed to get any reasonable theory that 
          overcomes this difficulty and defines complexity with better precision.
             Example.  Consider two description modes (decompressors) Di  and Z)2.  Let 
          us show that there exists a description mode D which is not worse than both of
                       OPTIMAL DESCRIPTION  MODES      3
       them.  Indeed, let
                           0 (0!/)  =   Di(y),
                           D(ly) = D2(y).
       In other words, we consider the first bit of a description as the index of a description 
       mode and the rest as the description (for this mode).
          If y is a description of x with respect to D\  (or D2), then 0y (respectively, 1 y) 
       is  a description  of x  with  respect  to  D  as well.  This  description  is only one bit 
       longer, therefore we have
                         Cd{x) ^ Cd1 (t) + 1,
                         Cd(x) ^ Cd2(x) + 1
       for all x.  Thus the mode D is not worse than both D\  and D^-
          This idea is often used in practice.  For instance, a zip-archive has a preamble; 
       the  preamble says  (among other  things)  which mode  was  used to  compress  this 
       particular file, and the compressed file follows the preamble.
          If we want to use N different compression modes,  we need to reserve initial 
       log2 N bits for the index of the compression mode.
          Using a generalization of this idea, we can prove the following theorem:
          Theorem 1 (Solomonoff-Kolmogorov).  There is a description mode D that is 
       not worse than any other one: for every description m,ode D'  there is a constant c 
       such that
                          Cd(x) ^ Cd'(x) + c
       for every string x.
          A description mode D having this property is called optimal.
          Proof.  Recall that a description mode by definition is a computable function. 
       Every computable function has a program.  We assume that programs are binary 
       strings.  Moreover, we assume that by reading the program bits from left to right, we 
       can determine uniquely where it ends, that is, programs are “self-delimiting”.  Note 
       that every programming language can be modified in such a way that programs are 
       self-delimiting.  For instance, we can double every bit of a given program (changing 
       0 to 00 and 1 to 11) and append the pattern 01 to its end.
          Define now a new description mode D as follows:
                           D{Py) = P{y),
       where P is a program  (in the chosen self-delimiting programming language)  and 
       y  is  any binary string.  That  is,  the algorithm D scans the input string from the 
       left to the right and extracts a program P from the input.  (If the input does not 
       start with a valid program, D does whatever it wants, say, it goes into an infinite 
       loop.  The self-delimiting property guarantees that the decomposition of input  is 
       unique:  if Py — P'y1  for two programs P and P\ then one of the programs is a 
       prefix of the other one.)  Then D applies the extracted program P to the rest of the 
       input (y) and returns the obtained result.  (So D is just a “universal algorithm”, or 
        “interpreter” ; the only difference is that program and input are not separated, and 
       therefore we need to use a self-delimiting programming language.)
          Let us show that indeed D is not worse than any other description mode P.  We 
       assume that the program P is written in the chosen self-delimiting programming
            4                  INTRODUCTION.  WHAT IS THIS  BOOK ABOUT?
            language.  If y is a shortest description of the string x with respect to P, then Py 
            is  a  description  of x  with  respect  to  D  (though  not  necessarily  a shortest  one). 
            Therefore, compared to P, the shortest description is at most l(P) bits longer, and
                                         CD(x)^CP(x) + l(P).
            The constant l(P) depends only on the description mode P (and not on x).      □
                Basically,  we  used  the  same  trick  as  in  the  preceding  example,  but  instead 
            of merging two  description  modes,  we join  all  of them.  Each  description  mode 
            is  prefixed  by  its  index  (program,  identifier).  The same  idea is used  in practice. 
            A  self-extracting  archive  is  an  executable  file  starting  with  a  small  program  (a 
            decompressor); the rest is considered as an input to that program.  This program 
            is loaded into the memory, and then it decompresses the rest of the file.
                Note that in our construction, the optimal decompressor works for a very long 
            time on some inputs (as some programs have large running time) and is undefined 
            on some other inputs.
                                      Kolmogorov complexity
                Fix an optimal description mode D and call Cd(x) the Kolmogorov complexity 
            of the string x.  In the notation Cx>(.t) we drop the subscript D and write just C(x).
                If we switch to another optimal description mode, the change in complexity is 
            bounded by an additive constant:  for any two optimal description modes D\  and 
            D2 there is a constant c(D\, D2) such that
                                     ICDl(x) - Cd2{x)\ < c(Dx,D2)
            for all x.  Sometimes this inequality is written as
                                       CdAx) = Cd2(x) + 0 (1),
            where 0 (1) stands for a bounded function of x.
                Could we then  consider the Kolmogorov complexity of a particular string x 
            without having in mind a specific optimal description mode used in the definition 
            of С(.т)?  No,  since by adjusting the optimal description mode, we can make the 
            complexity of x arbitrarily small or arbitrarily large.  Similarly, the relation “string x 
            is simpler than ÿ\  that is,  C(x) < C(y), has no meaning for two fixed strings x 
            and y:  by adjusting the optimal description mode, we can make any of these two 
            strings simpler than the other one.
                One may then wonder whether Kolmogorov complexity has any sense at all. 
            Trying to defend this notion, let us recall the construction of the optimal description 
            mode used in the proof of the Solomonoff-Kolmogorov theorem.  This construction 
            uses some programming language, and two different choices of this language lead 
            to two complexities that differ at most by a constant.  This constant is in fact the 
            length of the program that is written in one of these two languages and interprets 
            the other one.  If both languages are  “natural”, we can expect this constant to be 
            not  that  huge,  just  several  thousands  or even several hundreds.  Therefore if we 
            speak about strings whose complexity is, say, about 105  (i.e., a text of a long and 
            not very compressible novel), or 106  (which is reasonable for DNA strings,  unless 
            they are compressible much more than the biologists think now), then the choice 
            of the programming language is not that important.
                Nevertheless one should always have in mind that all statements about  Kol­
            mogorov complexity are inherently asymptotic:  they involve infinite sequences of
                          COMPLEXITY AND INFORMATION             5
         strings.  This situation is typical also for computational complexity:  usually upper 
         and lower bounds for complexity of some computational problem are asymptotic 
         bounds.
                          Complexity and information
            One can consider the Kolmogorov complexity of x as the amount of informa­
         tion in x.  Indeed,  a string of zeros, which has a very short description,  has little 
         information, and a chaotic string, which cannot be compressed, has a lot of informa­
         tion (although that information can be meaningless—we do not try to distinguish 
         between meaningful and meaningless information; so, in our view, any abracadabra 
         has much information unless it has a short description).
           If  the  complexity  of a  string  x  is  equal  to  k,  we  say  that  x  has  к  bits  of 
         information.  One can expect that the amount of information in a string does not 
         exceed its length, that is, C(x) ^ l(x).  This is true (up to an additive constant, as 
         we have already said).
           Theorem 2.  There is a constant c such that
                                C(x) ^ l(x) + c
         for all strings x.
           PROOF.  Let  D(y)  =  у  for all y.  Then  Cd{x) = l(x).  By optimality,  there 
         exists some c such that
                           C(x) ^ Cd{x) + c — l(x) + c 
         for all x.                                             □
           Usually this statement is written as follows:  C(x)  ^ l(x) + 0(1). Theorem 2 
         implies,  in particular,  that Kolmogorov complexity is always finite,  that is,  every 
         string has a description.
           Here is another property of “amount of information”  that one can expect:  the 
         amount of information does not increase when algorithmic transformation is per­
         formed.  (More precisely, the increase is bounded by an additive constant depending 
         on the transformation algorithm.)
           Theorem 3.  For every algorithm A there exists a constant c such that
                              C(A(x)) < C(x) + c 
        for all x such that A(x)  is defined.
           PROOF.  Let D be an optimal decompressor that is used in the definition of 
         Kolmogorov complexity.  Consider another decompressor D'\
                               D'{p) = A{D{p)).
         (We apply first D and then A.)  If p is a description of a string x with respect to D 
         and A(x) is defined, then p is a description of A(x) with respect to D'.  Let p be a 
         shortest description of x with respect to D.  Then we have
                         Cdi(A(x)) ^ l(p) = Cd(x) = C(x).
         By optimality we obtain
                        C(A(x)) ^ Cd.(A(x)) + c ^ C(x) + c
         for some c and all x.                                  □
                                     INTRODUCTION.  WHAT IS THIS BOOK  ABOUT?
              6
                   This theorem implies that the amount of information “does not depend on the 
              specific  encoding”.  For  instance,  if we  reverse  all  bits  of some  string  (replace  0 
              by 1  and vice versa), or add a zero bit after each bit of that string,  the resulting 
              string has the same Kolmogorov complexity as the original one (up to an additive 
              constant).  Indeed, the transformation itself and its inverse can be performed by an 
              algorithm.
                   Here is one more example of a natural property of Kolmogorov complexity.  Let 
              X and y be strings.  How much information does their concatenation xy have?  We 
              expect that the quantity of information in xy does not exceed the sum of those in x 
              and y.  This is indeed true; however, a small additive term is needed.
                   Theorem 4.  There is a constant c such that for all x  and y 
                                        C{xy) < C(x) + 2 log C(x) + C{y) + c.
                   Proof.  Let us try first  to  prove  the statement  in  a stronger  form,  without 
              the term 2 log C(x).  Let  D be the optimal description mode that is used in the 
              definition  of Kolmogorov complexity.  Define  the following description mode  D'. 
              If D{jp)  = x and D(q) — y, we consider pq as a description of xy,  that is,  we let 
              D'(pq)  = xy.  Then the complexity of xy with respect to D' does not exceed the 
              length of pq,  that  is,  l{p) + l{q).  If p  and  q  are minimal descriptions,  we obtain 
              Co'(з^у) ^ CD{x) + Co{y)-  By optimality the same inequality holds for D in place 
              of D', up to an additive constant.
                   What is wrong with this argument?  The problem is that D' is not well defined. 
              We let D'ijpq) = D(p)D(q).  However, D' has no means to separate p from q.  It may 
              happen that there are two ways to split the input into p and q yielding different 
              results:
                                   V\4i=V242  but  D{pi)D{qi) ф D(p2)D(q2).
                   There are two ways to fix this bug.  The first  one,  which we use  now,  goes 
              as follows.  Let us prepend the string pq by the length l(p) of string p  (in binary 
              notation).  This allows us to separate p and q.  However,  we need to find where 
              l(p) ends, so let us double all the bits in the binary representation of l(p) and then 
              put 01 as separator.  More specifically, let bin(fc) denote the binary representation 
              of integer  k,  and  let  x  be  the  result  of doubling  each  bit  in  x.    (For  example, 
              bin(5) = 101, and bin(5) = 110011.)  Let
                                            D'{ bin(/(p)) Olpq) = D{p)D(q).
              Thus D' is well defined:  the algorithm D' scans bin(/(p)) while all the digits are 
              doubled.  Once it sees 01,  it determines l(p), and then scans l(p) digits to find p. 
              The rest of the input is q, and the algorithm is able to compute D(p)D(q).
                   Now we see that Со'(ху) is at most 2/(bin(/(p))) + 2 + l(j>) + l{q).  The length 
              of the binary representation of l(p)  is at most log2/(p) + 1.  Therefore,  xy has a 
              description of length at most 2 log2 l(p) + 4 + l{jp) + l(q) with respect to D', which 
              implies the statement of the theorem.                                                         □
                   The second way to  fix  the bug mentioned  above goes  as  follows.  We could 
              modify the definition of Kolmogorov complexity by requiring descriptions  to  be 
              self-delimiting; we discuss this approach in detail in Chapter 4.
                      COMPLEXITY AND INFORMATION       7
          Note also that we can exchange p and q and thus prove that 
                    C(xy) ^ C(x) + C(y) + 2 log2 C(y) + c.
          How tight  is  the  inequality  of Theorem  4?  Can  C(xy)  be  much  less  than 
       C(x) + C(y)? According to our intuition, this happens when x and y have much in 
       common.  For example, if x — y, we have C(xy) = C(xx) = C(x) + 0(1), since xx 
       can be algorithmically obtained from x and vice versa (Theorem 3).
          To refine this observation,  we will define the notion of the quantity of infor­
       mation in x that  is  missing  in y  (for  all  strings  x  and  y).  This  value  is  called 
       the Kolmogorov complexity of x  conditional to y  (or  “given y” ) and is denoted by 
       C(x\y).  Its definition is similar to the definition of the unconditional complexity. 
       This time the decompressor D has access not only to the (compressed) description, 
       but also to the string y.  We will discuss this notion later in Section 2.  Here we 
       mention only that the following equality holds:
                     C(xy) = C(y) + C(x I у) + О (log n)
       for all strings x and у of complexity at most n.  The equality reads as follows:  the 
       amount of information in xy is equal to the amount of information in у plus the 
       amount of new information in x (“new”  = missing in y).
          The difference C(x) — C(x\ y) can be considered as “the quantity of information 
       in у about x”.  It indicates how much the knowledge of у simplifies x.
          Using the notion of conditional complexity, we can ask questions like this:  How 
       much new information does the DNA of some organism have compared to that of 
       another organism’s DNA? If d\ is the binary string that encodes the first DNA and 
       d2 is the binary string that encodes the second DNA, then the value in question is 
       C(d\ |^2)-  Similarly we can ask what percentage of information has been lost when 
       translating a novel into another language:  this percentage is the fraction
                     C (original | translation) / C (original).
          The questions about  information in different objects were studied before the 
       invention of algorithmic information theory.  The information was measured using 
       the  notion of Shannon entropy.  Let  us recall its definition.  Let  £  be a random 
       variable that takes n values with probabilities p\,... ,pn-  Then its Shannon entropy 
       #(£) is defined as
                        #(£) = X ^ ( “ log2Pi)-
       Informally, the outcome having probability pi carries log(l/pj)  = — log2Pi bits of 
       information (=surprise).  Then #(£) can be understood as the average amount of 
       information in an outcome of the random variable.
          Assume that we want to use Shannon entropy to measure the amount of infor­
       mation contained in some English text.  To do this, we have to find an ensemble of 
       texts and a probability distribution on this ensemble such that the text is “typical” 
       with respect to this distribution.  This makes sense for a short telegram, but for a 
       long text (say, a novel) such an ensemble is hard to imagine.
          The same difficulty arises when we try to define the amount of information in 
       the genome of some species.  If we consider as the ensemble the set of the genomes 
       of all existing species (or even all species that ever existed), then the cardinality of 
       this set is rather small  (it does not exceed 21000  for sure).  And if we consider all
                         INTRODUCTION.  WHAT IS THIS BOOK ABOUT?
          8
          its  elements as équiprobable, then we obtain a ridiculously small value (less than 
          1000 bits); for the non-uniform distributions the entropy is even less.
             So  we  see  that  in  these  contexts  Kolmogorov  complexity  looks  like  a  more 
          adequate tool than Shannon entropy.
                             Complexity and randomness
             Let  us  recall  the  inequality  C(x)  <  l(x) + 0(1)  (Theorem  2).  For  most  of 
          the strings its left-hand side is close to the right hand side.  Indeed, the following 
          statement is true:
             Theorem 5.  Let n  be an integer.  Then there are less than 2n  strings x  such 
          that C(x) < n.
             Proof.  Let D be the optimal description mode used in the definition of Kol­
          mogorov complexity.  Then only strings D(y)  for all y,  such that  l(y) < n,  have 
          complexity less than n.  The number of such strings does not exceed the number of 
         strings y such that l(y) < n, i.e., the sum
                            1 + 2 + 4 + 8 + ... + 2n_1 = 2n -  1 
          (there are 2k strings for each length к < n).                 □
             This implies that the fraction of strings of complexity less than n — c among all 
         strings of length n is less than 2n-c/2n = 2“c.  For instance, the fraction of strings 
         of complexity less than 90 among all strings of length 100 is less than 2“10,
             Thus the majority of strings (of a given length)  are incompressible or almost 
         incompressible.  In  other words,  a randomly chosen string of the given length is 
         almost  incompressible.  This can be illustrated by the following mental  (or even 
         real) experiment.  Toss a coin, say, 80000 times, and get a sequence of 80000 bits. 
         Convert it into a file of size  10000 bytes  (8 bits =  1  byte).  One can bet that no 
         compression software (existing before the start of the experiment) can compress the 
         resulting file by more than  10 bytes.  Indeed,  the probability of this event is less 
         than 2“80 for every fixed compressor, and the number of (existing) compressors is 
         not so large.
             It  is  natural to consider incompressible strings as  “random”  ones:  informally 
         speaking, randomness is the absence of any regularities that may allow us to com­
         press the string.  Of course,  there is no strict  borderline between  “random”  and 
          “non-random”  strings.  It is ridiculous to ask which strings of length 3  (i.e.,  000, 
         001, 010, Oil,  100,  101,  110,  111)  are random and which are not.
             Another example:  assume that we start with a “random” string of length 10000 
         and replace its bits by all zeros (one bit at a step).  At the end we get a certainly 
         non-random string (zeros only).  But it would be naive to ask at which step the 
         string has become non-random for the first time.
             Instead, we can naturally define the  “randomness deficiency”  of a string x as 
         the difference l(x) — C(x).  Using this notion, we can restate Theorem 2 as follows: 
         the  randomness  deficiency  is  almost  non-negative  (i.e.,  larger  than  a  constant). 
         Theorem 5 says that the randomness deficiency of a string of length n is less than 
         d with probability at least 1 — l/2d (assuming that all strings are équiprobable).
             Now consider the Law of Large Numbers.  It says that most of the n-bit strings 
         have frequency of ones close to  1/2.  This law can be translated into Kolmogorov 
         complexity language as follows:  the frequency of ones in every string with small
                        NON-COMPUTABILITY OF C AND BERRY’S PARADOX              9
           randomness deficiency is close to  1/2.  This translation implies the original state­
          ment since most of the strings have small randomness deficiency.  We will see later 
          that actually these formulations are equivalent.
              If we insist  on  drawing a strict  borderline between  random and non-random 
          objects,  we have to consider infinite sequences instead of strings.  The notion of 
          randomness for infinite sequences of zeros and ones was defined by Kolmogorov’s 
          student P. Martin-Löf (he came to Moscow from Sweden). We discuss it in Section 3. 
          Later C. Schnorr and L. Levin found a characterization of randomness in terms of 
          complexity:  an infinite binary sequence is random if and only if the randomness 
          deficiency of its prefixes is bounded by a constant.  This criterion,  however,  uses 
          another version of Kolmogorov complexity called monotone complexity.
                       Non-computability of C and Berry’s paradox
              Before discussing applications of Kolmogorov complexity, let us mention a fun­
          damental problem that reappears in any application.  Unfortunately, the function C 
          is not computable:  there is no algorithm that given a string x finds its Kolmogorov 
          complexity.  Moreover, there is no computable non-trivial (unbounded) lower bound 
          for C.
              Theorem 6.  Let к  be  a  computable  (not necessarily  total)  function from £ 
          to  N.  (In  other words,  к  is  an  algorithm that  terminates  on some  binary strings 
          and returns  natural  numbers  as  results.)  If к  is  a  lower  bound for Kolmogorov 
          complexity,  that  is,  k(x)  ^  C(x)  for  all x  such  that  k(x)  is  defined,  then  к  is 
          bounded:  all its values do not exceed some constant.
              The proof of this theorem is a reformulation of the so-called Berry’s paradox. 
          This paradox considers
                  the minimal natural number that cannot be defined by at most 
                  fourteen English words.
          This phrase has exactly fourteen words and defines that number.  Thus we get a 
          contradiction.
              Following  this  idea,  consider  the first  binary  string  whose  Kolmogorov  com­
          plexity is greater than  a given number N.  By definition, its complexity is greater 
          than N.  On the other hand, this string has a short description that includes some 
          fixed amount of information plus the binary notation of N  (which requires about 
          log2 N bits), and the total number of bits needed is much less than N for large N. 
          That would be a contradiction if we knew how to effectively find this string given 
          its description.  Using the computable lower bound k, we can convert this paradox 
          into the proof.
              Proof.  Consider the function B(N) whose argument N is a natural number. 
          It is computed by the following algorithm:
                  perform  in  parallel  the  computations  k(A), k(0),  fc(l),  k(00), 
                  k(01),  k( 10),  fc(ll),...  until some string x such that k(x)  > N 
                  appears; then return x.
              If the function к is unbounded, then the function В is total and k(B(N)) > N 
          by construction for every N.  As к is a lower bound for K, we have C(B(N)) > N. 
          On the other hand B(N) can be computed given the binary representation bin(iV)
               10                     INTRODUCTION.  WHAT IS THIS  BOOK ABOUT?
               of N, therefore
                        C(B(N)) < C(hm(N)) + 0(1) ^ /(bin(iV)) + 0(1) ^ log2 N + 0(1)
               (the first  inequality is provided by Theorem 3 and the second one is provided by 
               Theorem 2; term 0(1) stands for a bounded function).  So we obtain
                                             N <C(B{N))^\og2N + 0(l), 
               which cannot happen if N is large enough.                                                       □
                                 Some applications of Kolmogorov complexity
                   Let us start with a disclaimer:  the applications we will talk about are not real, 
               practical applications; we just establish relations between Kolmogorov complexity 
               and other important notions.
                   Occam’s razor.  We start with a philosophical question.  What do we mean 
              when we say that a theory provides a good explanation for some experimental data? 
              Assume that we are given some experimental data and there are several theories to 
              explain the data.  For example, the data might be the observed positions of planets 
              in  the  sky.  We  can  explain  them  as  Ptolemy  did,  with  epicycles  and  deferents, 
              introducing extra corrections when needed.  On the other hand,  we can use the 
              laws of the modern mechanics.  Why do we think that the modern theory is better? 
              A possible answer:  the modern theory can compute the positions of planets with 
              the same (or even better) accuracy given fewer parameters.  In other words, Kepler’s 
              achievement is a shorter description of the experimental data.
                   Roughly speaking, experimenters obtain binary strings and theorists find short 
              descriptions for them (thus proving upper bounds for complexities of those strings); 
              the shorter the description, the better the theorist.
                   This  approach  is sometimes called  “Occam’s  razor”  and  is  attributed to the 
              philosopher William of Ockham who said that  entities should  not  be multiplied 
              beyond necessity.  It is hard to judge whether he would agree with such an inter­
              pretation of his words.
                   We can use the same idea in more practical contexts.  Assume that we design 
              a machine that reads handwritten zip codes on envelopes.  We are looking for a 
              rule that separates, say, images of zeros from images of ones.  An image is given as 
              a Boolean matrix (or a binary string).  We have several thousands of images and 
              for each image we know whether it means 0 or 1.  We want to find a reasonable 
              separating rule (with the hope that it can be applied to the forthcoming images). 
              What does  “reasonable”  mean in this context?  If we just  list  all the images  in 
              our list  together with their classification,  we get a valid separation rule—at least 
              it  works  until  we  receive  a new  image—however,  the rule  is way too  long.  It  is 
              natural to assume that a reasonable rule must have a short description, that is, it 
              must have low Kolmogorov complexity.
                   Often an explanation for experimental data is only a tool to predict the future 
              elements of the data stream.  This aspect was the main motivation for Solomonoff 
              [187]; it is outside the scope of our book and is considered in the book of M. Hutter
              [es]-
                   Foundations  of probability  theory.  The probability theory itself,  being 
              currently a part of measure theory, is mathematically sound and does not need any 
              extra “foundations”.  The difficult questions arise, however, if we try to understand
                   SOME APPLICATIONS OF KOLMOGOROV COMPLEXITY  11
         why this theory could be applied to the real-world phenomena and how it should 
         be applied.
           Assume that we toss  a  coin  a thousand  times  (or  test  some other hardware 
         random number generator) and get a bit string of length 1000.  If this string contains 
        only zeros or equals 0101010101...  (zeros and ones alternate), then we definitely 
        will conclude that the generator is bad.  Why?
           The usual explanation:  the probability of obtaining a thousand zeros is negli­
        gible  (2“100°) provided the coin is fair.  Therefore,  the conjecture of a fair coin is 
        refuted by the experiment.
           The problem with this explanation is that we do not always reject the generator: 
        there should be some sequence a of a thousand zeros and ones which is consistent 
        with this conjecture.  Note, however, that the probability of obtaining the sequence 
        a as a result of fair coin tossing is also 2“1000.  So what is the reason behind our 
        complaints?  What is the difference between the sequence of a thousand zeros and 
        the sequence a?
           The reason is revealed when we compare the Kolmogorov complexities of these 
        sequences.
           Proving theorems  of probability  theory.  As an example,  consider  the 
        Strong Law of Large Numbers.  It claims that for almost all (according to the the 
        uniform Bernoulli probability distribution)  infinite binary sequences,  the limit of 
        frequencies of ones in their initial segments equals 1/ 2.
           More formally,  let  ft  be  the  set  of all  infinite  sequences  of zeros  and  ones. 
        The uniform Bernoulli measure on ft is defined as follows.  For every finite binary 
        string X, consider the set ftx consisting of all infinite sequences that start with x. 
        For example,  ft a  =  ft-  The  measure of ftx  is equal to  2~l(x'>.  For example,  the 
        measure of the set floi, that consists of all sequences starting with 01, equals 1/4.
           For each sequence со = u)qcvicv2 . . .  consider the limit of the frequencies of ones 
        in the prefixes of co, that is,
                              u>o + CJi + ... + <vn_i
                           hm  -------------------------- .
                           n—>oo     Tl
        We say that u> satisfies the Strong Law of Large Numbers (SLLN) if this limit exists 
        and is equal to 1/2.  For instance, the sequence 010101..., having period 2, satisfies 
        the SLLN, and the sequence 011011011..., having period 3, does not.
           The SLLN says that the set of sequences that do not satisfy SLLN has measure 
        0.  Recall that  a set  A  C  ft has measure 0 if for all e  >  0 there is  a sequence of 
        strings то, x\, X2,...  such that
                           A C ftXo U ftXl UftX2U...
        and the sum of the series
                          2~i(xo) _|_ 2~l(xi) -|- 2~dx2) _|_
        (the sum of the measures of ftXi) is less than e.
           One can prove SLLN using the notion of a Martin-Löf random sequence men­
        tioned above.  The proof consists of two parts.  First, we show that every Martin-Löf 
        random sequence satisfies SLLN. This can be done using Levin-Schnorr random­
        ness criterion  (if the limit does not exist or differs from 1/2, then the complexity 
        of some prefix is less than it should be for a random sequence).
           The second part is rather general and does not depend on the specific law of 
        probability theory.  We prove that the set of all Martin-Löf non-random sequences
           12               INTRODUCTION.  WHAT IS THIS BOOK  ABOUT?
           has measure zero.  This implies that the set of sequences that do not satisfy SLLN 
           is included in a set of measure 0 and hence has measure 0 itself.
               The notion of a random sequence is philosophically interesting in its own right. 
           In the beginning of twentieth century Richard von Mises suggested using this no­
           tion  (he called it in  German  Kollektiv)  as a basis for probability theory  (at  that 
           time the measure theory approach had not yet been developed).  He considered the 
           so-called  “frequency stability”  as a main property of random sequences.  We will 
           consider von Mises’ approach to the definition of a random sequence (and subse­
           quent developments) in Chapter 9.
               Lower  bounds  for  computational  complexity.  Kolmogorov complexity 
           turned out to be a useful technical tool when proving lower bounds for computa­
           tional complexity.  Let us explain the idea using the following model example.
               Consider the following problem.  Initially,  a string x of length n is located in 
           the n leftmost cells of the tape of a Turing machine.  The machine has to copy x, 
           that is, to get xx on the tape (the string x is intact and its copy is appended), and 
           halt.
               Since the middle of the 1960s it has been well known that a (one-tape) Turing 
           machine needs time proportional to n2 to perform this task.  More specifically, one 
           can show that  for every Turing machine M that  can copy every string x,  there 
           exists some e > 0 such that for all n there is a string x of length n whose copying 
           requires at least en2 steps.
               Consider the following intuitive argument supporting this claim.  The number 
           of internal states of a Turing machine is a constant  (depending on the machine). 
           That  is,  the  machine can keep in its  memory only a finite number of bits.  The 
           speed of the head movement is also limited:  one cell per step.  Hence the rate of 
           information transfer (measured in bit■ cell/step) is bounded by a constant depending 
           on the number of internal states.  To copy a string x of length n, we need to move n 
           bits by n cells to the right;  therefore, the number of steps should be proportional 
           to n2  (or more).
              Using Kolmogorov complexity, we can make this argument rigorous.  A string 
           is hard to copy if it contains maximal amount of information, i.e., its complexity is 
           close to n.  We consider this example in detail in Section 8.2 (p. 233).
              A  combinatorial  interpretation  of Kolmogorov  complexity.  We con­
           sider here one example of this kind (see Chapter 10, p. 313, for more detail).  One 
           can  prove  the  following  inequality  for  the  complexity  of three  strings  and  their 
           combinations:
                           2C(xyz) ^ C(xy) + C{xz) + C(yz) + O(logn)
           for all strings x, y, z of length at most n.
              It turns out that this inequality has natural interpretations that are not related 
           to  complexity at all.  In particular,  it implies  (see  [65])  the following geometrical 
           fact:
              Consider a body В in a three-dimensional Euclidean space with coordinate axes 
           OX, OY, and OZ. Let V be R’s volume.  Consider B's orthogonal projections onto 
           coordinate planes OXY, OXZ, and OYZ.  Let Sxy, Sxz, and Syz be the areas of 
           these projections.  Then
                                      V2 ^ Sxy • Sxz • Syz.
                                    SOME APPLICATIONS OF KOLMOGOROV COMPLEXITY                                           13
                     Here is an algebraic corollary of the same inequality.  For every group G and 
                its subgroups X, Y, and Z, we have
                                                Л,     r7\2  ^  \XDY\-\XnZ\-\YnZ\
                                              nrnzi2 >                           J  1   -------,
                where \H\ denotes the number of elements in H.
                     Gödel incompleteness theorem.  Following G. Chaitin, let us explain how 
                to use Theorem 6 in order to prove the famous Gödel incompleteness theorem.  This 
                theorem states that not all true statements of a formal theory that is “rich enough” 
                (the formal arithmetic and the axiomatic set theory are two examples of such a 
                theory) are provable in the theory.
                     Assume that for every string x and every natural number n, one can express 
                the statement C(x) > n as a formula in the language of our theory.  (This statement 
                says that the chosen optimal decompressor D does not output x on any input of 
                length  at  most  n;  one  can  easily  write  this  statement  in  formal  arithmetic  and 
                therefore in set theory.)
                     Let us generate all the proofs  (derivations)  in our theory and select those of 
                them which prove some statement of the form C(x) > n where x is some string 
                and n is some integer  (statements of this type have no free variables).  Once we 
                have found a new theorem of this type, we compare n with all previously found n’s. 
                If the new n is greater than all previous n’s, we write the new n into the  “records 
                table”  together with the corresponding xn.
                     There are two possibilities:  either (1) the table will grow infinitely, or (2) there 
                is the last statement C(X) > N in the table which remains unbeaten forever.  If (2) 
                happens, there is an entire class of true statements that have no proof.  Namely, all 
                true statements of the form C(x) > n with n > N have no proofs.  (Recall that by 
                Theorem 5 there are infinitely many such statements.)
                     In the first case we have infinite computable sequences of strings xq,x\,X2 • • • 
                and numbers no < n\ < П2 < ... such that all statements C(xi) > щ are provable. 
                We assume that the theory proves only true statements;  thus,  all the inequalities 
                                 are  true.  Without  loss  of generality,  we can  assume  that  all Xi  are 
                C(xi) > щ
                pairwise different (we can omit Xi if there exists j < i such that Xj = Xi ; every string 
                can occur only finitely many times in the sequence xq,x\,X2 ... since щ —> oo as 
                i —> oo).  The computable function k, defined by the equation k(xi) = щ, is then an 
                unbounded lower bound for Kolmogorov complexity.  This contradicts Theorem 6.
                               CHAPTER 1
                   Plain Kolmogorov complexity
                      1.1.  Definition and main properties
           Let us recall the definition of Kolmogorov complexity from the introduction. 
        This version of complexity was defined by Kolmogorov in his seminal paper  [78]. 
        In order to distinguish it from later versions we call it the plain Kolmogorov com­
        plexity.  Later,  starting  from  Chapter  4,  we  will  also  consider  other  versions  of 
        Kolmogorov complexity, including prefix versions and monotone versions, but for 
        now by Kolmogorov complexity we always mean the plain version.
           Recall that  a  description  mode,  or  a  decompressor,  is  a partial  computable 
        function  D  from the set  of all  binary strings  H  into  E.  A partial  function  D  is 
        computable  if there  is  an  algorithm  that  terminates  and  returns  D(x)  on  every 
        input  X  in the domain of D and does not terminate on all other inputs.  We say 
        that y is a description of x with respect to D if D{y) = x.
           The complexity of a string x with respect to description mode D is defined as
                         CD(x) = min{l(y) I D(y) = x}.
        The minimum of the empty set is +oo.
           We say that a description mode D\  is not worse than a description mode D2 
        if there is a constant c such that Cdx (x)  ^ Cd2 (x) + c for all x and write this as 
        CDl(x) ^ Cd2(x) + 0(1).
           A description mode is called optimal if it is not worse than any other description 
        mode.  By the  Solomonoff-Kolmogorov  universality  theorem  (Theorem  1,  p.  3) 
        optimal  description  modes  exist.  Let  us  briefly  recall  its  proof.  Let  U  be  an 
        interpreter of a universal programming language, that is,  U(p, x) is the output of 
        the program p on input x.  We assume that programs and inputs are binary strings. 
        Let
                             D(px) — U(p, x).
        Here p  и  p stands  for  any  computable mapping having the following property: 
        given p we can effectively find p and also the place where p ends (in particular, if p 
        is a prefix of q, then p = q).  This property implies that D is well defined.  For any 
        description mode D' let p be a program of D'.  Then
                           Cd(x) ^ Cd'(x) + l{p)-
        Indeed, for every description y of x with respect to D' the string py is a description 
        of x with respect to D.
           Fix  any  optimal description  mode  D,  and  let  C(x)  (we  drop  the  subscript) 
        denote the complexity of x with respect to D.  (As we mentioned, in the first paper 
        of Kolmogorov [78] the letter К was used, while in his second paper [79] the letter 
        H was used.  We follow here the notation used by Li and Vitånyi [103].)
                                  15
              16                        1.  PLAIN  KOLMOGOROV COMPLEXITY
                  As the optimal description mode is not worse than the identity function x >-»• x, 
              we obtain the inequality C{x) ^ l(x) + 0(1)  (Theorem 2, p. 5).
                  Let A be a partial computable function.  Comparing the optimal description 
              mode D with the description mode y (-»• A(D(y)), we conclude that
                                              0(А(ж))^0(ж) + 0(1),
              showing the non-growth of complexity under algorithmic transformations  (Theo­
              rem 3, p. 5).
                  Using this inequality, we can define Kolmogorov complexity of other finite ob­
              jects,  such  as  natural  numbers,  graphs,  permutations,  finite sets  of strings,  etc., 
              that can be naturally encoded by binary strings.
                  For example, let us define the complexity of natural numbers.  A natural num­
              ber n can be written in binary notation.  Another way to represent a number by a 
              string is as follows.  Enumerate all the binary strings in lexicographical order
                                  A, 0,1,00,01,10,11,000,001,010, Oil, 100,...
              using the natural numbers 0,1, 2,3,... as indexes.  This enumeration is more con­
              venient compared to binary representation as it is a bijection.  Every string can be 
              considered as an encoding of its index in this enumeration.  Finally,  one can also 
              encode a natural number n by a string consisting of n ones.
                  Using either of these three encodings,  we can define the  complexity of n  as 
              the complexity of the string encoding n.  Three resulting complexities of n differ 
              at  most  by an  additive constant.  Indeed,  for every pair of these encodings there 
              is  an  algorithm translating the first encoding into the second one.  Applying this 
              algorithm, we increase the complexity at most by a constant.  Note that the Kol­
              mogorov complexity of binary strings is defined up to an additive constant, so the 
              choice of a computable encoding does not matter.
                  As the length of the binary representation of a natural number n is equal to 
              logn + 0(1), the Kolmogorov complexity of n is at most logn + 0(1).  (By log we 
              denote binary logarithms.)
                  Here is another application of the non-growth of complexity under algorithmic 
              transformations.  Let  us  show  that  deleting  the  last  bit  of a  string  changes  its 
              complexity at most by a constant.  Indeed,  all three functions x i-». xO,  i  4  il, 
              x ^  (x without the last bit) are computable.
                  The same is true for the first bit.  However this does not apply to every bit of 
              the string.  To show this, consider the string x consisting of 2n zeros; its complexity 
              is at most C(n) + 0(1) ^ logn + 0(1).  (By log we always mean binary logarithm.) 
              There are 2n different strings obtained from x by flipping one bit.  At least one of 
              them has complexity n or more.  (Recall that the number of strings of complexity 
              less than n does not exceed the number of descriptions of length less than n, which 
              is less than 2n; see Theorem 5, p. 8.)
                  Incrementing a natural number n by  1 changes C(n)  at most by a constant. 
              This implies that C(n) satisfies the Lipschitz property,  for some c and for all m, n, 
             we have \C(m) — C{ri)\ ^ c|m — n|.
                  [T] Prove a stronger inequality:  \C(m) — C(n)\ ^ \m — n| + c for some c and for 
              all m, n € N, and, moreover, \C(m) — C(n)\ ^ 2 log \ m — n\ + c (the latter inequality 
              assumes that m Ф n).
                                   1.1.  DEFINITION  AND  MAIN  PROPERTIES                     17
                 Several times we have used the upper bound 2n  for the number of strings x 
             with C(x) < n.  Note that, in contrast to other bounds,  it involves no constants. 
             Nevertheless  this  bound  has  a  hidden  dependence  on  the  choice  of the  optimal 
             description mode:  if we switch  to  another optimal  description  mode,  the set  of 
             strings x such that C(x) < n can change!
                 |~2~| Show that the number of strings of complexity less than n is in the range 
             [2n-c; 2n]  for some constant c for all n.
                 (Hint:  The upper bound 2n is proved in the introduction, the lower bound is 
             implied by the inequality C(x) ^ l(x) + c:  the complexity of all the strings of length 
             less than n — c is less than n.)
                 Show that the number of strings of complexity  exactly n does not exceed 2n 
             but can be much less:  e.g., it is possible that this set is empty for infinitely many n.
                 (Hint:  Change an optimal description mode by adding 0 or 11 to each descrip­
            tion so that all descriptions have even length.)
                 [~3~| Prove that the average complexity of strings of length n is equal to n + 0(l).
                 (Hint:  Let ak denote the fraction of strings of complexity n — k among strings of 
            length n.  Then the average complexity is by     kak less than n.  Use the inequality
            ctfc  ^ 2~k and the convergence of the series Y^k/2k.)
                 In the next statement we establish a formal relation between upper bounds of 
            complexity and upper bounds of cardinality.
                 Theorem 7.  (a)  The family of sets Sn =   {x \ C(x)  <  n}  is enumerable,  and 
             |S„|  < 2n for all n.  Here |Sn|  denotes the cardinality of Sn.
                 (b)      IfVn  (n — 0,1,... )  is an enumerable family of sets of strings and \ Vn\ < 2n 
            for all n,  then there exists c such that C(x) < n + c for all n  and all x € Vn.
                 In this theorem we use the notion of an enumerable family of sets.  It is de­
            fined  as follows.  A set  of strings  (or natural numbers,  or other finite objects)  is 
             enumerable (= computably enumerable — recursively enumerable) if there is an al­
            gorithm generating all elements of this set in some order.  This means that there 
            is  a program that never terminates and prints all the elements of the set in some 
            order.  The intervals between printing elements can be arbitrarily large; if the set is 
            finite,  the program can print nothing after some time (unknown to the observer). 
            Repetitions are allowed, but this does not matter since we can filter the output and 
            delete the elements that have already been printed.
                 For example, the set of all n such that the decimal expansion of \p2 has exactly 
            n consecutive nines is enumerable.  The following algorithm generates the set:  com­
            pute decimal digits of \pl starting with the most significant ones.  Once a sequence 
            of consecutive n nines surrounded by non-nines is found, print n and continue.
                 A family of sets Vn  is called enumerable if the set of pairs  {(n, x)  \  x  €  Vn} 
            is  enumerable.  This implies  that  each of the  sets  Vn  is  enumerable.  Indeed,  to 
            generate elements of the set Vn for a fixed n, we run the algorithm enumerating the 
            set  {(n, x)  I  x € Vn}  and print the second components of all the pairs that have n 
            as the first component.  However, the converse statement is not true.  For instance, 
            assume that Vn is finite for every n.  Then every Vn is enumerable, but at the same 
            time it may happen that the set {(n,x) \ x € Vn} is not enumerable (say Vn — {0} 
            if n  6  S and  Vn  =  0 otherwise,  where S is any non-enumerable set of integers). 
            One can verify that a family is enumerable if and only if there is an algorithm that
                                        1.  PLAIN  KOLMOGOROV  COMPLEXITY
              18
              given any n finds a program generating Vn.  A detailed study of enumerable sets 
              can be found in every textbook on computability theory, for instance, in [184].
                   Proof.  Let us prove the theorem.  First, we need to show that the set 
                                      {(n,x) I X £ Sn} = {{n.x) I C(x) < n},
              where n is a natural number and x is a binary string, is enumerable.
                  Let  D be the optimal decompressor used in the definition of C.  Perform in 
              parallel the computations of D on all the inputs.  (Say, for к = 1, 2,...  we make к 
              steps of D on к first inputs.)  If we find that D halts on some у and returns x, the 
              generating algorithm outputs the pair  (l(y) + l,x).  Indeed, this implies that the 
              complexity of x is less than l(y) + 1, as у is a description of x.  Also it outputs all 
              the pairs (l(y) + 2, x), (l(y) + 3, x) ■ ■ •  in parallel to the printing of other pairs.
                  For those familiar with computability theory, this proof can be compressed to 
              one line:
                                       C(x) < n       3y (l(y) < n A D(y) — x).
              (The set  of pairs  (x, y)  such  that  D(y)  — x  is  enumerable,  being  the  graph  of 
              a  computable  function.  The  operations  of intersection  and  projection  preserve 
              enumerability.)
                  The converse implication is a bit  harder.  Assume that  Vn  is an enumerable 
              family  of finite  sets  of strings  and  | Fn|  <  2n.  Fix  an  algorithm  generating  the 
              set  {(n,x)  I  x £  Кг}.  Consider the description mode Dy  that deals with strings 
              of length n in the following way.  Strings of length n are used as descriptions of 
              strings  in  Vn.  More specifically,  let  xk  be the  fcth string in  Vn  in  the order  the 
              pairs  (n, x)  appear while generating the set  {(n, x) \ x £ Vn}.  (We assume there 
              are no repetitions, so xo, xi, X2, ■ ■ ■ are distinct.)  Let yk be the kth string of length 
              n in lexicographical order.  Then yk is a description of Xk, that is, D(yk) = Xk-  As 
              |V^|  < 2n, every string in Vn gets a description of length n with respect to D.
                  We need to verify that the description mode Dy defined in this way is com­
              putable.  To compute Dy(y), we find the index к of у in the lexicographical ordering 
              of strings of length l{y).  Then we run the algorithm generating pairs  (n,x)  such 
              that x £ Vn and wait until к different pairs having the first component l(y) appear. 
              The second component of the last of them is Dy(y).
                  By construction, for all x £ Vn we have Cdv{x) ^ n.  Comparing Dy with the 
              optimal description mode, we see that there is a constant c such that C(x) < n + c 
              for all x £ Vn.  Theorem 7 is proven.                                                     □
                  The intuitive meaning of Theorem 7 is as follows.  The assertions “the number of 
             strings with a certain property is small”  (is less than 2г) and “all the strings with a 
             certain property are simple”  (have complexity less than i) are equivalent provided 
             the  property  under  consideration  is  enumerable  and  provided  the  complexity  is 
             measured up to an additive constant (and the number of elements is measured up 
             to a multiplicative constant).
                  Theorem 7 can be reformulated as follows.  Let f(x) be a function defined on all 
             binary strings and which takes as values natural numbers and a special value +oo. 
             We call /  upper semicomputable, or enumerable from above, if there is a computable 
             function  (x, к)     F(x, к) defined on all strings x and all natural numbers к such
             that
                                         F(x, 0) ^ F(x, 1) ^ F(x, 2) ^        •
                                                          1.1.   DEFINITION  AND  MAIN  PROPERTIES                                                          19
                     and
                                                                         f(x) —  lim  F(x, к)
                                                                                      к—>oo
                    for all X.  The values of F are natural numbers as well as the special constant +oo. 
                    The requirements imply that for every к the value F(x, к)  is an upper bound of 
                    f(x).  This upper bound becomes more precise as к increases.  For every x there 
                    is a A: for which this upper bound is tight.  However, we do not know the value of 
                    that k.  (If there is an algorithm that given any x finds such к, then the function / 
                    is computable.)  Evidently, any computable function is upper semicomputable.
                           A function / is upper semicomputable if and only if the set
                                                                     Gf = {(x,n) I f(x) < n}
                    is enumerable.  This set is sometimes called the “upper graph of / ”, which explains 
                    the strange names “upper semicomputable”  and “enumerable from above”.
                           Let us verify this.  Assume that  a function /  is upper semicomputable.  Let 
                    F(x,k) be the function from the definition of semicomputability.  Then we have
                                                                  f(x) < n             3k F(x, к) < n.
                    Thus,  performing in parallel the computations of F(x, к)  for all x and k,  we can 
                    generate all the pairs in the upper graph of /.
                           Assume now that the set Gf is enumerable.  Fix an algorithm enumerating this 
                    set.  Then define F(x, к)  as the best  upper bound of / obtained after к steps of 
                    generating elements in Gf.  That is,  F(x,k)  is equal to the minimal n such that 
                    the  pair  (x, n + 1)  has  been  printed  after  к  steps.  If there  is  no  such  pair,  let 
                    F(x, k) — Too.
                           Using  the  notion  of an  upper  semicomputable  function,  we  can  reformulate 
                    Theorem 7 as follows.
                           Theorem 8.  (a)  The function C is upper semicomputable and
                                                                        \{x I C(x) < n}\ < 2n
                    for all n.
                           (b)            If a function C'  is upper semicomputable  and |{x  |  C'{x)  <  n}\  <  2n for 
                    all n,  then C(x) ^ C'{x) + c for som,e c and for all x.
                           Note that the upper bound 2n of the cardinality of \{x \ C'{x) < n}\  in item 
                    (b) can be replaced by a weaker upper bound 0(2n).
                           Theorem 8 allows us to define Kolmogorov complexity as a minimal (up to an 
                    additive constant) upper semicomputable function к that satisfies the inequality
                                                                    \{x I k(x) < n}\ ^ 0(2n).
                    One can replace the requirement of minimality in this definition by some other prop­
                    erties of G.  In this way we obtain the following axiomatic definition of Kolmogorov 
                    complexity [173]:
                           Theorem 9.  Let к be a natural-valued function defined on binary strings.  As­
                    sume that к satisfies the following properties:
                           (a)  к  is upper semicomputable  (enumerability axiom);
                           (b) for every partial computable function A from E to E,  the inequality
                                                                         k(A(x)) ^ k(x) + c
                         1.  PLAIN  KOLMOGOROV  COMPLEXITY
         20
         is  valid for some c  and all x  in the domain of A  (the axiom guarantees that com­
         plexity does not increase);
            (c)  the  number of strings x  such that k(x)  <  n  is  in  the  range  [2n~Cl ; 2n+c'2] 
        for som.e c\, C2  and for any n  (calibration axiom).
            Then k(x) = C(x) + 0(1), that is,  the difference |k(x) — C{x)|  is bounded by a 
         constant.
           Proof.  Theorem 8 implies that C(x) ^ k(x) + 0(1).  So we need to prove that
                              k(x) ^ C(x) + 0(1).
           Lemma  1.  There is  a  constant c  and a  computable sequence  of finite  sets  of 
         binary strings
                             M0 C Mi  C M2 C • • • 
         with the following properties:  the set Mi  has exactly 2г strings and k(x) ^ i + c for 
         all x € Mi and all i.
           Computability of Mo, Mi, М2, ■ ■ ■ means that there is an algorithm that given 
         any i computes the list of elements of M*.
           PROOF.  By axiom (c) there exists a constant c such that for all i the set
                             Ai — {x I k(x) < i + c}
        has at least  2г  elements.  By item  (a)  the family Ai  is enumerable.  Remove from 
        Ai  all the elements except 2*  strings generated first.  Let  Bi  denote the resulting 
        set.  The list of the elements of Bi can be found given i:  we wait until the first 2г 
        strings are generated.  The set Bi is not necessarily included in Bi+\.  To fix this 
        we define Mi inductively.  We let Mo = Bo, and we let Mi+1  be equal to Mi plus 
        any 2г elements of Bi+i that are outside Mi.  Lemma 1 is proven.
           Lemma 2.  There is a constant c such that k(x) ^ l(x) + c for all x (recall that 
        l(x)  denotes the length of x).
           Proof.  Let Mo, Mi, М2,...  be the sequence of sets from the previous lemma. 
        There is a computable one-to-one function A defined on the union of all Mi that 
        maps Mj+i \ Mi  onto the set of binary strings of length i.  (Recall that  the set 
        Mj+1 \ Mi  has exactly 2г  strings.)  By item  (b)  we have k(A(y))  <  k(y) + c'  for 
        some c' and all x.  For all x of length i there is y € M*+1 \ Mi such that A(y) — x, 
        hence k(x) ^ k(y) + c' ^ i + c for some c and all i.  Lemma 2 is proven.
           Let us finish the proof of the theorem.  Let D be the optimal description mode, 
        and let p be a shortest description of x with respect to D.  Then
                k(x) = k(D(p)) < k(p) + 0(1) ^ l(p) + 0(1) = C(x) + 0(1).
        Note that  we  have  used  property  (b)  twice:  in  the  proof of Lemma  2  and just 
        now.                                                    □
           |~4~[ Assume that strings over the alphabet  {0,1,2,3} are used as descriptions. 
        Prove that in this case the Kolmogorov complexity,  defined  as the length of the 
        shortest description (with respect to an optimal description mode), is equal to half 
        of the regular complexity (up to an additive constant).
           |~5~|  (Continued)  Formulate and prove a similar statement  for the n-letter al­
        phabet.
                                     1.2.  ALGORITHMIC PROPERTIES                         21
                |~6~| Assume that /  : N —> N is a total computable increasing function and
                                       Ihn inf f(n + l)//(n) > 1.
            Let An be an enumerable family of finite sets such that \ An\ ^ /(n) for all n.  Prove 
            that there is a constant c such that Cix) < log/(n) + c for all n and all x G An.
                |~7~|  Prove  that  for  some  constant  c  and  for every n the following holds.  For 
            every string x  of length  n  one  can  flip  a bit  in  x  so  that  the  resulting string  y 
            satisfies the inequality C(y) ^ n — logn + c.
                {Hint-.  For a given natural к consider a Boolean matrix of size к x (2fc — 1) whose 
            columns are all non-zero strings of length k.  (Such matrix is used for Hamming 
            codes.)  Consider the linear mapping B2  -1  —> Bfc  defined by this matrix,  where 
            В denotes the field {0,1}.  It is easy to verify that for every vector x one can flip 
            one bit in x  so that the resulting string у is in the kernel of this mapping,  and 
            the elements of the kernel have complexity at most 2k — к + 0(1).  This gives the 
            desired result for n = 2k — 1; if n does not have the form 2k — 1, we can flip one of 
            the first 2k — 1 bits for an appropriate k.)
                                    1.2.  Algorithmic properties
                The function C is upper semicomputable.  On the other hand,  it is not com­
            putable and, moreover, it has no unbounded computable lower bounds (Theorem 6, 
            p.  9).
                This implies that all optimal description modes are necessarily non-total, that 
            is, some strings describe nothing.  Indeed, if a description mode D is total, then we 
            can compute Cd{x) just by trying all descriptions in lexicographical order until we 
            find the shortest one.
                At first glance, this contradicts to our intuition:  the bigger the domain of D, 
            the better D is.  If the optimal decompressor D is undefined on some string y, then 
            we can define  another description  mode D'  as  follows.  Let  D'{y)  be equal to  a 
            string 2 of complexity  (with respect to D)  greater than l{y),  and let D' coincide 
            with D on all other strings.  The description mode D' is a bit better than D, as the 
            complexity of all strings except 2 remains the same while the complexity of 2 has 
            been decreased.
                There is no formal contradiction here, as D is still not worse than D' (they differ 
            only at one point, the difference between the complexities is bounded by a constant, 
            and both D and D' are optimal).  However, this is still a bit strange.  This obser­
            vation was made by Yu. Manin in his book Computable and non-computable [114] 
            (by the way,  in this book he also discussed the computational power of quantum 
            mechanics long before quantum computing became fashionable).
                A similar argument shows that the domain of every optimal description mode 
            is undecidable.  (The set of strings is called decidable, or computable, if there is an 
            algorithm that for any given string decides whether it belongs to the set or not.) 
            Indeed,  if there were an algorithm deciding whether D(x)  is defined or not, then 
            there would be a total computable extension of D (for example, let D{x) = 0 for all 
            x outside the domain of D).  This extension would be a total optimal description 
            mode, but this is impossible as we have seen.
                As a byproduct we get an algorithm whose domain is undecidable.  This is one 
            of the central theorems in computability theory (see, for example,  [184]).
                         1.  PLAIN  KOLMOGOROV COMPLEXITY
         22
            In  general  the  notion  of  Kolmogorov  complexity  has  a  number  of  connec­
         tions with computability theory.  Recently, many interesting facts were discovered; 
         see  [147,  49].  We consider here only two basic examples  (a simple set of simple 
         strings and the complexity of large numbers).
            1.2.1.  Simple strings and simple sets.  In this section, the word  “simple” 
         has  two  unrelated  meanings.  First,  when  applied  to  strings,  it  means  that  the 
         Kolmogorov  complexity  of the  string  is  small.  Second,  it  is  applied  to  sets  of 
         strings.  The notion of a simple set was introduced by the American logician Emil 
         Post and has no relation to Kolmogorov complexity.
           Definition.  An enumerable set A is simple (according to Post) if its comple­
         ment is infinite but has no infinite enumerable subset.
           Call a string x simple if C(x) < l(x)/2.
           Theorem 10.  The set of all simple strings is simple in the sense of Post.
           Proof.  That set S of all simple strings is enumerable.  Indeed,  the function 
         C is upper semicomputable, and if C(x) is less than  |x|/2, this can be seen while 
         approximating C(x) from above.
           The number of strings of complexity less than n/2 does not exceed 2n/2.  There­
         fore the fraction of simple strings among strings of length n is negligible, and the 
         complement of S is infinite.
           Assume now that the complement of S has an infinite enumerable subset U.  We 
         can use U to obtain a computable unbounded lower bound of C.  To find a string of 
         complexity greater than t, we can generate elements of U until we find a string щ of 
         length greater than 21.  As U is infinite, there is such a string.  The complexity of щ 
         is greater than t; otherwise, ut is simple.  Without loss of generality we can assume 
        that  the strings щ, t =  1,2,...  are pairwise different.  Thus the function щ  i-> t 
         is  a  computable  unbounded  lower  bound for  C.  This  contradicts to  Theorem 6 
         (page 9).                                              □
           Note that the choice of the threshold Z(x)/2 in the definition of a simple string 
        was not essential.  The proof of Theorem 10 would work as well with l(x) — 1 or 
        log log/(x) in place of Z(x)/2.
           1.2.2.  Complexity of large numbers.  Let us identify a natural number m 
        with the binary string having index m in the standard enumeration of binary strings. 
        In this way C becomes a function of a natural argument.  The function C(m) goes 
        to  infinity  as  m —>  oo.  Indeed,  for  all n  there  are only  finitely  many  integers of 
        complexity less than n.  However, the convergence is not effective.  That is, there is 
        no algorithm that, for every given n, finds a number N such that the complexity 
        of N and of all larger numbers is bigger than n.  Indeed, such an algorithm would 
        provide an effective way to describe the number N, whose complexity is at least n, 
        by logn + 0(1) bits.  We have seen this in the proof of Theorem 6 (p. 9).
           In  this  section,  we  study  in  detail  the  rate  of convergence  of C  to  infinity. 
        Following Chaitin  [31],  we consider for every natural n the largest number В in) 
        whose complexity is at most n:
                         В (n) = max{m G N | C(m) + n}.
        The function п\-л B (n) may be called the modulus of the convergence of C(m) to 
        infinity (see Figure 1).  Indeed, C(x) > n for all x > B (n) (and B (n) is the minimal
                                      1.2.  ALGORITHMIC  PROPERTIES                        23
                    Figure 1.  The definition of B(n): the value C(m) does not exceed 
                    n—1 form — B(n—1) (the case when C(B (n—1)) = n—1 is shown), 
                    and C{m)  ^  n for all m  >  B(n — 1).  At the point  m = B(n), 
                    the value of C does not  exceed n  (the case when C(B(n))  =  n 
                    is  shown),  and  C{m)  >  n  for  all  m  >  B(n).  The  case  when 
                    C(m) is even greater than n + 1 for all m > B(ri) is shown, thus 
                    B(n + 1)  =  B{n).  For m  G  (B(n — 1 ),B(n)],  the value of the 
                    function C^(m) is equal to n.
            number with this property).  Note also that it can happen (for small values of n) 
            that C{m) > n for all m.  In this case we let B(n) — —1.
                The function В can be considered as an inverse function to the function
                                     C^(N) = min{C(m) | m > N}.
            The function C^>  grows very slowly.  It takes the value n between B(n — 1)  and 
            B(n),  more precisely,  on the interval  (B(n — 1 ),Æ(n)].  The slow increase of C^ 
            corresponds to the fast increase of В.  The latter can be illustrated by the following 
            result.
                Theorem 11.  Let f be a computable function from N to N.  Then B(n) ^ f(n) 
            for all but finitely many n.
                Note that /  may be a partial function.  In this case we claim that B(n) > f(n) 
            for all sufficiently large n that are in the domain of /.
                Proof.  As algorithmic transformations do not increase complexity, for some 
            constant c for all n we have
                                   C(f(n)) < C{n) + 0(1) < logn + c.
            On the other hand,  the  definition  of В  and  the  inequality  f(n)  >  B(n)  imply 
            C{f{n)) > n.  Thus
                                        n < C(f(n)) < logn + c 
            whenever f(n) > B(n).  This can happen only for finitely many n.              □
                Let  us  reformulate  the  definition of B(n)  as  follows.  Let  D  be  the  optimal 
            description mode used in the definition of Kolmogorov complexity.  Then B (n) is 
            the maximal value of D on strings of length at most n:
                                     B(n) = max{£)(x) | l(x) < n}.
            Recall that we identify natural numbers and binary strings and consider the values 
            of D as natural numbers.  The minimum of the empty set is defined as —1.
                                     1.  PLAIN  KOLMOGOROV  COMPLEXITY
             24
                 Consider now any partial computable function d : S -A N in place of D, and let
                             Bd(n) — max{d(x) | l(x) < n and d(x) is defined}.
             The next  theorem shows  that  the  function  В  is  the  largest  function  among  all 
             functions Bd in the following sense:
                 Theorem 12.  For every function d there is a constant c such that
                                              В din) ^ B(n + c)
             for all n.
                 Proof.  For every x of length at most n, the complexity of d(x)  is less than 
             n + c for some constant  c.  Indeed,  the complexity of d(x)  exceeds at  most  by a 
             constant  the complexity of x,  which is less than n + 0(1).  Hence d(x)  does not 
             exceed the largest number of complexity n + c or less, i.e., B(n + c).             □
                 This  (trivial)  observation is useful in the following special case.  Let M be an 
             algorithm, and let X be a set of binary strings.  A halting problem for M restricted 
             to X is the following problem:  given a string x £ X, find out whether M terminates 
             on x or not.
                 A classical result in computability theory states that for some algorithm M the 
             unrestricted halting problem (X = £) for M is undecidable.
                 We are interested now in the case when X is the set of all strings of bounded 
             length.  Fix some algorithm M and consider the running time t(x) of M for some 
             input x.  If M does not halt on x, then t(x)  is undefined.  Thus the domains of t 
             and M coincide.  By definition, Bt(n) is the maximal running time of M on inputs 
             of length at most n.  If we know Bt(n) or any larger number m, we can solve the 
             halting problem for M and every input x of length at most n:  Run M on input x\ 
             if the computation does not terminate after m steps, it never terminates.
                 We have seen that Bt(n) ^ B(n + c) for some constant c (depending on M). 
            Therefore, the knowledge of B (n + c) or any greater number is enough to solve the 
            halting problem of M on inputs of length at most n.  In other words, the following 
            holds:
                 Theorem  13.  For  every  algorithm  M  there  is  a  constant  c  and  another 
             algorithm  A  having  the following  property.  For  every n  and for  every  number 
            t  >  B(n + c),  the  algorithm A,  given n  and t,  produces  the  list  of all strings x 
             of length at most n such that M halts on input x.
                 This theorem says that the halting problem for inputs of length at most n is 
             reducible to the problem of finding a number greater than B(n + c).
                 If M is the optimal decompressor D, then the converse is also true:  given n 
            and the list  of all strings x of length at most n in the domain of D, we can find
             в {n).
                 Continuing this argument, we can prove the following result:
                 Theorem  14.  Let  BB (n)  denote  the  maximal  running  time  of the  optimal 
             decompressor D on strings of length at most n (in the domain of D).  Then
                               BB (n) < B(n + c)     and  В (n) < BB (n + c) 
            for some c and all n.
                           1.2.  ALGORITHMIC  PROPERTIES        25
            Proof.  Let an be the most time-consuming description of length at most n, 
         that  is,  the string x of length at most n in the domain of D that maximizes the 
         running time of D on x.  Knowing n and an, one can generate the list of all strings 
         of length at most n in the domain of D, and hence the number BB(n).  Both n and 
         an  can be encoded in one string of length n + 1,  the string 0 • • • 01an  (there are 
         n — l(ctn) zeros in the beginning).  Therefore, the Kolmogorov complexity of BB (n) 
         is at most n + 0(1), and BB(n) < B(n + c) for some c and all n.
           Let us prove the second inequality of the theorem showing that every t > BB(n) 
         has complexity at least n — 0(1).  Assume that t has a description и of length k\ 
         we need to show that к > n — 0(1).  Knowing и and n, one can effectively obtain 
         a  string  of complexity  greater  than  n.  Indeed,  we  reconstruct  t  (from  и)  and 
         wait  t  steps  for  every  description  of size  at  most  n.  This  gives  us  all  strings  of 
         complexity at most n, and we can take some other string.  By definition of B(n) 
         we conclude that the pair (u,n)  has complexity at least n — 0(1).  On the other 
         hand,  this pair can be described using к + 0(log(n — к))  bits if we join the self­
         delimited description of n — k and u.  Therefore, к + 0(log(n — к)) ^ n — 0(1), and 
         (n — k) — 0(log(n — к))  <  0(1), hence n — к ^ 0(1).  (We assumed that n > k; 
         otherwise, there is nothing to prove.)                 □
           This theorem shows that, within an additive constant in the argument,  B(n) 
         is the maximal running time of the optimal decompressor on descriptions of length 
         at most n.  A similar function appeared in the literature under the name of “busy 
         beaver function”.  It was introduced by T. Rado [150] and is defined usually as the 
         maximal number of ones on the tape of Turing machine with n states and binary 
         tape alphabet (1 and blank) after it terminates (starting with blank tape).
           More generally, given n and any object from the following list, we can find any 
         other object from the list for a little bit smaller value of n:
            (a)  the  list  of  all  strings  of Kolmogorov  complexity  at  most  n  with  their 
               Kolmogorov complexities;
            (b)  the number of such strings;
            (c)  B(n);
            (d)  BB(n)\
            (e)  the  list  of all  strings  of length  at  most  n  in the  domain  of the  optimal 
               decompressor (the halting problem for the optimal decompressor restricted 
               to inputs of length at most n);
            (f)  the number of such strings;
            (g)  the most time-consuming input of length at most n for the optimal de­
               compressor;
            (h)  the graph Tn of the function C(x) on strings x of length n;
             (i)  the  lexicographically  first  string  7n  of length  n  with  Kolmogorov  com­
               plexity at least n (it exists since the number of strings of complexity less 
               than n is less than 2n).
           More specifically, the following statement holds.
           Theorem 15.  The complexity of every object in (a)-(i)  is equal to n + 0(1). 
         These objects are  equivalent to  each  other in the following sense:  Let Xn  and Yn 
         be objects described in two items among (a)-(i).  Then there is a constant c and an 
         algorithm that given n and Xn finds Yn- C•
                       1.  PLAIN  KOLMOGOROV  COMPLEXITY
        26
           P roof.  The equivalence of (d), (e), (f), and (g) is easy.  Each of the objects (d),
        (e),  (f), and (g) together with n determines the list of all terminating computations 
        of the optimal decompressor D on strings of length at most n.  Indeed,  knowing 
        BB(n), we can run D on all inputs of length at most n for BB(n) steps.  Knowing
        (e), that is, the list of strings of length at most n in the domain of D, we can run 
        D on all those strings until all the computations terminate (and we know that this 
        happens).  Knowing  (f),  the number of strings of length at  most  n on which  D 
        terminates, we run D on all strings of length at most n until the desired number 
        of computations do terminate.  Knowing the string (g), we run D on that string, 
        count the number of steps t, and then run D on all other strings of length at most 
        n for t steps.
          Conversely, the list of all halting computations of the optimal decompressor D 
        on strings of length at most n together with n identifies each of the objects (d)-(g) 
        as well as the objects  (a)-(c).  Therefore, by transitivity  (which is easy to check), 
        all the objects (d)-(g) are equivalent.
          Let us prove now that  (a)-(c) are equivalent to each other and equivalent to
        (d)-(g).  Given the list of strings of complexity at most n, we can find the number 
        of them (so (a)—>(b)) and the largest number of complexity at most n (so (a)—>(c)).
          It is not that easy to find (a) given (b) and n.  Given n and the number of strings 
        of complexity at most n,  we can reconstruct  the list of these strings  (generating 
        them until we obtain the desired number of strings)  and find a maximal number 
        among them ((b)—»(c)).  But we still do not know the Kolmogorov complexity of 
        the generated strings.  We will prove the implication (c)—»(a) indirectly, by showing
        (c)->(d);  we  know already that  (d)  implies  (a).  This will  prove  that  all objects
        (a)-(g) are equivalent.
          The implication (c)—>(d) follows from Theorem 14.  We know that B(n) is an 
        upper bound for BB(n — c)  (for appropriate c).  Thus, given n and B(n), we can 
        find BB (n — c) as follows:  run D on all inputs of length at most n — c within B (n) 
        steps.  Then find BB (n — c) as the number of steps in the longest run.
          It remains for us to consider the objects (h) and (i).  The implication (a)—»(h) 
        is easy.  Indeed, for some constant c the complexity of every string of length n — c 
        does not exceed n.  If we know the list (a) and n, then removing all the strings of 
        length different from n — c from the list, we get (h) for n — c.
          The conversion (h)—>(i) is straightforward.
          Thus it  remains to prove  (i)—>(a).  It is enough to show that,  given the lex­
        icographically first  string 7n  of length n  and  complexity at  least  n,  we  can  find 
        BB(n — 0(1))  or a number  greater  than  BB(n — 0(1)).  This  can  be  done  as 
        follows.
          Given 7n, find n, and for each string x of length n preceding 7n in lexicographi­
        cal order, find a description px of x that has length n or less, and find out the running 
        time tx of D on px.  (Note that px may be not the shortest description of x.)  Let T 
        be the maximum of tx for those x.  We claim that T > BB{n — c) for some c that 
        does not depend on n.  Assume that this inequality is false, that is, T ^ BB(n — c). 
        We will prove that then c is small.  Consider the most time-consuming description 
        an-c of length at most n — c; let n — c — d be its length.  Given an- c and c + d, we 
        can find n and BB (n — c).  From this we can find 7n:  run D on all strings of length 
        at  most n within BB (n — c) steps.  Consider all the strings of length n for which 
        we have found descriptions of length n or less.  Then 7n  is the lexicographically
                                       1.2.  ALGORITHMIC  PROPERTIES                        27
            first remaining string (since T ^ BB(n — c) according to our assumption).  As the 
            complexity of 7n is at least n, we have n ^ C(7n) < (n — c — d) + 2 log(c + d) + 0 (1), 
            hence (c + d) = 0 (1).
                We have thus proven the equivalence of objects  (a)-(h).  It remains to prove 
            that the complexity of each of them is n + 0 (1).
                Let Xn be one of objects (a)-(h).  We have just proven that Xn can be obtained 
            from 7n+c  and n  (actually,  we do  not  need n,  as  n  =  /(7n+c) — c).  Therefore, 
            C(Xn) ^ C(ln+c) + 0(1) < n + 0(1).
                To prove the lower bound of C(Xn), let n — d be the complexity of Xn.  For 
            some constant c the string 7n_c can be obtained from the shortest description of 
            Xn of length n — d and from d (note that n can be retrieved from the length of 
            the shortest description and d).  Thus, n — c ^ C(^n-c) ^ (n — d) + 21ogd + 0(1). 
            Therefore, d ^ 2 logd + c + 0(1) and, hence, d = 0(1).                          □
                [~8~| The objects in Theorem 15 depend on the choice of the optimal decompres­
            sor.  In the proof we assumed that the same optimal decompressor is used in all the 
            items  (a)-(h).  Prove that  the statement of the theorem remains true if different 
            decompressors are used.
                [~9~| Prove that the complexity of all the objects in Theorem 15 becomes O(logn) 
            if we relativize the definition of Kolmogorov complexity by (F, that is, if we allow 
            the decompressor to query the oracle for the halting problem.
                We have seen that there exist a constant c and an algorithm A that, given the 
            string 7n,  solves  the halting problem  for  the  optimal  decompressor  on inputs of 
            length at most n — c.  This means that given an  “oracle”  that finds 7n  for every 
            given n, we can solve the halting problem.  The same can be done given an oracle 
            deciding whether a given string x is incompressible, that is, C(x) ^ l(x).  Indeed, 
            using that oracle, we can find 7n by probing all strings of length n.
                Using the terminology of computability theory,  we can say  that  the halting 
            problem is  Turing reducible to the set of incompressible strings (or its complement, 
            the  set  of compressible  strings).  This  implies  that  the  halting  problem  is  also 
            reducible to the  “upper graph”  of C, that is, to the set {(x,k)  | C(x) < к}.  Using 
            the terminology of computability theory, we say that the set of compressible strings 
            (as well as the upper graph of C) is  Turing complete in the class of enumerable sets 
            (this means that it is enumerable and that the halting problem is Turing reducible 
            to it).
                 10 Find some upper bound for the number of oracle queries for the set
                                            {(x, к)  I C(x) < к}
            needed to solve the halting problem for a fixed machine M and for all strings of 
            length at most n.
                1111 Let  /  be a computable partial function from N to N.  Prove that there 
            is  a  constant  c such  that  for  all  n,  such  that  f(B(n))  is  defined,  the  inequality 
            B(n + c) ^ f(B(n)) is true.
                (Hint:  The complexity of f(B(n)) is at most n + 0(1).)
                 12  Call a set U r-separable  [137]  if every enumerable set V disjoint with U 
            can be separated from U by a decidable set, that is, there is a decidable set R that 
            includes V and is disjoint with U.
                      1.  PLAIN  KOLMOGOROV  COMPLEXITY
        28
           (a)  Prove that  the the set  {(x,k)  |  C{x)  <  k}  (the upper graph of C)  is an 
        r-separable set.  The set of compressible strings is r-separable, too.
           {Hint-.  Assume that the upper graph of C is disjoint with some enumerable set 
        V.  The set of the second components of pairs in V is finite, otherwise we get an 
        unbounded computable lower bound for C.  That is, V is included in a horizontal 
        strip of finite height.  The intersection of the strip with the upper graph is finite.)
           (b) We say that a set U\ is m-reducible to a set U2 if there is a total computable 
        function /  such that U\  = f~1{U2).  Prove that if U2 is r-separable and U\  is m- 
        reducible to [/2, then U\  is r-separable as well.
           {Hint:  If V is an enumerable set disjoint with U\, then /(V) is an enumerable 
        set disjoint with U2 .  If R is a decidable set separating /(V) and U2, then f~1{R) 
        is a decidable set separating V and U\.)
          (c)  Prove that  there is an enumerable set  that is not r-separable  (such a set 
        does not m-reduce to the upper graph of C).
          {Hint:  There is a pair of disjoint enumerable inseparable sets.)
           13 Following [74], prove that the following problems are equivalent:  “for a 
        given integer n find some string of complexity at least n” and “for a given algorithm 
        without input find some string that is different from its output”  (if the algorithm 
        does not terminate, any string is OK). An oracle that fulfills one of these tasks can 
        be used to (effectively) fulfill the other.
          {Hint:  Given an algorithm, we can provide an upper bound for the complex­
        ity  of its  output—it  is  bounded  by complexity  (and therefore the length)  of the 
        algorithm itself.  On the other hand, to provide a string of high complexity means 
        to provide a string which is guaranteed to be different from the outputs of finitely 
        many algorithms.  At first, this looks like a more difficult task than for one algo­
        rithm (as the oracle does).  However, the following trick helps:  we may assume that 
        the outputs  are tuples  and construct  a tuple that  differs  from  the output  of ith 
        algorithm in ith position.)
           14  (Continued) Prove that both these problems are equivalent to the problem 
        of computing a fixed-point free function:  “for every algorithm construct  another 
        algorithm that computes a different function”  (not the same as the first one).
           15 (Continued) Prove that an enumerable oracle can solve these problems if 
        and only if it solves the halting problem  (M. Arslanov proved this result without 
        using Kolmogorov complexity).
          {Hint:  Assume that an enumerable oracle A allows us to compute strings of 
        arbitrarily high complexity.  Then let us compute a string of complexity at least n 
        using this oracle,  and look at  all elements of A that were questioned during this 
        computation.  How many steps are needed to enumerate all this elements?  This is 
        a big number:  any T greater than this number, has Kolmogorov complexity of at 
        least n, since T-approximation of A can be used instead of A.  On the other hand, 
        having an oracle for A, we can find T for a given n.)
          Kolmogorov  complexity  and  functions  В  and  BB  turn  out  to  be  useful  in 
        studying  the  so-called  “generic”  and  “coarse”  algorithms  that  solve  the  halting 
        problem for most inputs  (the fraction of errors converges to zero);  see  [11].  The 
        versions of these functions based on prefix complexity were introduced by Gâcs [57]; 
        see  also  [4]  for  recent  results  related  to  the  busy  beaver  functions  for  different 
        versions of Kolmogorov complexity.
                       1.2.  ALGORITHMIC  PROPERTIES   29
          We have shown only several (simple) examples that show how Kolmogorov com­
       plexity is related to computability theory (also called recursion theory).  This area 
       is now actively growing, so we refer the interested reader to two recent monographs 
       [147] by A. Nies and [49] by R. Downey and D. Hirschfeldt.
          Theorem 15 selects some very special objects among all objects of complexity n 
       (in fact, one object up to equivalence is described above).  At first glance, this seems 
       strange:  our  intuition  says  that  all  random  (incompressible)  strings  of length n 
       should be indistinguishable,  and any special property of a string could be used to 
       compress it.  However, we have found a very special random string 7n of length n. 
       This paradox can be explained as follows:  the individual properties of 7n do allow 
       us to find a short description for 7n, but we need the oracle for O' to decompress 
       that description.
          We will come back to this question in Section 5.7, which discusses “the number 
       of wisdom”  fl, and in Section 14.3, which studies two-part descriptions.
          Finally, let us note that although all the objects in Theorem 15 are equivalent, 
       they have very different lengths.  The lengths of (a),  (b),  (e)-(i) are about n while 
       the length of (c) and (d) grows faster than every computable function of n.
                                     CHAPTER 2
            Complexity of pairs and conditional complexity
                               2.1.  Complexity of pairs
             As we have discussed, we can define complexity of any constructive object using 
          (computable) encodings by strings.  In this section we deal with pairs of strings.  A 
          pair x,y can be encoded, e.g.,  by a string [x,y] — xOly;  here x stands for x with 
          doubled bits.  Any other computable encoding i,!/4  [x,y\ could be used (of course, 
          we need that  [x,y\ ф [x',y']  if x ф x' or у ф у').  Any two encodings of this type 
          are equivalent  (there are translation algorithms in both directions), so Theorem 3 
          (p. 5) guarantees that complexities of the different encodings of the same pair differ 
          by 0(1).
             Let  us  fix some encoding  [x,y\.  The  Kolmogorov complexity of a pair x,y is 
          defined as C([x,y]) and is denoted by C(x,y).  Here are some evident properties:
               •  C(x,x) — C(x) + 0(1);
               •  C(x, y) = C(y. x) + 0(1);
               .   C(x) ^ C(x, y) + 0(1); C(y) ^ C(x, у) + 0(1).
             The following theorem gives an upper bound for the complexity of a pair in 
          terms of complexities of its components:
             Theorem 16.
             C(x,y) < C(x) + 21og C(x) + C(y) + 0(1);
             C{x,y) ^ C(x) + logC(x) + 2 log log C{x) + C{y) + 0(1);
             C(x, y) ^ C{x) + log C(x) + log log C(x) + 2 log log log C(x) + C(y) + 0(1);
             (We can continue this sequence of inequalities indefinitely.  Also,  one can ex­
          change x and y.)
             PROOF.  This proof (for the first inequality) was already explained in the in­
          troduction (Theorem 4, p. 6).  The only difference is that we considered the con­
          catenation xy instead of a pair.  Let us repeat the argument for pairs.
             A computable mapping x i-+  x  (here x and x  are binary strings)  is called a 
          prefix-free  encoding,  if for  any two different  strings x  and у the string x is not  a 
          prefix of the string y.  (In particular, x ф у if x ф у.)  This guarantees that both и 
          and v can be uniquely reconstructed from ûv.
             An example of a prefix-free encoding is ж и  xOl,  where x stands for x with 
          doubled bits.  Here the block 01 is used as a delimiter.  However, this encoding is 
          not the most space-efficient one,  since it doubles the length.  A better prefix-free 
          encoding is
                                 x i  ^ x — bin(/(x)) Olx,
                                          31
                          2.  COMPLEXITY  OF PAIRS  AND  CONDITIONAL COMPLEXITY
             32
             where  (bin(/(:r))  is  the  binary representation of the  length  l{x)  of the string x). 
             Now
                                       l{x) = l{x) + 21og/(:r) + 0(1).
             This trick can be iterated:  for any prefix-free encoding x h+ x,  we can construct 
             another prefix-free encoding
                                               x h-> bin(/(:r)):r.
             Indeed, if bin(/(:r)):r is a prefix of bin(l(y))y, then one of the strings bin(/(:r)) and 
             bin(/(?/)) is a prefix of the other one, and therefore Ып(/(ж)) = bin(/(r/)).  Therefore 
             ж is a prefix of у, and l(x) = l(y), so x = y.  (In other words, we uniquely determine 
             the length of the string, since a prefix-free code is used for it, and we then get the 
             string itself knowing where it ends.)
                 In this way we get a prefix-free encoding such that
                                l(x) = l(x) + logl(x) + 2 log log l(x) + 0(1),
             then (one more iteration)
                        l(x) = l(x) + log l(x) + log log l(x) + 2 log log log l(x) + 0(1),
             etc.
                 Now we return to the proof.  Let D be the optimal decompressor used in the 
             definition of Kolmogorov complexity.  Consider a decompressor D' defined as
                                           D'(pg) = [D(p),D(q)],
            where p is  a prefix-free  encoding  and  [•, •]  is  the  encoding  of pairs  (used  in  the 
            definition of pairs complexity).  Since p is a prefix-free encoding, D' is well defined 
             (we can uniquely extract p out of pq).
                 Let p and q be the shortest descriptions of x and y.  Then pq is a description 
            of [x,y], and its length is exactly as we need in our theorem.  (The more iterations 
            we use for the prefix-free encoding, the better bound we get.)                     □
                 Theorem 16 implies that
                                     C(x, y) < C(x) + C(y) + O(logn)
            for  strings  x  and  у  of length  at  most  n:  one  may say  that  the  complexity of a 
            pair does not exceed the sum of the complexities of its component with logarithmic 
            precision.
                  16  Suggest  a natural  definition  for  the  complexity of a triple.  Show  that 
            C(x, y, z) < C(x) + C(y) + C(z) + O(logn) for every three strings x, у, z of length 
            at most n.
                 A natural question arises:  is it true that C(x,y) ^ C(x) + C{y) + 0(1)?
                 A simple argument shows that this is not the case.  Indeed, this inequality would 
            imply C(x, y) ^ l(x) + l(y) + 0(1).  Consider some N.  For each n = 0,1, 2,..., A, 
            we have 2n strings x of length n and 2N~n strings у of length N — n.  Combining 
            them, we (for a given n) obtain 2N different pairs (x, y).  The total number of pairs 
             (all n = 0,1,..., Af give different pairs) is (N + 1)2N.
                 Assume that indeed C(x,y) < l{x) + l(y) + 0(1) = A + 0(1) for all these pairs. 
            Then we get (N + 1)2N different strings [ж, у] of complexity at most N + 0(1), but 
            this is impossible (Theorem 7, p.  17, gives the upper bound 0{2N)).
                                        2.1.  COMPLEXITY OF  PAIRS                          33
                I  17 I Prove that there is no constant c such that
                                   C(x, y) ^ C(x) + log C(x) + C(y) + c
            for all X and y.
                (Hint:  Replace C in the right-hand side by I and count the number of corre­
            sponding pairs.)
                 18 (a) Prove that
            for any prefix-free encoding i h î  (here S is the set of all binary strings).
                (b)  Prove that if a prefix-free encoding increases the length of an n-bit string 
            at most by f(n), i.e., if l(x) < l(x) + f(l(x)), then E n 2“^"' < oo.
                This problem explains why a coefficient 2 appears in Theorem 16 (p. 31):  the
            series
                           V   1     V       1        V           1
                           ^  n2 ’   ^  n(logn)2 ’    ^  n log n(log log n)
            converge, while the series
                              V 1      V  1          V          1
                                 ' n ’     n log n ’    '  n log n log log n ’
            diverge.
                The following problem describes functions that can be used for bounds similar 
            to Theorem 16.
                 19  Let /: N —>■ N be a non-decreasing total computable function.  Prove that 
            the following three properties are equivalent:
                (a)  C(x, y) < C(x) + C(y) + f(C(x)) + 0(1);
                (b)  C(x,y) < l(x) + l(y) + f(l(x)) + 0(1);
                (c)  E n 2"/(n)  < °°-
                (Hint: .(a)  obviously implies  (b);  to get  the reverse implication,  consider the 
            shortest  descriptions.  To  derive  (a)  from  (c),  one  can  count  pairs  with  l(x) + 
            f(l(x)) + l(y) < щ one can also use results about prefix complexity (see Chapter 4, 
            Problem 107).  Finally, to derive (c) from (b), note that the right-hand side in (b) 
            is at most n + 0(1) if l(x) = к and l(y) = n — к — f(k), for к + f(k) ^ n.  So the 
            number of such pairs is at least  '^/2к2п~к~^к">  = 2n^2k2~^k^  where the sum is 
            taken over all к such that к + f(k) ^ n.)
                 20  Prove that all the inequalities of Theorem 16 become false if the coefficient 
            2 is replaced by 1 but remain true with the coefficient 1 + e for any e > 0.
                (Hint:  See the preceding problem.)
                 21  Prove that
                           C(x, y) < C(x) + log C(x) + C(y) + log C(y) + 0(1).
                 22 (Continued) Prove a stronger inequality:
                            C(x,y) ^ C(x) + C(y) + log(C(z) + C(y)) + 0(1).
            (Note that C(x) + C(y) can be replaced by max(0(x), C(y)).  This gives a factor 
            at most 2, which makes 0(1) after taking logarithms.)
                    2.  COMPLEXITY  OF  PAIRS  AND  CONDITIONAL  COMPLEXITY
          34
              23  Prove that C(x, C(x)) = C(x) + 0(1).
             {Hint:  C(x,C(.x)) ^ C(x) + 0(1) for evident reasons.  On the other hand, the 
          shortest description of x determines both x and C{x).)
              24  Prove that if C(x) ^ n and C{y) ^ n, then C(x, y) ^2n + 0(1).
                              2.2.  Conditional complexity
             When transmitting  a file,  one  could  try  to  save  communication  charges  by 
          compressing that file.  The transmission could be made even more effective if an old 
          version of the same file already exists at the other side.  In this case we need only 
          describe the changes made.  This could be considered as a kind of motivation for 
          the definition of conditional complexity of a given string x relative to a (known) 
          string y.
             A  conditional  decompressor  is  a  computable  function  D  of two  arguments, 
          the  description and the condition (both arguments and the value of D are binary 
          strings).  If D{y, z) — x, we say that y is a (conditional)  description of x when z is 
          known (or relative to z).  The complexity Cd{x\z) is then defined as the length of 
          the shortest conditional description:
                            CD(x\z) = min{l{y) I D(y, z) = x}.
          We say that (conditional) decompressor D\  is not worse than D2 if
                                CDl(x\z) ^ Cd2(x\z) + c
          for some constant c and for all x and г.  A conditional decompressor is  optimal if 
          it is not worse than any other conditional decompressor.
             Theorem 17.  There exist optimal conditional decompressors.
             Proof.  This conditional version of the Solomonoff- Kolmogorov theorem can 
          be proved in the same way as the unconditional one (Theorem 1, p. 3).
             Fix some programming language where one can write programs for computable 
          functions of two arguments, and let
                                   D(py, z) =p{y,z),
          where p{y, z) is the output of program p on inputs у and г, and p is the prefix-free 
          encoding of p.
             It is easy to see now that if D' is a conditional decompressor and p is a program 
          for D', then
                               CD{x\z) ^ CD'(x\z) + l(p).
          The theorem is proved.                                          □
             Again, we fix some optimal conditional decompressor D and omit index D in 
          the notation.
             Let us start with some simple properties of conditional complexity.
             Theorem 18.
                                 C(x I y) ^ C{x) + 0(1); 
                                 C{x\x) = 0(1);
                             C{f{x,y)\y) < C(x\y) -f 0(1); 
                                 C{x\y) < C{x\g{y)) + 0(1).
                                                              2.2.    CONDITIONAL COMPLEXITY                                                           35
                           Here g and / are arbitrary computable functions (of one and two arguments, 
                    respectively) and the inequalities are valid if f{x,y) and g(y) are defined.
                           Proof.  First inequalitj':  Any unconditional decompressor can be considered 
                    as a conditional one that ignores the second argument.
                           Second inequality:  Consider D such that D(p, z) — z.
                          Third inequality:  Let D be the optimal conditional decompressor used to define 
                    complexity.  Consider another decompressor D' such that
                                                                   D'[p, y) = f(D(p,y),y),
                    and apply the optimality property.
                          A similar argument works for the last inequality, but D' should be defined in 
                    a different way:
                                                                     D'{p,y) = D(p,g(y)).
                    The theorem is proven.                                                                                                             □
                            25             Prove that conditional complexity is “continuous as a function of its second 
                    argument”:  C(x\yO) — C(x\y) + 0(1);  C(x\yl) — C(x\y) + 0(1).  Using this 
                    property, show that for every string x and for every non-negative integer I ^ C(x) 
                    there exists a string y such that C(x\y) — I + 0(1).
                          A similar argument based on two-dimensional topology is used in [156].
                            26 Prove that for any fixed y the function x                                 C(x \ y) differs from C at most
                    by 2C(p) + 0(1).
                            27 Prove that C([x, z] \ [y, z])  ^  C(x\y) + 0(1)  for any strings x,y,z  (here
                           stands for the computable encoding of pairs).
                            28 Fix some “reasonable”  programming language.  (Formally, we require the
                    corresponding universal function to be a Gödel one.  This means that a translation 
                    algorithm exists for any other programming language; see, e.g., [184].)  Show that 
                    the conditional complexity C(x\y)  is equal  (up to an 0(1)  additive term) to the 
                    minimal complexity of a program that produces output x on input y.
                          (Hint:  Let D be an optimal conditional decompressor.  If we fix its first argu­
                    ment p, we get a program of complexity at most l(p) + 0(1).  On the other hand, 
                    if program p maps y to x, then C(x\y) = C(p(y)\y) ^ C(p) + 0(1).)
                          This  interpretation  of conditional  complexity  as  a  minimal  complexity  of a 
                    program with some property will be considered in Chapter 13.
                          If we restrict  ourselves  to  total  programs  (that  terminate  on  all  inputs),  we 
                   get an essentially different notion of conditional complexity that can be called total 
                    conditional complexity.
                            29 Show that the notion of total conditional complexity CT(x \ y), the minimal 
                    (plain)  complexity of a total program  that  maps  y  to  x,  is  well  defined  (i.e.,  it 
                    changes at most by 0(1) when we change the programming language in a reasonable 
                   way).  Prove that
                                                                C(x\y)^CT(x\y)^C(x)
                   with 0(l)-precision.
                           30  Show that the total complexity sometimes exceeds significantly the usual
                   conditional complexity:  for every n there exist two n-bit strings x and y such that 
                                                         C(x\y) — 0(1) while CT(x\y) ^ n.
                2.  COMPLEXITY OF  PAIRS  AND  CONDITIONAL COMPLEXITY
        36
           [Hint:  Let us enumerate all programs of complexity less than n defined on all 
        n-bit strings, and maintain two n-bit strings x and y with the following property: 
        none of the  programs  found  maps  y  to  x.  When  a new program  is  found  that 
        maps y to x, we choose a fresh value of y and then choose an appropriate x.  This 
        process is effective if n (=length of y) is given, it defines a partial function у н-» x, 
        so C(x\y) = 0(1) for every pair selected.)
           31 Let x and у be bit strings such that  CT(x\y)  ^  n and  CT(y\x)  < n. 
        Prove that there exists a program of a computable permutation of the set of bit 
        strings that maps x to у and has complexity at most 2n + 0(1).
           (Hint:  It  is  easy  to  construct  a string v  of length 2n + 0(1)  that  encodes  a 
        pair  of total  programs  /   that  maps  x  to  у  and  g  that  maps  y  to  x.  We  may 
        assume without loss of generality that x and y have 0 as their first bits.  Consider 
        a binary relation R on the set of strings that have first bit 0, defined as R(u, v)  : 
        ( f(u) — v) and (g(v) = и)). This is a decidable one-to-one correspondence between 
        decidable sets of strings with infinite complements, and it can be easily extended 
        to a computable permutation.)
           32  Show that the upper bound in the preceding problem cannot be improved 
        significantly:  for  every к there are two strings x  and у  of length n — 2k + 0(1) 
        such that C(x),C(y) ^ к + 0(1)  (and therefore  CT(x\y), CT(y\x)  ^ к + 0(1)), 
        but every permutation of n-bit strings that maps x to у has complexity at least 2k.
          (Hint:  Let  us first  select  (arbitrarily)  2k  strings  у  and  pair  them with some 
        string x.  Let  us  enumerate  computable  permutations  of n-bit  strings  that  have 
        complexity less than 2k.  If and when all selected pairs are served by some of these 
        permutations,  choose a new string x that is connected (by existing permutation) 
        with at most half of the selected y-strings.  After that Q,(2k) new permutations are 
        needed to connect new x to all y-strings.  Therefore at most  22k/Q(2k) — 0(2k) 
        x-strings will be used, so the final x and у have complexity at most к + 0(1).  The 
        selection of x connected with at most half of selected y-strings is always possible, 
        since each of the у-strings is connected with a small fraction of x-strings,  and we 
        can change the order of summation in the double sum.  Note that this argument 
        may be used to guarantee that one of the strings x and у belongs to a given set of 
        2k strings.)
          See [136] for the detailed proofs of these results about total conditional com­
        plexity.
          Many properties  of unconditional  complexity  have  conditional  counterparts 
        with essentially the same proofs.  Here are some of these counterparts.
            •  Function C(-1 •) is upper semicomputable (this means that the set of triples 
              (x,y,n) such that C(x\y) < n is enumerable).
            •  For any у and n the set of all strings x such that C(x \y) < n has cardi­
              nality less then 2n.
            •  For any у and n there exists a string x of length n such that C(x \y) ^ n.
           33  Prove that for any strings у and 2 and for any number n there exists a 
        string x of length n such that C(x \ y) ^ n — 1 and C(x \z) ^ n — 1.
          (Hint:  Both requirements are violated by a minority of strings.)
                                                               2.2.    CONDITIONAL COMPLEXITY                                                            37
                           Theorem 19.  Let (x,y) h-* k(x,y)  be an upper semicomputable function such 
                    that  the  set  {x  \  k(x,y)  <  n}  has  cardinality  less  than 2n  for  any  string y  and 
                    integer n.  Then C(x | у) < k(x, у) + c for some c and for all x  and y.
                           The proof repeats the proof of Theorem 8.
                           Using conditional complexity, we get a stronger inequality for the complexity 
                    of pairs (compared with Theorem 16, p. 31):
                           Theorem 20.
                                                  C(x, y) < C(x) + 2 log C(x) + C(y I x) + 0(1).
                           P r o o f.  Let D\  be an optimal unconditional decompressor, and let D2 be an 
                    optimal conditional decompressor.  Construct  a new unconditional decompressor 
                    D' as follows:
                                                              D'(pq) = [Di(p), D2(q, Di(p))\.
                    Here p stands for the prefix-free encoding of p, and  [•, •]  is a computable encoding 
                    of pairs  used in  the definition  of the complexity of pairs.  Let p be the shortest 
                    D\-description of x, and let q be the shortest ^-description of y conditional to x. 
                    Then the string pq is a ^-description of [x, у].  Therefore,
                                                C(x,y) < CD'(x, y) + 0(1) ^ l{p) + l{q) + 0(1).
                    As we have seen, one can choose a prefix-free encoding in such a way that l(p) is 
                    bounded by l(p) + 2 logl(p) + 0(1)  (see the proof of Theorem 16, p.  31),  and we 
                    get a desired inequality.                                                                                                           □
                           As before, we may replace 2 log C(x) by log C(x) + 2 log log C(x), etc., getting 
                    a better bound.  We also can use conditional complexity in the logarithmic term 
                    and write
                                                C(x, у) ^ C(x) -I- C(y I x) + 2 log C(y I x) + 0(1).
                    (In the proof we should then replace D'(pq) by D'(qp).)
                            34 Prove that
                                               C(x I z) < C(x \y) + 2logC(x Iy) + C(y \ z) + 0(1)
                    for all x, y, z  (a sort of triangle inequality).
                          If we are not interested in the exact form of the additional logarithmic term, 
                    the statement of Theorem 20 can be reformulated as
                                                          C(x, y) ^ C{x) + C(y I x) + O(logn)
                    for all strings x, y of length at most n.
                          It turns out1  that this inequality is in fact an equality.
                          Theorem 21 (Kolmogorov-Levin).
                                                          0(x, у) = C(x) + C(y I x) + О (log n)
                   for all strings x, у  of length at most n.
                           1This  is  the  first  non-trivial  statement  in  this  chapter,  and  probably  the  first  non-trivial 
                    result about Kolmogorov complexity; it was proven independently by Kolmogorov and Levin and 
                    published in [79,  225].
           38          2.  COMPLEXITY  OF  PAIRS  AND  CONDITIONAL COMPLEXITY
                      Figure 2.  The section At of the set A of all simple pairs
               PROOF.  Since we already have one inequality, we need to prove only that 
                                C(x,y) ^ C(x) + C(y\x) + O(logn)
           for all X and y of length at most n.
               Let  X  and  y  be some  strings  of length  at  most  n.  Let  a  be  the  complexity 
           C(x, y) of the pair (x, y).  Consider the set A of all pairs whose complexity does not 
           exceed a.  Then A is a set of cardinality 0(2°)  (in fact, at most 2a+1) and (x, y) is 
           one of its elements.
               For each string t consider the  “vertical section”  At of A:
                                       At = {и I  (t,u) € A)
           (see  Figure  2).  The  sum  of the  cardinalities  of all  At  (over  all  strings  t)  is  the 
           cardinality of A and does not exceed 0(2a).  Therefore there are few “large” sections 
           At, and this is the basic argument we need for the proof.
               Let m be equal to  |_log2 |Âe|J  where x is the first component of the pair (x,y) 
           we started with.  In other words, assume that cardinality of Ax is between 2m and 
           2m+1.  Let us prove that
               (1)  C(y |x) does not exceed m significantly;
               (2)  C(x) does not exceed a — m significantly.
               We start with (1).  Knowing a, we can enumerate the set A.  If we know also x, 
           we can select  only pairs whose first component equals x.  In this way we get an 
           enumeration of Ax.  To specify y, it is enough to determine the ordinal number of у 
           in this enumeration (of Ax).  This ordinal number takes m + 0(l) bits, and together 
           with a we get m + O(logn) bits for the conditional description of у given x.  Note 
           that a = C(x,y) does not exceed 0(n) for strings x and у of length n.  Therefore, 
           we need only O(logn) to specify a and n, and
                                      C(y |x) < m + O(logn).
               Now let us prove (2).  Consider the set В of all strings t such that the cardinality 
           of At is at least 2771.  The cardinality of В does not exceed 2a+1/2m; otherwise, the 
           sum \A\ =    \At\  would be greater than 2a+1.  We can enumerate В if we know a 
           and m.  Indeed, we should enumerate A and group together the pairs with the same 
           first coordinate.  If we find 2m pairs with the same value of the first coordinate, we 
           put this value into B.  Therefore, the string x (as well as every element of В) can 
           be specified by (a — m) + O(logn) bits:  a — m + 1 bits are needed for the ordinal
                                       2.2.  CONDITIONAL  COMPLEXITY                          39
             number of x in the enumeration of B. and О (log 77,) is used to specify a and m.  So 
             we get,
                                        C(x) < (a -  m) + О (log 72), 
             and it remains to add this inequality and the preceding one.                     □
                 This theorem can be considered as the complexity counterpart of the following 
             combinatorial statement.  Let dbea finite set of pairs.  Its cardinality is (obviously) 
            bounded by the product of the cardinality of A's projection onto the first coordinate 
            and the maximal cardinality of the sections Ax.  This corresponds to the inequality 
             C(x,y)  ^ C(x) + C(y I x) + О (log 72).  The reverse inequality needs a more subtle 
            interpretation.  Let  A  be  a set  of pairs,  and  let p  and  q  be some  numbers  such 
            that  the cardinality of A does not  exceed pq.  Then we can split  A into parts P 
            and Q with the following properties:  the projection of P onto the first coordinate 
            has at most p elements, while all the sections Qx of Q (for element in Qx the first 
            coordinate equals x)  have at most q elements.  (Indeed,  let P be the union of all 
            sections that have more than q elements.  The number of such sections do not exceed 
            p.  The remaining elements form Q.)  We return to this combinatorial translation 
            in Chapter 10.
                 Note that in fact we have not used the lengths of x and y, only their complex­
            ities.  So we have proved the following statement:
                 Theorem 22 (Kolmogorov-Levin, complexity version).
                                 C{x, у) = C(x) + C(y I x) + О (log C(x, y)) 
            for all strings x  and y.
                 35  Give a more detailed analysis of the additive terms in the proof, and show
            that
                       C{x) + C{y\x) < C(x,y) + 31ogC(x,y) + 0(loglogO(x,7/)).
                 36  Show that if C(x,y\k, I) < k+l, then C(x \kj) < k+0( 1) or C(y \ x, k, I) <
            I + O(l).
                 (Hint:  This is what we actually proved in the proof of Theorem 22.}
                 37  Show that О (log 77.) terms are unavoidable in the Kolmogorov-Levin the­
            orem in both directions:  for each n there exist strings x and у of length at most n 
            such that
                                  C(x,y) > C(x) + C(y\x) + logn -  0(1) 
            as well as strings x and у of length at most n such that
                                 C(x,y) ^ C(x) + C(y\x) - log72 + 0(1).
                 (Hint:  For the first  inequality we can refer to the remark after  Theorem  16 
            (p.  31).  For the second note that C(x,l(x))  = C(x)  for every x, while C(x\l(x)) 
            can be equal to l(x) + 0(1) and C(x) + 0(1).  Then we can take a random length 
            between n/2 and n and a random string of this length.)
                 38  Prove that changing one bit in a string of length n changes its complexity
            at  most  by  log 72 + О (log log 77.).  Prove  the  same  for  the  conditional  complexity 
            0(х|тг).
                As we have seen in  Problem  7  (p.  21),  for  every  72-bit  string x  there  exists 
            another string x' of the same length that differs from x in one position only such 
            that  C(x')  <  72 — log72 + 0(1)  (and  therefore  C(x'\n)  < n — logn + 0(1)).  In
            40          2.  COMPLEXITY  OF  PAIRS  AND  CONDITIONAL  COMPLEXITY
            particular, if x is incompressible (given n), one can change one bit in x and decrease 
            C(x I n).
                One can also move in the other direction:  if C(x | n) is small enough (this means 
            that C(x\n) ^ an for some positive constant a), we can increase this complexity 
            by changing one bit in n:  there exists some a > 0 such that for each n-bit string x 
            with C(x I n) ^ an one can change one bit in x and get another n-bit string x' such 
            that  C(x'\n)  >  C(x\n).  (The proof of this statement  requires  a more involved 
            combinatorial argument [24] than the decrease in complexity.)
                 39       Fix some unconditional decompressor D.  Prove that for some constant c 
            and for all integers n and к the following statement is true:  if some string x has at 
            least 2k descriptions of length at most n, then C(x | к) ^ n — к + c.
                (Hint:  Fix some  k.  For  each n consider  all strings x  that  have  at  least  2k 
            descriptions  of length  at  most  n.  The  number  of these  strings  does  not  exceed 
            2n~fc, and we can apply Theorem 19, p. 36.)
                Using this problem, we can prove the following statement about unconditional 
            complexity (see [103, Exercises 4.3.9, 4.3.10]):
                 40      Let  D  be some optimal unconditional decompressor.  Then there exists 
            some constant c such that for any string x the number of shortest D-descriptions 
            of x does not exceed c.
                (Hint:  The previous problems show that C(x) ^ n — к + 2 log к + 0(1), so for 
            C(x) — n, we get an upper bound for k.)
                411 Prove that there exists a constant  c with the following property:  if for 
            some x and n the probability of the event C(x \ у)  ^  к  (all strings у of lengths n 
            are considered as équiprobable here) is at least 2~l, then C(x\n, I) ^ к +1 + c.
                (Hint:  Connect each string у of length n to all strings x such that C(x \y) ^ k. 
            We get a bipartite graph that  has 0(2n+k)  edges.  In this  graph the number of 
            vertices  x  that  have  degree  at  least  2n~l  does  not  exceed  0(2k+l).  Note  that 
            C(x\n, I) does not include к—this is not a typo!)
                This problem could help us in finding the average value of C(x\ y) for given x 
            and all strings у of some length n.  It is evident that C(x \y) ^ C(x\n) + 0(1) since 
            n — l(y) is determined by y.  It turns out that for most strings у (of given length) 
            this inequality is close to an equality:
                42  Prove that there exists some constant c such that for each string x and for 
            all natural numbers n and d the fraction of strings у such that C(x | y) < C(x | n)—d 
            (among all strings of length n) does not exceed cd?/2d.  Using this statement, prove 
            that the average value of C(x | y) taken over all strings у of a given length n equals 
            C(x\n) + 0(1) (the constant in 0(1) does not depend on x and n).
                43  Prove that C(x\k) < к implies C(x) ^ к + 0(1).
                (Hint:  See Theorem 7.  One can also note that if a conditional description of x 
            given к has length k, then к is known anyway, and if this description is shorter, we 
            have enough space to specify the difference between к and the description length.)
                A similar (though not identical) statement:
                44 Prove that C(x) = C(x \C(x)) + 0(1).
                (Hint:  Assume that x has a conditional description q with condition C(x) that 
            is  shorter  than  C(x).  Then one can specify x by providing q  and the difference
                                                               2.2.   CONDITIONAL  COMPLEXITY                                                            41
                    C(x) — l(q), and we get a description of x that is shorter than C(x)—a contradic­
                    tion.)
                            45 Prove that for every n there exists an n-bit string x such that
                                                                  C(C(x) |x) = logn — 0(1).
                    (This is a maximal possible value, since C(x) ^ n for n-bit string x.)
                           This result  (in a bit weaker form)  was proven long ago by P.  Gâcs  [55].  Re­
                    cently E. Kalinina and B. Bauwens suggested a simple game-theoretic proof of this 
                    statement.  Here  is  a sketch  of their  argument  (see  [6]  for  details).  Consider  a 
                    rectangular game board of width 2n and height n.  Two players, White and Black, 
                    make alternating moves and place pawns of their respective colors into the board 
                    cells.  Unlike chess,  each  cell may  contain  both white and  black pawns  (at  most 
                    one of each color).  At each move a player may place several pawns into different 
                    cells  (or no pawns at all); after a pawn is placed, it cannot be moved or removed. 
                    Also Black can irreversibly mark some cells.  The players should obey the following 
                    restrictions:
                           (a)  each of the players can place at most  2г  pawns at row i  (the bottom row 
                    has number 0, the upper row has number n — 1);
                           (b) Black can mark at most half of the cells in each column.
                           A white pawn is declared killed if its cell is marked or if there is a black pawn 
                    below  it  (in  the  same  column).  The  game  does  not  end  formally  (though  it  is 
                    essentially a finite game); White wins if in the limit there is at least one non-killed 
                    white pawn.
                          White has a winning strategy in this game:  place a pawn in the top row and 
                    wait until it is killed.  If it  is killed  by the black pawn below,  switch to the next 
                    column  (for example,  White can go from left to right starting with the leftmost 
                    column).  If the pawn was killed by marking its cell,  White places another pawn 
                    just  below the first  one,  etc.  (We  may  assume that  Black makes only the move 
                    needed to kill White’s pawn; since only the limit position matters in the game, all 
                    of Black’s other moves can be postponed.)  Recall that Black can mark at most half 
                    of the column, so Black is forced to put some pawn in the column at some point. 
                    It cannot be done in all columns, since the sum of 2г for all rows is less (by 1) than 
                    the width of the table.  Note also that White will not violate restrictions on the 
                    number of pawns in each row, since in all the columns (except the currently active 
                    one) under each white pawn (in row i) there is a black pawn (in some row j < i), 
                    and the sum of 2J  for all j < i is less that  2г  and there is a space for one more 
                    white pawn.
                          After a winning strategy for White is described, consider the following “univer­
                    sal”  strategy for Black:  the cell (x, i) is marked as soon as we find that C(i\x) < 
                    logn — 1;  a black  pawn  in  placed  at  (x,i)  when  a  conditional  description  of x 
                    (given n) of length i is found.  It is easy to check that Black obeys the game rules. 
                    White wins,  and a live white pawn at the cell  (ж,г)  means that C(x\n)  > г  and 
                    C(i\x)  ^  logn — 1.  Since the actions of White  (playing against the computable 
                    strategy of Black) are computable, we conclude that C(x |n) < i + 0(1):  the set of 
                    white pawns in row i is enumerable and it contains at most 2г elements.
                          This  argument  shows  that  C(C(x\n)\x)  ^  logn — 1  (not  exactly  what  we 
                    wanted).  To get the desired result,  we  should change the game and consider in 
                    parallel boards of all sizes.
                        2.  COMPLEXITY  OF  PAIRS  AND  CONDITIONAL  COMPLEXITY
            42
                ___Prove that for some constant c for any string x and for every number n,
                 46
            there exists a string y of length n such that
                                        C(xy) ^ C(x I n) + n — c.
                (.Hint :  For a given n the number of strings x, such that C(xy) < к for any у of 
            length n. does not exceed 2A’/2n, and this property is enumerable.  So we can apply 
            Theorem 19 (p. 36).)
                 47  Let /  be a function with natural arguments and values.  Assume that
                                f(n) + eh ^ /(n + h) < /(n) + (1 -  e)h
            for some e > 0 and for all n and h.  Prove that there exist an infinite bit sequence 
            Ш whose n-bit prefix has complexity f(n) + 0(1) for every n.
                (Hint:  Let us add blocks of length h where h is large enough.  Each new block 
            being added to an n-bit prefix increases complexity by more than f(n + h) — /(n) 
            or by less than f(n + h) — f(n),  depending on the current situation  (whether we 
            are below or above the boundary).  To find a block with a big complexity increase, 
            we may use the previous problem; for a block with a small increase, we can use a 
            block of zeros.  Note that  (large enough)  h is fixed,  so it is enough to control the 
            complexity on the blocks’ boundaries.)
                48  Prove that an infinite sequence xqX\X2 • • •  of zeros and ones is computable 
            if and only if the values C(xо • ■ • xn_\ | n) (the complexities of its prefixes conditional 
            to their lengths) remain bounded by a constant.
                (Hint:  Consider an infinite binary tree.  Let S be the enumerable set of vertices 
            (binary  strings)  that  have  conditional  complexity  (w.r.t.  their  length)  less  than 
            some constant c.  The horizontal sections of S have cardinality 0(1).  We need to 
            derive from this that each infinite path that lies entirely inside S is computable. 
            We may assume that S is a subtree (only the strings whose prefixes are in S remain 
            in S).
                Let u) be an infinite path that goes through S only.  At each level n we count 
            vertices in S on the left of oj  (ln vertices) and on the right of oj  (rn vertices).  Let 
            L = lim sup ln and R = lim sup rn.  Let N be the level such that L and R are never 
            exceeded after this level.  Knowing L, R, and N, we can compute arbitrarily large 
            prefixes of oj.  We should look for a path tt in a tree such that at some level above 
            N there are at least L elements of S on the left of тг and at some (possibly other) 
            level above N there are at least R elements on the right of 7Г.  When such a path tt 
            is found, we can be sure that its initial segment (up to the first of those two levels) 
            coincides with oj.  This result was published in [108]  (attributed to A.R. Meyer).)
                49  Prove  that  in  the  previous  problem  a  weaker  assumption  is  sufficient: 
            instead of C{xо ■ • • xn-\ |n) = 0(1), we can require that C(xо • • • xn_i) < logn + c 
            for some c and for all n.
                (Hint:  In this case we get an enumerable set S of strings (=tree vertices) with 
            the following property:  the number of vertices below level N is 0(N).  This means 
            that the average number of vertices per level is bounded by a constant.  To use the 
            previous problem, we need a bound for all levels and not for the average value.  We 
            can achieve this if we consider only vertices x G S that have an extension of length 
            2l(x) that goes entirely inside S.  This result was published in [33].)
               Following Problem 48,  we can suggest  different definitions of the complexity 
            notion for computable bit sequences:
                                                                 2.2.   CONDITIONAL  COMPLEXITY                                                              43
                                 •  A minimal complexity of a program that, given n, computes xq - • ■ xn-i. 
                                    We can also consider a program that computes xv for input n, which gives 
                                    the same (up to 0(1)) complexity.  We denote this complexity by C(x).
                                 •  A minimal complexity of a program that, given n, computes xq ■ ■ ■ xn for 
                                    all sufficiently large n.  For other n (finitely many of them) this program 
                                    may provide a wrong answer or never terminate.  Complexity defined in 
                                    this way is denoted by C00(x).
                                 •  max{0(x'o • • -x’n_i |n) | n — 0,1,...}, denoted by M(x).
                                 •  limsup,,^^ C(x0 ■ ■ ■ xn_i I n), denoted by Moc(x).
                           There are evident relations between the notions
                                                                      iliooty) ^ M(x) < C(x)
                     (up to 0(1) additive term) and
                                                                     Moo{x) ^ О00(.т) < C(x)
                     (with the same precision).
                             50  Prove that there exists a computable bit sequence x such that Оoc{x) is
                    much less than M(x) (and, therefore, much less than C(x)).  More precisely, there 
                    exists a sequence xm of computable sequences such that C00(xm) — О (login) and 
                    M(xm) ^ m.
                            (Hint:  Consider the sequence xm  = ym000 • ■ •, where ym  is the lexicograph­
                    ically  first  string of length m that  has conditional complexity  (given m)  at  least
                             51  Prove that for some computable sequence x the value of M(x)  is much
                    less than C(x).  More precisely, there exist a sequence xm of computable sequences 
                    such that M(xm) = O(logm) and C(xm) ^ m.
                           (Hint :  Consider the sequence xm — (lßß(m^000 • • • ), where the number of ones 
                    before trailing zeros equals BB(m), defined on p. 24.)
                             52  Prove that Coo(x) ^ 2M00(x) + 0(1).
                           (Hint:  Use the same argument as in Problem 48.)
                           In fact, the constant 2 in the preceding problem is optimal, as shown in [52].
                             53 Consider strings of length n that have complexity at least n (incompressible 
                    strings).
                           (a) Prove that the number of incompressible strings of length n is between 2n~c 
                    and 2n — 2n~c (for some c and for all n).
                           (b)  Prove that the cardinality of the set of incompressible strings of length n 
                    has complexity n — 0(1) (note that this implies the statement (a)).
                           (c)  Prove that  if a string x of length 2n is incompressible,  then its halves xq 
                    and xq  (of length n) have complexity n — 0(1).
                           (d)  Prove  that  if  a  string  x  of length  n  is  incompressible,  then  each  of its 
                    substrings of length к has complexity at least к — О (log n).
                           (e)  Prove that for any constant c <  1 all incompressible strings of sufficiently 
                    large length n contain a substring of \clog2 n\  zeros.
                           (Hints:  (a) There is at most 2n — 1 descriptions of length less than n, and part 
                    of them is used for shorter strings:  Any string of length n — d  (for some d)  has 
                    complexity less than n.  This gives a lower bound for the number of incompressible
                        2.  COMPLEXITY  OF  PAIRS  AND  CONDITIONAL  COMPLEXITY
            44
            strings.  To prove the upper bound, note that strings of length n that have a prefix 
            of к zeros could be described by 2 log к + (n — k) bits.
                (b) Let t be the shortest description of the number of incompressible strings.  If 
            t has n — k bits, then knowing t and log к additional bits, we can reconstruct first n 
            and then the list of all incompressible strings of length n, so the first incompressible 
            string has complexity less than n, a contradiction.
                (c)  If one part of the string is has a short description,  the entire string has a 
            short description that starts with prefix-free encoding of the difference between the 
            length and complexity of the compressible part.
                (d) If a string has a simple substring, then the entire string can be compressed 
            (we need to specify the substring, its position, and the rest of the string).
                (e) Let us count the number of strings of length n that do not contain к zeros in 
            a row; a recurrent relation shows that this number grows like a geometric sequence 
            whose base is the maximal real root of the equation x = 2 — (l/xk), and we can 
            get a bound for complexity of strings that do not have к zeros in a row.)
                 54 Prove that (for some constant c) for every infinite sequence X0X1X2 • • •  of
            zeros and ones there exist infinitely many n such that
                                    C(xqX\ • • • xn-\) ^ n — logn + c.
            Prove that there is a constant c and the sequence xqX\X2 • • •  such that 
                                   C(xqXi • • • £n_i) ^ n — 2logn — c
            for all n.
                (Hint:  The  series  J^l/n  diverges  while  the  series  ^)(l/n2)  converges.  For 
            details see Theorem 95 and 99.)
                This result was published by Martin-Löf [117] for conditional complexity (and 
            a reference to an earlier unpublished work in Russian was given for unconditional 
            complexity; see also [225, Theorem 2.6]).
                55  For a string x of length n let us define d(x) and dc(x) as follows: 
                             d(x) — n — C(x)    and   dc(x) = n — C{x\n). 
            Show that they are rather close to each other:
                            dc(x) — 2logdc(x) — 0(1) ^ d(x) ^ dc(x) + 0(1).
                (Hint:  We need to show that C(x\n) = n—d implies C(x) ^ n—d+21ogd+0(l). 
            Indeed, let us take the conditional description of x of length n — d and put it after 
            the self-delimiting description of d that has size 2 log d + 0(1).  Knowing this string, 
            we can reconstruct d, then n, and finally x.})
                56 Prove that d(xy) = d(x) + d(y\x) + О (log d(xy)) for every two n-bit strings
            x and y.  (Here d(u) = l(u) — C(u).)
                (Hint:  Use Problem 36.)
                The intuitive  meaning of the difference between length and complexity as a 
            kind of “randomness deficiency”  is discussed (for different complexity versions) in 
            Chapter 5 and Chapter 14.
                57 Prove that for sufficiently large values of a constant c the enumerable set
            of pairs (x,y) such that C(x\y) < c is Turing complete (one can solve the halting 
            problem using an oracle for such a set).
                (Hint:  Use Problem  15  and the fact that the output of a program has 0(1) 
            conditional complexity given the program.)
                                               2.3.    COMPLEXITY AS  THE AMOUNT  OF  INFORMATION                                                            45
                                             2.3.  Complexity as the amount of information
                            As we know (Theorem 18), the conditional complexity C(y\x) does not exceed 
                    the unconditional one C(y)  (up to a constant).  The difference C(y) — C(y\x) tells 
                    us how the knowledge of y makes x easier to describe.  So this difference can be 
                    called the amount of information in x about y.  We use the notation I(x:y).
                           Theorem 18 says that I(x:y) is non-negative (up to a constant):  there exists 
                    some c such that I(x:y) ^ c for all x and y.
                             58 Let / be a computable function.  Prove that I(f(x):y)  ^ I{x:y) + c for 
                    some c and for all x, y such that /(x) is defined.
                           A generalization of this statement to probabilistic algorithms is possible.
                             59 Let /(x, r) be a computable function of two arguments, and let r be chosen 
                    at random uniformly among n-bit strings for some n.  Then for each I the probability 
                    of the event
                                                                     I(f(x,r):y) > I{x:y)+l
                    does not exceed 2~l+ot'C^ +c^ .
                            (Hint:  Use the conditional version of Problem 41.)
                           These properties of information can be described as  conservation laws for in­
                    formation (about something)  in algorithmic or random processes.  As Levin once 
                    put it,  “by torturing an uninformed person you do not get any evidence about the 
                    crime.”  He discusses this property (for different notions of information) in [100].
                           Recall that
                                                       C(x, y) = C(x) + C(y I x) + О (log C(x, y))
                    (Theorem 22, p. 39).  This allows us to express conditional complexity in terms of 
                    an unconditional one:  C(y|x) = C(x,y) — C(x) + 0(logC(x,y)).  Then we get the 
                    following expression for the information:
                                 I(x:y) = C(y) - C(y\x) = C(x) + C(y) - C(x,y) + 0(logC(x,y)).
                    This expression immediately implies the following theorem:
                           THEOREM 23 (Information symmetry).
                                                             I(x:y) = I(y:x) + 0(logC(x,y)).
                           So the difference between I(x:y) and I(y:x) is logarithmically small compared 
                    to C(x, y).  The following problem shows that at the same time this difference could 
                    be comparable with the values I(x:y) and I(y:x) if they are much less than C(x, y).
                             60  Let x be a string of length n such that C{x\n) ^ n.  Show that 
                                                      /(x:n) = C{n) + 0(1) and /(n:x) = 0(1).
                           The property of information symmetry  (up  to  a logarithmic  term)  explains 
                    why I(x:y)  (or I(y:x))  is sometimes called  mutual information in two strings x 
                    and y.  The connection between mutual information, conditional and unconditional 
                    complexities, and pair complexity can be illustrated by a (rather symbolic) picture 
                    (see Figure 3).
                           It shows that strings x and у have I(x:y) ~ I(y:x) bits of mutual information. 
                    Adding C(x\y) bits (information that is present in x but not in y, the left part), 
                    we obtain
                                            I(y:x) + C(x\y) « (C(x) - C(x\y)) + C(x\y) = C(x)
            46          2.  COMPLEXITY  OF  PAIRS  AND  CONDITIONAL  COMPLEXITY
                       Figure 3.  Mutual information and conditional complexity
                                                  x,y
                       Figure 4.  Common information in overlapping substrings
            bits  (matching  the  complexity  of x).  Similarly,  the  central  part  together  with 
            C(y\x) (the right part) give C(y).  Finally, all three parts together give us
                 C(x I y) + I(x : y) + C{y I x) = C(x) + C(y \ x) = C(x \ y) + C(tj) = C(x, y)
            bits (all equalities are true up to О (log n) for strings x and у of length at most n).
                In  some  cases  this  picture  can  be  understood  quite  literally.  Consider,  for 
            instance,  an incompressible string r  =  rq • • -rn  of length  n,  so  C{r\ ■ ■ - rn)  ^  n. 
            Then any substring и of x has complexity l(u) up to O(logn) terms.  Indeed, since 
            и is a substring of r, we have r = tuv for some strings t, v.  Then
                       l(r) = C(r) ^ C(t) + C(u) + C(v) ^ l(t) + l(u) + l(v) — l(r)
            (up to a logarithmic error),  and therefore all the inequalities are equalities  (with 
            the same logarithmic precision).
                Now take two overlapping substrings x  and у  (Figure 4).  Then C(x)  is the 
            length of x and C(y) is the length of у (up to O(logn)).
                The complexity C(x, y) is equal to the length of the union of segments (since 
            the pair  (x,y) is equivalent to this union plus information about lengths requiring 
            0(log?r) bits).
                Therefore,  conditional complexities C(x\y), C(y \x)  and the mutual informa­
            tion I(x:y) are equal to the lengths of the corresponding segments (up to O(logn)).
                However,  the mutual information cannot  always be extracted in the form of 
            some string (like it happened in our example, where this common information is 
            just the intersection of strings x and y).  As we will see in Chapter 11, there exist 
            two strings x and у that have large mutual information I{x:y) but there is no string 
            г that represents (materializes) this information in the following sense:  C(z \ x) ~ 0, 
            C(z I y) « 0 (all information that is present in г is also present both in x and in y), 
            and C(z) ~ I(x:y)  (all mutual information is extracted).  In our last example we 
            can take the intersection substring for г, but in general this is not possible.
                 61  Prove that for any string x of length at most n the expected value of the
            mutual information I(x:y) in x and the random string у of length n is O(logn)
                                              2.3.   COMPLEXITY  AS  THE  AMOUNT  OF  INFORMATION                                                         47
                           Now we move to triples of strings instead of pairs.  Here we have an important 
                    tool that can be called relativization:  most of the results proved for unconditional 
                    complexities remain valid for conditional complexities (and proofs remain valid with 
                    minimal changes).  Let us give some example of this type.
                           A theorem about the complexity of pairs (p. 31) says that
                                                     C(x, y) < C(x) + 2 log C(x) + C{y) + 0(1).
                    Replacing all complexities by conditional ones  (with the same condition z  in  all 
                    cases), we get the following inequality:
                                             C(x,y\z) ^ C(x\z) + 2\ogC(x\z) + C(y\z) + 0(1).
                    By conditional complexity of a pair x,y relative to z we mean, as one can expect, 
                    the  conditional  complexity of its  encoding:  C(x,y\z) = C([x,y\\z).  As for  un­
                    conditional  complexity,  the choice of encoding is  not  important  (the  complexity 
                    changes by 0(1)).
                           The proof of this relativized inequality repeats the proof of the unrelativized 
                    one:  we combine the description p for x (with condition z) and the description q for 
                    y (with condition z) into a string pq that is a description of \p, q]  (with condition z) 
                    relative to some suitable conditional decompressor.
                           This is nothing really new.  However, we may express all the conditional com­
                    plexities in terms of unconditional ones:  recall that C(x,y\z) = C(x,y,z) — C(z) 
                    and C(x\z) — C(x, z) — C(z), C(y\z) — C(y, z) — C(z) (with logarithmic precision). 
                    Then we get the following theorem:
                           Theorem 24.
                                                 C(x. y, z) + C(z) < C(x, z) + C(y. z) + O(logn) 
                    for all strings x, y: z  of complexity at most n.
                           Sometimes this inequality is called the basic inequality for complexities.
                           The same relativization  can  be  applied  to  Theorem  21  (p.  37)  that  relates 
                    the  complexity of a pair and  conditional complexity.  Then we get the following 
                    statement:
                           Theorem 25.
                                                     C(x,y\z) = C(x\z) + C(y\x,z) + O(logn) 
                    for all strings x, y, z  of complexity at most n.
                           PROOF.  We can follow the proof of Theorem 21, replacing unconditional de­
                    scriptions by conditional ones  (with  г  as  the condition).  Doing this,  we replace 
                    C(y\x) by C(y Ix, z).  One can say that now we work in three-dimensional space 
                    with coordinates x,y,z and apply the same arguments simultaneously in all planes 
                    parallel to the xy plane.
                           If this argument does not look convincing, there is a more formal one.  Express 
                    all the conditional complexities in terms of unconditional ones:
                                                                C(x, y\z) = C{x, y, z) - C{z),
                    and for the right-hand side
                                       C(x\z) + C(y\x,z) = C(x,z) — C(z) + C(y,x,z) — C(x,z).
                    We see that both sides coincide (up to O(logn)).  (A careful reader may note that 
                    this simplified argument gives larger hidden constants in 0(logn)-notation.)                                                          □
           48          2.  COMPLEXITY  OF  PAIRS  AND  CONDITIONAL  COMPLEXITY
                62  Prove that in Theorem 25 the weaker assumption “C(x\z) and C(y\x,z)
           do not exceed n”  is sufficient.
               We also relativize the definition of mutual information and let I(x:y\z) be the 
           difference C(y\z) — C(y\x,z).  As for the case of (unconditional) information, this 
           quantity is non-negative (with 0(1) precision).  Replacing conditional complexities 
           by the expressions involving unconditional  ones  (with logarithmic precision),  we 
           can rewrite the inequality I(x:y\z) ^ 0 as
                    C(y I z) - C(y IX, z) = C(y, z) - C(z) - C(y, x, z) + C(x, z) ^ 0.
           So we get the basic inequality of Theorem 24 again.
               In fact,  almost all known equalities and inequalities that involve complexities 
           (unconditional and conditional)  and information (and have logarithmic precision) 
           are immediate consequences of Theorems 21 and 24.  The first examples of linear 
           inequalities for complexities that do  not follow from basic inequalities were found 
           fairly recently (see [222, 223]) and they are rather complicated and not very intu­
           itive.  We discuss them in Section 10.13; we conclude our discussion here with two 
           simple corollaries of basic inequalities.
               Independent  strings.  We say that strings  x  and  y  are  “independent”  if 
           I(x:y) ~ 0.  We need to specify what we mean by      but we always ignore the 
           terms of order O(logn) where n is the maximal length (or complexity) of the strings 
           involved.
               Independent strings could be considered as some counterpart of the notion of 
           independent random variables,  which is central in probability theory.  There is a 
           simple observation:  if a random variable £ is independent with the pair of random 
           variables (a,ß), then £ is independent with a and with ß (separately).
               The  Kolmogorov  complexity  counterpart  of this  statement  (if a string  x  is 
           independent with a pair (y,z), then x is independent with y and x is independent 
           with z) can be expressed as the inequality
                                        I(x : (y, z)) ^ I{x:y)
           (and the similar inequality for z instead of y).  This inequality is indeed true (with 
           logarithmic precision), and it is easy to see if we rewrite it in terms of unconditional 
           complexities,
                         C{x) + C{y, z) -  C(x, y, z) ^ C(x) + C(y) -  C(x, y),
           which after cancellation of similar terms gives a basic inequality (Theorem 24).  (In 
           classical  probability  theory  one may  also  apply  a similar  inequality  for  Shannon 
           entropies.)
               Complexity of pairs and triples.  On the other hand, to prove the following 
           theorem (which we have already mentioned on p.  12), it is convenient to replace 
           unconditional complexities by conditional ones:
               Theorem 26.
                         2C{x,y,z) ^ C(x,y) + C(x,z) + C(y,z) + O(logn)
           for all strings x, y, z  of complexity at most n.
               PROOF.  Moving  C(x,y)  and  C(x, z)  to  the  left-hand  side  and  replacing  the 
           differences C(x, y, z) — C(x, y) and C(x, y, z) — C(x, z) by conditional complexities
                                               2.3.    COMPLEXITY AS THE AMOUNT OF  INFORMATION                                                              49
                     C(z\x,y) and C(y\x,z), we get the inequality
                                                      C(z\x,y) + C(y\x,z) < C(y,z) + O(logn).
                    It remains to rewrite the right-hand side of this inequality as C(y) + C(z\y), and 
                    note that C(z\x,y) < C{z\y) and C(y\x,z) < C(y).                                                                                         □
                           Instead we could just add two inequalities (the basic one and the inequality for 
                    the complexity of a pair):
                                                 C(x, y, z) + C(y) < C(x, y) + C(y, z) + O(logn),
                                                              C(x, y, z) < C(y) + C(x, z) + O(logn),
                    and then cancel C(y) in both sides.  (This proof, as well as the previous one, has 
                    an important aesthetic problem:  both treat x, y,z in a non-symmetric way while 
                    the statement of the theorem is symmetric.)
                           We return to the inequality of Theorem 26 and refer to its geometric conse­
                    quences in Chapter 10.
                           We can provide a more systematic treatment of the different complexity quan­
                    tities  related to  three strings  as  follows.  There  are seven basic  quantities:  three 
                    of them are complexities of individual strings,  another three are complexities  of 
                    pairs, and one more is the complexity of the entire triple.  Other quantities such as 
                    conditional complexity and mutual information can be expressed in terms of these 
                    seven complexities.  To understand better what requirements these seven quanti­
                    ties  should  satisfy,  let  us  make  a linear  transformation  in the seven-dimensional 
                    space and switch to new coordinates.  Consider seven variables ai,û2,... ,а? that 
                    correspond to the seven regions shown in Figure 5.
                                                       Figure 5.  New coordinates a\, <12,. •., 0,7
                           Formally,  the coordinate transformation is given by the following equations: 
                                                           C(x) = ûi + Û2 T a4 T a5?
                                                           C(y)  =  Û2  +  a 3 +  a 5  +  a 6?
                                                           C(z) =04 + 05 + 06 + a75 
                                                       C(x, y) = ai  + a2 + a3 + a4 + a5 + a6,
                                                       C(x, z) = ai  + a2 +< 24 + a5 + a6 + a7,
                                                        C(y, z) = a2  + a3 + a4 + a5 + a6 + a7,
                                                    C(x, y, z) — ai  + Û2 +a  3 + a4 “b a5 + a6 + a7-
                        2.  COMPLEXITY  OF  PAIRS  AND  CONDITIONAL  COMPLEXITY
            50
            Indeed, it is easy to see that these equations determine an invertible linear transfor­
            mation of M7:  each 7-t.uple of complexities corresponds to unique value of variables
            <2i, ..., Й7.
                Conditional  complexities  and expressions  for  mutual  information  are combi­
            nations of complexities and therefore could be rewritten in new coordinates.  For 
            example,
            I{x:y) = C(x) + C(y) — C(x, y) = a2 + a5 and C(x\y) = C(x,y) ~C(y) = cq +a4.
                What is the intuitive meaning of these new coordinates?  It is easy to see that 
            ai = C(x\y, z) (with logarithmic precision).  The meaning of аз (and a?) is similar. 
            The coordinate a2  (with the same precision)  is  I(x:y\z);  coordinates a4  and üq 
            have similar meaning (see Figure 6).  In particular, we conclude that for any strings 
            x, y, z the corresponding values of coordinates a\. a2, аз, а4, üq, a-j are non-negative 
            (up to O(logn) for strings X, y, z of complexity at most n).
                      Figure 6.  The complexity interpretation of new coordinates
                The coordinate as  is more delicate.  Informally,  we would like to understand 
            it  as the  “amount of mutual information in three strings x, y, z”.  Sometimes the 
            notation I(x:y.z)  is used.  However,  the meaning of this expression is not quite 
            clear, especially if we take into account that as can be negative.
                Consider the following example where as  < 0.  Let x and y be two halves of 
            an incompressible string of length 2n.  Then C(x)  = n,  C(y) — n,  C(x,y)  = 2n, 
            and I(x\y) — 0 (up to O(logn)).  Consider a string z of length n which is a bitwise 
            sum modulo 2 of x and y  (XOR-operation).  Then each of the strings x,y, z can 
            be reconstructed if two others are known;  therefore,  the complexities of all pairs 
            C(x,y), C(y, z), C(x, z) are equal to 2n (again up to O(logn)), and the complexity 
            C(x, y, z)  is also 2n.  The complexity of z is equal to n  (it cannot be larger, since 
            the length is n; on the other hand, it cannot be smaller, since z and y form a pair 
            of complexity 2n).
                The values of a\,..., a-j for this example are shown in Figure 7.
                          2.3.  COMPLEXITY  AS THE AMOUNT OF INFORMATION            51
                   Figure  7.  Two  independent  incompressible  strings  of length  n 
                   and their XOR
               Note that even if a5 is negative, the sums 05+ 02, 05 + 04 and a5 + üq, being 
           mutual information expressions for pairs, are 11011-negative.  (In our examples these 
           sums are equal to 0.)
               This example corresponds to the simple case of secret sharing of secret 2 be­
           tween two people:  if one of them knows x and the other one knows y, then neither of 
           them has any information about 2 in isolation (since I(x:z) ~ 0 and I{y:z) « 0)), 
           but together they can reconstruct 2 as a bitwise sum of x and y.
               One can check that we have already given a full list  of inequalities that  are 
           true for complexities of three strings and their combinations (all a,;, except for 05, 
           are non-negative,  as well as the three sums mentioned above).  We return to this 
           question in Chapter 10.
               Our diagram is a good mnemonic tool.  For example,  consider again the in­
           equality
                               C{x, y, z) ^ C(x, y) + C(x, 2) + C(y, 2).
           In our new variables it can be rewritten as 02 + 04 + 05 + ae  ^ 0 (you can easily 
           check it by counting the multiplicity of each a* in both sides of the inequality).  It 
           remains to note that 02 + 05  ^  0,  04  ^  0,  and  ae  ^  0.  (Alas,  the symmetry is 
           broken again!)
                63 Prove that I(xy.z) = I(x:z) + I(y:z\x) + 0(\ogn) for strings æ,y, 2 of 
           complexity at most n.
               (Hint:  Use the diagram.)
               This problem shows that information in xy about 2 can somehow be split into 
           two  parts:  information  in  x  about  2  and  information  in  y  about  2  (when  x  is 
           known).  This is somehow similar to the equality C(x, y) = C(x) + C(y \ x), but now 
           complexity is replaced by the quantity of information about 2.  As a corollary we 
           immediately get that if xy is independent with 2, then x is independent with 2 and, 
           at the same time, y is independent with 2 when x is known.  (Here independence 
           means that mutual information is negligible.)  A symmetric argument shows that y 
           is independent with 2 and x is independent with 2 when y is known.
                64  Show that properties  “x  is independent with y”  and  ux  is independent 
           with y when 2 is known”  are quite different:  each of them can be true when the 
           other is false.
                65  We say that strings x, y, 2, t form a Markov chain (a well-known notion in 
           probability theory now transferred to algorithmic information theory) if I(x : 21 y) 
           and I((x,y) :t\z) are negligible.  (Of course, we need to specify what is “negligible” 
           to get a formal definition.)  Show that the reversed sequence of strings also forms a 
           Markov chain, i.e., that I(t:y\z) and I((t,z) :x\y) are negligible.
       52      2.  COMPLEXITY  OF PAIRS AND  CONDITIONAL  COMPLEXITY
          (.Hint:  Since I((x,y) :t\z)  = I(y:t\z) + I(x:t |y, z),  the left-hand side in this 
       equality is zero if and only if both terms in the right-hand side are zero;  and the 
       second term in the right-hand side does not change when the order of x, y, z, t is 
       reversed.)
                                                  CHAPTER 3
                                     Martin-Löf randomness
                 Here we interrupt the exposition of Kolmogorov complexity and its properties 
             to  define  another basic  notion of the  algorithmic  information  theory,  the  notion 
             of a Martin-Löf random  (or  “typical”)  sequence.  This  chapter does not  refer to 
             the preceding one, which is not used again until Chapter 5 where we characterize 
             randomness in terms of Kolmogorov complexity.
                 Let us recall some basic facts of measure theory for the case of the Cantor space 
             of infinite sequences of zeros and ones.
                                             3.1.  Measures on Q
                 Consider the set  Q  = BN  whose elements are infinite sequences of zeros  and 
             ones.  This set is called  Cantor space.  For a binary string x we consider a set     of 
             all infinite sequences that have initial segment x.  For example, Qoo is the set of all 
             sequences that start with two zeros, and Од = ^ (where Л is the empty string).
                 The sets Q.x are called intervals.  All intervals and all unions of arbitrary families 
             of intervals are called  open subsets of Q.  In this way we get a topology on Q, and 
             this topology corresponds to a standard distance function on Q defined as follows: 
             the longer the common prefix two sequences ш — coquji • • •  and ui1 = u>qU)'x • • •  have, 
             the smaller the distance between them:
                                                 d(w,w') = 2“n,
             where n is the smallest index such that и>п ф u)'n.
                  66 Prove that topological space Q is homeomorphic to the Cantor set on the
             real  line.  (This set  is obtained from  [0,1]  by deleting the middle third,  then the 
             middle third of two remaining segments and so on.)
                 However, we are interested in measure theory rather than topology.  A family 
             of subsets of Q is called a a-algebra if it is closed under finite or countable unions 
             and intersections, and under negation (taking the complement).
                 A minimal cr-algebra that contains all intervals Q.x (and therefore all open sets) 
             is called the algebra of Borel sets.
                 Consider an arbitrary cr-algebra that contains all intervals.  Let p be a function 
             that maps every set in this cr-algebra into a non-negative real number, and has the 
             following property (called a-additivity)-.
                       if a set  A  is  a union  of a countable or finite family of disjoint 
                       sets  Ao, A\, A2,...  that  belong to the cr-algebra on which p is 
                       defined, then
                                     p{A) — p{Aq) + p(Ai) + р{АФ) + • • •
                       (the right-hand side is a finite sum or a converging series with 
                       non-negative terms).
                                                        53
            54                         3.  MARTIN-LOF  RANDOMNESS
            Then fi is called a measure on Q, and the value ß{Ä) is called the measure of the 
            set A.  The set A for which ц(А) is defined, is called /r-measurable.
                A  measure  fi  such  that  //(Г2)  =  1  is  called  a  probability  distribution  on  Q. 
            Elements of the cr-algebra that is the domain of /j are called  events,  and /j(A)  is 
            called the probability of the event A.
                Any measure is monotone (А С В implies /r(A) ^ ß{B)).  Indeed,
                                       р(В)-1л{А) = ц{В\А)> 0.
                Another important property of measures is continuity:  if a set В is a union of 
            increasing sequence of sets
                                           B0 c Bx C B2 C • • •  ,
            then ß(Bn) tends to ti(B) as n    oo.  (Indeed, let us apply the additivity property 
            to  all  sets  Ai  =  Д  \ Д -i  and  then  to  all  sets  Аг  such  that  i  ^  n.)  A  similar 
            property holds for decreasing sequences of sets.
                For any measure /j on Q let us consider a function p defined of binary strings
            as
                                              p(x) = n(Qx).
            This function has non-negative real values and satisfies the additivity property
                                           p(x) = p(x 0) +p{x 1)
            for  any string x.  (Indeed,  the interval Qx  is the union of its two halves Qxq  and 
            ПХ1, which are disjoint sets.)
                As we know from measure theory (the Lebesgue theorem), an inverse transition 
            is possible.  Namely,  for every additive function p on binary strings that has non­
            negative real values, the Lebesgue theorem provides a measure pi such that /x(Qx) = 
            p(x) for all binary strings x.
                The measure provided by the Lebesgue theorem has the following additional 
            property:  if /r(A) = 0 for some set A and В C A, then pi(B) is defined (and therefore 
            pt(B)  =  0).  In  the  sequel  we  consider  only  measures  that  have  this  additional 
            property.
                We do not explain the Lebesgue construction here but refer the reader to any 
            textbook in measure theory,  e.g.,  [81,  63].  However,  let  us  recall  the definition 
            of sets  having measure 0,  since the Martin-Löf definition of randomness  uses its 
            effective version.
                Let p be an additive non-negative real-valued function on strings.  We call p(x) 
            the measure of the interval Qx.  A subset A C fl is a null set  (a set  of measure 0) 
            if for every e > 0 there exist a finite or countable family of intervals that cover A 
            and have total measure at most e.
                In other words,  a set  A is a null set if there exists a function  (e,i)  (->■ x(s,i) 
            (the first argument is a positive real, the second argument is a non-negative integer; 
            values are binary strings) such that
                   •  A C  f^x(e,0) LI    U f^x(e52) ' ' '  cUld
                   •  p(x(e, 0)) + p(x{e, 1)) + p{x{e, 2)) + • • ■ ^ e
            for every positive e.  Note that the family of intervals can be finite, since we do not 
            require the function x to be total (undefined values are skipped both in the union 
            and in the sum).
                                            3.2.  THE  STRONG  LAW  OF  LARGE  NUMBERS                                      55
                      Here are some simple but useful observations:
                          •  The definition does not change if we restrict ourselves to rational values 
                             of £ (or even let e = 2~k for integer k).
                          •  Any subset of a null set is a null set.
                          •  A finite or countable union of null sets is a null set.  (Indeed, to cover the 
                             union by a family of intervals of total measure less than e,  we combine 
                             the covers of its parts that have measure less than er/2, ^/4, er/8, etc.).
                          •  Assume that p is chosen in such a way that any singleton is a null set 
                             (it  is equivalent to the following property:  for any infinite sequence и =
                                       • ■ •  the  limit  of р(и>о ■ ■ -cjn)  (as  n      oo)  equals  0).  Then every 
                             finite or countable set is a null set.
                      A uniform measure on О assigns to each interval                        the number              :
                                             p(x) = 2~n for all strings x of length n.
                The uniform measure is closely related to the standard measure on E  (or,  more 
                precisely, on [0,1]).  Formally, the measure of a set А C fl is equal to the measure 
                of the set  of reals whose binary expansions are elements of A.  (In fact,  the cor­
                respondence between infinite binary fractions and reals in  [0,1]  is not a bijection, 
                since numbers of the form k/2l  for integer к and I have two representations, e.g., 
                0.01111 • • •  =  0.10000 • • •.  But  this  happens  only  for a countable  family of reals, 
                and measure theory easily ignores this.)
                      Indeed, the reals, whose binary expansions start with x, form an interval, and 
                the length of this interval is 2~n where n is the length of x.  This implies that for 
                every interval I C  [0,1]  the uniform measure of the sequences that represent reals 
                in I is equal to the length of the interval I.
                      Probability theory describes the uniform distribution as the probability distri­
                bution for the outcomes of independent fair coin tossing.  Indeed, for n independent 
                fair coin tossings, all 2n binary strings of length n appear with the same probability 
                2~n.  The set Qx is the event  “a random sequence of zeros and ones starts with ж”, 
                and this event has probability
                      Similarly, we may consider a biased coin assuming that coin tossings are still in­
                dependent.  The corresponding measure (probability distribution) is called Bernoulli 
                measure  (or Bernoulli distribution) with parameters q,p (probabilities of 0 and  1 
                respectively; we assume that p,q ^ 0, and p + q = 1).
                     With respect to this distribution,  the event  “sequence cj  starts with a string 
                x”  has probability qupv where и and v are the numbers of zeros and ones in x.  In 
                other words, we consider a function
                                                             x*->qu{x)pv{x\
                where u{x) and v{x) stand for the numbers of zeros and ones in x, respectively.  (It 
                is easy to check that this function has the additivity property.)
                                         3.2.  The Strong Law of Large Numbers
                     To see all these notions in action, let us state and prove the so-called Strong 
                Law of Large Numbers (SLLN).
                     Fix some p, q ^ 0 such that p + q — 1.  Let Ap be the set of all infinite sequences
                          • • •  of zeros and ones such that the limit frequency of ones exists and is equal
                                       3.  MARTIN-LÖF RANDOMNESS
            56
            to p, i.e.,
                                                  +   •  •  •  +  OJn-1
                                      lim  -------------------------- = p.
                                     n—>oo          n
                Theorem 27.  The set Ap has measure 1 with respect to Bernoulli distribution 
            with parameters p and q.
                In other words, the complement of Ap, i.e., the set of all sequences that either 
            do not have limit frequency at all or have a limit frequency different from p, is a 
            null set (according to this distribution).
                Proof.  We prove this theorem for the uniform case (i.e., for p — q — 1/2) by 
            an explicit calculation.  The general case is left as an exercise (see also Section 9.6).
                Let  us  consider  first  a  finite  number  of coin  tossings  and  fix  some  n.  All 
            binary strings of length n have the same probability.  We claim that most of them 
            have approximately nj2 ones.  Assume that some threshold e is fixed.  How many 
            sequences have more than  (1/2 + e)n ones?  The answer can be found using the 
            Pascal triangle:  we have to sum up all the terms in the nth row starting from some 
            point that is slightly on the right of the midpoint.  In this part we have a decreasing 
            sequence of less than n terms, so the sum in question is bounded by the first term 
            multiplied by n.  (We do not need to be very accurate in our bounds and ignore 
            factors that are polynomial in n.  So we can omit the factor n in our bound.)
                The first term of the sum is the binomial coefficient
                                                   n!
                                               k\(n — k)V
            where к is the smallest integer not less than (1/2 + e)n.  We use Stirling’s approxi­
            mation
                                     m! = \/(2tt + o(l))m     ^   ,
            where e is the base of natural logarithms.  Ignoring polynomial (in n) factors and 
            using the notation и = к/n, v = (n — k)/n, we get
                n!    ^         (n/e)n                nn
            k\(n — k)\   {k/e)k(Jji — k)/e)n~k   kk{n — k)n~k
                                                      _ ____^                1   _ 2H(u,v)n
                                                         (un)UTl(vn)vn    yUTlyvn           ’
            where
                                      H(u, v) = — и log и — v log V.
            The value H(u, v) is called the Shannon entropy of a random variable that has two 
            values whose probabilities are и and v.  (We study Shannon entropy in Chapter 7.) 
            Figure 8 shows the corresponding graph (note that v = 1 — u).  It is easy to check 
            that H(u, 1 — u) achieves its maximal value (equal to 1) only at и = 1/2.
                Now we see that the number of binary strings of length n that have frequency of 
            ones greater than (1/2 + e) does not exceed ро1у(п)2я (1/2+е,1/2~е)п and therefore 
            is  bounded by  2cn+°^n\   where  c  is some constant  less  than  1  (depending on e). 
            Therefore, the fraction of these strings (among all strings of length n) exponentially 
            decreases as n increases.  The same is true for the strings that have frequency of 
            ones less than(l/2 — e).
                                    3.2.  THE  STRONG  LAW  OF  LARGE  NUMBERS                       57
                                Figure 8.  Shannon entropy as a function of и
                  Let us see where we are.  For each fixed e  >  0 we have proved the following 
             statement:
                  Lemma.  The fraction  of strings  of length n  where frequency  of ones  differs 
             from 1/2 at least by e (among all strings of length n)  does not exceed some Sn  that 
             decreases exponentially as n increases.
                  This lemma (without any specific claims for the fast convergence Sn  —>  0)  is 
             called the Law of Large Numbers.  To prove the Strong Law of Large Numbers we 
             need to know that the series ]T)n Sn is convergent.
                  We need to prove that the set A\/2 of all sequences that have limit frequency 
             of ones  equal to  1/2  has  measure  1.  In other words,  we need to prove that  the 
             complement of this set (we denote this complement by B) is a null set.
                  According to the definition of limit, the set В is the union (over all e > 0) of 
             the sets B£.  Here B£ is the set of all sequences such that frequency of ones in their 
             prefixes exceeds 1/2 + e (or is less than 1/2 — e) infinitely many times.
                  Evidently, we can consider only a countable set of different e (e.g., only rational 
             values), and the countable union of null sets is a null set.  Therefore it remains to 
             prove that the set B£ is a null set for each e.
                  The set B£ consists of the sequences that have arbitrarily long “bad”  prefixes. 
             Here a bad prefix is a string where the frequency of ones differs from 1/2 by more 
             than  e.  Therefore,  for  each  N  the  set  B£  is  covered  by  the  family  of intervals 
             Qx where x ranges over all bad strings of length at least N.  The total (uniform) 
             measure of all these intervals does not exceed
                                            Sn + <5jv+i + Sn+2 + • • •  »
             and this sum can be made small since the series ]TA Si is convergent.
                  (Probability theorists call this argument the Borel-Cantelli lemma.  In its gen­
             eral form this lemma says that if the sum of measures of some sets Ao,A\,...  is 
             finite, then the set of all points that belong to infinitely many Ai is a null set.)    □
                  One can get a bound for the number of bad strings of length n without Stirling’s 
             approximation.  We do it separately for bad strings that have too many and too few 
             ones.  For example, let us consider the set of all  “bad”  strings that have frequency 
             of ones greater than 1/2 + e.  To get a bound for the cardinality of this set, consider 
             two distributions  (measures)  on the set of all strings of length n.  The first  one,
        58              3.  MARTIN-LOF RANDOMNESS
        called L, is the uniform distribution:  all strings have probability 2~n.  The second 
       one,  called  S,  is  biased  (ones  are  more  likely  than  zeros)  and  corresponds  to  n 
        independent coin tosses where one appears with probability p = 1/2 + e.  In other 
       words, S(x) = qupv for a string x that has и zeros and v ones (here q = 1/2 — £ is 
       the probability of zero outcome).  The ratio S{x)/L{x) increases when the number 
       of ones  in  x  increases,  and  for  all  bad  strings  this  ratio  is  at  least  2n/2H^p,q^n. 
       Therefore,  the total  L-measure  of all  bad strings  does  not  exceed  their  total  S- 
       measure divided by this lower bound.  Recalling that the total S'-measure of all bad 
       strings does not exceed 1, we conclude that the total L-measure (i.e., the fraction) 
       of all bad strings does not exceed 2H(j>,q'>n/2n. So we get another proof of our bound, 
       which is less technical (though more difficult to find).  This proof works not only for 
       the uniform Bernoulli measure (p = 1/2), but also for arbitrary p (after appropriate 
       changes).
          67  Prove the Strong Law of Large Numbers for arbitrary p.
          (Hint:  Let po  and  qo  be  fixed  positive  reals  such  that  po + qo  =  1.  Then 
       the expression — po logp — go log q, where p, q are arbitrary positive reals such that 
       p + q — 1, is minimal when p — po, q = go-  See also Section 9.6, p. 275.)
          People often say that  “the Strong Law of Large Numbers guarantees that in 
       every random sequence (with respect to uniform Bernoulli measure) the frequency 
       of ones  tends  to  1/2.”  (The case of non-uniform  Bernoulli measures  is similar.) 
       However, in this sentence the word  “random”  should not be understood literally: 
       the phrase  “every random sequence satisfies a”  (for some condition a)  is  an id­
       iomatic expression that means that the set of all sequences that do not satisfy a is 
       a null set.
          A natural question arises:  Can we define the notion of a random sequence in 
       such a way that this idiomatic expression can be understood literally?  Let us fix 
       some distribution on Q, say, the uniform Bernoulli distribution.  We would like to 
       find some subset of Q and call its elements  “random sequences”.  Our goal would 
       be achieved if for any condition a the following two statements were equivalent:
           •  all random sequences satisfy the condition a\
           •  the set of all sequences that does not satisfy a is a null set.
          In other words, the sets of measure 1 should be exactly those sets that contain 
       all random sequences (and, maybe, some non-random ones).
          One more reformulation:  the set of all random sequences should be the smallest 
       (with respect to inclusion) set of measure 1, and the set of non-random sequences 
       should be the largest  (with respect to inclusion) null set.  Now it easy to see that 
       our goal cannot be achieved.  Indeed, any singleton in Q is a null set.  However, the 
       union of all these singletons is the entire space Q.
          In 1965 Per Martin-Löf (a Swedish mathematician who was Kolmogorov’s stu­
       dent  at  that  time)  found  that  we  can  save  the  game  if we  restrict  ourselves  to 
       “effectively null sets”.  There exists a largest  (with respect to inclusion) effectively 
       null  set,  and  therefore we can define  the notion of a random sequence  is such a 
       way that any condition a is satisfied for all random sequences if and only if the 
       set of all sequences that do not satisfy a is an effectively null set.  The Martin-Löf 
       construction is explained in Section 3.3.
                                        3.3.  EFFECTIVELY  NULL  SETS                         59
                                        3.3.  Effectively null sets
                 Let a measure on fl be fixed, and let p(x) be the measure of the interval flx. 
                 We say that a set A c fl is an effectively null set  (with respect to the given 
             measure) if for every e > 0 one can effectively find a family of intervals that cover 
             A and whose total measure does not exceed e.
                 Some details  should  be  specified  in  this  definition.  First,  we  consider  only 
            rational values of e (otherwise it is not clear how e could be given to an algorithm). 
             Second, we need to specify how the sequence of intervals (that cover A) is generated. 
            We do this as follows:
                 Definition.  A set A c fl is called an  effectively null set  (with respect to a 
            given measure) if there exists a computable function x(-, •) whose first argument is 
            a positive rational number, its second argument is a natural number and values are 
            binary strings, such that
                  (1)  А С Дг(е,0) U Дг(е,1) и Пх(е,2) ' ' ' ,
                  (2)  p(x(e, 0)) + p(x(e, 1)) + p(x(e, 2)) + ■ ■ ■ ^ e
            for any rational e > 0.  Note that we do not require the function x to be total;  if 
            x(e,i) is undefined, the corresponding term (in both conditions) is omitted.
                 68 Show that we get an equivalent version of the definition if we consider an 
            algorithm that gets e > 0 as an input and enumerates a set of binary strings (by 
            printing its elements with arbitrary delays between elements) such that intervals 
            flx  for generated x cover A and have  total measure  (the measure of the union of 
            the intervals)  at most e.  (Note that the total measure can be much smaller than 
            the sum of measures, if the intervals are not disjoint.)
                 69  Show that  we get  an equivalent  definition  if we  consider  only  rational 
            numbers of the form 2~k  (for integer к)  instead of all rational e.  Show that the 
            definition does not change if we replace the sign ^ by < in the second inequality.
                 (Hint:  Subtract from each interval its part covered by previous intervals, pos­
            sibly splitting it into several intervals.)
                 70  Show that we get an equivalent definition if we require that for each e > 0
            the domain of the function i     x(e,i) is an initial segment of N (or N itself).
                I 7 1 1 Show that we get an equivalent definition if we require that the family of 
            intervals is decidable (instead of being enumerable).
                 (Hint:  An interval can be split into small parts, so we may assume that intervals 
            in  the  sequence  have  non-increasing  length,  and  the family of intervals  becomes 
            decidable.)
                Let us give some examples of effectively null subsets of fl (with respect to the 
            uniform measure).
                A singleton, whose only element is a sequence of zeros, is an effectively null set. 
            Indeed,  for every e  >  0,  we find an integer к such that  2'~  k  <  £,  and consider a 
            covering that consists of one interval fioo---o (corresponding to the string of к zeros).
                Formally speaking, x(e, 0)  = 0fc, where 0k stands for the sequence formed by 
            к  zeros,  and  к  is  the smallest  integer such that  2' ~k  <  £.  The values  x(e,i)  are 
            undefined for i > 0.
                In  this  example the sequence 0000 • • •  can  be replaced  by  an  arbitrary  com­
            putable  sequence of zeros  and ones;  we need only consider its prefix of length  к 
            instead of 0k.
                                                3.  MARTIN-LÖF  RANDOMNESS
               60
                    However, for non-computable sequences the situation could be different:
                     72  Prove that there exists a sequence u> E S7 such that singleton  {w}  is not
               an effectively null set.
                    (Hint:  Consider all computable functions x that satisfy the second condition of 
               the definition of an effectively null set.  There are countably many such functions. 
               For each of them consider the largest set  A that satisfies requirement  (1)  of the 
               definition (i.e., the intersection of the unions of coverings over all e).  This set is an 
               (effectively) null set, and the union of a countable family of those sets is a null set. 
               Therefore, there exists a sequence cu which does not belong to this union.)
                    Let us note that the statement of this problem is a straightforward corollary 
               of the Martin-Löf theorem on the existence of the largest effectively null set (The­
               orem 28, p. 61) proved later in this section,  and the hint follows its proof.  As we 
               will see later,  the set  {w}  is an effectively null set if and only if the sequence ui is 
               not  “Martin-Löf random”.
                    It is easy to construct a non-computable sequence u> such that the singleton {w} 
               is an effective null set.  Indeed, consider any sequence of the form w = 0?0?0?0 • ■ • 
               (each second term is zero, the rest is arbitrary).  Let us show that {w} is indeed an 
               effectively null set.  To find a covering with total measure 2“n, consider all strings 
               of length 2n that are formed by n arbitrary bits interleaved with n zeros (as in u>). 
               There are 2n  strings  of this  form,  and each  corresponds  to  an interval of length 
               2-2n, so the total measure is 2“n.
                    In  fact  we  have  proved  a bit  more:  the  set  of all  sequences  that  have  only 
               zeros at even positions is an effectively null set.  Therefore, each of its subsets  (in 
               particular, every singleton) is an effectively null set.
                    Let us now return to the definition of an effectively null set and separate the 
               requirements used in this definition.  We say that a computable function x is “regu­
               lar”  if is satisfies the requirement (2).  The requirement (1) then says that for every 
              rational e > 0 the set A is a subset of the union
                                               ^х(е,0) C ^x(£,l) LI Г2x(e,2)     •
              Therefore, a regular function  “serves”  all the subsets of the set
                                      (^a;(e,0) L ^x(e,l) L    x(e,2) ' ' ' )         ^ x(e,i)•
                                  £>0                                         £>0  i
              So for each (computable) regular function x we get an effectively null set  (defined 
              by the formula above),  and effectively null sets  are all these sets  (for  all regular 
              functions) and all their subsets, and that’s all.
                   Before we  formulate the  Martin-Löf theorem,  let  us  give  the  definition  of a 
               computable measure on the set Q.
                   A real number a is called computable if there exists an algorithm that computes 
              rational approximations to a with any given precision.  Formally, a is a computable 
              real if there exists a computable function e h-> a(e) defined on all positive rational 
              numbers and having rational values such that
                                                        I a — a(e)|  < £
              for all rational £ > 0.
                    73          Show that we get an equivalent definition if we additionally require that 
              all  approximations  given  by  a  are  approximations  from  below,  i.e.,  a(e)  <  a 
              for all £.
                       3.3.  EFFECTIVELY  NULL  SETS   61
          (Hint:  We can transform any approximation to the approximation from below 
       losing only factor 2 in precision.)
          74  Prove that the sum, difference, product, and quotient of two computable
       reals are computable reals.
          75 Prove that e (the base of natural logarithms) and тг are computable. 
          76 Prove that elementary functions  (roots,  sine,  exponent,  logarithm,  etc.)
       preserve  computability,  i.e.,  have  computable  values  for  computable  arguments. 
       (We assume, of course, that the base is computable in the cases of logarithms and 
       exponents.)
          A measure p on Q is computable if measures of all intervals are computable 
       reals, and, moreover, we can effectively find an approximation algorithm for p(Llx) 
       given X.  Here is a formal definition:
          Definition.  A measure p on the set Q is  computable if there exists a com­
       putable function {x,e) !->■ a(x,e), defined for all strings x and all positive rational 
       numbers e, such that
                          \ß(üx) -  a(x, e)| < £
       for all x and e.
          This definition does not assume that the measure of the entire space Q equals 1, 
       but in fact we will use it only in this case (i.e., for probability distributions).
          Theorem 28.  Let p be a computable measure on fl.  Then there exists a largest 
       effectively  null  set  with  respect  to  p.  In  other words,  the  union  of all  effectively 
       p-null sets is an effectively p-null set.
          PROOF.  As we have seen, for each regular function x we get a corresponding 
       effectively null set.  Since there is countably many regular functions, we get count­
       ably many effectively null sets, and their union contains every effectively null set. 
       Therefore, the union of all effectively null sets is a null set.  (When speaking about 
       null sets and effectively null sets, we have in mind measure p.)
          However, we need more:  we have to prove that this union is an effectively null 
       set.  To  achieve  this  goal,  we  enumerate  all  regular  functions  and  then  use  the 
       effective version of the theorem that says that the countable union of null sets is a 
       null set.
          For technical reasons it is convenient to change a bit the definition of a regular 
       function.  Namely, we now say that a computable function x(-, •) is regular if all the 
       finite partial sums of the series
                    p(x(e, 0)) + p(x(e, 1)) + p(x(£, 2)) + • • •
       are less than e (note the strict inequality).  Here p(x) stands for p(iïx).  This makes 
       our requirements for regular functions  a bit stronger  (if all partial sums are less 
       than e, the sum of the series does not exceed e, but the reverse is not always true). 
       However, the notion of the effectively null set is not affected, since we always can 
       replace e by (say) e/2.
          In the sequel the regular functions are understood in this modified sense  (in 
       fact, regular functions are used only locally in the proof of the Martin-Löf theorem).
          The following lemma allows us to enumerate all regular functions.
          Lemma.  There exists a computable (partial) function
                          {q,e,i) i-» X(q,e,i)
                           3.  MARTIN-LÖF  RANDOMNESS
         62
         (where q  and i  are natural numbers, e is a positive rational number)  such that for 
         any fixed q  we  get  a  regular function Xq  (of two  remaining  arguments),  and  all 
         regular functions can be obtained in this way.
           P roof.  Let  us  enumerate  all  programs  for  the  functions  of two  arguments 
         (whether  these  functions  are  regular  or  not);  we  get  a  computable  sequence  of 
         programs, and the gth term of this sequence is called the “gth program” in the rest 
         of the proof.
           Then we define X(q, e, i) as the output of the gth program on input e, i, assum­
         ing that some conditions are met; otherwise X(q,e,i) is undefined.  The conditions 
        guarantee that all Xq are regular, and that regular functions are untouched.
           To compute X(q,e,i), we apply the gth program in parallel to all pairs
                                (£,0), (£, 1), . • • ,
         (starting with one step of the first computation, then making two steps of the first 
        two computations, etc.).
           When some computation terminates with some output, we interrupt this pro­
        cess to verify that strings obtained so far do not violate the regularity condition. 
        This means that we start to compute more and more precise approximations to p(z) 
        for all of these strings until we could guarantee that the sum of all of these p(z) is 
        less then e (this happens if the sum of approximations is less than e minus the sum 
        of approximation errors).  (Since ß is computable, we can compute approximations- 
        to p(z) for any 2 with any precision.)
           It  is possible that we do not return from this interruption; this happens if the 
        sum of measures is not less than e.
           Now X (q, £, i) is defined as the output of the gth program on (e, i) if this output 
        appears and passes the test during the process described.
           If the gth program computes a regular function, the verification will never fail 
        and Xq coincides with this function.  On the other hand, for every q the function 
        Xq is regular:  if for some £ the gth program  (applied to £ and all i = 0,1,2,...) 
        generates strings whose total measure is too large, only finitely many of the strings 
        will be let through, and their total measure is still less than e.  The lemma is proven.
           I 77 I Explain why we need to change the definition of correctness.
           (Answer:  If the  sum  consists  of a  finite  number  of terms  and  their  sum  is 
        exactly £, we may never know this.)
           Now we finish the proof of the Martin-Löf theorem.  Let X  be the function 
        provided by the lemma.  For all q = 0,1,2,... consider the effectively null set Zq 
        that corresponds to the regular function Xq.  Every effectively null set by definition 
        is a subset of Zq for some q.  It remains to show that the union Zq U Z\ U • • •  is an 
        effective null set.
           We do the same trick that is used to prove that a countable union of null sets 
        is a null set.  To find a covering of total measure less that e for (J  Zq, we combine 
        the (£/2)-covering for Zq with (£/4)-covering for Z\, etc.
           More formally, we consider a function x(e,i), that is defined as
                           x(e, [q,k]) = X(q,E/2q+l,k).
        Here  [q, k]  stands  for  the  number  of pair  q, к  under  some  computable  bijection 
        between N2 and N.                                       □
                            3.3.  EFFECTIVELY NULL  SETS        63
            Now we are ready to give the definition of the Martin-Löf random sequence. 
         Assume that some computable measure ß on the set Q is fixed.
            Definition.  A sequence ш is called  Martin-Löf random  (ML-random)  with 
         respect to ß if u> does not belong to the largest effectively null set (with respect to 
         ß) provided by Theorem 28.
           Reformulation:  A sequence is Martin-Löf random if it does not belong to any 
         effectively null set.
           One more version:  A sequence ш is Martin-Löf random if the singleton {cj} is 
         not an effectively null set.
           A  digression:  terminology.  The  notion  of Martin-Löf randomness  is  a 
         refinement  of the  intuitive  idea  of a  “typical  sequence”.  One  could  say  that  a 
         sequence is  “typical”  if it does not have any regularities or special features which 
         separate it from most sequences.  (If somebody says that  “Mr. X is a typical math 
         professor”, she probably means that Mr. X has no special characteristics that make 
         him different from the majority of math professors.)  A “special feature” is a feature 
         that is possessed only by a negligible fraction of the objects considered (sequences). 
         For example, if a sequence ш starts with 0, this is not a special feature, since half 
         of the sequences start with 0.  On the other hand, if each other term of ш is zero, 
         this is indeed a special feature.
           This informal idea is implemented in the Martin-Löf definition:  a special fea­
         ture is a feature that corresponds to an effectively null set,  and therefore typical 
         sequences are sequences that do not belong to any effectively null set, i.e., Martin- 
         Löf random sequences.
           It would be more logical to use the word  “typical”  for Martin-Löf’s definition 
         and  reserve  the word  “random”  for a more general  intuitive notion  that  can  be 
         formalized in different ways  (and the idea of a typical sequence is one of them). 
         However, the attempts to introduce a new, more logical, terminology often make 
         the situation worse.  (Authors have to confess that this can be said about their own 
         attempts!)  And there is already a lot of confusion—the term  “random sequence” 
         is already used in different ways.
           So we keep the term Martin-Löf random sequence  (ML-random sequence) for 
        the definition given above and keep the general term random sequence for a vague 
        philosophical notion of randomness that needs additional clarification to become a 
        mathematical notion.  (End of digression.)
           The following statement is a trivial corollary of the Martin-Löf theorem; how­
        ever, it deserves careful consideration since it looks counterintuitive.
           Theorem  29.  A  set A  C  Q  is  an  effectively  null  set  if and  only  if all  its 
         elements are not Martin-Löf random (are non-typical).
           In particular, the set of all non-ML-random sequences is the largest effectively 
        null set, and the set of all ML-random sequences has measure 1.
           PROOF.  Indeed, any element of any effectively null set is not ML-random by 
        definition;  on  the other hand,  if all elements of some set  A  are not  ML-random, 
        then A is a subset of the largest effectively null set, and therefore A is an effectively 
        null set.                                               □
                                        3.  MARTIN-LÖF RANDOMNESS
             64
                 What is strange here?  Intuitively, a set A is a null set if it has  “few elements” ; 
             the nature of these elements does not matter much.  Any singleton {w}  C Q is a 
             null set, and this does not depend on the properties of the sequence u>.
                 On the other hand, now we see that if we replace null sets by effectively null 
             sets, the situation changes drastically:  We may put as many non-ML-random (non­
             typical)  sequences  in  a set  as  we  wish,  and  it  would  remain  an  effectively  null 
            set.  But just one ML-random (typical) sequence added is enough to destroy this 
            property.
                 For  example,  recall  that  any  computable  sequence  forms  an  effectively  null 
            singleton  (with respect  to  uniform measure).  We  immediately get  the following 
            corollary:
                 Theorem  30.  The  set  of all  computable  sequences  of zeros  and  ones  is  an 
             effectively null subset of fl ( with respect to the uniform measure).
                 It is interesting to note that this observation was made before Martin-Löf gave 
            the definition of randomness, while developing the constructive version of calculus 
             (the Zaslavsky construction [221] is used for many counterexamples; it deals with 
            real numbers instead of bit sequences).
                 In the next section we explore the properties of ML-random sequences (with re­
            spect to the uniform measure).  We end this section with the following nice criterion 
            for ML-randomness which is attributed to R. Solovay in [34].
                 Theorem 31.  A sequence u>  is not ML-random with respect to  a computable 
            measure p if and only if there exists s computable sequence of intervals with a finite 
            sum of measures that covers uj  infinitely many times,  i.e.,  a computable sequence 
             of binary strings xq,x\,X2, ■ ■ ■  such that
             and ui E QXi  for infinitely many i.
                 Proof.  Assume that w is not ML-random.  Then for each e we can effectively 
            find a computable sequence of intervals that covers {w} and has a sum of measures 
            less than e.  Then we combine these sequences for e = 1,1/2,1/4,1/8,...  and get 
            a computable sequence of intervals with a sum of measures not exceeding 2 that 
            covers и infinitely many times (at least once for each e).
                On the other hand, assume that there is a computable sequence xo, x\, X2,... 
            of strings such that the sum of measures of corresponding intervals Qx.  does not 
            exceed some constant c and infinitely many of them contain w.  We may assume 
            without loss of generality that c is a rational number.  To find a covering for w that 
            has a sum of measures less than e, we consider the set Mjv  of all sequences in fl 
            that are covered at least N times.  Here AT is a positive integer such that c/N < e. 
            It is easy to see that Mjv can be represented as the union of a computable sequence 
            of disjoint intervals (while reading xq,x\, ..., we discover more and more elements 
            of Мдг  and add respective intervals as they appear).  Therefore the set  {w} is an 
            effectively null set and the sequence uj is not ML-random.                         □
                Remark.  This result is a constructive version of the Borel-Cantelli lemma (if 
            the sum of measures of sets Aq, A\,... is finite, then the set of all points that belong 
            to  infinitely many A{  is a null set),  and our argument is an effective version of a 
            classical proof of the Borel-Cantelli lemma.  However, we should be careful since
                              3.4.  PROPERTIES  OF  MARTIN-LÖF  RANDOMNESS                65
            not any classical proof can be effectivized.  The standard proof (since the series is 
            converging,  its tails could be made as small is needed)  does not work here,  since 
            there is no way to find an appropriate tail given e.
                Martin-Löf randomness  is  defined  for  computable  measures:  we  used  com­
            putability to prove that the largest effectively null set exists.  One could reformulate 
            the definition and call a sequence random (for an arbitrary measure ß on Г2)  if it 
            does not  belong to  any effectively null set.  For this  notion B.  Kjos-Hansen sug­
            gested the name Hippocratic randomness (and P. Gâcs suggested the more neutral 
            name blind randomness).  This is not the only existing notion of randomness with 
            respect  to non-computable measures;  we do not  go into detail here and mention 
            only that one can use uniform tests of randomness (following Levin and Gâcs, see 
            [13]).
                 78       Prove that for an upper semicomputable measure there exists the largest 
            effectively null set.  (We do not assume here that the measure of the entire Q equals 
            1;  otherwise, all upper semicomputable measures would be computable.)
                 79  Construct an example of a (non-computable) measure for which there is
            no largest effectively null set.
                {Hint:  Construct a measure that has two properties at the same time:  (1) every 
            computable sequence forms a singleton that is an effective null set (moreover, some 
            prefix already has measure zero); (2) every algorithm that pretends to generate an 
            effectively null set  either gives  an interval whose  measure is  too big or does not 
            cover some computable sequence.  This can be done by a diagonal argument where 
            we consider one by one all the computable sequences and all possible algorithms.)
                80  Show that there is a non-computable measure for which there exists the
            largest effectively null set.
                (.Hint:  Consider a non-computable measure that is very close to the uniform 
            one (say, at most twice as large and at most twice as small for all sets).)
                            3.4.  Properties of Martin-Löf randomness
                The Strong Law of Large Numbers also provides an example of an effective null 
            set  (with respect to the uniform measure).
                Theorem 32.  A set of all bit sequences that do no have limit frequency 1/2 is 
            an effectively null set with respect to the uniform measure.
                Proof.  It  is  enough  to  prove  that  for  every  rational  e  >  0  the  set  of all 
            sequences such that frequency of ones is greater than 1/2 + e infinitely many times 
            (or less than 1/2 — e infinitely many times) is an effective null set.
                Indeed,  the upper bound for the measure of this set achieved in the proof of 
            the Strong Law of Large Numbers in the previous section (Theorem 27, p. 56) is 
            effective:  the set of intervals was the set of all sufficiently long strings with large 
            frequency deviation, and its total measure was effectively bounded by a tail of the 
            converging geometric series.                                                 □
                The statement of this theorem can be reformulated as the property of individual 
            ML-random sequences:
                                                                                                                                        3.  MARTIN-LÖF RANDOMNESS
                                         66
                                                        Theorem 33.  Let oj = ojqoj\ • • •  be an ML-random sequence with respect to the 
                                          uniform measure.  Then
                                                                                                                                   t               aJo + uji + • ■ • -f- a>n-i                                                            1
                                                                                                                                   lilTL  --------------------------- = -.
                                                                                                                                 n-¥ OO                                                                                                 n 2
                                                       A similar statement is true for arbitrary Bernoulli measure.  Let p and q be 
                                         computable positive reals such that p + q  =  1.  Consider the Bernoulli measure 
                                         with parameters q and p  (the sequence of independent  coin tossing with success 
                                         probability p).  It is easy to check that this is a computable measure (since p and q 
                                         are computable).
                                                       Theorem  34.  Any ML-random sequence  with  respect  to  Bernoulli  measure 
                                         with computable parameters q,p has limit frequency p.
                                                       Proof.  Indeed, the upper bound for the probability of large deviations  (ob­
                                         tained by comparing the given Bernoulli measure with the other one with shifted 
                                        p,  see Problem 67, p.  58), gives an explicit bound and an explicit set of intervals, 
                                         so we get an effectively null set.                                                                                                                                                                                                                                                      □
                                                       There are several other properties of ML-randomness with respect to the uni­
                                         form measure:
                                                       Theorem  35.  Let oj  be  an ML-random sequence with respect to  the uniform 
                                         measure.  Then any other sequence which is obtained from oj  by a finite number of 
                                         insertions/deletions/changes is also ML-random.
                                                       Proof.  It is enough to show that adding a zero/one in the beginning of an 
                                         ML-random sequence or deleting the first term of an ML-random sequence gives an 
                                         ML-random sequence.  Indeed, assume that sequence oj is not ML-random, i.e., it 
                                         forms an effectively null singleton:  for each e one can effectively construct a covering 
                                         by intervals with total measure less than e.  Let us add zero at the beginning of 
                                         all these intervals (i.e., the corresponding strings).  We get a covering for 0oj whose 
                                         measure is twice as small.  This argument shows that if oj is not ML-random, then 
                                         0oj  is not ML-random either.  (A similar argument works for 1 oj.)
                                                       On the other hand, if we delete the first bit of all strings that form a covering 
                                        for oj, we get a family of intervals of measure twice as large that covers oj' (obtained 
                                        from oj by deleting the first bit).  Therefore, oj' is not ML-random either.                                                                                                                                                                                                             □
                                                          81                             Prove that by replacing all zeros by ones and vice versa in an ML-random 
                                        sequence (with respect to the uniform measure) we get an ML-random sequence.
                                                       The following problem shows that a computable subsequence of an ML-random 
                                        sequence is ML-random.
                                                         82 Let no, ni, П2,... be a computable sequence of different integers (щ /  nj 
                                        if i /  j).  Let oj = OJ0OJ1OJ2 • • •  be an ML-random sequence.  Then its subsequence
                                                                                                                                                   oj\n — OJnoOJniOJn2 • • •
                                        is ML-random.
                                                       (Hint:  Any interval Q.x in a cover for oj\n produces a finite family of intervals 
                                        whose union is the set of sequences whose (no, n\,..., ni_i)-subsequence coincides 
                                        with X  (here i is the length of the string x).  The total measure of these intervals 
                                        equals 2”*, the measure of Qx.)
                              3.4.  PROPERTIES  OF  MARTIN-LOF  RANDOMNESS                 67
                More general selection rules are considered in Chapter 9 (p. 261) where the fre­
            quency approach to the notion of randomness (von Mises’ approach) is considered.
                 83 Let oj be an ML-random sequence with respect to the uniform measure.
            Let us split oj into two-bit blocks and then replace blocks 00 by zeros and blocks 01, 
            10 and 11 by ones.  Prove that the resulting sequence is ML-random with respect 
            to Bernoulli measure with parameters 1/4,3/4.
                (Hint:  We described a transformation F : Q —> Q.  The preimage of any open set 
            U is open, and the uniform measure of that preimage equals the (1/4,3/4)-measure 
            of the set U.)
                 84 (Continued)  Prove that every ML-random sequence with respect to the 
            non-uniform Bernoulli  (1/4,3/4)-measure can be obtained in this way from a se­
            quence that is ML-random with respect to the uniform measure.
                (Hint:  For any open set В С S7, consider the set B' of all sequences ш such that 
            ir-1({w}) С В (the set of sequences that do not have a preimage outside £, i.e., 
            the complement to the image of the complement of В).  The image of a compact set 
            is a compact set; therefore, B' is open.  Show that if В is a union of an enumerable 
            family of intervals,  then B'  is also a union of enumerable family of intervals,  and 
            the Bernoulli measure of B' does not exceed the uniform measure of B.  See also 
            the proof of a more general statement (Theorem 123, p.  181).)
                What can be said about the complexity of an ML-random sequence (with re­
            spect to the uniform measure) from the viewpoint of recursion theory?  We know 
            already that an ML-random sequence is not computable.  It also cannot be a char­
            acteristic function of an enumerable (recursively enumerable, computably enumer­
            able) set.
                Theorem 36.  Let A  be  an enumerable set of natural numbers.  Consider its 
            characteristic sequence aQaya^ • • •  (a;  — 0 for i  ф  A  and a;  —  1 for i  € A).  This 
            sequence is not ML-random.
                Proof.  Let к be an arbitrary natural number.  Let us enumerate the set A 
            and see  what  happens  with  к  first  bits  of its  characteristic  sequences.  As  (the 
            current version of) A increases, we get more and more ones in this /с-bit prefix.  In 
            this way we get at most к + 1 candidates; at some point we come to a final (true) 
            one, but we never know that this happened already.  Anyway, the set of candidates 
            is  enumerable  and  the  number  of candidates  does not  exceed  к + 1  (since  k-bit 
            prefix can have 0 ■■■ к ones).  The total measure of these intervals is (k + l)/2fc and 
            therefore can be made arbitrarily small.  (Note that the definition of the effectively 
            null set allows us to enumerate the intervals that form a covering, and this is exactly 
            what we can do in our case.)                                                  □
                A natural question  arises:  In what  sense can one explicitly provide  an ML- 
            random sequence?  As we have seen, neither computable sequences nor character­
            istic sequences of enumerable sets are random.  If you are familiar with the basics 
            of the recursion theory (see, e.g.,  [184]), you may appreciate the following result: 
            There exists an ML-random sequence that belongs to the class Е2ПП2 of the arith­
            metic hierarchy (this class can be also described as the class of all O'-computable 
            sequences).
                Theorem 37.  There exists a O'-computable sequence that is ML-random with 
            respect to the uniform measure.
                                      3.  M ARTIN-LOF  RANDOMNESS
            68
                Proof.  It is enough to show that for any enumerable set of strings {xo, x\,...} 
            such that Y2       <1/2 there exists a O'-computable sequence that does not have
            any of Xi as a prefix.  (Indeed, the largest effective null set has such a covering with 
            total measure less than 1/2, and any sequence that is not covered is ML-random.)
                The intervals Q,Xi  are divided into two groups:  some of them belong to the left 
            half of il (i.e., Xj starts with 0) and some belong to the right half.  Total measure of 
            both groups at most 1/2.  Therefore, at least one of the groups has total measure 
            at most  1/4.  However, looking at the sequence X{, we cannot find out which half 
            has this property (since at any moment a new large interval can arrive).
                However, the O'-oracle allows us to make this choice, since the event  “measure 
            exceeds  1/4”  is enumerable.  Then we divide this half into two parts of size  1/4 
            each and choose one of them where the total measure of corresponding intervals 
            does not exceed 1/8, and so on.
                In this way we get a O'-computable sequence with the following property:  Each 
            part of its prefix is at most half-covered by our intervals.  In particular,  no prefix 
            of this sequence can appear in the sequence Xi, and this is what we need.     □
                A similar but more geometric argument can be given if we consider reals in [0,1] 
            instead of binary sequences.  A point x €  [0,1]  is  ML-random with respect to  the 
            Lebesgue measure on [0,1] if its binary representation is ML-random with respect to 
            the uniform measure on the Cantor space.  (A point can have two representations, 
            but  in  this  case  both  are  computable  and  non-random,  so  we  may  ignore  this 
            problem.)  One can also give an equivalent definition of randomness directly.  A set 
            X C  [0,1]  is called an  effectively  null subset  of [0,1]  if there exists an algorithm 
            that  for  every rational e  >  0  enumerates  a cover of X  that  consists  of intervals 
            with rational endpoints whose sum of measures is less than e.  Then an ML-random 
            real  is  a real not  contained in any effectively null subset  of [0,1].  All this is just 
            a simple reformulation of corresponding notions and results for Cantor space since 
            the only difference is that some numbers have two representations (but this happens 
            only for countably many computable reals), and we consider intervals with rational 
            endpoints instead of intervals with dyadic-rational endpoints (this does not matter 
            since we can split an interval with rational endpoints into a computable sequence 
            of dyadic intervals).  In particular, the following statement is true:
                85 Prove that there exists the largest effectively null subset of [0,1]  and its 
            elements are reals whose binary representations are not ML-random with respect 
            to the uniform measure in Cantor space.
                Now we can point out an explicit ML-random point.  Consider an enumerable 
            family of open intervals that have total length less than 1  and cover all non-ML- 
            random points.  The union of this intervals is an open set which is a proper subset 
            of [0,1].  Its complement is a closed set,  and this closed set has a minimal point. 
            By construction this point is ML-random.
                86       Prove that the minimal point that is not covered by an enumerable family 
            of open intervals with rational endpoints is lower semicomputable, i.e., it is a limit of 
            an increasing computable sequence of rational numbers (and therefore is computable 
            with the oracle for the halting problem O').
                (See Section 5.7 (p.  157) for more detail about random reals and for an alter­
            native construction of a lower semicomputable ML-random real.)
                                                   3.4.   PROPERTIES  OF  MARTIN-LOF  RANDOMNESS                                                          69
                           The proof of Theorem 37 given above is a relativized version of the following 
                    result:
                            87  Assume that xq, x\ , X2,... is a computable sequence of binary strings, and
                    assume the sum
                                                                                       2-i(xi)
                    is  less  than  1  and is a computable real number.  Then there exists a computable 
                    sequence of zeros and ones that has neither of Xi as its prefix.
                           {Hint:  Let this sum be less than some rational S < 1.  By induction construct 
                    a computable sequence ujoujiuj2 • • •  with the following property:  the fraction of the 
                    set U — (J £lXi  among the sequences that have prefix ujq ■ ■ ■ u>k is less than S.)
                           This problem is related to the definition of randomness suggested by C. Schnorr 
                    in [166].  He gave a more restrictive definition of an effectively null set than Martin- 
                    Löf.  The additional requirement is that for every (rational) e > 0, the total measure 
                    of corresponding intervals  is  not  only  less  than  e  but  is  also  a computable  real 
                    (and the approximation algorithm computably depends on e).  This requirement 
                    is equivalent to the following one:  for every e > 0 and Ô > 0, one can effectively 
                    find out  how many terms in the series  JAp(x(e, г))  are needed to make the tail 
                    less than 5.  (For a series with non-negative terms the computability of the sum is 
                    equivalent to computable convergence.)
                           By  Schnorr  effectively  null  sets  we  mean  the  effectively  null  sets  according 
                    to  this  modified  definition.                   (Schnorr  calls  them  total  rekursive  Nullmenge,  see 
                    Definition 8.1  in  [166];  effectively null  sets  (as  in  the  Martin-Löf definition)  are 
                    called rekursive Nullmenge, see Definition 4.1 in [166].)
                            88             Let us change the definition of an effectively null set in another way:  now 
                    we require that the total measure of all intervals in the covering is exactly e.  Show 
                    that this definition is equivalent to the definition of a Schnorr effectively null set. 
                    (One can also consider the measure of the union of all intervals instead of the sum 
                    of measures.)
                           Problem 87 shows that for every Schnorr effectively null set there exists a com­
                    putable sequence outside this set.  (For simplicity let us consider the case of uniform 
                    measure.)  On the other hand, every computable sequence (i.e., the singleton made 
                    of it)  is  a Schnorr  effectively  null  set.  Therefore,  none of the Schnorr effectively 
                    null  sets  is  the  largest  one  in the class  (in other words,  the union of all  Schnorr 
                    effectively null sets is not a Schnorr effectively null set).  Nevertheless we can call a 
                    sequence which does not belong to any Schnorr null set a Schnorr random sequence 
                    (or Schnorr typical sequence).
                           Since now we have fewer effectively null sets, we may get the broader class of 
                    random sequences,  and this is indeed the case.  The following problem  (together 
                    with the results of Chapter 5) guarantees that there exist Schnorr random sequences 
                    that are not ML-random.
                            89 Prove that there exists a Schnorr random sequence w =                                                         • • •  whose
                    prefixes have logarithmic complexity, i.e., C{u>q • • -wn_i) = O(logn).
                           (Hint:  Problem 87 shows how one can construct a computable sequence that 
                    does  not  belong  to  a  given  Schnorr  effectively  null  set.  At  some  point  of this 
                    construction we can take into account another Schnorr effectively null set and get 
                    a computable sequence that does not belong to both.  (Indeed, we need to take a
             70                          3.  M ARTIN-LOF  RANDOMNESS
             sufficiently small cover for the second set that does not go out of the safety margin 
             in  the  construction for the first  set.)  Moreover,  we can consider infinitely many 
             Schnorr  effectively  null  sets  in  this  way  (adding  them  one  after  another).  This 
             will not give us a computable Schnorr random sequence (it does not exist at all), 
             because we need additional information that tells us which algorithms correspond to 
             Schnorr effectively null sets and which do not.  But if we postpone the introduction 
             of a new algorithm until the moment when the constructed prefix of our sequence 
             is  rather  long,  this  additional  information  is  logarithmic  compared to  the prefix 
             length.)
                 We return to  Schnorr’s  definition  of randomness  in  Section  9.8  where  it  is 
             reformulated in terms of computable martingales.
                  90 Prove that a sequence ui is not Schnorr random if and only if there exists a
             computable sequence of strings xq, aq,... such that the series     Pixi) computably 
             converges (has a computable sum) and infinitely many of Xi are prefixes of cj.
                 {Hint:  This is  a version of Theorem 31  for Schnorr randomness and can be 
             proven in a similar way.  In fact, in this case even the standard proof of the Borel- 
             Cantelli lemma works.)
                 Another version of effectively null  sets  is  obtained  if we  consider  only  finite 
             families  of intervals  (each  family  is  presented  as  the  list  of all  intervals  in  the 
             family):  Given a rational e,  the algorithm should output a finite list of intervals 
             that cover the set and have a sum of measures less than e.  This corresponds to the 
             Jordan construction of measure often used in elementary calculus textbooks.  In 
             this way we get a smaller class of null sets (e.g., a null set cannot cover all rational 
             points).
                  91  Prove that a set is an effectively null set in this sense if and only if it is 
             contained in the complement of some effectively open set of measure 1.
                 The sequences not covered by any effectively null sets in this sense (=contained 
             in  every  effectively  open  set  of full  measure)  are  called  Kurtz  random.  In  this 
            definition we again restrict  the class of effectively null sets and therefore enlarge 
            the  class  of random  sequences.  Indeed  we  get  more  random  sequences,  as  the 
            following problem shows.
                  92 Show that every Schnorr random sequence (with respect to uniform mea-
            sure) satisfies the Strong Law of Large Numbers, but there exists a Kurtz random 
            sequence that does not.
                 {Hint\  The proof of the Strong Law of Large Numbers gives a cover with a 
            computable sum of measures since the series converges exponentially fast.  For the 
            second part, one can consider generic sequences; see Section 5.9, p.  178.)
                                      3.5.  Randomness deficiencies
                 Martin-Löf’s definition requires that  for an effectively null set  A  there  is  an 
            algorithm that, given e > 0, produces a cover of A by intervals whose total measure 
            does not exceed e.  The union of these intervals in an open set of measure at most e.
                 In general, the unions of computable sequences of intervals are called effectively 
             open sets.  As in the definition of effectively null sets  (p.  59),  we allow the com­
            putable sequence to be non-total, so the empty set is also effectively open.  In other 
            words, an effectively open set is a union of an enumerable family of intervals.
                 Now the definition of an effectively null set can be reformulated as follows:
                                                               3.5.   RANDOMNESS  DEFICIENCIES                                                             71
                           Theorem 38.  A set A is an effectively null set with respect to a measure p if 
                    and only if A G (~)n Un for some uniformly effectively open sets Un  with p{Un)  ^ 
                    2~n.  We may assume also that U\ D U2 D • • ■ ■
                           Speaking about uniformly effectively open sets, we mean that there exists an 
                    algorithm that, given n, enumerates a family of intervals whose union is Un.
                           Proof.  There are several differences between this definition and the one we 
                    used earlier.  The first difference (a trivial one) is that we use only e = 2~n.
                           Second, we speak here about the measure of an effectively open set, not about 
                    the sum of measures of intervals whose union it is.  This does not matter either, since 
                    we may assume without loss of generality that the intervals forming an effectively 
                    open set are disjoint.  (When a new interval appears, we subtract all the intervals 
                    that  appeared  earlier  and  split  the  rest  into  a  union  of disjoint  intervals.)  For 
                    disjoint intervals the sum of measures is equal to the measure of their union.
                           Finally,  we  require  that  Ui+1  С                               To  achieve  this,  we  can  consider  the 
                    sequence U\,U\ П [/2, U\ П U2 П [/3,...  instead of the original one.  One needs to 
                    check only that the intersection of a finite number of effectively open sets U\C\- • -C\U\ 
                    is an effectively open set (and the corresponding algorithm can be effectively found 
                    given  the  algorithms  for  Ui).  Indeed,  assume that  we  have  two  algorithms  that 
                    enumerate intervals for U\  and U2.  At every stage the current approximations for 
                    U\  and U2 are finite unions of intervals, and their intersection is also a finite union 
                    of intervals.  Let us add all the intervals of this intersection to the enumerable set 
                    of intervals for U\ П U2.  (We can also make the intervals for U\ П U2 disjoint, see 
                    above.)                                                                                                                                □
                           In fact,  Martin-Löf gave his definition of randomness in this form in  [115];  a 
                    family Un with these properties was called a randomness test.  Given such a test, 
                    we can define the randomness  deficiency of a sequence oj  as the maximal i such 
                    that ш G Ui.  The randomness deficiency of a sequence uj is infinite when uj belongs 
                    to all Ui.  In this version, the test not only says that all elements of                                               Ui are non- 
                    random,  but also says for other sequences how close they are to non-randomness 
                    (the deficiency increases as the sequence get closer).
                           We know that there exists a universal test such that the set fji Ui is maximal. 
                    Martin-Löf noted that one can construct a test that is universal even in a stronger 
                    sense:
                           Theorem 39.  There exists a randomness test that corresponds to the maximal 
                    deficiency function  {up to an additive constant).
                           Proof.  In fact  we just  need to  look more closely at  the construction given 
                    above:  randomness tests
                                                                    U1: Ui D Щ D Щ D • • • 
                                                                    u2 :  U\  DU% D Ui D ■■■ 
                                                                    u3: U3 D Un  D Un  D ■ ■ ■
                    can be combined into a test
                                 {Ul U Щ U • ■ • U tff+1 U • ■ • ) D (Щ U U\ U ■ • • U Ui+2 U • • • ) D • • •  .
                                      3.  M ARTIN-LÖF  RANDOMNESS
            72
            It is indeed a test:  the measures of the sets are bounded by 1/4+1/8 + -- - ^ 1/2 
            (the first one), 1/8 + 1/16 + -- - ^ 1/4 (the second one), etc.  The deficiency function 
            for this combined test is at least dl — i for every i, where d1 is the deficiency function 
            for ith test.                                                                □
                The deficiency  function  can  be  considered  as  a compact  representation of a 
            decreasing family Un.  We consider a function whose values are natural numbers 
            and Too, and for every finite n the set Un of all u> where the function exceeds n is 
            effectively open (uniformly in n).
                One  can  slightly  extend  this  class  of functions  allowing  non-integer  values. 
            We say that a function и defined on О and having non-negative real values (plus 
            a  special  value  Too)  is  lower  semicomputable  if  for  every  rational  r  the  set 
            {uj I u(cj)  > r} is effectively open uniformly in r.
                The following statement provides  an equivalent definition of lower semicom­
            putable functions on Q.  Let us first consider  basic functions on the Cantor space 
            О that have rational values and depend only on some finite prefix of the argument. 
            To specify a basic function, we say how many input bits it needs to read (the length 
            of the prefix)  and provide a table that specifies the output value for every combi­
            nation of input bits.  If a basic function reads m bits, this table will consist of 2TO 
            rational numbers.  Such a table  (and a basic function that corresponds to it) is a 
            finite object, so we may speak about computable sequence of basic functions.
                Theorem 40.  The following properties of a function v with non-negative real 
            values  (Too  is also allowed)  are equivalent:
                (a)  v  is lower semicomputable;
                (b)  v  is  a pointwise supremum of a computable sequence of basic functions;
                (c)  v  is  a  pointwise  limit  of a  non-decreasing  computable  sequence  of basic 
           functions',
                (d)  v  is  a  sum  of a  series formed  by  a  computable  sequence  of non-negative 
            basic functions.
                Proof.  The two latter properties are equivalent since the sum and the differ­
            ence of two basic functions are basic functions.
                To convert the supremum into a limit, note that the maximum of a finite set 
            of basic functions is a basic function.
                It remains to show that the first property is equivalent to others (for example, 
            the second one).  Let v = sup{ Vi.  Note that supf Vi(uj) > r if and only if Vi(u>) > r 
            for some i.  This guarantees that the set {to  \ v(co)  > r}  is  (uniformly)  effectively 
            open.
                For  the  other  direction,  if  for  each  rational  r  we  can  effectively  enumerate 
            intervals  where  u{u>)  >  r,  then  и  is  the  pointwise  supremum  of an  enumerable 
            set of basic functions that are equal to r inside the corresponding intervals and 0 
            otherwise.                                                                   □
               Now we can give the following definition of a randomness test (without the in­
            tegrality requirement and switching to the exponential scale):  a probability-bounded 
            randomness test with respect to a computable measure p on fl is a lower semicom­
            putable non-negative function и such that
                                        p({tv I u(co) > c}) < 1/c
                                                               3.5.   RANDOMNESS  DEFICIENCIES                                                           73
                    for every positive rational c.  Informally, such a test finds regularities in w in such 
                    a way that (a) sequences with a lot of regularities (where the test value is greater 
                    than some c) form a small set  (of measure at most 1/c) and (b) if a sequence has 
                    some regularity (test value is big), this regularity will be eventually found (the lower 
                    semicomputablity requirement).
                           Theorem 41.  For every computable measure p on the Cantor space there ex­
                    ists  a maximal  (up to a constant factor) probability-bounded randomness test with 
                    respect to p.  Its binary logarithm coincides with the universal test from Theorem 39 
                    up to an additive constant.
                           Proof.  An arbitrary probability bounded test can be replaced by test with 
                    values  of the  form  2n.  We replace u(cu)  by  the maximal  power of 2  that  is less 
                    than u(lo).  The new test is still lower semicomputable and differs from the original 
                    one at most by a constant factor.  The probability bounded test of this restricted 
                    form corresponds to a decreasing sequence of uniformly effectively open sets,  and 
                    the  the probability  bound  means that  this sequence forms a randomness  test  in 
                    the Martin-Löf sense.  The reverse translation is also possible.  It remains to use 
                    Theorem 39.                                                                                                                         □
                           There is another slightly different notion of a randomness test.  An expectation- 
                    bounded randomness test uses a stronger restriction on the (lower semicomputable 
                    non-negative) function u:                            j  u(lu) dp(cu) ^ 1.
                          Theorem 42.  For every computable measure on the  Cantor space there is  a 
                    maximal (up to a constant factor) expectation-bounded randomness test with respect 
                    to this measure.
                          Proof.  We can enumerate all lower semicomputable functions on the Cantor 
                    space.  (For example, we can use their representations as supremums of enumerable 
                    sets  of basic  functions.)  Furthermore,  one  can  “trim”  a  lower  semicomputable 
                    function and guarantee that its integral does not exceed 2, and the functions whose 
                    integral  is  at  most  1  remain  unchanged  after  the  trimming.  This  can  be  done 
                    effectively (recall that the measure is computable).  In this way we get a sequence 
                    of uniformly lower semicomputable functions ui,U2,... that includes all tests and 
                    consists  only  of almost-tests  (up  to  factor  2).  Adding  all  the  functions  of this 
                    sequence (with computable coefficients that form a converging series with sum at 
                    most 1/2), we get a maximal expectation-bounded randomness test.                                                                    □
                          Fix some maximal probability-bounded (or expectation-bounded) randomness 
                    test with respect to a computable measure p.  The logarithm of it is called proba­
                    bility-bounded  (resp.  expectation-bounded)  randomness  deficiency with respect to 
                    p.  We will denote these deficiencies as dp and dp.  (We assume that the measure 
                    p is fixed and omit it in the notation.)
                          Theorem 43.
                                                       dp(w) — 21ogdp(w) ^ dp(w) < dp(w).
                          Both inequalities are true with 0(l)-precision (and the quantities that appear 
                    in them are well defined with the same precision).
            74                        3.  M ARTIN-LÖF  RANDOMNESS
                P r o o f.  The second inequality is obvious since every expectation-bounded test 
            is also probability-bounded.  To prove the first inequality, we need to show that the 
            function
                                       u{ ш) = 2dP(w)-21°sdP(w) 
            has finite integral.  (We need и to  be lower semicomputable;  this is because the 
            function X — 2 log X is an increasing function—strictly speaking, this is not true for 
            small values of x,  but  these values  are not  important  for finite integral,  and the 
            function can be corrected there.)
                To check that f  u(w) dfi(u) is finite, note that the set of <x such that dp(cj) is 
            in between n and n + 1, has measure at most  l/2n  (since 2dC  exceeds 2n on this 
            set),  and the function и on this set is bounded by 0(2n/n2).  It remains to note 
            that the series ^  1/n2 converges.                                           □
                93 Show that the constant 2 in Theorem 43 can be replaced by an arbitrary
            number greater than 1.
                We have shown,  in particular,  that  each type of test  can be used to give an 
            equivalent definition of Martin-Löf randomness (we have already discussed this for 
            a probability-bounded test):  a sequence is ML-random if and only it its randomness 
            deficiency is finite.
                The difference between probability- and expectation-bounded tests resembles 
            the difference between plain complexity C (studied so far)  and prefix complexity 
            К (see the next chapter).
                As we have said, randomness tests were introduced and studied in the 1960s and 
            1970s  (probability-bounded tests were introduced in a form of sequences of open 
            sets by Martin-Löf; expectation-bounded were considered by Levin and Gacs) but 
            then almost forgotten until recently.  For more information about these tests and 
            their applications, see  [13].  Recently G. Novikov  [149]  has studied the difference 
            between different versions of randomness deficiencies.
                               CHAPTER 4
             A priori probability and prefix complexity
                4.1.  Randomized algorithms and semimeasures on N
           In this section we consider algorithms  (=programs,  machines)  equipped with 
        a random number generator.  That is, algorithms may perform instructions of the 
        following form:
                               b := random.
        This  instruction  assigns  to  the  variable  (memory  cell)  b  a random  bit  (0  or  1), 
        both values are assigned with equal probabilities (and independently of all previous 
        random bits).  To perform this instruction we toss a fair coin and write its outcome 
        (heads/tails as 0/1) in the memory cell b.  Algorithms including such instructions 
        are called randomized or probabilistic.
           The result (output) produced by a randomized algorithm depends not only on 
        its input but also on the result of the coin tossing.  That is, for every fixed input, 
        the output of a randomized algorithm is a random variable.
           Speaking formally, the probability that a randomized algorithm A outputs x is 
        defined as follows.  Consider the uniform Bernoulli distribution on the space ft of 
        all infinite 0-1  sequences.  The measure of the set ftu of all infinite extensions of a 
        finite string и is equal to 2~l^uK
           Let x be an input for a randomized algorithm A, and let из e ft be an infinite 
        sequence of zeros and ones.  We denote by A(x,uj)  the output of A on input x if 
        random bits used by the algorithm are taken from the sequence uj.  More specifically, 
        each call of a random generator returns the next bit of uj.  If the algorithm A does 
        not halt  (for given x and uj), then the value A(x,uj) is undefined.
           Let y be a possible output of A.  Consider the set {uj \ A(x,uj) — y}.  This set is 
        the union of intervals ftz over all outcomes 2 of coin tossing that guarantee that A 
        outputs y having x as input.  The probability that A on input x outputs y is equal 
        to the measure of this set.
           In this section, we consider machines without input whose outputs are natural 
        numbers.  Here is an example of such machine.  It tosses a coin until 1 appears and 
        outputs the number of zeros preceding the first  1.  The probability pi of the event 
        “the output is i"  is equal to 2~(г+1\   Indeed, the algorithm outputs i if and only 
        if the first i random bits are zeros and the (i + l)-th bit is  1.  This happens with 
        probability 2~^l+1\
           The sum   is  equal to  1  in  this  example.  Indeed,  the algorithm does not 
        halt if and only if all random bits are zeros and this happens with zero probability. 
        But it is also possible that some other algorithm of the type considered does not 
        terminate with some positive probability.
           We assign to every probabilistic machine (that has no input and produces nat­
        ural numbers; after some number is produced, the machine terminates) a sequence
                                   75
                           4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
            76
            Po.pi,... of reals: pi is the probability that the machine outputs i.  We say that the 
            probabilistic machine generates the sequence po,p\,....  Which sequences Po,P\, ■.. 
            can be obtained in this way?  There is an obvious necessary condition:  ^2pi  <  1 
            (since the machine cannot produce two different outputs).  However, this inequality 
            is not sufficient, as there are countably many randomized algorithms and uncount- 
            ably many sequences satisfying this condition.
                Let  us answer first  a simpler question.  Consider the halting probability of a 
            randomized machine without  input,  i.e.,  the  probability that  the  machine halts. 
            Which real numbers can appear as halting probabilities of probabilistic machines 
            without input?  To answer this question,  we need to recall the notion of a lower 
            semicomputable real number.
               A real  number  a  is  lower semicomputable  if it  is  the  limit  of a computable 
            non-decreasing sequence of rational numbers.
                94  Prove that if a is a computable real number  (i.e.,  there is an algorithm 
            that  for  any given rational e  >  0  computes  a rational  approximation  to  a  with 
            precision e), then a is lower semicomputable.
                (Hint:  We can construct  an increasing sequence using approximations  from 
            below.)
                95  Show that a real number a is computable if and only if both numbers a
            and —a are lower semicomputable.
               A  real  number  a  is  lower  semicomputable  if and  only  if the  set  of rational 
            numbers that are less than a is enumerable.  (It explains why lower semicomputable 
            reals are sometimes called enumerable from below.)
               Indeed,  let  us  assume that a is the limit of a non-decreasing computable se­
            quence ao < ai ^ U2 ^ • • •  of rationale.  For each i enumerate all rational numbers 
           that are less than аг.  All rational numbers less than a (and no other) will appear 
           in the enumeration.
               Conversely,  assume that we can enumerate all rational numbers that are less 
           than a.  Omitting all numbers in this enumeration that  are less than previously 
           met ones, we obtain a non-decreasing sequence whose limit is a.
               Using the notion of a lower semicomputable real, we obtain the following answer 
           to the above question:
               Theorem 44.  (a) Let M be a probabilistic machine without input.  The halting 
           probability of M is a lower semicomputable real number.
               (b)     Every lower semicomputable real is the halting probability of some proba­
            bilistic machine.
               Proof,  (a) Let pn stand for the probability that M halts within n steps.  The 
           number pn is rational:  the algorithm can toss a coin at most n times within n steps, 
           thus the halting probability is a multiple of l/2n.
               We can find pn by simulating the run of the machine for all possible outcomes 
           of the coin tossing.  The sequence Po,Pi, ■ ■ ■ is non-decreasing and its limit is equal 
           to the halting probability of M.
               (b)  Assume that  a real q is lower semicomputable.  That  is,  there is  a com­
           putable sequence go  ^  Qi  ^  52  ^  • • •  of rational  numbers  such  that  q  =  limgn. 
           We have to construct a probabilistic machine whose halting probability is equal to 
           q.  Let the machine toss a coin, and let bo,b\,b2,... be the obtained random bits. 
           Consider the real number ß — O.&0&1&2 ■ ■ • ; it is uniformly distributed in [0,1].  Let
                         4.1.  RANDOMIZED  ALGORITHMS  AND  SEMIMEASURES  ON  N            77
                          Figure 9.  Comparing ß = 0.bob\b2 • • •  and q — limg;
            the machine (in parallel to coin tossing) compute the rational numbers qo,qi,q2, ■ ■ ■■ 
            The machine halts when it finds out that ß < q.  That is, the machine halts if for 
            some i the rational number ßi  =  O.bo&i • • • ЬДП • • •  (the  currently  known upper 
            bound for ß) is less than g*  (the currently known lower bound for q).  See Figure 9 
            for a symbolic representation of this argument.
                The constructed machine halts if and only if ß < q.  Indeed, assume that ß is 
            less than q.  The numbers g* tend to q and the upper bounds ßi for ß tend to /3, as 
            i —> oo.  Therefore for some i the number qi is greater than ßi.  On the other hand, 
            if the machine halts, then ß < q by construction.
                Thus the halting probability of the machine is equal to the probability of the 
            event ß < q.  The latter probability equals the length of the segment  [0,g), that is, 
            to q.  (Recall that ß is uniformly distributed in the segment  [0,1].)        □
                Let us return to probability distributions that can by generated by probabilistic 
            machines.  First,  a definition.  A sequence Po,Pi,P2, • • •  is  lower semicomputable if 
            there  is  a function p(i,n),  where  i,n  are integers  and p(i,n)  is  either  a rational 
            number or —oo, with the following properties:  the function p(i, n) is non-decreasing 
            in the second argument:
                                     p(i,0) <p(i, 1) <p(i, 2) < • ••
            and
                                            Pi =  lim  p(i, n)
                                                 n—»oo
            for all i.
                One could say that the sequence pi  is  lower semicomputable if the numbers 
            Pq,Pi,P2, ■ ■ ■ are “uniformly lower semicomputable”.  The next theorem provides an 
            alternative way to define lower semicomputable sequences.
                Theorem 45.  A sequence po,pi,P2, ■ ■ ■  is lower semicomputable if and only if 
            the set of pairs  (r,i),  where i  is  a natural number and r  is  a rational number less 
            than pi,  is  enumerable.
                Proof.  Recall that a set is enumerable if there is an algorithm that generates 
            all  its  elements in some order with arbitrary delays between consecutive elements 
            (the algorithm may not halt even if the set is finite).
                Assume that a sequence Po,Pi,P2, ■ ■ ■ is lower semicomputable.  Let p(i,n) be 
            the function from the definition of the lower semicomputability.  Arrange all the 
            pairs (r, i) in a sequence so that every pair appears in the sequence infinitely many
                          4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
            78
            times.  The algorithm enumerating all the pairs  (r, i)  with r < pi works in steps. 
            On step n compare r and p(i, n) where (r, i) is the nth pair in the chosen sequence. 
           If r  < p(i,n),  then output the pair  (r,i),  otherwise proceed to the next step.  By 
           definition, r < limnp(i,n) if and only if there exists n such that r < p(i,n).  Thus 
           we will output all the pairs in the set, and no other pairs.
               Conversely,  assume that the property r  < Pi  is enumerable,  and let A be an 
           algorithm  enumerating  all  such  pairs  (r,i).  To  compute p(i,n),  we  simulate  n 
           steps of the computation performed by A.  Consider all the pairs that  appeared 
           within n steps and have i as the second component.  Let p(i,n)  be equal to the 
           largest  first  component  of such  pairs.  If there  are  no  such  pairs,  let  p(i,n)  = 
            —oo.  As n increases,  new pairs may appear and p(i. n)  may increase.  The limit 
           limnp(2,n) is equal to pi, since all the rational numbers less than pi will appear in 
           the enumeration.                                                            □
               We are now able to characterize probability distributions generated by proba­
           bilistic machines.
               Theorem 46.  (a) Let M be a probabilistic machine without input that outputs 
           natural numbers.  Let pi  denote  the probability  that  the  machine  outputs  i.  The 
           sequence of pi  is lower semicomputable and YliPi ^ 1-
               (b) Letpo,p\,...  be a lower semicomputable sequence of non-negative real num­
           bers such that J2iPi ^ 1-  There is a probabilistic machine M that outputs every i 
           with probability exactly Pi.
               PROOF.  The proof of item (a) is similar to the proof of corresponding statement 
           in the previous theorem.  We let p(i,n) be the probability that M outputs i within 
           n steps.
               The proof of item (b) is also similar to the proof of corresponding assertion in 
           the previous theorem.  This time we assign to each natural i a subset of [0,1] and the 
           machine outputs i if the real number ß = 0.bç,b\b2 ■..  belongs to the set assigned 
           to i.  The sets assigned to different values of i do not overlap.  They may not cover 
           the entire segment  [0,1].  The set assigned to every i is a finite or countable union 
           of half-open intervals [a, b) of total length pi.  When an approximation for some pi 
           increases,  we add a new interval for this i  (its length is the increase) just on the 
           right  of intervals  allocated  earlier.  (So  at  each  moment  the used part  of [0,1]  is 
           [0, s) for some s.)
               In parallel,  we toss a coin and obtain digits of the random number ß.  When 
           we are sure that ß gets into the set assigned to some natural number, we output 
           that number.
               Here is a formal argument.  Let p(i, n) be the function of two variables from the 
           definition of lower semicomputability.  Without loss of generality we may assume 
           that p(i, n) ^ 0 for all г, n.  Indeed, we can replace all negative values by zeros.  We 
           may assume also that for all n only finitely many values p(i,n)  are positive  (let 
           p(i,n)  = 0 for all i  ^  n).  The probabilistic  algorithm that we construct  runs in 
           steps.  On each step we allocate some space inside [0,1].  Our goal is that after the 
           nth step the total length of intervals allocated to i is equal to p(i, n) (for all i).  This 
           requirement is easy to keep:  going from left to right, on step n we allocate for each 
           i  (such that p(i, n) > p(i, n — 1)) a new interval of length p(i, n) — p(i, n — 1).  We 
           need to do this only for finitely many i, as for i ^ n we have p(i, n) = p(i, n — 1) = 0.
                                                                 4.2.   MAXIMAL SEMIMEASURES                                                             79
                           The total length of used intervals does not exceed 1, as p(i, n) ^ Pi and YjPi ^ 1- 
                    Thus we will always be able to allocate the space we needed (at the left of the free 
                    space).
                           In parallel, the probabilistic machine tosses a coin, obtaining a random bit bn on 
                    step n.  It halts on step n and outputs i if it is known for sure that ß = O.&0&1&2 • • • 
                    belongs  to  (the  interior  of)  the  space  allocated  to  i,  i.e.,  if  the  closed  interval 
                    consisting  of  all  real  numbers  whose  binary  expansion  starts  with  bob\ • • • bn  is 
                    included in the interior of the space allocated to i.  (The interior of the segment 
                    [■и, v) is the interval (u, v).)  By construction, for all i the measure of this set (interior 
                    of the space allocated to г) equals pi.                                                                                              □
                           Any sequence pi satisfying the conditions of the previous theorem is called a 
                    lower semicomputable semimeasure (or enumerable from below semimeasure) on N. 
                    Sometimes we will use also the notation p(i) for p{.  We thus have two alternative 
                    definitions of a lower semicomputable semimeasure:  (1) a probability distribution 
                    generated by a randomized algorithm; (2) a lower semicomputable sequence of non­
                    negative reals whose sum does not exceed 1.  The above theorem states that these 
                    definitions are equivalent.
                          The word “semimeasure” may look strange, but unfortunately there is no other 
                    appropriate term in the literature.  Dropping the semicomputability requirement, 
                    one  can  call  any  function  i                     pi  with  YhiVi  ^  1  a  semimeasure  on  N.  Every 
                    semimeasure on N defines a probability distribution on the set N U {_L} where X is 
                    a special symbol meaning  “undefined”.  The probability of the number г is pi and 
                    the probability of X is 1 — YhiVi-  In the sequel we consider lower semicomputable 
                    semimeasures only (unless explicitly stated otherwise).
                          We have considered so far (lower semicomputable) semimeasures on the natural 
                    numbers.  The definition of a lower semicomputable semimeasure can be naturally 
                    generalized to the case of binary strings or any other constructive objects in place 
                    of natural numbers.  For example,  to define a notion of a lower semicomputable 
                    semimeasure on the set of binary strings, we have to consider probabilistic machines 
                    whose output is a binary string.
                          Important remark:  We will consider in Chapter 5 a notion of a semimeasure 
                    on the space consisting of all finite and infinite 0-1 sequences.  Such a semimeasure 
                    is generated by a probabilistic machine that prints its output bit by bit and never 
                    indicates that the output string is finished.  In particular the machine never halts. 
                    It leads to a different notion:  all the machines considered in this section are required 
                    to halt after printing the output; for such machines, there is no essential difference 
                    between printing a binary string and a natural number.
                          To stress the difference between these two frameworks, semimeasures defined in 
                    this section are called discrete semimeasures while the ones considered in Section 5 
                    are called continuous semimeasures, or semimeasures on the binary tree.
                                                             4.2.  Maximal semimeasures
                          Comparing two semimeasures on N,  we will ignore multiplicative constants. 
                    A lower semicomputable semimeasure m is called  maximal if for any other lower 
                    semicomputable semimeasure m' the inequality m'(i) < cm(i) holds for some c and 
                    for all i.  (The name greatest (instead of “maximal”) would be more accurate since 
                    we look for the greatest  element  of some partially ordered set,  not  the maximal 
                    one.)
                  80                      4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
                        Theorem 47.  There exists a maximal lower semicomputable semimeasure on N.|
                        Proof.  We have to construct a probabilistic machine M with the following 
                  property.  The machine M should output every number i with a probability that is 
                  at most a constant times less than the similar probability for each other machine 
                  M' (the constant may depend on M' but not on i).
                        This is easy to achieve:  consider a machine M that picks at random a proba­
                  bilistic machine M' and then simulates M'.  The probability of picking each machine 
                  M' should be positive.  If a machine M' is chosen with probability p, then M will 
                 output some i with probability at least p • (the probability that M' outputs i).  Thus 
                 one can let c = l/p.
                        It  remains to explain how to implement the random choice of a probabilistic 
                 machine.  Enumerate all probabilistic machines in a natural way; let Mo, M\, М2,... 
                 be the resulting sequence.  We toss a coin until the first 1 appears.  Then we simulate 
                 the machine Mj where i is the number of zeros preceding the first 1.                                                         □
                        It  is  instructive to prove this theorem once more using the language of lower 
                 semicomputable sequences instead of probabilistic algorithms.  Basically, we need to 
                 show that there exists a convergent lower semicomputable series that upper-bounds 
                 all other lower semicomputable convergent series (up to a multiplicative constant). 
                 More formally, we should consider only series with the sum at most 1, but this is 
                 not essential since we ignore constant factors.
                        To find such a series, we sum up with certain weights all the lower semicom­
                 putable series with sum at  most  1.  The weights  form  a computable converging 
                 series.  This implies that the resulting series (infinite linear combination) converges. 
                 By construction it will be maximal (up to a multiplicative constant).  There is only 
                 one problem left:  How do we guarantee that the resulting series is lower semicom­
                 putable?
                       The lower semicomputability of a semimeasure is witnessed by a computable 
                 function p :  (i,n)  ^  p(i,n).  There are only countably many such functions, since 
                 there are only countably many algorithms.  Enumerating all those functions, we get 
                 a sequence p^°\p^\p^2\ ...; then we may consider the function
                                                                              71
                                                              p{i,n) = ^ A fcp(fc)(i,n), 
                                                                            k=0
                 where  A*,  is  a  computable  sequence  of rational  numbers  with  ^2k\k  ^  1,  say, 
                 Afc  = 2~k~l.  The resulting function p is non-decreasing in n for every i.  Indeed, as 
                 n increases, the number of terms in the sum defining p increases and the value of 
                 every term increases, too.  And for all i we have
                                                       lim  p(i,n) = V  Afc  lim  p^k\i,n).
                                                     Tl—ïOO                Z —V      71—>00
                                                                             к
                 That is, the constructed semimeasure is indeed equal to the sum of all lower semi­
                 computable semimeasures with weights \ k.
                       However, there is a fault in this argument:  the function p(i, n) should be com­
                 putable,  and thus we cannot use arbitrary enumeration of lower semicomputable 
                 functions  in  our  construction.  We  need  to  arrange  them  so  that  the  function 
                 p  :  (k,i,n)  i—^  p(k^(i,n)  is  computable  as  a  function  of all  its  three  arguments. 
                 Note that we cannot just let p ^  be the function computed by /eth program:  it may
                                                                      4.3.   PREFIX  MACHINES                                                            81
                    happen that the fcth program does not define any lower semicomputable semimea­
                    sure.  (It may compute a function which is not total or a function that sometimes 
                    decreases in the second argument or a function whose sum is greater than 1.)
                           The bug can be fixed using the following:
                           Lemma.  Every program P computing a function of two natural arguments and 
                    taking  rational  values  (and possibly  the  value  — oo)  can  be  algorithmically  trans­
                    formed into a program P' having the following properties.  The program P'  defines 
                    a lower semicomputable semimeasure.  If the program P itself defines a lower semi­
                    computable semimeasure,  then P'  defines the same semimeasure.
                           PROOF.  Let P be any program satisfying the condition of the lemma.  (We do 
                    not assume that P is total.)  First we let P'{i,n) be equal to the maximal number 
                    output within the first n steps in the computations of Р(г,0),... ,P(i,n).  If none 
                    of these computations terminate within n steps or all the results are negative, we 
                    let  P'{i,n)  =  0.  This  definition  guarantees  that  P'(i,n)  is  non-negative  and  is 
                    non-decreasing in n.  For every г, if P(i,n) is defined for all n and is non-negative 
                    and non-decreasing in n, then limn P'(i,n) = limn P(i,n).
                          It remains to ensure that ^p'i ^ 1 where p\ — limnP'(i,n).  To this end first 
                    let  P'(i,n)  = 0 for all n                   i.  This transformation does not change the limit and 
                    preserves monotonicity in n.  The advantage is that now the sum of P'(i,n) over 
                    all  i  is  finite  and  can be computed for every n.  We need that this sum does not 
                    exceed 1.  To enforce this, we do not increase P' if we see that this would violate 
                    our restriction.  We first trim the value P'(i,n) for n = 0, then for n = 1, etc.  The 
                    lemma is proven.
                          Using  the  transformation  described  in  the  lemma,  we  arrange  all  the  lower 
                    semicomputable semimeasures into a computable sequence.  The weighted sum of 
                    all  its  terms  is  a  maximal  lower  semicomputable  semimeasure.  Thus  we  obtain 
                    another proof of Theorem 47.
                          Fix any maximal lower semicomputable semimeasure on the natural numbers. 
                    We will use the notation m(i) or mi for the probability of i and the letter m for the 
                    semimeasure itself.  The value m(i) is called the a priori probability of i.  (Another 
                    name for m is the  universal semimeasure on N.)  Here is an explanation of this 
                    term.  Assume that we are given a device  (a black box)  that  after being turned 
                    on produces a natural number.  For each i we want to get an upper bound for the 
                    probability that the black box outputs i.  If the device is a probabilistic machine, 
                    then a priori  (without  any other knowledge about  the box)  we can estimate the 
                    probability of i as m{i).  This estimate can be much greater than the  (unknown) 
                   true probability, but only 0(1) times less than it.
                          The  a priori  probability  of  a  number  i  is  closely  related  to  its  complexity. 
                    Roughly speaking, the less the complexity is, the larger the a priori probability is. 
                   More specifically, we will show that a slightly modified version of complexity (the 
                   so-called prefix complexity) of i is equal to — logm(i).
                                                                    4.3.  Prefix machines
                          The difference between prefix complexity and plain complexity can be explained 
                   as follows.  Defining prefix complexity, we consider only self-delimiting descriptions. 
                   This means that the decoding machine does not know where the description ends
                           4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
            82
            and has to find this information itself.  One can clarify this idea in several non­
            equivalent ways.  We will discuss all of them further in detail.
                Let us start with a following definition.  Let /  be a function whose arguments 
            and values are binary strings.  We say that /  is prefix stable, if the following holds 
            for all strings X, y.
               (f(x) is defined) and (x is a prefix of у) =*> f(y) is defined and f(y) = f(x).
                Theorem 48.  There exists an optimal prefix-stable decompressor (for the fam­
            ily of all prefix-stable decompressors).
                Proof.  Recall that a decompressor (description mode) is a computable func­
            tion mapping strings to strings.  (All strings are binary.)  The plain complexity is 
            defined using an optimal function in the class of all such functions.  Now we restrict 
            the class of decompressors to computable prefix-stable functions.  We assign to each 
            prefix-stable function D the complexity function Kd, which is as defined earlier: 
            Kp(x) is the length of a shortest description of x with respect to D (i.e., minimal 
            l(y)  among all у such that D(y) — x).  So the definition of Кр(х) coincides with 
            that of Cd(x);  we write К instead of C just to stress that we consider now only 
            prefix-stable decompressors.
                We have to show that there exists an optimal prefix-stable decompressor D (for 
            the class of all prefix-stable decompressors).  The latter means that for any other 
            prefix-stable decompressor the inequality Kd(x) ^ Kd<(x) + c holds for some c and' 
            all x.
                Recall that  for the plain  complexity we have constructed an optimal decom­
            pressor D by letting
                                             D(py) =p(y).
            Here p is  a self-delimiting description  of p,  say,  p = pOl  where p stands  for the 
            string p with all bits doubled.  The notation p(y) refers to the output of the pro­
            gram p given input у (more precisely, the string p is interpreted as a program in a 
            universal programming language).
               Is  this  decompressor  a prefix-stable  one?  Certainly  not.  Indeed,  there  is  a 
            program p computing a function that is not prefix stable, say, p(0) = a and p(00) = b 
           where a ^ b.  Then D(p0) — a and D(p00) = b.
               To construct an optimal prefix-stable decompressor, we modify the definition of 
            D as follows.  We enforce prefix-stability of programs by converting every program 
           p to another program \p\ that works as follows:
                (1)  Apply p to all  inputs in parallel.  If the computation of p on an input у 
           halts with output 2, we write down the pair (y, z).  Let {yi^zf) denote the resulting 
           sequence of pairs (enumerating the graph of p:  Zi = p(yi)).
                (2) We delete some terms of the sequence (p*, zf).  Let us call strings у and y' 
            compatible if one of them is a prefix of the other one (an equivalent definition:  both 
           strings are prefixes of some third string).  We say that a pair  {yi,Zi)  contradicts a 
           pair  (yj,Zj)  if yi  is compatible with pj,  but Zi ^ Zj.  We delete a pair  (yi,Zi)  if it 
           contradicts some other pair (pj, zf) with j < i.  (The argument would work as well 
           if we delete a pair only when it contradicts a поп-deleted previous pair.)
                (3)  Computing the sequence  (yi,Zi)  and  filtering out  some of its  terms  is  a 
           process that does not depend on the input for the program [p].  The input string у 
           is taken into account as follows.  We wait until a (non-deleted) pair (yi, zf) appears
                                         4.3.  PREFIX  MACHINES                          83
            such that ïji is a prefix of y.  Once we encounter such a pair, we print the result 2* 
            and halt.
                For every program p the function у    \p\(y)  is prefix stable.  Indeed,  assume 
            that  \p](y)  = 2.  By construction there is a non-deleted pair  (yi,z)  such that yi  is 
            a prefix of y.  Assume furthermore that у is a prefix of y'.  We need to show that 
            \p](y') — z•  The string yi is a prefix of y' as well, therefore \p\{y') — z or \p](y') = Zj 
            where {yj,Zj) is a non-deleted pair such that j < i and yj is a prefix of y'.  In the 
            latter case yj  is compatible with yi and, since the pair  (yi,z)  does not contradict 
            the pair (yj, Zj), we have Zj — z.
                If p is prefix stable,  then no pair is deleted in the run of its transformed ver­
            sion [p].  Therefore \p\(y) is defined as p’s output on у or a prefix of y.  As we assume 
            that p is prefix stable, the result is the same.
                Now we are able to finish the proof.  Let
                                            D(py)  =   lp](y)-
            We have to verify that D is prefix stable and optimal (in the class of all prefix-stable 
            decompressors).
                To prove the first statement, assume that p\yx  is a prefix of ргР2-  We need to 
            show that D(pxyx) and Б(р2У2)  coincide.  As both the strings pi, p2  are prefixes 
            of the string Р2У2 ,  they are compatible.  Thus pi  = P2  (as the encoding p (->■ p is 
            self-delimiting)  and  y\  is  a prefix of y2-  Since the program  [pi]  (=[рг])  is prefix 
            stable, we conclude that D(pxyx) = \px\(yx) = [Р1КЫ = \p2Ky2) = D(p2y2).
                So we have shown that D is prefix stable.  To prove optimality,  assume that 
            some prefix-stable decompressor D' is given and p is its program.  Then we have 
            D(py) — \p](y) — p(y)-  Therefore the complexity of all strings with respect to D' 
            is at most l(p) greater than the complexity with respect to D.               □
                Let us fix some optimal prefix-stable decompressor and omit the subscript D 
            in  K d (x),  speaking about  the prefix complexity K(x)  of x.  As well as the plain 
            complexity, the prefix complexity is defined up to an 0 (1) additive term.
                There is another way to define prefix complexity.  Instead of prefix-stable func­
            tions, we consider prefix-free functions.  A function is called prefix free if every two 
            different strings in its domain are incompatible.  If a prefix free function is defined 
            on a string, it is undefined on all its proper prefixes and extensions.
                This  time  we restrict  the  class  of decompressors  to  prefix-free ones,  that  is, 
            computable prefix-free functions.  We have the following theorem that is similar to 
            Theorem 48:
                Theorem 49.  The class of all prefix-free decompressors contains an optimal 
            element.
                Proof.  The proof is very similar to the proof of Theorem 48.  This time we 
            construct, for every program p, a prefix-free program {p} that works as follows:
                (1) Just as before, run the program p on all inputs to obtain a sequence (yi, zf) 
            of all pairs such that 2 = p(y).
                (2)  Delete all pairs  (yi, zf) such that yi is compatible with yj for some j < i.
                (3)  Let у denote the input to the program {p}.  We find the first non-deleted 
            pair (yi, Zi) with yi = у and output 2* = {p}(y).
                            4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
            84
                It is easy to verify that the mapping y i-> {p}(y) is prefix free for every p and 
            coincides with the mapping y i-a p(y) if the latter one is prefix free.  The rest of the 
            proof repeats the corresponding part from the proof of Theorem 48.              □
                Let  us  fix  some  optimal  prefix-free  decompressor,  and  let  K'(x)  denote  the 
            corresponding complexity.
                Which of the complexity measures К and K'  is  “the right  one”?  This is  a 
            matter  of taste.  We will  prove  in  Section 4.5  that  these  measures  differ  by  an 
            additive constant (and that both complexities coincide with the negative logarithm 
            of the a priori probability).  Thus the question is which of the two definitions (of the 
            same prefix complexity) is more natural.  Again this is a matter of taste.  Authors 
            believe that the definition based on prefix-stable functions is more natural than the 
            other one (which explains why we started with it).  However, sometimes the second 
            definition is more convenient.  For instance,  its use makes easier the proof of the 
            theorem on the complexity of a pair (Section 4.6).
                One can find the historical account in [18]  (see also the arXiv version of this 
            paper);  making the story  short,  let  us  mention  only that  prefix complexity was 
            independently introduced by Levin who used prefix-stable decompressors (and de­
            noted prefix complexity by KP) and Chaitin who used prefix-free ones (and denoted 
            prefix complexity by H).  Now most English-language papers, following [103], use 
            letter К for prefix complexity.
                The properties of К and K' are similar to those of the plain complexity but- 
            differ in some important aspects:
                   •  We start with a comparison of C and К :
                           C(x)^K(x) + 0( 1)  and  C(x) ^K'(x) + 0( 1).
                     These properties are straightforward, as both prefix-stable and prefix-free 
                     decompressors form a subclass in the class of all decompressors.
                   •  Recall that C(x) ^ /(ж)+0(1), as the optimal decompressor is better than 
                     the identity function.  This argument is not valid for prefix complexity, as 
                     the identity function is neither prefix stable nor prefix free.  We will show 
                     in Section 4.5 that this inequality is false for the prefix complexity.
                   •  Nevertheless there is  an upper bound for prefix complexity in terms of 
                     the length.  We will provide such bounds for K',  and the same bounds 
                     hold for K, the proofs being entirely similar.  Let us show that K'(x)  ^ 
                     2l(x) + 0(1).  Indeed, consider a decompressor
                                               D(x01) — X
                     where x stands for the string obtained by doubling all bits  in x.  This 
                     decompressor is prefix free and K d(x) = 2l(x) + 2.  By replacing 5Ю1 by 
                     a more efficient  self-delimiting encoding x,  we  can obtain better upper 
                     bounds.  For example, letting x = Ып(/(х))01.т, we obtain the bound
                                    K'(x) < l(x) + 21og/(x) + 0(1).
                     By iterating the construction, we obtain the bound
                              K'(x) ^ l(x) + logl(x) + 21oglog/(x) + 0(1)
                     and so on.
                         4.4.  A  DIGRESSION:  MACHINES  WITH  SELF-DELIMITING  INPUT          85
                    •  Like plain complexity, prefix complexity does not increase when algorith­
                      mic transformation is applied:
                                         A'(A(x))^ A'(x) + 0(1).
                      The constant  0(1)  depends on A but does not depend on x.  Indeed,  if 
                      О is a prefix-free decompressor, then so is the composition x    A(D(x)). 
                      This is true for prefix-stable decompressors as well, so we obtain a similar 
                      statement for К in place of K'.  Using this property, we can define prefix 
                      complexity of other constructive objects, such as pairs of strings, natural 
                      numbers, finite sets of strings etc., without specifying how to encode them 
                      by binary strings.
                    •  For prefix complexity, the inequality comparing the complexity of a pair 
                      of strings with their separate complexities is true up to a constant additive 
                      error term rather than a logarithmic one:
                                       K(x,y)tZK(x) + K(y) + 0( 1)
                      (see below Theorem 60 in Section 4.6, p. 97).
                    •  Let D be an optimal decompressor (from the definition of plain complex­
                      ity).  Since the the transformation р и  D{p) does not increase complexity, 
                      we have
                             К(D{p))  <   K(p) + 0(1) «  l(p) + 2 log Z(p) +  0(1).
                      Let p be a shortest description of x with respect to D, that is, D(p) = x 
                      and l(p) = C(x).  Then we have
                                 K(x) = K(D(p)) < l(p) + 21ogl(p) + 0(1)
                                       = C(x) + 21og C(x) + 0(1).
                      Using stronger bounds in place of the bound K(p) < l(p)+2 log l(p)+0(1), 
                      we obtain the inequality
                              К(x) ^ C(x) + log C(x) + 2 log log C(x) + 0(1)
                      and other similar inequalities.
                      4.4.  A digression:  Machines with self-delimiting input
                This  section  is  not  used  in  the  sequel;  here  we  analyze  the  meaning  of the 
            words “self-delimited input” and show that a different interpretation of them leads 
            to  prefix-free  and  prefix-stable  functions  (thus  providing  a  motivation  for  these 
            notions).
                Usually the input is given to a machine in such a way that the machine knows 
            where the input string starts and ends.  For example, for Turing machines we usually 
            assume that initially the head is located at the first symbol of the input string and 
            that its last symbol is followed be a special marker, say, a blank.
                Informally speaking,  a machine with a self-delimited input receives the input 
            bits one by one and has no indication which of them is the last one.  At a certain 
            time it should print a result and halt.
          86            4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
                             #   0    1   0   0    0   1
                           О
                          Figure 10.  A head on a one-way input tape
              4.4.1.   Prefix-free  functions.  Here is  a refinement  of this  idea.  Consider 
          a  Turing machine that  has  an  extra infinite  one-way read-only  input  tape.  The 
          leftmost cell of the tape contains a special marker #.  All the other cells contain 
          either 0 or 1  (Figure 10).
              Initially the input tape head is located in the leftmost cell and thus scans the 
          marker.  The instruction performed by the machine is determined by the symbol 
          on  the  input  tape  it  scans  (and  also  by  the  symbol  on  the  work  tape  and  the 
          machine’s internal state, as usual).  The possible actions are changing the internal 
          state,  writing a symbol on the work tape,  and moving some of the heads  (in any 
          direction on the work tape and to the right on the input tape).  The result of the 
          computation should be written on the work tape in the usual way.  The work tape 
          is initially empty.
              Let M be a Turing machine as described above.  Let us run this machine for 
          all possible contents of the input tape.  If some of the computations halt, we write 
          down two strings:  the string x consisting of all bits scanned by the input head and 
          the result у of the computation.  Let Гм denote the resulting set of pairs (x,y).  If 
          two different pairs  (x\,yi)  and  (^2,^2)  are in Гд/, then the strings 27  and £2  are 
          incompatible.  Indeed, assume that 37  is a prefix of X2.  Since the computation on 
          37  does not go outside 37, it will be valid for Х2 too, and the last bits of Х2 remain 
          unused; thus the pair (^2,2/2)  does not belong to Гм  (unless 27 = X2—in this case 
          У\  = У2 ,  and we get the same pair).
              In particular, the first components of different pairs in Гм  are different.  This 
          means that  Гм  is  a graph  of a function.  We  denote  this  function  by  7m-  Its 
          arguments and values are binary strings.  We say that M  computes 7м  in a prefix- 
          free mode.  It is easy to see that the function 7м is computable in the usual sense. 
          Indeed,  to compute 7m(x),  we write x on the input  tape and any symbols  (say, 
          zeros) to the right of x,  and we then run M.  If M  halts with output y, we verify 
          whether M  has scanned all symbols of x and no symbols beyond x.  If the verification 
          fails, we output no result; otherwise we output у and halt.
              It is easy to see that the function 7м is computable and prefix free (every two 
          different strings in its domain are incompatible).  The converse statement is true as 
          well:
              Theorem 50.  Every computable prefix-free function is computed by some ma­
          chine in a prefix-free mode.
              Proof.  This  statement  is  not  that  evident.  Indeed,  a  (standard)  machine 
          computing a prefix-free function / knows where the input ends and can use this 
          information.  We need to construct another machine M  such that 7м = /•
              Informally speaking, the machine M  reads the next bit only if it can be done 
          safely, i.e., when it is known that /  is not defined on a currently known part of the 
          input because / is defined on its proper extension.  More precisely, fix a machine
                         4.4.  A  DIGRESSION:  MACHINES  WITH  SELF-DELIMITING  INPUT          87
             computing / in the usual sense.  We simulate in parallel its computations on all 
             possible inputs.  Sometimes we will interrupt the simulation and scan a new symbol 
             from  the  input  tape.  More  specifically,  when  a  new  pair  (x,y)  with  f(x)  =  y 
             appears, we compare x with the already scanned part r of the input tape.  If r is 
             not a prefix of x,  then we do nothing and wait until the next pair (x, y)  appears. 
             If r coincides with x, we output y and halt.  Otherwise r is a proper prefix of x.  In 
             this case we read the input tape until we find the first bit where x differs from the 
             contents of the input tape, or we find out that the input tape starts with x.  In the 
             latter case we output y and halt.  In the former case we return to the simulation 
             process and continue it until the next pair (x, y)  appears.
                 How does M start its work?  Initially the scanned part of the input tape is 
             empty.  Once the first pair (x, y)  appears, we look at whether x is empty or not.  If 
             x is empty, we print y and halt.  Otherwise we scan the input tape until we read x 
             or find the first bit where x differs from the contents of the input tape (finding out 
             that x  is not  a prefix of the input).  In the first case we print y and halt.  In the 
            second case we wait for the next pair (x,y).
                 Formally speaking, we maintain the following invariant relation:  after process­
             ing each pair, if r is the scanned part of the input tape, then either:
                 (1)  f(r)  is defined and the machine halts with the output f(r); or
                 (2) r is not a prefix of x for all pairs (x, y) that have appeared so far, but every 
            proper prefix r' of r is a proper prefix of one of such x’s.
                 (A proper prefix of a string is its prefix that is different from the string itself.)
                 It  is  easy to verify that this invariant relation implies that /  = 7m-  We skip 
            this verification and explain informally the main idea of the construction:  if the 
            scanned part r of the input is a proper prefix of a string in the domain of /, then 
            f ( r )   is undefined,  and we can safely read the next bit of the input.         □
                 An equivalent model can be defined in more “practical” terms.  Consider com­
            puter programs that have instructions of the form
                                                b := NextBit.
            Executing this instruction, the program shows on the screen a prompt like  “Enter 
            the next bit”  and waits until the user hits one of the keys  “0”  or  “1”.  After she 
            does this, the input bit is recorded in b and the computation resumes.
                 One can assign a computable function /  to every program of this type.  Namely, 
            f(x)  equals  to  y  if the  program  prints  y  provided  the  user  enters  the  bits  of x 
            successively in response to the program’s prompts.  If the program prints the result 
            before the user enters all the bits of x or if it asks for a new bit after all the bits of 
            x are entered, then f(x) is undefined.
                 It  is easy to modify the arguments above to prove that programs of this type 
            compute all the prefix-free functions and no others.  (Moving the input head to the 
            right is just reading the next input bit.)
                4.4.2.      Prefix-stable functions.  In the previous section we considered block­
            ing  read primitive:  program stops and waits until the next bit  arrives.  There is 
            another possibility:  bits arrive asynchronously and are placed in the input queue; 
            the program may ask whether the queue is empty or not, and continue the execu­
            tion.  Also,  if the queue is not empty, the program may get the next bit from the 
            queue.
                                        4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
                 88
                       To be more specific, we assume that the program may use the instruction
                                                                b := NextExists
                 to find out whether the queue is non-empty.  To read a new input bit the program 
                 invokes the instruction
                                                                  b := NextBit.
                 This instruction removes the first (the oldest) bit from the queue and assigns it to 
                 the variable b.
                       One should specify what happens if the instruction NextBit is performed when 
                 the queue is empty.  We may agree that this causes a crash, or that the computation 
                 is delayed until the next bit arrives.  It is not essential which of these two options 
                 is chosen, since we may guard the input statement by a waiting loop: 
                        while not NextExists do {nothing}; 
                        b     NextBit
                       The advantage of a non-blocking read operation is that we can do some useful 
                 work while waiting for the next input bit.  On the other hand, it is not clear now 
                 how to define a function computed by a program, since the output of the program 
                 may depend not only on the input string, but also on timing.
                       We call a program robust if this is not the case (i.e., if the output is determined 
                 by the input string and does not depend on timing).  If the program is robust, for 
                 any input string x there are two possibilities:  (1)  the program does not produce 
                 output for  any delays  between the consecutive bits  of x;  or  (2)  for some y,  the 
                 program outputs y whatever delays happen between the consecutive bits of x.
                       In  this  way  every  robust  program  computes  a function  /   such  that  /(x)  is 
                 undefined in the first case and equals y in the second case.
                       Theorem  51.  (a)  The function  computed  by  a  robust program, is  both  com­
                 putable and prefix stable.
                       (b)         For every  computable prefix-stable function  there  exists  a robust program 
                 that computes it.
                       Proof,  (a)  The computability of /  is straightforward:  to compute /(x),  we 
                 start our robust program and enter all the bits of x (with arbitrary delays).  Then 
                 we wait until the program outputs a result, which by assumption is equal to /(x) 
                 if /  is defined on x and does not exist otherwise.
                       Let us prove that /  is prefix stable.  We have to show (recall the definition from 
                 Section 4.3) that if a robust program produces y for some input x, then it produces 
                 y on every input x' that is an extension of x.  Start the program and enter all the 
                 bits of x  (with arbitrary delays).  By assumption the program produces y and then 
                 halts.  After that, input all the remaining bits of x'  (the difference between x' and 
                 x)  with  arbitrary delays.  Obviously,  these extra bits do not  affect the output of 
                 the program.  Thus the program produces output y for input x' at least for some 
                 timing.  Being robust, it does the same for arbitrary timing.
                       (b) Let /  be a computable prefix-stable function /.  The robust program r that 
                 computes / works as follows:
                       Using a (non-robust) algorithm that computes /, program r computes in par­
                 allel /(x) for all inputs x.  At the same time r reads all available input bits.  Doing 
                 this,  r  looks for strings x and y such that f(x) = y and x is a prefix of the input 
                 sequence.  Once such a pair (x, y)  is found, program r outputs y and halts.
                         4.4.  A  DIGRESSION:  MACHINES  WITH  SELF-DELIMITING  INPUT          89
                 Assume that  f(x) — y and  all the bits of x are entered  (with some delays). 
             We have to prove that r outputs y and halts whatever the delays are.  Indeed, at a 
             certain time, r knows that f(x) = y and all the bits of x have been entered.  At that 
             time the program outputs y and halts unless it has been halted earlier.  The latter 
             indeed can happen:  the program can halt earlier with the result f(x') where x' is 
             some string compatible with x.  However, since /  is assumed to be prefix stable, we 
             have f(x') — y and the output is the same.
                 If f(x) is undefined and /  is prefix stable, then f(x') is undefined for all prefixes 
             x' of x, and hence the program does not terminate.                                □
                 This theorem provides a motivation for the notion of a prefix-stable function. 
                  96 Construct an algorithm transforming every program p that uses NextBit
             and NextExists calls into a robust program p1  that computes the same function 
             as p does, if p is robust  (and computes some prefix-stable function if p is not). 
                 (Hint:  Use the construction from the proof of Theorem 51 back and forth.)
                  97 (Continued)  Prove that  there exists no  algorithm that  for  a given pro-
            gram p decides whether p is robust or not.
                 (Hint:  This can be done in a standard way, by reducing the halting problem. 
             See, e.g.,  [184].)
                 4.4.3.      Continuous  computable  mappings.  There  is  another,  more  ab­
            stract,  motivation  for  the  notion  of a  prefix-stable  function.  It  goes  back  to  a 
            general theory of computable functionals of higher type, but we restrict our atten­
            tion to a special case we are interested in.  (See [176] for a more general approach.)
                 Let E denote the set of all finite and infinite binary sequences:  E = Sufi.  For 
            a finite string x let Ex denote the set of all finite and infinite extensions of x.  We 
            will consider E as a partially ordered set:  x ^ y if x is a prefix of y.
                 Consider a topology on E whose base consists of all sets of the form E^.  This 
            means that a set is open if it is a union of some sets of this form.  It is easy to verify 
            that we indeed get a topology.  (Note that the resulting topological space does not 
            satisfy the separation axiom.)
                 The following statement is almost obvious:
                 Theorem  52.  A  set A  С  E  is  open  if and  only  if it  satisfies  the following 
             conditions:
                 (1)  if a finite  string x  is  in A.  then  all finite  and infinite  extensions  of x  are 
             in A;
                 (2)  if an infinite sequence is in A,  then some of its finite prefixes are in A.
                 Proof.  Every union of base sets satisfies the conditions  (1)  and  (2).  Con­
            versely, if a set A satisfies both conditions, then it is equal to the union of E^ over 
            all finite strings x in A.                                                         □
                 Add to the natural numbers a new element _L (“undefined”), and let Nj_ denote 
            the resulting set.  Consider the following partial order on this set:  the element _L is 
            less than all natural numbers, and all the natural numbers are pairwise incompa­
            rable (Figure 11).
                 Consider the following topology on the set N U {_!_}.  A set is open if it either 
            does not include the element _L or it coincides with N U {_L}.  It is easy to verify 
            that we get a topological space (that does not satisfy the separation axiom either).
                              4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
             90
                                             0 12 3 4
                                    Figure 11.  The topological space Nj_
                 Let  us identify partial mappings from E  into N with total mappings from E 
             into Nj_; the value _L replaces all undefined values.  The next theorem characterizes 
             continuous mappings (recall that a mapping is continuous if the preimage of every 
             open set is open).
                 Theorem 53.  A (total) mapping F : E —> Nj_  is continuous if and only if the 
             following are true:
                 (1)  F is increasing, i.e., x ф y implies F(x)  ф F(y) (the sign ф refers to the 
             pre-ordering relations on Nj_  and E  introduced above);
                 (2)  if x is an infinite binary sequence and F(x) ф _L,  then x has a finite prefix 
             x' such that F(x') ф _L.
                 PROOF.  Let  F  be  a  continuous  mapping.  To  verify  condition  (1),  assume 
             that  x ф y but  F(x) ф F(y).  Then F(x)  is a natural number  (and not  _L)  and 
             F(x) Ф F(y).  The preimage of the open set {F(x)} contains x and does not contain 
             y, hence it is not open.
                 Let us verify condition (2).  Assume that x is an infinite sequence and F(x) ф _L. 
             The preimage of the set {F(x)} is open and contains x.  Thus it contains some finite 
             prefix of x.
                 It  remains to verify that  any function F satisfying conditions  (1)  and  (2)  is 
             continuous.  We need to verify only that the preimage of every natural number is 
             open  (indeed,  the preimage of the entire space is open,  and other open sets are 
             unions of singletons formed by natural numbers).  It is enough to verify that the 
             preimage  of every  natural  number  satisfies  the  conditions  (1)  and  (2)  from  the 
             previous theorem.  This is a straightforward corollary of our assumptions.  (Note 
             that if 
                    x' is a prefix of x and F(x') ф _L, then F(x') — F(x), as F is increasing.)   □
                 For any given continuous mapping F : E —> Nj_, consider the set Гр of all pairs 
             (x,n) eSxN such that F(x) = n. Note that the set Г p is only a part of the graph 
             of the mapping F (we consider only finite strings x and require that n ф _L).
                 Theorem 54.  The mapping                  is a bijection between continuous map­
             pings E —> Nj_  and sets AcSxN satisfying the following conditions:
                 (1)  (x,n) e A,  x фу  =>  (y,n) в A;
                 (2)  (x,n)  G A,  (x, m) E A  =>  m — n.
                 PROOF.  Assume that the mapping F is continuous.  If F(x) = n €  N,  then 
             condition (1) of the previous theorem guarantees that F(y) = n for every y ^ x. 
             This proves that the set  Гр  satisfies  condition  (1).  As F(x)  cannot be equal to 
             two different numbers,  condition  (2)  is also satisfied.  Thus,  for every continuous 
             mapping F the set Гp has properties (1) and (2).
                 It  is easy to see that the set Гp  uniquely determines F:  if x is a finite string, 
             then F(x) is the second component of the (unique) pair (x,n) € Г^.  If there is no
                             4.5.  THE  MAIN  THEOREM  ON  PREFIX  COMPLEXITY              91
            such pair,  then F{x) —  _L.  If x is an infinite sequence,  then F(x)  is determined 
            uniquely as F{x') where x' is a sufficiently long prefix of x.
                It  remains to show that every set A having properties (1) and  (2) is equal to 
            Г F  f°r  certain  F.  For every  finite x  define  F(x)  as  the  natural number n such 
            that  (x,n)  G  A;  such a number is unique due to  (2).  If there is no such n,  then 
            let  F(x)  = _L.  By condition  (1)  we get an increasing function.  For every infinite 
            xeE, let F(x) be equal to F(x') where x' is any prefix of x such that F(x') Ф _L. 
            If there is no such x', then let F(x) = _L.  By property (1) the value of F(x) is well 
            defined.  The constructed function F satisfies both conditions (1) and (2) from the 
            previous theorem and is continuous.  By construction we have Гp = A.           □
                Conditions (1) and (2) mean that the set A is a graph of a prefix-stable function. 
            We thus have a one-to-one correspondence between continuous mappings £ —»■ Nx 
            and prefix-stable functions.
                Call a continuous mapping F: £ —)■ Nx computable if the set Г/г is enumerable. 
            It is easy to verify that F is computable if and only if the restriction of F to those 
            strings x G E for which f{x) Ф _L is computable in the standard sense.  (A partial 
            function from E to N is computable if and only if its graph is enumerable.)  Thus 
            computable continuous functions £ —> Nx  are basically the same as prefix-stable 
            functions.  This gives an extra motivation for the notion of a computable prefix- 
            stable function.
                           4.5.  The main theorem on prefix complexity
                In this section, we prove that all three complexity measures, К (defined using 
            prefix-stable decompressors), K' (defined using prefix-free decompressors), and the 
            negative logarithm of the a priori probability coincide up to an additive constant. 
            To this end we prove that three inequalities
                                — logm(x) < K{x) A K'{x) < — logm(x) 
            are true up to a constant error term.  We start with two easy inequalities. 
                Theorem 55.
                                          K(x) < Kl(x) + 0( 1).
                PROOF.  This  inequality  would  be  evident  if every  prefix-free  function  were 
            prefix stable.  This is not the case:  a prefix-free function D is undefined on all the 
            extensions of every string и in the domain of D.  On the other hand, a prefix-stable 
            function D is defined on all the extensions v of every string и in the domain of D, 
            and D(v) = D(u).
                Therefore we need a (simple) construction.  Let D be a prefix-free decompressor. 
            Define another decompressor D' as follows:  D'{y) = x if and only if D(y') — x for 
            some prefix y' of y.  As D is prefix free, such y' is unique, thus D' is well defined. 
            To compute D'(y), we just apply D in parallel to all the prefixes y1 of у until we 
            find a prefix y' such that D{y') is defined.
                By construction the function D' is prefix stable and extends D.  Therefore the 
            complexity of each string with respect to D'  does not exceed its complexity with 
            respect to D.  (In fact, the complexities with respect to D and D' coincide, as the 
            described transformation D h* D' does not affect shortest descriptions.)       □
                We could try to prove the converse inequality in a similar way:  consider the 
            restriction of the given prefix-stable decompressor D to minimal descriptions.  That
                           4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
            92
            is, let D'(y) = z if D(y) = г, and at the same time D(y') is undefined for all proper 
            prefixes y'  of y.  This transformation is an inverse of the transformation used in 
            the proof of the last theorem; the resulting function D' is indeed prefix free.  The 
            problem is that it might be non-computable.
                 98  Find a computable prefix-stable function D for which the prefix-free func­
            tion D' constructed in this way is not computable.
                (Hint:  Let A be an enumerable undecidable set, whose complement is thus not 
            enumerable.  Let /(0п11ж) = 0 for all natural numbers n and all binary strings x. 
            Also let /(Onlx) = 0 for all n G A and all x.)
                This problem shows that, in a sense, the non-blocking read operation is more 
            powerful than the blocking one (see Section 4.4).
                Theorem 56.
                                      — logm(x) < K(x) + 0(1).
                Proof.  We have to prove that 2~K^   < cm(x) for some constant c and for 
            all  x.  Recall  that  m  is  the  maximal  lower  semicomputable  semimeasure.  Thus 
            it  suffices  to  find  an  upper  bound  for  the  function  x  i-»  2" ^   that  is  a  lower 
            semicomputable semimeasure.  (In this section we consider discrete semimeasures 
            on the set of all binary strings, as defined in Section 4.1.)
                Let  us  construct  a probabilistic  machine  generating this  semimeasure.  Toss 
            a coin to obtain a sequence 606162 • • •  of random bits.  Simultaneously,  apply the 
            optimal prefix-stable decompressor D (from the definition of К) to all prefixes of 
            the sequence 606162,....  If one of the computations
                                  D(A),D(b0), D(b0bi), D(b0bib2),...
            terminates with a certain result, output that result and halt.  Note that it does not 
            matter which of the terminating computations we choose:  the prefix-stability of D 
            guarantees that this choice does not affect the result.
                Let x be a binary string, and let p be a shortest description of x with respect 
            to  D.  Then the machine outputs x  with probability  at  least  2~l^p\  Indeed,  if 
            the random sequence starts with p, then the result of the machine is x.  Thus the 
            constructed machine generates a measure that is an upper bound for 2~K^ .    □
               There is a slightly different proof of the same theorem, which does not involve 
            probabilistic machines.  The function x ^  2~K^X')  is lower semicomputable.  Thus 
            it is enough to show that it is a semimeasure.
               Theorem 57.
                                            ]T2< 1.
                                             X
               Proof.  For every string x let px be some shortest description of x (with respect 
           to  the  optimal  prefix-stable  function  from  the  definition  of К).  For  every  two 
           different strings x and у the strings px and py are incompatible.  Thus the statement 
           is a direct corollary of the following:
               Lemma.  Let po,Pi,P2> • • •  be pairwise incompatible strings  (that is, neither of 
            the strings is a prefix of another one).  Then JT 2~1^  < 1.
               Indeed, for every i consider the set QPi of all infinite extensions of pi.  Its uniform 
           Bernoulli measure is equal to 2~l(pi').  As the strings pi are pairwise incompatible,
                                4.5.  THE  MAIN  THEOREM ON  PREFIX  COMPLEXITY                      93
             these sets are disjoint and the sum of the measures of all sets £lPi is at most 1.  The 
             lemma and Theorem 57 are proved.                                                        □
                  Theorem 57 implies that the inequality K (x) ^ l(x) + 0(1) is false (thus show­
             ing the difference between the plain complexity C and the prefix complexity К). 
             Indeed, if it were true, the series
                                                         2~ÄX)
                                                      X
             would converge.  However,  for  every n  the terms  of this series  corresponding to 
             strings  X  of length n  sum up to  1  (there are 2n  such terms  and each of them is 
             equal to 2~n).
                   99 Prove that even a weaker inequality K(x) ^ l(x) + log l(x) + 0(1) is false 
             (in other words, the difference K(x) — l(x) — logl(x) is not bounded by a constant).
                  (Hint:  Use the divergence of the harmonic series.)
                  It remains to prove the last  (and most difficult) inequality:
                  Theorem 58.
                                           K'(x) ^ — logm(x) + 0(1).
                  PROOF.  We present first a sketch of the proof.  The semimeasure m(x) is lower 
             semicomputable, so we can generate lower bounds for m(x) that converge to m(x), 
             but  no  estimates for the approximation error are given.  The larger m(x)  is,  the 
             smaller K'(x) should be, that is, the shorter description p we have to provide for x. 
             The descriptions reserved for different strings must be incompatible.  In geometric 
             terms:  for  every binary  string p we consider the  interval  Ip  formed  by  all  reals 
             whose binary expansion starts with p.  The descriptions p\ and p2 are incompatible 
             if the intervals IPl  and IP2 do not overlap.  The inequality l(p) ^ — log2 m(x) means 
             that the length of the interval Ip is at least m(x), i.e.,         ^ m(x).
                  Thus we have to assign to every string x an interval of length at least m(x) so 
             that the intervals assigned to different strings do not overlap.
                  Let  us  specify  more  carefully what  we  need.  First,  for  each  x  it  suffices  to 
             reserve an interval of the length em(x) rather than m(x), for some fixed positive e. 
             This relaxation causes the complexity to increase at most by a constant.
                  Second, we are allowed to use only properly aligned intervals, i.e., intervals Ip 
             for some binary string p.  However, given the above relaxation,  this restriction is 
             not essential.  Indeed, every interval I C  [0,1]  contains a properly aligned interval 
             that is at most four times shorter.
                  So we arrive at a problem that is quite similar to the problem considered in 
             Section 4.1.  There is a sequence of clients.  Each client asks for some space inside 
             [0,1]; a client may increase its request from time to time.  The important difference 
             is  that  now the clients  are interested not  in the total space  allocated but  in the 
             contiguous interval, and this makes our “space management” job more difficult.  To 
             compensate for this difficulty, we are allowed to reduce all the requests, multiplying 
             them by some constant e.
                  Imagine that  clients  are processes  running  on  a computer,  and  the  memory 
             manager has to allocate contiguous properly allocated memory according to their 
             requests that increase in time.  Once allocated, memory cannot be freed (and reused 
             for other process).
        94        4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
          The simplest strategy is to allocate a new interval (in the free memory) each 
        time the request increases.  This does not work, however:  if two clients’ requests in­
        crease in alternating order and in small steps, the overhead cannot be compensated 
        by any fixed e, and we will run out of space.
          The remedy is well known:  one should look forward and increase the allocated 
        interval significantly even if the current increase in the request is small.  For exam­
        ple,  one may allow only powers of 2 as the interval lengths  (then the sum of the 
        lengths is at most twice more than the maximal summand).
          It  is  not  hard  to  present  a detailed proof based on this strategy,  but we will 
        not do that.  Instead, we present a slightly different proof that uses the following 
        statement often called Kraft-Chaitin lemma.  This lemma can be considered as a 
        computable version of the Kraft lemma from information theory (see p. 214).
          Lemma.  Let /о, h, h, ■ ■ ■ be a computable sequence of non-negative integers such
        that
                              г
        Then  there  exists  a  computable  sequence  of pairwise  incompatible  binary  strings 
        xo, X\,X2 , ■ ■ ■  such that l(xi) — l{.
          Note that the inequality of the lemma is a necessary condition for the existence 
        of such a sequence:  the intervals IXi  do not overlap, and their lengths are equal to 
        2~li.  The lemma states that this necessary condition is also sufficient.
          Proof.  Again we have an infinite sequence of clients; the ith client demands 
        we allocate a properly aligned interval of length 2~li for her.  The intervals reserved 
        for different clients should not overlap.  We need to design a computable strategy 
        to fulfill all the clients’ requests.
          There are several ways to describe such a strategy.  Here is probably the simplest 
        one:  let us maintain the representation of the free space (part of [0,1]  that is not 
        allocated) as the union of properly aligned intervals of different lengths.
          Initially this list contains one interval [0,1].  We serve the requests lo, h,h, ■ ■ ■ 
        sequentially.
          Assume that the current request is li, so the required length is w = 2~li.  First 
        note that  one  of the  free  intervals  has  length  at  least  w.  Indeed,  if all  the  free 
        intervals had smaller lengths, their sum (the total amount of free space) would be 
        less than w since they have different lengths and the sum of powers of 2 less that 
        w = 2~l is less than w.
          If there is a free interval in the list that has size exactly w, our task is simple.  We. 
        just allocate this interval and delete it from the free list (maintaining the invariant 
        relation).
          Assume that this is not the case.  Then we have some intervals in the list that 
        are bigger than requested.  Using the best-fit strategy, we take the smallest among 
        these intervals.  Let w'  > w be its length.  Then we split a free interval of size w1 
        into properly aligned intervals of size w, w, 2w, 4w, 8w,..., w'/2; note that
                    w + w + 2w + 4w + 8w + • ■ • + w'/2 = w'.
        The first  interval  (of size w)  is allocated,  and all the other intervals are added to 
        the free list.  We have to check out the invariant relation:  all new intervals in the 
        list  have different sizes starting from w up to w'f 2;  old free intervals cannot have 
        this size since w' was the best fit in the list.
                                                                                                                             4.5.  THE  MAIN  THEOREM  ON  PREFIX  COMPLEXITY                                                                                                                                                                                                                                    95
                                                                      The lemma is proven.
                                                                         100  Prove that the described algorithm can be rephrased as follows:  for each 
                                                    i use the the leftmost properly aligned interval of length 2~li that does not overlap 
                                                   with previously allocated intervals.
                                                                      [Hint:  The construction used in the proof maintains also the following property: 
                                                   the lengths of the free intervals increase from left to right.)
                                                                      Corollary.  Let U  be a computable sequence of natural numbers such that 
                                                                   2~li < 1.  Then K'(i) < k + 0(1).
                                                                     Indeed,  the lemma provides a computable sequence of pairwise incompatible 
                                                   strings Xi of lengths lj..  Define a computable function D by letting D{xi) = i.  As Xi 
                                                   are pairwise incompatible, this function is prefix free.  And D is computable:  given 
                                                   an input x, we compare it with Xi for all i — 0,1, 2,... successively.  Once we find 
                                                   that X = Xi, we output i and halt.
                                                                     (Note that, in this proof, we go back and forth between natural numbers and 
                                                   binary strings when we speak about a priori probability and complexity.)
                                                                     Let us return to the proof of the theorem.  Consider the maximal lower semicom- 
                                                   putable semimeasure m.  By definition there exists a computable function m(x, Ï) 
                                                   taking rational values that is non-decreasing in i such that
                                                                                                                                                                                         m(x) =  lim m(x, i).
                                                                                                                                                                                                                             i—ïoo
                                                   Let  m '(x,i)  stand  for  the  smallest  power of two  (1,1/2,1/4,1/8,...)  that  is  an 
                                                   upper bound for m(x,i).  The function m '(x,i)  is computable and non-decreasing 
                                                   in i.  Its value is between m {x,i) and 2m(x,i).
                                                                     Say that a pair  (x,i)  is a boundary pair if m'(x,i) > m'(x,i — 1)  (or if i = 0 
                                                   and m'(x, 0) > 0).
                                                                     Let  us show that the sum of m'(x,i)  over all boundary pairs  (x,i)  does not 
                                                   exceed 4.  It is enough to show that for every fixed x the sum of m'{x,i) over all 
                                                   boundary pairs  (x,i)  is at  most 4m(x).  This is true since for every fixed x each 
                                                   term in this sum is at least twice bigger than the preceding term.  Thus the sum 
                                                   is  at  most  twice bigger than its last term,  m'(x,i)  for some i,  which is less than 
                                                   2m(x,i).  Now recall that m(x,i)  ^ m{x).  We see that the sum in question is at 
                                                   most 4m(x).
                                                                    The set of all boundary pairs (x, i) is decidable:  to find whether a pair (x, i)  is 
                                                   a boundary pair, we have to compare m '(x,i) and m '{x,i — 1).
                                                                    Enumerate all  the  pairs  (x, г)  and  exclude  all  non-boundary  ones;  we  get  a 
                                                   sequence (xo, io), (xi, ii), ■ ■ ■  of pairs.  Each boundary pair appears in this sequence 
                                                   exactly once.  Define ln by the equality
                                                                                                                                                                                         2~ln = m/(x„,i„)/4.
                                                   The sequence of ln is computable and
                                                                                                                                                                 ^   ^  2                             =          ^   ^                   m ixni in)  ^5  1-
                                                                                                                                                                     П                                                        П
                                                   The corollary  mentioned above implies that  K'(n)  ^  ln + 0(1).  As xn  can be 
                                                   computed given n, we have
                                                                                             K'(xn) < K'{n) + 0(1) < ln + 0(1) = -  logm'(xn, in) + 0(1).
                    4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
         96
         So  for  every  x  the  complexity  K'(x)  does  not  exceed  — logm'(x,i)  if  (x,i)  is  a 
         boundary pair.  Taking the maximal i with this property, we get — logm(x) + 0(1); 
         therefore
                            K'{x) ^ — logm(x) + 0(1).           □
           So all three values К, К' and — logm differ by at most a constant.  Given this, 
         we do not distinguish in the sequel between К and K' (unless the difference in their 
         definitions becomes essential for some special reason), and we use notation К for 
         prefix complexity.
           We tried to provide a detailed proof, and it may look complicated.  The main 
         idea is,  nevertheless, very simple.  Let us try to summarize it again.  In one direc­
         tion a short description p for a string x guarantees that x may appear with high 
         probability (when we decompress a random sequence).  In the other direction the 
         argument is a bit more complicated:  high probability does not mean that there is a 
        short description, and the string may have many long descriptions instead.  Never­
        theless our space allocation algorithm manages to consolidate them:  when the total 
         lengths of intervals for x reaches 2~k for some к, it allocates for x a fresh interval 
        of length fl(2~k).  This can be done from left to right or using the Kraft-Chaitin 
        lemma.
           Let us note that actually we have proven the following statement that will be 
         used in Section 5.6:
           Theorem 59.  For every lower semicomputable sequence of reals po,Pi, ■ ■ ■ such 
         that YliPi  ^  1,  one can effectively find a prefix-free decompressor D  such that 
         K'D{i) < -  log2 Pi + 2.
           This means that given some algorithm enumerating the set of pairs (r, i) with 
        r < pi,  we can find an algorithm for a decompressor D satisfying the inequality 
        for K'D.
                       4.6.  Properties of prefix complexity
           In this section we continue the study of prefix complexity.  We first revisit some 
        already established properties and present their alternative proofs based on the a 
        priori probability.
           It  is  well  known that the series  ^  1/n2  converges.  Multiplying its terms  by 
        a  constant,  we  obtain  a  lower  semicomputable  semimeasure.  Thus  the  a priori 
        probability of a natural number n is at least c/n2 for some constant c.  This implies 
        that
                             K(n) ^ 21ogn + 0(1).
        Let xn be the nth string in the sequence A, 0,1,00,01,10,11,000,... of all binary 
        strings.  Then
                 K(xn) < K(n) + 0(1) ^ 21ogn + 0(1) = 2l(xn) + 0(1);
        the last equality is true, since xn is n + 1 in binary notation without the leading 1, 
        so the length of xn is logn + 0(1).  (There is a special case n = 0, as both I/O2 and 
        logO are undefined; the changes needed to handle it are trivial.)
           So we get the inequality K(x) ^ 2l(x) + 0(1).
                                                      4.6.    PROPERTIES OF PREFIX COMPLEXITY                                                        97
                          To prove a better upper bound for prefix complexity, we may consider a con­
                    verging series                                          E 1
                                                                                  n log  n
                    (To prove its convergence, compare it with the corresponding integral.)  Using this 
                    series, we obtain the inequality K (n) < logn + 2 log log n + 0(1) or (for strings)
                                                             K(x) < l(x) + 2 log l(x) + 0(1)
                    (for the alternative proof of this inequality, see p. 84).
                          Using the series X) l/(n log n(log logn)2), ^  l/(n logn log log n(log log logn)2), 
                    etc., we can improve the bound further.
                          Now we prove the inequality relating the prefix complexity of a pair to prefix 
                    complexities of its components.
                          Theorem 60.
                                                            K(x,y)^K(x) + K(y) + 0( 1).
                          Just as in the case of plain complexity, we define K(x,y) as the complexity of 
                   the string [x, y] where (x, y) t-t [x, y] is a computable injective encoding of pairs of 
                   binary strings.  The complexity of a pair does depend on the choice of the encoding; 
                   switching to another computable injective encoding changes complexity by at most 
                   an additive constant.  Indeed, the translation between any two computable injective 
                   encodings is an algorithmic transformation.
                          PROOF.  Consider the function m! defined as
                                                                   m!{[x, y\) = m{x)m{y).
                   Here x and y are binary strings, [x, y\ is the encoding of the pair, and m stands for 
                   the a priori probability.  If z is not an encoding of any pair, we let m'(z) = 0.
                          The function m! is lower semicomputable  (take the product of lower bounds 
                   for m(x) and m(y) as a lower bound for m{x)m{y)).  Furthermore, we have
                         ^m'(z) = ^m'([x,y]) = ^m(x)m(y) = ^  m(x) ^m (y) <1-1 = 1.
                                              x,y                      x,y
                   Thus m! is a lower semicomputable semimeasure.  Comparing m! with the a priori 
                   probability,  we obtain the inequality m'([x,y])  <  cm([x,y])  for some constant  c. 
                   Hence
                                                           K([x,y])^K(x) + K(y) + 0( 1).
                   The theorem is proved.                                                                                                            □
                           1011 Prove that the sum                        m([x, y]) differs from m{x) by at most a constant
                   factor (in both directions).  Prove a similar statement for maxy m(x, y).
                           102 Let / : N —> N be a strictly increasing computable function.  Prove that
                   the value J2im(k)\f(n) < к < f(n + 1)} differs from m(n) at most by a constant 
                   factor  (in  both directions).  (So  if we  split  the  series  J2nm(n)                                           grouPs  in a 
                   computable way, the sums of the groups form essentially the same series!)
                          Let us prove now Theorem 60 using decompressors.  It turns out that we need 
                   to use prefix-free (and not prefix-stable) decompressors.
                          Let  us prove that  K'([x,y])  <  K'{x) + K'{y) + 0(1).  Let D be an optimal 
                   prefix-free  decompressor  used  in  the  definition  of K'.  Define  a  new  prefix-free
          98          4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
          decompressor D'.  Informally,  the  algorithm  D'  reads  the  input  until  it  finds  a 
          description of x.  Then it reads the rest of the input until it finds a description of y. 
          Formally, we define D' as
                                  D/(pq) = [D(p),D(q)}.
          Here pq stands for the concatenation of strings p and q.  In other words, we try to 
          split the input into two parts p and q in such a way that both D(p) and D{q) are 
          defined.
             We need to verify that D' is well defined.  Indeed, assume that x is represented 
          as pq in two different ways, x — pq = p'q',  and all the values D(p), D(q), D(p'), 
          D(q') are defined.  Then p and p' are compatible (being prefixes of the same string x) 
          and thus coincide (as D is prefix free), hence q — q'.
             In a similar way we can prove that the function D1 is prefix free.  Let pq be a 
          prefix of p'q',  and let both belong to the domain of D.  The strings p and p'  are 
          compatible and both D(p)  and D(p')  are defined, therefore p = p'.  This implies 
          that g is a prefix of q'.  As both D(q) and D(q') are defined, we have q = q'.
             The function D1 is computable: to find D'(x), we compute in parallel D(p) and 
          D(q) for each possible way to split x into p and q.  We have shown that there is at 
          most one representation of x as pq such that D{jp) and D(q) are defined.  If we find 
          such p and q, we output the string [D(p), D(q)].
             It remains to note that
                              KDi([x,y]) ^ KD(x) + KD{y).
          Indeed, let p and q be shortest descriptions of x and y with respect to D.  The string 
          pq is a description of [x,y] with respect to D1 and has length Kd(x) + Ko(y)- 
             In other words, D1 reads the input as D does until p and D(p) are found, then 
          reads the rest of the input again to find q and D(q).
              103 Prove Theorem 60 using the definition of prefix-free decompressors in
          terms of machines with blocking read operation (see Theorem 50 on p. 86).
              104  A set of binary strings is called prefix free if any two elements of it are
          incompatible.  Show that if sets A and В both are prefix free, then so is the set
                                AB = {ab \ a £ A,b £ B}.
             Which proof of Theorem  60  (using  a priori  probability  or  using  prefix-free 
          decompressors)  is easier and more natural?  It is  a matter of taste—the authors 
          believe that the first one is more natural.  The next theorem provides an opposite 
          example:  encoding arguments here seem to be simpler than the arguments using 
          the a priori probability.
             Theorem 61.
                               K(x,K(x)) = K(x) + 0( 1).
             (Problem 23 asks us to prove the same equality for plain complexity.)
             Proof.  The inequality K{x) ^ K(x, K(x)) + 0(1) is straightforward, as the 
          string x can by computed given the encoding [x, K{x)] of the pair.
             To prove the converse inequality, let D be an optimal prefix-free decompressor 
          used in the definition of prefix complexity K'.  Define a new decompressor D1 as
                                  O’ip) = [D(p),i(p)J.
                                       4.6.  PROPERTIES  OF  PREFIX  COMPLEXITY                               99
               The domain of D coincides with that of D,  hence D'  is prefix free.  Let p be a 
              shortest description of x with respect to D.  Then l(p) = K'(x) and therefore p is 
              a description of the string [x,K'{x)\ with respect to D'.  Thus
                                            KD.{[x,K'{x)}) ^l{p) = K'(x).
                   Is  the theorem proven?  There is one subtle point in the argument.  We have 
              proved the theorem for the complexity К', defined via prefix-free decompressors.  If 
              we substitute К for K' in the equality K'{:r, K'(x)) = K'(x) +0(1), its right-hand 
              side will change by an additive constant.  The similar statement for the left-hand 
              side is not straightforward, as K' has two occurrences there, and the second one is 
              inside the argument.  But at least we have K(x,K'(x)) = K(x) + 0(1).
                   To finish the proof,  it  remains to show that the function K(x,n)  changes at 
              most by a constant, as n changes by 1.  This easily follows from the computability 
              of mappings [x, n]        [x, n + 1]  and [x, n]     [x, n — 1].                                □
                   It is instructive to prove Theorem 61 using the a priori probability.  Let m(x) 
              be the a priori probability of x.  Define the function m! as
                                          m'([x, &]) =      Гк  if 2~k < m(x);
                                                              0   otherwise.
              This function is lower semicomputable:  given x and k, we generate lower bounds 
              for m(x) and output 0 until we find that 2~k < m(x), and then we output 2~k.
                   For every fixed x the sum of m'([x, к]) over all к is a geometric series formed 
              by  powers  of 2.  Therefore this  sum  is  less  than  2m(x)  (the  largest  term of the 
              series is less than m(x)).  Therefore, the sum of m'([x, &]) over all x and к is finite. 
              Comparing m'([x, &]) and the a priori probability of [x, к], we conclude that
                                                   m(x,k) ^ 2-fc+0(1) 
              if 2~k < m(x).  Taking the logarithms, we see that
                                                    K(x,k) ^k + 0( 1)
              whenever 2~k < m(x).  The latter inequality holds for к = —[\ogm(x)\ + 1 and 
              thus we have
                                        K(x, -Llogm(x)J + 1) < K{x) + 0(1).
              It  remains to recall that  the function K(x,n)  changes at most by a constant,  as 
              n changes by 1.  The second proof of Theorem 61  (in the non-trivial direction)  is 
              finished.
                    105  This argument proves a bit more:  K(x, и) ^ u+0( 1) whenever K(x) ^ u. 
              How do we derive this inequality from Theorem 61 (from its statement and not from 
              its proof)?
                   We proceed now  to  the  algorithmic  properties  of the  function  K(x).  Like 
              plain complexity, prefix complexity is upper semicomputable but not computable. 
              Moreover, there is no computable non-trivial (= unbounded) lower bound for K(x). 
              Indeed, since K(x) ^ 2C(x) + 0(1), every non-trivial lower bound of К would yield 
              a non-trivial lower bound of C.
                   Recall that the plain Kolmogorov complexity C(x) can be defined as the small­
              est upper semicomputable function к such that the cardinality of the set {x \ k(x) < 
              n} is 0(2n) for all n (Theorem 8, p. 19).  Here is a similar statement for the prefix 
              complexity:
              1 0 0            4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
                  Theorem 62.  The function К is the smallest (up to an additive constant term) 
              upper semicomputable function к  (mapping binary strings to natural numbers and 
              -foo)  such that the series      2~k^  converges.
                  Proof.  The function К is upper semicomputable and the series Y^x 
             converges.  Let к be another function having these properties.  Then the function 
             M(x) — c2~k(x^  where c is  a small  enough constant  is  a lower semicomputable 
             semimeasure.  As m(x)  is  the  maximal  lower  semicomputable  semimeasure,  we 
             have M(x) = 0(m(x)), that is, logM(:r) < logra(:r) + 0(1). It follows that K(x) ^ 
             k(x) + 0(1).                                                                              □
                  In  other words,  for  every  upper semicomputable  function к  mapping binary 
             strings to natural numbers and +oo, the two statements
                                  uK(x) ^ k(x) + 0(1)” and U^2X 2~k^  < oo”
             are equivalent.
                  Note that the requirement  “the series         2~k^   converges”  is stronger than
             the requirement “the number of x such that k(x) < n is 0(2n)” used in Theorem 8. 
             Indeed, if Y^x 2~k^  ^ O, then the number of x such that k(x) ^ n is at most C2n. 
             This observation gives another proof of the inequality C(x) ^ K(x) + 0(1).
                  It  is  instructive  to  compare  plain  and prefix complexity  in two  aspects:  the 
             average complexity of strings of given length and the number of strings that have 
             complexity not exceeding a given bound.  Let us start with the first question.
                  We have seen that the plain complexity of most strings of length n is close to n 
             (p. 8 and Problem 2, p.  17).  One could expect the prefix complexity to be slightly 
             bigger.
                  Theorem 63.  (a) K(x) < l(x) + K(l(x)) + 0(1).
                  (b)      For some constant c and for all n, d the fraction of strings x such that 
             K(x) < n + K(n) — d among all strings of length n is at most c2~d.
                  Proof,  (a)  Let  m(x)  be  the  a  priori  probability  of a  binary  string  x  and 
             m(n)  be the  a priori probability  of a natural  number n.  Consider the function 
             m'(x)  =  2~nm(n)  where n is the length of x.  The sum of m'(x)  over strings of 
             length n is equal to  m(n)  hence  ^^ra^a;)  ^  1-  Since the function ml  is  lower 
             semicomputable, we conclude that m'(x)  < cm(x) for some constant c and all x. 
             Taking the logarithms, we obtain the inequality
                                             K(x) ^ n + K(n) + 0(l)
             (the constant 0(1) does not depend on n).
                  (b)  Consider the function
                                               m'(n) =          m(x),
                                                         l(x)=n
             the total a priori probability of all strings of length n.  Since m'(n)  is lower semi­
             computable and Xlnm/(n)  ^  I**  we have m'(n)  = 0(m(n)).  On the other hand, 
             the a priori probability of the string consisting of n zeros is at least cm(n) for some 
             positive constant c.  Thus we have
                                         cim(n) <  ^   m(x) ^ C2m(n).
                                                    l(x) = Tl
                                       4.6.  PROPERTIES  OF  PREFIX  COMPLEXITY                            101
              So the sum of m(x) over all binary strings of length n coincides with m(n)  (up 
              to  a constant factor).  Thus the average of m(x)  over all strings x of length n is 
              m(n)/2n (up to a constant factor).  The fraction of strings x, such that m(x) is 2d 
              times bigger than the average, is at most 2~d (Chebyshev’s inequality).                       □
                    106  Prove that the average prefix complexity of strings of length n is equal
              to n + K(n) + 0(1).
                   (A similar question for plain complexity was considered in Problem 3.)
                   Now we estimate the number of strings with complexity at most n.
                   Theorem 64.  The number of strings x with K(x) <n is 2n~-ftr(n)+0(1).
                   Proof.  Let cn be the number of strings x such that K(x) < n.  Let us rewrite 
              the basic property of prefix complexity (the convergence of the series ^  2~K                 in 
              terms of cn.  There are exactly cn+1 — cn  strings of complexity n.  Therefore the 
              series
                                                   ^ 2 “n(cn+i -  Cn)
                                                    П
              converges.  Regrouping the terms of this series, we conclude that
                                       £(2-<"-‘) _ 2~n)cn =             2 ~ n Cn  < oo-
              Since the function cn  is lower semicomputable,  this implies that 2  ncn  does not 
              exceed the a priori probability m(n) of n.  Hence cn ^ m(n)2n = 2n  K(n)  (up to a 
              constant factor).
                   On the other hand, it is easy to construct an upper semicomputable function 
              к whose values are natural numbers (and Too) that takes the value n on (approxi­
              mately) m(n)2n arguments.  This can be done in many ways.  For example, let us 
              agree that for a string x of length n the value k(x)  can be either +oo or n;  it is 
              equal to n if the ordinal number of x  (in the list of all n-bit strings)  is less than 
              m(n)2n.
                   For this  function  A:,  the  series  ^ 2 ”^     converges.  So  K(x)  ^  k(x) + 0(1), 
              hence cn+o(i)  ^ m(n)2n.  Both m(n) and 2n change at most by a constant factor 
              as n increases by 1.  Thus m(n)2n = 0(cn).                                                    □
                   These results may create an impression that prefix and plain complexity mea­
              sure essentially the same quantity but using slightly different scales, so the prefix 
              complexity is (slightly) bigger just because of the shifted scale.  Or, maybe, is there 
              a more fundamental difference?  This question can be formalized as follows:  Are 
              there  two  sequences  an  and  bn  of strings  such  that  C(an) — C(bn)  —>  Too  but 
              K(an) — К (bn)  —>  — oo?  This  question was  answered  by  An.  A.  Muchnik  and 
              S. Positselsky who proved that sequences with these properties do exist [137].  An­
              other proof was provided by J.  Miller in  [122];  this paper contains other results 
              about the relation between plain and prefix complexities, but we restrict ourselves 
              to several simple remarks (see also Section 4.7.4, p.  112).
                   Iterating the inequality
                                                 K(x) ^ l(x) + K(l(x))
                               4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
              102
             we obtain the following series of inequalities:
                            K(x) ^ l(x) + l(l(x)) + K(l(l(x))) + 0(1),
                            K(x) ^ l(x) + l(l(x)) + l(l(l(x))) + K(l(l(l(x)))) + 0(1),
             etc.  Similar inequalities with C instead of I can be obtained as follows.  Let D be 
             the optimal decompressor for plain (not prefix) Kolmogorov complexity.  Combining 
             the inequalities K(D(y)) ^ K(y) + 0(1) and K{x) ^ l{x) + K{l{x)) + 0(1), we get 
             the following series of inequalities:
                  Theorem 65.
                                K(x)^C(x) + K(C(x)) + 0( 1),
                                K(x) < C(x) + C(C(x)) + K(C(C(x))) + 0(1),
              etc.
                  Note that the second inequality (as well as all others) follows from the first one 
             by iteration.
                  1107 I Prove that
                                          0(x, y) ^ K(x) + C(y) + 0(1)
             for all X, y.
                  {Hint:  One can compute the number of pairs for which the right-hand side is 
             less than n, but it is easier to use prefix-free descriptions.)
                  As we mentioned, one could define random n-bit strings as strings whose (plain) 
             complexity is close to n.  But one can also try to use prefix complexity and require 
             the prefix complexity to be maximal, i.e., close to n + K(n).  The following problem 
             shows that for such strings the plain complexity is also (almost) maximal.
                   108 Let X be an n-bit string such that C(x) ^ n — d for some d.  Show that 
             K(x) ^ n + K(n) — d + O(logd).
                  (Hint:  Join  the  prefix-free  descriptions  for  n  and  d  and  a plain  description 
             for X.)
                  The reverse statement is not true,  as R.  Solovay has shown;  see the already 
             mentioned paper of J. Miller [122] or [136, 6].
                     4.7.  Conditional prefix complexity and complexity of pairs
                  4.7.1.  Conditional prefix complexity.  What is conditional prefix complex­
             ity?  Each  of the  definitions  of prefix  complexity  can  be  modified  by  adding  a 
             condition.
                  We start with a definition using prefix-stable functions.  A function D(y,z) is 
             prefix stable with respect to y if for every 2 the function у     D(y, z) is prefix stable:
                              D(y, z) is defined and у < y'  =>  D(y\z) -  D(y, z).
             We assume here that the first argument of D is a binary string; the notation у ^y' 
             means that у is a prefix of y'.
                  Recall the definition of the (plain) conditional complexity from Section 2.2.  A 
             conditional decompressor (—description mode) is a computable function that maps 
             pairs of binary strings to binary strings.  If D(y, z) = x, then у is called a description 
             of x when z is known.  The complexity of x with condition 2 is the length of the 
             shortest description.  Then we fix an optimal conditional decompressor that gives 
             minimal complexity (up to a constant).
                     4.7.  CONDITIONAL PREFIX  COMPLEXITY AND  COMPLEXITY OF PAIRS              103
                 Now we consider only decompressors that are prefix stable with respect to the 
             first argument.  This smaller class of decompressors contains an optimal decompres­
             sor (for this class).  The proof of this statement is similar to the proof of Theorem 48 
             (p.  82)  where an optimal unconditional prefix-stable decompressor is constructed. 
             We modify this proof by adding the parameter г in all formulas.  More specifically, 
             let
                                             D'(py,z) = \p](y,z).
             Here  [p]  stands  for  the  program  obtained  from p  via  “prefix  stabilization  with 
             respect to у for each z”.  This means that for all p, z, the function у    \p]{y, z)  is 
             prefix stable, and if the function у   p(y, z) itself is prefix stable for some 2, then 
             it  coincides with the function у    \p](y,z).  It is easy to verify that this is indeed 
             possible and that D' is an optimal prefix-stable (with respect to the first argument) 
             decompressor.
                 Fix an optimal conditional prefix-stable decompressor, and denote the resulting 
             complexity by K(x\z), the prefix complexity of x with condition z.
                 If we consider prefix-free decompressors (instead of prefix-stable ones), we ob­
             tain an alternative definition of conditional prefix complexity.  The existence of an 
             optimal function in this class of decompressors is proved in a similar way.  The re­
             sulting complexity could be denoted by K'(x\z).  Like their unconditional versions, 
             functions K(x\z) and K'(x\z) differ by at most an additive constant, which does 
             not depend on x and г:
                                          K'(x\z) = K(x\z) + 0(l).
                 As in the case of unconditional complexities, this is proved using the conditional 
             a  priori  probability  m,(x\z).  It  can  be  defined  in  two  ways  (using  probabilistic 
             machines and lower semicomputable semimeasures).
                 Let M be a probabilistic machine with an input.  Let рм{х \ z) denote the prob­
             ability that  M outputs the string x for input  г.  The function  (x,z)  i-> pm(x\z) 
             is  lower  semicomputable,  and  for  all  г  the sum  Y1xPm(x\z)  does  not  exceed  1. 
             Conversely,  for  every  lower  semicomputable  function  (x, z)  ^  p(x \ z)  that  takes 
             non-negative real values such that  YhxP{x\z)  ^  1  f°r all z->  there exists a proba­
             bilistic machine M with рм = P-
                 The class of all functions рм has an optimal function, that is, the greatest one 
             up to a constant  factor.  Fixing an optimal function in this class,  we obtain the 
             conditional a priori probability m.(x\z)  of the string x with condition z.
                 The inequality K(x\z) ^ K'(x \ z) + 0(1) is easy (as in the unconditional case). 
             To show that all three quantities K(x\z), K'(x\z)  and — \ogm(x\z)  coincide up 
             to an additive constant, we need to show that — \ogm(x\z) ^ K(x\z) + 0(1) and 
             K' (x\ z)  ^  — \ogm,(x\z) + 0(1).  We  omit  those  proofs  since  they  repeat  their 
             unconditional versions.
                 One could say that these inequalities and their proofs are “relativizations” of the 
             respective unconditional inequalities and proofs.  The relativization is understood 
             here in  a non-standard way.  In the theory of computation,  relativization means 
             that  the  class  of computable  functions  is  replaced  by  the  class  of A-computable 
             functions, i.e., the class of functions computable with a given oracle A.  (Here A is 
             an arbitrary set of binary strings.  A function is computable with oracle A if it is 
             computed by an algorithm that is allowed to make queries of the form  “x G AT'. 
             That is,  the algorithm calls  an external procedure that on input x returns true
        104       4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
        or false depending on whether x is in A or not.)  Almost all known theorems in 
        general computation theory are relativizable,  i.e.,  they remain true if we replace 
        (everywhere) computable functions by А-computable functions.
          By the way, the notion of Kolmogorov complexity can be relativized in a stan­
        dard way, too.  That is, for every set A we can define the plain Kolmogorov complex­
        ity CA{x) and the prefix Kolmogorov complexity KA(x) (see Section 6.4).  However, 
        we do not consider relativized Kolmogorov complexity now.  Instead of algorithms 
        having oracle access to a set of strings, we consider algorithms having access to a 
        finite string z.  In this  way we obtain conditional complexity C(x\z)  or K(x\z) 
        instead of C(x) (resp.  K(x)).  Since z is finite, the access to it does not increase the 
        power of algorithms (any z-computable function is computable without z).  How­
        ever,  the access to z changes Kolmogorov complexity, if z contains non-negligible 
        information.  Here is another example of this kind of relativization:  the quantity 
        I(x : y\z) can be considered as common information in x and y relative to z.
          Up to now the structure (prefix relation) used in the definition of prefix-stable 
        and prefix-free functions was applied to descriptions only.  The described objects, as 
        well as conditions, had no structure at all.  The other approach is also possible:  we 
        could take into consideration the binary relation  “to be a prefix of”  on described 
        objects  as  well.  This  will  lead  us  to  monotone  complexity  (see  Chapter  5)  and 
        decision complexity (Chapter 6).  On the other hand, we could consider the relation 
        “to be a prefix of” on conditions as well (see Section 6.3).  The resulting complexities 
        make sense; however, they are not well studied yet.
          Note that all the requirements in the definitions of prefix-free and prefix-stable 
        decompressors treat different  conditions  separately.  For example,  requiring that 
        a  machine  can  tell  when  the  input  ends,  we  allow  this  decision  depend  on  the 
        condition.  This is an important remark, and we can come to wrong conclusions if 
        we forget about this.  One should be really careful here; for example, the statement 
        of Problem 28 (p. 35) is not true for prefix complexity:
           109 Show that K(y\x) does not exceed the minimal prefix complexity of a 
        program mapping x to y (up to an 0(1) additive term).  The converse statement 
        is  false.  (Both statements hold for every reasonable programming language;  the 
        additive constant depends on the chosen language.)
          (Hint:  It is easy to see that K{y\l{y)) ^ l(y) + 0(1).  Indeed, every string y is 
        its own self-delimiting description when l(y) is known.  If the inequality in question 
        were true, there would be 2n different programs of prefix complexity at most n.)
          4.7.2.  Properties of conditional prefix complexity.  Let us mention sev­
        eral simple results about conditional prefix complexity.
            •  K(x\z) < K(x) + 0(1).
                Indeed,  a  prefix-stable  (or  prefix-free)  unconditional  decompressor 
             y  i—^  D{y)  can  be  considered  as  a prefix-stable  (resp.  prefix-free)  con­
             ditional decompressor  (y, z)  i->  D(y)  that just  ignores the second argu­
             ment z.
                Using semimeasures:  any probabilistic machine without input can be 
             considered  as  a machine that  has  input  but  ignores  it.  And  any  lower 
             semicomputable semimeasure q(x) can be treated as a family q'{x\z)  = 
              q{x) indexed by z.
            •  K(x I x) = 0(1).
                     4.7.  CONDITIONAL  PREFIX  COMPLEXITY AND  COMPLEXITY OF  PAIRS            105
                           Indeed,  the  decompressor  D(y, z)  —  z  is  prefix  stable  (recall  that 
                      prefix-stability is about y, not z) and Kd(x\x) — 0.  We can also change 
                      it to get a prefix-free decompressor:  let D{A, z) = z where A is an empty 
                      string,  and  let  D(y, z)  be  undefined  if у  ф  Л.  Finally,  the  family  of 
                      semimeasures can be constructed as follows:  q(x\x) = 1 and q(z\x) — 0 
                      for z ф X.
                    • K(f(x, z)\z)  ^  K(x\z) + 0(1)  for  any  computable  function  /   and  for 
                      any strings x,z such that f(x,z) is defined.  (The constant in 0(1) may 
                      depend on / but not on x and z.)
                           Indeed,  let  D  be  the optimal  prefix-stable  (resp.  prefix-free)  condi­
                      tional  decompressor.    The  mapping  D' :  (y, z)  i-»  f(D(y,z),z)  is  also 
                      a  prefix-stable  (resp.  prefix-free)  decompressor  and  KD>(f(x,z)\z)  ^ 
                      KD(x\z).
                           In  terms  of semimeasures  the  same  argument  goes  as  follows:  let 
                      m{x\z)  be the a priori probability of x with condition z.  Consider the 
                      semimeasure
                                    q(x I z) = ^2{т(х/ I z) \ f(x', z) = x}
                      (for each z this is  an image of the semimeasure x h) m(x. z)  under the 
                      mapping x h) f(x, z)); it is easy to check that q is lower semicomputable, 
                      that  ^2xq{x\z)  <  1  and q{f(x, z) \ z)  ^ m(x\z).  Since m, is optimal,  we 
                      get the desired inequality for a priori probabilities and their logarithms.
                    •  K{f{x)\x) = 0(1) for any computable / and for all x such that f(x) is 
                      defined.
                           (A simple corollary.)
                    •  K(x\z) ^ K(x\f(z)) + 0(1) for every computable function /  and for all 
                      x, z if f(z) is defined (the constant in 0(1) may depend on /  but not on 
                      x and z).
                           (Indeed, consider the decompressor (y,z) н  D(y,f(z)) or the condi­
                      tional semimeasure q(x\z) = m(x \ f(z)).)
                    •  C(x I z) < K{x I z) + 0(1).
                          Indeed, prefix-stable and prefix-free decompressors form a subclass in 
                      the class of all decompressors used in the definition of C(x\z).
                    •  K(x I z) < C{x I z) + 2 log C(x\z) + 0(1).
                          This is a corollary of previous statements.  Indeed, let D be the opti­
                      mal conditional decompressor (not necessarily prefix stable or prefix free). 
                      Then
                           K(D{y,z)\z) ^K(y\z) + 0{1)
                                         ^ K(y) + 0(1) ^ l(y) + 21ogl(y) + 0(1).
                      If у is the shortest description of x with condition z, then l(y) = 0(.т|2:).
                          In the same way one can prove a stronger inequality
                         K(x\z) ^ C(x\z) + logC(x\z) + 2\og\ogC(x\z) + 0(1), 
                      etc.
                 4.7.3.  Prefix complexity of a pair.  As we have seen (Theorem 60, p. 97), 
             K(x, y) ^ K(x) + K(y) + 0(1).  Let us prove a stronger inequality:
                 Theorem 66.
                                      K{x,y) ^ K(x) + K(y\x) + 0(1).
                           4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
            106
                Proof.  We can use either prefix-free decompressors or semirneasures.  Both 
            versions are instructive.
                Using  prefix-free  decompressors.  Let  D  be  the  optimal  unconditional 
            prefix-free  decompressor.  Let  Dc  be  the  optimal  conditional  prefix-free  decom­
            pressor.  Consider the function D' defined as
                                     D'(uv) = [D(u),Dc(v,D(u))\
            (for  и  and  v  such  that  the  right-hand  side  is  defined).  Following  the  proof of 
            Theorem 60, we note that D' is well defined and is a prefix-free (unconditional) de­
            compressor.  The concatenation of the shortest D-description for x and the shortest 
            /^-description for у (with condition x) is a description for [x,y].
                (Note that  the  order  of и  and  v  is  crucial  for  this  argument.  Replacing  uv 
            by vu, we get into a trouble:  to find where v ends, we have to use the prefix-free 
            property of Dc, but it is valid only for a fixed condition and D(u) is not determined 
            yet.)
                Using semimeasures.  Let m(x) be the unconditional a priori probability of 
            x,  and  let  m(y | x)  be the  conditional  a priori probability of y  when x  is  known. 
            Consider the function m! defined as
                                       m!(\x,y\) = m(x)m(y\x)
            (we assume that m'(z) = 0 for strings г that are not encodings of any pairs).  Then 
            m! is lower semicomputable (being a product of two non-negative lower semicom- 
            putable functions), and
                     m'{z) = ^2 m(x)m(y \ x) = ^  [m{x) ^  rn{y \ x)]  ^ ^  m(x) ^ 1.
                   z          x,y                x        y              X
            Therefore, m([x,y]) ^ £m'([x,y]) = £m(x)m(y\x).                              □
                110 I Prove that C(x, y) ^ K(x) + C(y | x) + 0(1).
                (Hint:  One may use a prefix-free decompressor and append the (plain) descrip­
            tion of у given x to the prefix-free description of x.  We may also count the number 
            of pairs such that K(x) + C(y \ x) ^ n.  We have at most 2k • m(k) • 2n~k = 2nm(k) 
            pairs such that K(x) = k, and the sum over к gives 2n • 0(1).)
               Further improvements are possible.  First note that we can use pairs of strings 
            as conditions by using some computable injective encoding (changing the encoding, 
           we change the complexity by at most a constant).  For similar reasons we can speak 
            about complexity of a triple of strings.  Now we can write the following chain of 
            inequalities (the 0(1) terms are omitted):
            K(x, y) ^ K(x,K(x),y) ^ K(x,K(x)) + K(y\x,K(x)) = K(x) + K(y\x,K(x)).
           Here the equality K(x, K(x)) = K(x) (Theorem 61) is used as well as the inequality 
           for the entropy of pairs (Theorem 66).  We get an inequality that can be considered 
           as  a  strong  form  of Theorem  66,  since  K(y\x,K(x))  ^  K(y\x)  (because x  can 
           be produced from [x, K(x)] by an algorithm).  As noticed by Levin (see [55]) and 
           Chaitin (see [32]), this refined inequality is (remarkably) an equality:
               Theorem 67.
                                K(x,y) = K(x) + K(y\x,K(x)) + 0(1).
                                 4.7.   CONDITIONAL PREFIX COMPLEXITY AND COMPLEXITY OF PAIRS                                                        107
                           Proof.  In one direction the inequality is already known  (see the discussion 
                    above).  One can give also a direct argument:  to get a prefix-free description of a 
                    pair  (x,y),  it  is enough to start with the shortest prefix-free description of x and 
                    then append the prefix-free description of y with conditions x and К(x) (note that 
                    K(x) is just the length of the prefix-free description of x).  After the machine reads 
                    the first part and stops, we know both x (its output) and K(x)  (the length of the 
                    input), so we have all needed information to restore у (in a self-delimiting way).
                          Using semimeasures, we can prove the same inequality as follows.  Consider a 
                    function m! such that
                                                      m'([x,y})=                  ^2            2~km{y\x,k).
                                                                           {k\2~k' <2m(x)}
                    This function is lower semicomputable, and its sum over all x, у is finite (for each 
                    x and к the sum over all у does not exceed 1, then the sum over all к such that 
                    2~k < 2m(x) does not exceed 4m(x), and the sum over x does not exceed 4).  So we 
                    compare m' with the a priori probability and conclude that for к = — [log2 m.(x)J, 
                   we get the term that we want to estimate.
                          Note the important technical trick:  we cannot write just
                                                             m!([x,y]) = m(x)m(y\x,K(x)),
                   since  the semicomputability is  no  longer guaranteed.  To  avoid  the problem,  we 
                    extend the sum over all к ^ K(x).
                          Now let us consider the reversed inequality:
                                                      K(x) + K(y\x,K(x)) ^ K(x, y) + 0(1).
                    Let  us  start  with  a  simple  (but  incorrect!)  proof of a  stronger  (but  incorrect!) 
                   statement
                                                           K(x) + K(y\x) ^ K(x,y) + 0(\).
                   In terms of semimeasures this equality can be rewritten as
                                                                  m(x)m(y\x) ^ em([x,y])
                    (for some £ and for all x,y).  Here m stands for a priori probabilities (both condi­
                   tional and unconditional ones).  Let us rewrite this inequality as
                                                                     m(y\x) ^ £ m{[x,y})
                                                                                            m(x)
                   It is enough to show that the function
                                                                     m'(y\x) = £m([x,y])
                                                                                             m(x)
                   for any fixed x is a semimeasure (for some e); after that we can compare it with the 
                   maximal semimeasure m(y\x) and get the desired result.  We need to show that 
                   the sum of m'(y \x) over у does not exceed 1:
                                                          ^2m'{y\x) =£ T,ym([xM ^ I-
                                                            у                               m(x)
                   Indeed, the function x ^  ^ ym([x,y]) is a semimeasure (its sum over all x equals 
                    Т,х,ут([Х’У}) ^ 1), and therefoie this function is bounded by т(з:)/r foi some £.
                          What is wrong with this argument?  We have not checked that the semimea­
                   sure we constructed is lower semicomputable.  There are two cases where we need to
             108             4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
             check this.  In one of them it it is easy:  the function ^   m([x,y]) is lower semicom- 
             putable since m. is lower seniicomputable.  But in the other case, for the function 
             m([x, y])/m(x),  the  lower seniicomputable  function  m(x)  is  in  the denominator, 
             and when m(x) increases, the ratio decreases.
                 The correct proof of the weaker inequality follows the same scheme but uses 
             some additional tricks.  We have to prove that for z = K(x) the inequality
                                           m(y\x,z) ^ e m([x,y]) 
                                                             m(x)
             holds.  The problem is that the right-hand side is not lower seniicomputable.  But 
             for z = K(x) we can replace m(x) « 2~K^  by 2~z and consider the function
                                           m'(y\x,z) = m{[x,y])2z.
             This function is lower seniicomputable.  But now it is not a semimeasure:  the sum 
                 m'(y \x-. z) is bounded by 1 only if
                                              ^2m([x,y]) ^ 2'\
                                               y
             which is not true if z is large.  However, we know that
                                    ^2m([x,y}) = 0(m(x)) = 0( 2~K{x)), 
                                     y
             so there exists a constant c such that
                                     z ^ K{x)-c^ ^2tm> {y \x, z) ^ 1. 
                                                        y
             But this is not enough:  we need a family of semimeasures that satisfy this inequality 
             for all X and z (and not only for z ~ K(x), as needed for our result).  So we “trim” 
             the function m! and get another function m" such that:
                    •  function (y,x,z)    m"(y \x, z) is lower seniicomputable;
                    •  the inequality
                                              $>"(У|*,*) < 1
                                               y
                      is true for all x and z;
                    •  there exists a constant c such that
                                  z < K(x) — c => m//(y\x, z) — т'(у\х, z).
             How do we perform “trimming”?  This trick was explained in Section 4.2:  we look 
             at the increasing approximations from below and let them through only if they do 
             not violate the required bound for the sum.
                 Now,  comparing m" with the a priori probability and taking the logarithms, 
             we conclude that
                                z ^ K{x) — с => K(y I x, z) ^ K(x,y) — z + d
             for some c, c' and for all x, y, z.
                 Now we let z be equal to z — K(x) — c.  Note also that changing z by 1 changes 
             the value K(y\x, z) by at most 0(1) (increasing/decreasing the second component 
            of a pair is a computable function).  Therefore,
                                 K(y\x,K(x) -  c) = K(y\x,K(x)) + 0(1).                         □
                                                                                                                4.7.                      CONDITIONAL PREFIX COMPLEXITY AND COMPLEXITY OF PAIRS                                                                                                                                                                                                                                                                                                                                   109
                                                                                          Note that  Theorem 22  (p.  39),  which says that  C(x,y)  =  C(x) + C(y\x) + 
                                                                   O(logn) for strings of complexity at most n, can now be proved as a corollary.
                                                                                         Indeed, the replacement of К by C changes all three terms by at most O(logn). 
                                                                  It remains to note that the difference between C(y | x, K(x)) and C(y | x) is bounded 
                                                                  by O(logn).  In this way we get a new proof of Theorem 22 that replaces counting 
                                                                  by manipulations with semimeasures.
                                                                                         Recalling that m(x) ~                                                                                                                                  тп([х, у])  (up to a 0(1) factor, Problem 101, p. 97), 
                                                                  we may rewrite the statement of Theorem 67 as
                                                                                                                                                                                                                    m(y\x,K(x))                                                                                               ™([x,y])
                                                                                                                                                                                                                                                                                                                   Y,ym{[xMY
                                                                  The right-hand side of the equation can be interpreted as the conditional probability 
                                                                  of the event  “the second component of the pair equals у”  where the condition is 
                                                                    “the first component of the pair equals x”  (but one should remember that we deal 
                                                                  with semimeasures, not probability distributions).
                                                                                        I     111 I  Prove that
                                                                                                                                                                                                    K(x\z) ^ K(x\y) + K(y\z) + 0(1)
                                                                  for arbitrary strings x,y,z.  (This result can be improved; we may replace K(x\y) 
                                                                  by a smaller term K(x\y, z).)
                                                                                        I  112 I Prove the  “relativized”  version of Theorem 67:
                                                                                                                                                              К (x, у I z) — K(x I z) + K(y I x, К (x I z), z) + 0(1).
                                                                                         Using Theorem 67 twice,  we a get  a formula for the prefix complexity  of a 
                                                                  triple.  Indeed, the triple (x, г/, z) can be considered as a pair whose first component 
                                                                  is  (x, y)  and the second component is z.  Therefore,
                                                                                                                                                               K{x,y,z) = K(z\x,y,K(x,y)) + K(x,y) + 0(1).
                                                                  Using Theorem 67 once again, we get the following result:
                                                                                         Theorem 68.
                                                                                                                      K{x,y,z) = K(z\x,y,K(x,y)) + K(y\x,K(x)) + K(x) + 0(1).
                                                                                        We can change the order of transformations (using the z-relativized version of 
                                                                 Theorem 67) at the second step:
                                                                  K{x,y,z) = K(y,z\x,K(x)) + K(x)
                                                                                                                                                                                                            = K(z\y,K(y\x,K(x)),x,K(x)) + K(y\x,K(x)) + K(x)
                                                                  (we omit the 0(l)-terms for brevity).
                                                                                        It is interesting that this leads to a slightly different version of Theorem 68:  the 
                                                                 two last terms are the same but the first term is different.  We still have the con­
                                                                 ditional complexity of z but now we have two conditions K(x) and K(y\x,K(x)) 
                                                                  instead of K(x,y).  Note that the sum of the complexities in the condition is ex­
                                                                  actly K(x,y) according to Theorem 67.  Therefore, the pair of complexities has no 
                                                                  less information than K(x,y).  In fact the reverse is also true (when x and у are 
                                                                 conditions).  Indeed, let z be the pair (K(x), K(y\x, K(x))); in the second formula 
                                                                 the first term is zero (i.e., 0(1)).  So we get the following corollary:
                             4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
             no
                 Theorem 69.
                                            K(K(x)\x,y,K(x,y)) = 0(l), 
                                   K(K(y IX, K(x)) IX. y,K(x,y)) = 0( 1). 
                 (Of course the same is true for K(y) and K(x\y, K(y)).) 
                  113 Give a direct proof of Theorem 69.
                 (Hint:  Knowing x, y and K(x,y), we may look for an upper bound d for K(x) 
             such  that  K(y\x,d) + d  becomes equal  to  K(x,y).  The  coincidence  (up  0(1)) 
             implies that d — K(x) + 0(1); indeed, if d — К(x) + m for some m, the complexity 
             K(y\x,d)  can  decrease  (because  of this  m)  at  most  by  O(logm),  and  the  sum 
             becomes bigger.)
                 Using Theorem 67 we can easily show that the basic inequality of Theorem 24 
             (p.  47)  is true with 0(Imprecision for prefix complexity (recall that we have loga­
             rithmic error term for plain complexity):
                 Theorem 70.
                                K(x, y,z) + K(x) ^ K(x,y) + K(x, z) + 0(1) 
             for arbitrary strings x,y,z.
                 Proof.  Indeed, the right-hand side can be rewritten as
                               K{x) + K(y I x, K(x)) + K(x) + K{z I x, K{x)), 
             and the left-hand side equals
                                       K{x) + K(y, z\x, K(x)) + K(x).
             It remains to prove that
                              K(y,z\x,K(x)) ^ K(y\x,K(x)) + K(z\x,K(x)), 
             and this inequality is a relativized version of Theorem 60 (p. 97).                 □
                 Let us provide also a direct proof of Theorem 70 using semimeasures.  We have 
             to show that (up to O(l)-factors)
                                      m(x, y, z)m(x) ^ m(x, y)m(x, z),
             where m is the maximal lower semicomputable semimeasure.  Dividing by m(x), 
             we get an inequality
                                         m(x, y)m(x, z < m(x,y,z).
                                              m(x)
             Let  us  check that  the left-hand  side  of this  inequality has  a finite sum  (over  all 
             triples x, y, z).  Indeed,
                                              m(x,y)m(x,z)
                                         E--------г--------  ^ m(x)
                                                  mix)
             (since J2ym(x^y) ^ тп(х) and J2zm(x^z) ^ тп(х)\ we omit the 0(1) factors).
                 This is not enough:  since we have m(x) in the denominator, the fraction
                                                m(x, y)m(x, z) 
                                                     m(x)
             is not (necessarily) lower semicomputable, and we cannot use the maximality prop­
             erty.  So we need to use the following trick (similar to the trick used in the proof of 
             Theorem 67) to construct a lower semicomputable upper bound for this fraction.
                    4.7.  CONDITIONAL PREFIX  COMPLEXITY  AND  COMPLEXITY  OF  PAIRS      111
                For  each  n  consider  the  function  mn(x,y)  which  is  obtained  from  m(x,y) 
            by 2-n-trimming:  the sum  ^ ym(x,7/)  is  forced  to  be  at  most  2~n.  Note that 
            Ylym(,x^y)  — тп(х)  (up  to  O(l)-factors),  so  mn(x,y)  =  m(x,y)  for  n  =  K(x). 
            Then we consider the function
                                   (x,y,z)^  ^2     mn(x,y)mn(x,z)
                                             n^K(x)        2~n
            It is an upper bound since it contains the term with n = K(x).  On the other hand,
                              mn(x, y)mn(x, z)  ^             ^ ymn(x,y)Y2zmn(xiz)
                  E  E              2-7                       _V        2~7
                  x,y,z  n^K(x)                     x  n^K(x)
                                                « Е Е   2'"«E  2m(x) ^ 2.
                                                    x  n^K(x)        X
            (As before, we omit the 0(1) terms and factors; they lead only to the 0(1) factor 
            in the final inequality.)
                 114 Show that the inequality of Theorem 26  (p. 48)  is true for prefix com­
            plexity with О(Imprecision:
                            2K(x,y,z) ^ K(x,y) + K(x,z) + K(y,z) + 0(1)
            for all strings x,y,z.
                (Hint:  Add the basic inequality
                                  K(x,y,z) + K(z) ^ K(x, z) + K(y,z) 
            to the inequality K(x, y, z) < K(x, у) + К(z).)
                1115 I Prove that there exists c such that for every string x and for every positive 
            integer n there exists a string у of length n such that
                                        K(x, y) ^ K(x) + n — c.
                (Hint: For every z and n there exists a string у of length n such that K(y \ z) > n.)
                A similar statement can be formulated for n-bit extensions of a given string x 
            (its version for plain complexity makes Problem 46 on p. 42).
                Theorem 71.
                              max{K(xy) \ l(y) = n} ^ K(x\n) + n — 0(1).
                In other words, for some c and all x and n we can append n bits to x in such a 
            way that the complexity becomes at least K(x\n)+n — 0(1) (this is not exactly the 
            increase in the complexity since we compare K(xy) with K(x\n) and not K(x)).
                PROOF.  In terms of a priori probabilities, this inequality says that
                               2n min{m(xy) \ l(y) = n} ^ m{x \ n) ■ 0(1).
            The  left-hand  side  does  not  exceed  ^~2{m(xy)  \  l(y)  =  n}  (the  sum  may  only 
            decrease if we replace all summands by the least one).  But the latter sum is (as a 
            function of x and n) a lower semicomputable semimeasure, so it remains for us to 
            compare it with the maximal semimeasure m{x\n).                               □
                I  116 I Show that a bit weaker statement with K(x) — K(n) instead of K(x \ n) 
            (in the right-hand side) can be derived from the statement of Problem 115.
              1 1 2             4.  A  PRIORI  PROBABILITY  AND  PREFIX  COMPLEXITY
                  4.7.4.        Plain  and  prefix  complexities  revisited.  We  have  already  seen 
              some  bounds  for  prefix  complexity  in  terms  of plain  complexity  (Theorem  65). 
              There are many other relations between them.  For example, the following observa­
              tion (made by Levin) shows that plain complexity can be defined in terms of prefix 
              (conditional) complexity.
                  Theorem 72.  Plain complexity C(x) can be defined as the value of i such that 
              K{x\i)  = i.  More precisely,  K(x\C(x))  =  C(x) + 0(1);  on the other hand,  if 
              K(x\i) = i + 5, then C(x) = i + 0(S).
                  Proof.  We already noted (see Problem 44 on p. 40), that C(x) < C(x\C(x)) 
              with 0(l)-precision.  On the other hand, K(x\C(x)) < C(x) with the same preci­
              sion.  Indeed, the minimal (plain) description for x can be considered as a prefix-free 
              one if its length is given as a condition.  So the first statement is proven.
                  It remains to note that K(x\i) changes slowly (as i changes):  changing i by d, 
              we change this complexity by O(logd).  So the equation K(x\i) = i has a unique 
              (up to 0(1)) solution; when i deviates by some p from this solution, the difference 
              between i and К(x \ i) is proportional to p.                                               □
                  As noted recently by B. Bauwens, this statement can be used to relate plain and 
              prefix complexity.  Let us start with a special case of a formula for the complexity 
              of a pair:
                            K(x) = K(x, K(x)) = K(K(x)) + K{x I K(x), K(K(x))).
              This is true with 0(l)-precision.  If we ignore terms of order 0(K(K(K(x)))), the 
              pair  (K(x), К(K(x)))  in the condition can be replaced by K(x) — K(K(x)), and 
             this replacement gives us
                            K(x) -  K(K(x)) = K(x\K(x) -  K(K(x))) + 0(K{3\x))
              (where K^(x) stands for the ith iteration of К).  It remains for us to apply the 
             previous theorem, and we get the following result by R. Solovay [188]:
                  Theorem 73.
                                      C(x) = K(x) - K(K(x)) + 0(К^(х)).
                  This result can be rewritten as
              (*)                     K(x) - C(x) = K(K(x)) + 0(K{3\x)).
             Solovay noted also that we can replace К by C in the right-hand side of (*), i.e., 
             that
                                      K(x) - C(x) = C{C{x)) + 0(C(3)(x)).
             In fact,  0(K(3\x))  and  0(C^3\x))  denote the same precision,  and the equality 
             K(K(x)) — C(C(x)) holds with this precision.
                  Let us explain why.  First of all, the already proved formula (*) for K(x) — C(x) 
             implies that
                                IK(K(x)) - K(C(x))\ < K(3\x) + 0(logR(3\x))
             and
                                IC(K(x)) - C(C(x))\ < K^(x) + 0(logK^3\x))
                    4.7.  CONDITIONAL  PREFIX  COMPLEXITY  AND  COMPLEXITY  OF PAIRS     113
            since the difference between complexities of two numbers is bounded by the prefix 
            complexity of the difference between numbers.  On the other hand,  we can write 
            the formula for K(x) — C(x) for K(x) in place of x; in this way we get the equation
                                   K{K{x)) - C(K(x)) = 0{K^(x)).
            So all four versions of complexity of x (combinations of plain and prefix complexity) 
            differ at most by 0(K^3\x)).  In particular,
                                   K(K(x)) - C(C(x)) = 0(K{3\x)).
            It  remains to  show that  0{K^3\x))  and 0(C^3\x))  is the same precision.  Note 
            that \u — v\ = 0(K(u)) implies |K(u) — K(v)\ — 0(\ogK(u)), so K(K(K(x))) and 
            K(C(C(x))) are “logarithmically close”  (we say that a is logarithmically close to b 
            if \a — b\ — O(loga)).  This  “closeness”  relation is symmetric and transitive (if we 
            allow the constant to increase in О-notation).  Now note that C(v)  and K(v)  are 
            logarithmically close for every v, in particular, for v = C(C(x)), and the transitivity 
            shows that K(K(K(x))) and C(C(C(x))) are also logarithmically close, so indeed 
            0(K(3\x)) and 0(C^(x)) is the same precision.
                In this way we obtained another result of Solovay from [188]:
                Theorem 74.
                                 K(x) = C(x) + C(C(x)) + 0(C(3)(x)).
                In other words, the inequality from Theorem 65 (p. 102) is almost an equality.
                 117 Give a direct proof of the inequality
                               C(x) < K(x) - K(K(x)) + K(3)(x) + 0(1)
            by estimating the number of x that make the right-hand side of the inequality small.
                (Hint:  We have seen in Theorem 64 (p. 101) that the logarithm of the number 
            of strings  such that  K(x)  <  n  is  bounded by n — K(n) + c for some  c and  for 
            all n.  Given n, we can enumerate these strings, and each string x of this type can 
            be reconstructed from n and the ordinal number of x in this enumeration.  This 
            ordinal number can be represented by a string и of length exactly n — K(n) + c 
            (add leading zeros to its binary representation).  Knowing this representation, we 
            know n — K(n) (the constant c is fixed), and to reconstruct n it is enough to encode 
            K(n)  by a self-delimiting description of length  K(K(n)).  Now concatenate this 
            self-delimiting description  and the string и:  we get  a  (plain)  description of x of 
            length  K(K(n)) + n — K(n) + c.  This can be done for arbitrary string x with 
            K(x) < n, in particular for all strings of prefix complexity exactly n.)
                               CHAPTER 5
                        Monotone complexity
             5.1.  Probabilistic machines and semimeasures on the tree
           Chapter 4 defines a priori probability by using probabilistic algorithms  (ma­
        chines)  that may print some number as their output and then terminate.  In this 
        chapter we consider another type of probabilistic (=randomized) algorithms.  These 
        algorithms output a binary sequence bit by bit and do not necessarily terminate. 
        The output,  therefore,  is  a  random variable whose  values  are  finite  and  infinite 
        sequences of bits  (i.e.,  elements of the set  E  of all finite and infinite sequences of 
        bits).
           Consider the following simple algorithm of this type.  It just sends random bits 
        directly to the output:
                while true do 
                  6:=random;
                  OutputBit(è);
                od
        Its output therefore is a random variable that is uniformly distributed over £1, the 
        set of all infinite binary sequences.
           But  it  is  quite  possible  (for some other algorithm)  that  some finite sequence 
        is  printed  with positive probability.  This happens when  algorithm with positive 
        probability stops  after sending some bits  to  the output  (or runs forever without 
        sending more bits to the output).
           For each algorithm A of the described type we consider a function a that is 
        defined on binary strings and whose values are non-negative reals:
                      a(x) = Pr[the output of A starts with x].
           More formally this function is defined in the following way.  Each probabilistic 
        algorithm defines a mapping A of the set Q (infinite sequences of zeros and ones) 
        into the set E.  Namely, A(co) is a sequence of output bits that appears if we use the 
        terms of the sequence u) as random bits (this means that each statement b := random 
        assigns to b the first unused bit of cj).  For example, if A is the program mentioned 
        above, then Ä(cj) = ш for all w.
           Then a{x) is defined as the measure of the preimage of the set Еж under the 
        mapping À (where Ex is the set of all finite and infinite sequences having prefix x). 
        We say that A generates the distribution a.
           1118 I What are À and a,  if the algorithm A outputs an infinite sequence of 
        zeros (not using random bits at all)?
           A natural question arises:  what is the class of all functions a that correspond 
        to randomized algorithms A of the described type? The next two theorems provide
                                  115
                                          5.  MONOTONE COMPLEXITY
             116
             the  answer  (given  already  in  one  of the  first  papers  on  algorithmic  information 
             theory,  [225]):
                 Theorem 75.  Let A  be a randomized algorithm of the described type,  and let 
             a  be the corresponding function.  Then
                 (a)  a(x) > 0 for all x\
                 (b)  a(A) = 1  (here A is the empty string)-,
                 (c)  a(x) ^ a(xO) + a(x 1) for every string x\
                 (d)  the function a is lower semicomputable.
                 The notion of the lower semicomputable (enumerable from below) sequence of 
             reals was defined in Section 4.1 (p. 75).  For the functions on strings the definition 
             is quite similar:  we require that a(x) = limia(x,i)  where a(x,i)  is a computable 
             function of two arguments x and i, defined for all strings x and for all non-negative 
             values of i; the values a(x,i) are rational numbers or — oo, and for every fixed x the 
             value a(x, i) increases as i increases.
                 PROOF.  The first three claims are obvious:
                 (a)  Probability is always non-negative.
                 (b) a (A) = 1 since the empty string is a prefix of any output.
                 (c) a(x) ^ a(xO) + a(x 1), since the events “the output starts with xO” and “the 
             output starts with xl” are disjoint subsets of the event  “the output starts with x”.
                 Note that inequality (c) can be strict; the difference a(x) — a(xO) — a(xl) is the 
             probability of the event  “the output is exactly the string x”  (no bits appear after 
             it).
                 (d) To prove that a is lower semicomputable, we need to construct approxima­
             tions from below for a(x) for any given string x.  Let us simulate the behavior of A 
             for all possible values of random bits.  During this simulation we discover values of 
             random bits that guarantee that output starts with x, i.e., we find some intervals I 
             in Q, such that Ä(co) starts with x for all со £ I.  The probability a(x) is the measure 
             of the union of all these intervals, and the approximation a(x,i) is the measure of 
             the union of all the intervals discovered up to the step i of the simulation.       □
                 A function a that is defined on all binary strings,  that takes real values and 
             satisfies  the  conditions  (a)-(d)  of Theorem  75,  is  called  a  lower semicomputable 
             semimeasure on the  binary tree.  It  is important not to mix semimeasures on the 
             binary tree  and  discrete  semimeasures  defined  in  Chapter 4 that  were  functions 
             on  natural  numbers  (or  on  binary  strings  that  correspond  to  natural  numbers) 
             and correspond to probabilistic algorithms that print some number (or string) and 
             terminate.  So we use the name  continuous semimeasures or semimeasures on the 
             binary tree  for functions that satisfy conditions  (a)-(c);  the condition  (d)  selects 
             among them the lower semicomputable continuous semimeasures.
                 1119 I  Show  that  continuous  semimeasures  (functions  that  satisfy  conditions 
             (a)-(c)) are in one-to-one correspondence with measures on the set E of all finite 
             and infinite binary sequences.  Given a semimeasure a, find the measure of the set 
             that consists of all infinite sequences that have prefix x.
                 (Answer:  The measure of this set is the limit of the (decreasing) sequence
                         <Tn —      a(y)\y is a string of length n that has prefix x}.
             Here an is defined for n ^ l(x) and equals a(x) if n = l(x).)
                              5.1.  PROBABILISTIC  MACHINES  AND SEMIMEASURES  ON THE TREE                                      117
                        120  Show that for a semicomputable tree semimeasure the sum                                    a(x) can 
                 be infinite.
                       (Hint:  Consider the algorithm that copies random bits to output.)
                       The converse of Theorem 75 is also true:
                       Theorem 76.  Every lower semicomputable continuous semimeasure is gener­
                 ated by some probabilistic algorithm.
                       Proof.  The idea of the proof can be easily explained in terms of space alloca­
                 tion,  as was done for Theorem 46 (p. 78).  The difference is that now the requests 
                 are hierarchical.  Two big organizations (called 0 and 1) need space in Q (which we 
                 identify with [0,1]); the subsets allocated for 0 and 1 should be disjoint, and their 
                 space requests increase over time (but never become greater than 1 in total).
                       Each of the organizations has two divisions  (called 00,01  inside 0 and  10,11 
                 inside 1) that request some space inside the regions allocated to their organization as 
                 a whole.  Their requests also increase over time, but never become greater (in total) 
                 than the organization’s request (at the same time).  Then we consider subdivisions 
                 (say,  01  has  subdivision  010  and  Oil)  that  have  increasing  requests  that  do  not 
                 exceed (together) the request of their parent division, and so on.
                      For  each  subdivision  x  (at  any  level)  we  have  increasing  requests.  All  the 
                 allocations are final, i.e., the space allocated to some x remains allocated to x.
                      This scheme is used in the proof as follows:  Having a lower semicomputable 
                 semimeasure a, we construct a family of requests such that the limit of the requests 
                 for subdivision x is equal to a(x).  Then we choose a way to satisfy all the requests, 
                 and then we say that if a sequence of random bits gets into the region allocated to 
                 x, then the output of randomized algorithms starts with x.
                      It is more or less obvious that the requests can indeed be fulfilled.  The reader 
                 may skip the rest  of the proof,  where we provide a more formal argument  (and 
                 explain the intuitive meaning of its steps).
                      Lemma 1.  Let a  be  a lower semicomputable semimeasure on the  binary tree. 
                 Then there exists a total computable monotone  (in the second argument) function 
                 (x, i)     a(x, i)  whose values are non-negative rational numbers with denominators 
                 being powers of two, such that
                       (1)  limia(x,i) — a(x) for every string x;
                      (2)  for each i  the function x i->  a(x,i)  is  a continuous semimeasure that has 
                 only finitely many non-zero values.
                      In other words, the memory manager can impose the following additional re­
                 strictions:
                          •  all the requests are dyadic-rational numbers;
                          •  at each step only finitely many subdivisions have non-zero requests;
                          •  at each step requests are coherent  (the request of any subdivision should 
                             be greater than or equal to the sum of requests of its children).
                      PROOF.  Our goal is to change the function a from the definition of lower semi­
                computable semimeasure (but not change the semimeasure itself) so that it satisfies 
                the requirements of Lemma 1.  First, we make all values dyadic rationale.  To achieve 
                this,  we replace a(x,i)  by the maximal rational number with denominator 2l not 
                exceeding a(x,i)  (negative numbers are replaced by zeros).
                         5.  MONOTONE COMPLEXITY
        118
          Then we fulfill the second requirement and let a(x,i) be zeros for all strings x 
        whose length exceeds i.
          To fulfill the third requirement, we perform the replacement
                     a(x, i) := max (a(x, i), a(x0, i) + a(x 1, г))
        iteratively starting from long strings  and then decreasing the length of x.  Since 
        a(x) is by definition a semimeasure, these replacements do not violate the inequality 
        a(x, i) ^ a(x).
          It is easy to check that our corrections do not change the limit values lim* a(x, i) 
        (for all x), so this limit is still equal to a(x).
          Lemma 1 is proven.
          To formulate the next lemma we need several auxiliary definitions.  A simple 
        semimeasure  (on  the  binary  tree)  is  a semimeasure  that  has  only  finitely  many 
        non-zero values, and all these values are dyadic rationale.
          A simple set is the union of a finite number of intervals in Q.  (Recall that an 
        interval in Q is a set of the form flz  that consists of all infinite sequences having 
        prefix 2.  Therefore, a set is simple if we need to know only a finite prefix of ш to 
        decide whether u> belongs to this set.)
          A simple family is a family of simple sets Ax  (for all binary strings x)  such 
        that only finitely many sets among Ax are non-empty and for each string x the sets 
        Axо and Axi are disjoint subsets of Ax.  For such a family the function x ha p(Ax), 
        where p stands for the uniform measure on Cl, is a simple semimeasure.  We say 
        that the family Ax implements this semimeasure.
          Lemma 2.  Each simple semimeasure can be implemented by a simple family.
          Proof.  We construct this family starting from the empty string x and then 
        gradually increase the length of the index string x.  At each step our goal is to find 
        two disjoint simple sets Axо and Axl inside the set Ax that is already constructed. 
        This is possible since the required measures do not exceed (in total) the measure 
        of Ax.  Lemma 2 is proven.
          Lemma  3.  Let b(x)  be  a simple  semimeasure,  and let Bx  be  a simple family 
        of intervals  that  implements  b.  Let  c  be  another  simple  semimeasure  such  that 
        c(x)  ^  b(x)  for all x.  Then we  can  construct  a simple family Cx  implementing c 
        such that Cx D Bx for all x.
          Proof.  Let us repeat the argument  used to prove Lemma 2.  Now we have 
        two disjoint simple subsets of a simple set, and we need to increase their measures 
        (keeping them disjoint).  It  is easy to see that this is indeed possible if the space 
        restrictions are not violated.  Lemma 3 is proved.
          The proofs of Lemmas 2 and 3 are effective in a natural sense:  both simple 
        semimeasures and simple families are finite objects, and there is an algorithm that 
        constructs the simple family given the simple semimeasure(s).
          Now we apply Lemma 3 iteratively to the simple semimeasures provided by 
        Lemma 1.  In this way we get a two-parametric family of simple sets U(x,i) such 
        that
            •  the description of U(x,i) (i.e., the list of intervals) is a computable func­
              tion of x and г;
            •  the  uniform measure of the set  U(x,i)  is equal to a(x,i)  (and therefore 
              tends to a(x) as i -A oo);
                     5.1.  PROBABILISTIC  MACHINES  AND  SEMIMEASURES  ON  THE TREE       119
                   •  for each x and i the sets U(xO,i) and U(xl,i) are disjoint subsets of the 
                     set U(x,i);
                   •  U(x,i) C U(x,i + 1) for each x and i.
                Now the probabilistic algorithm that generates the semimeasure a can be con­
            structed  as  follows:  we  construct  the sets  U(x,i)  for  all  x  and  i  and  in  parallel 
            generate random bits obtaining a sequence со.  If at  some step we discover that 
            со E U(x, i) for some x and г, we output those bits of the string x that have not yet 
            been printed.
                Note that if a;  €  U(x.i),  then со  E  U(y,i)  for every prefix y  of x.  Note also 
            that со  cannot  be an element of both U(x,i)  and  U(x',i)  if strings x  and x'  are 
            incompatible  (neither of them is the prefix of the other one).  Therefore the bits 
            sent to the output never need to be “recalled”.
                An output of this algorithm starts with some string x if and only if the sequence 
            со of random bits belongs to the union of the increasing sequence of sets U(x, i) (for 
            г  =  0,1, 2,...).  The probability of this event is the limit  of measures of the sets 
            U(x,i),  and  this  limit  is  by  construction  equal to  a(x),  so  we have achieved  our 
            goal.                                                                         □
                Theorems  75  and  76  show  that  lower  semicomputable  semimeasures  can  be 
            equivalently  defined  as  probability  distributions  generated  by  randomized  algo­
            rithms (of the described class).
                There is an important special case when a randomized algorithm almost surely 
            generates an infinite sequence  (i.e.,  the probability of getting a finite sequence is 
            zero).  Such algorithms generate computable measures,  as the following theorem 
            shows.
                Theorem 77.  (a) Let у be a computable measure onfl.  Then function p defined 
            on the  Cantor space as p(x) —          a lower semicomputable semimeasure and
            p(x) — p(x0) +p(x 1) for all x.
                (b)  If  a  lower  semicomputable  semimeasure p  satisfies  the  equality p{x)  = 
            p{x0) + p(x 1) for all x,  then it determines some computable measure on £7.
                Proof,  (a) If a real number a is computable and an is a rational approximation 
            to a with accuracy 1/n, then bn — an — 1/n is a lower bound for a that is at most 2/n 
            apart from a.  The sequence bn constructed in this way can violate the monotonicity 
            requirement but we may replace it by the sequence cn = max(&o, 6i,  ,bn) and get 
            a non-decreasing sequence of rational numbers converging to a.  Therefore, every 
            computable real number is lower semicomputable.  Doing this in parallel for all x, 
            we obtain computable rational lower bounds for p(x) tending to p(x), and we prove 
            that every computable measure is a lower semicomputable semimeasure.  Since 
                                                                       p(x) = p{x0) +p{x 1).
            is the union of two disjoint subsets  and      we also have 
                (b)  Assume that p is a lower semicomputable semimeasure that satisfies our 
            condition, i.e., p(x) — p(x0) +p(x 1) for all x.  We show inductively how p(x) can be 
            computed with arbitrary precision for every x.  For empty x we have p(A) = 1 by 
            definition.  Imagine that we already know how to find p(x) with arbitrary precision 
            for some string x.  How can we do the same for p(x0) and p(x 1)?  We have to wait 
            until the sum of (increasing) lower bounds for p(x0) and p(x 1) becomes close enough 
            to the (decreasing) upper bound for p(x).  In other words, an upper bound for p(x 1) 
            can be obtained if we take an upper bound for p(x)  (constructed recursively) and 
            subtract a lower bound for p{x0), and vice versa.                             □
              120                              5.  MONOTONE COMPLEXITY
                   This theorem can be interpreted in the following way.  Assume that we need 
              a generator of random reals  (^sequences of zeros and ones)  whose output has a 
              prescribed  distribution p  (this  means  that  the  probability  of getting  an  output 
              that  starts  with  x  is  equal  to p(x)).  Then  Theorems  76  and  77 guarantee  that 
              if p is  a computable distribution, then such a generator can be implemented as a 
              randomized algorithm that uses the internal source of random bits that has uniform 
              distribution.
                   The construction used  in  the  proof of Theorem  76  can  be  applied  to  every 
              lower semicomputable continuous semimeasure;  in the special case when we deal 
              with computable measures, there is a much simpler approach.  Let us divide the 
              interval [0,1] into two parts of lengths p(0) and p( 1).  The first part is then divided 
              again into parts of length p(00) and p(01), the second one is divided into parts of 
              length p(10) and p(ll), and so on.  In this way for each string 2 we get an interval 
              7Tz  inside [0, 1], and the intervals ttz  for all strings 2  of any given length cover [0,1] 
              without overlaps.
                   Now construct the probabilistic algorithm as follows.  This algorithm uses in­
              dependent tosses of a fair coin to get a sequence a of random bits that has uniform 
              distribution.  This sequence is considered as a binary representation of some real in 
              [0,1];  this real is also denoted by a.  In parallel the probabilistic algorithms looks 
              for  binary  strings  2  such  that  the  real  number  a  lies  strictly  inside  the  interval 
              7Гz  (and this  is  guaranteed by the available information about  a  and the current 
              approximations to the endpoints of 7rz;  these approximations are computed with 
              increasing precision).
                   The  strings  2  discovered  in  this  way  are  compatible  (one  being  a  prefix  of 
              another).  The more bits of a we know,  the longer 2 can be.  These strings are 
              prefixes of some bit sequence that is the output of our randomized algorithm.
                   The algorithm described can output a finite sequence.  This happens if a coin­
              cides with an endpoint of some ttz.  However, there are countably many endpoints, 
              so this event has probability 0.  Note also that the output of the algorithm starts 
              with 2  if and only if a belongs to the (open)  interval ttz,  so  the probabilities are 
              correct.
                   More formally, we have described a transformation T of the input bit sequence 
              a into the output bit sequence ß = T(a) such that the image of uniform measure 
              under T is the measure p.
                   (This trick is well known.  For example, imagine that you have a fair coin and 
              you need to simulate the coin that has probabilities 2/3 and 1/3.  Then you generate 
              a random real uniformly distributed in [0,1] (by fair coin tossing) and compare this 
              real number with threshold 2/3.  To simulate the second coin tossing, you divide 
              both intervals  [0,2/3]  and  [2/3,1]  in the  same proportion  2  :  1.  The  algorithm 
              described earlier does exactly this.)
                   Theorem 76 shows that it is enough to have a physical generator of indepen­
              dent symmetric random bits  (a fair coin)  to emulate arbitrary other computable 
              probability  distribution  and  even  arbitrary  continuous  semimeasure.  In  fact,  a 
              “computably biased”  coin could work as well, as the following problem shows.
                   11211 Show that  in Theorem 75 one can replace uniform distribution by an 
              arbitrary computable distribution and even an arbitrary semimeasure.
                   (Hint:  The composition of two algorithmic transformations is an algorithmic 
              transformation itself.)
                                    5.2.  MAXIMAL  SEMIMEASURE  ON THE  BINARY  TREE                               121
                      122  Show that in Theorem 76 one can replace uniform distribution by arbi­
               trary computable distribution that does not have atoms,  i.e.,  every singleton has 
               measure 0.
                     (Hint:  A computable measure P can be transformed into the uniform one as 
               follows:  as we get 2 from a P-generator, we output a string x such that the segment 
               7Г% is entirely in the open interval Ix (of numbers whose binary representation starts 
               with x).)
                    It  is  important  here that the measure does not have atoms:  if uj has positive 
               measure, then the value Ä(uj) has positive probability and we cannot get a uniform 
               output distribution (that does not have atoms).  But, as we have seen, this is the 
               only obstacle.
                    More difficult problem arises if we do not know exactly the distribution of our 
               (“low-quality”) source of random bits.  Can we still generate some distribution that 
               is at least close to the uniform one?  This question can be formalized in terms of the 
               randomness extractors — both in combinatorial terms and in terms of Kolmogorov 
               complexity.  See the survey by A. Wigderson [219] and the references in this survey 
               for the combinatorial setting,  and the survey of M.  Zimand  [224];  we do not go 
               into this direction in our book.
                                  5.2.  Maximal semimeasure on the binary tree
                    Theorem 78.  The class of all lower semicomputable semimeasures on the bi­
               nary tree has the greatest element (up to a constant factor):  there eoästs a semimea­
                                                                                                                       
               sure a  in  this  class  such  that for  every  other a'  in  the  same  class  the  inequality
               a'(x) ^ ca(x)  holds for some constant c and for all x.
                    PROOF.  We can use the same idea as for semimeasures on N  (Theorem 47, 
               p.  80).   Consider  a  probabilistic  machine  A  that  first  chooses  at  random  some 
               probabilistic machine and then simulates it.  If a semimeasure a' corresponds to a 
               probabilistic  machine A',  then a'(x)  <  (1 /e)a(x)  where e  is the probability that 
               machine A' is chosen.                                                                                □
                    Another proof deals with functions, not machines:  first we construct a sequence 
               ao, a i,... of semimeasures and then consider the function a =                    A^a* where A* are 
               computable coefficients that have sum 1  (e.g., A* =
                    A delicate point:  we need a sequence that includes all (tree) semimeasures that 
               are  computable  from below,  and  the sequence  itself should  be computable  from 
               below.  This means that we need a lower semicomputable function (i,x) h* u(i,x) 
               such that  (1)  for any fixed i the function щ : x h-> u(i,x)  is a tree semimeasure; 
               (2) the sequence щ contains all lower semicomputable tree semimeasures.
                    This can be done either by enumerating all probabilistic machines (and that cor­
               responds to the first proof) or by enumerating all lower semicomputable functions 
               and then trimming them to make them semimeasures and leaving them unchanged 
               if they already are semimeasures.  See the similar argument for semimeasures on N 
               (Section 4.2, p. 79).  In this process, if the condition p(x) ^ p(x0)+p(xl) is violated, 
               we should increase p(x) and so on, unless in the end this makes p(A) greater than 1.
                     123 Provide the missing details in this argument.
                    Remark.  The first proof of Theorem 78 gives a bit more than we have claimed. 
               Indeed,  in  this  proof we  obtain  the  lower  bound  not  only  for  the  probability  of
              122                            5.  MONOTONE COMPLEXITY
              the event  “output  starts  with  x”,  which  is p(x),  but  also  a lower  bound for the 
              probability of the event “the output is exactly x”, which is p(x) —p(x0) —p{x 1).  So 
              not only a(x), but also a(x) — a(x0) — a(xl) is maximal for the universal machine 
              we constructed.
                   124 Prove that all these arguments can be applied to the case of algorithms 
              that send natural numbers (not bits) to the output one at a time.  These algorithms 
              correspond  to  lower  semicomputable  semimeasures  on  the  set  of all  (finite  and 
              infinite) sequences of natural numbers.
                   125  (Continued) Let m be the maximal lower semicomputable semimeasure 
              on the set of all  finite and  infinite sequences of natural  numbers.  Show that  its 
              restriction on the sequences of length  1  coincides  (up to an 0 (1) factor) with the 
              discrete a priori probability on natural numbers (Chapter 4), and its restriction to 
              binary sequences coincides (up to an 0 (1) factor) with the maximal semimeasure 
             provided by Theorem 78.
                   126 Show that a(Onl)  and m(n) differ at  most by an 0 (1)  factor in both 
             directions, where a is the maximal continuous semimeasure from Theorem 78, and 
             m(n) is the a priori probability of integer n as defined in Chapter 4.  (Instead of 
             Onl, one can use arbitrary prefix-free encoding of integers.)
                  (Hint:  See Theorem 79 below.)
                  Let us fix some maximal lower semicomputable semimeasure on the binary tree 
             and denote it by a(x).  It is known as the universal continuous semimeasure.  One 
             can call a(x)  the  continuous  a priori probability  of x,  to  distinguish  it  from the 
              discrete a priori probability defined in Chapter 4.  However, the expression
                                                KA (x) = — log a(x)
             can be called the  a priori  complexity of a string x with no risk of confusion:  the 
             minus logarithm of the discrete a priori probability (Chapter 4) coincides with the 
             prefix complexity and therefore does not  require a special name.  Since different 
             maximal semimeasures differ at most by an 0 (1) factor, the a priori complexity is 
             defined up to an additive 0 (1) term.
                  There is no universally accepted notation for the a priori complexity:  sometimes 
             it is denoted by KM(x).  We use К A (x) for the a priori complexity, reserving KM(x) 
             for monotone complexity as defined later in this chapter.  (When KM(x) is used for 
             the a priori complexity, the monotone complexity is usually denoted by Km(x) or 
             Km(x).)
                  In  the  next  section  we  study  the  properties  of the  a priori  complexity.  Let 
             us note that by definition the a priori complexity need not be an integer (or even 
             rational)  number.  But  this does not  matter much,  since most  of the statements 
             about complexity are true  “up to an 0 (1)  term”,  and we may replace — loga(x) 
             by a minimal integer n such that 2~n < a(x).  An important detail:  we use the 
             strict inequality since we want the resulting function to be lower semicomputable. 
             In the sequel we indicate the rare cases where this rounding (or its absence) can be 
             important.
                               5.3.  A priori complexity and its properties
                  Theorem 79.  (a)  The a priori complexity is monotone:  if x  is a prefix of y, 
             then KA (x) ^ KA (y).
                                                 5.3.   A  PRIORI  COMPLEXITY AND ITS PROPERTIES                                                      123
                           (b)  KA (X) < l(x) + 0(1) for each x.
                           (c)  KA (x) < K(x) + 0(1) for each x.
                           (d) Let £o,£i, • • •  be a computable sequence of incompatible binary strings  (i.e., 
                    none of them is a prefix of another one).  Then
                                                      KA (Xi) = K{Xi) + 0(1) = K(i) + 0(1).
                           (e)  K(x) ^ KA (x) + 2 logl(x) + 0(1).
                           (f)  Moreover, K(x) ^ KA (x) + K(l(x)) + 0(1).
                           (g)  and even more, K(x\l(x)) < KA (x) + 0(1).
                           (h)  A  sequence  of zeros  and  ones  is  computable  if and  only  if the  a priori 
                    complexity of its prefixes is bounded.
                           (i)  If f  : £ —> Nj_  is  a computable continuous mapping,  then
                                                                  K(f(x))^KA(x) + 0( 1) 
                   for each string x such that f(x)  is defined  (is not equal to _L).
                          Proof,  (a) The measure of a subset of a set does not exceed the measure of 
                    the set itself.
                           (b) The function p(x) = 2 ~1^  is a lower semicomputable semimeasure.  There­
                    fore p(x) ^ ca(x) for some c and all x.
                          (c)  The  machines  that  output  a  binary  string  (as  a  whole)  and  then  halt, 
                    form a subclass of the machines that generate output bits one by one.  Therefore, 
                    m(x) ^ ca(x) where m is the discrete a priori probability (as defined in Chapter 4).
                          It  is  instructive to rephrase this argument using semimeasures.  Let m'(x) be 
                    the sum of m(y) taken over all strings y that have prefix x (including x itself).  Here 
                    m is the discrete a priori probability.  Modify m! , and let m'(A) be equal to 1.  Then 
                   m! is a semimeasure on the binary tree and therefore m(x) < m'(x) = 0 (a(x)).
                          (d) Let Xi be a computable sequence of incompatible binary strings.  The func­
                   tion i H a(xi)  (where a is the continuous a priori probability) is a lower semicom­
                   putable semimeasure on N.  Indeed, it is lower semicomputable; the events “output 
                   starts with xf  are disjoint,  and therefore the sum of their probabilities does not 
                   exceed 1.  Therefore K(i) < К A (xf) + 0(1).
                          On the other hand, K(xi) = K(i) + 0(1), since i can be algorithmically trans­
                   formed into Xi and vice versa; finally, KA (xi) ^ K(xi) + 0(1) according to (c).
                          (e)  Let  a  be the universal continuous semimeasure.  Consider the function и 
                   defined  as  u(x) — a(x)/l(x)2.  It  is  lower  semicomputable.  Moreover,  since  the 
                   sum of a(x) over all strings x of length n does not exceed 1  (these strings are all 
                   incompatible), we get
                                                   £«<*> = £   £                                             =
                                                     X                 n  l(x)=n                   n
                   so we get the desired inequality.
                          (f)  This can be proved in a similar way.  This time we let u(x) = a(x)m(l(x)) 
                   where m is the a priori probability on N (as defined in Chapter 4).
                          (g)  Consider the function
                                                              u(x, n)             a(x),  if l(x) = n, 
                                                                                       0,  if l(x) /  n.
                                         5.  MONOTONE COMPLEXITY
             124
             Then for each n the function x    u(x, n) is a semimeasure in the sense of Chapter 4
             (the sum of values does not exceed 1), and we get the desired inequality.
                 (h) For a given computable (infinite) sequence w of zeros and ones, consider a 
             probabilistic algorithm that ignores random bits and just computes and sends to 
             the output the sequence u>  (bit by bit).  The corresponding semimeasure equals 1 
             on any prefix of w; therefore, the universal semimeasure (whose logarithm is the a 
             priori complexity) of all prefixes of w is greater than some positive constant.
                 The converse implication is a bit more complicated.  Assume that the a priori 
            probabilities  (the values of the universal semimeasure a on the binary tree) of all 
            prefixes of u> are greater than some rational e > 0.  Consider the set В of all binary 
            strings x such that a(x) > e.  The set В contains all prefixes of w and is a subtree (if 
            some string is in J3, then all its prefixes are in В).  Moreover, any prefix-free subset 
            of В (that does not contain a sequence and its prefix at the same time) has at most 
             \/e  elements  (since  the  corresponding events  are disjoint,  their total  probability 
            does not exceed 1).  Finally, the set В is enumerable (having more and more precise 
            approximations to a(x) from below, we eventually discover all elements in B).
                 These properties of В  are sufficient to conclude that the sequence u> is com­
            putable.  Indeed, consider the maximal (having the maximal cardinality) prefix-free 
            subset xi,... ,xn of B.  For each of Xi consider all its continuations that belong to 
            B.  All of them (for a given г) are prefixes of one sequence; otherwise, we can find 
            two inconsistent strings and replace Xi by them (and this is not possible, since the 
            subset is maximal).
                 So for each i we have a (finite or infinite)  branch in В going through Xi, and 
            it  is  computable since В is enumerable.  The sequence w is one of these branches 
            (otherwise we could add a sufficiently long prefix of u> to the set that is maximal—a 
            contradiction).
                 (i)  Consider the probabilistic machine that corresponds to the maximal semi-
            computable semimeasure on the binary tree,  and apply function /  to its output. 
            This composition is a probabilistic machine as defined in Chapter 4, and it remains 
            to compare it to the universal machine that generates the maximal lower semicom- 
            putable semimeasure on N (the logarithm of this semimeasure is К + 0(1)).          □
                Note that the a priori complexity is quite different from the complexities already 
            known (plain and prefix complexities).  Its definition uses a tree structure that exists 
            on the set of finite binary strings, and algorithmic transformations that ignore this 
            structure can increase the a priori complexity more than by 0 (1).
                 127  Show that one can find a string x that has an 0(1) a priori complexity 
            but  xR  (reversed  x)  has  arbitrarily  large  complexity.  (Formally:  there  exists  c 
            such that for every n there is a string x satisfying the inequalities KA (x) < c and 
            KA (xR) > n.)
                 (Hint:  The string x can be of the form 100 • • • 0.)
                So (unlike for plain or prefix complexity) we cannot define the a priori complex­
            ity of arbitrary constructive objects (pairs, graphs, finite sets, etc.) since it depends 
            on the encoding.
                The difference between the a priori complexity of a string x of length n and 
            other complexities of x  (plain,  prefix)  is still  O(logn).  However,  it  is  important 
            that n stands for the length of x,  not  for  the complexity of x.  (For example,  if
                               5.3.  A  PRIORI  COMPLEXITY  AND  ITS  PROPERTIES                125
             X is a string of n zeros,  its a priori complexity is bounded while plain and prefix 
             complexities are not.)
                  128 Prove that for every string x at least one of the numbers  К A (xO)  and 
             KA(x 1)  is  at  least  KA(x) + 1.  (In this  problem  it  is  important  that  KA(x)  is 
             defined as — loga(x) without integer rounding.)  Prove that for every string x and 
             every integer n there exists a string у of length n such that К A (xy) ^ К A (x) + n. 
             Prove that there exists an infinite binary sequence u> such that  К A (x)  ^ l(x) for 
             every prefix of u>.
                 Compare the last problem with Theorem 71  (p.  Ill) and Problem 46 (p. 42); 
             note that with the a priori complexity we can get rid of condition n and even the 
             constant 0 (1) appearing there.
                  129  Prove that the differences C(x) — К A (x) and К A (x) — C(x) could be of 
             order logn for some strings of length n (and for arbitrarily large n).
                 (Hint:  C(x) can be much greater than KA (x) if x consists of zeros only.  On 
             the other hand, C{x) is smaller than К A (x) if x is a prefix of a sequence from the 
             preceding problem.  In this case  KA(x) — l(x) + 0(1),  but  C(x)  can be smaller 
             than l(x) by log/(x); see Problem 54.)
                  130  Prove that
                                      KA (xy) < K(x) + KA (y) + 0(1),
             where xy is the concatenation of strings x and y.  It is important that x is on the 
             left of y:  show that for KA (yx) the statement is false.
                 (Hint:  Let U be a probabilistic algorithm in the sense of Chapter 4 that gener­
             ates the discrete a priori probability on strings.  Let V be the probabilistic algorithm 
             that generates the continuous a priori probability.  Then combine U and V as fol­
             lows:  first, run U until it outputs something and terminates.  Then run V using the 
             fresh random bits and add its output bits to the string generated by U.  To show 
             that К A (xy) cannot be replaced by К A (yx), let у = On and x = 1.)
                 (Cf.  Theorem 71 on p.  Ill and Problem 46 on p. 42; note that now we do not 
             have n as a condition, and we even do not have the term 0 (1) in the inequality.)
                 Another property of the a priori complexity is an immediate consequence of its 
             definition.  Let ß be a computable measure on Q.  Then for some c and every x we 
             have
                                          KA (x) ^ — log ß(Clx) + c-
             Indeed, the a priori probability on the binary tree is greater than ß (or any other 
             computable measure,  or even lower semicomputable semimeasure)  up to  a 0 (1) 
             factor, and it remains to take logarithms.
                 This (very simple) property is important since it is the basis for a criterion of 
             Martin-Löf randomness in terms of the a priori complexity:  a sequence u> is ML- 
             random with respect to a computable measure ß if and only if this inequality turns 
             into  an  equality  for prefixes of cj,  i.e.,  if the difference  — \ogß(Qx) — KA(x)  has 
             a constant upper bound for all x that are prefixes of ui  (it always has a constant 
             lower bound as we just mentioned).
                 This criterion follows from Schnorr-Levin theorem that provides randomness 
             criterion in terms of monotone complexity and we postpone its proof to Section 5.6
                 126                                     5.  MONOTONE COMPLEXITY
                 where  Schnorr-Levin criterion  is  considered.  But  first  we  have  to  define  mono­
                 tone complexity (Section 5.5), and this definition uses the notion of a computable 
                 mapping of the space E into itself (Section 5.4).
                       One can characterize the a priori complexity as the smallest upper semicom- 
                 putable (=enumerable from above) function that satisfies some condition,  as was 
                 done for plain complexity in Theorem 8 (p.  19) and for prefix complexity in Theo­
                 rem 62 (p. 100).  Here is the corresponding statement:
                       Theorem  80.  The function KA  is  a minimal  (up  to  an  additive  constant) 
                 upper semicomputable function к such that
                                                                        2~ k ^    ^   1
                 for any prefix-free set M  of binary strings.
                       PROOF.  Since the strings x € M are incompatible (none of them is a prefix of 
                 another one),  the corresponding sets Ea;  (of all finite and infinite sequences with 
                 prefix x) are disjoint and the sum of probabilities does not exceed 1.
                       On the other hand, let к be an upper semicomputable function that satisfies 
                 this condition.  We have to construct a lower semicomputable semimeasure that is 
                 greater that 2~k.  The latter function is lower semicomputable but is not necessarily 
                 a semimeasure;  its values on ж, ж0,  and ж1 can be unrelated.  So we need first to 
                 increase к when it is unavoidable.  Let u(x)  be the supremum of all sums of the 
                 form                                                     2~k(y)
                                                                    y£M
                 over all prefix-free M sets of extensions of ж.  It is easy to check that u(x) is indeed 
                 a lower semicomputable semimeasure and 2”^^ does not exceed a(x).                                                 □
                       11311 Let us consider functions b on binary strings with values in [0,1] that have 
                 the following property:  there exists a measure p on the tree such that b(x) ^ p($lx)-
                       (a)  Show that every semimeasure on a tree has this property.
                       (b)  Show that for every lower semicomputable function b with this property 
                 there exists a lower computable semimeasure on the tree that is an upper bound 
                 for b.
                                         5.4.  Computable mappings of type E —> E
                       In Chapter 4 we defined prefix complexity (in terms of shortest descriptions) 
                 and the a priori probability (in terms of probabilistic machines).  It turned out that 
                 it is essentially the same notion (one is the logarithm of the other).
                       In  this  chapter we  have defined the other notion of a priori probability  (the 
                 continuous one),  and a natural question arises:  Does it correspond to some nat­
                 ural notion of complexity defined in terms of descriptions?  Indeed, such a notion 
                 exists; it is called monotone complexity (though it differs slightly from the a priori 
                 complexity).  However, to give its definition (see Section 5.5 below), we first need 
                 to introduce some auxiliary notions.
                       The algorithms (machines) used in the definition of the universal semimeasure 
                 on the binary tree consist of two parts:  the random bits generator and the algorithm 
                 that  transforms the sequence of random bits into the output.  In this section we 
                 look  more closely  at  this  second  part  and  introduce the  notion  of a computable
                                        5.4.  COMPUTABLE MAPPINGS  OF TYPE E -> E                                    127
               mapping of the set  E  (of all finite and infinite sequences of zeros and ones)  into 
               itself.  Let us stress that we consider mappings that are defined everywhere on E; 
               however, some of their values can be equal to the empty string Л (that represents 
               an undefined value in some sense).
                     5.4.1.  Continuous mappings of type E —» E.  Let /: E —> E be a mapping 
               defined on the entire  E.  We say that  /  is  continuous  if it  has the following two 
               properties:
                     (1)  /  is monotone:  if x G  E is a prefix of some y G  E, then /(x)  is a prefix of
               /Ы-
                     (2)  The value f(uj)  for an infinite sequence cj is the least upper bound of the 
               values /(x) on all finite prefixes x of the sequence cj.
                    We use the notation x =4 y for the relation  “x is a prefix of y" ; here x, y G  E 
               may be finite or infinite.  We have x ^ x for any x; if x ^ y for an infinite sequence 
               x, then x — y.  Requirement (1) says that /  is monotone with respect to the partial 
               order =<! on E.  This requirement guarantees that the values /(x) for all finite prefixes 
               x of some sequence to are compatible  (extend each other);  their  “union”  (—least 
               upper bound under ^-ordering) coincides with /(oi) due to (2).
                      132          Show that the notion of continuity defined above is the standard conti­
               nuity notion with respect to the topology on E defined in Section 4.4.3 (p. 89).
                     (Hint:  A very similar notion of continuous mappings E —> Nx was studied in 
               the same section.)
                    Let /: E        E be a continuous mapping.  Consider the set Гf that consists of 
               all pairs  (x,y)  of binary strings x and у such that y =4 /(x).  (The set Гf  may be 
               called the lower graph of the mapping /.)
                    For any continuous /  : E —» E, the set Г/ has the following three properties:
                     (1)  (x, Л)  G  Г/  for every string x;
                     (2)  If (x, y)  G  Vf,  then  (x', y')  G  Г f  for every x'    x and y' ^ y.
                     (3) If (x, y\) and (x, yf) belong to Г/, then the strings y\ and г/2 are compatible 
               (one of them is a prefix of another one).
                    The first two properties are obvious.  The third one is true since any two prefixes 
               of a (finite or infinite) sequence are compatible.
                    The following theorem shows that a continuous mapping is defined uniquely by 
               its lower graph.
                    Theorem 81.  The mapping f                   Г/  is  a  one-to-one  correspondence  between 
               continuous functions of type E —> E  and sets of pairs of strings that satisfy condi­
               tions  (l)-(3).
                    Proof.  Let /  be a set of pairs satisfying conditions (l)-(3).  These conditions 
               guarantee that for any string x the set Fx of all у such that (x, y) G F is non-empty 
               and every yi,y2  £  Fx  are compatible.  Let /(x)  be the least upper bound of Fx. 
               Property (2) guarantees that x =4 x' implies /(x) ^ f(x')  (since Fx increases as x 
               increases).  Therefore we may define f(cj) as the union (least upper bound) of /(x) 
               for all strings x ^ oj.  Then the mapping /  is continuous.  It is easy to check that 
               we get a mapping which is an inverse mapping to the correspondence /  (->• Гf.  □
                    A continuous mapping / : E —» E is called computable if the corresponding set 
               Г/ is enumerable.  (By definition all computable mappings are continuous.)
             128                         5.  MONOTONE COMPLEXITY
                 This definition is self-contained and does not require any interpretation in terms 
             of machines.  All we say below about the interpretation of this notion in terms of 
             machines of special type is not necessary (and is not used in the sequel).  However, 
             to give a motivation for this definition, it is instructive to understand which type of 
             machine (program) corresponds to computable continuous mappings of type E —> E.
                 5.4.2.  Monotone machines with non-blocking read operation.  Let us
             consider programs that use a non-blocking read operation (we can get the next bit 
             from the input queue and also check whether this queue is non-empty).  We have 
             discussed this type of input  paradigm in Section 4.4.2,  p.  87.  However,  now we 
             assume that the output is created bit  by bit,  using the procedure  OutputBit(fe) 
            with a Boolean argument.
                 The output sequence generated by a program of this type can be finite or infi­
            nite.  In general, it depends not only on the input sequence but also on the timing 
             (the moments when keys “0”  and “1” were pressed).  We say that a machine (pro­
            gram) is robust if the timing does not matter, i.e., if the output sequence depends 
            only on the input sequence but not on the timing.  (Of course, the output timing 
            may still depend on the input timing.)  A robust program determines (computes) 
            some mapping of the set E into itself.
                 Theorem 82.  Robust programs compute computable mappings  (in the abstract 
             sense,  as  described  above);  on  the  other  hand,  every  computable  mapping  is  com­
            puted by some robust program.
                 Proof.  Assume that M is a robust program.  Let x and x' be two (finite or 
            infinite) sequences such that x =<! x'.  Let us show that M(x)    M(x') where M(z) 
            stands for the output of program M on the input 2 (since M is robust, the output 
            depends only on 2,  not  on the timing).  If x  is  infinite,  this  is  trivial  (x = x'). 
            Assume that x is finite.  There are two possibilities:  M(x) is either finite or infinite.
                 If M(x)  is  finite,  let  us  submit  input  x  and wait  until  M(x)  appears  at  the 
            output.  This should happen at some point;  after that  we submit the remaining 
            bits  of x'  (that  are not  in x)  to  the input.  Then we get  output  M(x')  which by 
            construction is the extension of M(x).
                 If M(x) is infinite, then every bit of M(x) should appear at some time after we 
            submit x to the input.  Since the remaining bits of x' can be sent after this moment, 
            this bit should appear also in M(x').  Therefore, M(x) = M(x') in this case.
                 It  is  also  clear that  for an infinite sequence из the value M(u>)  is the union of 
            M(x) for finite x =<! u)\  indeed,  at each moment only a finite number of input bits 
            have been read.
                 The set of all pairs of strings x, у such that у =<! M(x) is enumerable since we 
            can enumerate it by simulating the behavior of M on all inputs.  So each robust 
            machine computes a computable mapping.
                 On the other hand, let / be an arbitrary computable mapping.  We show how 
            to construct  a robust machine M that computes it.  The machine M enumerates 
            the lower graph Г/ of the mapping /.  At the same time M reads input bits and 
            stores them.  If it  turns out that Г/  includes a pair  (x, y)  such that x is a prefix 
            of the input sequence, we output the remaining bits of у (requirements (2) and (3) 
            guarantee that  all the strings у found in this way are compatible,  so there is no 
            need to recall the bits already sent to the output).                               □
                                       5.5.  MONOTONE COMPLEXITY                           129
                5.4.3.  Computable mappings can be enumerated.  The definition of com­
            putability based on robust machines seems to be more natural than the abstract 
            one.  However, it has the same drawback as in the case of prefix-stable programs: 
            there is no (algorithmic) way to find out whether a given program is robust.  So the 
            class of robust programs is not a syntactically defined class.
                Nevertheless, there exists an algorithmic transformation of programs that con­
            verts every program into a robust one (and does not change the mapping computed 
            by it if it was robust).  This transformation goes back and forth between mappings 
            and corresponding enumerable sets:  we transform a program into an enumerable 
            set of pairs (i.e.,  into an algorithm enumerating this set), then we trim this set of 
            pairs and transform it back into a program.
                We do not describe this process in detail, since robust programs are more a mo­
            tivation for the definition of a computable mapping than a technical tool.  Instead, 
            we prove that the set of computable mappings is enumerable in the following sense:
                Theorem 83.  There exists an enumerable set U  of triples (n,x,y)  (here n is 
            a natural number while x  and y  are binary strings)  such that:
                (a)  for every n  the set Un  —  {(x,y)  |  (n,x,y)  €  U}  is  a lower graph of some 
            computable  mapping un :  E  —>  E  (i.e.,  it  satisfies  requirements  (l)-(3)  of Theo­
            rem 81);
                (b)  every computable mapping of the set E into itself is equal to un for some n.
                Proof.  Consider the universal enumerable set W of triples:  every enumerable 
            set of pairs appears among Wn.  Then we trim W to enforce requirements (l)-(3) for 
            all  Wn  and leave unchanged the sets Wn that already satisfy these requirements. 
            After that  all  Wn  are  lower  graphs  for some  computable  mappings  wn  and  any 
            computable mapping appears among wn.
                The trimming is made in two steps:  first we delete inconsistencies and then we 
            fill  the gaps.  The inconsistency appears when two pairs  (x\,y\)  and  (£2,2/2)  are 
            found such that x\  is compatible with £2 but y\  is not compatible with 2/2 •  (It is 
            easy to see that two pairs with this property cannot both appear in the lower graph 
            of a continuous mapping.)  To eliminate it, we delete the pair that appeared later 
            in the enumeration.  Then we fill the gaps by adding all pairs (x, Л) and adding for 
            each pair (x,y)  all the pairs  (x',y')  with x1 )p x and y' =4 y.  It is easy to see that 
            the set remains enumerable and is the one we need.                             □
                Theorem 83 is used in the next section to prove that an (optimal)  monotone 
            complexity function exists.
                                     5.5.  Monotone complexity
                To define monotone complexity, we use computable mappings of type E —> E 
            as  decompressors  (description modes).  For a fixed decompressor D:  E  —>  E  the 
            monotone complexity of a string x  (with respect to D)  is defined as the minimal 
            length  of a  string  у  such  that  x  =<;  D(y).  Monotone  complexity  is  denoted  by 
            KM D(x).
                (This definition can be applied to infinite sequences x without any changes, but 
            we follow the tradition and consider KMd(x) only for finite x unless the opposite 
            is said explicitly.)
               130                               5.  MONOTONE COMPLEXITY
                     133          Prove that the monotone complexity of an infinite sequence (defined in 
               a natural way) is the limit of the increasing sequence of monotone complexities of 
               its prefixes.
                    Theorem 84.  There exists an optimal decompressor,  i.e.,  a computable map­
               ping D \  E  —>  E  such  that KM p  is  minimal up  to  an  additive  constant:  for any 
               computable D' : E —> E  there exists a constant c such that
                                                  KMd{%) ^ KMd'(x) -+■ c
               for every string x.
                    Proof.  Let U be the set of triples whose sections are the lower graphs of all 
               computable mappings (constructed in Theorem 83, p. 129).  Let Dn be a computable 
               mapping that has lower graph Un.  Then let us define a mapping D as
                                                        D(nz) = Dn(z),
               where n is the prefix-free encoding of the number n (say, its binary representation 
               with doubled digits followed by 01)  and 2 is an arbitrary element of E.  In terms 
               of the  lower  graph,  consider  the set  of all  pairs  (nu,v)  such  that  (n,u,v)  €  U. 
               It  is  easy  to  check  that  we  indeed  get  a  computable  mapping.  If some  (mono­
               tone) decompressor D' has number n (i.e., its lower graph coincides with Un), then 
               КМр(х) < KMd'(x) + l(n) for every x.                                                               □
                    As usual, we fix some optimal monotone decompressor (description mode), i.e., 
               some computable mapping D that satisfies the statement of this theorem, and define 
               monotone complexity of a string x as KMd(x).  We use the notation KM(x) (the 
               subscript D is omitted).
                    (Warning:  Sometimes the notation KM{x) is used for the a priori complexity. 
               Usually in this case the monotone complexity is denoted by Km,  as in  [103],  or
               K m .)
                    Theorem 85.  (a) Monotone complexity is a monotone function, i.e.,
                                                 KM(x) ^ KM(y) if x^y\
                    (b)  the function KM is upper semicomputable\
                    (c)  KM{x) <J(x) + 0(l);
                    (d)  KM(x) ^ K(x) + 0(1);
                    (e)  К A (x) ^ KM(x) + 0( 1);
                    (f)  an infinite sequence of zeros and ones is computable if and only if the mono­
               tone complexity of its prefixes is bounded;
                    (g)  if f  ■  T,  -ï  Y,  is  a  computable  mapping,  then KM(f(x))  < KM(x) + 0(1) 
               (the constant hidden in 0 (1)  may depend on f  but not on æ);
                    (h)  if f  :  E  —>  Nj_  is  a  computable  mapping,  then K(f(x))  ^  KM(x) + 0(1) 
               (the constant hidden in 0 (1)  may depend on f  but not on x).
                    It is instructive to compare these statements with the properties of the a priori 
               complexity given in Theorem 79 (p. 122).  Since monotone complexity is not smaller 
               than the a priori complexity (statement  (e)), some properties of the a priori com­
               plexity are automatically valid for monotone complexity.  In particular, we conclude 
               immediately that K(x \ l(x)) ^ KM(x)+0(l) and K(x) ^ KM(x)+K(l(x))+0( 1). 
               Note also that for computable sequences of incomparable strings (none is a prefix
                                                                  5.5.   MONOTONE COMPLEXITY                                                              131
                     of another one)  the prefix and  the a priori complexities coincide up  to  an addi­
                    tive constant  and  monotone complexity is between them.  Therefore it  coincides 
                    with them:  if xq,x\,...  is  a computable sequence  and  Xi  ^  Xj  for  i  ^  j,  then 
                     KM(xi) = KA {xi) + 0(1) = K{xi) + 0(1).
                           PROOF.  Statement (a) is a direct consequence of the definition:  if D(u)                                                       y, 
                    then D(u)             X for every x that is a prefix of y.  One could say that in the definition 
                    of monotone complexity one needs to describe not the string exactly, but any of its 
                    extensions,  and the longer the string is, the more difficult this task becomes (the 
                    set of extensions becomes smaller).
                           Statement  (b)  is true since the lower graph of a computable mapping is enu­
                    merable, and the set of triples (x,y,r), such that l(y) < r and (y,x) belongs to the 
                    lower graph, is enumerable, too.  The upper graph of KM is a projection of this set.
                           To prove (c) it is enough to note that the identity mapping E —> E such that 
                    D(x) — x for all x G E is computable.
                           To compare KM and К  (statement  (d))  it is enough to note that any com­
                    putable mapping E —» Nj_  becomes a computable mapping of type E —» E if Nx 
                    is  embedded into  E  (and  _L  becomes an empty string).  More formally,  let  D be 
                    a prefix-stable  decompressor used  in the definition of К.  In can be extended  to 
                    a computable mapping of type E  —>  E  (the strings where  D  was undefined  are 
                    mapped into Л, and the values on infinite strings are determined by the continuity 
                    requirement).
                           To compare  KM  and  К A  (statement  (e)),  we have to  recall the remark we 
                    started with:  a probabilistic  algorithm  is  a random bits  generator whose output 
                    is  fed  into  a computable mapping of E into itself.  Let D be the optimal decom­
                    pressor used in the definition of the monotone complexity.  Consider a probabilistic 
                    algorithm that feeds a random sequence into D.  What is the probability of getting 
                    some string x (or some its extension) as the output?  Obviously, this probability is 
                    at least 2~1^   for any string у such that D(y)                                    x, since the random string starts 
                    with у with probability 2~l^y\  and this guarantees that the output of D starts with 
                    x.  (We return to the comparison of KM and KA  in Theorem 87.)
                           In  statement  (f),  one  implication  is  a straightforward  corollary of the corre­
                    sponding  statement  of Theorem  79.  The  other  implication  is  obvious—all  the 
                    prefixes of a computable sequence ui have bounded complexity since there exists 
                    a computable mapping E —> E that is equal to ui everywhere.
                           To prove (g), let us consider the monotone decompressor that is the composi­
                    tion of an optimal monotone decompressor and the mapping /.  Note that in this 
                    statement the sequence f(x) can be infinite.  If we do not want to deal with the 
                    complexities of infinite sequences, the statement should be reformulated as follows: 
                    for each /  there exists a constant c such that for all x,y such that у                                                     f(x)  the 
                    inequality KM(y) ^ KM{x) + c holds.
                           A similar argument works for (h), but this time the composition of the optimal 
                    monotone decompressor and / is a prefix-stable decompressor.  (One can also derive 
                    this statement from a similar statement about the a priori complexity.)                                                                □
                            134               Prove that  KM(xy)  <  K{x) + KM{y) + 0(1)  (here xy stands for the 
                    concatenation of strings x and y).  In particular, KM(xy) ^ K(x) + I(y) + 0(1).
             132                         5.  M ONOTONE  COMPLEXITY
                 (Hint:  Consider  the  optimal  prefix-free  decompressor  Dp  and  the  optimal 
             monotone  decompressor  Dm.  Now  let  D'(uv)  =  Dp(u)Dm(v)  (when  Dp  stops 
             reading the input, the remaining part of the input is read by Dm).)
                  135       Show that in the preceding problem one can replace KM(y) by the condi­
             tional monotone complexity KM(y\x) defined in a natural way (we do not require 
             monotonicity with respect to condition x, see Chapter 6 for details).
                  136       Prove that statement (g) remains true if we replace KM by KA  (in the 
             both sides of the inequality).
                 (Hint:  The mapping /  can be applied to the output of a probabilistic machine; 
             the new probabilistic machine is not better than the optimal one.)
                 We can give an equivalent definition of monotone complexity that does not use 
            computable mappings of type E —> E; in this way we get a simpler (but somewhat 
            less natural, in our opinion) definition.
                 Let  £  be  the  set  of all  binary  strings.  Consider  the  binary  relation  “to  be 
            compatible”  on this set:  x is compatible with y    x ^ у ох y ^ x (an equivalent 
            property requires that x and y are prefixes of the same string).  An enumerable set 
             (binary relation) D C £ x £ is called consistent if it has the following property:
             (x\i Ui)i{x2, У2 )  and (xi  is compatible with X2)  =>  (y\  is compatible with г/2)
            for all xi, x2 , Vi, У2 -  Then the monotone complexity of a string у with respect to 
            D is defined as the minimal length of a string x such that  (x, y)  G D.  There is an 
            optimal consistent enumerable binary relation on £.
                1137 I Prove that this definition leads to a notion of monotone complexity that 
            differs from the previous one by at most 0 (1).
                 (Hint:  The lower graph of any computable mapping E —» E is a consistent bi­
            nary relation.  On the other hand, if О is a consistent binary relation, the gap filling 
            described in the proof of Theorem 83 makes it a lower graph of some computable 
            mapping.)
                 It is instructive to compare this definition with the definition of plain complexity 
             (where we use graphs of computable functions, i.e., uniform enumerable sets instead 
            of consistent  relations  D).  In  the  definition  of monotone  complexity  we  do  not 
            require D to be a graph of some function:  several pairs (x, y) with the same x and 
            different у are allowed;  we require only that all y’s in these pairs are compatible. 
            This makes KM smaller.  For example,  all prefixes of some computable sequence 
            (say,  0000 ••• )  have bounded complexity  (note that C(0n)  = C(n)  is about logn 
            for most n).
                 On the other hand we apply additional restrictions:  if a string x is a description 
            of some string y, then the strings that are compatible with x can be descriptions 
            only of strings  that  are compatible with y.  This makes complexity larger.  This 
            is especially clear when we consider complexities of the elements of a computable 
            sequence of pairwise incompatible strings:  in this case monotone complexity coin­
            cides with prefix complexity,  and the difference can be about  logn for strings of 
            length n.
                Summing up  (and recalling that  both the  a priori complexity and  the plain 
            complexity  differ  from  the  prefix  complexity  at  most  by  О (logn)  for  strings  of 
            length n), we come to the following conclusion:
                                                                  5.5.   MONOTONE COMPLEXITY                                                              133
                           Theorem 86.  The difference between C(x) and KM(x) is bounded by O(logn) 
                    for strings  of length n  and may be  both positive  and negative  with  absolute  value 
                    logn — 0 (1) for n-bit strings for infinitely many n.
                           We return to the comparison of different versions of complexity in Chapter 6. 
                    Now we provide only one statement of this type:
                           Theorem 87.  The difference KM(x)—KA (x) is not bounded from above; more­
                    over, for infinitely many n there exists an n-bit string x for which this difference is 
                    at least log logn — О (log log logn).
                           This theorem (proved by Day [48]) strengthens an old result by Gåcs [57] that 
                    established a weaker lower bound for the difference KM(x) — К A (x).  Both papers 
                    use a reduction to a game, for which a strategy for one of the players is constructed.
                           Recall  that  in  both  definitions  (of  KM(x)  and  К A (x))  we  use  computable 
                    continuous  mapping  /:  E  —>■  E  and  consider  the  preimage  of the  set  Ex  of all 
                    sequences starting with x.  Defining К A, we are interested in the measure of this 
                    preimage, while for KM we are looking for the largest interval of type Ey which is 
                    a subset of this preimage.  This shows that К A                                      KMf, and the difference can be
                    large, if the preimage is sparse (if it consists of large number of small intervals).  The 
                    question is how large this difference could be for an optimal computable mapping.
                           We have seen a similar situation before.  Recall our metaphor of space alloca­
                    tion (we allocate subsets of [0,1] for countably many clients) used in the proofs of 
                    Theorem 46 (p. 78) and Theorem 58 (p. 93).  The difference between prefix com­
                    plexity and the logarithm of the a priori probability on N has the same nature (the 
                    difference between the total measure and the maximal contiguous interval).  How­
                    ever, in that case we were able to perform some kind of consolidation by modifying 
                    the description mode, and the price was just a constant factor.
                           Now we have a more delicate task since our clients form a hierarchy.  This makes 
                    reorganization  more  difficult  and  consolidation  leads  to  more  than  the  constant 
                    factor overhead.
                           5.5.1.  The proof of Gåcs—Day theorem.  This is probably the most diffi­
                    cult argument in the entire book (though we tried hard to simplify the arguments 
                    from original papers of Gåcs and Day), and it is not used in the rest of the book, 
                    so feel free to skip it if it looks too difficult.
                           We start  by describing some  game.  The  two  players  are  called  Client  and 
                    Server.  The game has two parameters:  a rooted tree and some rational  d  ^  1. 
                    At each moment of the game the vertices of the tree are labeled by non-negative 
                    rational numbers;  the label of vertex x is called the request of this vertex (at the 
                    given moment).  The request of each vertex is at least the sum of requests of its 
                    sons, and the request of the tree root is at most 1 jd.
                           Requests are chosen by Client.  Server tries to serve these requests by allocating 
                    space in Cl.  At each moment of the game Server allocates some subset of Cl for each 
                    vertex.  This subset should be a union of finitely many intervals (=sets Г2Х).  A set 
                    allocated for each vertex x should contain the sets allocated to the sons of x, and 
                    the sets allocated to brothers should be disjoint.  This implies that sets allocated 
                    to incomparable vertices (one is not a descendant of the other) are disjoint.
                           The players  alternate.  Initially the requests  of all  vertices  are  zeros  and  all 
                    the allocated subsets are empty.  At every move,  Client may increase requests for 
                    some (or all) vertices but should not violate the restrictions stated above (otherwise
             134                           5.  M ONOTONE  COMPLEXITY
             she loses the game).  In response, Server may increase the sets allocated to vertices 
             (also obeying the restrictions).  Her goal is to satisfy all the requests in the following 
             sense:  the set allocated to each vertex i should contain an interval whose length is 
             at least the request of i.  (The length of Q,x is 2~n for an n-bit string x.  As we have 
             already explained,  we care about  the contiguous intervals in the allocated space 
             and not about its total size.)
                  If at some moment Server is unable to satisfy the requests made by Client, she 
             loses.  If the game is infinite (and both players follow the restrictions), we say that 
             Server wins.
                  One can  imagine  that  Client  is  the  CEO  of a big  hierarchical  organization 
             which needs some space for its divisions,  subdivisions,  subsubdivisions,  etc.  At 
             every  moment  it  is  known  how  much  space  each  group  requires,  and  the  space 
             allocated to each subgroup should be inside the space for the parent group.  The 
             space cannot be reused, it can only increase, and only the contiguous space counts 
             (the size of the maximal interval, not the sum of the sizes of intervals).
                  Increasing the  height  of the  tree  and  adding  branches,  we  make the task  of 
             Server harder.  The following statement says that it is enough to use trees of depth 
             0(d) and with (constant) branching factor 2°^°{d)  to let Client win:
                  Theorem 88 (Gacs-Day).  For each d ^ 1  and for a tree T of depth 0(d)  and 
             branching factor 2°0)O(d)  at every vertex,  Client has a computable winning strategy 
             in the  corresponding game.  (Computability means that there is an algorithm that, 
             given d,  implements this strategy.)
                  Gâcs has proven a similar result for trees with an infinite (or very large finite) 
             branching factor.  Day improved his construction and made it work for much smaller 
             branching factors.
                 Before constructing the winning strategy, let us explain how its existence im­
             plies  Theorem  87.  Note  the  branching  factor  can  be  decreased  if we  allow  an 
             increase in the depth:  for example,  the tree with a root  and 2n  sons of the root 
             can be embedded into a binary tree of height n (the requests for the intermediate 
             vertices are reconstructed as the sums of the requests of the leaves above them). 
             In a similar way the tree from Theorem 88 can be embedded into a binary tree of 
             height 0(d)o('d\  so Client wins for this binary tree.
                 Let d = 2C, where c is some natural number.  Theorem 88 guarantees that Client 
             has a computable winning strategy on the binary tree of height 2°^c2<:\  Let us use 
             this strategy against Server who follows the optimal computable mapping / : E —> E 
             used to define the monotone complexity.  This means that Server enumerates all 
             pairs  (y,x)  such that x    f(y)  (i.e.,  y  is  a description of x).  When a pair  (y,x) 
             appears, the interval Cty is allocated to vertex x (if x is inside our tree—if the length 
             of x exceeds 2°(c2 \ then x is ignored).  When (and if)  all the requests of Client 
             are satisfied, Server informs Client that she made her move.  After the next move of 
             Client, Server resumes the process and continues until the new requests are satisfied 
             (if it never happens, Server loses the game by not making a move).1
                 Theorem 88 guarantees that at some moment Server loses (she never satisfies 
             Client’s request).  This means that there exists a string x of length at most 2°(c2C)
                 1 Readers  from  the  former  USSR  and  similar  countries  should  be  familiar  with  planned 
             economies when supply does not follow demand:  the factories just produce what is planned until 
             the customers become satisfied (if they are lucky enough to make modest requests).
                                                                 5.5.   MONOTONE COMPLEXITY                                                              135
                    such that the request for x at some moment exceeds 2” KM(X).  On the other hand, 
                    during the game,  Client  (using her computable strategy against the computable 
                    Server)  enumerates  from  below  some  semimeasure  jic  on  the  tree:  its  value  at 
                    vertex x is equal to the limit (=supremum) of all requests for x.  Since Client wins, 
                    for some x we have KM(x) > — logßc(x).  The weighted sum of semimeasures /ic 
                    with weights 2c/c2 is a lower semicomputable semimeasure on the infinite binary 
                    tree and is bounded by the continuous a priori probability (up to an O(l)-factor). 
                    So the a priori probability is at least £цс(х)2с/с2 for some fixed e > 0 and for all 
                    c, x.  We conclude that for every c there exists a string x of length at most 2°^c2 ) 
                    such that
                                                         KM(x) > KA (x) + c — 2loge — 0(1).
                    Let n be the length of x; then c is at least log log n — O(logloglogn) and therefore 
                                                  KM(x) > К A (x) + log log n — О (log log log n).
                           Note that our argument constructs a string x of length n with this property for 
                    infinitely many n, but not for all (sufficiently large) n.
                            138  Prove that  for  infinitely  many  n  the  following  is  true:  for  all  strings 
                    of length n the difference KM(x) — К A (x)  is bounded by log log log n.  Here the 
                    iterated logarithm can be replaced by an arbitrary non-decreasing unbounded com­
                    putable function.
                           (Hint:  If some rare lengths are declared as very important, we can allocate the 
                    space for strings of each rare length in a special area reserved for this length, thus 
                    making the overhead rather small compared to length.)
                           P r o o f.  Let  us  start  the  proof of Theorem  88  with  an  informal  discussion. 
                    What is the source of difficulties for Server?  Imagine that Client requests a very 
                    small amount of space for some vertex.  Server then has a choice:  either allocate 
                    a part of the free zone (neighbor intervals are not allocated for any other vertex) 
                    keeping in mind the possible increase of the request,  or  do not  think about  this 
                    possible increase and allocate some (maybe) non-extendable space.
                           The danger in the first case is that this reserved space will never be used,  if 
                    Client will not increase the request or will increase it so much that this reserved 
                    space cannot  be used  (as  it  is  too  small  anyway).  In the  second  case,  if Server 
                    allocates neighbor intervals to other vertices, and then Client increases the request, 
                    the originally allocated interval is lost, since only contiguous intervals matter.
                           The winning strategy for Client exploits this dilemma.  To keep track of the 
                    process, we look at ^-neighborhoods of different vertices.  Let e < 1 be a negative 
                    power of 2.  By £-neighborhood of some set X in the Cantor space we mean the 
                    union of all intervals of length £ that have non-empty intersection with X.  The 
                    interim goals of Client are formulated as follows:  the ratio
                                           the size of e-neighborhood of the space allocated for x 
                                                                              request for x
                    is at least some k.  For large k, if the requested space is more than 1 /к, Server loses.
                           Following this plan, we construct strategies for Client achieving that at some 
                    moment
                                •  the request for the root is at most a;
                                •  the e-neighborhood of the space allocated for the root, is at least ß.
              136                            5.  M ONOTONE  COMPLEXITY
              Here a and ß are some parameters.  The e-neighborhood of the space allocated for 
              the root is called gray space:  these e-intervals can never be allocated for any other 
              vertex.  More precisely, we use the following parameters:
                      •  the tree for which the game is played;
                      •  e that is used to measure ^-neighborhoods;
                      •  maximal allowed request a for the root;
                      •  the required size ß for the gray area (i.e., the ^-neighborhood of the space 
                        allocated for the root).
              We are interested in the values of these parameters such that there exists a winning 
              strategy  for  Client,  i.e.,  she  can  achieve  that  ^-neighborhood  of the  space  that 
              Server allocates to the root has size at least ß while the root request is at most a.
                  Example  1.  Let e be an arbitrary negative power of 2,  let  ß  —  e,  and let 
              a be positive and much smaller than ß.  Then the (trivial) strategy  “just request 
              a for the root”  works for every tree:  whatever Server allocates for the root,  this 
              (non-empty) set has an ^-neighborhood of size at least e.
                  So it is easy to get arbitrarily high amplification (high ß/a) for small ß.  The 
              difficult case is when ß    e, and in this case we construct the strategy recursively by 
              combining strategies for different trees and using inside the strategy some recursive 
              calls of other strategies for the subtrees.  In this inductive (recursive) construction it 
              is convenient to add amplification as a parameter, introducing one more parameter 
              к and requiring that the ratio (size of the gray area)/ (request) is at least k.  For к = 
              ß/a, this requirement is obviously true, but we will use strategies that guarantee 
              given amplification k, while the request size (and the gray area size) may vary in 
              some limited way.
                  Example 2.  Let T be the tree where the root has m sons, and each of them 
              has two sons  (=grandsons of the root).  Let e be some  (negative) power of 2,  let 
              a — ß = me for some integer m, and let к = 3/2.  (We see that к is important here: 
             we do not specify the exact size of the request and the exact size of the gray area, 
              but the second one should be к times greater than the first one, and me should be 
              in between.)
                  Here  is  the  winning  strategy  for  these  values.  To  make  trouble  for  Server, 
              Client  selects  for  each  son  of the  root  one  of its  sons,  and  requests  e/2  for  all 
             these grandsons of the root.  (We specify here the requests for leaves only; for other 
             vertices the requests are computed as the sum of requests for the descendants.)  Now 
              Server should decide which grandsons should be paired with their cousins (getting 
             e/2 inside one interval)  and who should be  “a single occupant of a double room” 
              (the neighbor interval of size e/2 is kept free).  Looking at Server’s decision, Client 
             increases the requests trying to make life harder for Server:  for grandsons who do 
             not have the reserve (have neighbors), Client requests e/2 for their brothers, thus 
             making the father’s request e.  Then Server needs to allocate a fresh interval of size 
             e for the father (since the old one cannot be used, part of it is already allocated for 
             his niece; the reserves in other places are also too small).  Therefore, for each of m 
             sons of the root one of two things happen:  either e was grayed for e/2-request, or 
              (3/2)e was allocated for e-request.  In both cases the amplification is at least 3/2.2
                  2In fact,  it  is  easy  to  achieve  amplification  3/2  by  asking  for  each  son  of the  root  slightly 
             more than e/2:  the interval sizes are powers of 2, and Server is forced to allocate an interval of size
                                                                  5.5.   MONOTONE COMPLEXITY                                                              137
                           Let  us now try to combine two strategies with the same amplification factor 
                     k.  Our goal is to keep this amplification but increase the size of the request  and 
                    of the gray area.  Consider a tree where the root  has two sons with subtrees T\ 
                    and T2.  Assume that Client has a winning strategy for Ту, aq, ßy, e, and a winning 
                    strategy for T2, 0:2, /З2, £■  Let Client use these two strategies sequentially:  the first 
                    strategy  is  used  for  Tb  and  when  it  wins  (the  gray  area  is  large  enough),  the 
                    second strategy is used for T2.  (One can  assume without  loss of generality that 
                    during Ti-game nothing is allocated for the vertices in T2, since these allocations 
                    can be postponed.)  The total size of requests is then bounded by on + a2.  But we 
                    cannot claim the additivity for gray areas:  it is quite possible that the size of the 
                    e-neighborhood of the union is smaller than the sum of the sizes of ^-neighborhoods 
                    of the parts.  For example,  in the second game Server can use some space left as 
                    reserve in the first game.
                           To  avoid  this  problem,  we  use  different  values  E\  and  £2  for  the  strategies. 
                    Assume that E\  <C £2  and the second strategy uses only requests of size at least 
                    £\.  Then  Server cannot  use  the gray  area of the first  game  for the  second  one. 
                    Informally, we accumulate reserves “on different levels”, first on a micro level, then 
                    on a macro level.  However, we cannot say that the gray areas are added.  While the 
                    space allocated for T2 does not intersect the £j -neighborhood for Ту, the opposite 
                    is  possible:  space allocated for Ту  may well intersect the ^-neighborhood for T2. 
                    To deal with this problem, we again consider a more general setting and agree that 
                    some set A C Q is fixed before the game starts; we say that A is unavailable to the 
                    server, and count only the new gray intervals.  Let us explain in detail what all this 
                    means.
                           The final version of the game has the following parameters:
                                •  a tree T;
                                •  a subset A C Q ( “space unavailable to Server” ) ;
                                •  8 (shows how the neighborhood of the unavailable space is measured);
                                •  £ (shows how the neighborhood of the allocated space is measured);
                                •  the maximal allowed request a for the root;
                                •  the required size of gray area /3;
                                •  the required amplification factor k.
                           We assume that £ and 8 are both (negative) powers of 2, and £ ^ 8.
                           Here  are  the  rules  of the  game.  Client  increases  requests  for  vertices  of T 
                    (the request of a vertex should be at least the sum of requests for its sons).  The 
                    minimal request is 5,  and the root request should not exceed a.  Server allocates 
                    space for vertices of T, fulfilling the requests, and should use only intervals that do 
                    not intersect A.  Client wins if the size of the new gray area  (e-neighborhood of the 
                    allocated space minus 8-neighborhood of A)  is at least ß  and is at least к  times the 
                    request for the root.  As before, adding vertices to the tree or increasing e, we make 
                    Client’s task easier.  This happens also if we decrease 5, /3, or k.  One may assume 
                    without loss of generality that A is made of intervals of size at least 8 (since only 
                    the Æ-neighborhood of A matters).
                    at least e.  But this rounding effect cannot be scaled recursively, so we will ignore it.  Also we can 
                    use the sons only  (not the grandsons), first asking e/2 for each of them and then increasing the 
                    requests for the vertices where Server provides  no reserve.  However,  the version with grandsons 
                    is  closer to the strategy in the general case (see the proof of Composition Lemma,  page 138),  so 
                    we have chosen this version.
                                      5.  M ONOTONE  COMPLEXITY
            138
                When this definition is given, the arguments above prove the following state­
            ment.
                Composition lemma.  Assume that for some tree T\  and for every unavailable 
            set A,  Client can win the game with parameters £1, Si, ot\,ß\, k\.  Assume also that 
            for some  tree T\  and for  every  unavailable  set  she  can  also  win  the  game  with 
            parameters E2,&2 ,a2,02,^2.  Finally,  let us  assume  that E\  — Ö2  and k\  = ^2  (we 
            denote this value by к).  Then for the tree T that consists of the root with two sons 
            having subtrees T\  and T2  and for every unavailable set,  Client can win the game 
            with parameters e2, <5i, Qu + »2, ß\ + 0 2 , к.
                The composition game starts with some set A, the space unavailable to Server 
            during the game.  This set  is used without changes in the first  of two composed 
            games; for the second game we add to this set the area grayed during the first game. 
            It is easy to see then that the newly grayed areas in both games do not intersect.
                In fact, in the sequel we do not use exactly the statement of the lemma, but use 
            the same idea in a slightly different situation:  the subtrees where games are played 
            are not fixed in advance but are chosen during the game (the next subtree depends 
            on the game on the previous one).  Also we combine many strategies, not just two. 
            Because of this, we get a huge gap between the values of e and <5 in the combined 
            game:  <5 for each game is equal to e in the preceding one and significantly smaller 
            than £ in the current game.
                This is not enough to finish the proof:  the amplification factor for the combined 
            game is the same as for each game in the combination, so we need some other trick 
            to  increase amplification.  Before giving an example of amplification increase,  let 
            us  make  a simple technical  remark about  the game  definition.  We may assume 
            without loss of generality that the root request  is at  least ß/k at the end of the 
            game.  Indeed,  if it  turns out  to  be  less  (due to  some  unexpected luck),  we just 
            formally  increase  it  at  the  end  on  the  game,  and  the  winning  condition  is  still 
            satisfied.  We call this trick final adjustment in the sequel.
                Example 3.  Let us show how Client can achieve amplification factor 2.  The 
            idea is to follow the same scheme as in Example 2, but use (instead of direct ej2- 
            requests for grandsons) the recursive calls of the strategy that gives amplification 
            3/2 (from the same Example 2).
                Let us recall what was achieved there.  For a given a and for arbitrarily small 
            £  (such that m  = a je is  an integer),  Client  has  a strategy on  a tree of height  2 
            that allows her (for 5 — e/2 and for an arbitrary unavailable3 set A) to get at least 
            a newly grayed space with a root request  at most a  and an amplification factor 
            at  least 3/2.  The tree has m sons of the root, and each has two sons.  Note that 
            m should be an integer, but this is not a problem,  as we will use the strategy of 
            Example 2 only when a is a multiple of e.
               To compose strategies of this type, each next strategy should have an £-parame- 
            ter twice as big as the preceding one.  It is important that we can use the same value 
           of a in all the games;  the construction of Example 2 makes the choices of a and
               3In  Example 2  we did not  consider the unavailable space,  but  the same strategy works in 
           this  case:  we say that  a grandson  of the root  has  a reserve  if the e/2-interval  allocated  to this 
           grandson can be extended to the e-interval in place (the other half of the e-interval is not allocated 
           to the other grandson and does not intersect the unavailable space).  We use here that 5 = e/2: 
           the newly grayed area is disjoint with unavailable space because of this.
                                                                  5.5.   MONOTONE COMPLEXITY                                                              139
                    e independent.  Knowing in advance how many strategies we want to compose, we 
                     decide what should be the initial value of e and 5 (for the first strategy in a row). 
                    In the sequel we assume that the parameters of the composed games are chosen in 
                    this way and return to our task:  achieve к > 2, if the e-neighbor hood is measured 
                    at the end of the composed game (for some e), and we are free to choose Ö used to 
                    measure the ^-neighborhood of the unavailable space in the composed game.
                           We show how Client can achieve some к > 2 for arbitrary e, for every a = me 
                    and for small enough 5, with root request at most a and for newly grayed area at 
                    least a (so ß = a), if a tree is chosen in a suitable way.
                           Our construction is similar to Example 2.  The root has  (as before) m sons, 
                    but now has more grandsons.  Let us agree that each son of the root has 12 sons (it 
                    will be enough).  Instead of making direct requests for the grandsons (as was done 
                    in Example 2), we recursively call the strategies described above, so each grandson 
                    has a subtree of height 2 (its width should be big enough for all values of e used in 
                    the subgames), and the total height of the tree is 4.
                           At each moment we look at the sons of the root and consider those of them 
                    who currently do not have a reserved interval;  by a reserved interval for vertex x 
                    we mean an interval of size e that contains some space allocated to x and which 
                    does not contain any space allocated to vertices that are not descendants of a;, as 
                    well as any points of the unavailable set  (specified at the beginning of the game). 
                    In other words,  an interval is reserved when  (1)  its part  is  already allocated for 
                    x,  and  (2)  this  interval may still be used for x if the request  for x increases and 
                    becomes e.  (Note that the reserved interval may disappear later if some its part 
                    is  allocated  to  another vertex.)  So we consider some son x that does not have a 
                    reserved interval, take some son у of x (not used before for the same purpose), and 
                    run a strategy with amplification factor 1.5 and a = e/8 on у.  The request for у 
                    made by this strategy is at most a = e/8 and (because of final adjustment, see the 
                    paragraph before Example 3 on p.  138) at least a/1.5 = e/12.  After that  (when 
                    the strategy wins its game) look at x again:  maybe now x has a reserved interval, 
                    and maybe not.  In the latter case,  we can  apply  the same trick to some  other 
                    son of x—or,  if we wish,  we can select some other root’s son that does not have 
                    a reserved interval—both options are OK.  In any case,  we repeat this procedure 
                    until all sons of the root have their reserves.  One additional precaution is needed: 
                    if the request for some root’s son exceeds (7/8)e, we just increase its request up to 
                    e (which creates a reserved interval automatically) to avoid the possibility that the 
                    request increases by e/8 and becomes greater than e.  Each call of the 1.5-strategy 
                    increases the request of the corresponding son of the root by at least e/12, so we 
                    never need more than 12 sons for each son of the root.  For the same reason, the 
                    total number of these calls is bounded by 12m.
                           What do we achieve by all these tricks?  For each son of the root look at the 
                    last moment when we considered this son and finally got a reserved interval for it. 
                    This reserved interval has size £ and it was not a reserved interval before the last 
                    step.  Since it became a reserved interval (and continued to be a reserved interval), 
                    it contains no points of the unavailable set and no space allocated to other vertices 
                    (except for the descendants of x).  Since it was not a reserved interval before, it had 
                    no space allocated for x,  and therefore it was completely empty.  The conclusion: 
                    the space that was grayed during previous calls of the  1.5-strategies  is  not  part 
                    of the  reserved  intervals.  This  space  already  gives  us  1.5-amplification,  and  by
             140                         5.  M ONOTONE  COMPLEXITY
             adding (rather big and almost empty) reserved intervals, we get   amplification for 
             к = 20/9 > 2, as one can check.
                 Let  us  make  a detailed  accounting.  Let 7  be the sum of the requests  made 
             during all not-the-last  calls for all sons of the root.  These calls provide a grayed 
             area of size at least  (3/ 2)7 that does not intersect with the reserved intervals.  In 
             total, we get a grayed area of size at least (3/ 2)7 + me, and all our requests in total 
             are bounded by 'y + m,(e/8).  So there are two parts:  for one part, amplification is at 
             least 3/2, for the other part, the amplification is 8, and the second part is not too 
            small compared to the first, so in total we get a significant increase.  Technically, 
            7 ^ me implies               r3            20        e'
                                          -7 + me > —  7 + m-
                                                        9
             (a simple computation).  So we get a desired strategy for a = ß = me and к = 20/9 
             (so к > 2).
                 Now the big picture should be more or less clear.  Having the strategy for к = 
            20/9, we can call it recursively for the grandsons of the root (therefore considering 
            the tree of height 6 with large branching factor; one can get an explicit upper bound 
            for this factor).  With some tuning of the parameters, such a step can increase the 
            amplification factor к  almost by  1.  Indeed,  if we take a very small fraction of e 
            instead of e/8  (used in our last example),  the overhead that happens during the 
            last step (when the final reserved intervals appear) is negligible, and we get reserve 
            me in addition to a fc-times increase achieved during recursive calls.  If the total 
            request 7 is close to its maximal value me, we increase amplification almost by 1, 
            and if 7 turns out to be smaller, the amplification is even better.  To get the upper 
            bound  for the  width of the  tree,  we  recall  that  we  may  assume without  loss  of 
            generality that each recursive call increases our request by some guaranteed value. 
            This shows that for a tree of height O(k) and large enough branching factor, Client 
            can guarantee ^-amplification, and this is enough for Gåcs (but not for Day).4
                 Let  us  now  go  through  the details  of this  argument.  We  consider  values  of 
            к >  1 that are multiples of 1/2.  By induction we prove that for every e < a <  1 
            that is a power of 2 there exists some 8 ^ e  (also a power of 2) such that  Client 
            has a winning strategy in the game with parameters £, 8, a, a, к and an arbitrary 
            unavailable set  A on  the tree that  has  height  4(к — 1)  and  an  infinite  (or  large 
            enough) branching factor.
                The induction base (k = 1) is obvious.  For the induction step, we assume that 
            the statement is true for some k, and prove it for k +1/2.  We use the strategy from 
            Example 3 on the tree of height 4(k + 1/2 — 1), now applying the ^-amplification 
            strategy (induction assumption) for the grandsons of the root  (the subtree height 
            is exactly 4(к — 1) there).  The value of a for these games is chosen as a power of 2 
            in the interval (e/(6k),e/(3k)].  (Since the upper bound in this interval is twice as 
            big as the lower bound, it contains some power of 2.)  Each recursive call increases 
            the root request at least by e/(6k2),  so the number of recursive calls is bounded
                4In fact we need some additional steps to finish the proof of Gåcs’ result.  We have a strategy 
            for an arbitrarily large amplification k,  but what we need is a strategy with an arbitrarily large 
            ratio  ß/a:  we  need  the  grayed  area  to  be  more  than  1  and  the  request  at  most  l/d.  Such  a 
            strategy can be easily constructed as a composition.  For example, let us apply the strategies with 
            amplification к — 2d and a = 1/(4d) for the sons of the root until the total request becomes greater 
            than l/d — l/(4d).  We get a strategy with parameters a = l/d and ß = 2d(l/d — l/(4d)) = 3/2 
            for the tree of size O(d), which is enough.
                                         5.5.  MONOTONE COMPLEXITY                              141
             by 6mk2.  So we are able to choose in advance the parameters e and Ö for all the 
             recursive calls.  Also we can bound the number of grandsons of the root used in this 
             process:  it is enough to have Qk2 sons for each son of the root.
                 When all the sons of the root have their reserved intervals, we achieve our goal. 
             Indeed,  let  7  be  the  sum  of requests  made during non-last  recursive  calls.  This 
             gives us a grayed area of size kj outside the reserved intervals, so in total we get at 
             least kj + me for the grayed area while making requests for at most j  + m(e/(3k)). 
             Since 7 ^ me, we get к + 1/2 amplification factor
                                      kj + me ^ (k + 1/2) (7 + me/Зк)
             (a simple computation).
                 This strategy works for infinite branching and for large enough finite branch­
             ing (depending on e, a, k)—but the required branching factor is much larger than 
             needed for Theorem 88.  Let us explain why this happens.  The tree for given e, a, к 
             has branching factor a/e at the root.  In the sons of the root the branching factor 
             is  small  enough not  to  be  a problem,  but  we  should  look at  the  grandsons.  To 
             estimate the branching factor there, we need to bound the ratio a!/e' for the pa­
             rameters of the recursive calls made for the grandsons.  The parameter a' is about 
             ej(3k) and is the same for all the calls, but the parameter e' is different for different 
             calls.  The minimal e' corresponds to the chain of (a/e)6k2 application of the e    Ö 
             transformation from the  induction assumption,  and  it  is  much smaller than the 
             original e.  It  means that the branching factor for the grandson that is processed 
             first should be very large (in fact, we do not know which of the grandsons will be 
             processed first, so we need this large branching for the “oldest” son of every son).5
                 So the problem with our strategy is that it makes too many recursive calls.  It 
             turns out that 0(k2) recursive calls (instead of 0(k2a/e))  are enough if we use a 
             more clever strategy.  It is important that for this strategy the number of calls does 
             not depend on a/e.
                 Here we discuss the modification of the induction step.  Now the  (к + 1/2)- 
             strategy processes all the sons of the root that do not have reserved intervals yet, in 
             parallel (and not sequentially, as we did before).  More precisely, at each iteration we 
             consider all the sons that do not have reserved intervals, we choose one unprocessed 
             son for each of them,  and we process these sons  (who are grandsons of the root) 
             together, making a recursive call.  This means that the format of the game is now 
             changed:  it is played not for one tree,  but for a family of identical disjoint trees. 
             (Server should  provide  disjoint  intervals  for  vertices  that  are  in  different  trees.) 
             This modification alone is still not enough:  it may happen that for each iteration 
             only one son of the root does not have a reserved interval.  In this case there is no
                 5To get a bound for the branching factor for grandsons, we need to bound the ratio e/8 in 
             the strategy by some function fk(a/e).  The value fk+i/2 (a/e) is a product of (a/e)6k2 values of 
             the form fk{a'/e').  Here a!fe'  are different:  the first e'  can be a!, but the following ones should 
             be much smaller:  the second e' should be /*.( 1) times smaller than the first one, the third should 
             be fk(fk( 1))  times  smaller  than  the  second,  etc.  The  last  term  in  the  product  is  obtained  by 
             (a/e)6k2  iterations of fk  starting with 1.  Therefore,
                               fk+iM<*/e) « ЗА: ■ /*,(1) ■ / fc(/fc(l)) • fk(fk(fk(m  
             the product has  (a/e)6k2  factors, and the equation is only approximate since the first e'  is only 
             close to e/(3k).  And we can start, say, with / 3/2 (<*/£) — 2 (see Example 2).  Then /2  (а/e) grows 
             exponentially as а/e increases:  /2  (а/e) ^ 2a/£.  And /2 ,5  (a/e) is a tower of exponents with base 
             2  and  height  а/e.  (One  could  use  the  strategy  from  Example  3  and  increase  к  by  (almost)  1 
             during the induction step, but this would only slightly postpone the problem.)
                         5.  MONOTONE COMPLEXITY
        142
        real parallelism.  To avoid this problem, we should not wait until all the sons  (of 
        all the roots—now we have several trees) have reserved intervals; it is enough for 
        us if sons with reserved intervals form a large enough fraction.  The threshold for 
        “large enough”  should be greater than 1/2 (if we want a 1/2-increase in к); let us 
        use, say,  3/4 as the threshold.  This implies some loss:  the value of ß is now only 
        (3/4)a; in its turn, this makes the lower bound for the sum of requests for all the 
        roots smaller, only 3/4 of the old one, and we need slightly more iterations to get 
        the reserved intervals.  The final adjustment is now done as follows:  If the average 
        request for the tree roots is less than  (3/4)ot/k,  we increase some of the requests 
        that are less than a, to get the average (3/4) a/A:.
          As before, if the request of some son и of the root is so close to e that we may 
        cross threshold e while processing one more of its sons, we just increase the request 
        of и up to e.  It should be done in the same way, before the next recursive call.
          In this way the grayed area will be of size Ary + (3/4)me instead of kj + me, 
        as it was earlier  (here 7 is a grayed area that is due to grandsons who are not the 
        last processed among their siblings, and m is the total number of sons of the roots 
        of all trees), and the sum of requests is the same as before, j  + me/Зк.  Recall that 
        e/(3k) has appeared here as the a parameter for recursive calls.  To get the ratio 
        (grayed)/(requested) at least к + 1/2, we need to decrease slightly this parameter, 
        and e/(6 k) will be enough.  This makes the number of iterations twice as big, but 
        this is not a problem.  There are more details:  At each step the sum of the requests 
        of all sons will increase by a quantity that is proportional to me/к2.  The sum of 
        the requests of all sons cannot exceed me, therefore the number of recursive calls 
        is bounded by 0 (k2).
          Now let us provide the details.  First we should explain what changes are needed 
        in the definition of the game and the construction of a winning strategy.  Now the 
        game, in addition to k, e, ö, a, ß,  the tree T,  and the unavailable set A,  has an 
        integer parameter I, the number of trees.  The meaning of e, ö, A remains the same 
        as before,  a  is  the  upper  bound for the request  of each  root,  and ß is  the lower 
        bound for the average newly grayed area (per tree):  the total size of the grayed 
        area should be at least Iß.  Finally, the parameter к is the lower bound for the ratio 
        (total grayed area)/(sum of the root requests for all trees).
          As before, we may compose the strategies; however, now the composed strate­
        gies should have the same values of к, T, a.  We can apply first the strategy with 
        parameters e\,5\,a, ß\,k and some unavailable set A to some family of trees (each 
        tree is isomorphic to T).  Then we apply the strategy again with new parameters 
        £2 , Ö2 , a, /02; к and a*new unavailable set (the union of A and the £1-neighborhood of 
        the allocated area) for the second (disjoint) family of trees isomorphic to T. We as­
        sume that £1 = 62.  In this way we win the game with parameters £2,5i, a, ß\ +ß2, к 
        and unavailable set A.
          As before, we assume that к ^  1 is a multiple of 1/2 and use induction over 
        к to construct a winning strategy for Client for every e ^ a ^ 1, ß = (3/4)a and 
        for some 5 ^ e and some finite tree of height 4(A: — 1); its branching factor will be 
        specified later.  The numbers e, a, 5 are all negative powers of 2.  The strategy wins 
        the game for every I and A.
          The induction base (к — 1) is obvious.  Let us consider the induction step from 
        к to к + 1/2.  The tree,  as before, has aje sons of the root and each of them has 
        0(k2) sons (the exact value will be specified later); recall that now we have a family
                       5.5.  MONOTONE COMPLEXITY      143
       of isomorphic trees.  The strategy at each iteration does the following.  First of all, 
       we increase up to e the requests for those sons of the roots whose request already 
       exceeds e — e/6k.  After Server’s move we check how many sons of the roots do 
       not have a reserved interval yet.  If more that 25% of them do not have a reserved 
       interval, for each son that does not have a reserved interval we select an unprocessed 
       son (who is a grandson of one of the roots).  For this family of grandsons we perform 
       a recursive call of the strategy with parameter k, and a is chosen as a power of 2 in 
       the interval  (e/(12k),e/(6k)].  We repeat this procedure until the fraction of sons 
       that do not have a reserved interval becomes less than 25%.
          For every recursive call the sum of all requests increases by (3/4)m(e/(12fc2)) 
       or  more,  where  m  is  a  total  number  of sons  of all  roots.  And  the  sum  of the 
       requests for all roots cannot exceed me:  we guarantee that every son of every root 
       has request at most e (after that the reserved interval is guaranteed).  So the total 
       number of recursive calls is at most 0(k2).  Moreover, each root has request at most 
       a, since each root has a/e sons.  The total size of the newly grayed area is at least 
       the total length of all the reserved intervals, and it is at least (3/4)me, as required. 
       It remains to estimate the ratio (newly grayed area)/(total sum of requests).
          Let 7 be the sum of increases for root  requests,  if we do not  count  the last 
       increases that created reserved intervals.  This increase creates a k'y increase in the 
       grayed area, if we do not count the reserved intervals.  In total we obtain a grayed 
       area of size  к7 + (3/4)me  by  making requests  at  most  for 7 + m(e/6k).  Since 
       7 ^ me, the first number is at least к + 1/2 times greater than the second one:
                    k'y + (3/4)me ^ (к + l/2)(7 + me/6k)
       (a simple computation).
          It remains to bound the branching factor of the tree T needed for this construc­
       tion by a function of к and the ratio a/e (it is easy to see that only the ratio of these 
       numbers is important).  As we have discussed (after constructing of a strategy for 
       infinite trees), we should first compare <5 and e and prove that one can use Ö — e/c^ 
       for some sequence Ck that does not grow too fast.
          For к — 1 we had <5 = e, so C\  = 1.  The strategy for к + 1/2 makes at most 
       0(k2)  recursive  calls  of ^-strategy,  and  its  e/<5-ratio  is  the  product  of the  same 
       ratios for recursive calls, multiplied by the ratio e/e', where e is the parameter of 
       the game and s' is the similar parameter for the last recursive call.  The latter ratio 
       does not exceed 12A:, because the last call is made with parameter a' that is at least 
       e/(12k), and we can use the same value of e1.  So we get a recurrent formula
                         Cfc+1/2 = 0{kc°(k }),
       which gives Cfc = 2°(fe^4(A  1}.
          Now it is easy to bound the branching factor for the tree T by a function of к 
       and a/e.  Recall that in the construction of (k + l/2)-strategy we used a tree with 
       branching factor a/e in the root and 0(k2) in the sons of the root.  So at all odd 
       levels  (the root  level is 0)  the branching factor is 0(k2)\  it remains to bound the 
       branching on levels 2,4,....  At level 2 the branching factor is again equal to the 
       ratio a!/s' of the parameters of the strategies used.  It is easy to see that this ratio 
       does not depend on the original a and e.  Indeed, calling the strategy recursively 
       for the grandsons, we use the value of a' that does not exceed e/ (6k) and the value 
       of e'  that is at  least e/c*.+1/2-  Therefore, the branching factors for the grandsons
                                          5.  MONOTONE COMPLEXITY
             144
             are bounded by 6 kck+i/2 ■  The same is true for all other even levels (with a smaller 
             value of k).
                 Therefore, the ^-strategy wins on the tree with branching factor
                       max{a/£. 0 (k2), 6 (k — 1/2)с&} = тах{а/г, 0 (k2), 2 ° ^  (    }};
             here the second term (for large к) is dominated by the third one, and we can ignore 
             it.
                 Now we can finish the proof of the Gacs-Day result.  Let d be a power of 2. 
             We need to construct  a strategy with parameters a = 1 fd and some ß  >  1 that 
             wins on some tree T with a bounded depth and branching factor.  For that,  we 
             call the strategy constructed above (sequentially for each of the sons of the root) 
             with parameters к — 2d and a = l/(4d) until the request for the root reaches the 
             dangerous level Ifd — l/(4d).  In this way we request at most 1 fd, and the grayed 
             area is  at  least  2 d(l/d — 1/(4d)) >  1.  After each  call the root  request  increases 
             at least by (3/4)(l/4d)(l/к), so the number of requests is bounded by 0(d3), and 
             0(d3) sons of the root are enough.  The subtree rooted there should be suitable for
             a 2d-strategy with parameters a = l/(4d) and £ = (1/(4d))/c^d К  This requires 
             trees of height 0 (d) and the branching factor
                                     max{C^ 3\2°W 4(fc-1,} = 2°(d)0(d\
             This finishes the proof of the Gåcs-Day result.                                     □
                  139 Prove that the height of the tree in the Gacs-Day theorem cannot be 
             less that df4:  if it is smaller, Server has a winning strategy (instead of Client).
                 Returning to the gap between KM and KA, we observe that the upper and 
             lower bounds are still significantly different:  the only upper bound known says that 
             the gap is at most O(logn) for n-bit strings (and this is true even for К instead of 
             KM).  One small improvement is that we can replace n by К A (x), as the following 
             problem shows.
                  140 Prove that KM(x) < KA (x) + 0(log KA (x)).
                 (Hint:  In fact KM(x | KA (x)) < KA (x) + 0(1).  Indeed, if KA (x) = к, then x 
             at some point appears in the growing subtree of strings whose a priori complexity 
             is  less  than  к + 1.  This  tree  at  all  times  has  width  (the  cardinality  of maximal 
             antichain) at most 2k+1, so looking at the maximal elements of this tree, we cover 
             it  by 2k+ 1  growing branches.  For details see Theorem 127, p.  194.)
                                       5.6.  Levin—Schnorr theorem
                 The definition of the a priori complexity guarantees that for any lower semi- 
             computable semimeasure p the inequality К A (x) < — logp(x) + c holds for some c 
             and for every x.  It turns out that if p is a (computable) measure, then this inequal­
             ity  is  true  not  only  for  a priori  complexity  KA  but also for a  (larger)  monotone 
             complexity KM.
                 Theorem  89.  Let p  be  a computable probability distribution on O,  and let p 
             be the corresponding function on binary strings: p(x) = p(Q.x).  Then there exists a 
             constant c such that
                                           KM(x) < — logp(:r) + c
            for every string x.
                                     5.6.  LEVIN-SCHNORR THEOREM                        145
                                          p( o)                 P( 1)
                               P( oo)         p(01)        p(10)  p( 11)
                                  Figure  12.  The construction of ^
                Proof.  The  idea of the proof can  be  explained  as  follows.  The  difference 
            between KM and KA  appears because we are unable to allocate contiguous space 
            to hierarchical users’ requests, since we do not know which of the current requests 
            will increase in the future.  However, if we have a measure (and not a semimeasure), 
            we can solve this problem and allocate contiguous intervals.  (Feel free to ignore 
            this metaphor if it is confusing:  we provide a formal proof in the next paragraphs.)
                For each string x we define an interval ttx inside [0,1].  The interval ttx is defined 
            in such a way that:
                   •  the length of тгх equals p(x);
                  •  7Гд = [0,1]  (here A is the empty string);
                  •  for each string x the interval nx  is split by some its point  into intervals 
                     ttxq  (left part) and irx\  (right part)
            (see Figure 12).
                We consider also another family of intervals that corresponds to the uniform 
            measure.  Let Ix be the interval of reals whose binary representation starts with x. 
            We call the intervals Ix  binary intervals.
                Now consider the set G of all pairs (x, y) of strings such that (binary) interval 
            Ix  is located inside the interior of 7ry.  The set G is enumerable.  Indeed, since the 
            function p is computable, we can find the endpoints of intervals iry with arbitrary 
            precision, and if they are strictly greater (or less) than some rational number, this 
            fact will be discovered eventually.
                Note also that the property  (x,y)  G  G remains true if we replace x by some 
            extension (since Ix becomes smaller) or replace y by any prefix (since iry becomes 
            larger).  If (x,y\)  G  G and  (x,y2)  G  G, the segments iryi  and тгУ2  have a common 
            interior point (they both contain Ix), therefore the strings y\ and У2 are compatible. 
            So Theorem 81  (p.  127) guarantees that there exists a computable mapping of E 
            into  itself whose  lower  graph  is  G.  We  use  this  mapping  as  the  decompressor 
            in the definition of monotone complexity.  Then KMo(y) equals the minus binary 
            logarithm of the biggest binary interval that is located strictly inside iry.  It remains 
            for us to note that any open interval of length h contains a closed binary interval 
            of length h/4 and to compare D with the optimal decompressor.                □
               1141  Prove the claim about binary intervals (see above).
                (Hint:  Let и be a power of 2 such that  h/4  <  и  <  h/2. Then any interval
            of length  h intersects  at  least  three consecutive binary  intervals  of length и  and 
            contains the middle one.)
               Theorem 89 provides a theoretical justification for the following approach used 
            by Kolmogorov and his students to get upper bounds for the complexity of Russian 
            texts.  While reading the text  (one letter at a time), the reader tries to guess the 
            next letter.  The guess is formulated as a probability distribution over the alphabet.
        146              5.  MONOTONE COMPLEXITY
        Then the next letter is read and we add — logp to the complexity, where p is the 
        declared probability of that letter (i.e., its probability with respect to the guessed 
        distribution).
          If we believe that the behavior of the reader is computable,  the result  is an 
        upper  bound  for  the  complexity.  Indeed,  the  reader  provides  (some  part  of)  a 
        computable  probability  distribution  on  the  set  of strings  telling  the  conditional 
        probabilities along some path, and the complexity of text does not exceed the sum 
        of negative logarithms of these probabilities (Theorem 89).
          Of course,  it  is not  practical to require that the reader provides at each step 
        the  list  of probabilities  for  all  the  letters;  one  can  suggest  some  standard  types 
        of answers such as  “the next letter is A with probability 0.5,  all other vowels are 
        équiprobable  and  have  total  probability  0.3,  all  other  letters  are  équiprobable”. 
        Note also that we get an upper bound for the conditional complexity of the text 
        where the condition is the background of the reader.  (For example, if the reader 
        knows the text by heart or is just familiar with the author’s writings, the bound 
        can be very small.)
          The same trick used in compression algorithms is called arithmetic coding and 
        was even patented (many years after Kolmogorov’s experiments in the 1970s).
          Now we are ready to formulate the criterion of Martin-Löf randomness that 
        uses monotone complexity:  a sequence is ML-random if and only if the inequality 
        of Theorem 89 becomes an equality for its prefixes.
          Let us formulate this statement precisely.  Let /i be a computable probability 
        distribution on the set Q of all infinite bit sequences, and let p(x) be the measure 
        of the interval Q,x:  p(x) — p>(Q,x).
          Theorem 90 (Levin-Schnorr).  A sequence со e Q, is ML-random with respect 
        to  a computable probability distribution /i if and only if
                          — log p(x) — KM(x) ^ c 
        for some c and for every prefix x of со.
          Proof.  We have to prove this theorem in both directions.  Let us show first 
        that if (for a given sequence со)  the difference — logp(x) — KM(x)  is unbounded, 
        then this sequence is not ML-random (i.e., the set {cu} is an effectively null set).
          Fix some constant c and consider all strings x such that — logp(x) — KM(x) > c. 
        (This difference is sometimes called randomness deficiency, but this term has differ­
        ent meanings.  We have already used it in the previous chapter, and in Chapter 14 
        it  is used in a different way.)  This set is denoted by Dc.
          The set Dc is enumerable (since p is computable and KM is upper semicom- 
        putable, the difference is lower semicomputable).
          Lemma  1.  The  set  of all  infinite  sequences  that  have  a prefix  in  Dc  has /i- 
        measure at most 2~c.
          Informally speaking, this is true because on this set the measure /i is 2C times 
        smaller  than  the  a  priori  probability  (and  the  latter  does  not  exceed  1).  More 
        formally this argument can be explained as follows.
          We are interested in the measure of the union of intervals flx  for all x  G Dc. 
        Without changing this union, we may keep only minimal x G Dc (i.e., strings x e Dc 
        such that no prefix of x belongs to Dc).  Let xq,x\, ... be these minimal elements
                          5.6.  LEVIN-SCHNORR THEOREM         147
        of Dc.  (We do not claim the the set of minimal elements is enumerable,  so this 
        sequence may be non-computable.)
           For each X{ consider the minimal description pi  (according to the definition of 
        the monotone complexity:  Xi ^ D(pi) where D\ £ —> £ is the optimal monotone 
        decompressor).  Then l(pi) — KM(xi) < — logp{xf) — c.  Moreover, no pi is a prefix 
        of another one (otherwise, the corresponding Xi would be compatible).  Therefore 
        Zi2~liPi)  <  1  (being  the  sum  of uniform  measures  of disjoint  sets  f2Pi).  The 
        corresponding p(xi) are 2C times smaller, so we get the statement of Lemma 1.
           Our assumption guarantees that the sequence со has prefixes from Dc for every c. 
        To prove that  {ca} is an effectively null set, we need to cover со by an enumerable 
        family of intervals with total measure not exceeding 2~c, and we can use intervals 
        from Dc.
           However, there is a small technical problem here (that we already encountered 
        while speaking about randomness tests).  We know that for intervals from Dc the 
        total  measure  (i.e.,  the  measure of their union)  does  not  exceed  2~c  (as  Lemma 
        1  says),  but  the  definition  needs  that  the  sum  of measures  of all  intervals  does 
        not exceed 2~c.  We cannot solve this problem by considering only minimal points 
        (maximal  intervals),  since  the  set  of minimal  points  is  not  always  enumerable. 
        Instead we can use the following statement:
           Lemma 2.  Every enumerable set of strings xo,Xi,...  can be transformed into 
        an enumerable set of incompatible strings with the same union (J^ QXi.  This trans­
        formation is effective (an algorithm that enumerates the first set can be transformed 
        into an algorithm that enumerates the second one).
           Indeed,  if during the enumeration we get a string that is an extension of the 
        previously  enumerated  one,  this  string  can  be  omitted  (since  the  corresponding 
        interval is already covered).  If we get a string у that is a (proper) prefix of a string 
        X enumerated earlier, we have to split the difference fly \ Qx into a finite number 
        of disjoint intervals and replace у by strings that define those intervals.  Lemma 2 
        is proven.
           Applying Lemma 2, we get an enumerable set of incompatible strings;  these 
        strings may be not in Dc, but this is not important.  It is enough to know that they 
        correspond to disjoint intervals that cover со, and the union of these intervals has 
        /r-measure at most 2~c, according to Lemma 1.
           Proving the converse implication, we need to show that if a sequence со belongs 
        to  an  effectively null set,  then the differences between the negative logarithms of 
        the measure and the monotone complexity of ca-prefixes are unbounded.  The idea 
        of this construction may be explained as follows:  given a set of small measure, we 
        construct a monotone decompressor that treats favorably the elements of this set 
        (i.e.,  provides short descriptions for their prefixes).
           Let  us  provide  details  now.  Assume that  со  belongs  to  a set  U  which  is  an 
        effectively null set  (with respect to measure p).  For each c we can effectively find 
        a family of intervals  f2Xo, QXl,...  that  cover  U  (and  therefore со)  and  have  total 
        measure less  than 2~c.  If we multiply  the measures of all these intervals by  2C, 
        the  sum  is  still  less  than  1.  Consider  the  computable  sequence pi  —  2cp(flXi). 
        Applying Theorem  59  (p.  96),  we  get  a prefix-free  decompressor  for  which  the 
        prefix complexity of i does not exceed — logyu(S7Xi) — c + 2.  A composition of this 
        decompressor and the computable mapping i (->• Xi is a prefix-free decompressor Dc
             148                         5.  MONOTONE COMPLEXITY
             such that
                                      k 'dc (xi)  ^   -  log fj.(üxi ) -  c +  2.
             (The subscript  c in Dc  is  used to  stress  that  the  construction depends  on  c;  we 
             use prefix-free decompressors since it will be useful later.)  Monotone complexity 
             does not exceed the prefix one, so if the difference between the negative logarithm 
             of the measure and the prefix complexity is large, the same is true for monotone 
             complexity.  It  remains  to  combine the decompressors  Dc  into one decompressor 
             (not depending on c).
                 We use the same trick that was was used in the construction of an optimal 
            decompressor.  We want the string cu to be the description of the string v if и is a 
            description of v with respect to Dc.  Here c is a self-delimited encoding of length 
            O(logc) for a natural number c.  If the decompressor D is constructed in this way, 
            the following inequality holds (for all c):
                                   K'D{xi) < -\ogp(ttx.) -  c + O(logc).
            Since the monotone complexity does not exceed the prefix one, we replace K'D(xi) 
            by KM(xi) and conclude that all the strings Xi  (for a given c) have the difference 
            between — logp{xi) and KM(xi) at least c—O(logc).  If an infinite sequence belongs 
            to С/, it has a prefix of this type for any c, therefore the difference is unbounded for 
            its prefixes.
                 The Levin-Schnorr theorem is proven.                                         □
                 142  Show that in the first part of the proof (if a difference is unbounded, the 
            sequence belongs to an effectively null set) it is enough to have P upper semicom- 
            putable, while in the second part it is enough to have P lower semicomputable.
                In fact the proof gives us a bit more than we claimed.  Here are several modifi­
            cations of the Levin-Schnorr theorem that can be extracted from it:
                 Theorem 91.  We may replace the monotone complexity KM(x) by the a priori 
            complexity К A (x)  in the statement of the previous theorem.
                 P r o o f.  The  a priori  complexity  does  not  exceed  the  monotone  one,  so  the 
            difference may only increase.  So we need to change only the first part of the proof. 
            It  is  easy:  in the proof of Lemma 1 we should note that     2  KA(xi)  ^  -ц  since
            this sum is the sum of the a priori measures of disjoint intervals QXi.           □
                Theorem  92.  We  can  also  replace  the  monotone  complexity KM(x)  by  the 
            prefix complexity K(x).
                PROOF.  Here we go in the other direction and increase complexity, so only the 
            second part of the proof needs to be redone.  And this is trivial—recall that in fact 
            we got just an upper bound for prefix complexity.                                 □
                Theorem 92 is nowadays the most popular version of the Levin-Schnorr ran­
            domness criterion (see, e.g.,  [103] ; see [18] about the history of these results).
                The use of monotone or a priori complexity seems  (at  least  to  the  authors) 
            more  natural  (though  the  prefix  version  has  its  own  advantages;  see  below  the 
            formula for the randomness deficiency in terms of prefix complexity).  Note that if 
            we use prefix complexity, the difference in the Levin-Schnorr theorem can become 
            negative.  For example, in the case of the uniform measure — log/^flz) is just the 
            length of string x,  and the prefix complexity may be greater than the length (the 
            difference can be of order logn; see Theorem 63, p.  100).
                                        5.6.  LEVIN-SCHNORR THEOREM                           149
                 Moreover,  the  use  of the  monotone  complexity  allows  us  to  strengthen  the 
             Levin-Schnorr theorem as follows:
                 Theorem 93.  If a sequence со is not random with respect to measure p,  then 
             the  difference  — logp(x) — KM(x) for prefixes x  (of со)  is not only unbounded but 
             also tends to infinity.
                 P r o o f.  In the proof of Theorem 90 we constructed a prefix-free decompressor 
             that  provides short  descriptions pi  for strings  X{  and  guarantees  that  the prefix 
             complexity of Xi (with respect to this decompressor) does not exceed — log p(Qx.) — 
             c.  To get the required bound for monotone complexity, we may use (for each i) the 
             extensions of pi as descriptions of the extensions of Xi in such a way that the length 
             of the descriptions corresponds to the measure of described strings, as was done in 
            the proof of Theorem 89 (p.  144).
                 More formally, we can use the inequality KM(xy) ^ K(x) + KM(y\x)  (Prob­
             lem 135) and the relativized version of Theorem 89:  KM(y\x) ^ — logpx(fly) for 
             any computable family of measures that  (computably)  depends on parameter x. 
            Here px is the measure that is concentrated on the set ftx and is defined as follows:
            px(^ly) -- ^(S^xy) / piff^x)'
                 For  the  case  of uniform  measure  (where  — logp(Qx)  =  l(x)),  we  can  use  a 
            simpler argument and say that piz is a description of xiz for any string z.        □
                 This result can be reformulated as follows:  if the difference logp(x) — KM(x) 
            is uniformly bounded for infinitely many prefixes x of some sequence со, then со is 
            random.  For the prefix version, our argument does not work, but we still can prove 
            a weaker statement for computable sequences of lengths.
                  143 Let A be a decidable infinite set of natural numbers (lengths), and let со 
            be some sequence.  If K(x) ^ — logp(Qx) — c for some c and for every prefix x of 
            со with length in A, then со is random.
                 (Hint:  In the proofs of Theorems 90 and 92,  we can split  the intervals  into 
            parts to get the desired length.)
                 We provided some arguments in favor of using monotone complexity in the 
            randomness criterion.  However, a version that uses prefix complexity has its own 
            advantages.  Note that  the notion of an ML-random sequence is invariant  under 
            computable permutations of indices (if the measure is invariant or is changed ac­
            cording to the permutation), but the notion of a prefix (and therefore the criterion 
            of randomness in terms of prefixes)  is not.  As it was noted by A.  Rumyantsev, 
            using К one can get an invariant criterion of ML-randomness.
                 Let F be a finite set of indices  (natural numbers),  and let w be a binary se­
            quence.  By co(F)  we  denote the  restriction of со  onto  F,  i.e.,  the  binary  string 
            formed by bits coi such that г G F (in the same order as inw).
                Let p be a computable measure on Q.  For every finite set F C  N and string 
            Z whose length equals the cardinality of F, we consider the event co(F) = Z.  Its 
            /^-probability is denoted by pf,z-
                 144 Let a; be an ML-random sequence with respect to p.  Prove that
                                      K(F,w(F)) > -  logPfmf) ~ c
            for some c and for all finite F.
                                       5.  MONOTONE COMPLEXITY
            150
                (Hint:  The measure of the set of all sequences for which this inequality does 
            not hold for some fixed c, does not exceed 2~c multiplied by the sum of the a priori 
            probabilities of all pairs F, Z, and therefore does not exceed 2~c.)
                (Note that if F is an initial segment of N, then F is determined by w(F) and 
            can be eliminated, so we return to the previous statement.)
                In fact, the condition given by the last problem is also sufficient.  Moreover, it 
            is enough to require this inequality for any increasing computable sequence of finite 
            sets whose union is N.
                 145 Let Fo  C Fi C F2  C  • • •  be a computable sequence of finite sets and
            (J{F{ — N.  Assume that for some sequence u> we have
                                    K{Fi,uj(Fi)) ^   logßFi,u>(Fi) -  c
            for some c and for all i.  Then oj is ML-random with respect to p.
                (Hint :  Using permutation of indices, we may assume that F* are initial segments 
            of N.  Then  we  refer  to  Problem  143:  it  is  enough  to  repeat  the  proof of the 
            Levin-Schnorr theorem using only strings of appropriate lengths and splitting other 
            intervals into unions of appropriate intervals.)
                This statement implies, for example, that a two-dimensional bit sequence (i.e., 
            a mapping Z2  —>  {0,1})  is ML-random with respect to the uniform measure (all 
            bits  are  independent;  0  and  1  are équiprobable)  if and only if an  N x N square 
            centered at the origin has prefix complexity at least N2 — 0(1)  (for all odd N).
                Let us note one more reason that makes the appearance of prefix complexity in 
            the randomness criterion natural.  It turns out that one can prove a quantitative ver­
            sion of the Levin-Schnorr theorem and get a formula for the expectation-bounded 
            randomness deficiency (see Section 3.5):
                 146 Let p(x) = ß{£lx) correspond to a computable measure p on the Cantor 
            space.  Prove that the function
                                                       m{x) 
                                                  X=4U! p{x)  ’
            where the sum is  taken over  all  finite  prefixes  x  of oj  and  m{x)  is  the  discrete 
            a priori probability of x, is a universal expectation-bounded randomness test.
                {Hint-.  A lower semicomputable function on the Cantor space is a sum of char­
            acteristic  functions of intervals with non-negative coefficients.  When a new term 
            is added to this sum (for interval Q.x with coefficient r), we may imagine that the 
            “weight” of the vertex x of the binary tree increases by r.  The weights of all vertices 
            form a lower semicomputable function и on strings, and the expectation condition 
            for a test corresponds to the inequality ^ZXP(X)U(X)  ^  L  The maximal function 
            with this property is m(x)/p(x)  up to a ©(l)-factor.  One should also agree that 
            m(x)/p(x) is infinite if p(x) = 0 for some string x.)
                 147 I Prove  that  the  sum  in  the  preceding problem  can  be replaced by the 
            supremum, and thus we obtain a quantitative version of the Levin-Schnorr theorem 
            with prefix complexity.  For example, for the case of uniform measure, the expecta­
            tion-bounded randomness deficiency is equal to supn[n — K (ojq ■ ■ -uon- 1)].
                {Hint:  A lower semicomputable function that is equal to a inside some effec­
            tively open set and is equal to zero outside it can be represented by means of weights
                                        5.6.  LEVIN-SCHNORR THEOREM                             151
             that are equal to a and are placed in incompatible vertices.  Every lower semicom- 
             putable function can be represented up to a 0 (l)-factor as the sum t(oj) —    tk{u>),
             where tj.c(w) = 2k if t(ui) > 2k and tk(oo) = 0 otherwise.  If all tk are represented as 
             explained above, all the summands in the formula for the deficiency are powers of 
             two.  Then the sum equals the supremum up to a 0(l)-factor.  See [13] for details.)
                 The statement of the last problem was proved in an old paper by Gåcs [56].  The 
             proof gives as a byproduct the statement of Problem 146 rediscovered independently 
             in a more recent paper by J. Miller and L. Yu [123] under the name of “ample excess 
             lemma”.
                  148  (a)  Let  w  be  an  ML-random  sequence  with  respect  to  a  computable 
             measure p,,  and let p(x)  = p(£lx).  Prove that the difference — logp(x) — K(x)  is 
             not only bounded from above for prefixes of ш but also tends to —oo as the length 
             of the prefix increases.  In other words, if K(x) ^ — logp(æ) + c for some c and for 
             infinitely many prefixes x of cu, the cu is not ML-random.
                 (b) Prove that if K(x) < — logp(æ) + log/(x) + c for some c and for all prefixes 
             X  of oj,  then w  is not ML-random.
                 (Hint:  In both cases use the ample excess lemma, Problem 146.)
                 The case of uniform measure is rather important; let us write down all that we 
             have proven for this case:
                 Theorem 94.  (a)  Upper bound:
                                   KA (x) < KM(x) + 0(1) ^ l(x) + 0(1)
             for any string x.
                 (b)  Randomness  criterion:  the sequence ш  is  ML-random with respect to  the 
             uniform measure if and only if these inequalities become equalities for prefixes ofuj,
                                 KA ((w)n) = KM((w)n) + 0(1) = n + 0(1).
                 (c)  If и  is not ML-random with respect to uniform measure, then the difference 
             n — КМ((ш)п)  (and therefore n — KA ((w)n)  tends to infinity as n —>■ oo.
                 (d)  The sequence oo  is ML-random with respect to the uniform measure if and 
             only if К((ш)п) ^ n — c for some c and for all n.
                 (e)  The sequence oo  is ML-random with respect to the uniform measure if and 
             only if К (F,uj(F)) ^ IF I — c for some c and for all finite sets F.
                 Another version of the statement (d) is that a sequence ш is ML-random if and 
             only if the sum     2n_A'((a;)0  is finite (Problem 146).
                 For the case of uniform measure there exists one more criterion of Martin-Löf 
             randomness.  It is interesting since it uses only plain complexity (and not the prefix 
             or monotone versions).  It is a bit strange that this criterion was discovered only 
             recently (see [123]) since similar suggestions were considered at the end of the 1960s 
             (see  [225,  117]),  and the proof of this criterion uses only ideas and methods that 
             were well known at that time.
                 Theorem 95.  Assume that f: N —> N is a computable total function and the 
             series           converges.  Let ш  be  an ML-random sequence with respect to  the
             uniform measure.  Then
                                        C((w)n\n) ^ n -  f(n) -  0(1)
                                                 5.  MONOTONE COMPLEXITY
               152
               {i.e.,  there  exists c such that for every n  the inequality C((uj)n \ ri)  ^ n — f(n) — c 
               holds).
                    P r o o f.  Assume  that  the  claim  is  false.  This  means  that  for  every  c  there 
               exists an n such that
                                                C((uj)n\n) < n - f(n) -  c.
               In other words, for every c the sequence uj is covered by some interval Qx such that
                                                  C(x I n)  < n — f(n) — c,
               where n is the length of x.  For each n there are at most 2n~^n^~c intervals with 
               this  property and their total measure is at most  2~^n^2~c  (for a given n).  The 
               total measure of all such intervals (for all n) is
               and the sequence uj forms an effectively null set:  choosing an appropriate c, we get 
               a cover for uj  that has small measure.  Therefore, uj  is not ML-random.  (Note that 
               the sum of the series         2"^n)  may be a non-computable real number;  this does 
               not matter since we may use any upper bound for it.)                                             □
                    Remark.  In the proof we used only that / is upper semicomputable, so the 
               statement remains true for f(n) = K(n):  for every ML-random sequence uj  (with 
               respect to the uniform measure) we have
                                              C((cj)n \n)  ^  n -  K{n) -  0(1).
               As we will see in Theorem 98, this is a necessary and sufficient condition.
                    Theorem  95  implies,  for  example,  that  for  any  ML-random  sequence  (with 
               respect to the uniform measure) the plain complexity of its prefix of length n is at 
               least n — 2 log n — О ( 1 ) and even n — log n — 2 log log n — О ( 1 ), since the corresponding 
              series converge.
                    Making function / smaller, we make the claim of the theorem stronger.  It turns 
               out that for some / we get a randomness criterion in this way:
                    Theorem  96.  There  exists  a  total  computable function f:  N —» N  such  that 
               ^2n          <  oo  and having the following property:  if for some sequence uj  and for
               some c the inequality
                                                 C((uj)n\n)  ^  n -  f{n) -  c 
               holds for all n,  then uj  is ML-random with respect to the uniform measure.
                    PROOF.  We need to prove that every non-random sequence (i.e., every sequence 
              that belongs to the largest effectively null set) has simple prefixes.  Note that we 
              also need to choose the function /.
                   To explain how to do this, let us assume that we are given a family of intervals 
              with total measure at most e.  Let F be the set of strings that define these intervals 
               (i.e.,  the  family  consists  of intervals  flx  for  all  x  £  F).   Let  us  sort  strings  in 
              F  according to their length and for each length n  consider the total measure of 
              intervals that correspond to n-bit strings in F.  Let it be approximately equal to 
                       (we assume that / has integer values, so this cannot be done exactly, but 
              can be done up to factor 2 in both directions; for simplicity we ignore this bounded 
              factor in the sequel).  Then we have                        ^  £■  On the other hand,  the set
                                                                5.6.   LEVIN-SCHNORR THEOREM                                                           153
                    F contains                       strings of length n, and each of these strings can be described
                    (when n and other parameters of the construction are given) by n — f(n) bits.  This 
                    gives  an upper bound for the complexity of all the strings in F.  Note also that 
                    every infinite sequence that is covered by our intervals has a prefix in F.
                           Now we return to the proof.  Consider the largest effectively null set.  For each 
                    £ > 0 there exists its cover by intervals of total length at most e, and we can use the 
                    construction above to get the corresponding function /  with Y^,n 2“^ n*  ^ £•  We 
                    need to combine those functions for different e into one function /  as the theorem 
                    requires.  This is done as follows.
                           For  each  c  =  0,1,2,...,  consider  the  covering  by  a family  of intervals  with 
                    total  measure  not  exceeding  2~3c,  the  corresponding set  Fc  of strings,  and  the 
                    corresponding function /.  Then we decrease /  by 2c and obtain a function fc such 
                    that
                                                                          ^ 2 " /c(n) < 2~c
                                                                           П
                    (we get 2~c instead of 2"3c since we have decreased /  by 2c).  The set Fc contains 
                    2n-fc(n)-2c  b r in g s   of length n, and every non-random sequence has a prefix in Fc.
                           Then f{n) is defined by the equation
                                                                       2 ~f(n) —            2~fc(nK
                                                                                        C
                    This guarantees that
                                      E*■f(n)                                            J2J22~fc(n) <                            ^ L
                    On the other hand, the set Fc is enumerable given c (according to the definition of 
                    an effectively null set), so any element x of length n is determined (when n and c 
                    are known) by its ordinal number (in the enumeration of strings of length n in Fc), 
                    i.e.,  by n — fc{n) — 2c bits,
                                                           C(x I n, c) < n -  f c(n) -  2c + 0(1),
                    which implies
                                              C{x\n) < n — fein) — 2c + O(logc) < n — f(n) — c
                    for any x G Fc of length n (for large enough c).
                          Now let  Ш  be  any  non-random  sequence.  As  we  have  seen,  for  each  c  the 
                    sequence и has a prefix in Fc.  Let n be the length of this prefix.  Then
                                                                  C((w)n|n) < n - f[n) -  c
                    (assuming that c is large enough), which contradicts our assumption.
                          However, this does not complete the proof, since we need a computable function 
                    /, and the set Fc is only enumerable, so we do not know when all strings of length n 
                    have been appeared, and therefore cannot compute /.  To overcome this difficulty, 
                    recall that we started with a family of intervals (that cover the largest effectively null 
                    set).  In this covering we may split a large interval flz into many small intervals Qzt 
                    (for all strings t of some length).  This allows us to make fc computable if we require 
                    (without loss of generality)  that the length of the intervals in the enumeration of 
                    Fc can only increase.  The same argument can be applied to all fc in parallel and 
                    makes / computable.
         154                5.  MONOTONE COMPLEXITY
            Finally,  there is a (trivial)  technical problem:  the statement requires /  to be 
         integer valued, so some rounding is needed.            □
            The two last theorems together provide a randomness criterion that uses plain 
         complexity  (and  not  monotone  or  prefix  complexity).  This  criterion  is  robust: 
         one can replace the conditional complexity C((oo)n\n)  by the unconditional one, 
         C((co)n), or by a conditional prefix complexity, K((co)n \n).
            Indeed,  each  of these  replacements  only  increases complexity,  therefore only 
         Theorem 96 needs to be verified.  For the prefix complexity version, we use that for 
         each element x G A the inequality K(x\A)  ^ log |>4| + 0(1) holds (we consider a 
         prefix-free encoding by strings of length log |A|).
           The case of unconditional plain complexity is a bit more difficult.  As we do 
         not know n, we need to describe a string x € Fc,n (here FCyTl is the set of all strings 
         x E Fc that have length n) by its ordinal number in the entire set Fc  (and not by 
         its ordinal number in FCjTl  as before).  Enumerating Fc in increasing length order, 
         we need
                          log(|FC)o| + |-Fc,i| + • • • + |Fc,n|) 
         bits for that,  and this bound is enough if the last term  |FCjTl|  is greater than the 
         sum of all preceding terms (in this case the increase is at most twofold).  We can 
         achieve this using the same trick as before:  we replace a string by all its extensions 
         of some bigger length.  Note that this is done separately for each c, so the condition 
         c remains, but this does not matter since it gives only O (loge) additional bits.
           So we get the following result:
           Theorem 97.  A sequence u)  is ML-random if and only if for any computable 
         total function f: N —>■ N such that ^ 2~^n) < oo the inequality
                            0 ((w)n) ^ n -  f(n) -  0 (1)
         holds.
           This criterion uses only plain unconditional complexity and is the most popular 
         version of the Miller-Yu theorem.
           This criterion has a drawback:  there is a quantifier over /.  It can be placed 
         differently (there exists some /  that rejects all the non-random sequences, as The­
         orem 96 says),  but still it  would be nice to get  rid of /   completely.  It  is indeed 
         possible, but the price is that we have to reinsert prefix complexity into the state­
         ment:
           Theorem 98.  A sequence cj is ML-random with respect to the uniform measure 
         if and only if
                           C({cv)n) ^ n -  K(n) -  0(1).
           Proof.  If J2n 2“^ n) converges for a computable /, then K(n) ^ f(n) + 0(1). 
        Therefore the condition with prefix complexity is stronger than that in Theorem 97, 
        and thus we need to prove only the converse implication:  if for every c there exists 
        an n such that
                            C((w)n) < n -  K(n) -  c,
        then u) is not ML-random.  This can be done in the same way as in Theorem 95. 
        We need only note that the set of all strings x such that
                            C{x) < l(x) -  K{l{x)) -  c
                           5.6.  LEVIN-SCHNORR THEOREM          155
         (here l(x) stands for the length of x) is enumerable; see the remark after the proof 
         of this theorem on p. 152.                             □
           In this theorem we can also replace C((w)n) by C((uj)n\n).
            149 Verify that this is indeed possible, 
           This esult was proven in P. Gàcs paper [56, p. 391].
            150 Show that we cannot let f(n) = 2 log n in Theorem 96.
            (Hint:  Theorem 95 says that for an ML-random ui we have a stronger inequality 
         C((u)n) ^ n — logn — 2 log logn — 0(1).  Therefore, if we computably interleave a 
         random sequence with the zero sequence (and zeros are sparse enough), we get a 
         non-random sequence such that C((uj)n) > n — 2 logn — 0(1).  A similar argument 
         shows that we cannot get a computably convergent series 2“^ n*  for a function / 
         that makes Theorem 96 true.)
           All  the  results  above  still  do  not  answer  a  very  natural  question:  Can  one 
         eliminate /  completely and require that C((uj)n) ^ n — 0(1)  (similar to monotone 
         complexity criterion)?
           Of course, this would be the most natural version of the randomness criterion, 
         so it was tried in the very beginning.  Martin-Löf noticed that this approach does 
         not work:  any binary string is a substring of a random sequence, so any random 
         sequence contains arbitrarily large groups of zeros.  And if a string of length n ends 
        with к zeros, then its complexity is at most n — к + 2 log к + 0 (1)  (21ogfc bits are 
         needed for a prefix-free encoding of к and n — k bits for the rest), and the difference 
         between length and (plain) complexity is at least к — 2 log к — 0 (1).
           The following theorem  (see  [225,  117])  gives  a  more  precise  bound  for  the 
         unavoidable difference between length and  complexity  (we mentioned this result 
        earlier in Problem 54):
           Theorem 99.  There exists some c such that for any ui G Q the inequality
                             C((u)n) ^ n -  logn + c
         holds for infinitely many n.
           Proof.  For each n let us select (l/n)-th fraction of all strings of length n, i.e., 
         |_2n/nJ  strings  of length n.  We want to do this in such a way that each infinite 
        sequence  has  infinitely  many  selected prefixes  (and  the  set  of selected  strings  is 
        decidable).
           Why is this possible?  The series ^  1 /n diverges so we can split its terms into 
        infinitely many groups, and each group has sum greater than 1.  Using one group, 
        we get one layer of О-covering (this means that each sequence w £ 0 has a prefix 
        among the strings that correspond to that layer).  To do this, we consider the strings 
        in order of increasing length and select strings whose prefixes are not yet selected. 
         (There is  a rounding problem since 2n/n is not  an integer,  but  it  can be easily 
        fixed.)
           Every selected string of length n can be described (if n is known) by its ordinal 
        number, and this requires n — logn bits.  Therefore, the conditional complexity of 
        this string (with condition n) is at most n — logn + 0(1).  Moreover, if we make a 
        combined list of all selected strings (in the order of increasing length), the ordinal 
        number  increases  by  an  0(l)-factor.  Indeed,  the  number  of selected  strings  of 
        given length grows almost as a geometric sequence, and adding all selected strings
                                            5.  MONOTONE COMPLEXITY
              156
              of smaller lengths increases cardinality only by an 0(l)-factor.  This implies the 
             statement of Theorem 99.                                                                 □
                   151  Give another proof of this result using the following simple observation:
             the A;-bit prefix of a given sequence can be considered as a binary notation of some 
              integer N (we add 1 at the beginning of the prefix not to lose leading zeros), and 
              N bits following this prefix are enough to reconstruct all к + N bits.
                   152  Prove that the statement of Theorem 99 is true not only for some c but
             for every c (including the negative ones).
                  (Hint:  If the series             diverges,  we  can  increase  a bit  the  function  /
             keeping this property:  there exists a function g such that g(n) — f(n)  —>  oo and 
             £ 2~s(n) = oo.)
                   153 Show that the statement of Theorem 99 (the conditional complexity ver­
             sion) remains true if we replace the logarithm by an arbitrary computable function 
             /  such that the series             diverges.
                  Martin-Löf claims in [117] that the same generalization is possible for uncon­
             ditional complexity (and refers to an unpublished paper for the proof).  The same 
             statement (attributed to Martin-Löf) can be found also in [225].  (We do not know 
             how to prove it.)
                  Let us mention also that the statement of Theorem 95 has a slightly different 
             form in [117]:
                   154 Prove that if a sequence ш is ML-random with respect to the uniform
             measure and / : N —> N is a computable total function such that the series ^  2“^ n* 
             computably converges, then С((ш)п \ n) ^ n — f(n) for all sufficiently large n.
                  (Hint:  If a series computably converges,  and the inequality is false infinitely 
             many times, the tails of the series can be used to get covers that have small mea­
             sure.)
                  Another natural question follows:  What happens if we require high complexity 
             not for all (sufficiently long) prefixes but for infinitely many of them?  In the same 
             Martin-Löf paper [117] the following results are stated:
                   155  Prove  that  for  almost  all  (with  respect  to  the  uniform  measure)  se­
             quences и; € Cl there exists c such that C((cv)n \ n) ^ n — c for infinitely many n.
                  (Hint:  If it is not the case, then for every c there exists N such that an n-bit 
             prefix of u> has complexity less than n — c for every n > N.  For given c and N the 
             set  of all u) with this property has measure at most 2~c.  As N increases, this set 
             increases, and the union over all N has measure at most 2~c by continuity.)
                   156  If for a given sequence u> there exists c such that С((ш)п \ n) ^ n — c for 
             infinitely many n, then u> is ML-random with respect to the uniform measure.
                  (Hint:  If u)  is covered by some interval in a family of total measure less than 
             2~c, then every sufficiently long prefix of ш can be described (when length is given) 
             by its ordinal number in the set of all strings of this length covered by some interval, 
             and this requires 2 log c + n — c bits.)
                  1157 I Prove  that  the  statement  of the  previous  problem  remains  true  if we 
             replace conditional complexity С((ш)п\п) by unconditional complexity С((ш)п). 
                  (Hint:  Use Problem 6 or, better, Problem 55.)
                                 5.7.  THE  RANDOM  NUMBER О                157
              The last two problems refer to a set of measure 1 that is a subset of the set of all 
          ML-random sequences. Its complement is a null set; if it were an effectively null set, 
          we would get another criterion for ML-randomness.  However, this is not the case. 
          Recently in [121, 148] it was shown that this set has a natural description:  it is the 
          set of ML-random sequences relativized to oracle O'; these sequences are sometimes 
          called 2-random (while ML-random sequences are called 1-random).  See [16] for a 
          simple proof.  There is a similar criterion with prefix complexity:  a sequence u> is 
          2-random if and only if K((oj)n) < n + К (n) — c for some c and for infinitely many 
          n [124]  (see also [5] for a simple proof).
                               5.7.  The random number Q,
              The following theorem provides an interesting application of the randomness 
          criterion given in the previous section.  Let m be a maximal lower semicomputable 
          semimeasure on the set of natural numbers (e.g, let m(x) be equal to 2~K^X')\ we 
          can use also the distribution on the outputs of the universal probabilistic machine, 
          see Chapter 4).  Chaitin suggested considering the number
                                       Q, = y^m(n)
                                           П
          (the halting probability for the universal probabilistic machine; the sum of the max­
          imal lower semicomputable series) and made the following interesting observation:
              Theorem 100.  The binary representation of Г2 is ML-random with respect to 
          the uniform distribution.
             Note that the value of П depends of the choice of a maximal lower semicom­
          putable semimeasure, but the statement remains true for every choice.
              Proof.  Assume that the first n binary digits of Q, are given.  They form the 
          binary representation of a number Qn which is a lower bound for Q, with approxima­
          tion error at most 2~n.  Generate lower bounds for m(0), m(l), m(2),... in parallel 
          until  the sum of these lower bounds becomes greater than Q,n — 2~n.  This does 
          happen at some point since the sum of the series is Q, and hence is greater than our 
          threshold.  Then make a list of all i that appear in this sum (with a non-zero lower 
          bound for m(i)).
             Note that this list includes all i such that m(i)  ^ 2 • 2~n  (if some i with this 
          property were omitted, the approximation error would exceed 2~n).  Therefore, all i 
          such that K(i) < n — c (for some c that depends on the choice of the function m but 
          not on n) appear in this list.  Thus, the minimal integer that is not in the list has 
          complexity at least n — c.  This implies that both the list itself (which determines 
          this minimal integer)  and the n-bit prefix of Q,  (which allows us to construct the 
          list;  note that  n is determined by this prefix)  have complexity at  least n — c' for 
          some other c' and for all n.  It remains to use the randomness criterion in its prefix 
          complexity version (Theorems 92 and 94).                          □
             One can define the notion of a (Martin-Löf) random real number directly.  A 
          set X of reals is an effectively null set if there is an algorithm that for any rational 
          £  >  0  enumerates  a  cover  of X  by  intervals  with  rational  endpoints  and  total 
          measure  (length)  at  most  e.  A  real  number  is  ML-random  (with respect  to  the 
          standard measure on R) if it does not belong to any effectively null set (=does not 
          belong to the largest effectively null set).
             158                         5.  MONOTONE COMPLEXITY
                  158  Prove that a real number is random (according to this definition) if and
             only if its binary representation is a random sequence (with respect to the uniform 
             measure on Cl).
                  159 Prove that a square (sine, exponent) of a random real is a random real.
                 (Hint:  A preimage of a null set is a null set,  and this argument can be effec- 
             tivized.)
                  160 Can the sum of two random real numbers be a non-random real?
                 (Hint:  the numbers may be dependent.)
                 The random number Cl (or, better to say, any П-number, since different max­
             imal lower semicomputable semimeasures lead to different numbers) is not just an 
             interesting example.  The class of these numbers has several interesting characteri­
             zations [26, 87].  Our presentation follows [19], a survey that can be considered as 
             an extended version of a footnote in [102].
                 5.7.1.  Solovay reductions and completeness.  Recall that a real number 
             a  is  lower  semicomputable  if a  is  the  limit  of some  computable  non-decreasing 
             sequence of rational numbers.  (An equivalent definition is ... if the set of rational 
             numbers less than a is enumerable.)  We want to classify computable non-decreasing 
             sequences according to their convergence speed and formalize the intuitive idea “one 
             sequence converges better (i.e., not worse) than the other one”.
                 Let cq —>■ oc and bj  —>• /3 be two computable strictly increasing sequences con­
             verging to lower semicomputable reals a and ß  (approximations of a and ß from 
             below).  We say that an  —>  a converges better  (not worse)  than bn  —> ß if there 
             exists a total computable function h such that
                                              Q   ^h(i)  ^5 ß
             for every i.
                 In other words, we require that for each term of the second sequence one may 
             algorithmically find a term of the first one that approaches the limit as close as the 
            given term of the second sequence.  Note that this relation is reflexive and transitive 
             (take the composition of two reducing functions).
                 In fact, the choice of specific sequences that approximate a and ß is irrelevant: 
             any two increasing computable sequences of rational numbers that have the same 
            limit are equivalent with respect to this quasi-ordering.  Indeed, we can just wait 
            to get a term of a second sequence that exceeds a given term of the first one.
                 We can thus set the following definition.  Let a and ß be two lower semicom­
            putable reals, and let (an), (bn) be approximations of a and ß, respectively.  If (an) 
            converges better than (bn), we write a      ß (by the above paragraph, this does not 
            depend on the particular approximations we chose).
                 This definition can be reformulated in different ways.  First, we can eliminate 
            sequences from the definition and say that  a        ß  if there exists a partial com­
            putable function p defined on all rational numbers r < ß such that
                                      p(r) < a  and  a — p(r) < ß — r 
            for all of them.  Below, we refer to p as the reduction function.
                 I  1611 Prove that a lower semicomputable number a is computable if and only 
            if a    ß for every lower semicomputable ß.
                 Here is one more useful reformulation:
                                         5.7.  THE  RANDOM  NUMBER fi                           159
                 Theorem 101.  a =^i ß if and only if ß — a is lower semicomputable  (or,  said 
             otherwise,  if and only if ß = a + p for some lower semicomputable real p).
                 Proof.  To show the equivalence, note first that for every two lower semicom­
             putable reals a and p we have a =<h  a + p.  Indeed, consider approximations (an) 
             to a,  (rn)  to p.  Now, given a rational s < a + p, we wait for a stage n such that 
             an + rn  > s.  Setting cp(s)  = an,  it is easy to check that ip is a suitable reduction 
             function witnessing a =^i a + p.
                 It remains to prove the reverse implication:  if a =<h ß,  then p = ß — a is lower 
             semicomputable.  Indeed, let (bn) be a computable approximation (from below) for 
             ß,  and  let  ip  be  the  reduction  function  that  witnesses  a  =<h  ß.  Then  all  terms 
             bn — ip(bn)  are less than or equal to ß — a and converge to ß — a.  (The sequence 
             bn — ip(bn) may not be increasing, but still its limit is lower semicomputable, since 
             all  its  terms  do  not  exceed  the  limit,  and  we  may  replace  the  nth  term  by  the 
             maximum of the first n terms.)                                                      □
                 Here  is  a  special  case:  Let  Y lui  and  Y lvi  be  computable  series  with  non­
             negative rational terms (for i > 0; terms no and no are starting points and may be 
             negative) that converge to (lower semicomputable) a and ß.  If щ ^ n* for all i > 0, 
             then a =<h ß, since ß — a = £Т(п* — щ) is lower semicomputable.
                 The reverse statement is also true:  if a     ß, one can find computable series
             J2ui  = a and Yhvi  = ß with these properties  (0  ^  щ  ^  Vi  for i  >  0).  Indeed, 
             ß  =  a + p for  lower  semicomputable  p\  take  a  =  Y lui  and  p  =      and  let
             Vi = Щ + r».
                  162 Show that a stronger statement is also true:  not only can the series щ be
             chosen arbitrarily (see the argument above), but the same is true for n*.  Namely, 
             if a =4i  ß = ^2 Vi, where V{  ^ 0, then there exists a representation a = Ylui such 
             that 0 ^ щ ^ Vi for every i > 0.  (All series are computable.)
                 (Hint:  Construct щ sequentially maintaining the following invariant relation: 
             the current approximation A = )Cj<z из      a    below a and at least as close (to a) 
             as the current approximation В = Ylj^Vj  (to ß).  Initially, we choose щ applying 
            the reduction function to Vo.  When the current approximation becomes B' — B+Vi, 
            we apply the reduction function to get A', which is at least as close to a as B' is 
            to ß.  Then there are several cases:
                 (1) If A' < A, we let щ = 0, and the next approximation is A (it is close enough 
            by assumption).
                 (2)  If A  ^  A'  ^  A + Vi,  we let  щ  =  A! — A;  the condition guarantees  that
            Щ ^ Vi.
                 (3) Finally, if A' > A + Vi, we let щ = v%  (the invariant remains valid since the 
            distances to a and ß are decreased by the same amount).)
                 Let a be a lower semicomputable but not computable real.  By the results of 
            the previous section, one has
                                            Q.    2o:   3<У     • • •
            because for all к the difference (к + 1 )a — ka = a is lower semicomputable.  The 
            reverse relations are not true, because ka — (k + 1 )a — —a is not lower semicom­
            putable (if it were, then a would be computable).
                 One may argue that this relation is therefore a bit too sharp.  For example, a 
            and 2a have essentially the same binary expansion (just shifted by one position),
             160                         5.  MONOTONE COMPLEXITY
             so one may want a and 2a to be equivalent.  In other words, one may look for a 
             less fine-grained relation.  A natural candidate for this is called Solovay reducibility 
             (see  [188]).
                 We say that a is Solovay reducible to ß  (a =4 ß) if &     cß for some positive 
             integer c > 0.  (A convenient notation:  We say,  for some positive rational c, that 
             a =^c ß if a   cß.  Then a =<I ß if a =<IC ß for some c.)  This relation is also reflexive 
            and transitive (obviously).
                 Theorem 102.  There exists a biggest lower semicomputable real with respect 
             to Solovay reducibility.
                 P r o o f.  We can enumerate all lower semicomputable reals a.i in [0,1] and then 
            take their sum a — Y2wiai with computable positive weights Wi such that 
            converges.  This a can be represented as WiCti plus some lower semicomputable real, 
            so ai     (1 /wi)a.                                                                □
                 The biggest elements for the =<I-preorder are also called Solovay complete lower 
            semicomputable reals.  One can even define some qualitative notion of completeness 
             deficiency:  for a lower semicomputable real ß the completeness deficiency is defined 
            as minimal c such that a =<h  cß.  Here a is some fixed Solovay complete real;  the 
            deficiency function depends on the choice of a, but is still defined up to the 0 (1)- 
            factor.  The deficiency of ß is finite if and only if ß is Solovay complete.
                It  turns  out that Solovay complete reals can be equivalently described as fl- 
            numbers defined above [188, 26].
                 Theorem 103.  Complete semicomputable reals in (0,1)  are sums of universal 
            (maximal) semimeasures on N and vice versa.
                 Proof.  Any  lower  semicomputable  real  a  is  a sum  of a computable  series 
            of rationale;  this series  (up to a constant factor that does not matter due to the 
            definition of the Solovay reducibility) is bounded by a universal semimeasure.  The 
            difference between the upper bound and the series itself is a lower semicomputable, 
            and therefore a is reducible to the sum of the universal semimeasure.
                On the other hand, let a be a Solovay complete real in (0,1).  We need to show 
            that a is a sum of some universal semimeasure.  Let us start with the arbitrary 
            universal semimeasure      .  The sum ^      is lower semicomputable and therefore 
            J2mi  =^i  ca,  so  a  —  X^mi/C + r  f°r some  integer c  >  0  and some  lower semi­
            computable t .  Dividing m by c and then adding т to one of the values, we get a 
            universal semimeasure with sum a.                                                  □
                It  turns  out  that  these  reals  (Solovay  complete  lower  semicomputable  reals, 
            or Q-numbers) have one more description:  they are exactly lower semicomputable 
            ML-random real numbers in (0,1).  The equivalence proof consists of several parts; 
            let us consider them one by one.
                5.7.2.      Solovay complete reals are random.  We already have shown that 
            Solovay complete reals are random:  each of them is an Q-number, i.e., a sum of the 
            values of universal semimeasure, and this sum is random (Theorem 100).  Formally 
            speaking, this argument applies only to numbers between 0 and 1, but the general 
            case can be reduced to this special one by adding a rational number.  Still there 
            is an interesting direct argument that does not involve complexity and the Levin- 
            Schnorr criterion of randomness  (it is in the footnote in Levin’s paper [102];  this
                       5.7.  THE  RANDOM  NUMBER      161
       footnote compresses the most important facts about lower semicomputable random 
       reals into few lines).
          First, recall that one can prove the existence of a lower semicomputable random 
       real without references to Q (Problem 86).  So it is enough to prove that randomness 
        is upward-closed:  if a =<( ß and a is random, then ß is random.
          We may assume without loss of generality that a =4,\ ß (randomness does not 
       change if we multiply a real by a rational factor).  Let b{ —,► ß be a computable in­
       creasing sequence of rational numbers that converges to ß.  Assume that somebody 
       gives us (in parallel with hi) a sequence of rational intervals and guarantees that one 
       of them covers ß.  How do we transform it into a sequence of intervals that covers 
       a (i.e., one of the intervals covers a) and has the same (or smaller) total length?  If 
       an interval appears that is entirely on the left of the current approximation bi, it 
       can be ignored (since it cannot cover ß anyway).  If the interval is entirely on the 
       right of bi,  it  can be postponed until the current approximation bj  enters it  (this 
       may happen or not, in the latter case the interval does not cover ß).  If the interval 
       contains bi, we can convert it into the interval of the same length that starts at aj, 
       where aj is a rational approximation to a that has the same or better precision as 
       bi  (as an approximation to ß):  if /3 is in the original interval, a is in the converted 
       interval.
          So randomness is upward closed, and therefore complete lower semicomputable 
       reals are random.
          Remark.  The second part can be reformulated:  if a and ß are lower semicom­
       putable reals and at least one of them is random, then the sum a + ß is random 
       too.  The reverse is also true:  if both a and ß are non-random, then a + ß is not 
       random.  (Later we will see different proofs of this statement.)
          5.7.3.  Randomness and prediction game.  Before proving the reverse im­
       plication  (random lower semicomputable reals are Solovay complete), let us make 
       a digression and look more closely at the last  argument.  Consider the following 
       game.  An observer watches an increasing sequence of rationals (given one by one) 
       and from time to time makes predictions of the following type:  “the sequence will 
       never increase by more than Ö”  (compared to its current value).  Here Ö is some 
       non-negative rational.  The observer wins this game if
          (1) one of the predictions remains true forever;
          (2)  the sum of all numbers 5 used in the predictions is small (less that some 
       rational e > 0 which is given to the observer in advance).
          It  is  not  required  that  at  any moment  a valid prediction exists,  though one 
       could guarantee this by making predictions with Ô that are small and decrease fast 
       at each step.  Note also that every prediction can be safely postponed, so we may 
       assume that the next prediction is made only if the previous one becomes invalid. 
       Then at any moment there is only one valid prediction.
          One can give a criterion of randomness in terms of this game.
          Theorem  104.  Let ai  be  a computable  increasing sequence  of rational num­
       bers  that  converges  to  some  (lower semicomputable)  real a.  The  observer has  a 
       computable winning strategy in the game if and only if a is not random.
          P r o o f.  A  computable  winning strategy  gives  us  a  computable  sequence  of 
       prediction intervals of small total measure and guarantees that one of these (closed) 
       intervals contains a.  We can convert them to slightly bigger open intervals.
                                         5.  M ONOTONE  COMPLEXITY
             162
                 On the other hand, having a sequence of intervals that cover a and have small 
            total measure, we may use it for predictions.  To make the prediction, we wait until 
            the current approximation a* gets into some of the covering intervals, and we then 
            predict  that  it  will  never go  out  of this  interval.  When and if this  turns out  to 
            be false,  we wait  until the current  approximation is covered again,  etc.  If there 
            are several intervals covering the current approximation, we choose the first one in 
            the enumeration order.  Starting from some moment, we always have the interval 
            that  covers  a  as  one  of the  options,  so  this  rule  guarantees  the  predictions  will 
            stabilize.                                                                        □
                The following is a reformulation of the same observation that does not use game 
            terminology:
                 Theorem 105.  Let ai be a computable increasing sequence of rational numbers 
            that converges to a.  The number a is non-random if and only if for every rational 
            £  >  0  one  can  effectively find  a  computable  sequence  ho,h\,...  of non-negative 
            rational numbers such that ]>T hi < e and a ^ a* + hi for some i.
                 Proof.  This corresponds to the game where predictions hi are made on every 
            step.  As we have said, this does not matter since we may use zeros.              □
                Recall also the Solovay criterion of ML-randomness  (a constructive version of 
            the Borel-Cantelli lemma, Theorem 31 on p. 64):  A real number a is non-random 
            if and only if there exists a computable sequence of intervals that have finite total 
            measure and cover a infinitely many times.  The same modification can be applied 
            to the previous theorem, and we get the following result.
                Theorem 106.  Let ai be a computable increasing sequence of rational numbers 
            that  converges  to  a.  The  number a  is  non-random  if and  only  if there  exists  a 
            computable sequence ho, hi,...  of non-negative rational numbers such that     hi < 
            oo  and a ^ ai + hi for infinitely many i.
                PROOF.  If a is non-random, we apply the preceding result for e — 1,1/2,1/4,... 
            and then add the resulting sequences (with shifts 0,1, 2,... to the right).  Each of 
            them provides one value of i such that a ^   +     and these values are still suitable
            after shifts and cannot be bounded due to shifts.  On the other hand, if a ^ ai + hi 
            for  infinitely  many  i,  we  get  a sequence of intervals with finite sum of measures 
            that covers a infinitely many times (technically, we should replace closed intervals 
            by slightly bigger open intervals).  It remains to use Solovay’s criterion (or recall its 
            proof:  The effectively open set of points that are covered with multiplicity m has 
            measure at most 0 (l/m)).                                                         □
                The randomness criterion given in this section implies the following observation 
            (which may look strange at first).  Consider a sum of a computable series of positive 
            rational numbers.  The randomness of the sum cannot change if all summands are 
            changed by some 0(1)-factor.  Indeed, all hi can be multiplied by a constant.
                Now let us prove the result mentioned above:
                Theorem 107.  If a and ß are non-random lower semicomputable reals,  their 
            sum a + ß is non-random too.
                PROOF.  It now seems very easy at first:  Make predictions in the games for a 
            and ß, and then take their sum as prediction for a + ß.  (If for a we expect increase
                                                                   5.7.   THE  RANDOM  NUMBER П                                                             163
                     h and for ß we expect increase k, then for a + ß we predict increase h + к.)  But 
                     this simple argument does not work.  The problem is that the same prediction for a 
                     can be combined with many predictions for ß and therefore will be counted many 
                     times in the sum.
                            The solution is to make predictions for a and ß of the same size.  Let ai  and 
                     bi  be computable increasing sequences that converge to a and ß.  Since a and ß 
                     are non-random,  they are covered by sequences of intervals that have small total 
                     measure.  To make a prediction for the sequence ai + bi (after the previous prediction 
                    became invalid), we wait until the current approximations cq and bi become covered 
                    by the intervals of those sequences.  We take then the maximal h and к such that 
                     (ai, ai + h)  and  (bi,bi + к)  are entirely covered  (by the unions of already known 
                    intervals).  The prediction interval is declared to be  (a* + bi, ai + bi + 28)  where 
                    8 = min(h, k).
                           Let us show that one of the predictions will remain valid forever.  Indeed, the 
                    limit  values  a  and  ß  are  covered  by  some  intervals.  These  intervals  appear  in 
                    the sequences at some point and cover a and ß with some neighborhoods, say, o- 
                    neighborhoods.  If the prediction is made after a* and bi enter these neighborhoods, 
                    8 is greater than a and the prediction is final:  a* + bi  never increases more than 
                    by 28.
                           It remains to bound the sum of all 8 used during the prediction.  It can be done 
                    using the following observation.  When a prediction interval (a* + £>*, a* + Ь* + 28) 
                    becomes invalid, this means that either ai or bi has increased by 8 or more, so the 
                    total measure of the covers on the right of a*  and bi  has decreased at least by 8. 
                    Here we use that (a*, a* + 8) and (bi, bi + 8) are covered completely because 8 does 
                    not exceed both h and k.  It is important here that we take the minimum.                                                                 □
                           Let us return to the criterion for randomness provided by Theorem 105.  The 
                    condition for non-randomness given there can be weakened in two aspects.  First, we 
                    can replace computable sequence by a lower semicomputable sequence, and second, 
                    we can replace hi by the entire tail hi + hi+i + • • •  of the corresponding series, as 
                    follows.
                           Theorem 108.  Let ai be an increasing computable sequence of rational numbers 
                    that converges to a.  Assume that for every rational £ > 0  one can effectively find 
                    a lower semicomputable sequence hi  of non-negative reals such that                                                       hi < e  and 
                    a ^ ai + hi + hi+i + • • •  for some i.  Then a is not random.
                           Proof.  Assume that for every i there is a painter who gets hi units of paint 
                    and instructions to paint the real line starting at ai, going to the right, and skipping 
                    the parts already painted by other painters (but making no other gaps).  (Since hi is 
                    only semicomputable, the paint is provided incrementally and is used as soon as it 
                    becomes available.)  The painted zone is a union of an enumerable family of intervals 
                    of total measure                  hi (the total amount of paints).  If a < ai + hi + hi+1 + • ■ •, then 
                    a is painted since we cannot use hi + hi+1 + • • •  units of paint, starting between 
                    ai  and a  (recall that all aк  are less than a:)  and not crossing a:  by construction, 
                    we never cover the same point by several layers of paint.  (In the condition of the 
                    theorem we have ^ instead of <, but this does not matter since we can increase all 
                         to,  say, twice their original value.  For the same reason it is not important that 
                    hi
                    we covered a by closed intervals instead of open ones.)                                                                                 □
                 164                                     5.  M ONOTONE  COMPLEXITY
                       This result implies one more criterion of randomness for lower semicomputable 
                 reals:
                       Theorem 109.  Let a =                       be  a  computable series of non-negative rational 
                 numbers.  The (lower semicomputable) real a is non-random if and only if for every 
                 £  >  0  one  can  effectively produce  an  enumerable  set W  C  N  of indices such that
                 (1)  J2iewri < £ and (2) ^                   со-finite,  i.e.,  contains all sufficiently large integers.
                       PROOF.  If a is not random, it can be covered by intervals with arbitrarily small 
                 total measure.  It remains to consider the set W of all i such that
                                                  [fO +  • • • +  T4_i, Го +  • • • +       +    Г{\
                 is entirely covered by one of those intervals.  In the other direction the statement  is
                 a direct consequence of Theorem 108, just let a* =  ro                             +          • • • + r*_i and hi = r* for
                 i € W (and ff = 0 for i ф W).                                                                                   □
                       This result shows again that the sum of two non-random lower semicomputable 
                 reals is not random (take the intersection of two sets W\  and W2 provided by this 
                 criterion for each of the reals), so we get a new proof of Theorem 107.
                       The trick we used to prove Theorem 108 can be reused for the following problem 
                 (this argument was communicated to us by L. Bienvenu; the original proof in [87] 
                 is much more complicated).
                        163 Let U be an effectively open subset of [0,1] that has measure less than 
                 1.  Assume that U contains all non-ML-random reals.  (For example, U can be one 
                 of the open sets that form a universal Martin-Löf test.)  Prove that the measure of 
                 U is a lower semicomputable random real.
                       (Hint:  Let  a be the measure of U.  If the cover of a with intervals of small 
                 measure is given, we can construct the cover of the minimal real outside U that has 
                 the same measure.  How can we do that? As soon as the current approximation to a 
                 gets into some interval, we imagine that it will not get out of this interval, i.e., only 
                 a small set will be added to the current part of U and will paint an equally small 
                 part  of the current  complement of U going from left to right.  If our assumption 
                 is in fact true (and this will happen at some point), then we indeed will paint the 
                 minimal element outside U.  (The painted part is a union of closed intervals,  not 
                 the open ones, but this does not matter.))
                       5.7.4.  Random  lower  semicomputable  reals  are  complete.  Now it  is
                 easy to  prove the reverse implication  [87]:  Every lower semicomputable random 
                 real is Solovay complete.
                       Let  us start  with the following remark.  Consider two lower semicomputable 
                 reals a and ß presented as limits of increasing computable sequences ai                                     a and 
                 bi  —У ß.  Let  ff  = ai+i — ai  be the sequence of increases in the first sequence.  We 
                 may use the sequence hi  to construct  a strategy for the prediction game against 
                 the second sequence in the following way.  We shift the interval  [ao,^]  to get the 
                 (closed)  interval of the same length that starts at  bo  (Figure  13).  Then we wait 
                 until  bi  at  the  right  of this  interval  appears;  let  6^  be  the  first  term  outside  it. 
                 Then we shift the interval [ai, 02] to get the interval of the same length that starts 
                 at bij ;  let  bi2  be the first bi on the right of it, etc.
                       There are two possibilities:  either
                       (1)  the observer wins in the prediction game, i.e.,  one of the shifted intervals 
                 covers the rest of bi and the next bik  is undefined; or
                                                                  5.7.   THE RANDOM  NUMBER Г2                                                            165
                                                    Cl o           Cl 1         a2
                                  Figure  13.  Increases of ai are used in the prediction game for ß.
                           (2)  this process continues indefinitely.
                           In the second case a =<h ß since the difference ß — a is represented as a sum of 
                    a computable series  (“holes”  between neighbor intervals;  note that the endpoints 
                    of the shifted intervals also converge to ß).
                           After this remark it  is easy to show that every incomplete ß is not random. 
                    Indeed,  assume that  ß  is  not  Solovay  complete;  we  need  to  prove  that  ß  is  not 
                    random.  Since ß is not complete, there exists some a such that a ^ ß.  In particular, 
                    a        ß.  Therefore, for these a and ß the second alternative is impossible, and the 
                    observer wins.  In other words, we get a computable sequence of (closed) intervals 
                    of total size at  most  ^  hi  that covers ß.  Repeating the same argument for a /2, 
                    a/4,...  (we know that a/c                         ß for every c, since a ^ ß), we effectively get a cover 
                    of ß with arbitrarily small measure (since the sum of all hi is bounded by a integer 
                    constant even being non-computable); therefore ß is not random.
                           This finishes the proof of the result we mentioned:
                           Theorem 110.  A lower semicomputable real is Solovay complete if and only if 
                    it is ML-random.
                           5.7.5.             Slow convergence:  Solovay functions.  We have seen several results 
                    of the following type:  the limit of an increasing computable sequence of rationale 
                    is random if and only if the convergence is slow.  In this section we provide some 
                    other results of this type [12, 67].
                           Consider a computable converging series ^                                  of non-negative rational numbers. 
                    Note that г*  is bounded by 0(m(i)) where m(i)  is the  (discrete)  a priori proba­
                    bility  of integer i,  and  therefore prefix complexity K(i)  —  — logm(i)  is bounded 
                    by  —logTj + 0(1).  We say that the series J2ri  converges  slowly  in  the Solovay 
                    sense  (has the Solovay property) if this bound is 0(l)-tight infinitely often, i.e., if 
                    rj ^ em(i) for some e > 0 and for infinitely many i.  In other words, the series does 
                    not converge slowly if Гг/Шг —> 0.
                           Historically the name Solovay function was used for a computable bound S(i) 
                    for prefix complexity K(i) that is tight infinitely often, i.e., K(i) ^ S(i) + 0(1) for 
                    every i and K(i) ^ S(i) — c for some c and for infinitely many values of i.  Thus, a 
                    computable series ^2 ri of non-negative rational numbers has the Solovay property 
                    if and only if i i-»  — log2 r*  is a Solovay function.  (Usually integer-valued Solovay 
                    functions are considered, so some rounding is needed.)  We provide several results 
                    that relate randomness to slow convergence, mainly following [12, 67].
                           Theorem 111.  Let a — Yliri  be a computable converging series of non-nega­
                    tive rational numbers.  The number a is random if and only if this series converges 
                    slowly in the Solovay sense.
                          In other words, the sum is non-random if and only if the ratio ri/m(i) tends 
                    to 0.
                                              5.  M ONOTONE  COMPLEXITY
              166
                   PROOF.  Assume that Tijm{i) —» 0.  Then for every e we can let hi =  em{i) and 
              get a lower semicomputable sequence that satisfies the conditions of Theorem 108. 
              Therefore a is not random.
                   We can also prove that a is not complete (thus providing an alternative proof 
              of its  non-randomness).  Recall the argument used in the proof of Theorem  103: 
              if n  ^  m(i),  then  Y2ri  =$i              And if ri  ^  cm{i),  then  Yhri  ==*c
              This remains true if the inequality r* ^ cm{i) is true for all sufficiently large i.  So 
              for a fast (non-Solovay) converging series and its sum a we have a =<IC               rn(i) for 
              arbitrarily small c.  If a were complete, we would have also ^  m{i) =4.d ot for some 
              d and therefore a  =$cd  a for some d and all c >  0.  For small enough c we have 
              cd  <  1/2  and therefore a  =<h/2  a,  he.,  2a:      er,  so  the  difference  a — 2a  =  —a
              is lower semicomputable and a is computable.  (One could note also that for each 
              approximation to a from below we can find a twice better one, and we can iterate 
              this procedure.)
                   It  remains  to  show the reverse  implication.  Assuming that a  =                is  not 
              random, we need to prove that i'i/m(i) —» 0.  Consider the interval [0, a] split into 
              intervals of length ro,r\,...  (from left to right).  Given an open cover of a with 
              small measure, we consider those intervals (of length ro, r\,..., see above) that are 
              completely covered  (endpoints included).  They form an enumerable set and the 
              sum of their lengths does not exceed the measure of the cover.  If the cover has 
              measure 2~2n for some n, we may multiply the corresponding ri by 2n and their 
              sum remains at most  2~n.  Note also that for large enough i  the ith interval is 
              covered (since it is close to a and a is covered).  So for each n we get a semimeasure 
              Mn such that  Mn(i)fri  ^  2n  for  all  sufficiently  large  i  and  J2iMn(i)  ^  2~n. 
              Taking the sum of all Mn, we get a lower semicomputable semimeasure M such 
              that ri/M(i) —>■ 0.  Then ri/m(i) —> 0 also for the universal semimeasure m.                 □
                   This result provides a (third) proof that a sum of two non-random lower semi­
              computable reals is non-random (since the sum of two sequences that converge to 0 
              also converges to 0).
                   It  shows also that  Solovay functions exist  (which is not immediately obvious 
              from the definition).  Moreover, it shows that there exist computable non-decreasing 
              Solovay functions:  take a computable series of rational numbers with random sum 
              and make this series non-increasing not  changing the  sum  (by  splitting too  big 
              terms into small pieces).
                    164         Let  U be  an optimal prefix-free decompressor.  Consider the function 
              /(p, X, n) that is equal to l(p) if U produces output x on input p making exactly n 
              steps, and (say) 2Z(p) + 2l{x) + 21ogn otherwise.  Prove that /(р,т,п) is an upper 
              bound for K(p,x,n) and this bound is tight when p is the shortest description of 
              x that needs n steps to process,  and give an alternative proof of the existence of 
              Solovay functions.
                   It also implies that slow convergence (in the Solovay sense) is not a property of 
              a series itself, but only of its sum.  It looks strange:  some property of a computable 
              series  (of non-negative rational numbers), saying that infinitely many terms come 
              close to  the  upper bound provided by the  a priori probability,  depends only on the 
              sum of this series.  At first,  it seems that by splitting the terms into small parts 
              we can destroy the property not changing the sum, but it is not so.  In the next
                                           5.7.  THE  RANDOM  NUMBER П                              167
             section we try to understand this phenomenon providing a direct proof for it (and 
             as a byproduct we improve the results of this section).
                  5.7.6.       The Solovay property as a property of the sum.  First, let us note 
             that  the  Solovay  property  is  invariant  under  computable  permutations.  Indeed, 
             consider some computable permutation 7r.  It changes the a priori probability only 
             by a constant  factor:  m(r(i))  =  0(т(г)).  Then let us consider grouping.  Since 
             we want to allow infinite groups,  let  us consider  a computable series          j aij  °f 
             non-negative rational numbers.  Then
                      a =      aij = (aoo + ctoi + •••) + (aio + сщ + -- -) + ''' =       ^г,
                            i,j                                                         i
             where Ai =        a^.
                  We want to show that Ai and          are slowly converging series (in the Solovay 
             sense)  at the same time.  Note that slow convergence is permutation-invariant, so 
             it  is well defined for two-dimensional series.
                  However,  some clarifications  and  restrictions  are  needed.  First,  J^Ai  is not 
             in general a computable series, it is only a lower semicomputable one.  We extend 
             the  definition  of the  Solovay  property  to  lower  semicomputable  series:  for  such 
             a series we still have Ai  =  0(m(i)),  and we require this bound to be 0(l)-tight 
             infinitely  often.  Second,  such  a general  statement  is  not  true:  imagine  that  all 
             non-negative terms are in the first group Ao  and  all Ai,A2,...  are zeros.  Then 
                Ai does not have the Solovay property while ^  aij could have it.  The following 
             result  (essentially from [67]) provides the needed restrictions:
                  Theorem  112.  Assume that each group Ai  contains only finitely many non­
             zero terms.  Then the properties Ai/m(i) —> 0 and aij/m(i,j) —>• 0 are equivalent.
                  Here m(i,j) is the a priori probability of pair (i,j)  (its number in some com­
             putable numbering; the probability does not depend on the coding up to an 0(1)- 
             factor).  The convergence means that for every e > 0, the inequality а^/т(г, j) > e 
             is true only for finitely many pairs (г, j).
                  P roof.  Let us recall first that m(i) = J2j m(hj) up to a 0(l)-factor.  (Indeed, 
             the sum in the right-hand side is lower semicomputable,  so it  is  0(m(i))  due to 
             maximality;  on  the  other  hand,  already  the  first  term  т(г,0)  is  £l(m(i)).)  So 
             if aij/m(i,j)  tends to  zero,  the ratio  A i/J2jm(i,j)  does the same  (only finitely 
             many pairs have aij > em(i,j) and they appear only in finitely many groups).
                  It is more difficult to show that Ail mi —>• 0 implies aij/m(i,j) —> 0.  (Here we 
             need to use that only finitely many terms in each group are non-zero.)  For this it is 
             enough to construct some lower semicomputable rh(i,j) such that aij/ih(i,j) —> 0, 
             somehow using the fact that Ai/m(i) —> 0.  The natural idea would be to split m(i) 
             between m(i,j) in the same proportion as Ai is split between a^.  However, for this 
             we need to compute Ai (and not only to lower semicompute it).  It would be easy if 
             we knew how many terms among ецо, an,... are non-zero, but in general this is a 
             non-computable information.  (For the special case of finite grouping this argument 
             indeed works.)
                  So we use another approach.  For some constant c we may let m(i,j) be ca^- 
             while this does not violate the property J^j ^ ih j) ^ m{i).  (As m(i) increases, we 
             let m(i,j) increase when possible.)  If indeed Ai/m(i) —>• 0, for every constant c we
             168                           5.  MONOTONE  COMPLEXITY
                               О
                      Figure  14.  Two series with the same sum can be obtained by a 
                      different  grouping from the same third series.
             have cAi  ^ m{i) for all sufficiently large i, so a,ij/rh(i, j)  ^  1 jc for all sufficiently 
             large i  (and only finitely many pairs  (i,j)  violate this requirement,  because each 
             Ai has only finitely many non-zero terms).  So for each c we have constructed some 
             semimeasure mc such that aij/mc{i,j) ^ 1/c for almost all pairs (i,j), and the sum 
             Ylij^cihj) is at most ^2m(i) ^ 1.  It remains to perform this construction for all 
             c = 22n and combine the resulting m22n with coefficients 2~n.                         □
                  As a corollary of Theorem 112 we see (in an alternative way) that the Solovay 
             property depends only on the sum of the series.  Indeed,  if                   bj,  these
             two series could be obtained by a different grouping of terms in some third series
                 Cfc.  To construct Cfc, we draw intervals of lengths a\, a,2, ■ ■ ■ starting from zero 
             point, as well as the intervals of lengths bi, 62,...; combined endpoints split the line 
             into intervals of lengths ci, C2,...  (see Figure 14).
                 In  this  way  we  get  not  only  the  alternative  invariance  proof,  but  also  can 
             strengthen Theorem 111, which dealt with computable series of rational numbers. 
             Now we still consider series of rational numbers, but the summands are presented 
             as  lower  semicomputable numbers  and each has only finitely  many different  ap­
             proximations.  (So  ri  —  \imnr(i,n),  where  r  is  a computable  function  of i  and 
             n with rational values which is non-decreasing as a function of n and for every i 
             there are only finitely many different values r(i,n).)  Then the number           *s n°t
             ML-random if and only if Ti/m(i) —> 0.  Indeed, each         is  a sum of a computable 
             series of non-negative rational numbers with only finitely many non-zero terms.  So 
             we can split        into  a double series not  changing the sum  (evidently)  and the 
             Solovay property (due to Theorem 112).
                 Recall that an upper semicomputable function n 1—> f(n) with integer values is 
             an upper bound for K(n)  (up to an 0 (1) additive term)  if and only if          2“-^n) 
             is finite (Theorem 62, p.  100).  Now we can extend this statement:
                 Theorem 113.  This bound is tight for infinitely many n (i.eK (n) ^ f(n) — c 
             for some c and for infinitely many n)  if and only if the sum                  random.
                 P roof.  Indeed,  decreasing integer upper bounds for f(n)  provide increasing 
             lower  bounds  for  2“^"^  (with  finitely  many  changes),  so  we  use  the  preceding 
             result.                                                                               □
                 We end this section with an alternative proof that all complete reals have the 
             Solovay property.  First we observe that the Solovay property is upward closed with 
             respect to Solovay reducibility.  Indeed, if    ai  and    bi  are computable series of 
             non-negative rational numbers and ai converges slowly, then Yl(ai + ^) converges 
             slowly also (its terms are bigger).  So it remains to prove directly that at least one 
             slowly converging series (or, in other words, a computable Solovay function) exists. 
             It can be done as shown in Problem 164.  Another way to explain this construction 
             is  that  we watch how the values of a priori probability increase  (it  is convenient
                                               5.7.  THE  RANDOM  NUMBER П                                   169
               again to consider the a priori probability of pairs):
                                       m(0,0)  m(0,1)  m( 0,2)           m(0, 3) 
                                       m(l,0)  m(l.l)  m( 1,2)  m( 1, 3) 
                                       m(2,0)  m(2,l)  m( 2,2)           m(2,3)
               and we fill a similar table with rational numbers ац in such a way that aij/m(i, j) -f> 
               0.  How do we fill this table?  For each row we compute the sum of current val­
               ues m(i, •);  if it crosses one of the thresholds  1/2,1/4,1/8 • • •, we put the crossed 
              threshold value into the а-table (filling it with zeros from left to right while waiting 
              for the next threshold crossed).  In this way we guarantee that aij is a computable 
              function of i  and j\  the sum of a-values in every row differs from the sum of re­
              values in the same row at most by factor 2  (in both directions);  this implies that 
              the new series is convergent and that in every row there exists some а-value that is 
              at least half of the corresponding m-value.  Logarithms of а-values form a Solovay 
              function (and atj itself form a slowly convergent series).
                   Note that this construction does not give a non-decreasing Solovay function di­
              rectly (it seems that we still need to use the arguments from the preceding section).
                   5.7.7.  Busy beavers and convergence moduli.  We had several definitions 
              that  formalize  the  intuitive  idea  of a  “slowly  converging  series”.  However,  the 
              following one (probably the most straightforward) was not considered yet.  If an —> 
              a, for every e > 0 there exists some N such that  |a - a n|  < e for all n > N.  The 
              minimal N with this property (considered as a function of e, denoted by e h-> N(e)) 
              is called a modulus of convergence.  A sequence (or a series) should be considered 
               “slowly converging” if this function grows fast (as e —> 0).  Let us show how the the 
              Solovay property could be equivalently characterized in these terms.
                   In Section 1.2  (p. 21) we defined B(n) as a maximal integer whose complexity 
              does not exceed n.  We used plain complexity there (since at that time no other 
              versions were defined), but a similar definition can be given for prefix complexity. 
              Let BP(n) be the maximal integer whose prefix complexity does not exceed n.
                    165         Fix an optimal prefix-free universal machine M.  Let T(m) be the maxi­
              mal time needed for termination of all terminating computations on inputs of length 
              at most m.  Then
                                           BP(m — c) ^ T(m) < BP(m, + c)
              for some c and all m.
                   (Hint:  One  can  use  the  same  argument  as  for  plain  complexity  (see  Sec­
              tion 1.2).)
                   Now we can prove the equivalence of different notions of “slow convergence” :
                   Theorem  114.  The computable series of non-negative rational numbers Yhri 
              has the Solovay property (^  has a random sum)  if and only if its modulus of con­
              vergence grows fast:  N(2“m) > BP(m — c) for some c and for all m.
                   Proof.  Let a = J2ri —                 where      = ro + • • • +     i.  Assume that a is
              random.  We have to show that \a — a;| < 2“m implies K(i) > m — 0(1); this shows 
              that N(2~m) ^ BP(m — 0(1)).  Since K(i) = K(ai) + 0(1), it is enough to show 
              that every rational 2“'^approximation to a has complexity at least m — 0(1).  This 
              is a bit stronger condition than the condition K(ao ■ ■ ■ cc,n_i) > m — 0(1) (used in 
              the prefix complexity version of the Levin-Schnorr theorem) since now we consider
                                         5.  MONOTONE COMPLEXITY
             170
             all  approximations,  not only the prefix of the binary expansion.  However,  it can 
            be proven in a similar way.
                 Let  c  be  some  integer.  Consider  an  effectively  open  set  Uc  constructed  as 
            follows.  For  every  rational  r  we  consider  the  neighborhood  around  r  of radius 
            2-K(r)-c.^  ^he se£  jjc  is  thg  union  0f these  neighborhoods.  (Since  K(r)  is  upper 
            semicomputable, it is indeed an effectively open set.)  The total length of all intervals 
            is 2 • 2~c          ^ 2“(C_I).  Therefore, the sequence Uc forms a Martin-Löf test,
            and random a does not belong to Uc for some c.  This means that complexity of 
            2“m-approximations of a is at least m — 0(1).
                 In  the  other  direction  we  can  use  the  Levin-Schnorr  theorem  without  any 
            changes:  if N(2~m)  ^  BP(m — c),  then K(i)  ^  m — 0(1)  for every i such that 
            ai  is  a 2-m-approximation to a.  Therefore, the m-bit prefix of a has complexity 
            at least m — 0(1), since by knowing this prefix, we can effectively find an     that 
            exceeds it  (and the corresponding г).                                            □
                 Remark.  Note that this theorem shows equivalence between two formaliza­
            tions of an intuitive idea of “slowly converging series”  (or three, if we consider the 
            Solovay reducibility as a way to compare the rate of convergence).  However,  the 
            proof goes through Martin-Löf randomness of the sum (where the series itself dis­
            appears) .  It would be nice to find a more direct proof and (maybe) to connect the 
            Solovay reducibility  (not only completeness)  to the properties of the convergence 
            moduli.
                Reformulating the definition of BP (m) in terms of a priori probability, we may 
            define BP(m) as the minimal N such that all n > N have a priori probability less 
            than 2~m.  However, in terms of a priori probability the other definition looks more 
            natural:  let BP'(m) be the minimal N such that the total a priori probability of all 
            n > N is less than 2~m.  Generally speaking, BP'(m) can be greater than BP(m) 
            (see  [4]),  but  it  turns out  that  it  still can  be used  to  characterize randomness in 
            the same way:
                Theorem 115.  Let ai  be a computable increasing sequence of rational numbers 
            that converges  to  a random number a.  Then N(2~m)  ^  BP'(m — c) for some c 
            and for all m.
                P r o o f.  Since all г  >  N(2~m)  have the same a priori probability as the cor­
            responding ai  (up to  an  O(l)-factor),  it  is enough to show that for every m the 
            sum of a priori probabilities of all rational numbers in the 2“m-neighborhood of a 
            random a is 0(2~m) (recall that for each i > N(2~m) the corresponding a* belongs 
            to this neighborhood).
                As usual, we go in the other direction and cover all  “bad”  a that do not have 
            this property by a set of small measure.  Not having this property means that for 
            every c there exists an m such that  the sum of a priori probabilities of rational 
            numbers in the 2~m-neighborhood of a exceeds c2~m.  For a given c, we consider 
            all intervals with rational endpoints that have the following property:  the sum of a 
            priori probabilities  of all rational numbers in this  interval is more  than c/2  times 
            bigger than the interval’s  length.  Every bad a is covered by an interval with this 
            property (the endpoints of the interval (a — 2~m, a + 2~m) can be changed slightly 
            to make them rational), and the set of intervals having this property is enumerable. 
            It  is  enough to show that the union of all such intervals has measure  0(l/c),  in 
            fact, at most 4/c.
                                        5.7.  THE  RANDOM  NUMBER П                          171
                 It  is  also  enough  to  consider  a  finite  union  of intervals  with  this  property. 
             Moreover,  we  may  assume that  this  union  does not  contain  redundant  intervals 
             (that can be deleted without changing the union).  Let us order all the intervals 
             according to their left endpoints:
                                         {lo,r0), (ii,ri), (h,r2),
             where /о  ^  h  ^  h  ^  • • •.  It  is  easy to  see  that  right  endpoints  go  in  the same 
             order (otherwise, one of the intervals would be redundant).  So ro ^ rq < r2 < • • • • 
             Now note that rq  ^  h+2\  otherwise,  the interval  (Zj+i,?q+i)  would be redundant. 
             Therefore, intervals with even numbers (lo,ro), (l2,r2), (/4^ 4), • • ■ are disjoint, and 
             for each of them the length is c/2 times less than the sum of a priori probabilities 
            of rational numbers inside it.  Therefore, the total length of these intervals does not 
            exceed 2/c, since the sum of all priori probabilities is at most  1.  The same is true 
            for intervals with odd numbers, so in total we get the bound 4/c.                 □
                 This statement  raises some natural questions.  How much could  BP(m)  and 
             BP'(m) differ?  This question was answered in [4]  (the maximal difference corre­
            sponds to a logarithmic change in the argument),  but there are many others for 
            which we do not know the answers.  Can we use prefix-stable machines (instead of 
            the prefix-free ones) in the definition of BP in) as the computation time?  Can we 
            derive the last theorem from the version of the Levin-Schnorr theorem with a priori 
            complexity?  Can we use the methods of this section to prove that the real number 
            Ylxzx         is random for every prefix-free set X that contains the domain of an
            optimal prefix-free decompressor?
                 Returning to the “philosophical meaning” of the number Ll, let us note that it 
            can be considered as an “infinite version” of special objects of complexity n that are 
            considered in Theorem 15 (p. 25).  Moreover, there is a direct connection between 
            these notions.
                 Theorem 116.  Let fln  be the binary string formed by first n bits of the binary 
            representation  of Ll.  Then     has  the properties  described  in  Theorem  15  with 
            О (log n) -precision:  each of the objects listed there (say, B(n)) can be algorithmically 
             obtained from f2n+o(iogn)  and vice versa (Q,n can be obtained from B(n + 0(\ogn)).
                 PROOF.  We  already  have  seen  that  given    one  can  construct  an  integer
            t  >  BP(n)  (the  number  of steps  needed  to  exceed  fln).  The  difference  between 
            plain  and  prefix  complexity  (that  could  make  B(n)  greater  than  BP(n))  can  be 
            compensated by an 0(logn)-change in n.
                In the reverse direction, assume that B (n) and n are known.  How do we find 
            ^71-0(l0gn)?  We claim that in the current approximation for fl found after B(n) 
            steps the first n — O(logn) bits are final (i.e., they coincide with the corresponding 
            bits in fl).  If this is not the case, there exists a threshold ß that is a finite binary 
            fraction of length n — O(logn) bits that separates the current approximation and 
            fh  The complexity of ß is at most n — O(logn).  Knowing /3, we can construct a 
            number greater than B(n):  just count the steps needed to get an approximation 
            greater than ß.  For a large enough constant in O(logn), we get a contradiction.  □
                Therefore  we  see  that  knowing  n + O(logn)  bits  in  fl  allows  us  to  answer 
            any  question  about  the  termination  of a program  of size  at  most  n.  Since  the 
            question about the membership in any enumerable set  (e.g.,  questions whether a
        172             5.  MONOTONE  COMPLEXITY
        given statement of size n is provable in some fixed formal theory) has this form, we 
        can follow Chaitin and call Q  “the number of wisdom”  that contains information 
        about many important things.  (Sounds rather romantic, indeed.)
          Returning to more meaningful statements, we have proven that Q, is Turing- 
       equivalent to O'  (we can compute ft having an oracle for halting problem, and vice 
       versa).
                    5.8.  Effective Hausdorff dimension
          The notion of Hausdorff dimension is well known in measure theory  (and it 
       became popular in connection with fractals).  Here is the definition.  Let a > 0 be 
       some real number.  We say that a set A is an а-null set if for any e > 0 there exists 
       a sequence of intervals A that cover A such that
                                  < £■
       This definition assumes that A is a subset of a space where a class of subsets called 
        “intervals”  is chosen and the measure of intervals is defined.  We restrict ourselves 
       to  the  case  of the set  fl.  Here intervals  are the sets  £lx  (where  Q,x  is  the set  of 
       all infinite extensions of a binary string x).  The measure of the interval Q,x equals 
       2-i(x)%
          Let us start with a few simple remarks:
          (1)  Any subset of an a-null set is an a-null set.
          (2) For a = 1 we get the standard definition of a null set (set of measure zero).
          (3) For a > 1 any subset А С П is an a-null set.  Indeed, one can cover A by 2n 
       intervals that correspond to 2n strings of length n, and the sum of their a-measures 
       tends to 0 as n —> oo.
          (4)  Assume that 0 < a < a'.  Any a-null set is then an a'-null set  (note that 
       measure /i(I) of each interval / does not exceed 1 and therefore ß(I)a  ^ ß(I)a).
          166   Give a natural definition for an а-null set of reals, and show that a set 
       A  C  [0,1]  is  an  a-null  set  if and  only  if the  set  of binary  representations  of all 
       numbers in A is an а-null set according to the definition above.
          (Hint:  We need to verify that the more liberal notion of an interval in M where 
       we do not require any alignment, does not change the class of null sets.)
          Our remarks imply that for any set А С П there exists some threshold d £ [0,1] 
       with the following property:  if a  >  d,  the set  A  is  an a-null set;  if a  <  d,  it  is 
       not.  (For a — d the set may be an a-null set or not.)  This threshold is called the 
       Hausdorff dimension of the set A.
          167   The  Cantor set  is  the subset  of [0,1]  that  remains  if we  take out  the 
       middle third (1/3, 2/3), then take out the middle thirds of two remaining segments 
       (i.e.,  (1/9, 2/9)  out  of [0,1/3]  and  (7/9,8/9)  out of [2/3,1],  etc.).  Prove that the 
       Cantor set is a compact set homeomorphic to ft and has Hausdorff dimension log3 2.
          (Hint:  To get an upper bound for the Hausdorff dimension, one may consider 
       the standard intervals, i.e., the intervals that remain after several steps of the Cantor 
       set construction.  To get a lower bound, note that  (1) we may consider only finite 
       covers due to compactness;  (2) if a cover for the Cantor set is given, we can look 
       at its parts that cover the left third and the right third; each of these parts can be 
       scaled to the cover of the entire set due to the self-similarity of the Cantor set.  If
                                 5.8.  EFFECTIVE  HAUSDORFF  DIMENSION                   173
            a is smaller than the threshold, one of these covers is better than the original one, 
            so we may increase the sizes of intervals, and finally get a contradiction.)
                 168       Give a natural definition of the Hausdorff dimension for the subsets of 
            M3.  Explain why the dimension equals 3 for solids, 2 for surfaces, 1 for curves, and 
            0 for isolated points.  Show that for any d G [0,3], there is a subset of M3 that has 
            dimension d.
                The effective version of Hausdorff dimension is defined in a natural way [190, 
            152].  A set A C S7 is an  effective а-null set  (for a given a > 0)  if there exists an 
            algorithm that, for any given e > 0, enumerates a set /о, ii, /2,... of intervals that 
            cover A such that ^2(ß(Ik))a < £•  (Here /r is the uniform measure on fl).
                As in the classical case, the property is monotone (it remains true if cc increases 
            or  A  decreases).  The main difference between  the classical  and effective case  is 
            shown by the following theorem:
                Theorem 117.  For every rational a > 0,  there exists the largest  (with respect 
            to inclusion)  effectively а-null set.
                PROOF.  The proof goes in the same way as for effectively null  (=l-null) sets 
            (Chapter 3).  The countable union of cc-null sets (in the classical sense) is an cc-null 
            set.  In the same way the union of an enumerable family of effectively cc-null sets is 
            an cc-null set.  On the other hand, if cc is a rational number (or even a computable 
            real),  we can enumerate all effectively cc-null sets  (or,  better, the algorithms that 
            serve these sets) by enumerating all algorithms and changing them when too large 
            intervals are generated.                                                     □
                 169       Prove that the largest effectively cc-null set consists of all the sequences 
            oj such that the difference an — K(n) has 110 upper bounds.
                (Hint:  The proof is similar to the proof of the prefix complexity version of the 
            Levin-Schnorr theorem.)
                The following result (A.Khodyrev) is not used in the sequel (for the definition 
            of the Hausdorff dimension, rational cc’s are sufficient),  but it is interesting in its 
            own right.  Let a be an arbitrary real number.
                Theorem 118.  The largest effectively а-null set exists if and only if a is lower 
            semicomputable.
                P r o o f.  Assume that cc is lower semicomputable.  This means that we can gen­
            erate better and better approximations from below to cc, but we do not know their 
            precision.  If we use these approximations  (instead of true cc)  in the requirements 
            for the cover (in the definition of an effectively cc-null set), we get stronger require­
            ments.  Consider the algorithm from the previous theorem that generates covers of 
            the largest effectively cc-null set, and let it use rational lower approximations of cc 
            instead of cc itself, with the following modification.  Do not reject permanently the 
            intervals that violate these requirements, but postpone them and check again when 
            a new approximation to cc arrives.  If a cover satisfies the requirement for the true cc, 
            all its intervals will be eventually let through.
                On the  other  hand,  let  us  assume  that  for  some  cc  there  exists  the  largest 
            effectively  cc-null  set.  Consider  the  algorithm  that  generates  covers  for  it.  This 
            algorithm can be used to obtain lower bounds for cc.  Indeed, if for some rational e 
            the algorithm produces a finite family of intervals (at some step)  and /З-powers of 
            the measures of these intervals exceed e, this means that ß < a.
             174                          5.  MONOTONE COMPLEXITY
                 It  remains to prove that these bounds can be arbitrarily close to a.  Assume 
             that this is not the case and all of them are less than some of  < a.  In this case 
             every effectively er-null set would be at the same time a'-null set, which is not true 
             (there exist sets of any effective Hausdorff dimension; see Problem 170, p. 175).    □
                 The effective Hausdorff dimension of a set A C      is now defined as the infimum 
             of a such that  A is an effective er-null set.  This  number belongs to  [0,1]  and is 
             obviously greater than or equal to the  (classical)  Hausdorff dimension.  (Initially 
             the definition of effective Hausdorff dimension was given in a different way, using 
             computable martingales; see [111, 118], where the properties of effective dimension 
             were established.  See also Section 9.10 about computable martingales.)
                 We have mentioned a paradox:  The property of being an effectively null set 
             depends only on the type of its elements (whether they are random or not).  It is 
             not important  “how many”  elements are in the set.  A similar observation can be 
             made for Hausdorff dimension:
                 Theorem  119.  The  effective  Hausdorff dimension  of the  set is  equal  to  the 
             supremum of the effective Hausdorff dimensions of its elements.
                 (By  effective  Hausdorff dimension  of a point  td  €  Q,  we  mean  the  effective 
             Hausdorff dimension of the singleton {td}.)
                 PROOF.  Obviously  the  (effectively  Hausdorff)  dimension  of a set  cannot  be 
             less  than  the  dimension of its element.  It  remains  to  prove the converse:  if the 
             dimensions  of all  singletons  formed  by  elements  of a  set  A  are  less  than  some 
             rational number r,  and r'  > r is another rational number,  then the dimension of 
             A does not exceed r'.  This is a direct corollary of Theorem 117:  all singletons are 
             subsets of the largest effectively r'-null set, so A is a subset of the same set and has 
             dimension at most r'.                                                               □
                 Therefore  we  need  to  understand  only  what  is  the  (effective  Hausdorff)  di­
             mension of a singleton.  It turns out  that it has a simple description in terms of 
             Kolmogorov complexity.
                 Theorem  120.  The effective Hausdorff dimension of a singleton {td},  where 
             td = tdotditd2 • ■ •,  is  equal to
                                           lim inf C^ oUJl ' 
                                            n—>oо         n
                 (The statement uses plain Kolmogorov complexity of the prefixes of td.  How­
             ever, one can use other versions of complexity:  since the difference between different 
             complexity versions is of order O(logn) for strings of length n, and we divide the 
             complexity by n, we get a term 0(logn)/n that does not change the limit.)
                 Proof.  This result can be derived from the statement of Problem 169, but we 
             provide the direct proof.  We have to prove two inequalities:  one for each direction.
                 Assume that the lim inf is less than a rational number r.  We have to verify 
             that the set  {cd} is an effectively r'-null set for each rational r' > r.
                 For each n we consider all n-bit strings that have complexity less than rn.  There 
             are at most  0(2rn) such strings.  The condition about lim inf guarantees that for 
             infinitely  many  n  the n-bit  prefix of td  is  in the corresponding list.  Consider all 
             intervals Çtz  for all z in the list  (for some fixed n), and compute the sum required
                                   5.8.  EFFECTIVE  HAUSDORFF  DIMENSION                     175
             in the definition of an effectively r'-null set:  there are 0(2rn)  terms  and each  is 
             (2~n)r  _ 2~r n5 so the sum is 0(2(-r~r )n), and we get a converging geometric series
                                                    2(r-r')ri'
                                                 71
             Deleting an initial part of this series (considering only strings of length N or more) 
             we make the sum arbitrarily small (when N is large enough).  At the same time our 
             assumption  (about  liminf)  guarantees that remaining intervals still form a cover 
             for to.  So one inequality is proved.
                 Going in the other direction, assume that {cj} has effective dimension less than r 
             for some rational r.  Let us show that the liminf does not exceed r.
                 By definition, for each rational e > 0 we can generate a sequence of intervals. 
            We know that one of them contains и and the sum of rth powers of the measures 
            does  not  exceed  e.  Let  us  do  this  for  e  —  1,1/2,1/4,....  In  this  way  we  get 
            a sequence of intervals that have finite sum of rth powers of their measures,  and 
            infinitely many of them cover 
                                          oj.  In other words, there exists a computable sequence 
            of intervals xo, x\, x2, ... such that:
                 •  J22~rl(Xi)  < oo;
                 • Xi is a prefix of uo for infinitely many i.
                 The first statement implies that m(i) ^ c2“w^^ for some c and for all i (where 
            m is the discrete a priori probability of natural numbers considered in Chapter 4). 
            Taking the logarithms, we get the bound for prefix complexity,
                                   K(xi) < K(i) + 0(1)      rl(xi) + 0(1)
            for all i.  Note also that the lengths of Xi tend to infinity (since the series is conver­
            gent), that X{ is a prefix of uo for infinitely many i and that the plain complexity does 
            not exceed the prefix one.  (The definition of liminf guarantees that if a sequence 
            has infinitely many terms that do not exceed r, its liminf does not exceed r.)    □
                1170 I Prove the following corollary:  for any real a  G  [0,1]  there exists a set 
            (and even a singleton) that has effective Hausdorff dimension a.
                 (Hint:  The complexity of an initial segment can be increased by adding random 
            bits and decreased by adding zeros.)
                 171  Prove that for an effectively closed subset of the Cantor space (this means 
            that the complement of this set is the union of an enumerable family of intervals) 
            the effective Hausdorff dimension coincides with the classical Hausdorff dimension.
                 (Hint:  Due to compactness, one may consider finite covers and search for them 
            effectively.)
                I  172 I  Find  the  (classical)  Hausdorff dimension  of the  Cantor set  (see  Prob­
            lem 167) using the previous problem and the characterization of effective dimension 
            in terms of singletons and Kolmogorov complexity.
                 173 Prove that for every real a G  [0,1] there exists a set that has (classical) 
            Hausdorff dimension a.
                 (Hint:  Consider the set of all sequences that have zeros at specified places.)
                 174        I  Prove  that  the  definition  of effective  Hausdorff dimension  of a  set  A 
            remains the same if we require the existence of a computable sequence of intervals 
            that has finite sum of rth powers of the measures and that covers each element of 
            A infinitely many times.
             176                          5.  MONOTONE COMPLEXITY
                 {Hint:  If such a cover exists for some a, for a greater o' the same intervals have 
             smaller measure,  and the decrease is more significant  for smaller intervals.  Note 
             that we can delete all short strings from the cover,  due to our assumption  (each 
             element is covered infinitely many times).)
                 We return to the notion of effective Hausdorff dimension in Section 9.5 where 
             its relation to effective martingales is explained.  We show there how to translate 
             the proof of Theorem 120 into the martingale language.
                        5.9.  Randomness with respect to diifferent measures
                 5.9.1.  Changing the measure.  The notion of randomness evidently depends 
             on the underlying measure.  For example, the strong law of large numbers guarantees 
             that sequences that are ML-random with respect to the Bernoulli measure Bp have 
             limit frequency p, so for different p we get disjoint sets of random sequences.  Still 
             from the viewpoint of computability theory the properties of ML-random sequences 
             (with respect to a computable measure P) do not depend on P—except for some 
             trivial cases.
                 The trivial case we have in mind is the following one:  if a computable measure 
             // has an atom, i.e., if some sequence (a singleton) has positive //-measure, then this 
             sequence is random  (it cannot be an element of a //-null set).  Such a sequence is 
             always computable.  This is a corollary of Theorem 79(h) (p. 123), but has also the 
             following simple proof.  Assume that {w} has a positive probability e with respect 
             to  a computable distribution //.  Let us consider //-measures of the sets     where 
             X is a prefix of w.  These measures decrease as x becomes longer,  and their limit 
             is £.  Wait until some of them become less than  l.le.  If x is such a prefix,  only 
             one of the strings t0 and xl has //-measure greater than 0.9e, and this prefix can 
             be effectively  found since //  is  computable.  So  the sequence can  be  computably 
             extended starting from this point.
                 To  avoid  this  special  case,  we  consider  only  atomless  measures  where  each 
             individual  sequence  has  measure  0.  If /q  and  //2  are  two  computable  atomless 
             measures,  then the sets of ML-random sequences with respect to /q  and //2  are 
             essentially the same from the computability viewpoint:
                 Theorem  121.  Let /q  and //2  be two atomless measures.  Then there exists a 
             bijection between the sets of ML-random sequences with respect to /q  and //2  that 
             in both directions is a restriction of a computable mapping of type £ —>•£.
                 In other words, there exist oracle machines M12, M21 with the following prop­
             erties:  if an oracle is a sequence ш that is ML-random with respect to /q, then 
             is an infinite sequence that is random with respect to //2, and vice versa; these two 
             mappings are mutually inverse (on random sequences).
                 Proof.  Following [225], consider first a special case when one of the measures 
             (say, //2) is the uniform measure on [0,1].  We want to construct a one-to-one cor­
             respondence between sequences that are /q-random and uniformly random points 
             in  [0,1].  As usual,  we split  [0,1]  into two intervals:  the left interval 7To  of length 
            /q(L>0), and the right interval 7Ti of length /q(f2i).  Each of the intervals 7To and 7Ti 
            is then split in a similar way, etc.  Then for each sequence uj consider a real number 
            that is a common point of all 7rx  for all prefixes x of cj.  Since /q  has no  atoms, 
            such a common point is unique.
                      5.9.  RANDOMNESS WITH RESPECT TO  DIFFERENT MEASURES        177
               We have constructed a mapping of Q to  [0,1]  that  is an isomorphism in the 
           sense of measure theory.  It is not a one-to-one mapping since the endpoints of the 
           intervals have two preimages, but the endpoints form a (countable) set of measure 0. 
           The computability of the measure guarantees that effectively null sets with respect 
           to ßi  correspond to the effectively null sets with respect to the uniform measure, 
           therefore we get a bijection between the sets of ML-random sequences with respect 
           to corresponding measures.  (Note that the endpoints of the segments,  as well as 
           corresponding sequences xOOO • • •  and rrl 11 • • •, are not random.  Note also the ß\- 
           measure of some Qx can be zero, and then its image is one point, but this does not 
           matter; all the sequences starting with x are then non-random.)
               It  remains to do the same for ß2  and then take the composition of these two 
           bijections  (using  [0,1]  as  an  intermediate  step).  The  computabity  of the  corre­
           sponding mappings is easy to prove since both measures ßi and ßo are assumed to 
           be computable.                                                          □
               Using the language of computability theory,  we can state a corollary of this 
           result.  Recall that two sequences (or two sets:  we identify a set and its character­
           istic sequence) are  Turing-equivalent (belong to the same Turing degree) if each of 
           them is computable by a machine that uses the other sequence as an oracle.  The 
           equivalence classes  are called  Turing  degrees.  Our  theorem shows that  the class 
           of Turing degrees of ML-random sequences does not depend on the choice of an 
           atomless computable measure.
               175       Prove that  every sequence that  is  random with respect  to  some com­
           putable measure ß (not necessarily atomless) is either computable or Turing-equiv­
           alent to a uniformly ML-random sequence.
               (Hint:  Consider the intervals 7rx. for x that are prefixes of to and their common 
           point.  If it is not unique, then ui is computable.  If the common point 2: is unique, 
           then 2 is uniformly random and can be computed given an oracle for to.  On the 
           other hand, uj is computable if we have approximations to 2 as an oracle:  we use 
           that 2 is random and therefore different from all the endpoints of the intervals.)
              5.9.2.  “Absolutely non-random sequences”.  Consider some sequence uj. 
           We want to find a computable measure ß such that ui is ML-random with respect 
           to ß.  Is it always possible?  The answer turns out to be negative.
              Theorem 122.  There exists an infinite sequence of zeros and ones that is not 
           ML-random with respect to any computable measure on Q,.
              Sequences  that  are  random  with respect  to  some  computable  measure  were 
           called  “proper”  in  [225]  (English  translation).  The  theorem states  that  not  all 
           sequences are proper.  There are different ways to construct a non-proper sequence. 
           We start with the most intuitive one that uses the a priori randomness deficiency. 
           Recall  that  the  ML-randomness  criterion  (for  a computable  measure  P)  can be 
           reformulated in the following way.  For each string x consider the difference
                                 dp(x) = -  log2 P(flx) - KA (x).
           The sequence to is ML-random with respect to P if this difference is bounded (by 
           a constant)  for the prefixes of u).  So we may call this difference the  randomness 
           deficiency  of a string x  (with respect  to  computable  measure  P):  a sequence is 
           random if the deficiencies of its prefixes are bounded (by a constant).
                         5.  MONOTONE COMPLEXITY
        178
          The name “randomness deficiency” is quite general and may be understood in 
        different ways in different contexts.  We already considered the expectation-bounded 
        and probability-bounded deficiencies for infinite sequences, and in Chapter  14 we 
        consider the randomness deficiency of an element of a finite set.  However, in this 
        section by randomness deficiency we mean the function dp defined on finite strings 
        as explained above.
          The definition above assumes that P(QX) > 0; if P(QX) = 0 for some x, we let 
        dp(x) = Too.
          The randomness deficiency is always non-negative (up to a constant); see The­
        orem 89.
           176   Prove that for every string x the deficiency of at least one of the strings 
        xO and xl  does not  exceed the deficiency of x.  (We assume that  a computable 
        measure P used in the definition of the deficiency is fixed.)
          This problem shows that we can start with an arbitrary string with finite defi­
        ciency (non-zero measure) and extend it bit by bit not increasing its deficiency.  The 
        randomness criterion guarantees that in this way we get an ML-random sequence 
        with respect to the measure used in the definition of deficiency.
          After the notion of deficiency is introduced,  we return to the proof of Theo­
        rem 122.
          P r o o f.  To  get  a  “non-proper”  sequence w,  we need to ensure that for every 
        computable measure P there is a prefix of со that has large randomness deficiency 
        with respect to P.  So we get a countable family of requirements:  for each measure 
        P and for each c the corresponding requirement says that some prefix has deficiency 
        at least c with respect to P.
          Using a diagonal construction,  we fulfill  these  requirements  one  by  one.  At 
        each step we add to a current prefix some additional bits to ensure that the next 
        requirement is fulfilled.  So we need to check that  for each string x and for each 
        computable measure P and constant c there exists an extension у of x that has 
        deficiency at least c with respect to P.  Indeed, we may extend x by adding a bit 
        in such a way that the P-ineasure decreases at least by a factor of 1.5, then do this 
        again, etc.  This can be done effectively, so the complexity of the prefixes increases 
        slowly, while the measure decreases fast, so we get an arbitrary large deficiency.  □
          Essentially  the  same  argument  can  be  explained  using  “generic”  sequences. 
        Recall that a subset A of Q is everywhere dense if it has non-empty intersection with 
        every interval.  A famous Baire theorem says that the intersections of a countable 
        family of open sets  Ai  (an open set  is a union of intervals)  that  are everywhere 
        dense is non-empty and, moreover, everywhere dense.
          1177 I Prove the Baire theorem starting with any string and adding suffixes to 
        get inside dense open sets (one by one).
          Now we consider effectively open sets (unions of enumerable families of inter­
        vals)  that  are everywhere dense.  We get a countable family of open sets that are 
        dense everywhere.  Their intersection is an everywhere dense sets whose elements 
        are called generic sequences.  (The full technical name is weakly 1-generic sequences; 
        see  [147,  Definition  1.8.47].)  Informally speaking,  a generic sequence violates ev­
        ery law that prohibits an enumerable dense set of prefixes.  (Every string has an 
        extension that violates the law, and violations can be effectively discovered.)
                            5.9.  RANDOMNESS  WITH  RESPECT TO  DIFFERENT  MEASURES                       179
                    178 Prove that every generic sequence violates the Strong Law of Large Num­
              bers.
                   (Hint:  The set of binary strings of length greater that N that have more than 
              99% of ones is a dense effectively open set; the same is true for the set of strings 
              with more than 99% of zeros.)
                    179 Prove that no generic sequence is computable.
                   (.Hint:  The set of all sequences that differ from a given computable sequence is 
              open and everywhere dense.)
                   Note that  the definition of a generic sequence  (unlike randomness)  does not 
              refer to any measure.
                    180 Prove that a generic sequence is not  ML-random with respect  to  any 
              computable measure.
                   (Hint:  It  is enough to construct  an effectively open dense set that has small 
              measure.  This can be done by iteratively choosing a smaller half of an interval or 
              almost smaller if the halves have almost equal size.)
                   Zvonkin and Levin ([225], the remark after Definition 4.4) mentioned another 
              way to construct  a sequence that is not random with respect to any computable 
              measure.  They claim that it is easy to show that the characteristic sequence of 
              the universal enumerable set  is not ML-random with respect  to any computable 
              measure.  They don’t say what kind of universality is needed, but indeed one can 
              find an enumerable set with this property:
                    181  Show that there exists an enumerable set whose characteristic sequence 
              is not random with respect to any computable measure.
                   (Hint:  The complexity of the prefixes of every characteristic sequence of an 
              enumerable set is logarithmic; it remains to guarantee that any computable measure 
              of the prefixes decreases fast.  This can be done as follows.  We split N into countably 
              many arithmetic sequences and devote zth of them to an ith computable measure; 
              our goal is that the sequence of bits appearing at these places is not random with 
              respect to the projection of the zth measure on the corresponding coordinates.  It 
              can be done by choosing a direction where measure decreases fast.  (Then we use 
              Theorem 123.)  Since we do not know whether the ith algorithm indeed computes 
              a computable measure, we get an enumerable set, not a decidable one.)
                   It is interesting that not every enumerable set has this property:
                    182 Construct an enumerable undecidable set whose characteristic function 
              is ML-random with respect to some computable measure.
                   (Hint  (L.  Bienvenu):  Let  a*  be  a computable sequence  of rational  numbers 
              that is dense in [0,1].  Consider a computable mapping of Q, to itself:  a sequence 
              a is interpreted as a binary fraction in  [0,1]  and mapped to a sequence uj where 
              u>i  =  1  if a{  < a and щ = 0 if    > a.  (If a is one of the a*, then u>i is undefined.) 
              This mapping is almost everywhere defined (with respect to the uniform measure); 
              the  image of the uniform  measure is therefore a computable measure on fl,  and 
              the image of a lower semicomputable ML-random real is a sequence that is ML- 
              random with respect to the image measure and at the same time is a characteristic 
              sequence of an enumerable undecidable set.  (To prove undecidability, we use that 
              ai  are dense in [0,1].)  See Sections 5.7 and 5.9.3.)
        180             5.  MONOTONE COMPLEXITY
          We have constructed several examples of sequences that are not random with 
       respect to any computable measure.  But one may ask a different question:  Is there 
       a sequence that is not Turing-equivalent to any ML-random sequence?  Here we do 
       not  need to specify a computable measure,  since all the measures have the same 
       degrees of random sequences (see above).  This is possible, too:
          183    Prove  that  there  exists  a  non-computable  oracle  A  such  that  no  A- 
       computable sequence is random with respect to a computable measure (unless the 
       sequence is computable and the measure has an atom).
          (Hint:  First,  we  may  consider  only  the  uniform  measure.  Then  we  use  a 
       diagonal construction to get the required set  A.  First,  for every i we can add a 
       prefix of A that guarantees that A is not computed by the ith machine.  On the 
       other hand,  for  every i  we  can  add  a prefix  that  guarantees  that  either  (1)  the 
       ith machine with oracle A computes a non-total sequence, or (2) the ith machine 
       computes a sequence that has a prefix with large deficiency.  Indeed, if there is some 
       extension of the current oracle prefix that allows the ith machine to compute a long 
       sequence, choose the first such extension, and the corresponding long sequence will 
       have small complexity; if there is no such extension, the function computed by the 
       ith machine is guaranteed not to be total.)
          The  statement  of the  last  problem  is  also  a  corollary  of several  more  diffi­
       cult  results that are not included in our book.  First, V. Vyugin has shown  [215] 
       that there exists a probabilistic machine that with positive probability generates se­
       quences with this property (sequences that are not Turing-equivalent to any random 
       sequence).  This sounds like a paradox:  The property implies that for a sequence 
       a there is no computable measure that  “explains”  a  (makes a  random with re­
       spect to this measure).  On the other hand, there is a machine that generates such 
       “unexplainable”  sequences with positive probability—so why not take the output 
       distribution of this machine as an explanation?  The solution of this paradox:  The 
       output distribution is a semimeasure, not a measure (the machine generates finite 
       sequences with positive probability).
          There is another, completely different, argument:  We can derive the statement 
       of the problem from recent  (but already classical)  results about  low sets  (see the 
       books of A. Nies [147], R. Downey and D. Hirschfeldt [49]; a simplified exposition 
       can be found in [20]).  These results say that there is an enumerable undecidable set 
       A that is low for Martin-Löf randomness:  Adding A as an oracle does not change 
       the set  of ML-random sequences  (and it  also does not  change prefix complexity, 
       but this is not needed now).  For this A no А-computable sequence can be random 
       (since it is not А-random).  In this way we get a set A with an additional property 
       (A is enumerable).
          So for many different reasons there exists a sequence such that no ML-random 
       sequence is reducible to it.  In the other direction the situation is different:  Every 
       sequence  is  Turing-reducible  to  some  ML-random  (with  respect  to  the  uniform 
       distribution) sequence; see below Theorem 126, p.  189.  The proof of this theorem 
       implies also that every Turing degree above O' (every Turing degree that computes 
       the halting problem) contains a random sequence; see Problem 190 (p.  190).
          184   Prove that there exist a sequence ш that is Turing-equivalent to a uni­
       formly ML-random sequence, but ш itself is not random with respect to any com­
       putable measure.
                      5.9.  RANDOMNESS  WITH RESPECT TO DIFFERENT MEASURES        181
               (Hint:  We interleave two sequences:  at  positions  0,2,4,...  we put  a generic 
           sequence  7;  and  at  positions  1,3,5,...  we  put  an  ML-random  sequence  ы  that 
           computes 7.  The resulting sequence is Turing-equivalent to ui.  Note also that if a 
           sequence is ML-random with respect to some measure P, that its subsequence with 
           even indices is ML-random with respect to the projection of P on these coordinates; 
           see Theorem 123.)
               The sequences that are not random with respect to any computable measure, 
           are similar  (in a sense)  to non-stochastic objects in the sense of Kolmogorov (see 
           Section 14.2).  Moreover, one can show that if a sequence a is random with respect 
           to  a computable measure,  then its prefixes  are stochastic  objects  (Problem  349, 
           p. 430).
               5.9.3.    Image randomness.  We started this chapter by considering a proba­
           bilistic machine that consists of a (fair) random bit generator and an algorithm that 
           transforms this sequence of random bits into a finite or infinite output sequence.  Let 
           us return to this scheme and assume that with probability 1 the output sequence 
           is infinite.  In this case we get a computable output distribution ß.
               A (slightly philosophical) question arises:  Which infinite sequences are plausible 
           as outcomes of such a machine? There are two possible answers.
               First, we have a definition of Martin-Löf randomness that can be applied to the 
           computable distribution ß.  We can say that plausible sequences are the sequences 
           that are ML-random with respect to this distribution.  On the other hand, we can 
           look inside the machine and ask, Which sequences are plausible as the outputs of a 
           random bit generator?  The natural answer is ML-random sequences with respect 
           to uniform distribution.  According to this answer, plausible output sequences are 
           images of ML-random sequences (with respect to uniform distribution) under the 
           computable transformation performed by the machine.
              Which of these two answers is more philosophically convincing?  Fortunately, 
           we do not need to make a choice here, since these two classes coincide.  Here are 
           the exact statements and proofs.
              Let ß be a computable probability distribution on Q, and let /: E —> E be a 
           continuous computable mapping.  Consider the image of the measure ß with respect 
           to /, i.e., a measure и on the set E such that
                                       v(U) = li(rl(U))
           for  any  U  С  E.  In other words,  v is the probability distribution of the random 
           variable     where w is a random variable that has distribution ß.  In the general 
           case the distribution и is not  concentrated on S7  and may assign positive proba­
           bilities to finite sequences;  in our terminology и may be a semimeasure (and this 
           semimeasure is lower semicomputable),  not a measure.  Let us assume,  however, 
           that it is not the case and that и is a measure on Q.  (It is easy to see that in this 
           case v is a computable measure.)
              Theorem 123.  (a) For any sequence ui E fl that is ML-random with respect to 
           measure ß,  its image f(ui)  is  an infinite sequence  that is ML-random with respect 
           to measure v.
               (b)    Any sequence r that is ML-random with respect to v can be obtained in this 
           way,  i.e.,  there  exists a sequence w  that is ML-random with respect to ß such that 
           f  M  -  T.
             182                          5.  MONOTONE COMPLEXITY
                 Recently M. Hoyrup found that this statement remains true for the so-called 
             layerwise computable mappings.  This class contains all computable almost every­
             where defined mappings and looks like the right generalization making the proof 
             balanced and natural.  Still we restrict ourselves to the classical case of computable 
             mappings in this book and refer the interested reader to the exposition in [15] for 
             the general case.
                 Proof.  First,  let  us  prove  that  the  /-image  of a  /л-random  sequence  и  is 
             infinite.  If this  is  not  the  case  and  /(cu)  is  a finite string  z,  consider  all  infinite 
             sequences и such that f(iv) = z, i.e., the /-preimage of the set Ez \ (Ezo U Ezi).
                 The preimage of ftz  is  an  effectively  open  set  (the  union  of an  enumerable 
             set  of intervals),  and  the  preimage  of  Ezo U Ezi  is  another  effectively  open  set 
             that is a subset of the first one.  To get the contradiction,  we have to prove that 
             the preimage of the difference (=the difference of the preimages) does not contain 
             random sequences.  This is a special case of the following general statement.
                 Lemma 1.  Let p be a computable measure on ft, and letU с V be two effectively 
             open sets such that p(V \ U) = 0.  Then V \U  is an effectively null set  (=does not 
             contain random sequences).
                 Proof.  It is enough to consider one interval I in the set V (and replace U by 
             its intersection with I).  Enumerating the intervals that form the set U, we cover 
             more and more points in I.  By continuity the measure of the covered part converges 
             to the measure of the interval I (since V \ U has zero measure).  Therefore, we can 
             wait until the remaining part of I has measure less than e for any given e and find 
             a cover of I \ U by a (finite) family of intervals with small total measure.
                 Lemma 1 is proven (and we did not use that V is effectively open; the same is 
             true for every open set V).
                 To finish the proof of (a) we have to show that the image f(<w) of a /л-random 
             sequence w cannot be an infinite but not z/-random sequence.  Indeed, assume that 
             that  f(uj)  is  infinite  but  does  not  form  an  effectively  z/-null  set.  The  preimages 
             of the intervals that cover f(<w)  cover cj,  and we get  an effectively open set that 
             contains и and has small measure (recall that the /л-measure of the preimage of an 
            effectively open set is equal to the z/-measure of the set itself).  The statement (a) 
             is proven.
                  185       Prove a quantitative version of this statement:  The expectation-bounded 
            deficiency  of  the  sequence  f(cv)  with  respect  to  measure  v  is  bounded  by  the 
            expectation-bounded deficiency of и with respect to /л plus  a constant  that  de­
            pends on the measures and the mapping but not on u.  (In this problem we use the 
            randomness deficiency for infinite sequences as defined in Section 3.5.)
                 Let us now prove the statement (b) using the notion of deficiency (for finite se­
            quences, as defined on p. 177 using a priori complexity).  Assume that the sequence т 
            is ML-random with respect to the measure Q.  This means that the deficiencies of 
            its prefixes are bounded (by a constant).  Then we apply the following lemma that 
            can be considered as the finitary version of statement (b).
                 Lemma 2.  Let и be a string such that iy(ftu) > 0.  Then there exists a string w 
             such that и   f(w)  (u is a prefix of f(u>))  and dß(w) ^ dv(u) + 0 (1).
                 (The constant hidden in 0(1) may depend on /, /л, and v but not on гл; dß and 
            dv denote the corresponding deficiencies.)
                       5.9.  RANDOMNESS  WITH  RESPECT TO  DIFFERENT  MEASURES         183
                Proof.  Consider the preimage Fu — f~ 1(T,u) of Eu.  This is an effectively open 
            subset of E.  By definition, the //-measure of the set Fu  (recall that the measure // 
            is concentrated on infinite sequences) equals i/(Eu).  If the deficiency dv{u) is small, 
            i/(Eu) cannot be significantly less than the continuous a priori probability of Eu.
                Now consider the continuous a priori probability of the set Fu, i.e., the proba­
            bility of the event  “the output of an universal probabilistic machine M belongs to 
            Fu".  This event can be rephrased as follows:  the output of the machine foM  (that 
            applies /  to the output of M ) starts with u.  Comparing the machine /  о M and 
            the universal one, we conclude that the (continuous) a priori probability of the set 
            Fu can be only a constant times bigger than the (continuous) a priori probability 
            of Eu.  The latter is 2dv^  times bigger than vÇEu) that is equal to the //-measure 
            of the set Fu.  Therefore we get an inequality between two measures of Fu  (the a 
            priori probability a and //):
                                          a(Fu) < 0 (2d"(u)).
                                          KFu)
            Since the set  Fu  can be represented as the union of a (possibly non-enumerable) 
            family of disjoint intervals, we conclude that the similar inequality is true for some 
            interval T,w in this family:
                                               < ^Ли) -0{l).
            Since T,w  C  Fu, we conclude that f(w)  >  u, and the preceding inequality implies 
            that dß(w) < dv{u) + 0(1).  Lemma 2 is proven.
               Now we continue the proof of statement  (b).  Let tn  —  (r)n  be the prefix of 
            a i/-random sequence т that has length n.  The randomness criterion guarantees 
            that  ^-deficiencies  of ti  are  bounded.  Then  the  lemma says  that  there  exists  a 
            sequence of strings wq,w\, ... that have bounded //-deficiencies such that f(wi) is 
            an extension  of t{.  If we  knew  that  all Wi  are  compatible,  this  would  give  us  a 
            desired result  (a random preimage of t).  However,  there is no reason to expect 
            this.
               Nevertheless, a standard compactness argument shows that the sequence Wi has 
            a subsequence that either consists of identical strings or converges to some infinite 
            sequence oj.  The latter means that  any  (finite)  prefix of a;  is  a prefix of all  but 
            finitely many strings in the sequence.
               In the first case the sequence т is the image of the finite string w that appears 
            infinitely often in the sequence Wi.  This can happen for a i/-random sequence r if 
            this sequence (the corresponding singleton) has a positive measure; т is computable 
            in this case.  Then we let oj be any //-random continuation of the string w (we know 
            that it exists, since the //-deficiency of w is finite and ß(ftw) > 0).
               In the second case an infinite subsequence of the sequence W{  converges to oj. 
            To  prepare  ourselves  for  this  case,  let  us  make  a digression  and  prove  that  the 
            randomness deficiency is almost monotone.
               Recall the randomness criterion (Theorems 91 and 93).  It guarantees that for 
            ML-random sequences the deficiency of their prefixes is bounded while for non- 
            random sequences the deficiencies tend to infinity.  This implies that the interme­
            diate situation  is not  possible:  There is no sequence such that  deficiencies of its 
            prefixes are not bounded but do not tend to infinity.  This looks rather strange, and 
            one may ask why this happens.  The following theorem provides some explanation.
                                        5.  MONOTONE COMPLEXITY
            184
                Theorem 124.  Let P be a computable measure on Q.  There exists a constant 
            c  such  that,  for  every  string x  and for every string y  that has x  as  a prefix,  the 
            inequality
                                      dp(y) ^ dP(x) -  2 log dp (x) -  c
            holds.
                Informally speaking,  every continuation  of a string with high deficiency has 
            (almost  as)  high deficiency;  or a prefix of a string that  has small deficiency,  has 
            (almost as) small deficiency.  So the deficiency function is almost monotonie.
                Proof.  For each к consider the enumerable set of all finite sequences that have 
            deficiency greater than k.  All the infinite continuations of these sequences form an 
            open set Sfc, and the P-measure of this set does not exceed 2~k.  Now consider the 
            measure Pk on fl that is zero outside Sk and is equal to 2kP inside Sk■  That means 
            that  for every set  U the value Pk(U)  is defined as 2kP(U П Sk).  Actually,  Pk  is 
            not a measure according to our definition, since Pk(Ll) is not equal to 1.  However, 
            Pk can be considered as a lower semicomputable semimeasure if we change it a bit 
            and let Pk(Ll) = 1  (this means that the difference between 1 and the former value 
            of Pk(Ll) is assigned to the empty string).
                Now consider the sum
                                                   к
            It is a lower semicomputable semimeasure (the factor 2 in the denominator is used 
            to  make  the  sum  ]T l/(2A;2)  less  than  1);  again,  we  need  to  increase  5  so  that 
            S(Q) = 1.  Then we have
                               -  logS(:r) < — logP(x) — к + 2 log к + 0 (1)
            for every string x that has a prefix with deficiency greater than k.  Since 5 does 
            not exceed the continuous a priori probability (up to an O(l)-factor), we get the 
            desired inequality.
                Here we assume that the deficiency of x is finite, i.e.,  P(flx) Ф 0;  if P(flx) = 
            0,  then  P(Lly)  =  0  for  any  у  that  has  prefix  x,  and  the  deficiency  of у  is  also 
            infinite.                                                                        □
                Let us return now to the proof of Theorem 123.  We have a sequence of strings 
            (a  subsequence  of  Ц })  that  converges  to  some  lj  €  ft.  All  Wi  have  small  p- 
            deficiencies.  In this case:
                (1) Any prefix of w is a prefix of some Wi, and all Wi have bounded /U-deficiencies. 
            Therefore,  Theorem  124  guarantees  that  /г-deficiencies  of  all  prefixes  of lj  are 
            bounded.  So the sequence lj  is ML-random with respect to p.
                (2) As we have proved in Theorem 123(a), the sequence f(cj) is infinite.
                (3)  The sequence f(cj) cannot have a prefix that is not a prefix of r.  Indeed, 
            in this case lj  would have a prefix и  whose image is incompatible with r ;  then the 
            string и  is a prefix of almost all strings in the subsequence that converges to lj,  but 
            images of Wi have arbitrarily long common prefixes with r .
                This contradiction finishes the proof of Theoreml23(b).                      □
                This  proof of Theorem  123  illustrates  the  use  of the  randomness  deficiency 
            notion.  One can also give a more direct proof (suggested by Muchnik in the 1980s):
                      5.9.  RANDOMNESS WITH  RESPECT TO  DIFFERENT MEASURES         185
                186 Give a direct proof of Theorem 123(b) using the definition of an effectively
           null set.
               (Hint:  For a given e consider the family of intervals Z£ that covers the largest 
           effectively //-null set and has total /u-measure less than e\ let F be the (closed) set 
           of non-covered sequences.  All sequences in F are random,  so  /  is defined  (=has 
           infinite sequences  as images)  and continuous on F.  The image of a compact set 
           F is a compact set and therefore is closed.  It has measure at least 1 — e, since its 
           preimage contains F.  Its complement is an open set that has measure at most s 
           and covers all the points that do not have preimages in F.  The only problem is 
           that one should prove an effective version of the theorem that says that the image 
           of a compact set under a continuous mapping is compact,  and conclude that the 
           complement to /-image of F is not only an open set, but a uniformly effectively 
           open set.)
               A similar argument allows us to prove a quantitative version of the statement 
           Theorem 123(b) saying that the bound provided by Problem 185 is tight:  the expec­
           tation-bounded /'-deficiency of uj equals (up to an 0 (1) additive term) the infimum 
           of expectation-bounded //-deficiencies of all /-preimages of uj.  See  [15]  for more 
           detail.
               I  187 I  Prove  a statement  that  can  be  considered  as  a finitary version  of the 
           statement (a) of Theorem 123:  if и and w are binary strings such that и ^ f(w), 
           then
                                dv{u) ^ dß{w) + 21ogdß{w) + 0 (1).
               (Hint:  The set  of sequences having large  /'-deficiencies  can be covered  by  a 
           set of small /'-measure, therefore their preimages can be covered by a set of small 
           //-measure and have large //-deficiency.  Note that this statement is a generalization 
           of Theorem 124.)
               Theorem 123 has some (rather surprising) applications.  Here is an example:
                188      Let со be an ML-random sequence with respect to the Bernoulli distri­
           bution  (independent  coin tosses)  where  1  has probability  1/3.  Prove that  there 
           exists a sequence uj'  that is random with respect to the uniform distribution (1 has 
           probability 1/2) and can be obtained from со by replacing some zeros by ones.
               (Hint:  Consider  an  ML-random sequence of independent  random reals uni­
           formly distributed in  [0,1],  or,  better to say,  the random sequence of bits placed 
           in  a two-dimensional table where  (infinite)  rows are considered as infinite binary 
           fractions.  Then  convert  this  sequence  into  a  bit  sequence  using  threshold  2/3. 
           Theorem 123 guarantees that we get an ML-random sequence with respect to the 
           1/3-Bernoulli distribution and that any ML-random sequence with respect to this 
           distribution can be obtained in this way.  Then we can change the threshold to 1/2.)
               Another corollary of Theorem 123 and its generalizations are discussed in the 
           next section.
               5.9.4.  Michiel  van  Lambalgen’s  theorem.  Consider  a probabilistic  ma­
           chine that tosses a fair coin to get a sequence ujquj\uj2 ■ • ■  and then outputs every 
           other bit,  i.e.,  the sequence ujquj2UJ^ • ■ ■ ;  the output distribution of this machine is 
           uniform.  Theorem 123 for this machine therefore implies the following:
               (a) if ujquj\uj2 ■ • •  is ML-random with respect to the uniform Bernoulli measure, 
           then ujquj2UJ$ • ■ ■  is ML-random with respect to the same measure;
                                         5.  MONOTONE COMPLEXITY
             186
                 (b) for every sequence         • • •  that  is ML-random with respect to the uni­
             form Bernoulli measure, there exists a sequence 011013015 ■ ■ •  such that their mixture 
             010011012 • • •  is ML-random with respect to the same measure.
                 The first statement is more or less obvious, but the second is more difficult.  It 
             can be rephrased in terms of pairs of sequences:  If a is ML-random with respect 
             to the uniform measure, there exists a sequence ß such that the pair (a, ß) is ML- 
             random (in a natural sense, with respect to the product of uniform measures on 
             each coordinate).
                 We can go further and ask, We know that such a ß exists, but what properties 
             of ß  are  needed  to  make  the  pair  (a,ß)  random?  It  is  clear  that  ß  should  be 
             random (see above), but this is not sufficient.  For example, if we let ß = a, we get 
             a non-random pair (a, a):  it corresponds to the sequence u>q(jJ\ • • •  where each bit 
             is doubled.
                 The answer to this question is provided by van Lambalgen’s theorem [90]:  the 
            sequence ß should be ML-random and remain ML-random even if we allow the use 
            of a as an oracle in the definition of ML-randomness.
                 Let  P  and  Q  be  two  computable  distributions  on  Cl.  Consider the product 
             P X Q, which is a computable distribution onflxfl (this space is isomorphic to ft, 
             and the definitions of randomness can be easily extended onto it).
                 Theorem  125.  A  pair of sequences  {£,77)  is  ML-random with respect  to  the 
             distribution P x Q if and only if the following conditions are both true:
                 (1)  £  is ML-random with respect to P;
                 (2)  77  is ML-random relative to £  (with oracle £)  with respect to Q.
                 Speaking about  relativized  randomness,  we  mean that  the  algorithm,  which 
             (for a given e  >  0)  enumerates the intervals in the cover,  now has access to £ as 
            an oracle (so we get more enumerable sets, more non-random sequences, and fewer 
            random sequences).
                 Note also that  the conditions  (1)  and  (2)  are not  symmetric with respect to 
            £  and  77.  Theorem  125  implies  that  condition  (1)  can be replaced  by  a stronger 
            requirement:  £ is random relative to 77.  However, the non-symmetric version looks 
            more natural.  It can be read as,  “to produce a random pair, first choose a random 
            £ and then choose a random 77 knowing £ (=random relative to £)”.
                 Proof.  Let us prove first that conditions  (1)  and  (2)  are true for a random 
            pair (£ 77).
                 (1)  If the sequence £ is not  random and can be covered by intervals of small 
            measure,  then the same  intervals multiplied  by ft  (along the second  coordinate) 
            become rectangles (products of intervals along both coordinates) that cover (£, 77) 
            and have small measure.  (We can also refer to Theorem 123.)
                 (2) Assume that 77 is not random with oracle £.  Then for each e we can (using 
            £ as an oracle) enumerate intervals that cover 77 and have small Q-measure.  This 
            enumeration process can be run with any oracle and it will generate some intervals 
            using a finite amount of information about the oracle.
                Therefore, we get (for a given e > 0) a family of rectangles that is enumerable 
            (without oracle) and has the following property:  If the first coordinate is fixed to 
            be £, the rectangles become a family of intervals with total Q-measure at most e. 
            This family can be easily converted into a family of rectangles for which all vertical 
            sections (not only the ^-section) have the same property and all the sections where
                               5.9.  RANDOMNESS  WITH  RESPECT TO  DIFFERENT  MEASURES                              187
               this inequality was true before the conversion remain untouched.  This contradicts 
               the randomness of (£, 77), since we can get a family of rectangles that cover (£, 77)  and 
               have total measure at most e (since every vertical section has measure at most e).
                     Now let us prove that if the pair (£, 77) is not random, then one of the conditions 
                (1)  and  (2)  is  false.   Assume  {£,77)  is  not  random.  Let  U  be  the  union  of an 
               enumerable family of rectangles in Q, x ft of measure at most e that covers (£,77). 
               For each fixed value of the first coordinate x, let Ux denote the x-section of U, i.e., 
               the set  {y\{x,y)  €  U}.  Consider the values of x such that the Q-measure of Ux 
               is  greater than y/e.  We get a set of P-measure at most у/e that is a union of an 
               enumerable family of intervals.
                     There  are  two  possibilities:  Either £  is  covered  by  an  enumerable  family  of 
               intervals having total P-measure at most  yfe that we have constructed,  or  (£, 77) 
               is  covered  by  a  family  V  of rectangles  such  that  the  Q-measure  of                 does  not 
               exceed yfe.  (Other sections may have bigger measure, this does not matter.)  In the 
               second case rj is covered by a £-enumerable family of intervals of total measure at 
               most y/e.
                    We would like to apply this argument for every e and conclude that either £ is 
               not random or rj is not random with oracle £.  The first conclusion can be drawn 
               if for every e the first possibility happens; the second one, if the second possibility 
               happens for every e.  But what  should we do if both cases happen for different 
               values of el
                    The following simple  trick  helps.  For  every  к  =  1,2,3,...  we  perform  this 
               construction for e — 2~2k.  Then for each к we get a family V(k) of intervals (along 
               the first coordinate)  that  have total P-measure at most  2~k  =  y/2~2k.  Now the 
               two possibilities are as follows:
                     (a) the family V(к) covers £ for infinitely many k\
                     (b)  for sufficiently large к the family V(k) does not cover £.
                    If (a) happens, for each К the union of V(к) for all к ^ К gives us an enumer­
               able cover of £ that has total measure 2 • 2~K, so £ is not random.
                    If  (b)  happens,  then for each  к greater than some К one can £-enumerate a 
               family of intervals that covers rj and has total Q-measure at most 2~k, so rj is not 
               £-random.  (We do not know the value of K, but this does not matter.)                                □
                    This theorem also has a quantitative version  (see  [209],  or  [7]  for a detailed 
               exposition):  one can prove that the expectation-bounded deficiency d of the pair 
               (£, 77)  with respect to P x Q is equal to the sum of the expectation-bounded defi­
               ciency d\  of £ with respect to P and the expectation-bounded deficiency cfo  of rj 
               with respect to Q using the oracle for £ and a condition  |_dij  (the integer-rounded 
               value of the first deficiency).  To make this statement precise, one should give the 
               definition of expectation-bounded deficiency with oracle and condition (as a func­
               tion of a sequence, oracle,  and condition),  and this can be done.  In this way we 
               get a formula that resembles the formula for the prefix complexity of a pair (and 
               the  statement  of Problem  56,  p.  44).  (It  would  be  nice  to  prove  the  statement 
               about deficiencies using the statement about complexities and the expression for 
               the deficiency in terms of complexities, but it is not clear how to achieve this.)
                    It  would  be  also  nice  to  generalize  van  Lambalgen’s  theorem  to  the  case  of 
               dependent random variables and to prove that the pair (£, 77) is random with respect 
               to a computable distribution on Q, x Q, if and only if £ is random with respect to the 
               projection of this distribution on the first coordinate (called marginal distribution)
            188                       5.  M ONOTONE  COMPLEXITY
            and r] is random with respect to the conditional distribution (for the first coordinate 
            fixed to £).  However,  there are several problems here.  First,  one needs to define 
            the conditional distribution (which can be done, as Hayato Takahashi has shown); 
            second, the conditional distribution is not necessarily computable, so it is not clear 
            what the randomness means here.  Some results in this direction are proven in his 
            papers  [191,  192];  the detailed exposition of these results and a counterexample 
            constructed by Bauwens can be found in [7].
                5.9.5.  Kucera—Gåcs theorem.  Let us return to the question that we have 
            already discussed.  A probabilistic machine is given.  Which sequences seem to be 
            the  plausible  outputs  of this  machine  (or,  better  to  say,  which  sequences do  we 
            agree to believe are generated by this machine)?  This question is meaningful for 
            an  arbitrary  machine,  even  for  the  machine  that  generates finite sequences with 
            positive probability.
                More formally, consider a computable probability distribution /r on the set Г2 
            and  computable  continuous  mapping  /:  E  —>  E.  Together  they  generate some 
            output  distribution  v  that  is  the  image  of /r  under  /.  Now  we  do  not  assume 
            that  v  is  concentrated  on  infinite sequences,  so we get  an lower semicomputable 
            semimeasure v that is not necessarily a measure.
                On the other hand, we consider the images (under /) of sequences that are ML- 
            randoin with respect to fi.  The question is,  Can we characterize this set in terms 
            of vl  It would be nice if, say, the Levin-Schnorr type characterization in terms of 
            continuous a priori probability a(-) were possible (it would say that a sequence w is 
            in the image of /r under /  if and only if the ratio a(x)/u(x) is bounded for prefixes
            X   oj).
                Unfortunately,  the arguments  we  used  for the  case when  /   is  almost  every­
            where defined  (and  v is a computable measure)  do not work anymore  (there are 
            problems in both directions).  Moreover, as was shown in [14], the image cannot be 
            characterized in terms of v.
                189       Show that there are two computable mappings /1, /2 : E —> E that gener­
            ate the same output semimeasure (as the image of the uniform measure on Г2) but 
            the images Д (R) and /2 (R) of the set R of ML-random sequences (with respect to 
            the uniform measure) are different.
                (Hint:  Both machines for Д and /2 generate only zero bits (finitely or infinitely 
            many)  at  their  outputs.  Such  a  mapping  (restricted  on  Г2)  is  determined  by  a 
            decreasing sequence of effectively open sets  A\  D  A2  D  • • •  where A{  is the set 
            of inputs  where  г  or  more  output  zeros  are  generated.  The  image  semimeasure 
            is  determined  by  the  (uniform)  measures  of Ai.  So  it  remains  to  construct  two 
            sequences of sets with the same measures such that the intersection of one sequence 
            contains  a  random  element  and  the  intersection  of the  other  one  does  not.  To 
            construct the first one, consider a random number и that is a limit of a computable 
            increasing sequence r* of rational numbers, and consider the intervals (ri,u> + 1 /%). 
            For the second one consider the sequence of intervals of the same length with empty 
            intersection, say, with left endpoint 0.  Or take centered intervals whose intersection 
            contains only the non-random number 1/2.)
                                        5.9.   RANDOMNESS  WITH RESPECT TO DIFFERENT MEASURES                                                            189
                           However, some statement that would be a corollary of this “criterion” if it were 
                    true (which is not the case),6 is still true [86,  58]):
                           Theorem 126.  Let a be an arbitrary sequence of zeros and ones.  Then there 
                     exist  a sequence иj  that is ML-random with respect to  the uniform measure and a 
                     computable mapping f: E —> E such that f(uj) = a.
                           Using the terminology of recursion theory, this statement guarantees that every 
                    sequence of zeros and ones is Turing-reducible to some ML-random sequence with 
                    respect to the uniform measure.  (We have already mentioned this result on p. 180.)
                           Proof.  We prove a bit stronger statement and construct a computable con­
                    tinuous mapping / (the same for all a) such that the image f(R)  (where R is the 
                    set of all ML-random sequences with respect to the uniform measure) equals fl.
                           Moreover, for any effectively open set U (i.e., the union of an enumerable family 
                    of intervals) of sufficiently small measure we will construct a computable mapping / 
                    such that f(Q\U) covers the entire fb  Applying this construction to an effectively 
                    open set of small measure that covers the complement of R, we get the result.
                           Here is the idea of the construction.  First,  we split the sequences into blocks 
                    of length ко, ki,..., and in this way we represent the Cantor space as the space of 
                    paths in a tree with branching factors 2k°, 2kl,... (instead of the binary tree).  The 
                    numbers ki grow fast enough as i increases  (see below).  We choose some binary 
                    subtree in this tree and declare that / maps it onto a full binary tree in a natural 
                    way.  In other words, we select two strings so and si  of length ко that are mapped 
                    by /  to 0 and 1, respectively, then select extensions soo, soi  (°f so) and sio, sn  (of 
                    si), mapping them to 00, 01, 10, 11, respectively, etc.
                           At the same time, we enumerate the intervals of the effectively open set U.  If 
                    none of them covers any path in the chosen binary subtree,  we have nothing to 
                    worry about:  f(Q\U) will cover Q.  If an interval covers some vertex in the chosen 
                    binary subtree,  we replace this vertex by another one  (that  is  not  yet  covered), 
                    and extend / to this vertex (and the entire subtree rooted at this new vertex).  To 
                    prevent this, the adversary needs to make unusable all 2k° sons of the root except 
                    one;  to  make  each of them unusable one needs to  make unusable  all of its sons 
                    except one, etc.  We conclude that the set U must be of size at least
                                                       / 2*0-14  f 2kl — l\  /2*2-l\
                    (*'                               I     2fc°      )    V     2fci      J     V    2*2       )'•••’
                    and we can choose  ко, ki,...  growing fast  enough  to  make  this  product  strictly 
                    positive (or even close to 1).
                           Now let us explain the details.  For a given (computable) sequence ko,k\,k2 ,..., 
                    we consider strings of length ко (as 2k° sons of the root), then strings of length ко + 
                    ki, ко + k\ + k2 , etc., as vertices of the tree T (with branching factors 2k°, 2kl,...). 
                    We call them T-vertices (to distinguish from the vertices of the binary tree).
                           First, we choose (in some computable way) a binary subtree in T and map its 
                    vertices to the vertices of the binary tree in a natural way.  Then we enumerate 
                    the intervals that form the effectively open set  U.  Without loss of generality we 
                    may assume that all these intervals are formed by T-vertices.  When a new interval 
                    appears, we do the following:
                           6To  derive  this  statement  from  the  “criterion”,  one  can  take  a  mapping  /   whose  output 
                    distribution is the continuous a priori probability.
                                          5.  M ONOTONE  COMPLEXITY
             190
                    •  Declare the corresponding T-vertex as bad.
                    •  Propagate bad T-vertices to the root.  A T-vertex that has only one good 
                       son in T becomes bad, too.  In this way we get a chain of bad T-vertices.
                    •  If some  T-vertices  of the  binary subtree  of T become bad  (the subtree 
                       intersects the chain of bad T-vertices), take the first bad T-vertex in the 
                       subtree (closest to the root) and replace its by its good brother.  (This is 
                       possible since its father is good and therefore has at least two good sons.) 
                       Then grow a replacement binary subtree starting from the new T-vertex 
                       and using only good T-vertices.  (Again this is possible since every good 
                       vertex has at least two good sons.)
                    •  Extend the mapping / to the new part of the binary subtree of T.
                 There is only one case when this construction is impossible:  if the root becomes 
             a bad vertex.  If this happens, then all its T-sons (except maybe one) are bad, all 
             the T-sons of these bad T-sons (except maybe one) are bad, etc.  In this way we 
             get a subtree of bad T-vertices, and its leaves (the T-vertices that became bad not 
             because of their sons) are intervals of U.  Then backward induction shows that the 
             size of U is at least (*), and we get a positive lower bound assuming that the series 
                2~ki converges.  (The infinite product П(1 — &i) is positive if and only if       is 
             finite.)  So one may take, for example, ki =  \2 log г]  (for i ^ 2), and then for small 
             enough sets U the root will never become bad.
                 To justify this construction, we need to note that:
                    •  the set of bad T-vertices can only increase;
                    •  the current binary subtree of T avoids bad T-vertices;
                    •  the T-vertices excluded from the binary subtree will never be added to it 
                       again (so the extension of /  will not contradict the old values).
                 All these properties are direct consequences of the construction.  (The last one: 
             if a T-vertex was excluded, one of its ancestors was bad at the moment, it remains 
             bad, and the binary subtree can never use it again.)
                 It remains to prove that (for /  constructed in this way) every sequence a € fl 
             has  an  /-preimage outside  U.  By  definition,  at  any  stage t  of the  construction 
             there exists /-preimage ojt that is not covered by the already discovered part of U. 
             Moreover, as t increases, the points ojt converge to some limit sequence oj  (we prove 
             the stabilization property at level i by induction over i\  note that the number of 
             possible changes on level i is bounded by 2ki).  It remains to verify that oj  does not 
             belong to U and that f(oj) = a.
                 By way of contradiction,  assume  that  oj  is  in  U.  Then  oj  belongs  to  some 
             interval that is discovered on some step.  After that the sequences ojt do not belong 
             to this interval—a contradiction with the convergence.
                 Finally,  let us verify that  f(oj) = a.  Let 2 be an arbitrary finite prefix of a. 
             We have to show that f(oj) starts with z.  Let к be the length of 2.  At every stage t 
             there exists a /г-block string (a level к vertex of T) that is mapped to z.  When t 
             increases, this string ultimately reaches its final value and therefore oj  has a prefix 
             that guarantees that f(oj) starts with z.                                           □
                  190        Prove that the random sequence constructed in the proof is computable 
             given both the oracles for a and for O' (the halting problem).
                 (Hint:  The limit position of the embedded binary tree is computable given O'.)
                                         5.9.   RANDOMNESS  WITH  RESPECT TO  DIFFERENT  MEASURES                                                         191
                           11911 Using this argument, prove that for any sequence a there exists an ML- 
                    random sequence ш such that a is Turing-reducible to uj,  and this reduction needs 
                    only an n + o(n)-bit prefix of uj to generate an n-bit prefix of a.
                           (Hint :  Instead of the binary tree, one may use in the proof the tree of branching 
                    factor 2mi.  Then we need the convergence of the product (1 — 2mi )2ki), i.e., of the 
                    series 2mi~ki.  We may let m* = i and k{ = i + 2 log г.)
                           One may speculate about the  “philosophical meaning”  of this theorem as fol­
                    lows:  For any sequence a we can a posteriori explain how it could appear during 
                    an experiment.  Indeed, for a random uj  this is the philosophical assumption,  and 
                    the transformation /  is computable and therefore can be implemented.
                           The Kucera-Gâcs theorem could look strange if we compare it with another 
                    result:  If some  sequence  a  has  a  positive  probability  to  be  computable  with  a 
                    random oracle (the set of sequences that compute a has a positive measure), then 
                    a is computable.  To see why this result is true, note that the set of all oracles that 
                    compute a is a union of sets of oracles that compute a via some oracle machine M 
                    (the union is taken over all M).  So one of these sets has positive measure, the a 
                    priori probability of a is positive, and a is computable.  So for a non-computable a 
                    the set of all oracles that compute a is a null set.  On the other hand, the Kucera- 
                    Gâcs theorem says that there exists a random sequence that computes a.  There is 
                    no contradiction here; it just means that the null set in question is not an effectively 
                    null set.
                               CHAPTER 6
                  General scheme for complexities
                          6.1.  Decision complexity
           We started with a plain Kolmogorov complexity C and then considered also 
        a prefix  complexity  К  and  a monotone  complexity  KM.  All three  complexities 
        were defined in terms of shortest descriptions, but the notion of a description was 
        different in each case.  For plain complexity the description modes (decompressors) 
        were just computable functions, for prefix complexity the description modes were 
        computable continuous mappings of type E —» Nj_, and for monotone complexity 
        the description modes were computable continuous mappings of type E —> E.
           To be uniform, we may use computable continuous mappings of type Nx —> Nx 
        for plain complexity.  Recall that topology on the set Nx  (and the set itself)  was 
        introduced in Section 4.4.3 (p. 89).  It is easy to see that there are two possibilities 
        for a continuous mapping /: Nx —> Nx:  either /(_L) is some natural number (and 
        not  _L)  and  the  mapping  is  a  constant  one,  or  /(_L)  =  _L  and  the  values  f(n) 
        for natural n can be arbitrary.  There is a one-to-one correspondence between the 
        mappings of the second type and partial functions of type N  —>  N if we use  _L 
        as  a  replacement  for  an  undefined value.  As  before,  computability  is  defined  in 
        the  following  natural  way:  the  mapping  /: Nx  —>  Nx  is  computable  if the  set 
        of pairs  (X, y)  such that  у  =4  f(x)  is enumerable.  All the constant  mappings are 
        computable, and for non-constant ones computability means that the corresponding 
        partial function is computable.  (Recall that a partial function of type N —> N is 
        computable if and only if its graph is enumerable.)
           So using this  “new” definition of a description mode (decompressor) as a com­
        putable continuous mapping of type Nx —> Nx, we get the same plain complexity. 
        Indeed, we add constant functions that map everything, including the element _L, 
        to  some constant  c,  but  they do  not  change complexity more than by 0(1).  (A 
        meticulous reader will stress that the function that  maps everything to c should 
        not be identified with the function that corresponds to a total function N —> N that 
        maps everything to c, since the latter one still maps _L to _L.)
           All  this  formalism,  however,  is  used  only  as  a  motivation  for  the  following 
        scheme  that  explains  the  origin  of the  complexities  considered  (see  Figure  15): 
        Each of the three complexities is obtained when we consider computable continu­
        ous mappings of the description space into the object space as description modes 
        (decompressors).
           This table has an empty cell; for this cell the description modes are computable 
        continuous mappings of type Nx —> E.  Let us consider the corresponding definition 
        in more detail; we call this complexity decision complexity and denote it by KR (the 
        notation KD was used too, but now KD is often used for the so-called distinguishing
                                   193
         194           6.  GENERAL  SCHEME  FOR  COMPLEXITIES
                                     objects
                                    Nx    E
                               N±   C     ?
                                E   К    KM
                         Figure 15.  С, К and К revisited
         compleodty so we use KR  for decision compleodty to avoid confusion).  This notion 
        of complexity was first considered by D. Loveland [108].
           Let us give a definition of decision complexity using some class of machines. 
        Consider  a machine that gets a binary string as input  (and some end-marker is 
        written on the tape, so the machine knows where the input ends) and prints bits 
        on the output tape  (one by one).  The machine is not obliged to stop, so for any 
        input string X  we obtain a finite or infinite bit sequence as machine’s output.  (If 
        the output sequence is infinite, it obviously is computable.)
           Any machine of the type described defines a mapping of the set of all binary 
        strings  (that  can be  identified with the  natural  numbers  in Nj_)  into  a set  E  of 
        all  finite  and  infinite  sequences.  If M  is  a machine  of this type,  the  complexity 
        KR m(x) of a string x (with respect to decompressor M) is defined as the minimal 
        length of a string у such that M(y) (the output sequence for input y) starts with x.
            192    Check that  there  exists  an optimal  decompressor  M  in  the  described 
        class of decompressors  (i.e.,  the decompressor M that leads to smallest  KRm  up 
        to an 0 (l)-additive term).
            193    Give the definition of computable continuous mappings Nj_ —> E.  What 
        is  the  difference  between  this  definition  and  the  class  of the  machines  described 
        above, and why it is not important for the definition of complexity?
           (Hint:  A continuous mapping can map _L into some non-empty string.)
           Therefore we can fill the empty cell in our table (Figure 16).
           The following theorem lists the main properties of decision complexity:
           Theorem 127.  (a) If a string x is a prefix of a string y,  then KR (x) < KR (y).
           (b)  The  complexities  of prefixes  of a  sequence ui  €  fl form  a  non-decreasing 
        sequence that is bounded if and only if the sequence uj is computable.  ( The limit of 
        the  complexity of prefixes may be called the decision complexity of the sequence uj. 
         This complexity is finite for computable sequences and infinite for non-computable 
        ones.)
           (c)  KR (x) < C(x) + 0(1) for every string x.
           (d)  KR (x) < KM(x) + 0(1) for every string x.
           (e)  KM(x) ^ KR(x) + 0(logKR(x)) for every string x.
           (f)  C(x\l(x)) < KR(x) + 0(1) for every string x.
           (g)  // /  :  S -> E  is  a computable  continuous mapping,  then
        KR (f{x)) < KR (x) + 0(1)  (the constant in 0(1)  may depend on f  but not on x).
           (h)  If f  :  E —> Nx  is a computable continuous mapping,  then
        C(/(x)) ^ KR(x)) + 0(1)  (the constant in 0(1)  may depend on f  but not on x).
                          6.1.  DECISION  COMPLEXITY       195
                                   objects
                                 Nl    £
                             Nl   C   KR
                              £   К   KM
                        Figure  16.  Four complexities
           (i)  If f  :  Nj_  —> £  is a  computable continuous mapping,  then
        KR(f(x)) ^ C(x) + 0(1) (the constant in 0(1)  may depend on f  but not on x).
           (j)  A prefix-free set of strings  (none is a prefix of another one)  that have deci­
        sion complexity less than n,  has cardinality less than 2n.
           (k) The function KR  is upper semicomputable  (enumerable from above).
           (l)  The function KR  is the smallest  (up to  a constant) function satisfying the 
        last two conditions:  if some function к is upper semicomputable and for every n the 
        cardinality of every prefix-free set of strings x  such that k(x)  < n for all elements 
        of this set is 0(2n),  then KR (x) ^ k(x) + 0(1).
           (m)  KR (x) ^ К A (x) + 0(1) for all strings x.
           PROOF,  (a)  This is an immediate corollary of the definition  (description of a 
        string is at the same time description of any its prefix).
           (b)  Assume  that  the  sequence uj  is  computable.  Consider  the  machine that 
        ignores  its  input  and prints cj  bit  by bit,  as  a decompressor  (description mode). 
        All prefixes of uj  have zero complexity with respect to this decompressor (since the 
        empty string is their description),  and therefore they have 0 (l)-complexity (with 
        respect to the optimal decompressor), On the other hand, if the complexities of all 
        prefixes of cj  are bounded,  some string has to be a description of infinitely many 
        prefixes, therefore cj  is computable.
           (c)  Any partial computable function whose arguments and values are binary 
        strings can be considered as a KR-decompressor  (do not output anything before 
        the computation is finished, then print the result bit by bit).
           (d)  Any continuous computable mapping £ —>• £ can be considered as a KR- 
        decompressor (after restriction to finite strings; we may say that we type the input 
        string  on  the  keyboard  of a robust  machine  immediately  after  the  computation 
        starts, and we do not touch the keyboard anymore).
           (e)  Let  R  :  N  —>•  £  be  an  optimal  decompressor  used  in  the  definition  of 
        decision complexity.  Consider a computable mapping R :  £ —>  £ defined as fol­
        lows:  R(xu) = R(x), where x is a self-delimiting encoding of x (say, the x itself is 
        prepended by the binary encoding of l(x) with duplicated bits and the separator 
        01) and и is an arbitrary string (needed to ensure the monotonicity).
           (f)  Let  again R : N —> £ be an optimal KR-decompressor.  Define the condi­
        tional decompressor 5 by letting S(y, n) be the first n bits of the sequence R(y)  (if 
        n exceeds the length of R(y), then S(y, n) is undefined).
           (g)  Consider  a  new  KR -decompressor  that  is  a composition  of the  optimal 
        KR -decompressor and the mapping /, and compare this new decompressor with 
        the optimal one.
           196                6.  GENERAL SCHEME FOR COMPLEXITIES
               (h)  Consider the composition of an optimal KR -decompressor and / as a C- 
           decompressor.
               (i)  Consider the composition of an optimal C-decompressor and f as a KR -de­
           compressor.
               (j) Two incompatible strings cannot share a description (since in this case they 
           would be prefixes of some sequence,  and the shorter string would be a prefix of 
           the longer one).  If all elements of a prefix-free set of strings have complexity less 
           than n, then their descriptions are different strings of length less than n, and there 
           exist fewer than 2n such strings.
               (k)  Applying in parallel  the optimal  description  mode to  all strings,  we get 
           upper bounds for KR  (that may decrease when new descriptions are found); they 
           converge to KR.
               (l)  This is the first interesting claim in this theorem  (up to now we had only 
           simple variations on known themes).
               Let A: be a function that satisfies (j) and (k).  Adding a constant to k, we may 
           assume without loss of generality that there are at most 2n pairwise inconsistent 
           strings ж such that k(x) < n.
              We construct a description mode that gives every string x such that k(x) < n 
           a description of length exactly n.  This is done independently (and in parallel) for 
           each n.  Namely, we watch the decreasing upper bounds for к and write down the 
           (increasing) list of strings x such that k(x) < n.  Consider a subtree of a full binary 
           tree that is formed by the strings in the list and all their prefixes.  This is a growing 
           subtree that has (all the time) at most 2n leaves.  (Indeed, the leaves are pairwise 
           incompatible strings x such that k(x) < n.)  Let us attach a label to each leaf; this 
           label is a string of length n.  When the subtree grows by adding some new string, 
           this string either extends one of the leaves (so it is not a leaf anymore) or creates a 
           new branch (being attached to some internal node).  In the first case the new string 
           is a leaf, and this leaf keeps the label of the superseded one.  In the second case we 
           provide a new label for the new leaf (which is possible since we have fewer than 2n 
           leaves).
              Let us fix a label and look what happens with leaves carrying this label.  Initially 
           the label is unused.  It is possible that the label remains unused forever (we do not 
           need that many labels), but if it is not the case, the label is attached to some leaf 
           and then moves up the tree (the next position is a son of the previous one).  So this 
           label marks some branch of the tree (finite or infinite sequence of zeros and ones). 
           In this way we get a function that maps strings of length n (i.e., labels) to £ (the 
           strings that are not labels are mapped to A, the empty sequence).
              Combining these mappings for all n, we get a A72-description mode that guar­
           antees complexity at most n for all strings x such that k(x) < n, just as we claimed.
               (m) If xi are pairwise inconsistent binary strings, then ^)2~ KA^Xi1 ^ 1  (since
           2" KA    equals the a priori probability of the set £Xi, and these sets are disjoint). 
           Therefore we have at most 2n strings such that K A (ж*) < n and may refer to the 
           previous statement.                                                    □
               194      Prove that KR (ж) can be defined as follows:  for any computable function 
           S of two arguments  (the first is a binary string,  the second is a natural number; 
           values of f  are zeros  and  ones),  let  KRs(x)  for  a string ж  =  жо ■ • ■ xn-i  be  the 
           minimal length of the string у such that S(y, i) = ж,; for alH = 0,1,— 1.  Then
                         6.2.  COMPARING  COMPLEXITIES     197
        we choose an optimal function among all functions of this class, and it defines the 
        decision complexity.
           195 Show that the decision complexity of a string x equals (up to 0(1)) the 
        minimal value of C(p) for all programs p (in a given programming language, say, 
        Pascal) that ignore their input and output the string x or some its extension.
           196  Show that if we replace C by К in the preceding problem, we get in a 
        similar way an upper bound for monotone complexity.  Show that this bound is not 
        0 (l)-tight.
           (Hint:  The monotone complexity of all n-bit strings is bounded by n + 0(1). 
        The programs for these strings  (or their extensions)  should be all different,  and 
        there are not enough strings having prefix complexity n + 0 (1).)
                        6.2.  Comparing complexities
           There are four complexities in our table (two options for the space of objects are 
        combined with two options for the space of descriptions).  The following diagram 
        (Figure 17) shows the inequalities between them (up to an 0(l)-additive term):
                                     KM
                                KR
                   Figure 17.  Inequalities between complexities
           Some people would like to avoid references to topological notions like continu­
        ous mappings, though these notions are quite relevant here as the theory of abstract 
        data types shows (Dana Scott lattices and related notions of /о-spaces in the sense 
        of Ershov);  see  [176].  Those readers will appreciate the following simplified con­
        struction [195] that is still enough to define the four complexities in the table.
           Consider the set H = B* of all binary strings and two binary relations:  x = у 
        means that strings x and у are equal;  x x у means that x and у are compatible 
        (one is a prefix of the other one).  Let a and ß be one of these two relations  (so 
        there are four combinations for the pair a,ß).
          A set S C £ x £ is called a-ß-regular if the following condition is true for any 
        strings xi, 12,2/1,2/2:
                     (zi,2/i) € S, (Х2 У2) G S,  X]_ax2  =>  yißy2
          For example, =-—regular binary relations are just graphs of functions.
           197 (a) Show that every x-=-regular relation determines a continuous map­
        ping of type E —> Nx-
           (b) Show that every x-x-regular relation determines a continuous mapping of 
        type E —> E.
           (c)  Show that every =-x-regular relation determines a continuous mapping of 
        type Nx —»• E.
           198                6.  GENERAL SCHEME FOR COMPLEXITIES
               Now by a-ß-description mode we mean an enumerable a-/9-regular binary re­
           lation on E X  E.  For each description mode S we define the complexity function 
           К s'-  let Ks(x) be the minimal length of a description of x, i.e., the minimal value 
           of l(y) for all у such that  (y, x) G S.
               Theorem  128.  For each of the four combinations a,ß G  {=,x}  there exists 
           an  optimal a-ß-description  mode  (that provides  minimal  complexity function up 
           to  0 (1))  and the  corresponding  complexity is  one  of the four known  complexities 
           C, K, KM, KR.
               Proof.  In all four cases enumerable a-/?-regular relations correspond to com­
           putable continuous mappings of the corresponding sets (see Problem 197) that gives 
           the same complexity function, and vice versa.                          □
               So we can provide new labels for rows and columns of our table (Figure 18):
                                                objects
                                               -      —
                                         — C KR
                                         X     К     KM
                                  Figure 18.  a-/?-complexities
               198 For pairs of strings show how one can define:
              (a) monotone complexity (using computable continuous mappings E —> E x E 
           as  decompressors;  such mappings are in one-to-one correspondence with pairs of 
           computable mappings
              (b)  a  priori  probability  (using  probabilistic  machines  that  have  two  output 
           tapes where bits are printed sequentially);
              (c) decision complexity (using computable continuous mappings Nj_ —> E x E).
               199 Prove that  the  decision  complexity  of a  pair  (x,y)  (see  the  previous
           problem) does not exceed l(x) + l(y) + 0 (1).
              (Hint:  The string z can describe the pair (z,zR), where zR is z from right to 
           left.)
              A surprising result:  this property remains  true for triples  [72]  and  even  for 
           ^-tuples of every fixed к  (it is a corollary of the results of [146]).  For monotone 
           complexity a similar  property  is  not  true  as  was  shown  by  Pavel  Karpovich  in 
           [72]:  the  value  of KM(x,y)  may exceed l(x) + l(y)  by a quantity of order logn 
           for n-bit strings.  (Therefore the monotone complexity of pairs may exceed a priori 
           complexity by the same margin,  since a priori complexity of a pair is obviously 
           bounded by the sum of lengths.)
              Another classification scheme for complexities (which goes back to [95]) defines 
           each version of complexity as the smallest upper semicomputable function in some 
           class (of functions that satisfy some restrictions).  We have already considered these 
           restrictions, so we just collect the results obtained and give the conditions for each 
           complexity version:
                        6.2.  COMPARING  COMPLEXITIES     199
            •  the number of strings x such that k(x) < n is 0 (2n) (plain complexity C, 
              Theorem 8, p. 19);
            •  the series Ylx 2~k^  converges (prefix complexity AT, Theorem 62, p. 100);
            •  every prefix-free set of strings x such that k(x)  < n has 0 (2") elements 
              (decision complexity KR, Theorem 127, p. 194);
            •  Y^xex 2~k^   <  1  for every prefix-free set  X  of binary strings  (a priori 
              complexity KA, Theorem 80, p. 126).
          This scheme gives the same four complexities with one important exception: 
        we get a priori complexity instead of monotone complexity.  (There is no problem 
        with prefix complexity, since it coincides with the negative logarithm of the discrete 
        a priori probability, the largest lower semicomputable semimeasure on N.)
          Combining these two quadrilaterals, we get a pentagon (Figure 19).
                                К
                        Figure 19.  Five complexities
          Let us recall the basic results that relate complexities in this pentagon.  First, 
        all  five  complexities  differ  at  most  by  O(logn)  for  strings  of length  n.  Indeed, 
        Theorem 65, p. 102 says that K{x) < C(x) + 0{\ogC{x)).  On the other hand,
                  C(x) < C(x\l(x)) + C(l(x)) < KR (x) + O(logn).
        So the two most distant complexities in the pentagon (the upper one and the lower 
        one) differ at most by O(logn) for strings of length n.
          A more complicated picture arises if we want to bound the difference between 
        two complexities in terms of the complexities, not their length (note that a com­
        plexity can be much less than length).  This is indeed possible for two lines that go 
        in the north-east directions:
                         K(x) ^ C(x) + 0(\ogC(x))
        (see Theorem 65) and
                       KM(x) < KR (x) + О (log KR (x))
        (Theorem 127).  (A similar inequality with KA  instead of KM follows, as we have 
        already mentioned in Problem 140, p.  144.)  For  “north-west”  lines the situation 
        is different:  KM and KR  are bounded for prefixes of a computable sequence (e.g., 
        for strings that contain only zeros) while C and К are not  (the string of n zeros 
        has the same complexity as the integer n, and this complexity is of order log n for 
        some n).  We have already discussed this question in Theorem 86 and noted that 
        the  difference  between  К  and  KM  can be of order  logn  in  both directions  (for 
        infinitely many n and for some x of length n).  Theorem 87 says that the difference
            200                   6.  GENERAL  SCHEME  FOR COMPLEXITIES
            between KM and К A  for n-bit strings can be about log log n (so here we have a 
            gap between the known lower and upper bounds).
                 None of the mentioned results guarantees that the difference between К(x) and 
            C(x) tends to infinity as x goes to infinity (here we consider a; as a natural number). 
            But this follows from Theorem 73  (p.  112).  Some other bounds relating different 
            versions of complexity are mentioned in [195].
                                     6.3.  Conditional complexities
                We have already considered several versions of conditional  complexity  (of a 
            string relative to the other one).  In Section 2.2 we have defined the conditional 
            complexity C(x \ y) as the minimal length of a string p that describes x when у is 
            given, i.e., a string p such that S(p, y) — x.  Here S is the conditional decompressor 
            that is optimal in the class of all partial computable binary functions.
                In  Section 4.7 we defined the conditional prefix complexity K(x\y).  In this 
            definition we required S(p, y) to be prefix stable with respect to p for every fixed, y: 
            this means that if S(p,y) = x for some p, then S(p',y) = x for all strings p' that 
            have prefix p.
                Finally,  in the proof of Theorem 93 we mentioned the conditional monotone 
            complexity  KM(x\y).  For its definition a description  mode  (decompressor)  is  a 
            computable family of computable continuous mappings Dy : E  —>  E  (indexed by 
            string y).  The computability of this family means that the set of triples  (y,u, v) 
            such that v ^4 Dy(u) is enumerable.
                The conditional decision complexity can be defined in a similar way.
                In these four definitions we consider conditions as terminated bit strings, and 
            the behavior of the decompressor is unrelated for different conditions:  if we know 
            that p is  a description of x relative to y,  this gives us no information about  the 
            values of decompressor for other values of y.
                In other words, a decompressor (say, for the conditional prefix complexity) can 
            be considered as a computable mapping
                                             P:Ex N -» Nj_;
            in the pair  (p, y)  G  E x N,  the string p  is  considered as  a description  (and  D  is 
            monotone with respect to p) and y is a condition, and no monotonicity is required.
                If we change this and consider conditions also as vertices of a binary tree re­
            quiring monotonicity over conditions, we get four other versions of conditional com­
            plexity.  These versions are not widely used ([40] is a rare exception).
                In this way we get eight versions of conditional complexities (for each of three 
            components, i.e., conditions, descriptions,  and objects,  we have two possibilities). 
            The most non-technical definition of these complexities goes as follows.  Let a,ß, 7 G 
            {=,x} (see Section 6.2).  An (a, ß) | 7-decompressor (description mode) is an enu­
            merable set S of triples (p,x,y), such that
                       {pi,Xx,yi) e S,    (Р2,Х2,У2) £ S,   piap2,   y\iy2^X\ßX2 
            The we define Ks(x\y) as the minimal length of a string p such that (p,x,y) G S.
                Theorem 129.  In all eight cases there exists an optimal decompressor S that 
            gives the smallest complexity K$  (up to 0 (1))  among all the decompressors ofthat 
            class.
                                      6.3.  CONDITIONAL COMPLEXITIES                        201
                 200  Give a detailed proof of this theorem  (it follows the same scheme as in
            the case of plain or prefix conditional complexity).
                 In each of eight classes let us fix some optimal (a, ß) | 7-decompressor and denote 
            the  corresponding complexity by  K(Q,/3)|7.  In  this  notation  K(x\y)  (as  defined 
            earlier) is K ^ =) \ =  and C(x\y) is K(=,=) | = -
                 201  Show that by replacing = by x in place of 7 we may only increase the 
            complexity.
                 (Hint:  This replacement adds more restrictions for a decompressor, so we get 
            fewer decompressors.  For the same reasons the plain complexity does not exceed 
            the prefix one.)
                It would be interesting to studj' how large this increase could be (and establish 
            other properties of these conditional complexities).
                Let us give an example of a statement that involves conditional complexities 
            as they are defined above:
                 202 Prove that
                             C(x) < K(=.=) i *(* I y) + KR (у) + О (log KR (»)).
                Let  us  now  describe  one  more  approach  to  the  definition  of the  conditional 
            complexity that goes back to Kolmogorov’s interpretation of logical connectives as 
            operations on problems [76].  The conditional complexity of x when у is known can 
            be described as the complexity of the problem “transform у into ж” ; moreover, this 
            problem can be considered as a set of all functions that map у into x (each function 
            that maps у to ж is a “solution”  of this problem).
                More formally, let us consider the space F whose elements are all partial func­
            tions whose arguments and values are natural numbers.  Let us introduce the fol­
            lowing partial order on this set:  Д  ^ /2  if /2 is an extension of /1  (i.e., /i(y) = x 
            implies /г(у) = x).  By finite elements of F we mean functions with finite domain. 
            For each finite element  /   G  F consider its cone,  i.e.,  the set  of all its extensions 
            {y  I  /   ^  I/}  (both finite and infinite).  We call a continuous mapping T: Nx  —> F 
            computable if the set of pairs  (a, f)  such that  a  G  Nx,  /  is a finite element  of F 
            and /  ^ T(a), is enumerable.  Continuous computable mappings Nx —> F are used 
            as decompressors for functions.  For each function /  G F. we define the complexity 
            of /   (with respect  to  decompressor T)  as the minimal length of a string  (or the 
            logarithm of the number—recall that we identify strings with natural numbers) a 
            such that /  ^ T(a).
                 203  Prove that there exists an optimal decompressor (in this sense) and that 
            the complexity of the function y    x (whose domain is a singleton {y} and whose 
            value is x) is C(x\y) + 0(1).
                We can give a similar interpretation of all eight conditional complexities defined 
            above:  for  every  two  spaces  Y, X  G  {Nx, £},  we  define  the  space  of functions 
            (У  —> X)  and  then  consider  computable  mappings  of the space  of descriptions 
            P  G  {Nx,£}  into  the function space  (Y  —> X).  The definition  of the function 
            space is given in the spirit of Scott domain theory  (or the theory of /о-spaces in 
            the sense of Ershov, see [176] for details).
                A slightly different interpretation of (plain) conditional complexity as the com­
            plexity of the problem “transform y to ж”  is considered in Chapter 13; it does not 
            use computability notions for function spaces.
            202                   6.  GENERAL SCHEME  FOR COMPLEXITIES
                 A related notion of complexity for functions was considered by Schnorr [168, 
             170].  Recall that a numbering  (an important notion in the recursion theory) is a 
            mapping v that maps each natural number n into some (partial) function vn whose 
            arguments and values are natural numbers.  A numbering v is  computable, if the 
             (partial) function of two arguments
                                               (n,x) ^  un(x)
            is  computable.  A numbering v is called a  Gödel numbering if for any other com­
            putable numbering p there exists a computable function that  reduces  p to  и in 
            the following sense:  pn  =  Vh(n)  for every n.  (In particular,  the range of a Gödel 
            numbering is the set of all computable functions.)
                Following  Schnorr,  we make this  condition stronger and  require additionally 
            that h(n) = O(n) (in other words, the length of the string h(n) exceeds the length of 
            string n at most by a constant, if we identify natural numbers with binary strings). 
            If such a function h exists for every computable numbering p, the numbering v is 
            called optimal.
                Theorem 130.  There exist optimal numberings.
                Proof.  Consider any reasonable programming language for functions of two 
            arguments, and let ûv be a p-numb er of the function obtained by fixing first argu­
            ment equal to v in the function that has program u.  (Here û is some self-delimiting 
            encoding of и, e.g., и with doubled bits and 01 appended.)                      '□
                Schnorr  [168,  170]  defined  the  complexity of a computable  function  as  the 
            logarithm of its minimal number on an optimal numbering.  (As before, the minimal 
            complexity of a function that maps x to у turns out to be equal to C(y \ x).)  Schnorr 
            has shown that any two optimal numberings v\ and        can be translated into each 
            other by a computable permutation 7Г that changes the size at most by 0 (1) (in both 
            directions):  this means that v\(n)  =  v-i(тг(п))  for every n and that 7r(n)  =  O(n) 
            and 7r- 1(n) = O(n).  The detailed proofs of these results can be found also in [11].
                                    6.4.  Complexities and oracles
                6.4.1.      Relativized  complexity.  Relativization  is  a well-known  method  in 
            computability theory.  We take  a definition  or  statement  that  involves  the class 
            of computable functions and replace computable functions by functions that are 
            computable with some oracle  (computable  relative  to  this oracle).  The oracle is 
            usually a total function a whose arguments and values are natural numbers and/or 
            binary strings, for example, a characteristic function of some set A.  An algorithm 
            is  allowed to call an  external procedure that computes the value a(n) for a given 
            value of the parameter n.  If ck is a characteristic function of a set A, this means 
            that we may ask whether some n belongs to A or not.  If the function a is not 
            computable, this permission to ask a-oracle increases our capabilities, and we get 
            a class of a-computable functions that contains all computable functions but also 
            some non-computable ones (including a).
                Then we can develop the general  theory  of algorithms  as  usual  and  define, 
            say,  tt-enumerable sets,  or ct-computable real numbers,  or  (closer to our subject) 
            a-lower-semico input able  semimeasures,  etc.  And  practically  all  the  theorems  of 
            general theory of algorithms  (and their proofs) remain valid, we just neqfl to add 
            “a-”  for all the notions.  This procedure is called relativization.
                                     6.4.  COMPLEXITIES  AND  ORACLES                      203
                In particular, for a given set A we may define the notion of Л-relativized Kol­
            mogorov complexity allowing decompressors to use oracle A.  This can be done for 
            plain, prefix, and all other versions of complexity that we have considered (uncon­
            ditional or conditional).  The use of an oracle is shown by a superscript,  so,  e.g., 
            KA(x) denotes prefix complexity relativized by oracle A.
                In  fact  we  can  do  a bit  more:  instead  of defining complexity for a given  or­
            acle  A  up  to  an  0(l)-additive  term  (by  proving  the existence  of an  optimal  Л- 
            decompressor), we may define (with the same precision) the function of two argu­
            ments:
                                             (A, X) i—У kA(x)
            (here к is one of the complexity versions, say, К or KM).
                 204 Show that this indeed can be done and that the resulting complexities
            coincide with the limits of conditional complexities defined in Section 6.3: 
                               KA(x) = KA=(x) =  lim  K{~,= |Ж)(ж|An),
            where  An  is  the  prefix  of length  n  of the  characteristic  sequence  of the  set  A. 
            (Similar statements are true for other complexity versions.)
                Note that relativized complexity does not exceed the non-relativized one  (up 
            to 0 (1)), since the algorithm with an oracle is not obliged to use it, so all decom­
            pressors are Л-decompressors.
                For  some oracles Л  and some strings  x  the Л-complexity of x  can  be  much 
            smaller than oracle-free complexity.  For example, let Л be the universal enumerable 
            set:  This set is usually denoted by O'.  In other words, the O'-oracle is an oracle for 
            the halting problem.  We may send any program (with its input) to this oracle, and 
            the oracle will tell us whether this program terminates for this input.
                Using this oracle,  we  can  find  for  every  string  x  its  shortest  description  (in 
            the standard sense,  without  oracle)  since the oracle tells  us which  computations 
            terminate.  Therefore,  the function  C is  O'-computable  (the same is true for  K, 
            conditional complexities, etc.), and the list of all strings of complexity less than n 
            (that  has n + 0 (l)-complexity without the oracle),  as well as the numbers B(n) 
            and BB(n) (see Section 1.2) now have O'-complexity only 0 (log?2).
                On the other hand, most strings of length n have O'-complexity n — 0(1), and 
            therefore their O'-complexity is close to their non-relativized  complexity  (and  to 
            their length).
                 205       Assume that for some set Л its use  (as an oracle) does not change the 
            plain complexity function,  i.e.,  C(x) = CA(x) + 0(1).  Show that Л is decidable. 
            Show that the same is true also for KM, KR, К A  instead of C.
                (Hint:  One can characterize the computability of a binary sequence in terms 
            of complexities of its prefixes, see Problem 49, p. 42.)
                It  is  not  the  case  for  prefix  complexity:  there  exist  К -low  sets  that  do  not 
            change prefix complexity being used as oracles.  This is a very important  recent 
            result  (see  [147,  49], or the popular exposition in  [20]).
                This  result  implies  that  there  is  no  formula  that  can  express  the  value  of 
            plain (monotone, decision, a priori) complexities in terms of prefix complexity with 
            0(Imprecision.  Note that the same is true for conditional  prefix complexity:  it 
            cannot  be  expressed  in  terms  of the  unconditional  one,  since  it  determines  the 
            class of computable functions.  Indeed,  a sequence a is computable if and only if
             204                    6.  GENERAL SCHEME  FOR COMPLEXITIES
             K(a0 ■ ■ • а,г-1 I n) ~ 0(1).  Note that Theorem 72 characterizes plain complexity in 
             terms of conditional prefix complexity.
                  6.4.2.      Complexity  with  large  numbers  as  conditions.  Let us define a 
             new type of conditional complexity, i.e., the complexity of a string x relative to the 
             set A.  Informally speaking, we want to measure the complexity of the task,  “obtain 
             x given an arbitrary element of A”.  This complexity has several equivalent (up to 
             0 (1)) definitions.
                  Here is one of them.  Fix some reasonable programming language.  (Formally 
             speaking,  “reasonable”  means that the numbering corresponding to this language 
             is  a  Gödel  numbering,  i.e.,  there  exists  a  translation  algorithm  from  any  other 
             programming language,  see  [184]  for details.)  Now let  us define the conditional 
             complexity of an object x with condition A  as the minimal  (plain)  Kolmogorov 
             complexity of a program that maps  every element of A into x.  (A generalization 
             of this definition is considered in Chapter 13.)
                  The existence of a translation  algorithm  guarantees  that  this  notion  is  well 
             defined, i.e., that the complexity defined in this way does not depend on the choice 
             of a programming language (Gödel numbering).
                  One should not mix this complexity with a completely different notion:  a con­
             ditional complexity of x with condition A, where the finite set A is given as a finite 
             object (say, as the list of its elements).  In our case we do not get the list of all ele­
             ments of A, but only one of them, and we should be prepared to deal with arbitrary 
             elements of A.  To stress this distinction, we use the notation C(x || A) for the new 
             complexity (while C(x \ A) denotes the condition complexity of x if a finite set A is 
             given as a list of its elements).
                 A different  (but equivalent) definition of C(x || A) can be given as follows.  Let 
             D  (decompressor)  be a computable partial function of two arguments.  Let x be 
             a binary string,  and let  A be a set of binary strings.  We define Cd{x\\ A)  as the 
             minimal length of a string p such that D(p,y) = x for every y G A.
                  206         Prove that there exists an optimal decompressor in this class (that gives 
             the minimal function Cd(-||-)  up to an  0(l)-additive term).  Prove that Cd  for 
             optimal D coincides (up to an O(l)-tenn) with the complexity defined above.
                 For a singleton A = {a}, both the complexities C(x\ A) and C(x\\A) coincide 
             with the standard conditional complexity C(x\a)  up to an 0(l)-term (see Prob­
             lem 28).
                 Now let  A  be  the  set  of all  integers  greater  than  some  (presumably)  large 
             number n.  (As usual, we identify natural numbers with binary strings.)  The com­
             plexity of a string x with respect to this set is denoted by C(x\\  ^ n).  Obviously, 
             this complexity does not exceed C(x) and is a non-increasing function of n  (and, 
             more generally, C(x\\ A) can only decrease if A becomes smaller; it becomes 0(1) 
             for the empty set A).  So there exists some limit as n       oo.
                 Theorem 131.
                                        lim  C(x II  ^ n) — C° (x) + 0(1).
                                       n —>oо
                 Proof.  Assume that the limit  equals  k.  Then there exists a program p of 
             complexity к that maps all sufficiently large numbers to i.  If an oracle O' is avail­
             able,  this program can be considered as a O'-description of x.  Indeed,  given this 
             program, we search for N and у such that p does not map any n ^ N into an object
                                       6.4.  COMPLEXITIES  AND  ORACLES                        205
             that differs from y.  The emphasized property can be checked using a O'-oracle since 
             it  has  an enumerable negation.  And our assumption guarantees that y equals x. 
             Therefore,
                                      C° (ж) <  lim  C(x К  ^ n) + 0(1).
                                                n—> oo
                 On the other hand,  let  у be  a description  of x  with respect  to  a O'-optimal 
             decompressor,  and  let  к  be  the  length of y.  Consider  a following program  that 
             has additional input N:  make N steps of the enumeration of the universal set O' 
             and then  use  the  set  of enumerated  elements  as  an oracle  for  decompression  of 
             y.  This program can be constructed effectively given у,  therefore its complexity 
             does not exceed C{y) + 0(1) ^ l(y) + 0(1) — к + 0(1).  On the other hand, if N is 
             large enough, this program generates x (since only a finite number of oracle calls are 
             performed during the decompression of y, for all sufficiently large N these questions 
             get correct answers even if the oracle is replaced by its iV-approximation).       □
                 It  turns  out  that  a  similar  result  is  true  where  we  replace  C(x\\  ^  n)  by 
            supm^n C(x\m).  Note that
                                         sup C(x\m) < C(x II  ^ n),
                                         m^n
            since  the optimal  program  in the right-hand side  can be  used  for  any m in  the 
             left-hand side.  This is easy; the surprising result is that both sides have the same 
            limit as n —> oo (up to an O(l)-term):
                 Theorem 132.
                                      limsupС(жIn) — О0 (ж) + 0(1).
                                       n—>oo
                 Proof.  We have to prove that if (for some string x and integer к)
                                   C(x I n) < к for any sufficiently large n,
            then O'-complexity of x does not exceed k + 0( 1).  The difficulty here is that (unlike 
            in the previous theorem) the program of length less than к that maps n to x may 
            depend on n, and none of these programs is guaranteed to work for all sufficiently 
            large n.
                 Note that  there is  less  than  2k  strings  x  with  this  property  (for  a given  k). 
            Indeed, if we have more of them, then for sufficiently large n we run out of programs 
            of length less than k.
                 It would be enough to prove that the set of strings x that have this property is 
            a O'-enumerable set whose enumeration effectively depends on к (in other words, it 
            would be enough to prove that the function x i-> lim sup C(x \ n) is O'-enumerable 
            from above).  However, the natural description of this set,
                                         3N (Vn ^ N) [C(x\n) < k],
            shows only that  it  is  a  Ез-set  (the condition  in  brackets  is  enumerable  and two 
            quantifiers precede it), so we choose an another approach.
                 Note that we do not really need this set to be O'-enumerable.  It is enough to 
            show that it is a subset of a O'-enumerable set that contains less than 2k  elements 
            for a given k.  This can be done as follows.
                 Consider two-dimensional enumerable set of pairs (n, x) such that C(x | n) < k. 
            This set  (for each k)  is  thin in the following sense:  all vertical sections of this set 
             (for fixed n) contain fewer than 2k elements.
              206                      6.  GENERAL  SCHEME  FOR COMPLEXITIES
                   Consider some point (n, x).  Let us try to add a horizontal ray that goes on the 
              right from this point to our set  (i.e.,  add all pairs  (m,x)  for all m ^ n).  The set 
              may remain thin or not,  and these two cases can be distinguished by a (У-oracle. 
              Indeed, the negation of being thin is an enumerable property (there exists a section 
              that has at least 2k different elements including the added one).
                   Let us perform these attempts (to add the horizontal ray starting from some 
              pair  (n,x))  sequentially  for  all  pairs  in  some  order.  (If some  ray  is  added  suc­
              cessfully,  then  its  elements  are  taken  into  account  for  all  subsequent  attempts.) 
              This process is (У-computable and therefore the ordinates of all added rays form a 
              (У-enumerable set.
                   This set has fewer than 2k elements (since we add rays only if the resulting set 
              is  still  thin)  and contains every x such that lim sup C(x \ n) < k.  Indeed,  for such 
              an x there is some ray that lies entirely in the initial set, and this ray can be added 
              at any time.                                                                                  □
                   (This proof is a simplified version of the proof given in  [196].  See also similar 
              arguments in [16].)
                   We can also obtain the results for prefix complexity that are similar to The­
              orems 131 and 132.  However, the definition of conditional prefix complexity with 
              respect  to  a set  is  quite subtle,  so  we postpone  its  discussion  and start with the 
              second theorem.
                   Theorem 133.
                                           limsupK(x\n) = K° (x) + 0(1).
                                            n—J-OO
                   Proof.  Using a priori probabilities (conditional and unconditional), we rewrite 
              the statement as follows:
                                                lim inf m(x \ n) = m° (x)
                                                 n —» oo
              (the equality is understood up to a bounded factor in both directions).
                   Let  us  show  first  that  the  left-hand  side  is  greater  than  the  right-hand  side 
              (up to  an O(l)-factor).  Indeed,  consider a (У-oracle probabilistic machine whose 
              output has distribution m° .  Then for any integer n we may run this machine with 
              a changed oracle:  instead of the entire oracle we use its approximation obtained 
              after n steps.  This, of course, changes the output distribution, however, the lim inf 
              of the probabilities to get some x using n-approximation to the oracle (as n —> oo) 
              is  greater  than  or  equal  to  the  probability  of getting  x  with  the  entire  oracle. 
              Indeed, the latter probability is the measure of an open set of all bit sequences that 
              are mapped to x using a (У-oracle.  This open set is a union of intervals,  and for 
              each interval the computation depends only on some finite part of the oracle, and 
              therefore the same random bits will give the same output x if the approximation 
              to  the oracle is good enough  (i.e.,  n is sufficiently large).  (Note that  lim inf can 
              be bigger than the probability of getting x with the final oracle, since approximate 
              oracles can force output x for combinations of random bits that do not generate x 
              with the final oracle.)
                   Now let  us  prove  the  reverse  inequality.  This  proof resembles  the  proof of 
              Theorem 132.  We have a lower semicomputable family of semimeasures:  for each n 
              the function x h-> m(x\n)  is a semimeasure (i.e.,  J2xm (x \n)  ^  1  for each n).  It
                                                    6.4.  COMPLEXITIES  AND  ORACLES                                           207
                 follows that the function
                                                          77?/ (x) — lim inf m(x \ n)
                                                                       П —^OO
                 is  also  a semimeasure,  i.e.,  the sum ^ Xm'{x)  does not exceed  1.  If this function 
                 were O'-lower-semicomputable,  this would finish the proof;  however,  we have the 
                 equivalence
                                 r < lim inf m(x | n) ^  (3q > r) 3N (Vn > N) [q < m{x | n)],
                                        n—too
                 where the right-hand side has too many quantifiers (note that the property in the 
                 brackets is enumerable, not decidable).  But again we may replace the function m' 
                 by any larger function, so it remains to construct a OMower-semicomputable upper 
                 bound for m!.
                      To  achieve  this  goal,  let  us  consider  triples  (N, x, e)  (where  e  is  a  positive 
                 rational number).  For a given triple we try to increase the values m(-1 •)  up to e 
                 on a ray that consists of pairs  (n, x)  for fixed x and for all n ^ N.  This change is 
                 performed only if we get semimeasures (i.e., for every n the sum over all x does not 
                 exceed 1).
                      As before, we can check whether such an increase is possible using a O'-oracle. 
                 (Indeed,  the  violation  is  an  enumerable  event.)  Let  us  consider  sequentially  all 
                 triples and perform the increase when possible (the increased values are taken into 
                 account  on  the  subsequent  steps).  Then  for  each  possible  increase we  keep  the 
                 values of x and e.  In other words, we consider a function that on every x is equal 
                 to the upper bound of all e that are used for increase together with that x.  In this 
                way we get a O'-enumerable family of semimeasures that is an upper bound for m!. 
                Indeed, if m! is greater than e for some x, the function m is greater than e on some 
                 ray, an increase does not really change anything and therefore is permitted.                                   □
                      To formulate a similar statement for K(x ||  ^ n), we should first of all define 
                 this prefix complexity relative to a set.  Here we have several possibilities, and it is 
                 unclear which of them is  “the right thing”.
                      We may try to define K(x\\A)  and the minimal prefix complexity of a pro­
                gram that outputs x when applied to every element of A.  However, Problem 109 
                 (p.  104) shows that this definition does not match K(x\a) for singleton conditions, 
                so probably this definition is not a good one.
                      Another definition is similar to the approach used in Problem 206.  Consider 
                an arbitrary computable function (p, x) H>- D(p, x) that is prefix stable with respect 
                to its first  argument  (if the second one is fixed).  For any x and for any set A we 
                then define Кв{к\\А)  as the minimal length of a string p such that f(p,n)  =  k 
                for  all  n  €  A.  The  difference  (compared to plain complexity)  is  that  we require 
                the conditional decompressor to be prefix stable with respect to the first argument. 
                There  exists  an  optimal  decompressor  in  this  class  that  gives  the  least  function 
                Kd (up to an 0(l)-additive term).  This function can be called prefix complexity 
                K{x\\A).
                       207            Show that the same complexity (up to 0(1)) is obtained if decompressors 
                are  computable continuous  mappings  E  —>  F  (here  E  is  the space  of finite  and 
                infinite sequences of zeros and ones,  and F is the space of partial functions from 
                N to N) and complexity is the length of a shortest string that is mapped to some 
                partial function that is equal to x on all elements of A.
              208                      6.  GENERAL SCHEME FOR COMPLEXITIES
                   We can also define the prefix complexity with set  condition using prefix-free 
              functions instead of prefix-stable ones.  Again, in the class of computable prefix-free 
              functions there exists an optimal one (that gives the smallest complexity function 
              Kf(x\\A)).  In this way we get the definition of some function K'(x\\A) that re­
              sembles the conditional complexity K'(k\n) and coincides with it  (up to 0(1)) if 
              A = {n).
                   Finally  one  can  define  a  priori  probability  m(x\\A).  For  that  we  consider 
              some  probabilistic  machine  that  has  input  y  and  the  measure  of the  set  of all 
              sequences uj G  0 that  (being used as random bits)  makes the machine transform 
              every input y G A into x.  Again, there exists an optimal machine that maximizes 
              this probability  (up to an 0(l)-constant factor), and for singletons this definition 
              coincides with our definition of the conditional a priori probability.
                   The inequalities
                               -  log m(k K A) < K(k II A) + 0(1) < K'(k || A) + 0(1),
              can be proved in the same way as for conditional prefix complexity, but the argu­
              ment that showed that all three expressions coincide does not work as before.  As 
              Elena Kalinina [71] has shown, the second inequality is not an equality; we do now 
              know what happens with the first inequality.  But it is easy to see that all three 
              expressions are not less than
                                           — log inf m(k I x) — sup K{x | a),
                                                  xEA             xEA
              so each of them can be used in the theorem similar to Theorem 131.  In particular, 
              for K(x II A) (which seems to be most natural among all three) we get the following 
              result:
                   Theorem 134.
                                           lim  K(x II  ^ n) = K° (x) + 0(1).
                    208 Prove that all three quantities K(k || A), K'(k || A), and C(k || A) differ at 
              most by 0(the logarithm of the smallest one), i.e., by 0(\ogC(k || A)).
                   We do not know whether 0(x|| A) can be bounded by a linear (or even com­
              putable) function of — log m(k || A) (at least for finite A, or even for A that contains 
              only two elements).
                   Let us mention here that there is another type of problem in which the natural 
              notions of complexity and a priori probability differ significantly:  the enumeration 
              problems considered by R.  Solovay  [189].  Let us consider non-terminating algo­
              rithms whose input is a binary string.  Such an algorithm enumerates some (finite 
              or infinite) set by printing its elements one by one.  (If an algorithm starts to print 
              some output element, it is obliged to print it completely, and then it may resume 
              the computation.)  If A is an algorithm of this type and S is some enumerable set, 
              we define the complexity of S with respect to A as the minimal length of the input 
              for which A enumerates S:
                                     CEa(S) = min{l(p) I M(p) enumerates S'}.
              As usual,  it  is  easy  to  see  that  there  exists  an  optimal  algorithm  A  that  makes 
              CEa  minimal up to 0(1).  We fix an optimal A and call CEa(S) an enumeration 
              compleodty of S.  It is denoted by CE(S) and is finite if and only if S is enumerable.
                                     6.4.  COMPLEXITIES  AND  ORACLES                      209
                On the other hand, we may consider probabilistic enumeration algorithms, i.e., 
            the non-terminating algorithms without  input  equipped with a fair random bits 
            generator and producing output elements as explained above.  The output set of 
            a probabilistic enumeration algorithm A is a random variable, and for a given set 
            S we consider the probability of the event  “A enumerates 5”.  This probability is 
            denoted by rriA(S).  Again it  is easy to  see that  there  exists  an optimal A  that 
            makes тд maximal up to an 0(l)-factor;  we fix some A and omit the subscript 
            A,  calling m(S)  the  enumeration  a priori probability  of S.  It  was shown by de 
            Leeuw, Moore, Shannon, and Shapiro [91] that m(S) is positive if and only if S is 
            enumerable.
                 209  Prove the statement above.
                (Hint:  If a subset of Г2 has positive probability, there is an interval where the 
            fraction of this subset exceeds 1/2.)
                It is easy to see that CE(S) ^ — logm(5) + 0(1).  The reverse inequality, even 
            with logarithmic precision, i.e., the inequality — logm(5) ^CE(S) + 0(\ogCE(S)), 
            is unknown.  There are some partial results.  It is true with factor 3:
                                 -logm(S) ^ 3-CE(S) + 0(\ogCE(S)), 
            as shown in [189], and for finite sets the constant 3 can be replaced by 2 (see [197]).
                6.4.3.  Limit  frequencies  and  O'-relativized  a  priori  probability.  We
            conclude this section by a result from [133]; it relates the frequencies in computable 
            sequences to the O'-relativized prefix complexity.  (See also the simplified exposition 
            in  [16].)
                Let /(0), /(1),... be a computable sequence of natural numbers.  For a given n 
            and    let us count the appearances of к among /(0),..., /(n — 1) and divide the 
            result by n.  The ratio can be called the frequency of к  among the first n terms of 
            the sequence.
                Now for a fixed к consider the liminf of this frequency as n —)■ oo;  we call it 
            the lower frequency of element к in the sequence /.
                Let pk be a lower frequency of к in a given sequence.  It is easy to check that 
            YlkPk ^ 1-  Indeed, if some partial sum of this series exceeds 1, then a finite sum 
            of lim inf’s exceeds 1, and for sufficiently large n the sum of the frequencies among 
            the first n terms of the sequence exceeds 1, which is impossible.
                The following statement is true for any computable sequence /:
                Theorem 135.  The function к i-+ pk  is 0'-lower-semicomputable.
                (Here pk is the lower frequency of к; the definition of the lower semicomputable 
            function is given in  Section 4.1;  now we consider a O'-relativized version of this 
            definition.)
                PROOF.  Indeed,  the statement r  <   pk  (where r  is some rational  number)  is 
            equivalent to the following one:
                     there exist a rational number p > r and an integer N such that 
                     the frequency of к among the first n terms of f exceeds p for all 
                     n > N.
                The property printed in italics is со-enumerable (has an enumerable negation): 
            if it is not true, we can establish it by showing a number n that violates it.  Therefore
             210                     6.  GENERAL  SCHEME FOR COMPLEXITIES
             this property is O'-decidable (we apply the oracle to the algorithm that searches for 
             that n).  So the property r < pk is O'-enumerable.                                     □
                  In fact we use the following general observation:
                   210 Let  rn  be  a  computable  sequence  of  rational  numbers.        Show  that 
             lim inf r  is  a  O'-lower-semicomputable  real  number  and  the  corresponding  0'-
             algorithm can be effectively found given an algorithm for r
                  By the way, the reverse statement is also true:
                   211 Any O'-lower-semicomputable real number is a lim inf of a computable
             sequence of rational numbers.
                  {Hint-.  This  number  is  a supremum of a O'-computable sequence of rational 
             numbers  rn.  Each  rn  is  an  ultimate  value  of a  stabilizing  sequence  rn      Let 
             sk  be the  maximum of ro,fc, ■ ■ •, rt-i,k  where t  is  the minimal  number such that
             rt,k + n,k- !•)
                  It turns out that for an appropriate computable sequence / the function к         pk 
             is  a  maximal  O'-lower-semicomputable  semimeasure.  This  is  a  corollary  of the 
             following result:
                  Theorem  136.  For any O'-lower-semicomputable  sequence qo,qi,...  of non­
             negative reals such that JT qi < 1,  there exists a computable sequence /(0), /  (1),... 
             such that for all к  the lower frequency of к  in the sequence f   is at least qk-
                  This allows us to give an equivalent definition of O'-relativized prefix complexity 
             of A::  it is the negative logarithm of the lower frequency of к in the optimal sequence 
             /   (that gives maximal lower frequencies up to O(l)-factor).
                  P r o o f.  Since  qk  is  lower  semicomputable,  the  set  of pairs  (г, к)  where  r  is 
             a  rational  number  smaller  than  qk  is  O'-enumerable.  As  we  know  from  general 
             computability theory  (see,  e.g.,  [184]),  0' -enumerable sets are  E2-sets,  i.e.,  there 
             exists a decidable property R such that
                                           r < qk    Эи     R(r, к, u, v).
             We use a slightly  different  representation  of E2-predicates:  there  exists  a com­
             putable total function (г, к, n) i-» S(r,k,n) with 0/1-values such that r < qk if and 
             only if the sequence S(r, к, 0), S(r, к, 1) • • ■  has finitely many zeros.  The sequence 
             S(r, к, 0), S(r, к, 1) • • •  can  be  constructed  as  follows.  We  consider  (sequentially) 
             the values и — 0,1, 2,...,  and for each и we search for v such that R(r, k,u,v)  is 
             false.  While searching, we extend the sequence adding zeros.  When v is found, we 
             add 1 to the sequence and switch to the next value of u.  The number of zeros in the 
             constructed sequence is finite if and only if the search was unsuccessful for some -u, 
             i.e.,  if r < qk.
                 It  is  convenient  to  visualize  this  process  as  follows.  From  time  to  time  the 
             request “please make qk greater than r” appears for some к and r (and the previous 
             request  with the same к and r is canceled).  Then we consider the requests that 
             appear and are never canceled later; they correspond to pairs (r, k) such that r < Qk- 
             (The moments when requests appear correspond to zeros in the sequence S.)  This 
             process is computable.  We may also assume without loss of generality that at each 
             given moment there is only finitely many requests.  (This does  not matter since 
             only the limit behavior of the sequence is important.)
                                6.4.  COMPLEXITIES  AND  ORACLES               211
              Recall that we need a computable sequence /(0), /(1),... for which the lower 
           frequency of к is at least q^.  To achieve this goal, it is enough to represent the given 
           O'-lower-semicomputable semirneasure as the liminf of a computable sequence of 
           measures with rational values, i.e., to construct a two-dimensional table of rational 
           numbers
                                      Po   Pi   P°2
                                      Po   p\   P\
                                      Po   PÏ   Pi
           such that each row has only a finite number of non-zero elements, these elements 
           have sum 1, and the liminf in the kth column is at least q^.  Indeed, let us assume 
           that such a table is constructed.  Without loss of generality we may suppose that in 
           the ith row all the numbers are multiples of 1 /i (we may take approximation with 
           precision 1 /г not changing the limit).  Then the sequence /  can be constructed as 
           follows:  first  we  use  the  first  row  as  the  table of frequencies,  then switch to the 
           second row and use it for a much longer time  (to make the influence of the first 
           row negligible), then we use the third row even longer (to make the influence of the 
           first and second rows negligible), etc.
              So it remains to construct a table pj with the following property:  if some request 
           “please make qk greater than r” appears at some moment and is not canceled later, 
           then the &th column has lim inf at least q^.  This is done as follows:  in constructing 
           the nth row (at time n), we try to satisfy all current requests (that have appeared 
           and are not canceled)  according to their age  (the oldest request is treated first). 
           For each request we increase the corresponding pi- up to a given r if this is possible 
           (if it does not make the sum greater than 1).  We may assume that there are many 
           requests, and at some point the sum becomes greater than 1.  At that moment we 
          cut the last request (so the sum is 1), and this finishes the construction of nth row.
              Why does this help?  Imagine that r < qь  is true.  Then the request  “please 
          make q^ greater than r"  appears at some moment and is never canceled later.  (It 
          need not to be the first appearance of this request.)  Let us look at all requests that 
          appear before this one.  Some of them are canceled later (while others are “final”). 
          Let us wait until all these cancellations happen.  After that,  only  “true”  requests 
           (ones that are never canceled later) are older than our request, and for these true 
          requests we have r' < q^.  Their sum therefore does not exceed 1 together with our 
          request, so the requests with high priority at that time will not interfere with our 
          request.                                                              □
               212  Prove that there exists a computable sequence where the lower frequen­
          cies coincide with
              (Hint:  Combine the proof of this theorem with the solution of Problem 211.) 
              One more result from the same paper [133]:
               213 Prove that Theorem 136 remains true if we consider partial computable 
          functions /  from N to N instead of sequences:  for any partial computable function / 
          from N to N there exists a (total) computable sequence g(0), g(l),... that has the 
          same  (or bigger)  lower frequencies:  for any к the lower frequency of к in g is at 
          least its lower frequency in /(0), /(1),...  (which is defined as the limit  inferior of 
          the number of appearances of к among /(0),..., f(N  — 1) divided by N).
        212          6.  GENERAL  SCHEME  FOR COMPLEXITIES
          (Hint [16]:  For every N the frequencies in the initial segment of length N form 
        a lower semicomputable semimeasure  (it was a measure for total sequences);  the 
        construction used in the proof of Theorem 133 allows us to find an upper bound 
        for the limit frequencies that is a O'-lower-semicomputable semimeasure.  Then we 
        apply Theorem 136.)
                                             CHAPTER 7
                Shannon entropy and Kolmogorov complexity
                                       7.1.  Shannon entropy
                Consider an alphabet A that contains к letters a\,..., a^.  We want to encode 
            each letter a* by a binary string c^.  Of course, we want all c* to be different to avoid 
            confusion.  But this is not enough if we write codewords without any separator.  For 
            example,  imagine that  letters A,  B,  and  C  are encoded  by strings  0,  1,  and  01. 
            All three codes are different, but two strings AB AB and ABC have identical codes 
            0101.  So additional precautions are needed to guarantee unique decoding.
                We want the code to allow unique decoding.  At the same time we want it to 
            be space efficient.  It  is good to  have the strings c*  as short  as possible  (without 
            violating the  unique  decoding property).  And  if we  cannot  make  all  codewords 
            short, the priority should be given to the frequent letters.  (Similar considerations 
            were taken into account when Morse code was designed.)
                7.1.1.     Codes.  Let us give formal definitions.  A  code for a ^-letter alphabet 
            A =  {ai,..., afc}  consists  of к  binary strings  Ci,..., c*,.  These strings  are called 
            codewords  (for the code considered);  letter  ai  has  encoding  Ci.  Any  А-string  (a 
            finite sequence of letters taken from A) has an encoding; to get it, we encode each 
            letter and concatenate their codes (without separators).
                A code is injective  if different letters have different codes.  A code is  uniquely 
            decodable  if every two different А-strings have different codes.  A prefix code  is a 
            code where no  codeword is a prefix of another codeword.  (This is a traditional 
            terminology; however, the more logical name prefix-free code is also used.)
                Theorem 137.  Every prefix code is uniquely decodable.
                PROOF.  The first  codeword  (the  encoding  of the  first  letter)  is  determined 
            uniquely (due to the prefix property), so we can separate it from the rest.  Then 
           the second codeword is determined, etc.                                       □
                214 Show that there exist  uniquely  decodable  codes which  are  not  prefix 
           codes.
                (Hint:  Consider a suffix code.)
                215 Construct an explicit bijection between the set of all infinite sequences
           of digits 0,1, 2 and the set of all infinite sequences of digits 0,1.
                (Hint:  Use the prefix code 0 (->• 00,  1 (->• 01, 2 (->• 1.)
                216 Consider prefix codes ci,..., Ck  (for a ^-letter alphabet)  and d\,..., di
            (for an /-letter alphabet).  Show that strings Cidj (concatenations of codewords from 
           these two codes) form a prefix code for a ^/-letter alphabet.
               Before asking which of two codes is more space efficient, we should fix frequen­
           cies of the letters.  Let pi,... ,pk be non-negative reals such that pi + • • • +pn — 1-
                                                  213
                214               7.  SHANNON  ENTROPY AND KOLMOGOROV COMPLEXITY
                The number p* will be called the frequency or probability of letter a^.  For each code 
                ci,..., Ck  (for alphabet ai,..., ak) its average length is defined as
                                                              ^PiKci)-
                Now we can formulate our goal:  for a given pi,... ,Pk  we want to find a code of 
                minimal average length inside some class of codes, e.g., a uniquely decodable code 
                of minimal average length.
                      217  Which injective code has minimal average length (among injective codes)
                for given pi,... ,pn?
                     (.Hint:  Put all letters in order of decreasing frequency and all binary strings in 
               order of increasing length.)
                     7.1.2.  The  definition  of Shannon  entropy.  Shannon entropy provides a 
               lower bound for the average length of a uniquely decodable code.  It is defined (for 
               given non-negative pi such that YliPi = 1) as
                                   H — p\{— logpi) + p2(— logp2) H----+Pfc(-bgPfc).
                (We assume that plogp = 0 for p = 0, making the function plogp continuous at 
               the point p = 0.)
                     There is some motivation for this definition.  Letter a* appears with frequency 
               P i,  and each occurrence of           carries — logp*  “bits of information”, so the average 
               number of bits per letter is H.  But then we should also explain why we believe 
               that  each  occurrence  of the  letter  that  has  frequency pi  carries  — logpj  bits  of 
               information.  Imagine that somebody has in mind one of 2n possible numbers, and 
               you want to guess this number by asking yes-or-no questions.  Then you need n 
               questions,  and  each  answer gives  you  one  bit  of information;  so  when  an  event 
               having probability l/2n happens, it brings us n bits of information.
                     Of course,  the previous paragraph is just  a mnemonic rule for the definition 
               of entropy.  The formal reason to introduce this notion is given by the following 
               theorem:
                     Theorem 138.  Let pi,... ,pn be non-negative reals such that pi + - • -+pn — 1-
                     (a) The average length of every prefix code cq,..., c^ is at least H ( the entropy ):
                                                          ^ 2 р А съ)  >  H .
                                                            i
                     (b)  There exists a prefix code such that
                                                       'Ери*) <H + 1.
                                                         i
                    Proof.  Note that this theorem deals only with the lengths of codewords (but 
               not the codewords itself).  So it is important to know when given integers ni,..., nk 
               could be lengths of codewords in a prefix code.  Here is the criterion:
                    Lemma  (Kraft inequality).  Assume that non-negative integers                                    are
               fixed and we want to find binary strings c\,..., Ck  of these lengths  (/(q) = щ)  that 
               form a prefix code  (i.eC i  is not a prefix of Cj for i ф fi).  This is possible if and 
               only if
                                                           ^ 2 " ni  ^ 1.
                                             7.1.  SHANNON  ENTROPY                              215
                 We have already seen this statement, see the lemmas used to prove Theorems 
             56 (p. 92) and 58 (p. 93).  In one direction, if c* is never a prefix of the other string 
             Cj,  then  the  corresponding  intervals  of lengths  2~Ui  are  disjoint  and  the sum  of 
             their lengths does not exceed 1.  (Using probabilistic language, a random string of 
             zeros and ones has prefix cb with probability 2“"1 ;  these к events are disjoint, so 
             the sum of probabilities does not exceed 1.)
                 Going in the opposite direction, we can use a simpler argument that was used 
             before (in the proof of Theorem 58).  Simplification is possible since we have only 
             a finite number (
                               к) of integers and they are given in advance.  We can simply place 
             the corresponding intervals of lengths 2~Uj  inside [0,1]  from left to right going in 
             decreasing order of length.  Then each interval is properly aligned and corresponds 
             to a binary string of length щ.  The lemma is proven.
                 Let us prove the theorem now.  Without loss of generality we may assume that 
             all Pi  are  strictly positive  (since  null  values  do  not  change  Shannon  entropy and 
             average code length).  Theorem 138(a) says that if щ are non-negative integers and 
             £*2-”'  <  1,  then Т,РгПг  ^  L  It  is true for any non-negative reals щ  (even if 
             they are not integers).  Indeed,  let  qi  be equal to  2“n\  In these coordinates the 
             statement reads as follows:  if qi > 0 and       ^ 1, then
                                       ^2 pi(-log qi) ^ ^2 Pi (“ log Pi)-
             This inequality is sometimes called the  Gibbs inequality.  To prove it,  we rewrite 
             the difference between right-hand side and left-hand side as
             (*)                                  J2Pi l0S
             Then we use the convexity argument:  the weighted sum of logarithms does not 
             exceed the logarithm of the weighted sum ^pilogUi ^ log(52iPiui)  (if all щ are 
             positive).  In our case we see that (*) does not exceed
                                                     log Ç22 qi)  ^ loS1 = 0.
             Item (a) is proven.
                 Let us mention also that the non-negative number
                                                  J2pi log — 
                                                    i       qi
             is called the Kullback-Leibler distance between two probability distributions pi and 
             qi (so we assume that ^2 qi = 1) or the Kullback-Leibler divergence; the latter name 
             is better since this “distance” is not symmetric.  The convexity of the logarithm (its 
             second derivative is negative everywhere)  guarantees that this distance is always 
             non-negative and equals zero only if Pi = qi for all i.
                 To prove item (b), consider the integers щ —  [— \og2 pi]  (where |"u] is a minimal 
             integer greater than or equal to u).  Then
                                                Pi  ^   q—rii  ^
                                                 2  < 2     ^ Pi-
             The  inequality  2~Tli  ^  pi  allows  us  to  use  the  lemma,  so  there  exist  codewords 
             of corresponding  lengths.  The  inequality pij2  <  2~Ui  implies  that  ni  exceeds 
             (—logPi) by less than 1,  and this remains true after averaging:  the average code 
             length {J2Pini) exceeds H = J2Pi(~ l°gP;) by less than 1.                            □
             216           7.  SHANNON  ENTROPY AND KOLMOGOROV  COMPLEXITY
                 This proof is a kind of a “relaxation argument” :  if we forget that code-lengths 
             should be integers and allow any щ such that        2~Ui  <  1, the optimal choice is 
             Ш — — log pi (convexity of the logarithm function); making щ integers, we lose less 
             than 1.
                 Theorem 139.  The entropy of the distribution pi,... ,pn  (with n possible val­
             ues)  does not exceed logn.  It equals logn  only if all pi  are equal.
                 Proof.  If n is a power of 2, the inequality H ^ log n follows from Theorem 138 
             (consider a prefix code where n codewords all have length logn.  In the general case 
             we  use  Gibbs  inequality  for  g*  =  1/n  (for  all  г)  and  recall  that  this  inequality 
             becomes an equality only if pi = g*.                                               □
                 7.1.3.      Huffman code.  We have shown that the average length of an optimal 
             prefix code (for given Pi,... ,Pk) is somewhere between H and H + 1.  But how can 
            we find this optimal code?
                 Let n\,..., nk be the lengths of codewords for an optimal code (for given fre­
            quencies pi,... ,pk).  Rearranging the letters, we may assume that
                                             Pi  ^ P2  < • • •  < Pk-
            in this case we may assume that
                                             ni ^ n2 ^      > nk.
            Indeed, if one letter has a longer code than another letter that is less frequent, the 
            exchange of codewords (between these two letters) decreases the average length of 
            the code.
                 One can note also that щ — n2 for an optimal code (the two less frequent letters 
            have the same code length).  Indeed, if щ  > n2, then n\  is greater than all n*.  So 
            the first  term in the sum      2~Ui  is  smaller than  all other terms,  the inequality 
            Xa 2“ni < 1 cannot be an equality (all terms except the first one are multiples of the 
            second term), and the difference between its two sides is at least 2~ni.  Therefore, 
            we can decrease щ  by  1  and still not violate the  inequality      2~ni  <  1.  This 
            means that the code is not optimal (contrary to our assumption).
                 We can look for an optimal code among codes that have n\ = n2; this optimal 
            code minimizes the sum
                     Pini + p2n2 + p3n3 H-----+ pknk = (pi + p2)n + p3n3 + ■ • • + pknk
             (here n is the common value of щ  and n2).  In the last expression the minimum 
            should be taken over all sequences n, n3,... ,nk such that
                                    2~n + 2~n + 2~Пз + • ■ • + 2~Пк  ^ 1.
            This inequality can be rewritten as
                                      2-(n-l) +  2-n3 +  . . . +  2~nk  ^
            and the expression that is minimized can be rewritten as
                              (pi +P2) + (pi +p2)(n- 1) +p3n3 + • • • +pknk.
            The term  (pi + p2)  is  a constant  that  does  not  influence  the  minimal  point,  so 
            the problem reduces to finding an optimal prefix code for к — 1 letters that have 
            probabilities Pi + Р2,Рз, ■ • • ,Pk-
                                    7.2.  PAIRS  AND  CONDITIONAL  ENTROPY                      217
             We obtain the recursive algorithm that finds the optimal prefix code as follows:
                 • combine the two rarest letters into one (adding their probabilities);
                 • find the optimal prefix code for the resulting probabilities (a recursive call);
                 •  replace  the  codeword  x  for  a  “virtual”  combined  letter  by  two  codewords 
             xO and .xl  which are one bit  longer  (note that  this  replacement keeps the prefix 
             property).
                 The optimal code constructed by this algorithm is called the Huffman code for 
             a given distribution pi,... ,pn-
                 7.1.4.  Kraft—McMillan inequality.  So far we have studied prefix codes.  It 
             turns out that they are as efficient as general uniquely decodable codes, as shown 
             in the following theorem.
                 Theorem 140 (McMillan inequality).  Assum,e that ci,... ,ck  are codewords of 
             a uniquely decodable code,  and let щ = /(c,;)  be their lengths.  Then
                                                 E2'"'<L
                                                  i
                 Therefore (recall the lemma above) for any uniquely decodable code there is a 
             prefix code with the same lengths of codewords.
                 PROOF.  Let us use letters и and v instead of digits 0 and 1 when constructing 
             codewords (e.g., the codes 0, 01, and 11 are now written as u, uv, vv).  Now take a 
             formal sum (c\ + ■ ■ ■ + ck) of all codewords and consider its Nth power (for some N 
             that we choose later).  Then we open the parentheses without changing the order 
             of factors и and v (as if и and v were two non-commuting variables).  For example, 
             the code above gives (for N = 2) the expression
             (и + uv + vv)(u + uv + vv)
                                 = uu + uuv + uvv + uvu + uvuv + uvvv + vvu + vvuv + vvvv.
             Each term in the right-hand side is a concatenation of some codewords.  The unique 
             decoding property guarantees that all the terms are different.  Now we let и = v = 
             1/2.  The left-hand side  (ci + ■ ■ • + ck)N  becomes  (2~711  + • • • + 2~nk)N.  For the 
             right-hand side we have an upper bound:  if it consisted of all strings of length t, 
             it would contain 2* terms equal to 2~b  (each), so the sum would be equal to 1  (for 
             each length t).  Therefore, the right-hand side does not exceed the maximal length 
             of strings in the right-hand side, which equals N тах.(щ).
                 if £ 2 - 'n'  >  1, we immediately get a contradiction, since for large enough N the 
             left-hand side grows exponentially in N while the right-hand side is linear in N.   □
                 This proof looks like an extremely artificial trick (though a nice one).  A more 
             natural proof (or, better to say, a more natural version of the same proof) is given 
             below; see p. 222.
                                   7.2.  Pairs and conditional entropy
                 7.2.1.  Pairs of random variables.  Dealing with Shannon entropies, we use 
             the terminology which is standard for probability theory.  Let £ be a random variable 
             which takes finitely many values £i,... ,£& with probabilities pi,... ,Pk■  Then the 
             Shannon entropy of a random variable £ is defined as
                                   H(£) =Pi(-logPi) + •• • +pk(-logpk).
                                              7.  SHANNON  ENTROPY  AND  KOLMOGOROV  COMPLEXITY
                     218
                     T h is  d e fin itio n   allo w s  u s  to   c o n sid e r  th e   e n tro p y   o f  a   p a ir  o f  ra n d o m   v a ria b le s  £ 
                     a n d   77  ( t h a t   h a v e   a   c o m m o n   d is tr ib u tio n ,  i.e .,  a re   d e fin e d   o n   th e   s a m e   p r o b a b ility  
                     space).  Indeed, this pair is also a random  variable w ith a finite range.  T he follow ing 
                     t h e o r e m   s a y s  t h a t   th e   e n tr o p y   o f a   p a ir   d o e s   n o t  e x c e e d   th e   s u m   o f e n tr o p ie s   o f its  
                     com ponents.
                             Theorem 141.
                            We  consider  random   variables  w ith  finite  ranges,  so  this  is ju st  an   inequality 
                     involving  logarithm s.  L et  is  w rite  th is  inequality.                                 Assume  th at  £  has  к  values 
                     £1....,            a n d   77  h a s   I  v a lu e s  7/1,..., 77/.        T h e n   th e   m a x im a l  p o ssib le   n u m b e r  o f
                     values for th e p air  (£, 77)  is kl,  an d  th ese values are  (& , 77j)  (som e of th e m  m ay  never 
                     a p p e a r   o r  h a v e   p r o b a b ility   0 ).  T h e   d is tr ib u tio n   fo r  (£,77)  is  th e re fo re   a   ta b le   th a t 
                     h a s   к  ro w s  a n d   I  c o lu m n s.  T h e   n u m b e r ptj  (z th   ro w ,  jth  c o lu m n )  is  th e   p ro b a b ility  
                     of th e   e v e n t  “ (£  =   £*)  a n d   (77  =   77^ )”  (h e re   i = 1 ,... ,k  a n d  j — 1 ,...,/).  A ll pij  a re 
                     n o n -n e g a tiv e   a n d   th e ir   s u m   e q u a ls   1.  (S o m e   o f  th e   prJ  c a n   b e   e q u a l  to   0 .)
                            Adding the num bers in each row, we get the probability distribution for £:  the 
                     p ro b a b ility  o f th e  e v e n t  £  =   £*  e q u a ls           Pij •  W e d en o te  th is su m   by Pi*.  S im ilarly, 
                     77  ta k e s   v a lu e   77j  w ith   p ro b a b ility   p*j  w h ic h   e q u a ls  th e   s u m   o f  a ll  n u m b e rs   in   th e  
                     jth colum n.
                            Therefore,  th e  th eo rem   in  q u estio n   is  an   in eq u ality   th a t  is  ap p licab le  to   an y  
                     m atrix  w ith   n o n -n e g a tiv e   e le m e n ts  a n d   su m   1:
                                            ^>2 pij {-log pij)  ^                        ( - l o g  p i* )  +   ^ 7 V / ( - l o g p * j )
                                             i,j                                г                                 j
                     (here pi*  and p*j  are  th e sum s  of th e  row s  an d   co lu m n s).
                            T h is  in e q u a lity   a g a in   is  a   c o n s e q u e n c e   o f  th e   c o n v e x ity   o f  lo g a rith m ,  b u t  it  is 
                     useful to  u n d e rsta n d   its  in tu itiv e  m ean in g .  L et  u s  fo rg et  for  a  w h ile  th a t  e n tro p y   is 
                     n o t  e x a c tly  e q u a l  to   th e   le n g th  o f th e  s h o r te s t  p re fix   c o d e   ( a n d   ig n o re   th e   d iffe re n c e  
                     t h a t   d o e s   n o t  e x c e e d   1 ).  T h e n  th is  in e q u a lity  c a n  b e  p ro v e n  a s fo llo w s.  A s s u m e  t h a t 
                     space-efficient prefix codes for £ and 77 are given,  and th ey  have codew ords ci,..., 
                     and cfi,... ,di,  respectively.  T h en  consider a code for  (£, 77)  th a t assigns to  th e  value 
                     (£*, Vj) th e  strin g  ctdj (th e  c o n c a te n a tio n  o f c*  a n d   dj w ith o u t  a  s e p a ra to r).  W e g et 
                     a   p re fix   c o d e   (in d e e d ,  to   s e p a r a te   a   c o d e w o rd   t h a t   s t a r t s   a n   in fin ite   se q u e n c e ,  w e 
                     first  find th e  p refix  c*  a n d   th e n   th e  p refix   dj in  th e   re m a in in g  p a rt;  b o th  o p e ra tio n s 
                     can  be  perform ed  uniquely).  T he  average  length  of  this  code  equals  th e  sum   of 
                     t h e   a v e ra g e   le n g th s   o f  its   c o m p o n e n ts .   T h is  code  m ay  be  n o n -o p tim al  (w hich  is 
                     n a tu r a l,  sin c e   th e   in e q u a lity   c o u ld   b e   s tr ic t) ,  b u t  it  p ro v id e s  a n   u p p e r   b o u n d   fo r 
                     t h e   le n g th   o f th e   o p tim a l  c o d e .
                            Proof.  Let  us  transform   this  inform al  argum ent  into  a  proof.                                               Recall  the 
                     p ro o f o f T h e o re m   138  (p.  21 4 ).  W e  h av e  seen  th a t  th e   e n tro p y   is  a   m in im a l  v alu e 
                     of                 b g 2 Qi) ta k e n   over  all  tu p le s  o f  n o n -n e g a tiv e   re a ls  qt  th a t  h a v e   su m   1.
                     I n   p a r tic u la r ,  th e   e n tr o p y   o f  th e   p a ir   ( th e   le f t-h a n d   sid e )  is  th e   m in im a l  v a lu e   o f
                                                                               ^2 pij{-logqij)
                                                                                i,j
                                                            7.2.  PAIRS  AND  CONDITIONAL  ENTROPY                                                               219
                     t a k e n   o v e r  a ll  tu p le s   q^  o f n o n -n e g a tiv e   re a ls   t h a t   s u m   u p   to   1.  L e t  u s  r e s tr ic t  o u r 
                     a t t e n t i o n   to   “r a n k   1”  tu p le s   t h a t   h a v e   th e   fo rm
                                                                                  Qij — ([i* ' q*j
                     for  som e  tu p le s  o f  n o n -n e g a tiv e   re a ls  qi*  a n d   g*j  (b o th   tu p le s   h a v e   s u m   1).  T h e n  
                     ( — log qij)  can  be  decom posed  into  (— logg**)  +   (— logg*j),  and  th e  en tire  sum   is 
                     decom posed  into  tw o  parts,  w hich  after  p artial  sum m ation  over  one  coordinate 
                     becom es equal to
                                                                               EPi*(~log 4i*)
                                                                                 i
                     a n d
                                                                                j
                     respectively.  T h e  m inim al  values  of th e   tw o  p a rts  are  H(£)  a n d   H{rf).
                            Therefore,  th e  left-hand  side  of our  inequality  is  th e  m inim um   over  all  tu p les 
                     a n d   th e   rig h t-h a n d   sid e   is  th e   m in im u m   o v er  ra n k   1  tu p le s,  a n d   th e   in e q u a lity   is 
                     proven.                                                                                                                                      □
                            7.2.2.  Conditional entropy.  Recall the definition of conditional probability. 
                     Let  A and  В be  tw o  events.  T he  conditional probability of  В w ith  condition  A 
                     ( d e n o te d   a s  P r[£ ? |A ])  is  d e fin e d   a s  th e   ra tio   P r[A   a n d   B\/'Px[A\.                T h is  d efin itio n  
                     assum es th a t Pr[A ]  >   0 .  T h e  m o tiv a tio n  is clear:  W e a re  in te re ste d  in  th e  fra c tio n  of 
                     o u tc o m e s w h en  В h a p p e n e d  b u t re s tric t o u r a tte n tio n  to  th e  case w h en  A h a p p e n e d .
                            Let  A be an event  (th at  has non-zero probability),  and  let £ be a random  vari­
                     able w ith finite range                   ,...,£&•  T h en  we m ay consider th e  conditional distribution 
                     of  £  w h en   A h ap p en s.             We  get  a  new   random   variable:  now                                 h a s   p r o b a b ility  
                     Pr[(£ =                   i n s te a d   o f  P r[£   =   £*].  T h e   e n tro p y   o f th is   d is tr ib u tio n   is  c a lle d   con­
                     ditional entropy of £ with condition A and is denoted by Н(^\А). (T he distribution 
                     itself could  be  d en o ted   by  (£|A ).)
                              218 Show that H(£\A) can be greater than H(£)  and can be less than H(£). 
                            (Hint The distribution  (£|A )  has  not  m uch  in  com m on  w ith  th e  d istrib u tio n
                     of £,  especially  if A has  sm all  probability.
                            I n fo rm a lly   sp e a k in g ,  H(f\A)  is  th e   m in im a l  a v erag e  c o d e  le n g th   if th e   av erag e 
                     is  ta k e n   o n ly   o v e r  th e   c a s e s  w h e n   A h a p p e n s .
                            Now let us consider two random  variables £ and 77  (as w as done in the previous 
                     section).  L et  as  assum e  th a t  each  value  of b o th   £  an d   77  h as  non-zero  p ro b ab ility  
                     ( z e r o - p r o b a b ility   o u tc o m e s   c o u ld   b e   ig n o re d ).    For  each  value  rjj  (for  77)  consider 
                     t h e   e v e n t  77  =   777.  ( I t s   p r o b a b ility   w a s  d e n o te d   b y   p*j.)  C o n s id e r  th e   c o n d itio n a l 
                     e n tro p y   o f  v a ria b le   £  h a v in g   th is   e v e n t  a s  th e   c o n d itio n .  In   o th e r   w o rd s,  c o n s id e r 
                     t h e   e n tr o p y   o f  th e   d is tr ib u tio n   i          Pij/p*j■         T h e n   w e  av erag e  th e se   e n tro p ies, 
                     using  probabilities  of th e  events  77 =   rjj  as  w eights.  T h e  resu ltin g   average  is  called 
                     conditional entropy of £ with condition 77.  It is denoted by H(£\ij). So by definition
                                                              Щ£\г}) = E  Prfo = ГЬ]Н(Л\П = Vj) 
                                                                                 3
                     or,  u sin g   th e   n o ta tio n   a b o v e ,
                                                                                                             log^
                                                                               j           г            '          p*j
                     220                      7.  SHANNON  ENTROPY AND  KOLMOGOROV COMPLEXITY
                            T h e  follow ing th eo rem  sum s u p  th e  basic p ro p erties o f co n d itio n al en tro p y   (th a t 
                     are  tru e   for  an y   ra n d o m   v a ria b le s  £  a n d   p)\
                            Theorem 142.  (a)  H(£\p) ^  0;
                            (b)  H(£\p) =   0  if and only if £  =   f(p)  with probability 1  for some function f 
                     (in other words,  we iqnore the cases that have zero probability);
                            (c)  Я(£|'„) < Я(£);
                            (d)  Я ((£,т,) ) = Я ( 1?) +  Я(£|7)).
                            Proof.  Item   (a)  is  evident:  all  H(f\rj = pj) are  non-negative,  so  th e  sam e  is 
                     t r u e   fo r  th e ir   w e ig h te d   su m .
                            (b)  If th e   w eig h ted   su m   o f n o n -n e g a tiv e   te rm s   e q u a ls  zero ,  th e n   a ll  th e   te rm s 
                     t h a t   h a v e   n o n -z e ro   w e ig h ts  a re   e q u a l  to   z ero .   So  for  each  value  pj th e  restricted  
                     variable  (£|^  =   pf) has  zero  entropy,  and  therefore  has  only  one  value  if we  ignore 
                     values th a t  have  p ro b ab ility   0.
                            S ta te m e n t  (c)  c a n   b e   e x p la in e d   a s  follow s:  H(£\p) is  th e   av e ra g e   le n g th   o f a n  
                     o p tim a l  co d e  for  £  if  w e  allow   d ifferen t  co d es  for  £  for  d ifferen t  v alu es  o f  p (for 
                     each  value  of  p we  consider  th e  code  th a t  is  o p tim al  w ith   resp ect  to   co n d itio n al 
                     d is tr ib u tio n ).  T h is   p ro v id e s  so m e   a d d itio n a l  fre e d o m   (c o m p a re d   to   th e   c a se   w h e n  
                     t h e  s a m e  c o d e  s h o u ld   b e  u se d  fo r  a ll v a lu e s o f p), a n d   th is   fre e d o m  c a n  o n ly  d e c re a s e  
                     t h e   o p tim a l  c o d e   le n g th .
                            T h e   s a m e   a r g u m e n t  is  m a d e   fo rm a l:  F o r  e a c h   j th e   v a lu e   o f H(f\p — pj)  is  th e  
                     minim al value of the sum
                     t a k e n   o v e r  a ll  n o n -n e g a tiv e   v a lu e s  o f th e   v a ria b le s   q\j  +       +   •  ■  ■  +   qkj  =   1  (w e  u se
                     different  variables  for  each j). T herefore,  H(f\p) is  th e  m inim al  value  of th e sum
                                                                         j            i   p*j
                     t a k e n  o v e r  a ll  ta b le s   t h a t   c o n ta in   n o n -n e g a tiv e  r e a ls   qtj  a n d   e a c h   c o lu m n   h a s  s u m   1. 
                     I f   w e   r e s t r i c t   o u r s e lv e s   to   ta b le s   w h e re   a ll  c o lu m n s   a r e   e q u a l  (qij =   qf), th e   su m  
                     t u r n s   in to
                                                                                              P t j ( - l o g f t )   =   ^ 2  Pi* { - log qi),
                                      j           i    P*j                          j     i                               i
                     a n d   its   m in im u m   is  ü f(£ ).  T h e re fo re   H(£\p)  ^   Я (£ ).
                            Finally,  item   (d)  is ju st  an   exercise  in  tra n sfo rm a tio n   of lo g arith m s:
                          Y^PiÅ-b g Pij) = J2 p*j                                                          b g p*j)
                           ij                                j            i    P*j              P*j
                                                       =                                  log ^                                           log^ )  
                                                             j            i   P*3               p*3           j            {   p*3
                                                       = ^ 2 p*3H {^\v = Vj) + Y ^ p* j(-l°èp*jî = Я ^1г^) + H ^l)'
                                                             3                                     3
                     T h e   th e o re m   is  p ro v e n .                                                                                                     □
                            T h is  th e o re m   im p lies  T h e o re m   141  (p.  2 1 8 ).                 We  see  also  th a t  th e   en tro p y  
                     of a  p air  of  ra n d o m   v ariab les  c a n n o t  b e  less  th a n   th e   e n tro p y   o f  a n y   o f  v a ria b le s
                                                         7.2.   PAIRS AND CONDITIONAL ENTROPY                                                           221
                    (since  conditional  entropy  is  non-negative).  Thus  we easily  obtain the  following 
                    statement:
                           Theorem 143.  Let £ be a random variable with a finite range,  and let f  be a 
                    function defined on that range.  Then
                                                                          я(/(£)) < #(£),
                    where /(£) is a random variable that is a composition of f  and £ (i.ef is applied 
                    to the value o/£).
                           In terms of distribution the transition from £ to /(£) means that we combine 
                    several values into one that sums up the corresponding probabilities.
                           Proof.  Indeed, the random variable (£,/(£)) has the same distribution as £, 
                    and its entropy cannot be less than the entropy of the second coordinate.                                                            □
                            219 Provide an interpretation of this result in terms of minimal average length
                    of codes,  and th e direct  proof.
                            220  W hen does the inequality of Theorem   143  becom e an equality?
                           7.2.3.  Independence and entropy.  The notion of independent random  vari­
                    ables could be easily expressed in term s of entropy.  R ecall th e variables £ and 77 are 
                    called  independent if th e  probability  of th e  event  “£  =   &  and  77  =   77j”  is  equal  to 
                    t h e   p r o d u c t  o f  p r o b a b ilitie s   o f th e   e v e n ts   £  =   &   a n d   77  =   77j.  (T o   re fo rm u la te :  T h e  
                    conditional distribution of £ w ith condition 77 =  r/j  coincides w ith  th e unconditional 
                    d is tr ib u tio n .  A lso  w e  c a n   e x c h a n g e   £  a n d   77  a n d   sa y   t h a t   c o n d itio n a l  d is tr ib u tio n  
                    of 77  w ith   c o n d itio n   £  =   £*  c o in c id e s  w ith   th e   u n c o n d itio n a l  d is tr ib u tio n .)
                          I n   t h e   n o ta tio n   u se d   a b o v e   th e   in d e p e n d e n c e   c a n   b e   w r itte n   a s  pij  — Pi*p*j 
                    ( p r o b a b ility   m a tr ix   h a s   r a n k   1).
                           Theorem 144.  Random variables £ and 77 are independent if and only if
                                                                   я«£.ч)) = я«) + я(ч).
                          I n   o th e r   w o rd s,  w e  g e t  a n   in d e p e n d e n c e   c rite rio n :    T h e   in e q u a lity   o f  T h e o ­
                    r e m   141  b e c o m e s  a n   e q u a lity .  U sin g   T h e o re m   1 4 2 ,  w e  c a n   re w rite   th is   c rite rio n   as 
                    # ( £ )   =   # ( £ |t7 )   (o r,  sy m m e tric a lly ,  #(77)  =   Я (т 7 ^ )).
                           PROOF.  Let us use once more that the logarithm is a strictly convex function: 
                    the inequality
                                                                 b g
                    holds  for  all  po sitiv e  w eights  pi  w ith   su m   1  a n d   all  p o sitiv e  ж*.                     T h is  in e q u a lity  
                    becom es  an equality only if all  ж*  are equal.
                          Therefore,  for positive pi  w ith sum   1  th e expression
                                                                            ^  Pi (-log ft)
                    (w here  ft  are  p o sitiv e  a n d   su m   u p   to   1)  ta k e s  its  m in im a l  v alu e  o n ly   a t  th e   p o in t 
                    f t   =   Pi-
                          Now recall the proof of T heorem  141 above.  T he m inim um  over rank 1 m atrices 
                    ( t h a t   m a k e s   th e   r ig h t- h a n d   s id e   e q u a l  to   th e   s u m   o f e n tro p ie s )  w a s  a c h ie v e d   fo r
                                                                            Qij  — Pi*  ' P*j •
                      222                       7.  SHANNON ENTROPY AND KOLMOGOROV COMPLEXITY
                      I f  t h i s   m in im u m   c o in c id e s  w ith   th e   m in im u m   ta k e n   o v er  a ll  m a tric e s   qjj  (th e   la tte r  
                      is  a c h ie v e d   fo r  qij — pij),  th e n   w e  h av e
                                                                                     Pij = Pi*  ' P*j i
                      a n d   v a ria b le s   f   a n d   77  a r e   in d e p e n d e n t.                                                                              □
                               221 Provide an another  (though sim ilar)  proof using T heorem   142.
                               222 Prove  th at  three  random   variables  a, ß, 7  are  independent  (this  m eans 
                      t h a t   t h e   p r o b a b ility   o f  th e   e v e n t  (a = cti,ß  =   ßj, 7   =   7*,)  e q u a ls  th e   p ro d u c t  o f 
                      t h r e e   p r o b a b ilitie s   fo r  e a c h   o f th e   v a ria b le s )  if  a n d   o n ly   if
                                                                H((a, /3,7)) =  Ща) + H(ß) + H (7 ).
                              T h e o re m s  141  a n d   144  show   th a t  th e   d ifference  H(£) + H(rj) — H((£,r)))  is 
                      alw ays  non-negative and equals zero if and only if f  and  77  are independent.  So we 
                      can  tak e  th is  difference  for  a   q u a n tita tiv e   m e asu re  o f  d e p e n d e n c e   b e tw e e n   f   a n d  
                      77.   T h is  d ifference  is  d e n o te d   b y   /( £   :  77)  a n d   is  c a lle d   th e   mutual  information  of 
                      tw o  ra n d o m   v a ria b le s  £  a n d   77.  T h e o re m   142  allo w s  u s  to   re w rite   th e   d e fin itio n   for 
                      / ( £   :  77)  in   t h e   fo llo w in g   w ay :
                                                          Hi ■ v) = H(v) - я(г,Ю = як) - як|„)
                      ( m u tu a l  in fo rm a tio n   sh o w s  how   m u ch   th e   k n o w led g e  o f  o n e   ra n d o m   v a ria b le   d e ­
                      creases th e entropy of th e o th er one).
                              To see all these notions in action,  let us retu rn  to th e M cM illan inequality.  Now 
                      we  change  th e  order  and  prove  first  th a t  a   u n iq u ely   d eco d ab le  code  for  a  ran d o m  
                      variable f   h as  th e   av erag e  le n g th   o f th e   co d ew o rd   a t  le a st  H(£).
                              F irs t  n o te  th a t   fo r  a n   in je c tiv e  c o d e  w h e re   a ll  c o d e w o rd s  h a v e   le n g th   less  th a n   c 
                      t h e   a v e ra g e   le n g th   is  a t  le a s t  H(£) —  lo g e .              I n d e e d ,  if  щ  a re   th e   le n g th s  o f  th e  
                      codewords, the sum  of 2 ~ni  does not exceed c  (for every fixed length th e sum  does 
                      n o t  e x c e e d   1 ).     Therefore,  th e  inequality  of  T heorem   138  is  violated  at  m ost  by 
                      lo g e .
                              T h is  is  n o t  e n o u g h ,  a n d   to   g e t  a   tig h t  b o u n d   w e  c o n s id e r  N  in d e p e n d e n t  id e n ­
                      tic a lly   d is tr ib u te d   c o p ie s  o f th e   ra n d o m   v a ria b le   £.  W e  g e t  a   ra n d o m   v a ria b le   t h a t 
                      could  be  denoted  by  ÇN.  Its  entropy  is  NH(£).  L et  us  use  our  code  for  each  of 
                      N coordinates and then concatenate all the strings.  T he unique decoding property 
                      g u a r a n te e s   t h a t   th is   is  a n   in je c tiv e   c o d e .  I ts   a v e ra g e   le n g th   is  N  tim e s   g r e a te r  th a n  
                      t h e   a v e r a g e   le n g th   o f in itia l  c o d e   fo r  f   (lin e a rity   o f e x p e c ta tio n ) .  A n d   th e   m a x im a l 
                      l e n g th   d o e s   n o t  e x c e e d   cN  w h e re   c  is  a n   u p p e r   b o u n d   fo r  th e   le n g th   o f  th e   c o d e ­
                      words of the uniquely decodable code we started w ith.  So  the previous paragraph 
                      gives  us
                               N •  (average length of th e  uniquely decodable code)  ^   NH(£) — \og(cN).
                      Now we divide over  N and  take  N —>  00.  Since  \og(cN)/N —>  0  as  N —>  00,  this 
                      gives us th e bound  H(£)  for th e  average  length of a uniquely decodable code.
                             Now  the  M cM illan  inequality  is  easy.  A ssum e  th a t  th e  uniquely  decodable 
                      code  has  code  lengths  П1,...,пд,  and                                    2 ~Ui  >  1.          We  sta rt  w ith   p ro b ab ilities 
                      Pi  =   2 ~n‘  a n d   th e n   p ro p o rtio n a lly   d e c re a se   a ll  o f  th e m   m a k in g   th e ir  su m   e q u a l 
                      t o   1.   Consider  th e  random   variable  th a t  has  th e   d istrib u tio n   Pi  (o b tain ed   in  th is 
                      way)  and its coding by m eans of our uniquely decodable code.  T he average length 
                      is  YlPini  w hich  is  less  th a n   H = Y^Pi(~^°ëPi)  (recall  th a t  щ <  — lo g pi  since  we 
                      have decreased the values 77).
                                                             7.2.     PAIRS AND CONDITIONAL ENTROPY                                                                 223
                               223 Look closely at  this  proof and  trace the  correspondence  betw een  it  and
                      t h e   p r o o f   g iv e n   a b o v e .
                             7.2.4.  “Relativization”  and basic inequalities.  All the statem ents about 
                      e n tro p y   h a v e   “r e la tiv iz e d ”  (c o n d itio n a l)  v e rsio n s.  F o r  e x a m p le ,  w e  c o u ld   a d d   so m e 
                      r a n d o m   v a ria b le   a   a s  a   c o n d itio n   in   th e   in e q u a lity
                                                                         я((М)<я(0 + Щ||)
                      a n d   g e t  its   c o n d itio n a l  v e rsio n
                                                                   H((t,v)\a) < H(t\a) + H(V\a).
                      T h e   c o n d itio n a l  v e rsio n   is  a n   e a s y   c o n s e q u e n c e   o f  th e   u n c o n d itio n a l  o n e .  In d e e d , 
                      for  each  fixed  value  cq  of a  ran d o m   v ariab le  a,  w e  have
                                                     H{(£,T])\a = ai) < #(f|a = cci) + Н(т)\а = aß)
                      ( T h e o re m   141  is  ap p lie d   to   c o n d itio n a l  d is trib u tio n s   o f  £  a n d   rj  w ith   c o n d itio n  
                      a =   aß).  T h e n   w e  su m   u p   all  th e s e   in e q u a litie s  w ith   w e ig h ts  P r [ a   =   aß.
                             So  we  get  a  co n d itio n a l  in e q u a lity   as  a   co n se q u e n c e   o f  th e   u n c o n d itio n a l  on e. 
                     Now,  going in  th e opposite  direction  and  using  th e equation
                                                                      Я(/9|1) = Я(08,7»-Я(7),
                     we can express all conditional entropies in  term s  of unconditional ones.
                             A fter  canceling som e  term s,  we get  th e  follow ing  inequality:
                             Theorem 145  (Basic inequality).
                                                            H(t 4, a) + H(a) < H((, a) + Я(Ч, a).
                             We use a sim plified  n o tatio n   an d   w rite  Н(^,г),а) in stead   Н((^,г),а))  (or  even 
                     more form al H((((,T]),a))).
                             Sim ilar  relativization  (adding  random   variables  as  conditions)  can  be  applied 
                     t o   m u tu a l  in fo rm a tio n .  F o r  e x a m p le ,  w e  c a n   n a tu r a lly   d e fin e   I (a :  /3 |у )  as
                                                                  Я(а|7) + Я(,а|7)-Я«а,/?)|7).
                     T h e  b a s ic  in e q u a lity   ( T h e o re m   1 4 5 )  sa y s t h a t  I (a : ßl'j)  ^   0 fo r a ll ra n d o m  v a ria b le s
                              224 Prove that  I((a,ß)  : 7)  ^  I(a : 7). 
                              225 Prove that
                                                               I((a, ß) :  7)  =   I{a :  7 )  +  I(ß : ^\a).
                             I f   I  (a :  7 |/3 )  =   0 ,  th e   r a n d o m   v a ria b le s   a a n d   7   a re   c a lle d   independent relative 
                      to ß (w hen ß is know n).  E x p erts  in  p ro b ab ility   th e o ry   say  in  th is  case  th a t  a,ß, 7  
                     form   a   Markov  chain w here  th e  dependence  betw een  th e  past  (a)  and  th e future 
                      (7)  is  c a u se d   o n ly   b y   th e   current state  (ß).
                              226  Prove th a t in th is case I(a : 7)  < I (a : /?), and therefore I(a : 7)  ^   H{ß).
                            To  prove  all  th ese  (and  sim ilar)  sta te m e n ts,  one  could  use  th e   d iag ram s  th a t 
                     are  sim ilar  to   th e   d ia g ra m s  for  K o lm o g o ro v   co m p lex ity   d iscu ssed   in   C h a p te r  2 . 
                     T h e   d ia g ra m   fo r  tw o   v a ria b le s  c o n s ists  o f th r e e   re g io n s.  E a c h   re g io n   c a rrie s  a   n o n ­
                     n e g a tiv e   v a lu e .   T h e   s u m   o f  th e s e   v a lu e s  fo r  tw o   le ft  re g io n s  is  H(a)  a n d   fo r  tw o  
                     r i g h t  re g io n s  is  H(ß)  (see  F ig u re   2 0 ).
            224           7.  SHANNON  ENTROPY AND KOLMOGOROV COMPLEXITY
                             Figure 20.  Entropies of two random variables
                For three variables a, ß, 7 we get a more complicated diagram (Figure 21).  The 
            central region carries a number that is denoted by I (a : ß : 7).  It can be defined 
            as I (a : ß) — I (a : ß\j) or, equivalently, as I(a : 7) — I (a : 7|/3), etc.  In terms of 
            unconditional entropies we get the following expression:
                            F ig u r e   21.  Entropies of three random variables
                Note that  (unlike the other six values shown) the value of I (a : ß : 7) can be 
            negative.  For example, this happens if variables a are ß independent, but become 
            dependent when 7 is known.
                227 Construct three variables a, ß, 7 with this property.
                (Hint  Following the example given on p.  51,  consider uniformly distributed
            independent variables a and ß with range {0,1}, and let 7 = (a + ß) mod 2.)
                228 (Fano inequality) Prove that if the random variables a and ß differ with
            probability at most e < 1/2 and a takes at most a values, then
                                        H(a\ß) < £loga + h(e),
                                                         7.2.   PAIRS  AND  CONDITIONAL ENTROPY                                                         225
                    where h(e) is the entropy of a random variable with two values and probabilities e 
                    and 1 — e.
                           (Hint:  Let 7 be a random variable with two values;  7 = 0 when ot ф ß and 
                    7 = 1 when a — ß.  Then H(a\ß) < H{7) + H(a|/5,7).  The first term is Д(er), and 
                    the second one can be rewritten as
                                             Pr[7 = 0}H((oi\ß)\j = 0) + Pr[7 = l]H((oi\ß)\j = 1),
                    i.e.,
                                           Pr[a ф ß\H((a\ß)\a ф ß) + Pr[a = ß]H((a\ß)\a = ß),
                    which does not exceed e log a + 0.)
                            229 Assume that H(a\ß,'y) — 0 and I(ß : a) — 0.  Prove that                                                  7) ^ H(a).
                           This problem has the following interpretation.  If a spy wants to send to the 
                    headquarters a secret message a as a plain text ß using a key 7 (that is agreed upon 
                    in advance) and wants the adversary, who does not know 7, to get no information 
                    about a, then the entropy of key 7 cannot be less than entropy of the message a. 
                    This statement is sometimes called the Shannon theorem on perfect cryptosystems.
                            230 Prove that
                                                     2H(a, ß, 7) < H(a, ß) + H(ß, 7) + H (a, 7)
                    for any three random variables a, ß, 7.
                           (Hint:  See the proof of the corresponding statement about Kolmogorov com­
                    plexity, Theorem 26 (p. 48).)
                            231 Prove a similar inequality for n random variables:
                                   {n - l)H{oi\,... ,an) ^ H(a2, ...,£*„) +  ■■■ +  #(£*!,... ,an_i).
                    (The right-hand side contains n terms where one of the variables is omitted.)
                            232 (Shearer inequality  [43])  Prove the following generalization of the pre­
                    ceding inequality.  Let T\,... ,Tk  be arbitrary tuples made of (some of)  the ran­
                    dom variables 07,...,an,  and each variable appears  in exactly r  tuples  (among 
                    Ti,...,Tfc).  Then
                                                    rH(al,a2,. ■. ,an) ^ H(T\) н------\-H(Tk).
                            233  Prove that the inequality of Problem 231 implies the upper bound for the 
                    volume of an n-dimensional body in terms of the volumes of its (n— l)-dimensional 
                    projections onto coordinate hyperplanes (the case n = 3 was mentioned on p.  12): 
                    If V is the volume of the body and Vi,..., Vn are volumes of its projections, then
                                                                    V^ - 1  < Vi • V2                  vn
                           (.Hint:  First consider the discrete version when the body is made of unit cubes 
                    on the grid.  For this a random variable that is uniformly distributed among these 
                    cubes is useful.  An arbitrary case can be treated as the limit of the discrete one.)
                          The  last  inequality  is  a  special  case  of a  general  Loomis-Whitney  inequal­
                    ity [105].
             226             7.  SHANNON  ENTROPY  AND  KOLMOGOROV  COMPLEXITY
                                        7.3.  Complexity and entropy
                  As you surely have  noticed,  the  properties  of Shannon  entropy  (defined  for 
             random variables)  resemble the properties of Kolmogorov complexity (defined for 
             strings;  see Chapter 2).  Is it possible to formalize this similarity by converting it 
             into exact statements?
                  This question has two interpretations.  First,  one can prove that Kolmogorov 
             complexity and Shannon entropy have similar properties (in particular,  the same 
             linear inequalities are true for them; see Section 10.6, p. 326).  On the other hand, 
             one may compare the numeric values for complexity and entropy, and this is what 
             we do in this section.
                  The problem here is that Kolmogorov complexity is defined for strings while 
             Shannon entropy is defined for random variables, so how could one compare them? 
             However, sometimes this comparison is possible, as we shall see.  Let us start with 
             a very vague and philosophical description of the results below:  Shannon entropy 
             takes into account only frequency regularities while Kolmogorov complexity takes 
             into account all algorithmic regularities, so in general the latter is smaller.  On the 
             other hand, if an object is generated by a random process in such a way that it has 
             only frequency regularities, entropy is close to complexity with high probability.
                  Let us give now some specific results that illustrate this general statement.
                  7.3.1.       Complexity and entropy of frequencies.  Consider an arbitrary fi­
             nite alphabet A which may contain more than two letters.  Kolmogorov complexity 
             for А-strings can be defined in a natural way.  (Note that we have never used that 
             finite  objects  whose complexity is defined are binary strings.  However,  it  is  im­
             portant that binary strings are used as descriptions:  complexity measured in bytes 
             would be eight times less than complexity measured in bits!)
                  Let  X  be  an  А-string  of length  A/",  and  let  p\,..., pk  be  the  frequencies  of 
             letters  in  x.  All these frequencies are fractions with denominator N and integer 
             numerators.  The sum of frequencies equals  1.  Let  h(p\,... ,pk)  be the Shannon 
             entropy of corresponding distribution.
                  Theorem  146.         C(x)                      O(logiV)
                                         N < h{pi, ■ ■,Pk) +          N
                  Here the constant in О (log N)  does not depend on N, x and the frequencies 
             pi,... ,pk-  However, this constant may depend on к (we consider an alphabet of a 
             fixed size).
                  PROOF.  In fact this is a purely combinatorial statement.  Indeed, the complex­
             ity C(x\N,p\,... ,pk) does not exceed logC(iV,pi,... ,p*.) + 0(1), where
                                                                   N\
                                  C{N,pu ...  pк)       (pt jV)!(p2iV)! • • • (pkN)\
             is  the  number  of А-strings  of length  N  that  have  frequencies pi,...,pk-  (Each 
             string with given frequencies can be determined by its ordinal number in this set if 
             the parameters N,p\,... ,pt are known, and this number has log C(A/’,pi,... ,pk) 
             bits.)
                  The number C(iV,pi,... ,Pk) can be estimated using Stirling’s approximation. 
             Ignoring factors bounded by a polynomial in N (that appear due to the term \fbik 
             in Stirling’s approximation formula k\ ~ V2Îтк(к/е)к), we get exactly 2Nh^Pl'---'pA.
                          7.3.  COMPLEXITY AND  ENTROPY         227
         This computation was performed  (for к = 2) when we proved the Strong Law of 
         Large Numbers  (Theorem 27,  p.  56).  The general case  (for  arbitrary  k)  can  be 
         treated in the same way.
            Finally, note that we need about к log TV bits to specify TV, p\,..., pk (we need to 
         specify к integers whose sum is TV), so by deleting the condition in C(x\N,pi,... ,pk) 
         we increase the complexity by 0(log TV)  (and the constant in О (log Annotation is 
         close to k).                                           □
           Another proof uses the upper bound for monotone complexity (Theorem 89, 
         p.  144).  Consider  a probability  distribution  on  infinite  А-sequences  that  corre­
         sponds to independent trials with probabilities Pi, ■ ■ ■ ,Pk hi each trial.
           The event “a sequence with prefix 2 appears”, where 2 is an А-string of length TV 
         that has frequencies q\,..., qk, equals
         (letter ai has probability pi  and appears qiN times).  The binary logarithm of this 
         probability is equal to
                        -N  - (çi(— logpi) + ----bç^-logpfc))-
         For the special case qi — pi we get —Nh(pi,... ,Pk)', therefore the monotone com­
         plexity has upper bound Nh(p\,... ,Pk)-  (Recall also that monotone complexity 
         differs from other complexity versions by a term О (log TV) for strings of length TV.)
           In fact, this argument is flawed.  When we proved the upper bound for mono­
         tone complexity,  we had  assumed that  distribution  is fixed.  The constant  term, 
         therefore,  may depend on the distribution.  And now we try to estimate KM(x) 
         using a measure that depends on the letter frequencies in the string x.  So formally 
         Theorem 89 is not applicable.  But if we recall its proof, we see that it provides a 
         bound for conditional monotone complexity when pi,■■■,Pk are given.  The differ­
         ence between this conditional complexity and the unconditional one is О (log TV), so 
         we indeed get another proof for Theorem 146.
            234 What is a value of a constant hidden in О (log TV)  (as a function of к)?
           (Hint:  Both proofs give k(l + o(l)) log TV.)
            235 Show that when all frequencies pi,... ,Pk  are not  very close to 0,  the 
         statement of the previous problem could be improved up to (k/2 + 0(1)) log TV.
           (Hint:  In the first proof one should take into account the square roots in Stir­
         ling’s approximation—most of them are in the denominator.  The second proof can 
         also be modified:  instead of exact values of frequencies, one can consider approxi­
         mate frequencies with an error of order 0(l/v/TV).  This gives a weaker bound, but 
         the difference is  bounded  by  a constant.  (Recall that  a smooth function is qua­
         dratic near its minimum.)  In this way we can save half of the bits when specifying 
        Pi,--- ,Pk-)
           Note that the inequality provided by Theorem 146 may be very far from equal­
         ity.  Indeed, if A has two letters and they alternate in a string x, then the right-hand 
        size equals 1 and the left-hand size is of order (log TV)/iV.  This is not surprising and 
        fits well into the general picture:  the complexity is small since it reflects all the reg­
         ularities  (not only frequencies).  In the next sections we prove that the complexity 
        of a randomly generated string is close to the entropy with high probability.
             228           7.  SHANNON  ENTROPY AND KOLMOGOROV COMPLEXITY
                 7.3.2.  Expected  complexity.  Let  us  fix  k,  a  ^-letter  alphabet  A,  and  к 
             positive numbers Pi, ■ ■ ■ ,Pk whose sum is 1 (for simplicity we assume that all pi are 
             rational numbers).
                 Consider a random variable £, whose values are letters of A and probabilities are 
            Pi,... ,Pk-  For each N consider a random variable £jv consisting of N independent 
             identically distributed copies of £.  Its values are Л-strings of length N.  Now we may 
             ask a question:  What is the expected complexity of a string generated according 
             to this distribution?
                 Theorem 147.  The expected value of K(£n\N) is NH(£) + 0( 1) (the constant 
             in 0(1)  may depend on £ but not on N).
                 Note that  (for positive pi) all Л-strings of length N are among the values of 
            £n .  Some of them have complexity much greater than NH (except for the case of 
            uniform distribution), but others have complexity much less than NH.
                 PROOF.  For each Л-string of length N (i.e., for each value of £jv) consider its 
            shortest description (with respect to some fixed prefix-stable decompressor).  These 
            descriptions form a prefix code (in the sense of Section 7.1.1).  The average length 
            of the codeword is exactly the expected value of K(£N).  Therefore, Theorem 138 
             (p.  214) guarantees that this expected value cannot be less than H(ÇN) = NH(£). 
            The lower bound is proved (and even the 0(l)-term can be omitted).
                 The same theorem is useful for the upper bound, too.  Indeed, it guarantees that 
            there exists a prefix code that has average length of a codeword at most H(£N) +1. 
            Such a code can be constructed by an algorithm if N (and numbers pi, which are 
            fixed)  is  given.  For example,  one may  use the construction used in the proof of 
            Theorem 138, or use Huffman code, or even just try all codes until a good one is 
            found.
                 Anyway, the constructed code can be used as a conditional decompressor (with 
            N as the condition) such that average length of the shortest description of     does 
            not exceed H(£N) + 1 = NH(£) + 1.  Replacing this decompressor by an optimal 
            one, we increase the average length by 0(1).                                       □
                 236  Show that one can slightly improve the upper bound and prove that the 
            average value of monotone complexity KM(ÇN) does not exceed NH(£) + 0(1).
                 (Hint:  Apply Theorem 89 to the distribution of £°°.)
                We assumed that pi, ■ ■ ■ ,Pk are fixed rational numbers.  One may wish to get a 
            uniform bound that is true for all tuples pi,... ,pk-  Then we should add pi,... ,pk 
            to  the condition and prove bounds for the expected value of K(£N\N,pi,... ,pk) 
            instead of K(£n\N).  The lower bound is not  affected at  all,  since it  is true for 
            any prefix code, and for the code construction the information in the condition is 
            sufficient.  (We assume that pi are rational numbers.  This is not very important, 
            since one may replace arbitrary reals by their approximations with sufficiently small 
            error.)
                 237  Formulate the exact statement and prove it.
                This theorem says that  average  complexity equals entropy though individual 
            values of complexity could be much smaller or much larger.  In fact,  a stronger 
            statement it true:  most values of     have complexity close to NH(£).  More for­
            mally, the event  “the complexity of string ÇN differs significantly from NH(£)” has 
            small probability.  This statement could be considered as an algorithmic version of
                                         7.3.  COMPLEXITY AND  ENTROPY                              229
              the  Shannon theorem on  (noiseless)  channel capacity,  and we will return to this 
              question in Section 7.3.4.
                  7.3.3.       Prefixes of random sequences and their complexity.  In this sec­
             tion we consider infinite ML-random sequences and compare complexities of their 
             prefixes with the entropy of a generating distribution.  Again, let A be an alphabet 
             that has к letters, and let pi,... ,p/~ be a probability distribution on A.  We assume 
             that pi,... ,Pk are computable positive reals.
                  Consider the space A°° of infinite А-sequences and the probability distribution 
             on this space that corresponds to independent identically distributed variables with 
             distribution pi,... ,Pfc.  This is a computable probabilistic measure on A°°, so the 
             Martin-Löf definition of randomness can be used.  (In fact, we have defined Martin- 
             Löf randomness for a two-letter alphabet,  but essentially the same definition can 
             be used for any finite alphabet.)
                  Theorem 148.  Let to  be an ML-random sequence with respect to this distribu­
             tion.  Let (co)n  be its prefix of length N.  Then
                                                lim           = tf,
             where H is the Shannon entropy, i.e., H — ^2pi(— logpi).
                   238 Prove that for uniform distribution this statement is an immediate con­
             sequence of the randomness criterion (Theorem 90, p.  146).
                  (It is a rare occasion when the uniform case is really special.)
                  The statement refers to the plain complexity C\ however, this is not important, 
             since different versions of complexity differ only by O(logiV) = o(N).  So we may 
             use monotone complexity in the proof, and this is convenient.
                  P r o o f.  The  Levin-Schnorr randomness criterion  (Theorem 90,  p.  146)  says 
             that complexity of a prefix of a random sequence is close to the negated logarithm 
             of probability that this prefix appears.  The probability refers to the distribution on 
             A°° considered above, and the negated logarithm equals N ^  qi( — logp*) where qi 
             is the frequency of ith letter in (cj)n -  It remains to use the SLLN, which guarantees 
             that qi tends to pi as N —> oo for a random sequence.                                   □
                  Looking at this proof, we see that the difference between the complexity (per 
             letter)  and  entropy  has  three  reasons:  first,  the  randomness  deficiency  from  the 
             Levin-Schnorr theorem that gives an 0(1)/N difference; second, the difference be­
             tween the plain and monotone complexities (of order 0(\ogN/N))-, and, finally, the 
             difference between frequencies and probabilities which makes the most important 
             term.  (The law of iterated logarithm says that this term typically is a bit larger 
             than 0{VN)/N.)
                  We have assumed that pi are computable reals, otherwise the notion of Martin- 
             Löf randomness cannot be used.  If they are not computable, we can still consider 
             the set of sequences such that complexity of their prefixes (per letter) do not have 
             entropy as a limit.  Then we can prove that this set has measure zero (with respect 
             to the corresponding distribution).
                  239  Prove this statement.
                  {Hint:  For an upper bound we can use some approximations forp*; the precision 
             1/N2  is enough if we consider prefixes of length N.  The additional information
                              7.  SHANNON  ENTROPY  AND  KOLMOGOROV COMPLEXITY
              230
              needed to specify these approximate values is of size О (log N).  The lower bound 
              does not use at all the algorithmic properties of pp, for example, we can get a bound 
              for relativized complexity with any oracle A that makes all Pi computable.)
                   7.3.4.  The complexity deviations.  Theorem 148 is asymptotic.  One may 
              look for a bound of difference between complexity and entropy of frequencies for 
              finite  sequences.   (This  follows  the  example  provided  by  the  probability  theory 
              that has the SLLN for the limit values as well as large deviation bounds for finite 
              sequences.)
                   Let A be a ^-letter alphabet, and let p\,... ,pk  be a distribution on A.  Again 
              we assume for simplicity that pi  are rational  (or at least computable).  Consider 
              the  product  distribution  on  AN  that  corresponds  to  N  independent  trials  with 
              probabilities p\, ... ,p^.  So each А-string of length N has certain probability (and 
              certain complexity).  We already know from Theorem 147 that the average value of 
              complexity is close to NH, where H — ^p*(—logp*).  But we want to know also 
              how far this complexity deviates from its average value.
                  The simplest case of two équiprobable letters (which is quite untypical, as we 
              shall see) gives a uniform distribution on all binary strings of length N.  We know 
              that all these strings have complexity at most N + 0(1) and the (overwhelming) 
              majority of strings has complexity close to  N:  The fraction of strings that  have 
              complexity less than N — c is at most 2~c.  So in this case the significant difference 
              between complexity and entropy has an exponentially small probability.
                  The case of uniform distribution on a ^-letter alphabet is similar.  However, if 
              not all the letters have the same probability, the situation changes significantly.
                  Here is the key observation.  For any string x of length N we compare proba­
              bilities pi with empirical frequencies qi(x) (frequencies of letters in x).  It turns out 
              that with high probability the complexity of a random string (with respect to our 
              distribution on AN)  is close to k(x)  = N                   logpi).  Indeed,  Theorem 89
              (p.  144)  says  that  monotone  complexity  can  exceed  k(x)  by  at  most  0(1).  On 
              the other hand, the argument used in the proof of Levin-Schnorr theorem (p. 146, 
              Lemma 1) shows that for any c the probability of the event  KM(x)  <  k(x) — c 
              (according to the distribution considered) does not exceed 2~c.
                  Therefore, the question about complexity reduces to a question about the dis­
              tribution of empirical frequencies.  This question has been studied in probability 
              theory for centuries.  It is known (Moivre-Laplace theorem) that this distribution is 
              close to a normal (Gaussian) one:  the expectation of frequency equals the probabil­
              ity, and the variance is proportional to 1/N.  This is the main term, since it is much 
              larger than terms caused by the О (log A") difference between different complexity 
              versions and by using N as a condition.  This argument (made precise) gives us the 
              proof of the following statement:
                  Theorem  149.  Let £  be  a random variable  with к  values.  For each positive 
              £ > 0 there exists c such that for all N the probability of the event
                                     NH{£) -  cy/N < C(x) < NH{£) + cVN 
              is  at least 1 — e.  (Here x  is a string formed by N independent copies of £.)
                  In fact our arguments assumed that pi are computable.  However, this assump­
              tion can be dropped if we replace pi by their approximations with sufficiently small 
              error (the precision 1/A2 is enough and requires only 0(logN) additional bits).
                                         7.3.  COMPLEXITY AND  ENTROPY                            231
                  7.3.5.  Shannon coding theorem.  The theorem of the last section is a natu­
             ral translation of classical Shannon results into complexity language.  These results 
             deal with the length of a code that allows us to transmit iV-letter blocks correctly 
             with high probability (according to the given distribution).
                  Let f be (again) a random variable with к values (letters of A) and some fixed 
             distribution.  Let N be a positive integer.  By     we denote a random variable with 
             range AN that is formed by N independent copies of £.  We want to encode values 
             of £n by m-bit strings (see Figure 22).
                               : N                                               -N
                               Figure 22.  Using m bits for transmission of
                  Here  an  encoder  is  any  mapping  of type  AN  —>  Bm,  and  a  decoder  is  any 
             mapping of type B'm  —y  AN.  A given value of ÇN  causes  an  error  if the  input 
             and  output  А-strings  (of length N)  differ.  The  probability of error  is  measured 
             according to the distribution of ÇN.  The question is:  What conditions on m and N 
             guarantee the existence of an encoder/decoder pair that has small error probability? 
             First, let us make the following evident remark:
                  Theorem  150.  For given N,m  and e > 0,  the code with error probability at 
             most £ exists if and only if the 2m  most probable values of£N  have total probability 
             at least 1 — e.
                  P r o o f.  Indeed, when m bits are used for encoding, one may transmit (without 
             errors) at most 2m values.  To minimize the error probability, we should choose 2m 
             most probable values.                                                                 □
                 In the next theorem the alphabet A and the random variable f are fixed.
                 Theorem 151.  For each e > 0 there exists a constant c such that:
                  (a)  The values of£N  can be encoded/decoded with NH(^) + c\/N bits with error 
             probability at most e;
                  (b)  Any code for ÇN  of length at most NH(£) — cy/N has error probability at 
             least 1 — e (i.eth e probability of correct decoding is at most e).
                 PROOF,  (a) As we know, for a suitable c the value of random variable fN  has 
             complexity less than m — NH(£) + cy/~N with probability at least 1 — e.  So for these 
             values one can use shortest descriptions (see the definition of plain complexity) as 
             codes.  (Formally speaking, we get strings not of length m, but of length less than m, 
             but there are at most 2m of them, and they can be replaced by strings of length m.)
                 Note that coding is not performed by an algorithm, but the theorem (as stated) 
             does not say anything about that, it claims the existence of a code mapping.
                  (b)  Here we  need  to  use  some  trick.  If there  exists  a code  of given  length, 
             then such a code can be constructed algorithmically using the previous theorem 
             (or just by an exhaustive search).  Then the decoding function for this code can be 
             considered as a conditional decompressor (where conditions are pi and N).  There­
             fore,  all  values  of f N  that  are  decoded  without  error,  have  complexity  at  most
         232      7.  SHANNON  ENTROPY AND KOLMOGOROV COMPLEXITY
         NH(£) — cVN + O(logiV)  (the latter term corresponds to the complexity of pa­
         rameters and can be omitted if we increase c).  As we know (Theorem 149, p. 230), 
         the probability of this event is at most e.            □
            240 As before, we assume that probabilities pi are known exactly, and if Pi 
         are not computable, we get some problems.  Correct the argument replacing pi by 
         their approximations with sufficient precision.
            241 Give a statement and proof for a similar result about conditional coding
         and conditional entropy.
            (Hint:  Assume that two dependent random variables £ and rj are given.  We 
         make n independent trials, the value of rjN  is known both to the sender and the 
         receiver, and the sender wants to send m bits in such a way that the receiver could 
         reconstruct the value of ÇN.  How large should m be?)
                                                              CHAPTER 8
                                                    Some applications
                                          8.1.  There are infinitely many primes
                      Let us start with a toy example and prove that there are infinitely many primes.
                      Assume that there are only m different prime numbers pi,...,pm,■  Then every 
                positive integer x has a prime decomposition of the form
                                                           X =PilP22 •••Pm*
                and  can  be  described  by  the  list  of exponents  k i,...,k m.  Each  of ki  does  not 
                exceed logx, since the base is at least 2, and has complexity at most O(loglogx) 
                (its binary representation has O(loglogx) bits).  Since m is fixed, i.e., m is the same 
                for different x’s,  the complexity of the tuple  (ki,k2,..., km)  is O(loglogx).  As x 
                can be obtained from that tuple, its complexity is O(loglogx).  But for a “random” 
                (incompressible) n-bit integer x the complexity is close to n and is not O(logn), as 
                this formula says (the logarithm of an n-bit number does not exceed n).  Euclid’s 
                theorem is proven.1
                      Is this a real application of Kolmogorov complexity or just cheating?  A skep­
                tical reader would say that we just retell, in terms of Kolmogorov complexity, the 
                following counting arguments.  If there are only m prime numbers, then there are 
                at most (log x)m different integers between 1 and x, since any integer in this range 
                is  determined by the m powers in its decomposition,  and each power is less than 
                logx.  This immediately leads to a contradiction, since x > (logx)m for large x.
                      This  is  indeed  true:  our  reasoning  using  Kolmogorov  complexity  is  a direct 
                translation  of this  argument  (and  is  a bit  more  cumbersome  due  to  asymptotic 
                notation).  However, such a translation may still have sense, since the new language 
                provides  new  intuition,  and  this  intuition  may  be  useful  even  if later  the  same 
                argument can be translated into the standard language.
                      We return to this discussion after looking at other applications.
                                         8.2.  Moving information along the tape
                      The other toy example  is  a well-known  result  saying that  duplication  of an 
                n-bit  string on  the  tape  of a Turing machine  (with one tape only)  requires en2
                      Hn [103] this argument continues as follows.  If N has a prime factor pn (the nth prime), then 
                we may encode AI as a pair (n, N/pn), so C(N) ^ K(n) + C(N/pn) + 0(l).  If N is incompressible, 
                then  logAr  ^  C(N)  ^  K{n) + log(AI/pn)  with  0(l)-precision,  so  logpn  ^  K(n) + 0(1).  This 
                gives an upper bound for pn  for infinitely many n; one should note only that  (as we have seen) 
                incompressible integers may have arbitrarily large factors.
                      The combinatorial translation of this argument goes as follows.  Let us choose some threshold 
                m, and consider all integers that have only prime factors below m.  These integers form a minority 
                among large integers, and all other integers have large prime factors.  This implies that every tail 
                of the series     1/p (inverse primes) is at least 1/2, so this series diverges.
                                                                     233
          234                      8.  SOME  APPLICATIONS
                           L                                 R
                                            b
                              Figure  23.  A buffer zone of size b
          steps in the worst case.  This classical result was obtained in 1960s using so-called 
          crossing sequences; our proof is just a translation of this argument into the language 
          of Kolmogorov complexity.  (We assume that the reader is familiar with the basic 
          notions related to Turing machines; see, e.g.,  [184]).
              Consider a zone of size b on a tape of a one-tape Turing machine;  this zone 
          is  considered a buffer,  and we want to transmit information across this zone, say, 
          from left (L) to right (R); see Figure 23.
              Initially the buffer zone and R are empty (filled with blanks), and L is arbitrary. 
          We give an upper bound for the complexity of R after t steps.  The upper bound 
          is  (t \ogm)/b + O(logt) where m is the number of states that our Turing machine 
          has and b is the width of the buffer zone.  Informally the argument is quite simple: 
          each state of the Turing machine carries logm bits of information, and during one 
          computation step this information can be moved to the neighbor cell, so moving it 
          at the distance b requires b times more time.
              Now we have to convert this intuitive explanation into a formal argument.
              Theorem  152.  Let M  be  a  Turing machine  that has m  states.  Then  there 
          exists  a  constant c  such  that for any b  and for any  computation that starts  with 
          an  empty  buffer zone  of size b  and an  empty  tape  on  the  right  of the  buffer zone 
          the  complexity  of the  contents  R(t)  of the  right part  of the  tape  after t  steps  of 
          computation does not exceed
                                    t logm + 4 logt + с.
                                       Г
              P roof.  Let  us  consider  some  line  between cells  inside  the  buffer  zone  as  a 
          border, and let us write down the state of M when it crosses the border from left 
          to right  (as was done in the time of the Iron Curtain).  The sequence of states is 
          called the  crossing sequence.  Knowing the crossing sequence, we can reconstruct 
          the behavior of M  abroad  (on the right of the border)  not using the contents of 
          the tape on the left.  Indeed, we should artificially put the machine into the first 
          state of the crossing sequence and let it go abroad.  When M returns back, we put 
          it  in the second state of the crossing sequence and let it go abroad again.  In this 
          way we correctly reconstruct the abroad behavior of the machine (since it does not 
          remember anything except its state when crossing the border).  In particular,  at 
          some moment t' the tape on the right of the buffer zone contains R(t).  Note that 
          t! may be different from t since we do not take into account the time M spends on 
          the left  of the border,  but  t'  cannot  exceed t.  Therefore,  to reconstruct  R(t)  we 
          need to know the crossing sequence, t',  and the distance between the border and 
          iü-zone.  So there exists a constant  c  (depending on M but not on b and t)  such 
          that for any crossing sequence S and any b and t we have
                              C(R(t)) ^ l(S) logm + 41ogt + c.
                               8.2.  MOVING  INFORMATION  ALONG  THE TAPE                235
                                    X
                                            и                             и
                                    n                    n/2
                                Figure 24.  Buffer zone for duplication
            Here we multiply the length  l(S)  of the crossing sequence by logm since S  is  a 
            string in an m-letter alphabet and each letter carries logm bits.  To add b' and t' 
            in a self-delimiting encoding, we need at most 2log 6 + 2logt bits.  We may assume 
            that t > 6, otherwise R(t) is empty since the head never visited R.  The constant c 
            appears when we switch to the optimal decompressor.
                This inequality is true for any contents of L and for any placement of the border. 
            Now if for the given contents of L we consider the shortest crossing sequence, the 
            length of this sequence is less than t/b (there is 6+1 possible positions of the border, 
            and at each step only one of the positions is crossed, so the sum of the lengths of 
            crossing sequences does not exceed t).  In this way we get the inequality stated by 
            the theorem.                                                                 □
                 242 Show that this bound can be improved by replacing 6 in the denominator
            by 26.
                (Hint:  The return trips need almost the same time (the difference is at most
            6).)
                The quadratic lower bound for the duplication of an n-bit string immediately 
            follows.
                Assume that a one-tape Turing machine M duplicates its input:  If initially the 
            tape contains a binary string x (followed by blanks), at the end of the computation 
            the tape has a second copy of x (i.e., it contains xx).
                Theorem 153.  There exists a constant e > 0 such that for every n there exists 
            an n-bit string that requires at least en2  steps to duplicate it.
                P r o o f.  For  simplicity  let  us  assume  that  n  is  even,  and  let  x  be  a  string 
            whose second half и has complexity close to its length  (i.e., to nf2).  Then apply 
            the inequality we have proven considering the zone of size nj2 on the right of x as 
            the buffer (Figure 24).
                Assume that duplication takes t steps.  Then the complexity of R zone after t 
            steps  (which is at least n/2) does not exceed t logm/6 + 4 logt + c, where 6 is the 
            size of the buffer zone, i.e., n/2.  Therefore,
                                        n   t log m 
                                        2                g
            We may assume without loss of generality that t < n2  (otherwise the statement is 
            trivial).  Then we replace 4logt by 81ogn and conclude that
                                           4 log m — O(nlogn);
             236                             8.  SOME  APPLICATIONS
             the second term is small compared to the first one for large n (we may then formally 
             extend the result to every n by decreasing the coefficient e).                       □
                 Is Kolmogorov complexity essential in this proof?  The skeptical observer may 
             say again that we in fact just counted the number of different strings that can be 
             copied in a limited time  (using the fact that different  strings should have differ­
             ent  crossing sequences,  otherwise the behavior of the machine at the right of the 
             boundary would be identical).  Indeed,  the original proof follows this scheme  (in 
             fact,  it  deals with palindrome recognition,  not the duplication,  but the technique 
             is the same).  Does the language of complexity make the proof more intuitive and 
             easy to understand?  Probably this is a matter of taste.
                 Many bounds in computational complexity theory can be proven in the same 
             way, using the string of maximal complexity as the worst case and proving that the 
             violation of the bound would imply this string is compressible.  Many applications 
             of this type  (and further references)  are given in the classical textbook  [103];  its 
             authors,  Ming Li and Paul Vitânyi,  played an important role in development of 
             this approach,  called the incompressibility method.  Note that in several cases the 
             historically first proof was obtained using Kolmogorov complexity.
                 In the next section we consider one more application of the incompressibility 
             method, then we switch to other applications.  The most interesting thing in these 
             applications is not the statements in themselves but the various methods that allow 
             us to apply Kolmogorov complexity to prove statements that do not mention it»
                                8.3.  Finite automata with several heads
                 A finite  automaton  with  к  heads  is  similar  to  the  ordinary  one  (we  assume 
             that  the reader  is  acquainted with  basic  notions  related to  finite  automata;  see, 
             e.g.,  [185]),  but  it  has к one-way read-only heads.  Here  one-way means that the 
             head can only move from left to right.
                 Initially all к heads observe the leftmost character of the input string.  At each 
             step the behavior of the automaton is determined by its state and the к symbols 
             it  observes  (under к heads):  the automaton changes the state and instructs some 
             heads (at least one) to move to the right.  Then the automaton performs the next 
             step, etc.
                 The input string is followed by a special marker; the automaton terminates if all 
             the heads observe this marker.  (We assume that the head that sees the marker does 
             not move to the right.)  Automaton  accepts the string if it gets into an  accepting 
             state after processing this string.  We say that the automaton recognizes the set of 
             all accepted strings.
                 Example.  Consider the language (=set of strings) x#x where x is any binary 
             string.  It  is  well  known  that  this  language  cannot  be  recognized by  a standard 
             (one-head) automaton.  However, it is easily recognized by a two-head automaton. 
             Indeed, we should send one head to look for the separator #, when the separator is 
             found, two heads move synchronously and check that they read the same symbol.
                 So two heads are better than one (more languages can be recognized).  It turns 
             out that the same is true for more heads:  к + 1 heads are (strictly) better than к 
             heads.
                 Theorem 154.  For every к there exists a language that can be recognized by a 
             (k + 1 )-head automaton but not by a к-head one.
                             8.3.  FINITE  AUTOMATA WITH SEVERAL HEADS               237
               Proof.  For each m ^ 1, consider the language Lm that consists of all strings
                                      Wl# •             ■■■Wi
            (for  any  binary strings w\,..., wm).  Each wt  is repeated twice,  and in the right 
           half the strings Wi go in reverse order (this is crucial for the argument).
               A /с-head automaton can recognize this language if m is not very large (is at 
           most  (2),  see below).  One of the heads goes to the right half,  and the remaining 
           к — 1 heads are placed before w i,..., Wk-i■  Then each of these к — 1 heads checks 
           its string while the first head crosses its copy.  After that the first к — 1 strings are 
           checked,  the first head is of no use  (it is at the end of the input string),  but the 
           remaining к —I heads are useful since they are on the left of the remaining strings 
           Wk,Wk+1,....  Now we repeat the same trick:  one of the к — 1 heads is sent across 
           the right half, к — 2 heads check the next к — 2 strings, etc.  Repeating this, we can 
           check
                            (*_1) + (t_ 2) + ... + 1 = *(*^) = (*)
           strings.  (Note that m is fixed, so for finding a substring with a given number, the 
           finite memory is enough.)
               Therefore, the language Lm can be recognized by a к-head automaton ifm ^ (2).
               It  remains to show that if m >  (2),  the  language Lm  cannot be recognized by 
           a  к-head  automaton.  Assume that is  not  the case  and some  fc-head  automaton 
           M recognizes this language.  To get a contradiction,  let us consider independent 
           random strings W\,..., wm of sufficiently large length N.  More formally, consider 
           a string of length mN and complexity at least mN and split it into m strings of 
           length N denoted by wi,..., wm.  By assumption, the string
                                    W = W 1# ■ • ■ WmftWm# ---Wi
           is accepted by M; we get a contradiction by showing that either w\ • • • wm is com­
           pressible or the automaton does not recognize Lm.
               Let us say that a given pair of heads of M visited Wi if at some moment (while 
           processing W by M) these heads were simultaneously inside two copies of wi.  A key 
           observation:  a given pair of heads cannot visit both Wi  and Wj for i ^ j.  Indeed, 
           consider the moment when Wi was visited.  After that the left head reads only Wj 
           with j > i and the right head visits only Wj with j < i.
               By our assumption m > (2) ; therefore there exists i such that Wi is not visited 
           by any pair of heads.  Let us show that either Wi is compressible or one of its copies 
           can be counterfeited in such a way that M will still accept the string (so M does 
           not work correctly).
               Let us observe the actions of M on W.  Special attention is needed when one 
           of the heads enters or leaves Wi  (any of two copies):  We write down the positions 
           of all heads and the state of M at these moments.  The obtained  “log-file”  P has 
           complexity O(logA^) where the hidden constant depends on k, m, and the number 
           of states in M but not on N.  Indeed, there are at most 4к moments to consider 
           (four per head) and at each moment we record the state of the automaton and head 
           positions, and this requires O(logA^) bits.
               Let us show that (if M recognizes Lm correctly) the string Wi can be uniquely 
           reconstructed if all other Wj  (with j /  i) and P are given.  This would imply that 
           the complexity of the string w\ ■ ■ ■wm does not exceed  (m — 1)A^ (the number of
          238                      8.  SOME  APPLICATIONS
          bits in Wj for j ^  i) plus O(logiV) (the complexity of the log-file) plus 0(1), which 
          is less than m,N for large N, so we get the desired contradiction.
              The reconstruction goes as follows:  we try all strings of length m as candidates 
          for  Wi  (keeping Wj  with j  ф  i  intact).  For  each  candidate w  we  run  M  on  the 
          resulting string and check whether we get the same protocol P.  There are three 
          possible cases:
              (1)  If (for some w)  M rejects  (does not accept)  the string,  then M does not 
          recognize our language.
              (2) M accepts all these strings (for all candidates) and the protocol P appears 
          only once, for w = W{.  Then reconstruction is possible (and W\ • • ■ wm is compress­
          ible).
              (3) M accepts all these strings, and P appears both for Wi and for some w ф Wj,. 
          Let us show that in this case M accepts a string not in Lm, namely, the string W  
          that has Wj, in the left half while in the right half Wj, is replaced by w.
              Indeed, there are two accepting computations of M :  one if Wj, is used on both 
          sides and the other one for w.  Let us split both of them into parts;  the splitting 
          points are moments when one of the heads enters or leaves Wj, (or w).  The positions 
          of all other heads and the states of M are recorded in P so they are the same for 
          both computations.  (Note that the moments of time can be different since they 
          are not recorded.  In fact,  we may add them also,  but this is not needed.)  So we 
          can glue the computation intervals for both cases; let us show that we can get an 
          accepting computation of M on a bad string (the left half has Wi while the right 
          half has w).
              By our  assumption  during  the  processing  of  W,  there  is  no  moment  when 
          both copies of Wj, carry some heads; since the border crossings for both copies are 
          recorded  in  P,  the same is  true when Wi  is  replaced  by  w.  So  for  each interval 
          between two protocol events related to Wj/w there are three possibilities:  (a) there 
          is  a head in the ith string on the left;  (b)  there is a head in the ith string on the 
          right;  (c)  neither of the  above.  Then we can copy-and-paste the intervals into a 
          new computation:  for part (a) we use the computation of M on W; for part (b) we 
          use the computation of M of changed input  (where Wi is replaced by w); for part
          (c)  we can use either of two (they are the same).  Then we get a computation of M 
          on a mixed string W', so M does not work properly.                 □
                               8.4.  Laws of Large Numbers
              The Strong Law of Large Numbers was proven in Section 3.2  (Theorem 27, 
          p.  56) without any references to Kolmogorov complexity by straightforward count­
          ing.  We consider (mainly) the uniform case.  In this case the SLLN says that the 
          set of all sequences cj = cno^i • • •, such that the sequence
                                      CUo + Wi + • • • + UJn- 1
                                 Pn —         П
          has limit  1/2 as n tends to infinity, has full measure (with respect to the uniform 
          Bernoulli measure on Q).  In other words, the SLLN says that the complement of 
          this set  (i.e., the set of sequences cj such that pn either have no limit or have limit 
          not  equal to  1/2)  is  a null  set.  Later  (Theorem  32,  p.  65)  we  have  shown  that 
          this null set is in fact an effectively null set; this implies that for every ML-random 
          (with respect to the uniform measure) sequence cj the sequence pn converges to 1/2 
          (Theorem 33, p. 65).
                                       8.4.  LAWS  OF  LARGE  NUMBERS                        239
                 However, we can go in the other direction.  Namely,  we may first prove that 
             for any ML-random sequence the frequencies converge to 1/2 using the randomness 
             criterion  in  terms  of complexity  (Theorem  90,  p.  146).  This  criterion  says  that 
             for an ML-random (with respect to the uniform Bernoulli measure) sequence u> the 
             monotone complexity of its prefix (cj)n of length n is n+0( 1).  This property implies 
             that the frequency of ones in (ui)n (i.e., pn) converges to 1/2.  Indeed, Theorem 146 
             (p.  226) says that the complexity of u>n does not exceed nh(pn, 1 — pn) + O(logn), 
            so  h(pn, 1 — pn)  —  1 + 0(\ogn/n)  for any ML-random sequence.  (Note that the 
            difference between plain and prefix complexity of u>n  is O(logn),  so any of them 
            can be used.)  This implies that pn —> 1/2 as n —> oo (see the graph of the entropy 
            function, Figure 8, p. 57).  So the SLLN is true for all ML-random sequences, which 
            form a set of full measure.
                 The skeptical observer would say that this is not a different proof, or we have 
            just  repeated the same arguments using different language.  And she is probably 
            right.  If we recall the proof of Theorem 146, we see that it uses the same estimate 
             (based on Stirling’s approximation) that was used for the proof of SLLN. (Another 
            argument, where monotone complexity is bounded by a negative logarithm of the 
            measure, Theorem 89, also has a direct translation in the probabilistic language; it 
            was discussed in Section 3.2 after the proof of Theorem 27 on p. 56.)
                 So what do we get by using complexity language?  First, we find a broader class 
            of sequences that satisfy the SLLN:
                 Theorem 155.  Letu> be a binary sequence such that C((u>)n) = n + o(n).  Then 
             the sequence pn  (the frequency of ones in (cu)n)  converges to 1/2.
                 P r o o f.  The proof remains essentially unchanged:  in this case h(pn, 1 —pn) is
            still  1 + o(l).                                                                  □
                Second,  we  not  only  can  prove  that  pn  —>  1/2  but  we  also  can  give  some 
            estimates for the convergence speed.  The corresponding result in probability theory 
            is known as the Law of the Iterated Logarithm, and V. Vovk [208] has shown that it 
            is valid for ML-random sequences.  Following his argument, let us use Kolmogorov 
            complexity to give a (rather simple) proof of the upper bound provided by this law.
                Theorem  156.  Let u>  be an ML-random sequence with respect to the uniform 
            measure.  Let pn  be  the frequency  of ones  in  (cj)n.  Then for  every e  >  0,  the 
            inequality
            holds for any sufficiently large n.
                P r o o f.  Let  us  first  look  at  what  bound  can  be  obtained  by  the  argument 
            above (that uses Kolmogorov complexity).  We know that
                            n -  0(1) < KM((u)n) ^ nh(pn, 1 ~Pn) + O(logn)
            therefore
                                      h(pn, 1 -  pn) > 1 -  0(\ogn/n).
            The function
                            p    h(p, 1 -  p) = p (-logp) + (1 -  p )(-log(l -  p))
              240                                8.  SOME  APPLICATIONS
              has maximum at p = 1/2, and the second derivative at this point is non-zero (equals 
               —4/In 2).  Therefore, the Taylor expansion gives us
                                            h{ 1/2 + <5) = 1 -          + o(62)
              as Ô —> 0, and for 6n = pn — 1/2 we have
                                                     Si = 0(logn/n),
              So we get at least something, though the bound we need is much stronger.  (Let us 
              mention that in the probability theory the final bound was obtained in many steps. 
              First Hausdorff (1913)  proved the bound 0{n£ / л/п)\  then Hardy and Littlewood 
              (1914)  improved it  and replaced n£  by  ^/logn;  then Steinhaus  (1922)  came with 
              the bound (1 + e) y/(2 In n)/n, and only later Khinchin (1924) got the final result. 
              So we are now on the level of Hardy and Littlewood in this respect—not that bad.)
                   Let  us think about  possible improvements for the upper bound that we had 
              for KM((oj)n).  This upper bound was obtained by comparing KM((oj)n) and the 
              negative logarithm of the probability of the prefix (cu)n with respect to the Bernoulli 
              measure with parameter pn.  This  logarithm  is  exactly  nh(pn, 1 — pn),  but  the 
              Bernoulli measure used for comparison depends on n, so the construction used in 
              the  proof of Theorem 89  needs an additional term that  is K(pn)  (we start  with 
              a self-delimiting encoding of pn).  Here K(pn) does not exceed (2 + e) log n, since 
              both numerator and denominator of the fraction pn do not exceed n.  Altogether 
              we get the bound 
                                 2
                               — (pn -  1/2)2 « 1 -  h(pn, 1 — Pn) < (2 + e) logn/n,
              which is still not good enough.
                   What else can we do?  Note that we already know that pn is rather close to 1/2: 
              with denominator n the numerator differs from n/2 by л/п or a bit more.  So (when 
              the denominator n is known) we can use fewer bits to describe the numerator, and 
              this allows us to replace 2 by 1.5 in the right-hand side.  But this is still not enough 
              for us.
                   The crucial idea is to use approximations for pn instead of the exact values.  Let 
              us assume that pn = 1/2 + Sn > 1/2 (the case when pn < 1/2 is similar).  Instead of 
              pn we use (while constructing the Bernoulli measure used to get the upper bound for 
              complexity) its approximation 1/2 + <5^ where 6'n is an approximation to Sn from 
              below with a small  (fixed)  relative error.  For example,  let  us take  S'n  such  that 
              0,9<5n < S'n  < Sn.  Such a 6'n can be founded among the geometric sequence (0,9)k, 
              and its complexity is about log A:, i.e., about log(— log Sn/log 0,9) = log(— log<5n)+c. 
              Note that if 6n < lf^/n, then we have nothing to prove, so the complexity of 6'n can 
              be upper bounded by (1+e) log log n (for every e this bound holds for all sufficiently 
              large n).
                   This is good news; the bad news is that we have a more complicated bound for 
              the complexity of (ш)п.  Now instead of h(pn, 1 — pn) we have
              (*)                       Pn[~ bgp'n] + (1 - Pn)[~ log(l - p'n)\,
                            8.5.  FORBIDDEN  SUBSTRINGS         241
         where p'n  =  1/2 + S'n.  Recalling  our  discussion  of entropy,  we  may  say  that  a 
         sequence (cu)n where frequencies of zeros and ones are pn and 1 — pn is encoded by 
         a code adapted to the simplified frequencies p'n and 1 — p'n.  The expression (*) can 
         only increase if we replace pn by p'n:  since pn > p'n > 1/2, the second expression in 
         square brackets is greater than the first one, and increasing its weight by decreasing 
         pn, we increase the entire expression (*).
            Finally we get the bound
                      n -  0(1) < nh(pn,l-p 'n) + (1 +£)loglogn
         for every e > 0 (the inequality holds for all sufficiently large n).  As before, it implies
                          ô'n  <  (1 + е)л/\п2 • loglogn/2n.
         For a  true  Sn  we get  a slightly  bigger bound  (1/0.9  times  bigger);  since 0.9  can 
         be replaced by an arbitrary number less than 1, we get the desired statement (the 
         factor In 2  is  used  to  convert  the  binary  logarithm  to  the natural one,  while the 
         replacement of the second binary logarithm by the natural one can be compensated 
         by a change of £ in the factor (1 + e)).               □
            243    Show that this argument can be used to prove the statement of Theo­
         rem 156 not only for ML-random sequences but also for arbitrary sequence oj such 
         that n — KM((u))n) = o(loglogn).
                           8.5.  Forbidden substrings
           8.5.1.  Forbidden and simple substrings.  The statement we prove in this 
         section is an example of a non-trivial application of Kolmogorov complexity (that 
         cannot be directly translated into a counting argument).
           Theorem  157.  Let a <  1  be a positive real number.  Assume that for each n 
         some binary strings are called forbidden strings and there are at most 2an forbidden 
         strings for any length n.  Then there exists some c and an infimité sequence of zeros 
         and ones that does not have forbidden substrings of length c or more.
           For  example,  we can declare strings  of length n  and  (plain)  complexity less 
         than an as forbidden strings.  Then we get the following corollary (called Levin’s 
         lemma, see [51]):
           Theorem 158.  Let a < 1 be a positive real number.  There exists an infinite se­
         quence of zeros and ones such that any of its substrings of sufficiently large length n 
         has complexity at least an.
           It  is  instructive to compare this statement with the randomness criterion for 
         the uniform measure  (Theorem 94,  p.  151).  In this criterion we considered only 
         the prefixes of the sequence (instead of all substrings); on the other hand the lower 
         bound for complexity was n — 0(1)  instead of a weaker bound an that we have 
         now.  (The bound n — 0(1) was for monotone complexity; it implies the n — O(logn) 
         bound for plain complexity that we use now).  The following problem shows that 
         such a strong bound cannot be true for all the substrings  (not a surprise, since a 
         truly random sequence contains all substrings, including simple ones).
            244    For each infinite sequence u) of zeros and ones there exist a < 1 and infin­
         itely many substrings that have complexity per letter (the ratio complexity/length) 
         at most a.
             242                            8.  SOME  APPLICATIONS
                 (Hint:  Consider two cases:  If all binary strings appear as substrings, the claim 
             is  evident.  If ш  does  not  contain  some  string  и  of length  к,  we  can  split  long 
             substrings into blocks of length к and use efficient coding that takes into account 
             that block и is never used and does not need a code; this gives complexity per letter 
             at most (log(2fc — 1 ))/k.)
                 The proof of Theorem 157 consists of two steps.  First we prove its special case, 
             Theorem 158.  Then it turns out (surprisingly) that the general case follows from 
             this special one.
                 PROOF.  To  prove Theorem  158,  let  us consider an intermediate ß such that 
             a < ß <  1.  Using Theorem 71  (p.  Ill), we find a number N with the following 
             property:  to each string x we can append N bits (on the right) in such a way that 
             the prefix complexity of the string increases at least by ßN.
                 Let us use this property iteratively starting from the empty string.  We get an 
             infinite sequence of iV-bit  blocks;  the prefix complexity increases at  least  by ßN 
             when the next block is appended.  This implies that the complexity of every group 
             of consecutive к blocks is at least ßkN — 0(1).  Indeed, by appending this group, we 
             increase complexity by ßkN at least, but the inequality K(xy) ^ K(x)+K(y)+0( 1) 
             shows that K(y) ^ K(xy) -  K(x) -  0(1).
                 This implies that for every substring и (not necessarily block aligned) the com­
             plexity of и is at least ßl(u) — 0(1) since the change in complexity and length due 
             to boundary effects (by cutting the incomplete block on the border) is 0(1).  It re­
             mains to note that we have some reserve due to the difference between a and /3, and 
             this reserve is enough to compensate both the boundary effects and the difference 
             between plain and prefix complexities (for sufficiently long substrings).           □
                  245  Give  a similar  argument  that  uses  plain  complexity  instead  of prefix
             complexity.
                 (Hint:  Use Problem 46, p. 42.'
                  246  Prove the statement of Problem 47 (p. 42) with prefix complexity instead
             of plain complexity.
                 PROOF.  Now  let us prove Theorem 157;  the simplest  approach is to use the 
             relativized version of complexity.  Let us consider the set F of forbidden strings as 
             an oracle;  this means that we consider algorithms that can ask (for free) whether 
             a given string is forbidden or not.  As usual, this relativization goes smoothly both 
             in  the  statement  of Theorem  158  and  its proof,  and  this  theorem  is true  for  F- 
             relativized complexity.
                 Now all forbidden strings of length n have F-complexity at most an + 0(\ogn), 
             since each forbidden string can be determined by n and by its ordinal number in the 
             list of all forbidden strings of length n.  In fact the stronger bound an + 0(1) is valid 
             since we can use the list of all forbidden strings in the order of increasing length, 
             but this does not matter much since a small change in a covers this difference.  So 
             it  remains to apply Theorem 158 and to get a sequence which does not have long 
            substrings with complexity less than ß (per letter), for some ß > a.                □
                 One can also make the following (rather unexpected) observation [160]:  The­
            orem  157 can be derived from Theorem  158 directly,  without  any relativization, 
             using the following statement:
                            8.5.  FORBIDDEN  SUBSTRINGS         243
            Theorem  159.  If for some  rational a  and some  set F  of forbidden  strings 
         the statement of Theorem  157  is false  (F  has less  than 2an forbidden strings for 
         each n,  but there is no  infinite  sequence without long forbidden strings),  then the 
         same happens for a decidable set F.
            (Note that for a decidable F the relativization does not change anything; the 
         restriction to rational a is also not important, since we can increase a to a greater 
         rational number.)
           P r o o f.  Assume that for some a < 1 and some set F the statement of Theo­
         rem 157 is false.  Then for each c we may find a set Fc in such a way that:
            (a) Fc contains only strings of length greater than c;
            (b)  Fc contains at most 2ak strings of length к (for every k);
           (c)  each infinite sequence contains at least one substring that belongs to Fc.
           (Indeed,  we can let  Fc be the set of all strings in F that have length greater
         than c.)
           The standard argument (compactness, König’s lemma) shows that every suffi­
         ciently long string has at least one substring in Fc, so one can find finite Fc with the 
         same properties.  Moreover, such a finite set can be found by an exhaustive search, 
         so we get Fc that has these properties and can be found effectively when c is given. 
         (Why do we first need to switch to finite sets?  To make the search possible.)
           Now we construct the sequence c* such that q+i is greater than the lengths of 
         all strings in FCi.  The union of all FCi  is a decidable set that violates the statement 
         of Theorem 157.                                        □
           Note the structure of our arguments:  knowing that an object with some prop­
         erty exists, we perform an exhaustive search and effectively find a (perhaps different) 
         object with the same property.  This observation is often useful when dealing with 
         Kolmogorov complexity.
            247    Prove that  if for some set  F of strings there exists  a  (one-sided)  infi­
         nite sequence that does not contain substrings from F, there exists a bidirectional 
        sequence that does not contain substrings from F.
           (Hint:  The compactness argument shows that both properties are equivalent 
        to the existence of arbitrarily long finite sequences that do no contain substrings 
         from F.)
           J.  Miller  [125]  suggested  a direct  proof of Theorem  157,  where the required 
        sequence is constructed inductively, and we at each step guarantee that some quan­
        tity (the emergency level) is not very large.  Let us explain how the emergency level 
         is computed and why it can be kept bounded.
           Fix  a  set  of forbidden  strings  F.  The  emergency  level  for  a  string  x  (the 
         already  constructed  part  of the  sequence)  is  denoted  by  wc(x),  where  c  will  be 
        some constant slightly greater that 1/2.  The value of wc(x) is big if we have almost 
        got a forbidden substring.  The definition follows:  For every possible occurrence of 
        a forbidden string z e F in the possible extension of x (this means that 2 is on the 
        right of x, but there is a non-zero overlap, Figure 25), we take the number к of bits 
        of 2 that are missing in x, and add ck to wc(x).  In other words, wc(x) is the sum 
        of ck  for all 2  G  F and for all possible occurrences.
            244                           8.  SOME  APPLICATIONS
                                                        Z
                                                          к
                    Figure 25.  A possible occurrence of a forbidden string 2:  part of 
                    z is already in x, but к bits are missing.  This occurrence adds ck 
                    to wc(x).
                When c = 1/2, it is easy to explain the meaning of wc(x):  it is the expected 
            number of occurrences of forbidden strings that overlap x, assuming that x is ex­
            tended to the right by a sequence of independent random bits.  Having this inter­
            pretation in mind, it is easy to see that
                            wi/2 (x) = ±wl/2(x0) + ±w1/2(xl) -  ^ (l/2 )'(z).
                                                                zeF
            Indeed, we add 0 with probability 1/2 and add 1 with probability 1/2, and we need 
            to take into account that in this way we count also those occurrences that happen 
            immediately after x (and they should not be counted—the definition requires non­
            zero overlap with x).  In fact,  this equation is a purely combinatorial fact and is 
            valid for arbitrary c (assuming that x does not contain forbidden strings):
                                 wc(x) = cwc(x0) + cwc(x 1) — ^  c^z\
                                                              zeF
            Initially  (when x is empty) the value wc(x)  is zero,  and if x contains a forbidden 
            substring, then wc(x) is at least  1.  So it is enough to show that we can maintain 
            the invariant relation  “wc(x) < 1”  when adding the next bit.  It is enough to prove 
            that
                              wc(x0) + wc(xl) = - (wc(x) -I- ^  c'(z))  < 2
                                                  C          zÇF
            (assuming wc(x) < 1), and this does happen if 1 + J2zeF      < 2c.
                To finish the  proof of Theorem  157,  it  remains to make the sum  J2zeF °1^  
            finite by choosing c close enough to 1/2 (the value of c depends on a and becomes 
            closer to 1/2 as a approaches 1), and then to make this sum small by deleting the 
            strings of small lengths from F.
                I 248 I Using this argument, prove an effective version of Theorem 157.  If the set 
            F satisfies the conditions and is decidable, then there exists a computable sequence 
            that does not have short substrings from F.
                The result of the last problem can be extended to bi-infinite sequences (as noted 
            by K.  Makarychev):  One can also prove the existence of a computable bi-infinite 
            sequence with the same property.  (His argument follows Miller’s scheme,  but we 
            should define  “left”  and  “right”  emergency levels and look how they change when 
            several characters are added:  one of the levels should decrease significantly while 
            the other should not increase much.)  A more general argument is given in [159]; 
            it uses an effective version of Lovâsz local lemma (see the next section) and can be 
            generalized to multidimensional case.
                                           8.5.  FORBIDDEN  SUBSTRINGS                             245
                  8.5.2.      Lovâsz  local  lemma.  We  have  seen  how  a  statement  about  Kol­
             mogorov complexity (the existence of a sequence without simple substrings) may be 
             used to prove the combinatorial version of this result  (the existence of a sequence 
             without forbidden strings).  In this section we move in the opposite direction.  We 
             start  with  a combinatorial  statement  (namely,  the Lovâsz local lemma)  and use 
             it  to  prove statements about Kolmogorov complexity.  But first let us make some 
             general remarks.
                  Probabilistic  existence proofs.  To  prove that  there exists an object  satisfying 
             some conditions, one can consider a probability distribution on objects and compute 
             for each condition the probability that it is violated.  If these probabilities are very 
             small  and their sum  (over all conditions)  is less than  1,  the random object with 
             positive probability satisfies all the conditions, and the existence is proven.
                  In this argument we use the following (trivial) property:  If the probability of 
             an event Ai  is at most e*, then the probability of the union of the events A\,...An 
             is bounded by the sum of e*, and the probability of avoiding all these events is at 
             least
                                              1 — £\ — £2 — ' ' ' ~ £n-
                  When computing probabilities, we often count  “bad” elements in some class; if 
             the total number of bad elements is less than the cardinality of the class, there exist 
             “good”  elements.  This reasoning can also be translated to complexity language: 
             If there are only a few bad elements, then bad elements have small complexity, so 
             every random (incompressible) element of the class is good.
                  However, we cannot use arguments of this type to prove Theorem 157.  Indeed 
             the probability of finding a bad string in a given position is small  (2(a-1)n),  but 
             if there are many possible positions,  the sum of probabilities  exceeds  1.  (Recall 
             that we need to prove the existence of arbitrarily long strings without  forbidden 
             substrings.)  However, there is an important observation that can be used to save 
             the argument:  If two positions are disjoint (do not overlap), then the appearance of 
             a bad string in the first position and in the second position are independent events. 
             This is what the Lovâsz local lemma is about.
                  The case of independent events.  Let us consider first the case when all events 
             Ai are independent.  If the probability of Ai equals e*, then the probability of the 
             event  “none of Ai happens”  is equal to
                                         (1 -ei) • (1 - £ 2) ■ ••• • (1 - £ n)
             (and  it  is  also  greater  than  1  — eq  — £2  —  ■ ■ •,  as  guaranteed  by  the  Bernoulli 
             inequality).
                 So for any independent Ai  the probability of avoiding all Ai  is positive even if 
             the sum of £i  exceeds 1; the only thing we need is that each £i  is less than 1.
                 The Lovâsz local lemma deals with an intermediate situation when there are 
             many events (so our first observation does not help),  and not all events are inde­
             pendent (so our second observation does not help either).
                 Assume that n events A i,..., An are given, and for each i G  {1,..., n} some 
             set  N(i)  C  {1,... ,n}  is  fixed that  does not  contain i.  The elements of N(i)  are 
             called neighbors of i in the sequel.  (We do not require the neighborhood relation 
             to be symmetric, so a number may not be a neighbor of its neighbor.)
                 Assume that each event  Ai  is  independent  with all other events,  except  for 
             i  itself and г’s neighbors  (for simplicity we identify index i and event  Ai).  More
          246                      8.  SOME  APPLICATIONS
          precisely,  we  assume  that  Ai  is  independent  with  the  tuple  of all  non-neighbor 
          events (not only with each of them).  Then the following bound can be proven.
              Theorem 160 (Lovâsz local lemma (LL)).  Assume that for each i — 1,2,..., n, 
          a positive real Ei < 1  is fixed such that
                                  Pr[Ai]<£i  Yl  (! —
                                           j€N(i)
          for all i.  Then the probability of avoiding all the Ai  is at least
                               (1 -£i) • (1 - e 2) • ... • (1 - e n).
              So we get the same bound as in the case of independent events, but the condi­
          tions are stronger:  For each neighbor event j we need to add the factor (1 — £j)  in 
          the right-hand side of the assumption.
              PROOF.  The proof of LL is a bit strange:  All the steps are quite easy, but the 
          intuition behind them is rather unclear  (so it was probably difficult  to invent it, 
          and it is even quite difficult to reproduce it).  So we prepare ourselves by making 
          simple observations first.
              (a)  For every two events A and В we have
          Indeed, the conditional probability is Pr[AAß]/Pr[ß] and Pr[AAß] < Pr[A].  (As 
          usual, Л stands for “and”.)
              (b)  One can add some condition C to all the events in the previous inequality 
          (the relativization trick), and get
                                  Pr[A IВ Л C\ < Pr[A I C\ 
                                               Pr[F? IC] '
          This observation is used in the proof of Lovâsz local lemma for independent A and 
          C (in this case the numerator Pr[A | C] equals Pr[A]), and the denominator Pr[В \ C] 
          is not very small.
             Now we are prepared to prove LL by induction.  As often happens, we need a 
          stronger statement for induction purposes.  Let us prove the following statements 
          (here -i stands for the negation, or complement, of the event):
              (1) For every i and for every p,q,... that are not equal to i and to each other, 
          we have
                                Pr[Af I “iAp A “iAq Л • • • ]  < Ei.
              (2) For every two disjoint families of events i,j,... and p,q,... we have
                  Pr[->Ai Л ->Aj Л ■ • • I -iAp A ->Aq A ■ ■ ■ ] > (1 -  Ei) ■ (1 — Ej) ■ —
          Note that the first statement implies the second one for the case when the family 
          i,j,... consists of one event i:  If the probability of Ai  (with some condition) is at 
          most Ei, then the probability of its negation is at least (1 — £i).
             Moreover, this argument can be extended to the case when there is more than 
          one event in the family i,j,..
          Pr[~^Ai A ->Aj I ~>Ap A —>Aq A • • • ]
                          = Pr[-iAi I ~>Aj A —'Ap A —iAq A • • • ] • Pr[-iAj I —>Ap A —>Aq A • • • ];
                                      8.5.  FORBIDDEN  SUBSTRINGS                      247
            it remains to apply (1) to each factor.
                On the other hand,  the following argument derives (1)  from (2).  Let us split 
            the conditions  in  (1)  and  consider separately the events  inside  N(i)  and  outside 
            N(i).  (Here i is the number of the event in the left-hand side of (1).)  Let N and 
            F be the conjunctions of the negations of the events in these two groups (near and 
            far).  Then,  following the scheme explained above, we estimate the probability as 
            follows:
                                Pr[Ai\N A F]  < Pv[Aj\F\     Pr [A] 
                                                Pr[iV IF]   Pr[iV IF] ’
            We can use inequality (2) and conclude that the denominator in the last fraction is 
            at least the product of (1 — Et) for all t G iV, and it remains to recall the assumption 
            of LL where these factors (and, maybe, others) appear.  We assume here that there 
            are neighbor events among the conditions.  If not, the left-hand side in (1) equals 
            Pr[Aj]  (due to independence) and is bounded by E{.
                It remains to explain why the reductions of (1) to (2) and vice versa (which we 
            have described)  do not lead to a vicious circle.  Reducing  (1)  to  (2)  as explained 
            above,  we use  (2)  in  the situation where  the number of events  in  the  inequality 
            (on both sides of  “|”)  is smaller than in inequality  (1),  which we want  to prove. 
            (Indeed, the event A* disappears).  The other reduction, where we derive (2) from 
            (1), does not increase the total number of events in the inequality.        □
               Here is an example of a combinatorial problem where LL is useful:
                249  A finite tape is given where each cell may contain a number between  1
            and N.  For each borderline between neighbor cells some pairs of numbers (l.r) are 
            prohibited, in the sense that one should not put I on the left and r on the right of 
            this border.  Prove that if for each border the fraction of prohibited pairs (among 
            N2 pairs) is at most 4/27, then one can fill all cells satisfying all restrictions.
                (Hint:  For each border consider the event  “a prohibited pair appears”.  Each 
            event has at most two neighbors, and for Ei = 4 one can apply the LL.)
                250 Prove a similar  result  (even  with  slightly  better  parameters)  without 
            using the LL. If each set of forbidden pairs contains less than 1/4 of all pairs, then 
            one can satisfy all the restrictions.
                (Hint:  In each position more than half of candidates accept more than half right 
            neighbors, and more than half of candidates accept more than half left neighbors. 
            So there exists some candidate that accepts more than half of left neighbors and 
            more than half of right neighbors at the same time.  Starting with this number, one 
            may add numbers from the left to the right, using the fact that two sets containing 
            more than half of the elements always have a common element.)
               8.5.3.  Lovâsz lemma and forbidden strings.  Now let us use LL to prove 
           Theorem  157.  As  usual,  it  is  enough  to  prove  the  existence  of arbitrarily  long 
           strings without forbidden substrings (due to the standard compactness argument).
               So we fix some length and consider random strings of this length where all the 
           bits are independent and uniformly distributed.  A bad event happens when some 
           forbidden string appears  at some fixed position  (different positions give different 
           events).  For every n consecutive bits, the appearance of a forbidden string in this 
           position has probability 2^a-1^n.  Using the LL, we need to fix some number Ei for 
           each event A*, and for this event  (the appearance of a forbidden string in a given
            248                           8.  SOME  APPLICATIONS
            position of length n), we use 2^-1^n for some constant ß G  (a, 1).  Then we need 
            to check that for suitable ß the conditions of the LL are satisfied.
                Let I be the position of some event (the interval where we look for a forbidden 
            string).  The neighbor events happen at intervals J that overlap with I (all other 
            events are independent).  The bounds Si  depend on the lengths,  so we group all 
            possible intervals J according to their lengths.  There exist n + к — 1 intervals J of 
            length к that have a non-zero overlap with a given interval I of length n.  Each of 
            them adds a factor (1 —          on the right-hand side of the condition of the LL,
            and in total we get
                                           (1 — 2^~l^k)n+k~l
            Now we have to multiply these expressions for all k, starting with some N (if we 
            construct a sequence with forbidden substrings of length at least N).  So to apply 
            the LL, we need to prove the inequality
                                2(a-i)n ^ 2^~l)n .     (i _ 2('0-1)fc)n+/c_1. 
                                                   k^N
            (In fact, we included I while considering intervals of length к — n, though we were 
            not obliged to,  but this makes our task only more difficult.)  Now we use a quite 
            rough bound:  we replace n + к — 1 by nk, take nth roots,  and use the Bernoulli 
            inequality.  It remains to prove that
                                        2a-ß ^ 1 - ^ 2  k2iß~1)k. 
                                                    k^N
            The infinite series ^2k k2^ß~l^k converges when ß < 1, and the left-hand side is less 
            than 1 for a < ß, so the inequality is true if N is large enough.
                Let us repeat what we are doing.  First, we take arbitrary ß G (a, 1) and then 
            choose a suitable N that makes the tail of the series small.  Then we apply the 
            LL to an arbitrarily large finite length and show that there exists a string of that 
            length which does not have forbidden strings of length N or more.  (Our bounds 
            work for arbitrary lengths.)  Finally, we use the compactness argument to get an 
            infinite sequence.
                 251  Prove a two-dimensional version of Theorem 157:  one can fill an infinite 
            cell  paper by zeros  and ones in such a way that  every rectangle of large enough 
            area is not forbidden.  (We assume that for every rectangle of area k, at most 2ak 
            forbidden combinations of zeros and ones inside this rectangle are fixed, for some 
            constant a < 1.)
                (Hint:  Similar bounds can be proven, and the LL can be used.)
                8.5.4.  Forbidden subsequences.  In the previous section we considered for­
            bidden substrings, i.e., forbidden combinations of consecutive bits.  But why should 
            the bits be consecutive?  This looks artificial, and we may consider a more general 
            setting, as in [158].  Assume that we have a countable family of Boolean variables 
            and some restrictions;  each of them forbids some combination of values for some 
            variables.  We are interested in a result of the following type:  If the restrictions are 
            not too numerous,  there exists an assignment of Boolean values to all the variables 
            that satisfies all the restrictions.  (In this kind of result, we do not care about exact 
            combinations of values that are forbidden; the only thing we use is that there are 
            not too many restrictions.)
                                      8.5.  FORBIDDEN  SUBSTRINGS                       249
                In other words, we want to prove the satisfiability of a formula in a conjunctive 
            normal  form  (CNF),  i.e.,  a  conjunction  of several  clauses.  For  example,  in  the 
            formula
                                    (->a V b V с) Л (a V с V ->d) A • • ■
            the first clause (~>a V b V c)  forbids the combination of values a = 1,  6 = 0, c = 0. 
            Our goal is to find a satisfying assignment,  a combination of values that satisfies 
            all the requirements.
                Assuming that all variables are independent, we observe that disjoint clauses 
            (that do not have common variables) are independent.  So, if we want to apply the 
            LL, we have to bound the number of clauses that contain a given variable.
                Let us fix some notation.  Let lj — cjocjicj2 • • •  be an infinite sequence of bits. 
            For a finite set F C N, we denote by w(F) a string composed of    for i  e F  (in 
            order of increasing i).  Consider a pair (F, X) where F is a finite set of indices and 
            X is a binary string whose length is equal to the cardinality of F.  We say that a 
            sequence w is forbidden by the pair (F, X) if w(F) = X.  We call the pair (F, X) a 
            restriction,  and the number of elements in X is called the size of this restriction. 
            We say that the restriction  (F, X)  covers the indices in F.  Now we are ready to 
            formulate and prove the statement we spoke about [158]:
                Theorem  161.  Let a  €  (0,1)  be  a  constant.  Assume  that we  have  a set  of 
            restrictions  (F,X)  such that for every position i  and for every positive integer n 
            there are at most 2an restrictions of size n that cover i.  Then there exist a number 
            N  and a sequence  that is  not forbidden  by any  of the  restrictions  of size greater 
            than N.
                PROOF.  For compactness reasons, it is enough to prove the statement for finite 
            sequences (and for some N that is the same for all lengths).
                For each restriction we have the event  “this restriction is violated”.  The prob­
            ability of such an event for a restriction of size n is 2~n.  To apply the LL, let us 
            choose Ei  for restrictions of size n as 2~l3n where ß is some constant  in  (a. 1)  (in 
            fact, every value in this interval can be used).
                The neighbors of some restriction are the restrictions that have common vari­
            ables with the first one.  To apply the LL, we need to consider a restriction of size 
            n and check that 2~n  does not exceed 2~l3n times the product  of all the factors 
            (1 — 2~@m) for all neighbor restrictions.
               We split the product into parts that correspond to common variables.  There 
            are  n  parts  (for  each  of the  variables  involved  in  the  restriction).  If a neighbor 
            shares two or more variables, we arbitrarily break the tie and choose one of them. 
            In each part, we classify the factors according to their sizes.  Then for each variable 
            and for each size m, we get at most 2am factors, each equal to (1 — 2~l3rn).  Then 
            we take the product  over all m,  and the nth power  (since we have n parts that 
            correspond to n possible common variables).  So we need to show that
                                   2_n ^ 2-ßn  Y[{1- 2~ßm)2“"'n:
                                               m>N
            or (since all the terms are nth powers)
                                                8.  SOME  APPLICATIONS
              250
              Using the Bernoulli inequality, we see that it is enough to prove that
                                              2ß~l  < 1 -        2am2~ßm.
                                                           in > N
              The left-hand side is less than 1, and
                                                          2(a-ß)m
                                                       m
              is a converging geometric series, so this inequality is true for large enough N.
                   Let us repeat how the proof goes:  First we choose some ß £ (a, 1), then we note 
              that  the series is converging and choose a suitable N,  then  (for every length) we 
              apply the LL and show that there exists an assignment of this length that satisfies 
              all the restrictions, and finally we use the compactness argument.                         □
                   A direct proof of Theorem 161 (that does not refer to the LL but uses some ideas 
              similar to the proof of the LL) was suggested by An.  Muchnik and A. L. Semenov. 
              This proof goes as follows.  Assume that a set of restrictions is fixed that satisfies 
              the conditions of this theorem.  Let N be the minimal size of restrictions in this set.
                   For each finite set of indices I C N, let us denote by c(J) the number of valid 
              partial assignments,  i.e.,  the number of mappings I —>  {0,1}  that do not violate 
              any  restrictions.  (Here  we  consider  only  restrictions  {F,X)  where  F  с  I.  Our 
              mapping is defined only on  /,  and we cannot  check the restrictions that  involve 
              variables outside I.)  For empty I we let c(I) = 1.
                  Fix some ß £ (a, 1).  Let us prove that c(I) is multiplied by at least 2ß when 
              we add a new point to I.  (We assume that N is large enough.)  This implies that 
              c(J)  ^ 2ßk if I contains к variables.  In particular, c(J) > 0 (this is what we really 
              need, but for induction purposes we use a stronger statement).
                  Imagine that we add to I a new point  (^variable, index) i, and Г — I U {?’}. 
              Every good assignment for I creates two assignments for I' (the new variable may 
              have  two  values),  but  not  all  of these  2c(I)  assignments  for  I'  are  good,  so  we 
              need  to  subtract  the  number  of assignments  that  violate the  restrictions.  Since 
              the /-assignment was good, the violated restriction should contain i in addition to 
              some other points in I.  Fix some restriction,  and let  К  С  I be the set of these 
              other points used in this restriction.  How many assignments do we lose because 
              of this restriction?  Since the variables that are part of the restriction are fixed to 
              make it false,  the number of lost assignments is bounded by the number of good 
              assignments on I\K, and this number is bounded by c(I)/2ßk due to the induction 
              assumption.  (Indeed, if we increase the number of assignments by at least a factor 
              of 2ß when adding a new point, then we decrease the number of assignments by at 
              least the same factor when deleting one of the points.)  Now we need to sum up all 
              the deleted assignments for all Z = N — 1, N,..., I, and for each к there is at most 
              2a'(fc+i)  restrictions that involve i and also к elements in I.
                  In this way we get the bound
                                                             hi    Q(fc+1) C(j)
                                         c(J') £ 2c(J) -    E 2 2ßk ■
                                                          k=N-l
                                                                 8.5.    FORBIDDEN  SUBSTRINGS                                                         251
                    Let us make it weaker:  replace 2a by 2, and include all к ^ N — 1 in the sum.  Then 
                    we get
                                                              <!')> MI)( 1-  £   J ) .
                                                                                    V       k^N-l Z           /
                    The series in the right-hand side converges; therefore for large enough N the factor 
                    in the right-hand side (in parentheses)  is at least 2/3_1,  and the induction step is 
                    finished.  (Note that we applied the inductive assumption only to sets of size less 
                    than |/|, so there is no circle in our argument.)
                           8.5.5.             Complex subsequences.  Now we want to translate the result of the 
                    previous section into complexity language and prove that there exist  a sequence 
                    that has complex subsequences and not only substrings (as before).
                           What kind of statement can we get?  Can we guarantee that every subsequence 
                    of (large enough) length m has complexity at least am for some a < 1?  (A similar 
                    result was true for substrings.)  Of course not—one can select a subsequence that 
                    consists only of zeros  (or ones).  But in this case the set of indices may have high 
                    complexity.  So we should take into account both the complex^ of the set of indices 
                    and the complexity of the subsequence.
                           Indeed  a result  of this  type  can be  proven,  as we  saw  in Problem  145  (and 
                    Theorem 94,  p.  151,  for  the  case  of uniform  measure):  if a sequence  cu  is  ML- 
                    random with respect to the uniform measure, then
                                                                      K(Fcj(F))^\F\-c
                    for some c and for all finite sets F.
                           But now we want to prove a different result [158]:
                           Theorem 162.  Let a £ (0.1)  be a real number.  There exist a sequence u>  and 
                    a constant N such that
                                                                   nvc\xC(F,u>(F)\t) ^   ot\F\ 
                    for every F that contains at least N elements.
                          To understand better the meaning of this result, let us consider the following 
                    corollary:  for every finite set F of size at least N there exists t £ F such that
                                                             C(u(F)\F,t)^a\F\-2C(F\t)
                    (the constant 2 can be made smaller, but we want a simple statement).  Omitting 
                    t in the left-hand side, we may conclude also that for every finite F the inequality
                                                          C(u(F)\F) ^ a\F\-2mzxC(F\t)
                    holds.
                          This implies that all substrings are complex.  Indeed, if F is an interval, then 
                    the complexity C(F\t) is logarithmic for every t £ F and can be absorbed by a small 
                    change in a.  Moreover, this gives us also a two-dimensional version:  If indices form 
                    a planar grid  and  F is a rectangle,  the complexity C(F\t)  is also  logarithmic  in 
                    the size of F,  and the same trick works.  So we get the statement of Problem 251 
                    (p.  248)  as a corollary.
                          Note also that this corollary shows that ML-random sequences do not have the 
                    required property and LL is essential here.
            252                          8.  SOME  APPLICATIONS
                P r o o f.  In  fact.  Theorem  162  is  just  a  complexity  reformulation  of Theo­
            rem 161.  Indeed, consider the set of all restrictions  (F,Z) such that C(F,Z\t) < 
            a\F\ for all t G F.  Then for every index t the number of restrictions (F, Z) of size 
            k, where F contains t, is at most 2ak. and we can apply Theorem 161.           □
                8.5.6.     The  “effective”  proof of the Lovâsz local lemma.  Are the prob­
            abilistic existence proofs  “constructive”?  No, in the sense that they do not provide 
            an  explicit  example  of an  object  with  required  properties.  (One  can  perform  a 
            brute-force search and call the first object with the property an explicit example, 
            but this looks more like cheating, and the search usually takes a very long time.) 
            On the other hand, if the probability of the event  “random object has the required 
            property” is close to 1, we at least have a probabilistic algorithm that generates an 
            object with required properties, with small probability of error (and rather fast).
                What can be said about existence proofs based on the LL? In these proofs the 
            probability is exponentially small (though positive).  Random choice is no more an 
            option:  We cannot just take a random object according to the distribution used 
            in  the  LL.  However,  we  can  use  random  bits  in  a  more  clever  way,  and  in  this 
            section we explain how (and get a new proof for the LL in some special cases as a 
            byproduct).
                Assume that we want to construct a binary string (an assignment)  that sat­
            isfies some restrictions  (^clauses of a CNF, see Section 8.5.4).  Let us first choose 
            independent random values for all the bits.  Most probably some small part of the 
            restrictions will be violated.  Take one of them and try to improve the situation 
            by resampling all the variables that appear in this restriction.  (Resampling means 
            that we assign fresh random bits to these variables.)  Most probably this will solve 
            the problem with this restriction; it is quite unlikely that we will get the same bad 
            values for these variables once more.  Of course, other restrictions may still be vi­
            olated, and new violations may happen (for the restrictions involving the changed 
            variables).  Then we can repeat the process:  Take some restriction that is currently 
            violated, and perform the random resampling for its variables.  And so on.
                More formally,  the  initial  values  of all  variables  are  chosen  at  random,  and 
            then we iterate the following procedure:  While some restrictions are violated,  take 
            one of them (say, the first one in some ordering, or the random one, or use some 
            other rule)  and perform the resampling for all variables that appear in it.  This is 
            repeated until all restrictions are satisfied.  It looks like a miracle, but R. Moser and 
            G. Tardos recently proved [130, 131] that this trivial algorithm indeed achieves the 
            goal rather fast and with high probability.  (Before them, much more complicated 
            algorithms  were  studied  and  much  weaker  results  with  much  more  complicated 
            proofs were obtained.)
                We do not present their proof in full generality; instead we consider a special 
            case when all the clauses have the same number of variables.  Moreover, we assume 
            that the resampling is made in some special order (determined by recursive calls, 
            see below).  In this case a simple argument using Kolmogorov complexity can be 
            used, and we explain this argument, following L. Fortnow.
                So let us assume that a CNF is given with n variables and N clauses, and each 
            clause has some fixed length m (contains m variables).  We say that two clauses are 
            neighbors if they have a common variable.  Assume that every clause has at most t 
            neighbors.  We claim that if t is not very large, the LL guarantees the satisfiability 
            of the CNF in question.
                                                                   8.5.    FORBIDDEN  SUBSTRINGS                                                            253
                            How large can be t to make the LL applicable?  Since all the clauses have the 
                     same size,  it  is natural to use the same value of e for all of them.  This e should 
                     satisfy the inequality
                                                                             2“m
                     (the left-hand side is the probability that a given clause is false).  The right-hand 
                     side is maximal when e — l/(i + 1), but to simplify the computation we let e — 1ft 
                     instead.  Then the right-hand side is (1 — l/t)1 ft, which is almost If et.  So we need 
                     (approximately) t ^ 2m/e to apply the LL. In the constructive proof we use a bit 
                    stronger requirement, namely, t ^ 2m/8.
                            Theorem  163.  There exists a probabilistic algorithm that founds a satisfying 
                     assignment for a given CNF with n variables and N clauses of size m where each 
                     clause has at most 2m/8 neighbors,  in time polynomial inn + N and with success 
                    probability at least If2.
                            (As usual, the bound for success probability can be amplified easily:  Repeating 
                    the  algorithm  s  times,  we  find  a satisfying  assignment  with  probability  at  least 
                     1- 2“s.)
                           P r o o f.  Our algorithm uses the recursive procedure Fix(d)  (where d is some 
                    clause) and works as follows:
                                                               for all clauses d of a given CNF : 
                                                                   if d is false:  Fix(d)
                    All the clauses of a given CNF are processed in some order.  The processing of a 
                    clause d is simple:  If d is not satisfied yet, is is  “fixed”  by calling Fixfd).  To prove 
                    the correctness of the algorithm, we need the following property of the procedure 
                    Fix(d):  It makes clause d true and keeps true all clauses that were true before the 
                    call.  (Some clauses that were false before the call may become true;  this is even 
                    better for us since it saves some future work.)
                           The procedure Fix(d) is simple, too:
                                               resample all variables in d using fresh random bits; 
                                               for all clauses d' that are neighbors of d: 
                                                   if d' is false:  Fix{d')
                           Note that it may happen (with small probability) that the new random values 
                    are in fact the same as before, so the resampling does not make d true.  It would be 
                    natural to perform the resampling again until we get new values, but it is easier to 
                    postpone this and just consider d as its own neighbor (so that the resampling will 
                    be performed later as part of the loop, if it would still be necessary at that time).
                           The correctness of this procedure (assuming that the recursive calls work cor­
                    rectly)  is  obvious:  During the resampling only the clauses that  are neighbors of 
                    d  may  become false,  and they all will be fixed in  the  loop  (including d itself,  if 
                    necessary).  The only problem is to prove that  the process terminates with high 
                    probability in polynomial time.  For that let us analyze how this process uses ran­
                    dom bits.  (We assume that random bits are produced in advance and used when 
                    needed.)
                           First of all, we use n random bits as initial values of the variables.  Then each 
                    call  Fix(d)  uses the next m random bits to resample variables in d.  (Recall that 
                    we do not resample d twice even if the resampling gives the same bad values;  it 
                    simplifies our analysis.)  The following is a crucial observation:  At every step
         254                 8.  SOME  APPLICATIONS
               the values  of used random bits can be reconstructed if we know 
               (1)  the  current  values  of all  the  variables  of the  CNF;  (2)  the 
               list  of clauses for which the procedure Fix (d)  was  called,  in the 
               order of calls.
           Indeed, the call Fix(d) is performed only when d is false, and this determines 
        the values of variables in d before the call.  And their values after the call are just 
        the next random bits.  Unrolling the execution backwards, we can reconstruct the 
        values of variables between the calls and finally n initial values, so we know all the 
        random bits used.
           Now the idea of the proof can be explained as follows.  If the algorithm makes a 
        lot of calls, then we can compress the values of random bits used by the algorithm, 
        because the list of clauses for which Fix was called has a shorter description.  To 
        finalize the proof,  we should estimate the complexity of this list.  Here it is very 
        important that Fix(d) calls Fix(d') only for those d! that are neighbors of d, and 
        these d'  can be specified by their ordinal number in the list of neighbors.2  (Here 
        we use the bound for the number of neighbors.)
           Now let us compare the number of random bits used and the number of bits 
        needed to describe them  (as explained in the previous paragraph).  Consider the 
        situation after к calls of Fix.  At that time the algorithm has used n + km random 
        bits.  To reconstruct them, we need to know the following:
             •  the current values of the variables;
             •  for which clauses the procedure Fix was called in the main loop;
             •  which recursive calls of Fix were made during each of those calls.
           Current  values are n bits;  the list  of clauses called  in  the  main  loop can be
        described by N bits (for each clause we say whether it was processed or not;  the 
        order  of clauses  is  fixed,  so  N  bits  are  enough).  To  estimate  the  complexity  of 
        the third component, let us consider trees of recursive calls.  For example, the tree 
        illustrated  in  Figure  26  starts with a call  Fix (a).  This  call generates three calls 
        for  b, c, d;  the  call  for  b generates calls for e, f, g,  the call for  c does not  generate 
        anything, and the call for d generates only one call for h.  The chronological order 
        of all the calls is a, b, e, f, g, c, d, h  (the left to right ordering of the sons of a vertex 
        corresponds to the order of calls).  Indeed, we call c only after we return from 6-call 
        (that  generated  calls  for  e,f,g),  and  then  make  d-call  that  generates  /г-call.  In 
        other words, the order of calls can be described as follows:  imagine that our picture 
        is a bird’s view of a wall; we start walking around it from a and always touch the 
        wall by the right hand.  The we visit the vertices in the order
                         a-b-e-b-f-b-g-b-a-c--a-d~h~d-a,
        and this corresponds to the control flow during the execution.  New random bits 
        are used when we come to some vertex for the first time (from below).
           So to specify the processed clauses  (and the order of processing)  it is enough 
        to  encode  the  tree  walk.  It  consists  of steps  up  and  down.  For  a step  up,  we 
        need to specify not only the fact that we are going up but also the number of the 
        neighbor where we are going.  In total we use 1 + logt bits (one for the direction, 
        and one for the number).  Here t is the upper bound for the number of neighbors,
           2So (as we have mentioned)  it is a bit surprising that the result is true for other rules that 
        select the next clause for resampling.  (We want to stress that the argument we provide depends 
        on the choice of the rule, though the result does not.)
                                              8.6.  A  PROOF  OF  AN  INEQUALITY                                255
                                                          e  f   9   h
                                                                     U
                                                                 a
                                                          Figure 26
               i.e.,  2m/8  =  2m~3  according to the assumption.  When going down,  only one bit 
               that indicated the direction is sufficient.  (In other words, when making a recursive 
               call, we perform a push operation for the stack of calls, and we should specify the 
               top of the stack; for pop operation no additional information is needed.)
                    So for each vertex  (except  the root)  we use log t + 2  bits  (we  need log t + 1 
               bits when we come to this vertex from below, and then one more bit when going 
               back to its father).  In total (for all the vertices) we need N + n + k(\ogt + 2) bits 
               for description.  If the random bits used in the algorithm are incompressible, then 
               N + n + к (log t + 2) ^ n + km, and we get an upper bound for k.  Namely, we get 
               the bound к ^ N (recall that logt + 2 = m — 1), so we make at most N calls of the 
               procedure Fix, and the algorithm is polynomial (in N + ri).
                    Some final clarifications are needed still.
                    1.  If we literally use Kolmogorov complexity, then some constant appears, and 
               we should keep track of all these details.  As usual, when the idea is clear, we can 
               switch to the probabilistic language:  If к — N + c, then the difference between the 
               number of random bits used and the number of bits in the description is c.  This 
               means that the number of random bit strings that cause N + c or more calls of Fix 
               is  2C  times smaller than the total number of possible strings, so the probability is 
               bounded by 2~c.
                    2.  When we describe several objects by a sequence of bits,  we should check 
               that no separators are needed to perform the decoding.  Here it is indeed the case: 
               The number of variables, clauses, and the clause size (as well as the bound for the 
               number of neighbors)  are known;  after a bit that specifies the direction (whether 
               we go up or down) is read, the decoder knows how many bits it should read next.
                    3.  The last  problem:  It  may  happen that  we stopped  the execution at  the
               moment when one of the trees is only partially processed, so we should be able to 
               describe the unfinished tree walk.  But our way of description works in this case as 
               well; we should only note that at every moment the number of steps down does not 
               exceed the number of steps up (the number of Fix-calls).                                          □
                                             8.6.  A proof of an inequality
                    As we have said (see p.  12), the inequalities for Kolmogorov complexity have 
               quite unexpected consequences.  In this section we explain one of them, a version 
               of the Loomis-Whitney inequality (this topic will be continued in Chapter 10).
             256                             8.  SOME  APPLICATIONS
                  Theorem 164.  Let X, Y, and Z be finite sets.  Let f  :                  YxZ —»• M,
             and h : X xZ  —> M  be some functions with non-negative values.  Then
                     f(x>y)9(yiz)h(x,z)
                 x,V,z
                  PROOF.  Believe it or not, this inequality is in fact a corollary of the inequality
                             2K(x,y,z) < K(x,y) + K(y,z) + K(x,z) + 0(logn)
             for prefix complexity (Theorem 26, p. 48).  We wrote the last inequality for prefix 
             complexity,  not plain complexity,  but this does not  matter since the difference is 
             O(logn).  (For prefix complexity this inequality is true up to 0(l)-precision  (see 
             Problem 114, p.  Ill); for now the 0(logn)-precision is enough.)
                 It  is convenient to assume that elements of the finite sets X, Y, Z are binary 
             strings.  It is enough to show that if the sums in the right-hand side of the inequality 
             do not exceed 1, the same is true for the left-hand side.  (Indeed, we can multiply 
             /  by an arbitrary constant c, and both sides of the inequality are multiplied by the 
             same factor, so we can normalize /; the same for g and h.)
                 Now assume that          y f2(x,y)  =  1  and that  the same is true for two other 
             sums.  We have to show that J2X  z /(x, y)g(y, z)h(x, z) < 1.
                 The  idea  is  simple:  The  function  / 2  is  a  probability  distribution  on  pairs 
             (x,y),  so  K(x,y)  <  — log/2(x,y)  =  —21og/(x,y)  (we  temporarily  ignore  the 
             constant in the comparison of this distribution and the a priori one).  Similarly, 
             K(y,z) < —2 logg(y,z) and K(x,z) < —2\ogh(x,z).  Then we apply the inequal­
             ity for K(x,y,z)  (temporarily ignoring the logarithmic term) and get
                           2K(x,y,z) ^ -2 log f(x,y) -  21ogg(y,z) -  21ogh(x,z),
             i.e.,
                                       f(x,y)g(y,z)h(x,z) ^ 2~K{x'y'z).
             Since the sum of 2~K(x,y,z')  over all triples x,y,z does not exceed 1  (Theorem 57, 
             p.  92), we get the desired inequality.
                 This argument is, of course, too simple to be valid.  All our bounds are of as­
             ymptotic nature, so we have to switch somehow from individual strings to sequences 
             of strings.  Let us show how it can be done.
                 We start with a simple remark:  It is enough to prove the inequality for functions 
             /, g, h with rational values (by continuity).
                 Let N be some natural number (later we take the limits as N tends to infinity). 
             Consider  the  sets  XN, YN,  and  ZN  whose  elements  are  ALtuples  (of elements 
             of X,  Y,  Z,  respectively).  Consider  a probability  distribution  on XN  x  YN — 
             (X xY)n that corresponds to N independent copies of distribution f2 on X x Y. 
             Formally speaking, the probability of a point  ((xi,..., хдг), (y\,...,улг))  is equal 
             to the product f2(xi,yi) ■ ... ■ /2(хм,Уы)-  We get a family of distributions that 
             computably depends on N.  Therefore, there exists a constant c such that
                         K((xi,... ,xN), (yi,... ,yN)\N) < 2 ^ ( —log f(xi,yi)) + c
                                                                  г
             for all N and for all x\,..., хдг, yi,..., y^ (we compare our distribution with a priori 
             probability).  We can delete the condition N in the left-hand side, and replace c by
                                                            8.6.    A  PROOF  OF  AN  INEQUALITY                                                   257
                    clog N in the right-hand side.  Then (as before) we add three inequalities of this 
                    type and apply the inequality for complexities.  Then we get
                    A"((x\, ■ • •             (î/i) • • • )Un)) (^i? ■ • ■ ) )
                                  < ]T (-log/(^,2/z)) + ^{-\ogg{yi,zi)) + Y^(-^h(xi,Zi)) + clog AT
                                        i                                  i                                i
                   for some constant c and for all N,  aq,..., x^, г/i,..., ум, z\,..., zpj.  (Note that 
                   the total length  of all  the strings Xi,yi,Zi  for г  =  1,..., N  is  O(N),  so  all  loga­
                   rithmic terms are absorbed by clogN.)  Combining this bound with the inequality 
                          2~K(U) ^ 1, we conclude that for every N the sum
                                                             ЕП f(xi,yi)g(yi, Zi)h(xi, Zi)
                    (over all tuples aq,... ,xn, г/i,... ,yw, z 1,... ,zpj) does not exceed 2°(logiV), i.e., it 
                   is bounded by a polynomial in N.  But this sum is the Nth power of the sum
                                                                  Z  f(x,y)g(y,z)h(x,z),
                                                          {x,y,z)(zXxY X Z
                   so  polynomial growth is possible only if the latter sum does not exceed  1.  This 
                   ends the proof.                                                                                                                  □
                           252               Show that this inequality implies the bound for the volume of a three- 
                   dimensional body in terms of its two-dimensional projections mentioned on p.  12.
                          (Hint:  We can let /, g, h be the characteristic functions of the projections.  This 
                   works for the discrete case; for the continuous case we should either approximate 
                   the body using a cubic grid or approximate the integral by finite sums.)
                          For comparison let us give two other proofs of the same inequality.  Here is the 
                   first one (rather simple) using the Cauchy-Schwarz inequality (u,v)2 < ||u,||2 • ||v||2, 
                   or, in coordinates,  Ç^UiVi)2 < (Y^ui)(J2vi )•  We can argue as follows:
                             Y  f(x,y)g(y,z)h{x,z)) ^ ( Y f 2(x'yï) ( Y ( Y 9^y'z^ x'z^) )
                            x,y,z                                 J          \x,y                /  \x,y  \  z                           J  /
                                                = (z/2(^))                                          fz^’^V
                                                     \ x,y               J  \y,z                J  \x,z                 J
                          Another proof uses Shannon entropy (and can be considered as a translation of 
                   the Kolmogorov complexity argument into probabilistic language).  Let us assume 
                   that        f 2 =        g2 =         h2 — 1.  We want to prove the inequality                               y z p(x> Viz) ^ 
                   1, where p(x, y, z) = f(x, y)g(y, z)h(x, z).  Assume that is not the case and this sum 
                   equals c > 1.  Then we can multiply it by 1/c and get a probability distribution p' 
                   on X X Y X Z\
                                                         P'{x,y,z) = \f(x,y)g(y,z)h(x,z).
                   The corresponding random variable (whose range is X x Y x Z) is denoted by £.  It 
                   can be considered as a triple of (dependent) random variables £x, £y, £z.  One can
              258                                 8.  SOME APPLICATIONS
              also consider the joint distributions £xy —  (£е,£у), etc.  For example, the random 
              variable £xy takes the value (x,y) with probability                       z).
                   Recall that by definition the Shannon entropy of the distribution  (pi,... ,py) 
              equals Y^Pi{~ logp,;);  it does not exceed                 logg,;)  for any other distribution
               (<7i,..., Çfc).  Therefore the entropy H(£xy) can be bounded (from above) by using 
              f2(x,y) as the “other”  distribution,
                                   Hitxy) ^ Y  \Y p \x,y,z)\ (-2log f(x,y)).
                                                x,y  \   z           /
              Then we write similar bounds for two other projections and apply the inequality
                                Я«) =                   < \(Н(Ь,) + Я (^ ) + Я «„))
              (Problem 230, р. 225).  We conclude that
                           я (£) ^  Y P'(x^y^z)(-lo&f{x,y) -  logg(y,z) -  logh{x,z))
                                    X,y,z
                                  = Y p,(x'y’z^~logp(x'y'z^-
                                    x,y,z
              By definition H(£) =             z p'(x, y, z)(— \ogp'(x, y, z)),  so we get a contradiction, 
              since p' is c times smaller than p (and therefore — log;/ exceeds — logp by loge).
                              8.7.  Lipschitz transformations are not transitive
                   In this section we apply Kolmogorov complexity to analyze the properties of 
              infinite sequences.  Let us start with the following definition related to the Cantor 
              (metric) space Q of infinite binary sequences.
                   A mapping / : Q —>         is a Lipschitz one if
                                              d(f(wi),/(w2)) ^ cd(ui,w2)
              for some constant c and for all u>i,u>2  £          Here d is the standard distance in the 
              Cantor space defined as 2~k where к is the first place where two sequences differ.
                   Informally speaking, Lipschitz property means that the first n digits of the se­
              quence f(co) are determined by n + 0(l) first digits of to.  In particular, all mappings 
              defined by local rules (each bit in f(co) is determined by some its neighborhood in со) 
              have Lipschitz property.
                   We are interested  in the following property of a mapping  /:  For  every  two 
              sequences co\, cü2  and  for every e  >  0,  there exist  a number N and sequences 
              and cd2 such that
                                     u'2 = /(/(/(■•• /(^i) • • ■ )))    (N iterations)
              and
                                             d(u)i,U)[) < e,    d(co2,co'2)  < £.
              (In other terms, for any two open neighborhoods there exists an orbit that starts in 
              the first one and gets inside the second one.)  We call this property the transitivity 
              of /   (in this section).
                   It  is  easy  to  check  that  left  shift  (that  deletes  the  first  bit  of the  sequence) 
              is  transitive:  If we  need  a sequence that starts with x\  and is transformed  (after 
              several shifts) into a sequence that starts with X2 , just take a sequence that starts 
              with X1X2 .
                        8.7.  LIPSCHITZ TRANSFORMATIONS ARE NOT TRANSITIVE          259
               Now we pose the following question:  Does the left shift remain transitive if we 
           change the definition  and  replace  Cantor distance d by the so-called  Besicovitch 
           distance,
                                  p{u)\, U)2 ) = Uni Slip dn (u>i, U)2 )/n
                                                » oo
           (where dn is a number of discrepancies among the first n terms, i.e., the number of 
           i  < n such that ith. terms of uj\  and 0J2 differ)?
               It  turns out that  in this case the left shift  is no more transitive  (is not  Besi- 
           covitch-transitive).  Moreover,  the following statement  is  true  (we reproduce  the 
           proof given in [17]):
               Theorem  165.  No Lipschitz mapping can be Besicovitch-transitive.
               (Speaking about the Lipschitz property, we have in mind the original definition 
           using Cantor distance.)
               The reason is quite simple.  The Lipschitz mapping does not significantly in­
           crease  the  complexity  of the  prefixes  of a  sequence,  since  n  bits  of the  output 
           sequence are determined by n + 0(1) bits of the input sequence.  (We assume that 
           transformation rule is computable; if not, we have to relativize complexity by a suit­
           able oracle.)  On the other hand, if two sequences are Besicovitch-close, then their 
           prefixes have almost the same complexities (a change in a small fraction among the 
           first n bits can be encoded b}' a short string compared to n).
               PROOF.  For a formal proof it is convenient to use the notion of effective Haus- 
           dorff dimension of a sequence (which is equal to liminf C(ujo • • •u)n-\)/n for a sin­
           gleton {ta}; see Theorem 120, p.  174).
               Lemma  1.  A  computable  Lipschitz  mapping  does  not  increase  the  effective 
           Hausdorff dimension of a sequence.
               (Speaking about computability of a Lipschitz mapping / : Q, —y Q, we mean that 
           n first bits of f(uj)  are effectively determined by n + c first bits of uj for some c.)
               P r o o f.  Indeed,  if f(uji)  =   uj2 ,  then  the  complexity of an  n-bit  prefix of uj2 
           does not exceed (up to 0(1)) the complexity of an (n + c)-bit prefix of uq, and for 
           the dimension these constants are not important.
               Lemma 2.  If Besicovitch distance p(uji,uj2)  is less thane,  then effective Haus­
           dorff dimensions ofuj\  and uj2  differ at most by H(e).
               (Here H(e) is the Shannon entropy of a random variable with two values that 
           have probabilities e and 1 — e.)
               P r o o f.  Indeed, if the first n terms of uj\ and uj2 differ in к places, then the com­
           plexities differ at most by the complexity of the bitwise xor of these two sequences 
           (since knowing one sequence and their xor we easily get the other one).  And every 
           sequence of length n that has к ones has complexity at most nH(k/n) + O(logn) 
           (Theorem 146, p. 226).  Lemma 2 is proven.
               So if we take a sequence of a zero dimension (say, a computable sequence), then 
           any sequence that is Besicovitch-close to it has small dimension, and a computable 
           Lipschitz mapping does not increase this dimension, so we can get only sequences 
           of small effective Hausdorff dimension.  On the other hand,  any sequence that is 
           Besicovitch-close to a random sequence (that has dimension 1 ) has dimension close 
           to  1  (Lemma 2 again).
         260                 8.  SOME  APPLICATIONS
           So we have proven our theorem for computable Lipschitz mappings.  It remains 
         to note that all our arguments are relativizable and that every Lipschitz mapping 
         is computable relative to some oracle.                 □
                                                                                       CHAPTER 9
                           Frequency and game approaches to randomness
                                                              9.1.  The original idea of von Mises
                              Nowadays the axiomatic approach to probability (that makes it a special part 
                      of measure theory)  is standard,  and it  is difficult  to  forget  all we know now and 
                      return to the situation in the beginning of the twentieth century when Richard von 
                      Mises suggested basing probability theory on the notion of a random sequence (he 
                      used the word Kollektiv).  Still let us try to describe von Mises’ ideas.
                              Some natural phenomena are easy to predict (after we have discovered the laws 
                      of nature they obey).  For example, the laws of classical mechanics can be used to 
                      predict the positions of planets in the sky with very high precision.  But there exists 
                      another class of phenomena:  Even as we try very hard to predict the outcome of 
                      coin tossing,  usually we get about  50% of predictions correct.  Those phenomena 
                      are the subject of probability theory.
                              So the basic notion of probability theory (according to von Mises) is the notion 
                      of a Kollektiv—a sequence oj of outcomes (we will assume there are two possible 
                      outcomes 0 and 1) that is hard to predict.  Since this is a basic notion, we do not try 
                      to give a definition that would reduce it to other mathematical notions; instead we 
                                            frequency stability axiom that captures the main property of Kollektivs:
                      formulate a 
                                        There exists a limit
                                                                               ,.       Шо +           + • • • + шп-1
                                                                     p =  Inn  -------------------------- .
                                                                             n—5-00                     п
                                        Moreover, p remains the limit if we consider not the entire se­
                                        quence oj but some of its subsequence selected according to some 
                                        rule; for example, the subsequence oj2n, or the subsequence of ojn 
                                       with composite n,  or the terms that follow ones  (i.e.,  шп  such 
                                       that ojn- 1 = 1).
                      This p is called the probability of 1 in a given Kollektiv.
                              Why do the Kollektivs exists?  We know that gambling facilities are commer­
                      cially successful,  and this would be impossible if some selection rule existed that 
                      allowed the gamblers to select a subsequence of games with different frequencies of 
                      outcomes.
                              This  is  a  short  (but  faithful,  we  hope)  summary  of what  von  Mises  wrote; 
                      see, e.g., his book  Wahrscheinlichkeit,  Statistik und  Wahrheit  [127].  But his book 
                      was written not  in  the times  of Euclid  or  Spinoza,  but  in  the  beginning  of the 
                      twentieth century, when people tend to ask nasty questions about exact definitions 
                      and for detailed proofs.  Indeed,  one can declare that the existence of sequences 
                      with some properties is an axiom that is confirmed experimentally (though to speak 
                      about experimental confirmation of the statement that deals with limits of infinite
                                                                                                261
        262     9.  FREQUENCY AND  GAME APPROACHES TO RANDOMNESS
        sequences is a bit strange).  But even then one should say exactly what property 
        we have in mind.
          The problem is with the selection rules:  We did not say what kind of selections 
        are allowed.  Mises gave only some examples of admissible selection rules (we gave 
        three examples of this type),  and  noted that  the decision to  select  (or do  not 
        select)  some wn should not depend on the value of cjn  itself;  otherwise,  we can 
        select a subsequence of zeros (or ones) only from every sequence, and this violates 
        the frequency stability property.
          Trying to make Mises’  ideas precise,  one can give different  formal definitions 
        of admissible selection rules and can therefore get different notions of Kollektivs. 
        After some version is chosen, one can ask whether Kollektivs exist.  This question 
        is  a  mathematical  one,  while the  question  of whether  coin  tossing really gives  a 
        Kollektiv belongs to natural sciences or philosophy (and can be put aside).
          For simplicity we restrict ourselves to the case of a symmetric coin  (p = 1/2) 
        unless the opposite is not said explicitly.  To simplify the statement, let us define 
        a  balanced sequence of zeros and ones as a sequence where the frequency of ones 
        (and, therefore, the frequency of zeros) has limit  1/2.
                     9.2.  Set of strings as selection rules
          The first (and, probably, the most natural) interpretation of Mises’ ideas of an 
        admissible selection  rule  is the following one.  We decide whether to select some 
        term ojn  looking at  all the preceding terms,  i.e.,  coocoi • • -wn-i-  So an admissible 
        selection rule is a function that maps all binary strings ujq - ■ ■ ojn-i to a two-element 
        set {select, do not select}.  In other words, a selection rule is a set R of binary strings 
        (corresponding to the value select).
          Formally speaking, for every set R of binary strings we define a selection rule as 
        a mapping Sr that maps an infinite binary sequence ш € ft into a (finite or infinite) 
        subsequence Sr(co). Namely, Sr(co) consists of terms шп such that ujq • • -wn-i G R. 
        (The order of terms is the same as in the original sequence.)
          We give an example:  If R consists of strings whose lengths belong to some set 
        {no, ni, • • •}  (where no <n\ < ■ • •  is an increasing sequence of integers), then Sr{uj) 
        is шПоШщ • • •  (note that the length of xo • • • z/c-i  is k).  We give another example: 
        The rule  “select terms that follow ones”  corresponds to the set R which contains 
        all strings with last bit  1.
          Assume that we fix some R and then go to a casino where a sequence w of zeros 
        and ones is generated by tossing a fair coin.  Then we get some subsequence Sr (co). 
        (In other words,  we use R to decide when to make bets).  It is natural to expect 
        that this selection does not give us any advantage, and the limit frequency of ones 
        in the subsequence is still 1/2.  There is an important point, however:  We assume 
        that we have chosen R before we came to the casino.  After the game it is easy to 
        find a rule R that would win if it were used in the game.  In other words, we make 
        the following (obvious) observation:  For every sequence to there exists a set R such 
        that Sr(co) consists only of zeros or consists only of ones,  and therefore Sr (uj) is 
        not balanced.  So we cannot define the Kollektiv as a sequence и such that Sr(co) 
        is balanced for all R.  With this definition there are no Kollektivs at all.
          However, as Wald noted in [217], for every countable family of selection rules 
        SRj  (that  corresponds  to  a countable  family  of sets  Ri)  there  exists  a sequence
                                   9.2.  SET  OF  STRINGS  AS  SELECTION  RULES                   263
             uj  that  has  a frequency stability property with respect to all  Rp.  For every i the 
             sequence Sr^uj) is balanced (or finite).
                  This is easy to prove by a probabilistic argument:
                  Theorem  166.  Let R  be an arbitrary set of strings.  Then the set of all se­
             quences uj £ Cl such that Sr(oj) is an infinite unbalanced sequence is a null set with 
             respect to the uniform measure on Cl.
                  This theorem says that every selection rule discards a null set.  So a countable 
             class of selection rules generates a countable family of null sets,  and the union of 
             these null sets is a null set.  So there are sequences not discarded by any selection 
             rule in the class (moreover, this happens with probability 1).
                  PROOF.  This statement follows from the Strong Law of Large Numbers  (the 
             set of unbalanced sequences is a null set, see Section 3.2) and the following lemma 
             that holds for every selection rule Sr.
                  Lemma.  Let U C Cl be a null set.  Then its preimage S^l(U) is a null set.
                 Informally, each next bit of the sequence Sr(uj) has the same chance to be zero 
             and one (for every fixed combination of previous bits); the difference with uniform 
             distribution is that the next bit may be absent  (if the sequence is finite), but this 
             may only decrease the probability.
                  (Recall  an old  question:  Will  the percentage  of men  change  if families  stop 
             giving birth to children after a son is born to keep their heir unique?  The answer 
             is negative for the same reasons.)
                 Now let us present the formal argument.
                 Consider the set £x of all finite and infinite extensions of x and two of its subsets 
             £xo and Exi.  Let us prove that 5ß-preimages of £xo and £xl have equal measure 
             (in other words, 0 and 1 can appear after x in Sr(uj) with the same probability).
                 Indeed, consider all strings г such that z £ R and Sr selects x from z.  They 
             correspond to  the situation when x  is  already  selected  and  the  next  bit  will  be 
             selected right now.  So every two strings 2 with this property are incompatible, and 
             the sets Clzo  are disjoint.  The union of these sets is the preimage of the set  £xo- 
             Similarly,  the preimage of £xi  is the union of disjoint sets Clzi.  So we have split 
             the preimages into equal parts so the preimages have equal measures.
                 Now it  is  easy  to  prove  by  induction  that  the  measure  of the  S^-preimage 
             of £x  is  bounded  by           Therefore,  the  preimage of a null  set  is  a null  set
             too.  Indeed,  consider the cover of U  by intervals Clx.  with small  total  measure. 
             Consider  the preimages of £Xj. ;  each of these  preimages  is  a countable  union  of 
             intervals.  Combining all these intervals, we get a cover of     1 (£/) with small total 
             measure.  So the Lemma—and Theorem 166 as well—is proven.                             □
                 Note that the standard measure-theoretic argument  (a measure of a set is the 
             infimum of the measures of its covers) now implies that
                                               p(Sfi\U))^p(U)
             for every measurable U C  fi.  If Sr(oj)  is infinite for every uj  (or for almost every 
             uj),  then we can guarantee also that Sr(uj) is uniformly distributed,
                                              ß{S-R1(U)) = ß(U). 
             for every measurable U.  (Consider U and its complement.)
          264        9.  FREQUENCY AND  GAME APPROACHES TO RANDOMNESS
              253      Fix some selection rule R.  Show that if ш has a Bernoulli distribution 
          (independent trials with the same probability p, not necessarily equal to 1/2), then 
          Sr(co) has the same distribution (assuming that it is infinite with probability 1).
             So the definition of Kollektiv gives a non-empty notion (Kollektivs exist) if we 
          restrict  ourselves to some countable family of sets R and consider corresponding 
          selection rules.  But which countable family should we choose?
                             9.3.  Mises-Church randomness
             The ideas of von Mises appeared before the notion of algorithm (or computabil­
          ity)  was  formalized.  As  soon  as the  notion of computable function  appeared,  it 
          became possible to use it in the Mises’ scheme.  This was done by A. Church [44], 
          so selection rules Sr that correspond to decidable (=computable, recursive) sets R 
          are called  Church-admissible in the sequel.  The corresponding class of sequences, 
          i.e., sequences ш such that Sr(cj) is finite or balanced for every Church-admissible 
          rule, are called Mises-Church random, or  Church stochastic.
             We know already that they exist and form a set of full measure.  Moreover, the 
          following stronger statement is true:
             Theorem 167.  Every ML-random sequence (with respect to the uniform mea­
          sure)  is Mises-Church random.
             Proof.  The effective version of the SLLN  (Theorem 32, p.  65;  see also Sec­
          tion 8.4)  guarantees that the set  U of non-balanced sequences  (that do not have 
          the limiting frequency or have it different from 1/2) is an effectively null set.
             Let  us  show  that  for  a  Church-admissible  selection rule  Sr  the  preimage  of 
          an effectively null set is an effectively null set.  Indeed,  if R is decidable, the con­
          struction used in the proof of Theorem 166 becomes effective  (one can effectively 
          enumerate all the intervals that form a preimage of a given interval).  So an ML- 
          random sequence does not belong to this preimage, i.e.,  its image is balanced (or 
          finite).                                                          □
             What else  can  we  prove  about  Mises-Church  random  sequences,  except  for 
          the SLLN (that is satisfied by definition)?  For example,  we can prove that each 
          substring (not only each symbol) appears with a correct frequency:
             Theorem  168.  Let oj  be  a  Mises-Church  random sequence,  and  let  U  be  a 
          binary  string.  Consider  the  positions  к  where  U  appears  in и  (this  means  that 
          UqU\ • • • = cjkCJk+i • • • )•  The fraction of those i  among the first N positions tends 
          to 1/2^)  as N —» oo.
             Proof.  We  already  know that  zeros  appear  in  (approximately)  half of the 
          positions.  Consider now the rule  “select terms that go just after zeros”.  Mises- 
          Church randomness guarantees that the selected subsequence contains  (approxi­
          mately) equal numbers of zeros and ones.  This means that the groups 00 and 01 
          have approximately the same frequency, so the limit frequency of each group is 1/4. 
          The same is true for 10 and 11.  Now consider the rule “select terms that follow 00” 
          (or  “select terms that follow 01”), etc.                         □
              254     Consider a Mises-Church random sequence and split it into fc-bit blocks 
          (for some k).  Show that in the resulting sequence (in a 2fc-letter alphabet) each of 
          2k blocks appears with limit frequency l/2fc.
                      9.3.  MISES-CHURCH  RANDOMNESS  265
          {Hint-.  This problems differs from the preceding theorem, because now we take 
        into account only ^-aligned blocks.  However, the same argument works.)
          The sequences where each combination of letters (of every fixed size) has the 
       same limit frequency (as claimed by Theorem 168), were considered independently 
        of Mises; they are called normal
          255 Let us change the definition of normality of a bit sequence a and require
       that  for  every  m  the sequence  of m-bit  strings,  obtained  by  splitting  a  into  m- 
       bit  blocks,  contains every m-bit string with limit frequency 2~m.  Prove that this 
       definition is equivalent to the original one.
          {Hint:  The difference is that now we consider only aligned m-bit occurrences 
       instead of all occurrences.  Still we can prove equivalence considering not only m-bit 
       blocks but also M-bit blocks where M is a large multiple of m.  Assume that aligned 
       M-bit blocks appear with right frequencies.  Then for a fixed position inside each 
       long M-bit block (modulo M) all short m-bit blocks appear with right frequencies, 
       and short blocks that cross the boundaries between large blocks are rare (m <C M). 
       In  the other direction,  assume that  non-aligned frequencies are OK.  Most  M-bit 
       blocks are good in the sense that frequencies of short blocks inside them are almost 
       right.  Bad blocks are exponentially (in M) rare in terms of non-aligned frequencies. 
       Aligned frequencies could be at most M times bigger, and the factor M is absorbed 
       by the exponent.)
          The reals whose binary representations are normal sequences, are called normal 
       in base 2; similarly one can define reals that are normal in base b.  If a real is normal 
       in base b for every integer b, it is called absolutely normal
          256 Prove that the same reals are normal in base b and in base bk.
          (Hint:  Use the preceding problem.)
          One can prove that the class of normal in base b reals depends on b, but this is 
       a non-trivial number-theoretic result [161], and we will not prove it here.
          257  Let  us  consider  a bit  sequence ш  as a binary  representation of a real
       a G [0,1].  The tails of oj form a sequence of points in [0,1] which is the orbit of a 
       under the mapping x (->■ {2x} where {u} stands for the fractional part of u.  Show 
       that oj is normal if and only if this orbit is uniformly distributed in  [0,1].  (The 
       latter means that for every interval the fraction of points that are in this interval 
       has a limit proportional to the length of the interval.)
          258  Prove that multiplication by an integer factor preserves normality:  If a
       is normal in base 2 and к is an integer, then ak is normal in base 2.  (The same is 
       true for other bases.)
          {Hint:  Use the preceding problem.  Applying the nth iteration of the mapping 
       x и  {2x} to some real u, we get {2nu}.  For every integer к the number {2n{ku)} 
       is obtained from {2nu} by the transformation у i-> {ky}.  It remains to prove that 
       this mapping preserves the uniform distribution property.)
          One can prove that normality is also preserved when we divide a number by 
       some integer (and therefore, when we multiply a number by an arbitrary rational 
       number).  This was shown by D. Wall [218]  (see also  [88]), but the proof is non­
       trivial and we do not provide it here; see [182] for the proof.
          We know that Mises-Church random sequences are normal, but one can also 
       find a computable normal sequence.  For example, if we write numbers 1, 2,3,... in
           266        9.  FREQUENCY  AND  GAME APPROACHES  TO  RANDOMNESS
           binary and concatenate all these strings, we get a normal sequence 
                        110111001011101111000100110101011- - •
           (Champernowne’s example [36];  he considered base  10,  but this does not matter 
           much).
               259  Prove this statement.
              (Hint:  Fix k, the block size.  Starting from some point, the numbers have many 
           more than к digits, and after that the boundaries between numbers do not change 
           the frequencies significantly.  And the average of block frequencies in all strings of a 
           given length N is as it should be.  (Some care is needed to deal with the case when 
           we stop in the middle of strings of length N.))
              This construction of a computable normal sequence is performed for one base, 
           and we cannot  use it  to  get  a computable  absolutely normal  number.  But  such 
           a number (whose 6-ary representation is a computable normal sequence for every 
           base n) exists.  This observation was made long ago by Turing in his unpublished 
           notes; see [8].
               260  Prove that a computable absolutely normal number exist.
              (Hint:  Numbers that are not normal in base 6, form a Schnorr effectively null 
           set;  this  is  true  for  all  b,  and  the union of these sets is  also  a Schnorr effectively 
           null set, so there exists a computable point outside it.)
              Unlike  normal  sequences,  Mises-Church  random  sequences  cannot  be  com­
           putable for obvious reasons  (otherwise we can select a sequence of zeros or a se­
           quence of ones by a computable rule).  Moreover, the following statement is true:
              Theorem 169.  For every total algorithm that gets bits of a sequence from left to 
           right and predicts the next bit before getting it, the fraction of successful predictions 
           for a Mises-Church random sequence tends to 1/2.
              Proof.  Indeed,  a (total)  algorithm that makes predictions can be converted 
           into two selection rules:  one selects the terms where the algorithm predicts zero, the 
           other selects the terms where the algorithm predicts one.  So our sequence is split 
           into a  “mixture”  of two subsequences,  and Mises-Church randomness guarantees 
           that each of the two sequences is  balanced  (or finite,  but  then  the statement  is 
           trivial).  So the fraction of successful predictions for each subsequence tends to 1/2; 
           so the total fraction of successful predictions tends to 1/2.         □
              This statement can be generalized further.  Consider the following game:  before 
           the next  term of the sequence appears,  we may make a bet on zero or one;  the 
           amount of a bet is a rational number in [0,1].  If our guess is correct, we get the 
           doubled amount;  if not,  we lose the money.  A strategy in a game of this type is 
           a function S whose arguments are binary strings  (the bits already disclosed)  and 
           the values are rational numbers in  [—1,1].  The positive values mean that we bet 
           on 0, the negative values mean that we bet on 1.  The total gain of the strategy S 
           playing with the initial segment шо • • • шп-1 is then
                                   71—1 
                                   i= 0
           the negative values correspond to our loss (in this game we can go below zero).
                                          9.4.  VILLE’S  EXAMPLE                          267
                Theorem 170.  Let S be a total computable strategy of this type,  and let ui be a 
            Mises-Church random sequence.  Then the gain of S playing against ui  is o(n)  after 
            n steps.
                P r o o f.  Assume that the strategy may have only values 1 and —1.  Then it is 
            essentially equivalent to guessing the next bit.  We already know that the fraction 
            of successful guesses  tends  to  1/2,  and  this  means that  the  average gain per  bit 
            tends to 0.
                Now consider more general strategies whose values  are rational numbers be­
            tween — 1 and 1 with denominator к (i.e., multiples of 1 /к), for some fixed k.  Every 
            strategy S of this type can be considered as an average of 2к strategies with values 
            — 1  and  1  only,  and the gain of S after n bits is at most en,  if n is large enough. 
            We know that for each of them the gain is o(n), so the average is also o(n).
                Finally, we consider an arbitrary strategy with rational values.  For each e > 0 
            we need to prove that the gain of S after n bits is at most en, if n is large enough. 
            So let us fix some e.  Choose к in such a way that  1/k < e, and approximate S by 
            a strategy S' whose values are multiples of 1/k (taking the closest multiple).  The 
            approximation error is bounded by e/2.  For S' we already know that its gain is 
            o(n), so it is less than (e/2)n for large enough n, and the difference between gains 
            of S and S' is at most (e/2)n.                                                 □
                Here is one more property of Mises-Church random sequences.  (It was men­
            tioned by Mises as one of the basic property of Kollektivs.)
                Theorem 171.  Applying a Church-admissible selection rule to a Mises-Church 
            random sequence,  we get either a finite sequence or a Mises-Church random one.
                P r o o f.  It is easy to see that the composition of two Church-admissible selec­
            tion rules is a selection rule of the same type.  If we select some terms (by looking at 
            the previous ones) into a subsequence and then again select some terms of these sub­
            sequence looking at the previous ones, the resulting decision for some uji  (whether 
            it will survive the first and the second selection or not) is determined by ojo • • • u>i_i. 
            (And the composition of two computable selection rules is computable.)         □
                Later (Section 9.12, p. 291) we consider more general selection rules (non-mono­
            tonic ones) and modify accordingly the notion of randomness (the so-called Mises- 
            Kolmogorov-Loveland randomness or Kolmogorov-Loveland stochasticity).  This 
            new class of selection rules will  not be closed under composition,  and,  moreover, 
            the  corresponding notion  of randomness  in  not  closed  under selection rules  (see 
            Theorem 203, p. 307.)
                We have not discussed yet the relation between Martin-Löf randomness and 
            Mises-Church randomness.  As we will see soon,  they differ,  and  not  all Mises- 
            Church random sequences  are  Martin-Löf random.  But  first  let  us  make  some 
            remarks about Mises’ definition.
                                         9.4.  Ville’s example
                We have seen already that for every countable family of sets Ri there exists a 
            sequence that satisfies the frequency stability property with respect to all selection 
            rules Sri  (each of these rules selects a finite or balanced subsequence).  Indeed, the 
            set  of sequences with these properties has measure 1.  This is an existence proof;
             268           9.  FREQUENCY  AND  GAME APPROACHES TO  RANDOMNESS
             can we give a more explicit construction of such a sequence? Indeed this is possible, 
             and we now explain such a construction following A. Wald  [217],  J.  Ville  [206], 
             and D. Loveland [106].
                 Let  us  first  consider  the  case  when  there  is  only  one  selection  rule  Sr  that 
             corresponds to some set R.  Then it is easy to construct  a sequence ш such that 
             Sr(oo) = 01010101 • • •  (zeros and ones alternate), so Sr(oo) is balanced.  Indeed, we 
             construct ш from left to right.  When the rule Sr informs us that it intends to select 
             the next term, we look at the number of this term in the subsequence (whether it 
             is even or odd) and choose the next term to be 0 or 1 depending on this number. 
             (The terms of ш that are not selected by Sr can be chosen arbitrarily.)
                 Now assume that we have m sets Ri,..., Rm  that define selection rules.  We 
             want to construct a sequence и such that Sr^cj) is finite or balanced for each Ri. 
             Again we construct the sequence from left to right.  Before we choose the value of 
             the next term oon, let us apply all the rules to the previous terms and see which of 
             the rules SRi  will select un.  We get a ?n-bit vector, so we can classify the terms 
             of и into 2m classes depending on this vector, even before the value of the term is 
             chosen.  The sequence со, therefore, is a mixture of 2m interleaving sequences (some 
             of them may be finite).
                 We have not said yet  how we construct to.  We use the following rule.  All 
             the  2m  subsequences  (corresponding to  2m  values of the m-bit  vector)  should be 
             01010101 • • •.  This  can  be  achieved  in  a  unique  way:  Before  oon  is  chosen,  we 
             know ojq ■ • ■ ojn—i,  and  we  know  which  rules will select  con,  so  we  know  in which 
             subsequences is con and we can choose its value.
                 Note that Sr^ co) is a mixture of 2m~1 subsequences (that correspond to 2m~1 
             bit vectors that have 1 at position г).  Therefore Sr^ co) is balanced; moreover, we 
             can guarantee that in each prefix of Sr^oo) the number of ones does not exceed the 
             number of zeros, and the difference is bounded by 2m~1  (one for each subsequence).
                 Now we switch to the general case of countably many rules R{.  The main idea 
             is that we add these rules one by one, and at each moment deal with finitely many 
             rules.  If we do it  slowly,  the transition effects are negligible,  and every selection 
             rule selects a balanced subsequence.
                 If this  is  not  convincing,  here  are the  details.  Assume that we have already 
             constructed some prefix ooo ■ ■ -ojn-1  of the sequence to.  Then it is already known 
             which rules Sr1  will select  the next  term oon  (while the value of con  is  yet  to be 
             determined).  This  information  is  now  not  an  m-bit  vector,  but  an  infinite  bit 
             sequence iqiq---  (where щ  —  1  if Srï  selects the next term).  We consider the 
             sequence U\U2 ‘ ■ ■  as a path in an infinite binary tree.
                 Fix  some increasing sequence ko  <  к±  <  k,2  <      of positive  integers.  We 
             assume that it grows fast enough; for example, we may let кг = 22г.  At each step 
             of the construction (for each term cjn) one of the tree vertices will be declared as 
             active.  Namely,  following  the path  щщ - ■ ■,  we  select  the  first  vertex  that  was 
             active fewer than ki times, where г is the height of this vertex,  and we declare it 
             as  active.  In  other  words,  an  active vertex  (at  the step when con  is chosen)  is  a 
             shortest string x such that
                    •  the ith bit of x is 1 if and only if 5д;  selects wn;
                    •  at previous steps of the construction (when ooo ■ • -u;n_i was constructed), 
                      the vertex x was active fewer than кцх) times.
                                               9.4.  VILLE’S  EXAMPLE                                269
             So first the root is active, until it happens ko times.  Then 0 or 1 is active (depending 
             on whether the rule S ^  selects the next term or not) until they become “tired” of 
             being active ki times, etc.
                  In  this  way we construct  a sequence coocoi ■ • •  that  is a mixture of countably 
             many finite subsequences that correspond to countably many possible active ver­
             tices.  The subsequence that corresponds to vertex x (the terms constructed at the 
             steps  when x  was  active)  has  length  at  most  кцх)  (but  it  may  be  shorter).  As 
             before,  we  choose u)n  in such  a way  that  all  these  subsequences  are  of the  form 
             010101 •••.
                  Let  us  look  at  the  subsequence selected  from cj  by       and show that  it  is 
             balanced (or finite).  Now the situation is a bit more complicated:  First, we have 
             countably many subsequences, and second, the rule Sд. was initially ignored (when 
             the active vertices were shorter than i).  Let us look at the subsequence selected 
             by SR.  more closely.  It consists of terms of two types.  First, there are some terms 
             that correspond to active vertices of height less than i, so          was not taken into 
             account.  Second,  Sr.  includes  all  the  terms  that  correspond  to  active  vertices 
             where the ?th bit equals  1.  The number of terms of the first type is bounded by 
             2°ko + • • • + 2l~lki-\, so we can safely ignore them.
                  As  for  the  terms  of the  second  type,  note  that  for  every  active  vertex  the 
             subsequence  corresponding to  this  vertex  is  010101 • • •,  and  each  of its  prefixes 
             contains no more ones than zeros, and the difference is at most 1.  So the imbalance 
             in the selected subsequence (if we ignore terms of the first type) at some moment 
             t  is  bounded by the number of active vertices appearing at that moment.  Let N 
             be the maximal height of the active vertices used before £; we assume that N ^ i 
             (otherwise there is no term of the second type).  Then at most 0(2N) active vertices 
             were used, and the imbalance is at most 0(2^).  On the other hand, since the vertex 
             of height N became active, the preceding active vertices should be used completely, 
             so the length of the sequence is at least км-1-  It remains to use that 2N = о(км-i).
                  So we have described an explicit construction of a sequence that has the fre­
             quency stability property with respect to a given countable family of selection rules. 
             Does it  give something really new when compared to the probabilistic existence 
             proof?  Yes.  For example, we may note that in this sequence each prefix contains 
             at least as many zeros as ones, since this is true for all the 010101 ■ • ■  pieces.  So we 
             have proved the following result:
                  Theorem  172  (Ville’s  example).  There  exists  a  Mises-Church  random  se­
             quence where each prefix contains at least as many zeros as ones.
                  (We can also get a sequence whose prefixes have strictly more zeros than ones 
             just by starting with first bit 1 and then using the construction.)
                  This result can be used to prove that there exists a Mises-Church random se­
             quence that is not  ML-random.  For that it would be enough to prove that this 
             property  (more zeros than ones)  is not  possible for an ML-random sequence.  It 
             is  indeed that  the case,  and it  is  a consequence of the Effective Law of the Iter­
             ated Logarithm—but,  unfortunately,  not  the part  that  we proved in Section 8.4 
             (Theorem 156).
                   261        Prove that in this case we do not really need the effective version:  If the 
             set of sequences that have more zeros than ones in all prefixes is a null set, then it 
             is an effectively null set.
             270           9.  FREQUENCY  AND  GAME  APPROACHES TO  RANDOMNESS
                 {Hint\  Let pn be the probability of the event  “up to length n all prefixes have 
             more zeros that ones”.  The sequence pn is a decreasing computable sequence, and 
             its limit is the measure of the set in question.  So if this limit is 0, for a given e > 0 
             we can wait until pn becomes less than e.  One can also refer to the results about 
             Kurtz randomness (p.70).)
                  262        Prove that the set of sequences that contain more zeros than ones in all 
             prefixes in an effectively null set, not referring to the Law of the Iterated Logarithm.
                 (Hint :  For every n the probability of the event “the n-bit prefix has more zeros 
             than ones”  is about 1/2.  If we take a sequence of values of n that grow fast, these 
             events will be almost independent (the deviations for the short prefixes is negligible 
             compared to the expected deviation for the long prefixes).)
                 We do not provide the details of this argument here.  Instead,  we prove in a 
             different way that there exist Mises-Church random sequences that are not Martin- 
             Löf random.  Namely, we show that there exists a Mises-Church random sequence 
             whose prefixes have logarithmic complexity, using the same explicit construction.
                 Theorem  173.  There  exists  a Mises-Church random sequence d = l)qL)\ • • • 
             such that
                                          C{u)0 • • -wn_ i) = О (log n).
                 PROOF.  To construct such a sequence, we apply our construction to the count­
             able list of all Church-admissible selection rules:  The sets R{ are all decidable sets 
             of strings.  This is not an effective construction, since we cannot enumerate all de­
             cidable sets (all total algorithms)—this is not a surprise, otherwise we would get a 
             computable Mises-Church random sequence!
                 We can enumerate all programs, but then we need some extra advice:  Some­
             body should tell us which of the programs define decidable sets (so we can replace 
             the bad ones by some fixed decidable set).  This information for the first m. programs 
             takes m bits (one bit per program), and it is enough to perform our construction 
             until we reach active vertices of height  (length) m.  At that moment we have con­
             structed at least km-\  = 22m-2 bits of the sequence.  So the amount of additional 
             information (advice) is logarithmic in the length of the prefix.                    □
                 Let us repeat again the important corollary of this result:
                 Theorem  174.  There  exists  a  Mises-Church  random  sequence  that  is  not 
             Martin-Löf random {with, respect to the uniform measure).
                 If the Mises-Church definition is too weak, maybe we should make it stronger? 
             For example, one can consider a broader class of selection rules or a different type of 
             gambling.  In the following sections of this chapter we consider some generalizations 
             that involve non-monotonic rules (the order of terms in the subsequence is not the 
             same as in the entire sequence)  and  martingales  (where we start playing with a 
             fixed amount and can bet all the money we have).
                                              9.5.  Martingales
                 When discussing why Kollektivs exist, we referred to gambling practice.  But 
             from the practical viewpoint our gambling framework looks quite unnatural:  a gam­
             bler comes to a casino where a fair coin is tossed, he selects some of the bits (before 
             they are produced) and then  “wins”  (discredits the casino’s source of randomness) 
             if the selected outcomes are imbalanced (do not have limit frequency 1/2).
                           9.5.  MARTINGALES          271
          As we have said  (trying to  make the game more natural),  we get  the same 
       definition  if we  allow  the gambler to  make a bet  of fixed size  at  some  moments 
        (having the unlimited credit needed for long sequences of losses),  and we require 
       that the gambler’s average gain (per game) tends to zero as the number of games 
       increases.  One can also allow variable bets of bounded size to be made; see above.
          J.  Ville suggested another setting that looks more natural.  Here the gambler 
       comes to the casino with some fixed amount of money,  say $1.  Before a coin is 
       tossed,  the gambler splits the capital into  two parts:  the first  is used to make a 
       bet on 0, and the second is used to make a bet on 1.  One of the bets is successful; 
       the corresponding amount is doubled (and the other part is lost).  For example, a 
       cautious gambler may split the current amount into two equal parts, then one is lost 
       and the other one is doubled, so the capital remains unchanged.  (It is clear therefore 
       that a special option to leave some part of the money aside is not necessary, it can 
       be emulated anyway.)  Now we cannot go into negative,  so both parts should be 
       non-negative numbers.
          After a game is described, it is clear how the notion of a gambler’s strategy in 
       this game should be defined:  a strategy is a function that maps the history of the 
       game (the sequence of already seen bits)  to the next move  (how much should be 
       bet on 0 and on  1).  In fact, we will use more a convenient  representation of the 
       strategy:  let m(x) be the gambler’s capital after playing with x (if she follows the 
       strategy).  This  (non-negative)  function  determines the strategy uniquely:  After 
       seeing x,  we  bet  m(:r0)/2  on  0  and  m(x 1)  on  1.  Not  all non-negative  functions 
       correspond to strategies; two conditions are necessary (and sufficient):
           •  m{A) — 1  (as we agreed, the initial capital when we observed the empty 
             string A equals 1);
           •  m(x) — (m,(x0) + m(x 1 ))/2  (the sum of bets on both outcomes is equal 
             to the current capital).
          A non-negative function m that has both properties is called a martingale with 
       respect to  the uniform measure on the  Cantor space.  Later we also consider mar­
       tingales with respect to other measures on the Cantor space.  In probability theory 
       a more general notion of martingale is used, but for our purposes this will be suffi­
       cient.  So from now on we speak mostly about martingales instead of corresponding 
       strategies.
          Let v be an arbitrary measure on the Cantor space.  It is easy to check that 
       the ratio v(Ctx)/p(Q,x)  (here p is the uniform measure on the same space and Ltx 
       is the set of all extensions of x)  is a martingale, and every martingale is obtained 
       in this way from some measure.
          263 Show that this is indeed the case.
          The following intuitively obvious statement is sometimes called the Doob  in­
       equality or Kolmogorov inequality.
          Theorem 175.  Let m be some martingale, and let к be some positive threshold. 
       Consider the set of strings where the martingale exceeds k,  and consider all infinite 
       sequences that have a prefix in this set.  Then the  (uniform)  measure of the set of 
       all these sequences does not exceed 1/k.
          Proof.  Let us follow the strategy that corresponds to m, but when the capital 
       achieves к (or more), we stop playing and go home.  This modified strategy has an
              272            9.  FREQUENCY  AND  GAME APPROACHES TO  RANDOMNESS
              expected return of at most 1  (since the game is fair), so that the probability that 
              we achieve к or more is at most l/к.
                   To make this argument formal, consider the corresponding measures.  Assume 
              that m(x) is u(Qx)//j,(Ctx) for some measure v (and p is the uniform measure).  We 
              consider vertices (strings) x such that the ^-measure of cone Гlx is к (or more) times 
              bigger than the /^-measure of the same cone.  Now consider only the minimal strings 
              x with this property (this does not change the union of Qx).  They correspond to 
              disjoint  cones.  The total /^-measure of these cones is  к  times smaller than their 
              total    measure (or even smaller), and the latter is at most 1.                            □
                    264 Prove that for a lower semicomputable martingale m the function
                                                    t(u))  =  sup m(x)
              is a probability-bounded randomness test in the sense of Section 3.5.
                   We have seen that the set where the martingale wins a lot has small measure. 
              The reverse statement is also true:  for every set S of small measure, there exists a 
              strategy (martingale) that wins a lot on every sequence in S.
                   Theorem 176.  Let U c Q  be an open set of measure e > 0.  Then there exists 
              a martingale m with the following property:  Each sequence id €  U  has  a prefix x 
              where m(x) ^ l/e.
                   PROOF.  Consider a measure v such that v(X) = p(X П U)/e.  (This measure 
              is  zero  outside  [/,  and it  is  (l/e)  times the uniform measure inside U.)  Then the 
              function m(x) — u(Qx) / p(Qx) is a martingale with the required properties.  Indeed, 
              if id Ell, there exists a prefix ж of id such that Qx CU and m{x) = l/e.                    □
                  This  theorem  can  be  explained  as  follows.  Imagine  the there  are  dishonest 
              people in the casino’s management who are ready to sell some “insider information”. 
              Namely, they specify some open set U and guarantee that the sequence of future coin 
              tossing (due to cheating) is in U.  What is the  “market value”  of this information 
              (together with the option to start the game with initial capital  1  in the casino)? 
              Our theorem says that it is l/p(U).  For example, if the insiders tell us (in advance) 
              the first N bits, the corresponding open set has measure 1/2N, and indeed we can 
              win 2N  by betting all the money on the known outcome for N first games.  The 
              same is true for more complicated types of cheating.  For example, if the insiders tell 
              us that some outcome is not possible (“In our casino we never have N consecutive 
              zeros after opening”), this is still something valuable (this information allows the 
              gambler to make 2N/(2N — 1) dollars out of 1).
                  The proof of the theorem also can be explained easily in these terms.  If at the 
              first step the set U is split between По and Hi proportional to ao : a\, we split our 
              money in the same proportion.  (For example, if all elements of U start with 0, we 
              bet all the money on 0.)  Then the ratio
                                                    current capital
                           the fraction of U among the extensions of current situation
              does not change during the game.  Initially the numerator is 1 and the denominator 
              is £  (or even less).  When we bump into U (and this will surely happen, unless the 
              insiders  sold us  false  information),  the  denominator  is  1,  so the numerator is  at 
              least  l/e.
                                           9.5.  MARTINGALES                             273
                Similar  statements  are  true  for  the  limit  behavior  of a  martingale.  Let  us 
            say that a martingale m wins on a sequence oj  if the values of m on the prefixes 
            of oj  are not  bounded.  The following result,  discovered by Ville  (who introduced 
            martingales), was one of his main motivations.
                Theorem 177.  (a) Let m be a martingale.  Then the set of sequences on which 
            m wins has measure 0.
                (b)  Let X  be  a set of measure 0.  Then there  exists  a martingale m  that wins 
            on all elements of X.
                Proof,  (a)  The set  Uk  of sequences,  where m reaches the value к or more, 
            has measure at most l/к and is open; all sequences where m wins belong to Uk for 
            every к.
                (b) For every к consider the open set Uk of measure at most 1 /к that contains 
            X, and the corresponding martingale     that achieves к or more on all elements of 
            Uk-  Now we need to combine these martingales into one.  Note that the weighted 
            sum of martingales is a martingale (we may split the capital into pieces and use a 
            separate strategy for every piece).  Let us use martingale 7774«  with initial capital 
            (weight) 2~n for all n in parallel  (note that  1 = J2n 2~n)-  Then for all sequences 
            that  belong to  £/4n,  we guarantee  a return of 4n • 2~n  =  2n  (plus,  maybe,  some 
            return from other investment strategies).  So the return on every element of X is 
            infinite.                                                                     □
                The  proof of this  result  is  similar  to  the  proof of the  randomness  criterion 
            (Theorem 90, p.  146); we can say that we now have proved the classical version of 
            Theorem 90 by the same argument.
                In fact we have proved a bit more that was promised.  Let us say that m strongly 
            wins on the sequence oj  if its values on the prefixes of oj  are not only unbounded 
            but have limit +00.  In the proof of Theorem 177 we have constructed a martingale 
            that strongly wins on all elements of X.  (Indeed, the martingale constructed in the 
            proof of Theorem 176 is at least \ je on all sufficiently long prefixes.)
                Again,  Theorem 93 on p.  149 can be considered as a constructive version of 
            this stronger result.  (We will discuss later the connection between randomness and 
            effective versions of the martingale notions.)
                Combining these observations, we get the following corollary:
                Theorem 178.  For every martingale m there exists  (another)  martingale m! 
            that strongly wins on all sequences where m wins.
                Proof.  As we have noted, we can obtain m! going to sets and back.  There is 
            also a very intuitive direct construction.  The martingale m! should behave like a 
            wise stock market player:  when it achieves capital 2 (using the m-strategy), it puts 
            aside half of its money as a safety measure (i.e., this part of the money is bet on 
            0 and 1 in equal parts), and the other half is used according to m (but with twice 
            smaller amounts).  When the capital reaches 4 (i.e., when m would reach 8), again 
            the half (2) is saved, and the rest is used for playing, etc.
                Here is another version of the argument  (which is better if we want  to keep 
            the martingales enumerable from below):  For each martingale m and each number 
            c  >  0,  consider the martingale mc which imitates m while the capital is smaller 
            than c, and then stops.  Then the limit of mc is at least c on every sequence where 
            m reaches c at least once.  It remains to take the weighted sum of m±k with weights 
            2~k.                                                                         □
            274         9.  FREQUENCY  AND  GAME  APPROACHES  TO  RANDOMNESS
                Choosing the weight more carefully, one may prove the following general state­
            ment [171, 47].  Let /  :  [1, +oo) -> [0, -boo) be a non-decreasing continuous function 
            such that Jj00 fit) ft2 dt ^ 1.  Then for every martingale m  there exists a martingale 
            m! with the following property:  If at some sequence ш the martingale m reaches c 
            at some moment, then m! reaches /(c) at the same moment and never goes below 
            /(c) later.  (The integral bound in the condition is sharp.)
                Up to now we assumed that the coin is symmetric—the probabilities of heads 
            and  tails  declared  by  the  casino  are equal.  This  means  that  in  a fair  game  the 
            bets on zeros  and ones should  be doubled.  But  we can consider other settings. 
            Imagine that casino claims that 0 appears with probability 1/3 and 1 appears with 
            probability  2/3.  To  make  the game  rules  consistent  with  this  claim,  the  casino 
            should return bets on 0 multiplied by 3, and bets on 1 multiplied only by 1.5.  The 
            definition of the martingale changes accordingly:  m(x), the capital after x, should 
            be equal to the sum of its two parts—the amount bet on 0 equals m{x0)/3,  and 
            the amount bet on 1 equals to 2m(xl)/3.  So we get the condition
                                              1         2
                                      m(x) = - m ix0) + -m {x 1),
            which can also be read as “the capital before the next game is equal to the expected 
            capital after it”.
                Let us now give a formal definition.  Let 7Г be an arbitrary probability distri­
            bution on f2 (informally, the casino claims that the coin behaves according to it). 
            The corresponding function on strings will also be denoted by 7Г, so 7г(х) = тг(0,х).
                A non-negative function m on binary strings is called a martingale with respect 
            to 7Г  (with initial capital 1) if m(A) = 1 and
                                т(х)тг(х) = т(х0)тг{х0) + т(а;1)7г(а:1)
            for all x.  (This definition corresponds to the informal discussion above:  Dividing 
            the equation by 7г(ж), we get conditional probabilities тг(х0)/тг(х) and 7г(ж1)/7г(:г) 
            of 0 and 1 after x.)
               In other words, we require that the function т[х)тт[х) is a measure, so a mar­
           tingale with respect to 7Г (or a ir-martingale) is just a ratio of some other measure 
            and 7Г.  Now we can extend the results above (essentially with the same proofs) to 
           the case of arbitrary measures:
                (1)  Let m  be a 7r-martingale, and let к be some threshold.  The ^-probability 
           of the event  “m reaches к on some prefix of ш"  is at most 1 /к.
                (2)  For every open set U there exists a martingale that reaches 1/tt(U) on all 
           elements of U.
                (3)  A set A is a 7r-null set if and only if there exists a 7r-martingale that wins 
            (or strongly wins) on all elements of X.
               The Doob-Kolmogorov inequality guarantees that every martingale is bounded 
           almost everywhere.  The following stronger statement (Doob’s theorem) is also true:
               Theorem 179.  For every n-martingale m for тг-almost every sequence ш,  the 
            values  of m   on prefixes  of ш  have  a finite  limit.
                (In o th e r   w o r d s ,  t h e   s e t   o f   s e q u e n c e s   ш,  w h e r e   m   d o e s   n o t   h a v e   a   f in it e   lim it , 
           i s   a   7 T -n u ll  s e t . )
                         9.6.  A  DIGRESSION:  MARTINGALES  IN  PROBABILITY  THEORY          275
                 Proof.  Kolmogorov’s inequality guarantees that m is bounded on prefixes of 
            u) with probability 1.  So it remains to prove that for every rational p, q such that 
            0 < p < q, the following event has probability zero:  “The capital on prefixes of ш 
            oscillates becoming less than p infinitely often and greater than q infinitely often.” 
            To show this, we consider another martingale m' that is unbounded at all sequences 
            to where the oscillations happen.  The martingale m' implements the classical “buy 
            low—sell  high”  strategy:  it  looks  at  the  capital  of the  original  martingale,  but 
            keeps its own capital unchanged until m becomes less than p.  Then m!  behaves 
            like m (with some constant factor) until m reaches capital greater than q\ then in! 
            again keeps the capital unchanged until m goes below p, etc.  At each iteration m' 
            increases its capital by factor  (q/p), so it tends to infinity for sequences where m 
            oscillates.                                                                       □
                This theorem can be used to define conditional probabilities.  Consider some 
            measure pi on the product flxfl.  Then we can consider pi, which is the projection 
            of p at the first coordinate (the marginal distribution).  We also want to define the 
            conditional distribution of the second coordinate when the first coordinate is equal 
            to some öéQ. We cannot use the elementary definition of conditional probability 
            with some event  as  a condition since the event  “the first  coordinate  is  equal  to 
            a”  often has zero probability.  Usually the conditional probability is defined  (for 
            //x-almost every a)  using the Radon-Nikodym derivative, but in our case we can 
            give a more concrete definition using the Doob theorem.
                Let  A  be some property of the second coordinate.  Consider  the conditional 
            probability of A with the condition “the first coordinate has prefix a = ao ■ • • a^-i”. 
            For a fixed A this probability (as a function of a) is a pi-martingale (up to a con­
            stant), so the Doob theorem guarantees that for p\-almost every a these probabili­
            ties converge to some limit.  This limit (defined pi-almost everywhere) is called the 
            conditional probability of A when the first coordinate is equal to a.  (It is possi­
            ble that some prefix has probability 0, and then the conditional probability is not 
            defined, but this creates problems only for a set of measure 0.)
                The advantage of this construction is that it allows us to define the conditional 
            probability for computable measure p on fl x Q and every ML-random (with respect 
            to pi) sequence.  See [7] for details.
                Returning to our main topic, we conclude this section with the following (evi­
            dent) observation:
                Theorem 180.  For every martingale there exists a sequence on which it does 
            not win  (and,  moreover,  is bounded by 1  on all prefixes).
                Proof.  The definition of martingale implies that one of the numbers m(x0) 
            or m(x 1) does not exceed m(x), so to each x we can add one bit not increasing the 
            value of the martingale.                                                          □
                (If the casino can choose the outcome of coin tossing after the gambler makes 
            a bet, it can guarantee that the gambler never wins anything.)
                       9.6.  A digression:  Martingales in probability theory
                Theorem 177 can be interpreted as follows:
                (a)      to  prove  that  some  set  has  //-measure  0,  it  is  enough  to  construct  a p- 
            martingale that wins on all its elements;
        276     9.  FREQUENCY AND GAME APPROACHES TO RANDOMNESS
           (b)  this method can be applied to every null set (by finding a suitable martin­
        gale).
          This interpretation is important for two reasons.  First, from a purely technical 
        viewpoint, we get a tool to prove that some set has measure zero (by constructing 
        a martingale that wins on all its elements).
          To illustrate the point, let us present in this style the proof of the Strong Law 
        of Large Numbers.  Let ß = Bp, the Bernoulli distribution with independent trials 
        and success probability p.
          For  a  given  q  > p,  let  us  prove  that  the  Bp-probability  of the  event  “the 
        frequency of ones exceeds  q infinitely many times”  is  zero.  (For  q < p and the 
        event  “the  frequency of ones falls below q infinitely many times”  the arguments 
        are similar.)  To achieve this,  consider £ p-martingale Bq/Bp.  For a sequence z of 
        length n, where the frequency of ones is r  (with nr ones and n(l — r) zeros), the 
        value of this martingale is
                            qnr(l — g)n(1~r) 
                            pnr(l — p)n(1~r) ’ 
        and the logarithm of this value is
              n[(r\ogq + (1 -r)log(l -  q)) - (rlogp + (1 - r) log(l -p))].
        Since q > p, the latter expression is an increasing linear function of r\ the coefficient 
        is log[q/p] + log[(l —p)/{ 1 — g)] and both terms are positive.  So for r > q (the case 
        we are interested in), we can only decrease this expression replacing r by q, so the 
        logarithm of the martingale value is
              n[{qlogg + (i -  g) bg(i -  g)) -  (giogp + (i -  g) iog(i -p))].
        The Gibbs inequality (p. 215) guarantees that the expression in the square brackets, 
        the  Kullback-Leibler  distance  between  the  distribution  (g, 1  — g),  (p, 1  — p),  is 
        positive.  So the martingale is unbounded on the sequences where frequency exceeds 
        q infinitely often.
          This proof of the SLLN does not follow completely the scheme outlined above: 
        We consider not one martingale but a family of martingales (one for each g).  Each 
        of the martingales is used to prove that some set has measure zero,  and then we 
        observe that the countable union of null sets is a null set.
          Instead, we could take a countable family of g,-  (say,  all rational g), construct 
        a martingale for each g7, and then mix all these martingales with positive weights. 
        If some of the martingales are infinite, the mix will be infinite, too.
          Essentially  the same proof of the SLLN was discussed in Section 3.2  (Prob­
        lem 67, p. 58), but there we considered finite sequences and did not use the term 
        martingale speaking just about the ratio of two measures.  (Similar arguments will 
        be used later in Section 9.13.)
          The second reason why martingales are important is more philosophical.  What 
        do we do when we prove some theorem using the martingale approach? We consider 
        some property L of binary sequences ( “to be balanced”  for the case of the SLLN) 
        and some martingale m.  Then we prove that for every binary sequence oj at least 
        one of two things happens:
            •  the sequence oj has the property L\
            •  the martingale m wins on oj.
                            9.7.  LOWER SEMICOMPUTABLE MARTINGALES              277
           Moving in this direction, one can suggest the following market (or game) approach 
           to the notion of randomness and say that
                   the  randomness  of a bit sequence is not the property of the se­
                   quence but the type of insurance provided for this sequence.
              It sounds a bit strange at first, but it still makes sense.  Imagine a shop where we 
           can pay $1 in exchange for a bit sequence written on a scratch card.. The sequence 
           is guaranteed to be random:  The seller guarantees that our martingale (its copy is 
           given to the seller in a sealed envelope) will not win much on that sequence.  More 
           precisely, we discover the bits on the card one by one (from left to right),  and at 
           every moment we may get back the value of martingale on the currently discovered 
           bits.
              In other words, we come to the shop with $1  and a description of some mar­
           tingale in a sealed envelope.  Giving the money and the envelope to the seller, we 
           get  in exchange the scratch card with an  (infinite)  bit sequence.  Then we reveal 
           the bits on the card sequentially, and at every moment  (at our discretion) we can 
           get m dollars as a refund, where m is the value of our martingale on the sequence 
           of bits that we have read.  (After that the seller has no other obligations.)
              Note  that  it  is  important  that  we  do  not  see  the  next  bits.  Otherwise  we 
           could cheat—if the next bits decrease the martingale, we demand the refund now, 
           otherwise we wait for a better refund.
              Buying the random bits from such a seller, we may hedge the risks of getting a 
           “bad” sequence of random bits.  If we have a randomized algorithm that works fast 
           for most values of random bits and we were unlucky and bought a bit sequence that 
           makes it work long, then we can at least get some refund according the martingale 
           (it  was  carefully  chosen when we made the purchase—this martingale should be 
           large on rare sequences that make the algorithm work long).  So we need to deposit 
           different martingales depending on the future use of the sequence.  For example, if 
           we use the sequence in the probabilistic algorithm that generates large primes (i.e., 
           produces a large prime number with high probability),  the martingale should be 
           large on random sequences that lead to composite numbers.  Then, if we lose some 
           money because of the nonprimality of the generated number, we at least can get a 
           refund from the randomness provider.
              To  make the  story  more  realistic,  one  should  consider  finite  sequences,  but 
           the scheme remains the same.  Also note that the parties should agree about the 
           measure on bit sequences when making a deal (because the notion of a martingale 
           depends on it).  According to this philosophy,  one may say that the probability 
           distribution does not exist anywhere in the real world, but is a part of the contract. 
           (However,  a wise  seller  would  take  into  account  this  part  of the  contract  when 
           producing the sequence for sale.)
              This  approach  to  probability  theory  is  discussed  thoroughly  in  the  book  of 
           V. Vovk and G. Shafer [172].
                          9.7.  Lower semicomputable martingales
              The results about  martingales proven above have a natural effective version. 
           We already have studied the notion of effective null sets.  Since null sets are related 
           to martingales, one could expect that effectively null sets correspond to some class 
           of martingales.  This is indeed the case.
         278      9.  FREQUENCY  AND  GAME APPROACHES TO RANDOMNESS
            Fix some computable measure 7Г on Q.  In this section we consider martingales 
         and null sets with respect to 7r;  we do  not require now that the initial capital of 
         the martingale is  1.  Now consider lower semicomputable martingales in the sense 
         of Section 4.1  (a function m is lower semicomputable if the set of pairs (r, x), where 
         a rational number r is less than m(x), is enumerable).
           The following problem explains why we should not require the initial capital 
         to be 1.
            265  Show that a lower semicomputable martingale m with m(A) = 1 is always
         computable.
           The following effective version of Ville’s result  (Theorem  177)  was discovered 
         by C. Schnorr [166]:
           Theorem 181.  (a)  Let m be a lower semicomputable martingale.  Then the set 
         of sequences on which m wins is an effectively null set.
           (b)  Let X  be an effectively null set.  Then there exists a lower semicomputable 
         martingale m that wins on all sequences in X .
           P r o o f,  (a)  Since  m is  lower  semicomputable,  the set  of sequences where it 
         exceeds an integer к at  some prefix is effectively open and has measure at  most 
         l/k.  (Here we consider the measure of an open set that is a union of a computable 
         sequence of intervals.  As usual,  we should modify the sequence and make these 
         intervals disjoint.)
           (b) If a set is effectively open and has measure less than l/k, then the martingale 
         constructed in  the proof of Theorem  176  is  lower  semicomputable  (when  a new 
         interval appears, the approximation to the martingale increases).  One precaution 
        is necessary,  though:  we should divide the measure of the intersection not by the 
        measure of the set (it may be non-computable) but by its upper bound l/k (so we 
        should multiply the measure by k).  The root value of the martingale is then less 
        than 1, but this is allowed.
           It remains to sum up the martingales for different к with suitable (computable) 
        coefficients, as is done in the proof of Theorem 177.  Note that the sum will also be 
        semicomputable.                                         □
           This result can be strengthened in two directions.  First in the proof of (b) we 
        actually construct  a martingale that  strongly  wins on all sequences in X  (as we 
        have discussed).  In fact, we also can repeat the second proof of Theorem 178 and 
        convert a lower semicomputable martingale into another martingale which is also 
        lower semicomputable and strongly wins on all sequences where the first one wins.
           Second, we can extend the notion of martingale and consider lower semicom­
        putable  semimartingales,  also  called  supermartingales.  Supermartingales  corre­
        spond to games where the player at each step can donate some part of the capital. 
        The definition of a supermartingale requires that
                           m{x) ^ (m(x0) + m(xl))/2
        (for the uniform measure) or
                       m(x)7г(х) ^ m(x0)7r(x0) + m{xl)n{xl)
        (for arbitrary measure 7r)  instead of the corresponding equality.  Since the dona­
        tions can only decrease the capital, the upper bound for the probability of winning
                      9.8.  COMPUTABLE MARTINGALES    279
       remains the same,  so the proof of the part  (a)  still works.  And the part  (b)  be­
       comes only weaker, so we can replace martingales by supermatringales everywhere 
       in Theorem 181.
          It is clear that a 7r-supermartingale is just the ratio of some semimeasure and 
       7Г.  Since 7Г is computable,  lower semicomputable martingales correspond to lower 
       semicomputable semimeasures.  Therefore, we immediately see that there exists the 
       largest (up to O(l)-factor) supermartingale, and it is equal to
                          m(x) — a(x)l тг(х).
          This gives us a new proof of the Levin-Schnorr theorem in the version for an 
       a priori probability (Theorem 91, p. 148):  A sequence uo is ML-random with respect 
       to  a computable measure 7Г if and only if the ratio a(x)/ir(x)  is bounded for the 
       prefixes of co.
                      9.8.  Computable martingales
          The notion of a lower semicomputable martingale is rather unnatural from the 
       gambler’s point of view:  The proportion in which the capital is split between two 
       bets is then a ratio of two lower semicomputable reals, which is rather strange.
          Maybe we should consider only computable martingales?  Let us assume that 
       a computable measure 7Г on Cl is fixed and all the values 7г(а;) = тг(£1х) are strictly 
       positive  (this  is  important  since  these  values  are  in  the  denominators).  Then  a 
       computable martingale corresponds to a computable (in the natural sense) strategy 
       in the game.
          We say that a sequence со is  computably random with respect to 7Г if no com­
       putable 7r-martingale wins on it, i.e., every computable martingale is bounded on 
       its prefixes.  (The name  “computably random”  sounds a bit strange;  it would be 
       better to say something like “random with respect to computable martingales”, but 
       here we stick to the commonly used terminology even if it is not perfect.)
          266   (a) Show that we get an equivalent definition if we consider only martin­
       gales that are separated from zero; for example, we can consider only martingales 
       with values at least 1/2.
          (b)  Assume that 7г(^а;)  are positive rational numbers that can be computed 
       given X  (exactly).  Show that we get an equivalent definition if we consider only 
       martingales with rational values and require them to be exactly computable.
          {Hint:  (a) Take the average of a given martingale and the constant 1.  (b) If all 
       the values are separated from zero, we can approximate the proportions by rational 
       numbers, and it is easy to guarantee that the approximation error does not affect 
       the winning property.)
          How does the notion of a computably random sequence relate to other defini­
       tions of randomness?  The following theorem gives some answer to this question. 
       The first two statements are valid for every computable measure 7Г, while the two 
       following ones are for the uniform measure.  (They can be also stated for the case of 
       Bernoulli measure Bp with computable probability p\ the proof remains essentially 
       the same.)
          Theorem 182.  (a)  Every ML-random sequence is computably random.
          (b)  There exists a computably random sequence whose prefixes have logarithmic 
       complexity.  (So the previous statement cannot be reversed.)
          (c)  Evey computably random sequence is Mises-Church random.
        280     9.  FREQUENCY AND GAME APPROACHES TO RANDOMNESS
           (d)  Not every Mises-Church random sequence is computably random.
          Proof,  (a) We know from Theorem 181 that even lower semicomputable mar­
        tingales (not only computable ones) cannot win on an ML-random sequence.
           (b) We have already mentioned that for every martingale there exists a sequence 
        on  which  this  martingale  is  bounded  (we  should  go  in  the  direction  where  the 
        martingale does not increase).
          If a martingale is computable, one can find a computable sequence on which 
        this  martingale  is  bounded.  It  is  a bit  more  difficult—now  we  cannot  find  the 
        minimal value among m(x0) and m(x 1) since we can compute these numbers only 
        with some precision.  But this  is  in fact  not  needed.  It  is  enough  to  choose  an 
        extension where the martingale increases at most by  l/2n  (and this can be done 
        easily if the approximation errors on step n are small compared to l/2n).
          (An immediate corollary is  that the largest  computable  martingale does not 
        exist.  This is one of the main reasons to consider lower semicomputable martingales 
        and supermartingales.)
          But we need to continue the proof of (b).  The next step is to consider two com­
        putable martingales and find a computable sequence where both are bounded.  It is 
        easy to achieve.  Take a weighted sum (e.g., the average) of these two martingales; 
        it is a computable martingale, so we know already that there exists a computable 
        sequence where the  average martingale  is  bounded.  Then both martingales are 
        bounded (with twice bigger bound—recall that martingales are non-negative func­
        tions).
          A similar argument can be used to deal with a computable sequence of com­
        putable martingales (i.e., of programs for them).  Then we may mix all the martin­
        gales in a weighted sum with weight 2“! for 2th martingale.
          The problem is that there is no computable sequence that would include all 
        computable martingales (otherwise there would be a computable sequence on which 
        all  computable  martingales  are  bounded,  which  is  evidently  not  the  case—it  is 
        easy to win on a computable sequence).  So to construct a sequence ш such that 
        no computable martingale wins on it requires some non-algorithmic steps.  There 
        is  additional  information  that  allows  us  to  perform  this  construction:  For  each 
        program we should be informed whether this program computes a martingale, so 
        we require one bit of information per program.  To get a sequence with logarithmic 
        complexity,  we  should  use  this  information  in  a very economic way,  taking into 
        account the information about 2th program only after a long prefix of the sequence 
        (say, of length 2г, or even more) is constructed.
          Let us describe the construction in more detail.  At every step we have some 
        bit string X  (the bits already fixed) and some linear combination
                       mi (x) + £2m2(x) H------(- ekmk(x)
        with positive coefficients.  Here m*  is a martingale computed by 2th program  (or 
        some replacement martingale, or just zero, if the 2th program does not compute a 
        martingale according to the advice we got).  We maintain the invariant relation: this 
        combination is strictly less than 2.  (Initially, x = A, we have only one martingale 
        mi, and the combination is equal to 1.)
          As we already discussed, the string x can always be extended by one bit in such 
        a way that the expression remains less than 1  (and this can be done effectively as 
        we know the programs for martingales).  So we can extend x while keeping к (the
                                         9.8.  COMPUTABLE MARTINGALES                               281
              number of martingales involved)  unchanged.  On the other hand,  we may  (from 
              time to time) add a new term £^т^(ж) to this linear combination, choosing £k > 0 
              so small that the sum remains less than 2 (the closer the combination is to 2 and 
              the bigger is the value rrik(x)  for current ж, the smaller     should be).
                  In this way we get a sequence on which all rrii  are bounded, because each m; 
              appears in the sum (bounded by 2) with a positive coefficient (though maybe very 
             small one).
                  The decision  complexity of this sequence is bounded  by the number of used 
             advice  bits  and  can  grow  as  slow  as  we  want  (if we  add  new  martingales  only 
             rarely).  And the plain (or prefix) Kolmogorov complexity of the initial segments is 
             O(logn), as we promised.
                  (c) Recall that the SLLN says that the set of unbalanced sequences has measure 
             zero, and the corresponding martingale (that wins on all unbalanced sequences) can 
             be chosen to be computable (see Section 9.6 where we constructed martingales for 
             each threshold and then mixed them; it can be done in a computable way).
                  Moreover,  if R is  a set  and  Sr is  the  corresponding  selection  rule,  we  can 
             easily construct  a martingale  that  wins on every sequence uj  such that  Sr{oj) is 
             not balanced.  Indeed, the martingale should ignore the terms that are not selected 
             by Sr (keeping the capital unchanged) and use the martingale from the preceding 
             paragraph playing with the selected terms.
                  For computable R we get a computable martingale, so for every sequence that 
             is not Mises-Church random, we can find a computable martingale that wins on it 
             and thus proves that it is not computably random.
                  (d)  Consider a Mises-Church random sequence where each prefix contains as
             many zeros as ones (or more); see Theorem 172.  Let pn be the probability (with 
             respect  to the  uniform Bernoulli distribution)  that  all prefixes of length at most 
             n contain at least as many zeros as ones.  As we already discussed (Problems 261 
             and 262), the probabilities pn form a computable decreasing sequence that converges 
             to zero.  For each n we can computably find a martingale Mn that wins 1 fpn on 
             every sequence such that all prefixes up to length n contain at least as many zeros 
             as ones.  It remains to take a weighted sum of some Mn  (in such a way that  1 fpn 
             increases  faster  than  the  coefficients  decrease)  and  get  a computable  martingale 
             that is not bounded on the sequence we started with.                                    □
                   267  Give an explicit construction of a martingale used in the proof of (d).
                  (Hint:  Assume that we come to a casino knowing in advance that every prefix 
             of the  game  sequence  has  at  least  as  many  heads  as  tails.  Then  we  can  make 
             bets  of fixed  size  being  sure  that  we  never  run  out  of money.  If the  difference 
             between the number of heads and tails tends to infinity, this is the winning strategy 
             (martingale).  If it is not the case, there exists some moment t and some number I 
             such that, starting from t, the difference between heads and tails is at least I and 
             is equal to I infinitely often (liminf).  Then after t we can make a bet on tails when 
             the difference is I, and we always win.  So we get a martingale for the first case and 
             a family of martingales (with parameters t and I) for the second case; it remains to 
             combine them into one martingale.)
                  Note that the statements (b) and (c) imply that there exists a Mises-Church 
             random sequence with logarithmic complexity of prefixes.  In this way we get a new 
             proof of Theorem 173 (following [120]).
        282     9.  FREQUENCY AND  GAME APPROACHES TO RANDOMNESS
           268 Prove the following stronger  version  of the  statement  (b)  in  the  last 
        theorem:  Let  /   be  a total  computable  noil-decreasing  unbounded  function with 
        natural arguments and values.  There exists a computably random sequence u> such 
        that C(u)q • • -oJn-i |n) ^ /(n) + 0(1) for all n.
           (Hint:  Adding a new martingale costs us one bit of advice—it should be done 
        only when the value of / increases.)
          Moreover, there exists a computably random sequence u> such that the state­
        ment  of the  last  problem  is  true  for  every  total  computable  non-decreasing  un­
        bounded function / [120].
           269 Show that for a computable measure P on fî x О and for every sequence
        a that  is  computably random with respect  to the first  projection of P,  one can 
        define the conditional probability along the second coordinate with condition  “the 
        first coordinate equals a”, using a computable version of Doob’s theorem (p. 275).
          Let us stress again that all the results about computable martingales can be 
        translated into  the  language of computable gambling strategies  (algorithms that 
        look  on  the  known  bits  and  compute  in  which  (rational)  proportion  the current 
        capital should be split between two bets.  (Recall that the underlying measure P, 
        which determines the rules of the game, is assumed to be computable and strictly 
        positive for all intervals.  When performing the rational approximations,  we may 
        assume that martingale values are separated from zero, e.g., by taking the average 
        with a martingale that equals 1 everywhere.)
                  9.9.  Martingales and Schnorr randomness
          The notion of computable randomness is closely related to Schnorr randomness 
        (see Section 3.4).  Both these notions were introduces in C.  Schnorr’s book  [166] 
        The following statement was also proved there:
          Theorem  183.  Let tt  be a computable measure,  and let all intervals £lx  have 
        positive tt-measure.  A  sequence u>  is not Schnorr random if and only if there  ex­
        ists  a  computable it-martingale m  and computable total non-decreasing unbounded 
        function g : N —> N such that
                          m(u!0u!i ■ ■ -u!n-i) > g{n)
        for infinitely many n.
          This theorem says that sequences that are not Schnorr random are not com­
        putably random, and, moreover, there is a martingale that is not only unbounded, 
        but  unbounded in a strong sense  (exceeds infinitely often some computable non­
        decreasing unbounded function).
          P r o o f.  Assume that u> is not Schnorr random.  As we have seen in Section 3.4 
        (Problem 90, p.  70), there exists a sequence of strings xo,x\,X2 , ■ ■ ■ such that the 
        series X^7r(:r0 computably converges and infinitely many of Xi are prefixes of u>.
          Let us split the series 7г(жо) + 7r(æi) + 7г(а:2) + • • ■ + тт(х{) + ■ • •  into groups (each 
        contains finitely many consecutive terms) in such a way that the sum of kth group 
        is  at  most 4~k  (discard some initial segment of the series if necessary).  Since the 
        series converges computably, this splitting can be performed in a computable way. 
        We may also assume without loss of generality that the groups can be separated 
        by string lengths:  there exists a computable sequence no  <  n\  < щ  <  • • • ,  and
                                9.9.  MARTINGALES  AND  SCHNORR RANDOMNESS                       283
             strings in the kth group have length in the interval [п^,п^+1).  Indeed, every string 
             Xi  can be replaced by a group of strings of some large length (the interval is split 
             into  many intervals of the same size),  and we can do this for all X{  sequentially 
             using longer and longer strings (this does not change the covering property and the 
             sums in groups).
                 Consider now separately the strings from kth group.  The corresponding inter­
             vals have total measure less that 4“fc, and there exists a martingale      that reaches 
             4k on all these strings.  Now we mix all these martingales and get a combined mar­
             tingale m =      2~knik-  It  reaches  2k  at  the strings of kth group.  It  remains to 
             let g(n) = 2k for all n between nk  and n^+i  and note that infinitely many groups 
             contain prefixes of oj.
                 Now the reverse direction.  Assume that  a computable martingale  m  and  a 
             total unbounded computable function g are given.  We need to cover a sequence oj 
             for which we know only that m(ojQ • • -ojn-i) ^ g(n) for infinitely many n.  And the 
             measure of this cover should not exceed some given e > 0 and be computable.  How 
             do we do this?
                 First of all, we increase g at some initial segment of N and assume that it is at 
             least  \fe + 1  everywhere  (this does not matter when we speak about events that 
             happen infinitely often).  Now we consider all the strings in the order of increasing 
             lengths and select those where m exceeds g.  (More precisely, since we know rn only 
             with some precision, we select strings in such a way that m > g — 1 for all selected 
             strings and all strings with m > g are selected.)
                 The assumption guarantees that the intervals, which correspond to the selected 
             strings,  cover oj.  Since for  these strings  the martingale is at  least  1/e,  the total 
             measure of intervals does not exceed e.  Finally, the sum of measures is computable: 
             To find it with error at most 5, we wait until g becomes bigger than 1/5 + 1; all the 
             subsequent (longer) intervals can change the sum of measures at most by 5.           □
                 A similar  argument  can  be  used  to  prove  the  following  criterion  of Schnorr 
             randomness in terms of prefix complexity [10]:
                  270 Prove that a sequence oj is Schnorr random with respect to a computable
             measure g if and only if for every computable total upper bound к for prefix com­
             plexity and for every non-decreasing unbounded computable function h: N —>  N 
             the inequality
                                   К(и)п) ^ -  log2 At(fi(w)n) -  h(n) -  0(1)
             holds for all n (the constant in 0(1) does not depend on n).
                 (Hint :  For computable к and h the cover constructed in the proof of the Levin- 
             Schnorr theorem has computable measure.  The argument in the other direction is 
             similar to the proof of the preceding theorem:  We split  the cover into groups of 
             strings of the same length, where the nth group has total measure less than 4“", 
             increase the measure of strings in nth group to get 2~n instead of 4n, and use the 
             Kraft-Chaitin lemma to get a computable bound for prefix complexity;  h can be 
             found since all strings in the nth group have the same length.)
                 This result can be used to show that there exists a Schnorr random sequence 
             (with respect to uniform measure) that is not Mises-Church random [10].  Indeed, 
             it  shows that  if K((oj)n)  >  n — h(n) — 0(1) for some non-decreasing unbounded 
             function h that tends to infinity slower than every computable non-decreasing un­
             bounded function,  then oj  is  Schnorr  random.  Such  functions  h  exist  (diagonal
        284    9.  FREQUENCY AND  GAME APPROACHES TO RANDOMNESS
       construction), and it remains to select one of them and find a sequence that satis­
       fies the inequality above but is not Mises-Church random.
          How can we do this?  Take a random sequence a,  and insert  zeros  at some 
       places.  These zeros will later form an unbalanced subsequence  (so the sequence 
       is  not  Mises-Church  random).  What  are  the requirements  for  the places  of ze­
       ros?  They  should  be  (1)  computable  (or  at  least  one  can  find  them  by  look­
       ing  at  the  previous  bits),  and  (2)  very  rare  (so  the  complexity  of prefixes  does 
       not  decrease significantly).  The  requirement  (2)  hints  that  the function  F:n  4  
       (place of the nth insertion) should grow faster than any computable function, but 
       how do we combine this with (1)?  The key idea is that the Kucera-Gåcs theorem 
       says that we can take arbitrary F and then find a sequence a that computes F. 
       There is a subtle point:  We are able to compute F having access to the entire a, 
       and only some prefix is available.  But this is not a problem—if the current prefix 
       of a is not enough to compute F, we just wait  (and place zero after the value of 
       F is computed; if this place is farther than we planned, the better).  It remains to 
       estimate prefix complexity.  Here we note that by adding zero at predictable places, 
       we do not change the prefix complexity of an initial segment.
          271 Provide the missing detail in this argument, and prove that there exist
       Schnorr random sequences that are not Mises-Church random.
          (These sequences will not be computable random, so we also know now that 
       Schnorr randomness is strictly weaker than computable randomness.)
          It turns out that Kurtz randomness also can be characterized in terms of mar­
       tingales.
          272  Prove that a sequence uj is not Kurtz random (Section 3.4, p. 70) if and 
       only if there  exists  a computable martingale  such that  computable converges to 
       infinity on prefixes of to  (an equivalent formulation is that there is a computable 
       monotone lower bound that is not bounded).
          (Hint:  The sets of small measure that cover u> and are finite unions of intervals 
       can be converted to martingales.  We know how long the prefix of u> should be for 
       this martingale to work, and this can be used to find a computable lower bound for 
       the final martingale.  On the other hand, knowing the martingale m and a length 
       I,  where the martingale should exceed some c,  we can consider all the strings of 
       length I where m exceeds c and get a finite cover of u> of measure at most 1/c.)
          This shows that Schnorr random sequences are Kurtz random (and the inclusion 
       is  strict,  since  some  Kurtz  random  sequences  do  not  satisfy even the  SLLN;  see 
       Problem 92, p. 70).
                 9.10.  Martingales and effective dimension
          In the previous section we have seen how the notions of null,  effectively null, 
       and Schnorr null sets can be translated into the language of martingales.  A similar 
       translation is possible for the notion of Hausdorff dimension.  In one sentence this 
       translation can be described as follows:  The smaller the dimension of a set is, the 
       faster martingales can grow on its elements.  (In this section we consider martingales 
       with respect to the uniform measure.)
          Let us start  from a statement  that  relates classical Hausdorff dimension  (no 
       algorithms) and martingales.
                              9.10.  MARTINGALES  AND  EFFECTIVE  DIMENSION               285
                Theorem  184.  A  set A  c  Q,  is  an а-null  set  if and  only  if there  exists  a 
            martingale m such that for every со € A the ratio
                                                  m(x)
            is  not bounded on prefixes of uj.
                (For a = 1, we get Theorem 177.)
                This result can be reformulated as follows.  Assume that the gambler is taxed. 
            After each game she pays a tax proportional to her capital, so the capital is multi­
            plied by some constant factor 2a_1 < 1.  Then the capital m(x)  (as a function of a 
            history X of the game) is no more a martingale, but satisfies the condition
                                     2a~1m(x)     m(x 0)    m(x 1)
                                                     2   +    2
            and this can be rewritten as
                                       2  am(x) = m(x 0) + m(x 1).
            Functions that satisfy this condition are called (following [110]) a-gales.  They also 
            can be defined equivalently in terms of measures:  An a-gale is a function
                                               p(x) 2al<x\
            where p(x) = ir(Qx) and 7r is a measure on Q (we do not require here that 7r(f7) = 1 
            or m(A) = 1).  Similarly we can define a-supergales, where an additional decrease 
            in capital is allowed after each game.  The condition is
                                       2am(x) ^ m(x0) + m(x 1).
            In this language the statement of Theorem 184 says that for every a-gale the set 
            of sequences,  for whose prefixes this a-gale is unbounded,  is an cc-null set;  every 
            a-null set is contained in a set of this kind (for some a-gale).
                P r o o f.  The  proof is just  a slightly modified argument  used to prove Theo­
            rem 177.  We use the language of cc-gales (see above).  Let m be an arbitrary a-gale. 
            We need to show that the set of sequences, where m is unbounded, is an а-null set. 
            It is enough to show that strings x, where m achieves к (or more) for the first time, 
            have the sum of а-powers of measures at most 1/k.  Writing m(x) as p(x)2al(x\  we 
            see that for these strings we have p(x) > k2~al^x\   All the strings are incompatible 
            (none of them is a prefix of any other).  So the sum of p-measures is at most 1, and 
            so the sum of 2~al№ for all these x (=the sum of а-sizes of corresponding intervals) 
            does not exceed 1/k.
                In the other direction, let A be an а-null set.  We need to construct an a-gale 
            that is unbounded on prefixes of uj for every wed.  For each k, consider a cover 
            of A by intervals with sum of а-sizes at most 1/k.  We will construct an a-gale m*, 
            that reaches к on these intervals.  (Then we compute the sum of all a-gales mAk 
            with coefficients 2k, since the sum of a-gales is an a-gale.)
                How do we construct m^?  For each x we consider an a-gale that equals 1 on x 
            and equals 0 on all other strings of the same length; the values for shorter strings 
            are determined uniquely by the definition of a-gale, and for longer strings we choose 
            some extension.  On the root (empty string) the value of this a-gale is 2~al(x\  i.e., 
            the а-size of x.  So the sum of values of this a-gale (over all x in the cover)  is at 
            most 1/k in the root, and multiplying it by k, we get m*,.                    □
         286      9.  FREQUENCY AND  GAME APPROACHES TO  RANDOMNESS
           Now we switch to the effective version of this  theorem.  Let  a  €  (0,1]  be a 
         computable real.  We can define the notions of lower semicomputable a-gale  (or 
         a-supergale)  in  a  natural  way.  As  in  the  case  a  =  1,  we  do  not  require  that 
         m(A)  =  1,  only that ra(A)  <  1.  One may expect that lower semicomputable ci­
         gales (or supergales) correspond to effectively а-null sets and this can be proved by 
         an effective version of the argument above.
           In one direction it is indeed the case:
           Theorem  185.  Let a e (0,1]  be a computable real number,  and let A c SI  be 
         an effectively а-null set.  Then there  exists  a lower semicomputable a-gale  that is 
         unbounded on prefixes of uj for every u> € A.
           P r o o f.  Indeed, the construction above gives computable mjt and their mix is 
         lower semicomputable.                                  □
           In  the  other  direction  the situation  is  more  complicated.  Let  m  be  a lower 
         semicomputable a-gale.  For some integer к we may consider the set of strings x 
         such that m(x)  > k.  This is an enumerable set.  Moreover, the sum of а-sizes of 
         its  minimal elements is bounded by 1/k.  The problem is that the set of minimal 
         elements of an enumerable set is not guaranteed to be enumerable, and if we consider 
         all  (not only minimal)  elements, we do not have the bound for the sum of a-sizes 
         anymore.  So we cannot use this argument to prove that the set of sequences, where 
         a given lower semicomputable a-gale is unbounded, is an effectively а-null set.
           In fact this is not true.  Lower semicomputable a-gales correspond to a weaker 
         notion  of an effectively  а-null  set  where we  bound  not  the sum of a-sizes  of all 
         intervals in a cover but only the sum of a-sizes of subfamilies of disjoint intervals. 
         (An equivalent definition considers only maximal intervals that are not part of other 
         intervals in the cover.)  But if we are interested only in effective dimensions, all these 
        subtle differences are easily compensated for by an arbitrarily small change in a, 
         and the following statement is enough:
           Theorem  186.  Let m  be  a lower semicomputable a-gale.  The set  of the se­
         quences u>,  such that m is not bounded on the prefixes of uj,  is an effectively ß-null 
         set for every ß > a.
           (We assume here that a and ß are computable.)
           P r o o f.  Let к be a positive integer.  Consider strings x such that m{x) > к and 
        the corresponding intervals.  We get a cover of the set in question.  What can be said 
        about the sum of /З-sizes of the covering intervals?  As we have seen, every subset of 
        disjoint intervals in this family has a sum of a-sizes at most 1/k.  In particular, for 
        every length N the sum of a-sizes for strings of length N in the family is at most 
         1/k, and the sum of /3-sizes is at most  (l/k)2~N^~ a\  So taking the sum over all 
        lengths,  we multiply the bound  1/k by the sum of the geometric series,  which is 
        finite.                                                 □
           Two last results give the following corollary:
           Theorem  187.  For an arbitrary set A <z Lt,  its effective Hausdorff dimension 
         is equal to the infimum of the set of a such that there exists a lower semicomputable 
        a-gale that is unbounded on all elements of A.
                                     9.11.  PARTIAL  SELECTION  RULES                    287
                The same is true for a-supergales instead of a-gales (with the same proof).
                This result provides an alternative proof of Theorem 120.  Indeed, a-supergales 
            are semimeasures multiplied by 2al^x\   So there exists a maximal lower semicom- 
            putable Q-gale that corresponds to a maximal lower semicomputable semimeasure 
            (=continuous a priori probability).  In the last result we can therefore consider only 
            this Q'-supergale,  and the effective dimension of {w}  is equal to the infimum of a 
            such that a{ujQbJi • • -tdn_i)2an  is  an unbounded function of n.  The logarithm of 
            this expression is an — К A     • • • con-i), so the infimum of those a is
                                        limtof        - 1*.—‘>,
                                                      n
            (In Theorem 120 we used plain complexity instead of a priori complexity, but the 
            difference is O(logn) while we have n in the denominator.)
                                    9.11.  Partial selection rules
                Returning to the selection rules, note that we required the selection rule to be 
            total  (the selection always says, in finite time, whether to select the next term or 
            not,  for  all  possible sequences).  But this condition can be relaxed.  Of course,  if 
            the rule is undefined on some prefix of the given sequence, then it does not select a 
            subsequence.  But the rule may hang in some other situations (that do not happen 
            for our sequence).
                Let us define this broader class of rules formally.  Let r be a computable par­
            tial  function that  maps  (some)  bit strings to  {0,1}.  To decide whether the term 
            ujn  should be selected  (while applying the rule to some sequence lo)  we compute 
            r(cjо • • • tdn_i).  The value 1 means that we select cjn, the value 0 means that we do 
            not select cju;  if the value is undefined,  the selection process hangs,  and we get a 
            finite sequence.  This selection rule is denoted by Sr.  (It is equivalent to the rule Sr 
            where R is the set of all x such that r is defined on all prefixes of x and r(x) = 1. 
            Note that this R is not always decidable for computable partial functions /.)
                This  class  of selection  rules  was  considered  by  R.  Daley  [46];  we  call  them 
            Church-Daley  admissible  selection  rules.  The sequence  is  called  Mises-Church- 
            Daley random if every Church-Daley admissible rule selects a balanced (or finite) 
            sequence.1
                273  Prove that a Church-Daley admissible selection rule applied to a Mises-
            Church-Daley random sequence gives a Mises-Church-Daley random sequence.
                This extension of the class of selection rules makes the class of random sequences 
            smaller, which follows from Theorem 173 (p. 270) and the following result proven 
            by W. Merkle [120]:
                Theorem 188.  There is no Mises-Church-Daley random sequence oj such that 
                                       C{u)Q • • -wn_i) = O(logn).
                P r o o f.  Assume that
                                        C(u)Q • • -(jJn-i) < clogn
                1W. Merkle called them the “Mises-Wald-Church stochastic sequence” in [120], though the 
            historical  reasons  for this  name are unclear.  Church never  considered  partial computable rules, 
            while Mises and Wald did not consider computability at all, so the difference between partial and 
            total rules was not essential for them.
            288          9.  FREQUENCY  AND  GAME  APPROACHES  TO  RANDOMNESS
            for some c and for some large enough n.  We want to show that u> is not Mises- 
            Church-Daley random, i.e., construct a rule that selects an unbalanced sequence.
                Let us first consider the case when c <  1.  The set of all strings of complexity 
            less than clogn is an enumerable set of at most nc elements, and for large n the 
            number or elements in this set (we denote it by Cn) is bounded, say, by n/10.  Fix 
            some of those large values of n.
                Reading the n-bit prefix from left to right, we try to the predict the next bit 
            (after reading all the previous ones).  Let us show that we can guarantee at least 
            90% success.  Enumerating Cn, we find a first element in this enumeration, call it 
            the  “current  candidate”,  and predict  the bits that  are there until  our prediction 
            turns out to be incorrect.  As soon as this happens, we continue the enumeration of 
            Cn until we find a new element that is consistent with all already discovered bits. 
            Then it becomes the current candidate, and it is used for predictions until one of 
            the predictions turns out to be incorrect, etc.  Since we know that the actual prefix 
            is in Cn, we will never run out of candidates, and the number of changes (=number 
            of errors) is bounded by the cardinality of Cn, i.e., by n/10.  So at least 90% of the 
            predictions are correct.
                This can be done for every large enough n.  To deal with an infinite sequence, 
            we consider  a fast  growing computable sequence no  <  n\  <  П2 ■ • ■  where no  is 
            large enough (so our prediction method works for all щ).  Using CUi for predictions 
            between Щ-\  and щ, we make at most 0.1щ errors, and in total we get at most 
            0.2n errors  (even if all  previous predictions  are false,  which is  not  the  case,  but 
            we do not need a better estimate).  So our prediction method will be successful 
            infinitely often.
                It remains to note (as was done in Theorem 169) that the prediction algorithm 
            corresponds to two selection rules:  one selects terms when we predict ones, and the 
            other selects terms when we predict  zeros.  If predictions are successful,  at  least 
            one of these selection rules will select a highly unbalanced sequence.  This ends the 
            proof for c < 1.
                This trick does not work for c > 1.  For example, if c = 1.5 we have n1-5 candi­
            dates,  and all our predictions could be false (leading to the change in the current 
            candidate without any contradiction).  But we can use a more clever argument.
                Let us split the string u>o • ■ -cun_ i  into two halves and get a pair  (u,v)  where 
            и and v are (n/2)-bit strings.  The complexity of this pair is at most  1.5 log n (we 
            still  consider  our  example with c =  1.5).  On the other hand,  the complexity of 
            the pair is equal to C(u) + C(v\u) up to 0(\ogC(u, v)), so either C(u) < 0.8logn 
            or C(v\u)  < 0.8logn.  In both cases we can apply the trick used for c <  1, since 
            n0-8 is much less than nf 2.  Note also that, while predicting the bits in the second 
            half, we already know the bits in the first half, so the condition и in the inequality 
            C(v |u) < 0.8 log n is not an obstacle.
                So at least one of the two prediction algorithms is successful (on its half).  Then 
            one of the two selection rules corresponding to this algorithm will select a highly 
            unbalanced sequence.  (The selection rule does not select any terms from the other 
            half.)
                There is a problem, however.  All this can be done for every n, but how do we 
            combine the selection rules for different n?  Imagine, for example, that we tried to 
            predict bits in the left half assuming that C(u) < 0.8n while in fact that is not the 
            case.  Then our algorithm can make many errors (this is not a big problem)  and
                                        9.11.  PARTIAL  SELECTION  RULES                        289
             can even hang (and this is the problem, because then it cannot be used as a step 
             in a prediction algorithm for an infinite sequence).
                 To get around this problem, we should recall the proof of the formula for the 
             complexity of pairs  (Theorem 21, p.  37)  and use it as a part of our construction. 
             Let us explain what this means.
                 As before, we make predictions for the left and the right halves  (и and v) of 
             the n-bit prefix separately.  When we read the right half v bit by bit, we enumerate 
             the set  Cn  of possible candidates for the n-bit prefix (^strings of complexity less 
             than 1.5n), waiting until a candidate appears that is consistent with и and already 
             known bits of v.  When such a candidate is found, we use it for predictions until 
             one of the predictions turns out to be false.  Then we look for the next candidate, 
             etc.
                 Will this prediction algorithm be successful?  It depends on u.  More specifically, 
             it  depends on the number of different v such that uv E Cn.  If there are many of 
             them, we can make an error and change the candidate at each step.  But at least 
             our prediction algorithm will not hang as far as uv is indeed in Cn.
                 Now we discuss the left half.  Here we use as candidates the values of и such 
             that there is at least n0,8 different v with uv 6 Cn.  The prediction in the left half 
             is guaranteed to be successful if и is among the candidates  (and this will happen 
             if the predictions in the right half are not successful).  But if not, this prediction 
             algorithm may hang (at some moment we could wait forever for a candidate which 
             is consistent with known bits).
                 What happens when we combine these algorithms for different prefixes?  First 
             we consider the joint algorithm based on the predictions of right halves for each щ. 
             This algorithm never hangs (we assume that щ is large enough, so all prefixes of 
             length щ have complexity less than 1.5пг).  If for infinitely many i the prediction is 
             successful, then we are done (the fraction of successful predictions does not converge 
             to  1/2).  So it is enough to consider the case when the right half prediction works 
                                   i.  Then for all sufficiently large i the left half prediction works, 
             only for finitely many 
             and the finite number of bad prediction algorithms can be replaced by something 
             safe (that never hangs).
                 So we see that in both cases и is not Mises-Church-Daley random.
                 This proves the theorem for c = 1.5 (and the same trick works for every c < 2). 
             But what should be done for bigger values of c?  One can split the sequence not into 
             two halves, but into к pieces of equal size for some к > 2.  One should take к greater 
             than c, and repeat the same argument.  The prediction algorithm for the rightmost 
             piece never hangs, so we can combine these algorithms into a prediction algorithm 
             for the entire sequence.  If it is successful for infinitely many prefixes, we are done. 
             If not,  it  fails starting from some moment,  and then the prediction algorithm for 
             the second (from the right)  piece is total  (but not  necessarily successful).  If it  is 
             successful infinitely often, we are again done.  If not, we should consider the third 
             piece, etc.  (A more formal exposition with all details can be found in [120].)    □
                 So we know the Mises- Church-Daley random sequence cannot have O(logn)- 
             complexity  of prefixes.  However,  it  can  only  slightly  exceed  this  bound  (e.g., 
             O(lognloglogn) complexity is possible), as shown in [120]:
                 Theorem 189.  Let f : N —> N be a total non-decreasing unbounded computable 
            function.  Then there  exists  a Mises-Church-Daley random sequence whose n-bit 
             prefix has complexity at most f(n) logn + 0(1) for all n.
            290         9.  FREQUENCY  AND  GAME  APPROACHES  TO  RANDOMNESS
                Proof.  Recall how we constructed Mises-Church random sequences in Theo­
            rem 173.  The advice information there was very small, only one bit per algorithm 
            (that may or may not compute a selection rule) — we needed to know whether it 
            would compute a selection rule.  Now this is not enough, because the selection rules 
            are partial.  Now we can enumerate all selection rules,  but for each selection rule 
            we need to know when it becomes undefined for the first time (so it can be replaced 
            by something harmless starting from this moment).  So, if we use fin) programs 
            to construct the first n bits,  the total size of the advice needed is  /(n)logn bits 
            (for each of /(n) programs we need logn bits to specify the first place where it is 
            undefined — we may agree that this place is n + 1 if it is defined for all currently 
            known prefixes).2
                Note that we use n in the condition, but the О (logn) change does not matter 
            since this corresponds to the 0(l)-change in /.                              □
                A similar extension  (allowing partial functions)  can be done for martingales. 
            Recall that we may define computably random sequences using total computable 
            functions with rational values as martingales.  Now we can consider also partial 
            computable functions requiring the equation  (defining martingales)  to  be true if 
            all three quantities m(x), m(x0), and m(x 1) are defined.3  We call these functions 
            partial martingales.  A partial martingale wins on a sequence u> if it is defined for 
            all prefixes of ui and is unbounded.  A sequence is partial-computably random if no 
            computable partial martingale wins on it.  Now, following [120], we may generalize 
            Theorem 189:
                Theorem 190.  (a)  Every partial-computably random sequence is also Mises- 
            Church-Daley random.
                (b)  Let f  : N —> N  be  a non-decreasing unbounded computable function.  Then 
            there exists a partial-computably random sequence u> such that the n-bit prefix of со 
            has complexity at most f{n) logn + 0(1).
                (These two statements together imply the statement of Theorem 189.)
                PROOF,  (a) We use that same construction to convert a selection rule into a 
            martingale as in Theorem 182(c).  If the rule is partial,  we get  a partial martin­
            gale.  But  if the  rule  selects  an  infinite  subsequence  from some  sequence u>,  the 
            corresponding martingale is defined on all prefixes of u>.
                (b)  Here  again  we  may  follow  the  argument  used to  prove Theorem  182(b). 
            For each martingale that is added to the construction, we need to know at which 
            moment it becomes undefined  (so we can replace it by something harmless,  e.g., 
            by its last value).  This information requires at most log n for each martingale used 
           to construct the first n bits of the sequence, and if at this moment we use at most 
            f{n) martingales, we get the required bound.                                 □
               2 One may ask also why we need to know exactly the moment when the algorithm becomes 
           undefined for the first time, not just one bit saying whether this happened or not.  This is because 
           the constructions for different n should give prefixes of the same infinite sequence.
               3From the gambling point of view it is  natural to require that m(a:0)  and m(x 1)  are both 
           defined or both undefined; one cannot toss the coin before both bets are made.  However, it is not 
           important:  If m (x) and m(a:0) are defined, we can compute m(x 1) knowing that m isa martingale, 
           and vice versa.
                                9.12.  NON-MONOTONIC  SELECTION  RULES                  291
                               9.12.  Non-monotonie selection rules
                Up to now we considered selection rules that keep  the  ordering of terms  in 
            the  input sequence  (read  it  from  left to  right).  However,  this  restriction can be 
            relaxed in a natural way.  Such a relaxation was suggested by Kolmogorov in  [77] 
            and independently by D. Loveland in [106,  107].
                Let us explain informally how it is done.  Imagine that a casino outsources the 
            coin tossing to some producer of random bits who writes these bits on paper cards 
            and puts the cards on a table face down.  The gambler may then ask to reveal some 
            bit, then the corresponding card is turned over.  Also the gambler may select some 
            bit that is not revealed yet;  then the corresponding card is also turned over,  and 
            the bit written on it is added to the subsequence.
                More formally this class of selection rules can  be described  as  follows.  The 
            cards  (and  corresponding  bits)  are  indexed  by  natural  numbers.  The  selection 
            rule is determined by a pair of functions F and G.  The function F maps binary 
            strings to natural numbers and says which bit should be revealed at the next step 
            (depending on the  bits  already  revealed).  We  assume  that  the  values  of F  on 
            every two compatible bit strings  (one is a prefix of the other)  are different.  This 
            guarantees that the same bit is never requested again.  The second function, G, is 
            also defined on binary strings and has values in {0,1}.  The value 1 means that the 
            bit chosen by F is selected (and becomes the next bit of the output subsequence); 
            the value 0 means that it is observed but not selected.
                According to this description, for every two partial functions F (that satisfies 
            the condition above) and G, we define the selection rule Sf,g '•  £ as follows.
            First we consider a (finite or infinite) sequence of integers no, Щ,... where
                            n0 = F{A),  ni = F(cjno),  n2 = F(cjnocjni),  ...
            (the construction stops when the next value of F is undefined).  The condition for 
            F guarantees that all щ are different.
                Then we select the terms coni  for which the value of G on cjnocjni • • -Ь0П1_Х  is 
            defined and equal to 1, and, moreover, the values of G on all prefixes of this string 
            are  defined.  The  corresponding  usni  (in  order  of increasing  i)  form  the  output 
            subsequence           (We call it a subsequence though usually subsequences are
            defined as monotonie subsequences, keeping the ordering of the initial sequence.)
                The selection rules Sptc that correspond to computable partial functions F and 
            G are called Kolmogorov-Loveland admissible selection rules.  A sequence cj € Q is 
            called Mises-Kolmogorov-Loveland random, or Kolmogorov-Loveland stochastic, if 
            every Kolmogorov-Loveland admissible rule selects a balanced (or finite) sequence.
                We consider mainly the case of the uniform measure, but a similar definition 
            can be given for Bernoulli measure Bp (independent trials with success probability 
            P)-
                The following simple (though unexpected) observation was made by W. Merkle 
            in [119]:
                Theorem  191.  Restricting the class of selection rules and requiring F  and G 
            to be total,  we get the same class of Mises-Kolmogorov-Loveland random sequences.
                PROOF.  Assume that some selection rule      applied to some sequence cj se­
            lects an infinite unbalanced subsequence.  Let us split the selected subsequence into
            292          9.  FREQUENCY  AND  GAME  APPROACHES  TO  RANDOMNESS
            two:  ojni  is included in the first subsequence if щ is even, and in the second subse­
            quence otherwise.  At least one of these two subsequences is infinite and unbalanced. 
            So we can assume without loss of generality that the selection rule produces an in­
            finite unbalanced sequence that consists only of terms with even numbers (or only 
            of terms with odd numbers—this case is similar).  Knowing that we select only bits 
            with even numbers, we can read other bits at any time; this will not interfere with 
            the selection since these bits will be never selected.  So if the partial computable 
            rule starts a long computation, we may in parallel read the bits with odd numbers 
            (they may be requested later by the original rule or not; if the original rule requests 
            them, we have them already at hand and do not read them again).  This new selec­
            tion rule is defined by total functions F' and G' (if the original algorithm hangs at 
            some point, then the new one reads the terms with odd numbers one after another, 
            and never selects anything).                                                  □
                This proof reduces one partial selection rule to two total ones.
                How is the new definition of randomness related to the one previously given? 
            A partial answer is provided by the following theorem:
                Theorem  192.  (a)  Every  Mises-Kolmogorov-Loveland  random  sequence  is 
            Mises-Church-Daley random (and, therefore, Mises-Church random).
                (b)  Every ML-random sequence is Mises-Kolmogorov-Loveland random.
                More precisely,  (a)  holds for every real p G  (0,1);  in  (b)  we assume that the 
            measure is uniform or equals Bv for some computable p (and both notions of ran­
            domness are understood accordingly).
                PROOF,  (a) Church-Daley admissible selection rules (including Church admis­
            sible rules) are a special case of Kolmogorov-Loveland admissible rules.
                (b) Here we use essentially the same argument as for Mises-Church randomness. 
            Assume that some computable p is fixed in the definition of Mises-Kolmogorov- 
            Loveland randomness,  and we require that every selected subsequence should be 
            finite or have limit frequency p.  For Martin-Löf randomness we consider the com­
            putable Bernoulli measure Bv that corresponds to independent trials with success 
            probability p.
                Fix some computable (partial) functions that consider the corresponding selec­
            tion rule Sf,g■  For every integer n and for every rational q, consider the set Dnq 
            of all n-bit  strings where the frequency of ones exceeds q.  We know that for the 
            fixed q  > p and for n —> oo, the Hp-measure of the set Dn^q  (more precisely,  the 
            Hp-measure of the set Dn%q  of all sequences that have a prefix in Dn^q)  decreases 
            exponentially.
                Now consider the preimage of this set with respect to Sf,g\ more precisely, con­
            sider the set of all sequences u> for which the selection rule produces a subsequence 
            of length at least n and the frequency of ones among the first n terms exceeds q.  It 
            is easy to see that Вp-measure of this set is bounded by Bp(Dn^q).
                Informally speaking, this happens because the output distribution of the selec­
            tion rule  applied to  Hp-distributed sequence has the same distribution Bp,  if we 
            ignore that some sequences are cut at some point (output sequence can be finite), 
            and cutting may only decrease the probability.  More precisely,  let f be a binary 
            string of length к — 1.  Consider the conditional probability of the event  “E = kth 
            bit  of the selected sequence is  1”  with the condition  “C = selected sequence has 
            length at least к and preceding bits are t”.  This probability is equal to p.  Indeed,
                                9.12.  NON-MONOTONIC  SELECTION  RULES                  293
            there are many cases when bits equal to t are selected and the next bit to be se­
            lected is chosen (depending of the values of the revealed but not selected bits) so 
            the condition C  can be split  into  a union of disjoint subsets  Ci,  and  for each of 
            them the conditional probability Pr[E\Ci]  equals p:  the event  Ci  determines the 
            position of the bit  that will become the kth bit in the selected sequence,  and Ci 
            is  determined by the values of the other bits  (before this position).  Then we (by 
            induction) conclude that the probability that the first n selected bits form a given 
            string и is bounded by the probability of getting и according to Bv,  and we sum 
            these inequalities over all и G DnA.
                It remains to note that the set of sequences u>, such that <SV,g(w) has a prefix in 
            Dn>q, not only has small measure but also is effectively open (since we can enumerate 
            different scenarios when such a prefix could appear).  So for each q we get  (as in 
            the proof of the effective version of the SLLN) an effectively null set.  (Similar sets 
            should be considered for all rational q < p.)  So an ML-random sequence does not 
            belong to these sets, and this finishes the proof.                           □
                In the next section we prove the following generalization of statement  (b):  if 
            a computable sequence pn of real numbers in (0,1) computably converges to some 
           p  G (0,1), then every sequence that  is  ML-random with respect  to  the product 
            measure (independent trials with success probability pi  in the ith trial) is Mises- 
            Kolmogorov-Loveland  random with parameter p.  This  statement  is  an  impor­
            tant  tool  (suggested by M.  van Lambalgen)  for constructing examples of Mises- 
            Kolmogorov-Loveland  random sequences  with  pathological  properties  (not  ML- 
            random, having more zeros that ones in all prefixes, and others).
                Now we take another direction and show  (for the case of uniform  measure) 
            that every Mises-Kolmogorov-Loveland sequence has almost maximal complexity 
            of prefixes.  (Recall that this is not the case for Mises-Church and Mises-Church- 
            Daley random sequences.)
                Theorem  193.  Let to  be a binary sequence such that C{ojq • • -wn_i)  < an for 
            some a  <  1  and for all sufficiently  large n.  Then oj  is  not  Mises-Kolmogorov- 
            Loveland random.
               This result (proven by An. Muchnik in the late 1980s) was later strengthened: 
            it  turned out  that  if the inequality is true for infinitely many n,  the sequence is 
            not Mises-Kolmogorov-Loveland random.  But we prove only the original weaker 
           statement.
               For this  proof we need some auxiliary statement  about  the price of  “insider 
            information” in the game with fixed size bets.  Let us state and prove this statement 
           first,  and then come back to the proof of Theorem 193.
               Assume that we come to a casino when a sequence of random bits is generated 
            by coin tossings,  and before each of them we can make a bet и G [— 1,1], where 
           positive (resp. negative) и means that we bet on 1  (resp. on 0).  After the coin is 
           tossed, we get и dollars if the bit is 1 and — и dollars if the bit is 0.
               Note that (unlike for martingale games) our maximal bet is always 1 and does 
           not  depend on how much we have won  (or lost)  in the previous games.  So our 
           potential loss is not bounded (in the martingale games the loss was bounded by the 
           initial capital).  To avoid confusion, let us stress also that we play with the bits in 
           the same order as they appear (we do not consider the non-monotonic rules yet).
            294         9.  FREQUENCY  AND  GAME  APPROACHES  TO  RANDOMNESS
                Lemma.  Assume  that we  know in  advance  some  set A  of n-bit  strings  that 
            contains  at most 2s  elements for some s < n.  Then there  exists a strategy in the 
            described game that guarantees  that we win at least n — s  on every element of A 
            (for every series of n games when the sequence of outcomes belongs to A).
                For example, if A contains only one string (in other words, we know in advance 
            all the outcomes for all n games), the lemma says that we can win n dollars (not 
            a surprise:  we win one dollar in each game).  For comparison,  in the martingale 
            setting we could make 2n  dollars out  of 1  dollar.  If we  know  results  of some  к 
            games, the lemma guarantees that we can win к dollars (and this is again trivial). 
            A bit more complicated example is to assume that we know that the number of 
            ones is even;  in this case s = n — 1.  The lemma says that we can win one dollar. 
            (Indeed, we can make zero bets until the last game, and then put 1 dollar on the 
            right outcome which is known at that moment.)
               P r o o f.  At each moment we know some prefix w of the sequence; let j be its 
            length.  There are 2n-J  possible extensions of j  (up to an n-bit string),  but only 
            some of them are in A.  Let us consider their fraction (the conditional probability 
            of A after w)\ a negated logarithm of this fraction is called the information capital 
            of the player.
               Initially this capital equals n — s.  We will show that we can make bets in such 
            a way that the sum of the information and real capitals never decreases.  Then at 
            the end of the game, when the sequence is in A and the information capital is 0, 
            the real capital is at least n — s, as required by the lemma.
               Why can we make a bet that guarantees the non-decrease?  Assume that the 
            information capital is now (—logp) for some p (the current fraction of A-elements 
            among the extensions).  Knowing A, we can compute this capital.  We know also 
           how it will change after the next game:  if 0 appears, it becomes equal to (— logpo)) 
           and if 1 appears, it becomes equal to (— logpi), where po and pi  are fractions of A 
           among the extensions of wO and w 1.  Evidently, p = (po +pi)/2.  We need to find 
           a bet d such that in both cases the sum of information and real capitals does not 
           decrease:
                                        -logpo -  О  -logp, 
                                          log Pi +d ^   log p,
           or (finding the corresponding bounds for d)
                                 -logpo T log P > d ^  log p T log pi,
                                      log(p0/p) ^ d ^ log(pi/p).
           Such a d exists if and only if p/po ^ Pi/p■  This can  be rewritten  as p2 ^ popi 
           and  follows  from  the  inequality  between  arithmetic  and  geometric  means.  Note 
           also that po and p\ do not exceed 2p, so the bounds for d (and d itself) are in the 
           interval [—1,1].  The lemma is proven.
               One can explain the relation of this lemma and martingales (and an alternative 
           proof of the lemma)  as  follows.  The martingale makes some decision  about  the 
           proportion between two opposing bets, and this decision determines a multiplication 
           factor for the capital in a given round.  The possible choices are parametrized by a 
           point in a closed interval.  The factors for different rounds are multiplied, so their 
           logarithms are added.  The choices for the different values of the parameter can be
                                       9.12.  NON-MONOTONIC  SELECTION  RULES                            295
                        Figure 27.  Possible choices of a gambler presented in usual (left) 
                       and logarithmic  (right)  scale.  A  dashed  line  represents  possible 
                       choices for the game with bounded bets considered in the lemma.
              shown in a logarithmic scale, then we get a curve (instead of a line segment, if we 
              use a normal scale);  see Figure 27.  It  is easy to see that this curve is below the 
              tangent line, so the game will be better for us if we replace the curve by the line. 
              And this line corresponds exactly to the game with bounded bets considered in the 
              lemma.
                   Now it is easy to prove the following statement (where both the condition and 
              the claim are stronger than in Theorem 193):
                   Theorem 194.  Let к be an arbitrary computable upper bound for the function 
              C,  and let ш  be a sequence such that
                                                  k(üjQ ' '  '    А   (У .П
              for  some  a  <  1  and for  all  sufficiently  large  n.   Then ш  is  not  Mises-Church 
              random.
                   P r o o f.  For every n we can compute the list  An for all n-bit strings x such 
              that k{x) < an.  This list contains at most 2an+°(1) strings, and for all sufficiently 
              large n the n-bit prefix of ш is among them.
                   For these n the strategy constructed using our lemma  (for the set  An)  wins 
              at  least  (1 — a)n — 0(1)  dollars  playing  with  the  first  n  bits  of oj.  Consider  a 
              computable fast growing sequence of щ (we assume that n^-i/n* —> 0), and combine 
              the strategies for all AUi  into one.  In fact, the strategy for AUi  will be used only 
              after Пг-i  (where the preceding one stops), but this is a negligible fraction of щ. 
              So the combined strategy is successful:  Its gain on the first n bits exceeds en for 
              some fixed e and for infinitely many n.  (Take e < 1 — a and n = щ for large i.)
                   This is not possible for a Mises-Church random sequence (see Theorem 170 on 
              p.  267).                                                                                   □
                   Now we are prepared to prove Theorem 193.
                   PROOF.  Following the same scheme, we consider the set An of all n-bit strings 
              that have complexity at most an; it contains about 2an strings, including the prefix 
              of oj.  However, now we cannot compute the list of all elements of An, we can only 
              enumerate it, and we never know whether all the elements appeared or not.  So we 
              cannot  use An  in the lemma.  To overcome this problem,  we use non-monotonic 
              selection rules.
                  Again, we need to select a fast growing sequence щ, for example, щ = г!, and 
              cut the sequence into pieces of size щ — Щ-\.  Increasing a, we may assume that 
              the complexity of the ith piece is  at  most a  times its  length,  so  the complexity 
              per letter is at most a.  Let Ai be the set of all strings of length щ — n*_i  where
         296      9.  FREQUENCY AND GAME APPROACHES TO RANDOMNESS
         complexity per letter is at most a.  We know that the zth piece of из (we denote it 
         by U3i  in the sequel)  is in Aj,  and we can enumerate A* given i.  (The problem is 
         that we cannot compute Ai as a list of strings.)
            Let ti be the number of steps of the enumeration of Ai that are needed for w* 
         to appear.  Let us group the values of i into pairs and compare the values £2™ and 
         t2m+i-  Trivially, either £2™ ^ ^2m+i or £2m+i ^ t2m (or both).  How does this help? 
         We can now construct two strategies:  One reads U32m not making any bets, then 
         waits until U32m appears in A2m, thus finding t2m, and then makes the same number 
         of steps enumerating A2m+i  and uses the discovered part of A2m+i to construct a 
         gambling strategy (in the hope that u>2m+i is already discovered).  This works only 
         if t2m  ^ ^2m+i) otherwise we may lose all bets.  But then the symmetric strategy 
         (the one that reads U32m+i)  waits until uj2m+i  appears  in A2m+i  and makes the 
         same number of steps enumerating A2m will win.
            So for every sufficiently large m we have a pair of strategies (that makes bets 
         in  [—1,1])  and we  know that  at  least one of them is successful  (it wins  at least 
         l — o: per bet).  Omitting small m, we can combine them into two strategies in the 
         infinite game.  One of them is monotone, and we may (as we did in Theorem 170) 
         approximate it by an average of finitely many selection rules.  The number of the 
         selection rules depends on the required precision;  we need the error to  be small 
         compared to  1 — a,  and this can be achieved by a fixed  (=not depending on m) 
         number of Church admissible selection rules.  We denote this number by N.  The 
         other strategy is not monotonie,  and we get N Kolmogorov-Loveland admissible 
         selection rules.  In total we get 2N selection rules.
           Recall that Щ-\/щ is small; we note that the frequency deviation for some m 
         cannot be compensated by any behavior for previous m.  So for each m at least 
         one of 2N selection rules leads to a significant deviation.  Therefore,  there exists 
         one rule that leads to a significant deviation for infinitely many m,  and из is not 
         Mises-Kolmogorov-Loveland random.
           Theorem 193 is proven,                               □
           Together with Theorem 189 we get the following corollary:
           Theorem  195.  There  exist Mises-Church-Daley random sequences  that  are 
         not Mises-Kolmogorov-Loveland random.
           There is a natural question related to Theorem 193:  Is there some finite coun­
         terpart for this result?  Assume that we know that the complexity of some (finite) 
         string X is small.  Is there a non-monotonic selection rule of small complexity that 
         selects from x some unbalanced sequence?  Of course, the exact statement of this 
         type should include several parameters:  the length n of the strings, its randomness 
         deficiency d, the complexity of the selection rule (with n as a condition), and the 
         required imbalance in the selected subsequence.  In [53] the following result in this 
         direction is proven:  There exists a selection rule of complexity 0(\og(n/d))  (with 
        n as a condition)  that selects a subsequence where the number of ones and zeros 
        differ at least by Q(n/ log(n/d)).  In particular, if the randomness deficiency is pro­
        portional to n  (as in Theorem  193),  then the complexity of the selection rule is 
        bounded, and the imbalance is proportional to the length.
                          9.13.  CHANGE IN THE MEASURE AND RANDOMNESS             297
                        9.13.  Change in the measure and randomness
               In this section we describe a tool (suggested by M. van Lambalgen) that allows 
           us to construct Mises-Kolmogorov-Loveland random sequences with pathological 
           properties (not ML-random, with more ones that zeros in prefixes, etc.).
               9.13.1.    Randomness with respect to two measures.  Let us start with a 
           question that is interesting in itself:  Imagine that we change slightly the measure. 
           What happens with the class of random sequences (with respect to this measure)?
               Here are two examples of opposite types.
               Example  1.  Let  ß  be  the  uniform  Bernoulli  measure  ß  =  Вг/2.  Consider 
           another measure ß' that has independent trials with success probability 1/2 in all 
           trials except the first one where the success probability is (for example) 2/3.  It is 
           intuitively clear that the same sequences should be random with respect to both 
           measures (for all reasonable notions of randomness):  only the first trial is different, 
           and in both cases both outcomes are possible (though the probabilities are not the 
           same).  Indeed,  this happens for Martin-Löf randomness:  The effectively null sets 
           are the same  (because for every set its ^-measure and //-measure differs at most 
           by a factor of 2).
               274 Show the same result using the complexity criterion for randomness.
               275 Show that the class of computably random sequences for these two mea­
           sures is the same.
               Example 2.  Consider the uniform Bernoulli measure BXj2 and also some other 
           Bernoulli measure, say B2/3.  Is it possible that some sequence is ML-random with 
           respect  to  both  of them?  No,  because  for  a  random  sequence  with  respect  to 
           Bernoulli measure Bp the limit frequency is p, so it cannot be both 1/2 and 2/3 at 
           the same time.
               So  we come to  the  following question:  Imagine  that  two  sequences  of reals 
           PiiPi  £  (0,1)  are given.  Consider the measures for independent trials with prob­
           abilities pi  (call  it  ß)  and p[  (call  it  ß').  What  can  be said  about  the  classes of 
           ML-random sequences with respect to these two measures?  Our examples suggest 
           that if pi  and p[  are close to each other, then these classes should coincide, and if 
           Pi  and p'i differ significantly, these classes should be disjoint.
               Let us prove that this is indeed the case, assuming that p i and p[ are separated 
           from  0  and  1,  i.e.,  all  belong to  (e, 1 — e)  for  some  positive  e.  We  also  assume 
           that pi and  are computable sequences of computable real numbers (we need the 
           measures to be computable; otherwise, Martin-Löf randomness is not well defined).
              V. Vovk proved the following result [207] which is a constructive version of the 
           classical Kakutani’s theorem:
               Theorem  196.  (a)  If the sum J2(Pi ~ Pi)2  *s finite,  then the  classes  of ML- 
           random sequences with respect to ß and ß!  coincide.
               (b)  If the sum J2(Pi ~ Pi)2  *s infinite,  these classes are disjoint.
              The classical version of this result [70] says that in the first case the classes of 
           null sets coincide, and in the second case the measures are orthogonal,  i.e.,  there 
           exists a set that has probability 0 with respect to one measure and probability  1 
           with respect to the other.
                     298                    9.  FREQUENCY AND GAME APPROACHES TO RANDOMNESS
                            P r o o f.  Let  us  first  try  the  following  naive  approach  to  (a).  Assume  that 
                     oo  G  Q  is  random with respect  to p  (that  corresponds to probabilities Pi).  Then 
                     the (monotone) complexity of its n-bit prefix is close to a negated logarithm of the 
                     measure of the corresponding interval, which equals
                                                                                      П— 1
                                                                                      IT-
                                                                                      i=0
                     where           = pi  if ooi  =  1  and r*  =  1 — Pi  if ooi  —  0.  If pi  is  close to p'{,  then r*  is 
                     close to            (defined in  the similar way as r*,  but  for the other measure).  So the 
                     product of all Ti is close to the product of all r(; thus, randomness with respect to 
                     one measure implies randomness with respect to the other.
                            All this is indeed true and can be formalized easily, but for this argument we 
                     need to know that the sum
                     (*)                                            X^(“ 1oST) -  JZ(-logr')
                                                                    i=0                       i=0
                     is  bounded;  this is indeed true if ^2 \Pi — p[\  <  oo  (recall that we assume that pi 
                     and р[ are separated from 0 and 1).  But this is a much stronger assumption than 
                     the one we have—we know only that the sum of squares is bounded.
                            How  can  we  improve  this  argument?  Note  that  it  is  enough  for  us  if the 
                     difference (*) is bounded for every random with respect to p sequence.  Let us see 
                     why this happens.  Indeed, if oo is random with respect to p, then
                                                                                          n— 1
                                                        KM(U0 ■ ■ ' OJn—l ) = ^ (-lo g r;) + 0(1).
                                                                                           2=0
                     Since  the  complexity  and  the  negated  logarithm  of measure  differ  by  0(1),  the 
                    upper bound for the complexity in terms of p!  (recall that we can use arbitrary 
                    measure to get an upper bound for monotone complexity) guarantees that
                                                            71—1                      71—1
                                                            ^ (-lo g ri) ^ ^ (-lo g r') + 0(1). 
                                                            i—0                       2=0
                    Let us denote r[ — ri by S{.  Using this notation and taking the exponents, we get 
                    the following inequality (that is true up to a constant factor):
                                                                         71—    1       71—1
                                                                                        П(^+й)-
                                                                         i—0            2—0
                    We know that J2i $ï < °°, so Si —> 0 as i —> oo.  Therefore, for sufficiently large i 
                    the value of 6i  is smaller than e  (the gap between probabilities and 0,1).  We can 
                    change finitely many p\ and assume that it is true for all i.  Then we may consider 
                    a measure p" that is symmetric to p (i.e., pi  is the middle point between p\  and 
                    p'l).  For this measure we can write a similar inequality, only the sign before Si  is 
                    different:
                                                                         72—     1      71—1
                                                                          П г ^   ш * -  ^
                                                                          2 =  0        2 =  0
                          9.13.  CHANGE IN THE MEASURE AND RANDOMNESS            299
           (it is also true up to a constant factor).  Multiplying these two inequalities, we get 
           the inequality             n—1    n—1
                                      ГИ^п ^  ~
                                      i=0    i=0
           which is obvious anyway  (without any constant).  Due to our assumptions about 
           Ti  (separation from 0)  and Si  (the convergence of ^2 Sf),  the last inequality is an 
           equality  (up to a 0(l)-factor).  Indeed,  the product  П(1 ~ Ы)  is strictly positive 
           (assuming that 0 < hi < 1) if and only if ^  hi < oo.
               Now the main point:  Since the product of two inequalities is an equality, then 
           each  of them  is  also  an  equality  (up  to  a  0(l)-factor).  Then  the  randomness 
           criterion  (Theorem 90) guarantees that on is random also with respect to p'  (and 
           also p", but this does not matter).  So the statement (a) is proven.
               For statement  (b)  we postpone the proof since it works for the more general 
           case of dependent trials, and we will prove it soon in this more general version; see 
           Theorem 197 below.                                                     □
              The classical version of Kakutani’s theorem can now be proven as a corollary:
               276 Let Vo,V\,P2, ■ ■ •  and Po,p[,P2 , ■ ■ •  be sequences of reals in  (e, 1 — e)  for 
           some £ > 0.  Consider the measures p and p' that correspond to independent trials 
           with success probabilities pi and p', respectively.  Prove that:
               (a) if XXPi ~Pi)2 < oo, the classes of null sets with respect to p and p' are the 
           same;
               (b)  if YliVi — Pi)2  = oo, then p and p' are orthogonal  (there exists a set that 
           has measure 0 with respect to one measure and measure 1 with respect to the other 
           one).
               (Hint:  Use Theorem 196 in a relativized version.  Note that for every null set 
           one can find an oracle that makes the measures computable and makes this set an 
           effectively null one.  For (b) consider the set of random sequences.)
              Now we formulate this more general statement.  Let p be a measure on Q, and 
           let p(x) = p(£lx) be the corresponding function on binary strings.  If p(x) > 0 for 
           some X,  we can consider the conditional probabilities of 0 and  1  after x,  i.e.,  the 
           ratios p(0\x) — p(x0)/p(x) and p( 1 \ x) = p(xl)/p(x)  (their sum is 1).  For the case 
           of independent trials these values depend only on the length of x (and were denoted 
           earlier by pi).
              If,  in  addition to p,  some sequence on €  Q, is fixed,  we may consider the con­
           ditional probabilities of 1  after each term of this sequence, i.e., the sequence of pi 
           defined as
                        Pi = p (l|w 0 • ■ -Wi-i) =p(w0 • • -^_11)/р(а;о ■ --Wi-i)
           (they  are well  defined  if p  is  not  equal  to  zero  on  the  prefixes  of cu).  The  next 
           result  [207]  shows  that  these  probabilities  for  a  given  sequence  do  not  depend 
           much on the measure that makes this sequence random:
              Theorem  197.  Let w  be  a sequence  that  is  ML-random  with  respect  to  two 
           computable measures p and p!.  Assume that the conditional probabilities pi  and p\ 
           along ua  constructed using p and p'  are all in the interval (e, 1 — e) for some e > 0. 
           Then
                                       ^ ( P i- P ’i)2 < oo.
             300          9.  FREQUENCY AND GAME APPROACHES TO RANDOMNESS
                 Before proving this statement, let us note that it directly implies statement (b) 
            of Theorem  196.  Note also that  for a random sequence the numbers pi  are well 
            defined:  the  denominator p(x)  in p{xl)/p{x)  is  non-zero  for  a random sequence 
             (otherwise Q.x is an effectively null set).
                 PROOF.  Consider one more measure ß that averages the conditional probabil­
            ities for ß and p!.  Namely, the probability of 1 after x for ß" is an average of two 
            similar probabilities for ß and ß'.  (Note that ß" is not an average of ß and ß1:  if 
            we let ß(X) = (ß(X) + ß'(X))/2, we also get a measure, but a different one.)
                 Since the sequence to is random with respect to ß, we can use the Levin-Schnorr 
            criterion
                                                     n— 1
                                  KA (ш0 • • • LJn-i) = £ ( - 1 о ё гч) + 0(1),
                                                      i=0
            where (as before) n = pi if     = 1 and r* = (1 — pi) otherwise.  Similar equality is 
            valid for r[ that correspond to p!.  For the intermediate measure ß and corresponding 
            fi we know only the inequality (the upper bound for complexity).
                Thus, we have the following inequalities for conditional probabilities:
                                    n—1            n—1
                                    £(-logr;) ^ £ (-1 о 6тч) + 0(1),
                                    i=0             i=0
                                    n—1            n—1
                                    £ ( - 10^ )  < £(-iogfi) + o(i).
                                    i=0             i—0
            Taking the average of these two inequalities, we get
                             E  (-bgrO + C-logri) ^ g (_ logfJ + 0(1)
                                                          i=0
            Recalling that fi = (г* + г[)/2, we get
                          ^ ( (-‘ogn) + (-togfi) _  ( - b g ^ - t d ) )   <0(1).
            Each summand in the left-hand side is non-negative  (the logarithm is a concave 
            function) and is (up to a 0(l)-factor) equal to (г* — r()2, so
                                     £ ( n  -  A f ~ £ ( р { -  p'if < oo,
            as we claimed.
                (This  argument  has  a subtle  point  which  needs  to  be  mentioned.  We  con­
            sider the average of conditional probabilities, which creates a problem if one of the 
            measures has zero values:  In this case the computation of conditional probabilities 
            never terminates.  So the average measure ß is in fact a semimeasure,  but this is 
            enough for us (since the upper bound for complexity is still valid, and to never goes 
            through problematic points being random with respect to both measures).           □
                Note that this theorem says only that ß and ß' are close to each other along uj 
            (that is random with respect to both).  Of course, they can be completely different 
            in other parts of the Cantor space.
                            9.13.  CHANGE IN  THE  MEASURE  AND  RANDOMNESS              301
                 277  Give a corresponding example.
                (Hint:  Consider two measures that are uniform in the left half of the Cantor 
            space and differ significantly in the right half.)
                 278 Prove that in fact for this  argument we do  not  need Martin-Löf ran- 
            domness It is enough to know that cj is computably random with respect to both
            measures  (assuming that  both  measures  are  non-zero  on  all  Ц  and  conditional
            probabilities are well defined).
                This proof has an interesting game interpretation.  Assume that there are two 
            bookmakers who take bets for the same (sequence of) events, but who have different 
            ideas about their probabilities.  Each event has two possible outcomes 0 and 1:  the 
            first  bookmaker thinks that the probability of 1  is p (and the probability of 0 is 
            q — 1 — p) and therefore returns the bets on 1  [0] with coefficients 1/p [1/ç];  and 
            the second bookmaker does the same for probabilities p' and q'.  The probabilities 
            P)Ç)P/)Ç/ таУ be different for different events.
                (Note also that we assume that the bookmakers are altruistic; a more practical 
            bookmaker would use coefficients Co and C\ such that 1/co + l/ci  > 1.)
                Now assume that we are allowed to play with both bookmakers at the same 
            time, having a separate account for each (so we cannot use money from one book­
            maker to make bets with the other one).  In this language the argument  above 
            shows that if all p, ç,p/, q1 are separated from 0 and the sum YliP ~ P')2 ls infinite 
            (for the sequence of events),  then we can achieve unbounded capital in  (at least) 
            one of the games for sure (for every sequence of outcomes).
                How do we achieve this?  Assume that in the first game our current  capital 
            is  и,  in  the second game our capital is v,  and the  (assumed)  probabilities of the 
            outcomes are p,q (in the first game) and p\q'  (in the second one).  Let us split и 
            between two bets in the proportion that brings us (p + p')u/2p for outcome 1 and 
            (q + ql)u/2q for outcome 0.  (It is easy to check that these bets are valid, i.e., the 
            expected value of capital after the game according to the (p, ^-distribution is u.) 
            In the second game capital v becomes (p + p/)u/2p/ (for outcome 1) or (q-\-q')v/2q' 
            (for outcome 0).  Let us track the product of capitals in both games.  It is multiplied 
            either by (p + p/)2/4pp/ or (q + q')2/^qq'■  The inequality between arithmetic and 
            geometric means (the concavity of the logarithm function) guarantees that in both 
            cases the product increases.  Taking the logarithm and estimating the increase, we 
            note that for p^q^p'^q' separated from 0 and for the case Ylip ~ p')2  —  +°° the 
            product of the capitals tends to infinity, so at least for one of the games the capital 
            is unbounded.
                In other words, we have constructed two martingales (with respect to two mea­
            sures), and they have the following property:  For every sequence where conditional 
            probabilities for both measures are separated from 0 and 1 and the differences of 
            probabilities  have  infinite  sum  of squares,  at  least  one  of the  martingales  is  un­
            bounded.
                9.13.2.  The  Law  of Large  Numbers  for  variable  probabilities.  The
            SLLN says that for every p G  (0,1) and for the corresponding measure Bp  (inde­
            pendent trials with success probability p) the set of sequences with limit frequency 
            p has measure 1 (and its complement, the set of sequences where the limit does not 
            exist or is different from p, is a null set).
               302             9.  FREQUENCY  AND  GAME APPROACHES  TO RANDOMNESS
                    Now assume that the trials are independent, but the success probabilities are 
               different  for  different  trials  (pi  for  the  îth  trial).    One would  expect  that  if pi 
               converges to some p, then again for almost all (with respect to the product measure) 
               sequences,  the  limit  frequency  is p.  The  intuitive  explanation  follows:  Consider 
               some e > 0.  All pi except for a finite number of them (which could be ignored) are 
               smaller than p+ef 2.  We know that if we replace all pi by p+ej2, then almost surely 
               the frequency will be less than p + e starting from some moment.  By monotonicity 
               reasons, this should be true also for the original pi.
                    With some effort, this argument can be made precise, but we prefer to prove 
               a more general statement that is useful in many cases.  It deals with an arbitrary 
               measure ß on Q, let p(x) = ß(flx) be the corresponding function on strings.  For 
               every binary string x = xq ■ • • xn_i  consider the number of ones in x, and also the 
               sum of conditional probabilities pi of the appearance of 1 in the ith position of x 
               (after the corresponding prefix),
                                             Pi =p{x0 • ■ -Xi-il)/p(x0 ■ ■ -Xi_i).
               Both quantities (m and the sum of pi) depend on x; the bound below guarantees 
               that with high ß-probability for a given n these quantities are close to each other. 
               Here is the exact statement (we have explained what m and Pi are; the probability 
               is taken with respect to ß):
                    Theorem 198.
                                       Pr[|m -  (po + ----bPn—\)\ > d] ^ 2e~d2/4n.
                    This theorem is essentially finite,  and this inequality is true for every proba­
               bility distribution on n-bit strings.  This is a weak form of a classical inequality in 
               probability theory called the Azuma-Hoeffding inequality.  We will give a simple 
               argument that uses the same technique that we used for the proof of the SLLN, 
               though it does not give the best possible result  (in fact, the constant  1/4 can be 
               replaced by 2, so one can get the bound with right-hand side 2e~2d ln  [66, Theo­
               rem 2]).
                    P r o o f.  We  consider  separately  the  probabilities  of m  being  too  big  or  too 
               small.  Both cases are similar,  so we need to consider one of them and prove,  for 
               example, that
                                         Pr[m -  (po H------ bpn-i) > d] ^ e~d2/4n.
               We use a standard trick and construct some measure ß' for which the ratio ßl jß is 
               big for all sequences that belong to the event in the left-hand side.  (The ratio ß'/ß 
               is a martingale that reaches ed !4n if this event happens.)
                    Since we want to increase the measure for the sequence with many ones, it is 
               natural to increase the conditional probability of 1 in ß' (compared to ß).  Namely, 
               if for the original sequence the conditional probabilities of 1 and 0 after some x are 
               p and q, in the new measure we let these probabilities be
                                                p' =p + epq,  q' = q~ epq,
               where e is some positive real.  The value of e will be chosen later; now we say only 
               that £ does not exceed 1/2.  It is easy to check the p' and q' are still in [0,1].
                    So we get another probability distribution on n-bit strings.  Let us compute the 
               ratio of these two distributions on some string x of length n.  Each bit of x adds a 
               factor p'/p (if this symbol is 1) or q'/q, where p, q,p', q' are conditional probabilities
                                                                                      9.13.           CHANGE IN THE MEASURE AND RANDOMNESS                                                                                                                                      303
                                    of 1  and 0 in the old measure and in the changed measure.  In logarithmic scale, 
                                    the logarithm of the ratio in question is equal to the sum of
                                                                                    ln(p7p) = ln(l + eq) ^ eq -  (eq)2 ^ e(l -  p) -  e2
                                    or
                                                                                        \n(q'/q) = ln(l — ep) ^ —ep — (ep)2 ^ —ep — e2
                                    for  all  bits  in  x  (with corresponding probabilities p  and q).  The first expression 
                                    is  used  for  ones  in  x  and  the  second  is  used  for  zeros.  (We  used  the  inequality 
                                   ln(l + h) ^ h — h2 that is true for all h with \h\ ^ 1/2.)
                                                What happens when we add all these inequalities?  We get a lower bound for 
                                   the logarithm of the ratio p'(x)/p(x) of two measures for x; this lower bound has 
                                   a common factor e.  Then we have m ones (each term (1 — p) contributes one, and 
                                   there are m of terms of this type) minus the sum of all p*, where pi is the conditional 
                                   probability of 1 at zth place, and minus ns2,
                                                                                                                  i      P'W ^  (                                     V-           a                 2 
                                                                                                                  m —                      > £{m -  l^pi) -  пе  . 
                                                                                                                          p{x)
                                   We are interested in x where the excess m — ^P i exceeds d.  For them we have
                                                                                                              ,      P'{x)                         ,                                                     ч
                                                                                                              In           .    .      > Ed — ПЕ  = E{d — пе). 
                                                                                                                      p(x)
                                   This is true for all e  G  (0,1/2), so let us choose e  to get the strongest statement. 
                                   The right-hand side  achieves  maximal value when  e  =  d/2n  (note  that  d  ^  n, 
                                   because the number of ones in x does not exceed n, so £ = d/2n < 1/2).
                                               Therefore
                                                                                                                                     In V'(x) > d2/An
                                                                                                                                             p{x)
                                   for x where the excess is greater than d, and
                                                                                                                                       P'(x)  .                    d2/4n
                                                                                                                                        p(x)
                                   as we claimed.  This finishes the proof.                                                                                                                                                                                                       □
                                               It is instructive to look at the special case where zeros and ones are équiproba­
                                   ble.  The probability that the number of ones exceeds its expectation (n/2) by 2^/n 
                                   is bounded by 1/e, and expectation exceeds 2кл/п with probability at most l/efc 
                                   Comparing this with the  classical  de  Moivre-Laplace  theorem,  we  see  that  our 
                                   bound is not optimal, but the difference is only in the coefficient (in the exponent).
                                               Now we can repeat the proof of the SLLN with this bound and get the following 
                                   result that is true for arbitrary measure p on the space fh
                                               Theorem  199.  For a sequence cj — cjquji ■ ■ ■  that has  distribution p,  the fol­
                                   lowing property holds with probability 1:
                                                                                                              lim  frn  _   pp H-------- Vpn-1
                                                                                                           n—>oo \ n                                               n
                                   where m  is  the  number of ones  among wo,wi, • • ■ ,Wn-i  omd p  is  the  conditional 
                                   probability of 1  after cjqcji • • -u^-i  according to p.
             304          9.  FREQUENCY AND  GAME APPROACHES TO RANDOMNESS
                 In particular,  this theorem implies that for independent trials where success 
             probabilities Pi  converge to some limit p, the limit frequency p exists with proba­
             bility 1  (since the limit of averages (po + • • • +pn-i)/n is also p).
                 On the other hand, we need more:  We wanted to show that the sequence is 
             random according to a Mises-style definition  (in one of the versions),  so we need 
             to ensure that the limit frequency is p not only for the sequence itself, but for all 
             its  subsequences selected by some admissible rule.  Let  us look at this  question, 
             starting with monotone selection rules (the Church-Daley definition).
                 9.13.3.      The Law of Large Numbers for subsequences.  Let us generalize 
             Theorem  198 by adding selection rules.  This theorem  considered some measure 
             д on fi, and the corresponding function p{x) = p(fix)  and conditional probability 
            p(l I x) = p(xl)/p(x).  For a sequence из and number n we considered two quantities:
                    •  the number m of ones among wo,..., wn_i ;
                    •  the sum of conditional probabilities of 1 at n first positions, i.e.,
                          p(l|A) +p(l|w0)+p(l|w0wi) -I-----hp(l|wo---wn_2).
             Theorem 198 guaranteed that (for every n) these two quantities differ significantly 
             only for a д-small fraction of sequences из (the д-measure of the set of из such that 
             the difference exceeds d is bounded by 2e~d /4n.
                 Now let us add some selection rule Sr to this picture.  Here R is an arbitrary 
            set  of  binary  strings  (the  set  of prefixes  before  the  selected  terms).  For  every 
            sequence из the rule Sr selects some positions го, ii, • • • and outputs a subsequence 
             Sr(u3) = U3i0u3i1 ■ ■ ■  made by the terms at those positions.  Now we compare:
                    •  the number m of ones among the first n selected terms, i.e., the number 
                      of ones among n first bits of S#(w);
                    •  the sum of conditional probabilities of ones for the n first  selected posi­
                      tions, i.e.,
                  P( 1 l^oWl • • • Wio-l) + p( 1 l^oWi • ■ -U3ii_i) -I- • • ■ +p( 1 |Ш0^1 • • ■U3in_1-i).
            When R is the set of all strings,  we get the same quantities as in Theorem  198. 
            The following theorem says that the same bound is valid in the general case:
                 Theorem 200.  The p-measure of the set of из where these two quantities differ 
             by d,  is  at most 2e~d2^4n.
                 Note that for some из the sequence Sr(co) may contain less than n terms.  These 
            из are not elements of the set whose measure is bounded.
                 Example.  Consider an arbitrary measure д and a selection rule R that selects 
            the  terms where  the  conditional  probability  of  1  is  at  most  1/2.  Theorem  200 
            guarantees that the д-probability of the event “there is at least 51% of ones among 
            the first n selected terms”  is exponentially small (as n increases).
                 P r o o f.  This theorem is also a simple consequence of a general Azuma-Hoeff- 
            ding inequality for martingales in the sense of probability theory, but we can easily 
            modify the argument used to prove Theorem 200.
                 First note that this theorem now deals with the entire sequence из instead of its 
            prefix of some length (since the first n selected terms may appear arbitrarily late). 
            But due to the continuity of measure, it is enough to prove the same inequality for
                            9.13.  CHANGE IN  THE  MEASURE AND  RANDOMNESS             305
            first  N bits of u)  (it gives a smaller set, but the union of all these sets for all N is 
            the set in question).
                As in the proof of Theorem 200, we construct a new measure, but now we change 
            conditional probabilities only in the positions where the next term is selected, i.e., 
            the conditional probabilities of p(l\x)  for x  G  R.  Then the ratio of probabilities 
            (for  the  modified  and  original  measures)  will  include  the  length  of the  selected 
            subsequence (instead of the sequence itself) and the sum of conditional probabilities 
            for  the  selected  terms  (instead  of the  sum  of all  conditional  probabilities),  i.e., 
            exactly what we need.                                                       □
                This result gives us a tool (following van Lambalgen [90]) to construct Mises- 
            Church--Daley sequences.  Let Po,Pi, ■ ■ ■  be a computable sequence of computable 
            reals that computably converges to some p G (0,1). This means that for every given 
            £ > 0, one can compute some N such that  \pi — p\  < e for all i ^ N.  (It is easy 
            to see that in this case p is computable.)  Consider a computable measure ß that 
            corresponds to a sequence of independent trials with probabilities pi.
                Theorem 201.  Every ML-random sequence with respect to ß is Mises-Church- 
            Daley random with frequency p.
                PROOF.  Consider some rational e > 0 and some Church-Daley admissible rule 
            Sr  (not  necessarily  total).  We  need  to  show  that  the set  U  of all  sequences uj, 
            where Sr(oj) gives an infinite sequence in which the frequency of ones exceeds p + e 
            infinitely often,  is an effectively null set.  (The argument for frequencies less than 
           p — e is similar.)
               Fix some n, and consider the set Un of sequences uj such that Sr{u)) has length 
            at least n and the frequency of ones among the first n terms exceeds p + £.  This set 
            is effectively open (applying the computable partial selection rule to all branches, 
            we enumerate all strings that guarantee that its every extension is in Un).
               Theorem 200 provides the upper bound for the ß-measure of Un.  If n is large 
            enough, the average of conditional probabilities is less than p + e/2 (and we know 
            how large n should be,  since the sequence pi  converges computably to p).  This 
            bound decreases exponentially as n increases.  So one can cover U by an effectively 
            open set of arbitrarily small measure (taking all the intervals in Un, Un+i, Un+2 , ■ ■ ■ 
            for arbitrarily large N; recall that by definition of U each element of U belongs to 
            infinitely many Un).  Therefore, U is an effectively null set and cannot contain an 
            ML-random sequence.                                                         □
               Now  we  extend  this  statement  to  Mises-Kolmogorov-Loveland  random  se­
            quences.
               Theorem  202.  Every  ML-random  sequence  with  respect  to  ß  is  Mises- 
            Kolmogorov-Loveland random with limit frequency p.
               PROOF.  The application of a Kolmogorov-Loveland admissible selection rule 
            Sp,G   a sequence uj  consists of two phases.  First we use F to select a sequence 
           of revealed bits (both selected and not selected).  Then G is used to select some of 
           the revealed bits, and this operation is a Church-Daley admissible selection rule.
               After this decomposition is done, let us consider the distribution of a sequence 
           up  that  is  obtained  at  the  first  step.  (We  assume  that  the original sequence  is 
           obtained by independent trials with probability pi  in the îth trial.)  Let  us first 
           assume that F and G are total functions.
             306           9.  FREQUENCY AND  GAME APPROACHES TO RANDOMNESS
                 The first  term of uip  is chosen when no bits are revealed;  it occupies a fixed 
             position in u>.  This position is no  = F(A).  So the probability that  1  appears as 
             the first term in up  is pno•  Now consider the second term of up.  Its position in 
             Q. depends on the value of uino  and can be either F(0) or F( 1).  So the conditional 
             probability for the second term of uip to be one (given the first term as the condition) 
             is either Pf(o)  or Pf{i)-  In general, the probability of 1 after x in uip equals Pf{x)-> 
             since the next revealed bit has number F(x).  Note that the restrictions of F  (no 
             bit  is  revealed  twice)  guarantee  that  conditional  probabilities  along  any  branch 
             form a subsequence of {p*}  (without repetitions), so the number of terms outside 
             of (p — e/2,p + ef 2) for this subsequence is bounded by the number of such terms 
             in the original sequence.
                 This  allows  us  to  apply  the  bound  of Theorem  200  to  sequence cop  and  the 
             selection rule determined by G in the same way as in the proof of Theorem 201.
                 The same bound is true for partial F and G:  Extending them to some total 
             functions, we may only increase the set whose measure is bounded.  (A computable 
             partial function may have no computable extensions, but here we are interested in 
             the bound only, so a non-computable extension can be used.)
                 Having proven the bound for partial F and G, we now observe that for com­
             putable F and  G the set  of u>  such that  Sf,g(w)  starts with x,  is  an effectively 
             open set (uniformly in x).  So now we can finish the argument as in the preceding 
             theorem.                                                                            □
                 Remark.  One can give a more direct proof of Theorem 202.  Here is one of 
             the possible arguments (taken from [177]).
                 Fix some (rational) e > 0 and some Kolmogorov-Loveland admissible selection 
             rule Sf,g-  We need to show that the set  U of sequences u> such that  Sf,g(w)  is 
             infinite and the frequency of ones exceeds p + e infinitely often is an effectively null 
             set.  (The argument for p — e is similar.)
                 By Un we denote the set of all sequences u> such that Sf,g{^>) contains at least 
             n terms and the frequency of ones among the first n terms exceeds p + e.  We need 
             to show that the series    ß(Un) computably converges.  (Here ß is our measure; it 
             corresponds to independent trials with success probabilities pi.)
                 By <2n,fc0r ? • • •, тп) we denote the probability of getting more than к ones on n 
             independent trials with success probabilities r\,... ,rn.  The function an^ is a non­
             decreasing one with respect to each argument r*.  (A side remark:  it is a multilinear 
             function, i.e., a polynomial of degree at most 1 with respect to each argument.)  It 
             is easy to see also that anj. < anj if к ^ I.
                 We claim that ß(Un) ^ &п,к{г1-> • • • ? rn), where к = n(p + e)  and r*  is the ith 
             element of {po,Pi, • • •} in decreasing order, or, more precisely,
                     ri = sup Р»,   r2 = sup min (pi,pj),  r3 =  sup  min (pi,pj,pk), ■■■■
                                         ij£j                    i^jßk
             Let us show that this bound implies the convergence of the series ß{Un).  Obviously, 
            ri  ^ r2  ^  ■ ■ ■  and limn  = p.  Let us replace all n that exceed p + ej2 by 1; let s 
            be the number of these replacements.  We conclude that
                                   KUn) ^ &n—s,k—s{p “b ^/2j ■ • ■, p T ^/2),
            so we have reduced the question to the case of constant probabilities (that we have 
            already discussed several times).  It is important to note that for large n the ratio 
             (k — s)/(n — s) is close to p + e and exceeds p + e/2 significantly.
                                      9.13.  CHANGE  IN  THE  MEASURE  AND  RANDOMNESS                                   307
                     It remains to prove the bound for ß(Un) mentioned above.  Again imagine that 
                the bits of the sequence are written on cards that lay on the table face down, and 
                the selection rule determines which bit should be revealed next and whether it is 
                used just for information or is selected.  While applying the selection rule,  let us 
                record which cards were turned over and the corresponding bits; we get an (infinite) 
                record, or log file.  Let 7Г be the initial segment of such a record.  By п(тг) we denote 
                the number of terms that were selected during 7Г, and by к (it) we denote the number 
                of ones among these terms.  By rfiir) we denote ith biggest number in a sequence 
                obtained from рсьРь • • • by deleting the terms that correspond to the bits already 
                revealed (for information or for selection).
                     Let ß(Un 17r) be the conditional probability of Un with the condition “the record 
                starts with 7r”.  Let us prove the generalized version of the inequality in question: 
                If n(7г) < n, then
                (*)                      ß(Un\ir) ^ ап-п(тг),к-к(тг)(Г1(1г),Г2(тг), .. .).
                For empty 7r we get the bound we are interested in.  This inequality can be proven 
                by backward induction.  If п(тг) = n, this inequality becomes an equality (left- and 
                right-hand sides are either both zeros or both ones).  Let п(тг)  < n,  and let m be 
                the index of the bit that is turned over immediately after pi (if F(tt) is not defined, 
                then ß(Un 17г) = 0 and we have nothing to prove).  Then
                                        H(Un\ir) =Pmß(UnU\) + (1 -PmM^nko).
                where  7To  and  7Ti  are  obtained  from  7Г  by  adding  that  the  mth  card  contains  0 
                (respectively 1).  Let us show that if (*) is true for 7To and 7Ti, then it is true for it. 
                Indeed, in this case ß(Un 17r) does not exceed
                (**)      Рт&п—п{-к\ ),k — fc(7ri) (^1 (tTI )?■•■) "Ь (1    Pm)otn—n(no),k — k(no) (^"l (^o)? • • •)•
                If the bit on the mth card is used only for information, then n(7To) = n(7Ti) = п(тг) 
                and к(тто) — к(тт\) — k(ir), and it remains to use the monotonicity of an^ and the 
                inequality Гг(7Го) = r.j(7Ti) ^ rfiir).  On the other hand, if the mth bit was selected, 
                then ti(ttq)  = п(тг\)  = п(тт) + 1,  к(тто)  = к(тт),  and k(TTi)  = к(тт) + 1,  and  (**)  is 
                equal to
                                             ап—п{ж),к — к{ж) (P m ? n(7Tl),7-2(7ri), • • •)■
                Note that r;(7Ti) = rfittq) and therefore does not exceed
                                                 ^n—n(n),k — k(n) (Т1 (^") ) Г 2 (^")> • • ■)•
                This ends the proof of the inequality (*) in the case when all 7Г with п(тт) — n have 
                bounded length.  If not, this argument gives a bound for ß(Un,t 17r) where Untt  is 
                the set  of all  sequences  from which a subsequence of length n with more than к 
                ones is selected after revealing at most t bits.  It remains to let t tend to infinity.
                     9.13.4.          Examples.  Now it is easy to  prove  the existence  of Mises-Kolmo- 
                gorov-Loveland sequences with some pathological properties.
                     Theorem  203.  (a)  There  exists  a  Mises-Kolmogorov-Loveland  random  se­
                quence with frequency 1 /2 that is not ML-random with respect to the uniform mea­
                sure.
                     (b)  There exists a Mises-Kolmogorov-Loveland random sequence with frequen­
                cy 1/2  where each prefix has more zeros than ones.
               308             9.  FREQUENCY AND GAME APPROACHES TO RANDOMNESS
                    (c)  There exists a Mises-Kolmogorov-Loveland random sequence with frequency 
               1/2 such that some Kolmogorov-Loveland admissible (and even Church admissible) 
               rule selects from it a subsequence that is not Mises-Kolmogorov-Loveland random.
                    (d)  There  exists  a  Mises-Church-Daley random sequence  with frequency 1/2 
               that becomes not Mises-Church random after a computable permutation of its terms.
                    P r o o f,  (a)  Consider  a  computable  sequence  of rational  numbers  that  con­
               verges to 1/2 very slowly, for example,
                                                              1        1
                                                       Pi    ~ 2 ”   JT+ Î
               (here 5 is added to make all pi positive).  Consider the computable measure p that 
               corresponds to independent trials with success probabilities p*.
                    Since the series XXp* — 1/2)2 diverges, no ML-random sequence (with respect 
               to ft)  is ML-random with respect to the uniform measure (Theorem 197).  On the 
               other hand, all these sequences are Mises-Kolmogorov-Loveland random with limit 
               frequency 1/2 (Theorem 202).
                    (b) This statement can be proved in a similar way, but one should consider the 
               sequence pi that converges to 1/2 even more slowly.  For example, let
                                                        _  1          1
                                                     Рг       2 log(i + 5) '
               What is the probability (according to ft) of the event  “n-bit prefix contains fewer 
               zeros than ones”  (the frequency of ones exceeds 1/2)?  Theorem 198 says that this 
               probability (let us denote it by 6n) is bounded by e“n/°(log                  (the threshold d is 
               about n/0(logn), and d2/4n = n/O(log2n)).  The series                           converges, so the
               probability of the event  “all prefixes of length at least N contain at least as many 
               zeros as ones”  is positive.  The set of positive measure  (obviously)  contains some 
               ML-random (with respect to ft) sequence.  Now we know that there exists an ML- 
               random sequence (with respect to ft) where all prefixes of length at least N have at 
               least as many zeros as ones.  This sequence is Mises-Kolmogorov-Loveland random 
               (Theorem 202).  Adding jV+1 zeros before it, we get a Mises-Kolmogorov-Loveland 
               sequence with the required property.
                    (c)  As W.  Merkle has noted,  here we can use the same trick to generate the 
               required sequence.  However, the sequence Pi should be chosen in a more compli­
               cated way.  Let  us split  the  sequence into  blocks:  the  kth  block  consists  of one 
               term,  where probability  (to be  1)  equals  1/2,  and two  parts  of equal  (and  large 
               enough) length Пк.  In the first part the probabilities of 1 are the same and exceed 
               1/2 slightly, while in the second part they are slightly less than 1/2 (and again the 
               same);  see Figure  28.  Here Ek  is strictly positive and converges to 0 as  к  —>  oo,
                                                   Пк                              Пк
                                 1        2  +£k,--- 1  ■  _              1    _        1-£k
                                 2                      2  + £k           2 _ £k' ' ' 1 2
                                   F ig u r e   28.  The kth. block and the probabilities
                            9.13.  CHANGE IN  THE  MEASURE AND  RANDOMNESS             309
            but the convergence is slow.  More precisely, we need the following relation between 
            Пк and Ek :  The probability that for Пк independent trials with success probability 
            I + Ek the fraction of ones is strictly greater than 1/2 is at least  1 — 2“^fc+3l   (In 
            fact, this can be achieved for any sequence Ek if Пк are sufficiently large.)
                If Пк and Ek are chosen in this way, we get with positive probability a sequence 
            u)  such  that  for  all  к  in  each  of the  two  halves  of the  kth.  block  the  imbalance 
            between zeros and ones is in the expected direction (more ones in the left half and 
            more zeros in the right half).
                Therefore, there exists an ML-random (with respect to fi) sequence uj with this 
            property, and Theorem 202 guarantees that it is also Mises-Kolmogorov-Loveland 
            random.
                Now let us show that some Church admissible selection rule selects from lo a 
            sequence that  is  not  Mises-Kolmogorov-Loveland  random.  The selection rule is 
            very simple:  we select the first bit of each block all the time, and depending on its 
            value we select either all bits in the left half or all bits in the right half (of the rest 
            of the block).
               Why is the selected subsequence ui' not Mises-Kolmogorov-Loveland random? 
            This is easy—the imbalance condition allows us to reconstruct the value of the key 
            (the first bit of the block) by counting zeros and ones in the following Пк bits.  So 
            we first read  bits that go after the key bit and then predict correctly the value 
            of the key bit.  This ends the proof of (c).
               We may also note that the sequence uJ is Mises-Church-Daley random, since 
            the  composition  of two  Church-Daley  admissible  rules  is  again  a Church-Daley 
            admissible rule.  On the other hand, if we permute ui' (computably) by placing the 
            key bit  after  all  the  other  bits  of the same block,  we get  a sequence that  is  not 
            Mises-Church random.  So statement (d) is also proven.                      □
               Note that this result shows that all the Mises-style definitions of randomness 
            are  not  very  natural.  Indeed,  Mises  noted  the  following  important  property  of 
            Kollektivs:  Applying  an  admissible  selection  rule  to  a Kollektiv,  we  should  get 
            another Kollektiv.  And statement (c) shows that the Mises-Kolmogorov-Loveland 
            definition does not  have this property.  The definitions with monotonie selection 
            rules  (Mises-Church,  Mises-Daley)  have  this  property,  but  are  not  stable  with 
            respect to computable permutation of terms, which is also quite strange.
                279  Prove that by applying a Kolmogorov-Loveland admissible selection rule 
            to a Mises-Kolmogorov-Loveland random sequence we get a Mises-Church-Daley 
            random sequence.
                280  Prove that there exists a Mises-Kolmogorov-Loveland random sequence 
            (with limit frequency 1/2) that is not even Kurtz random.
               (Hint:  Take a sequence that is random with respect to a slightly biased measure. 
            One of the martingales constructed in the proof of Theorem 197 is bounded on it, so 
            the other one has a computable lower bound that computably converges to infinity.)
               It seems that the examples of this section tell us that the Mises-Kolmogorov- 
            Loveland definition is probably too weak from the intuitive viewpoint.  The same 
            can be said about the definition of computably random sequences.  Now the nat­
            ural question arises:  What happens if we combine these two definitions?  Assume 
            that  we  play with  a sequence of bits  that  is selected in arbitrary  (not  necessar­
            ily monotonie) order as in Kolmogorov-Loveland selection rules, but we can make
        310     9.  FREQUENCY AND  GAME APPROACHES TO  RANDOMNESS
              Figure  29.  Relations between different notions of randomness
        bets  of arbitrary  computable  size  (not  exceeding  the  current  capital),  as  in  the 
        definition of computable random sequences?  One can naturally define this class of 
        sequences;  they are sometimes called  unpredictable  or Kolmogorov-Loveland ran­
        dom.  It is easy to prove that all ML-random sequences have this property, but it is 
        not known now (2017) whether these two classes coincide.  One can note also that 
        for non-monotonic martingales  (games with non-monotonic subsequences) it does 
        not matter whether they are total or not  (the same argument as in Theorem 191 
        (p.  291) works).
          Finally, let us summarize what we know about relations between different no­
        tions of randomness (Figure 29).  We have two columns; in each column the notion 
        becomes weaker  (and  the  class  of random  sequences  increases)  as  we  go  down. 
        The left column contains game definitions (extending Ville’s idea in different ways) 
        that  use martingales of different  types.  The right  column  (extending von Mises’ 
        ideas in different ways)  uses selection rules of different types.  Both columns start 
        with  Martin-Löf randomness  (that  is  equivalent  to  randomness  with  respect  to 
        lower semicomputable martingales) and the definition of randomness with respect 
        to non-monotonie (partial) computable martingales.  We do not know whether these 
        definitions are equivalent or if the second one is weaker.
          In the right column the class of admissible selection rules decreases; in the left 
        column the requirements  for a winning strategy increase  (see Section  9.9,  Theo­
        rem  183,  and  Problem  272).  So  the corresponding classes of random  sequences 
        increase when we go down.
          To see why all the implications shown (except for the top one) are irreversible 
        and there are no other implications, let us consider the relations between columns. 
        As we have seen, Mises-Kolmogorov-Loveland randomness does not imply even the 
        weakest notion in the left column—Kurtz randomness (Problem 280)—so there are 
        no implications going from right to left.
                             9.13.  CHANGE  IN  THE  MEASURE  AND  RANDOMNESS             311
                When going  from  left  to  right,  the  “random  with  respect  to  partial  com­
            putable martingales sequences”  are Mises-Church-Daley random  (Theorem  190, 
            p.  290);  computably random sequences are Mises-Church random (Theorem 182, 
            p.  279);  Schnorr randomness implies SLLN (Problem 92, p.  70).  But these impli­
            cations cannot be improved:  Kurtz randomness does not imply the SLLN (Prob­
            lem  92,  p.  70);  Schnorr  randomness  does  not  imply  Mises-Church  randomness 
            (Problem 271, p. 284); computably random sequences may be not Mises-Church- 
            Daley random, since in the first case the complexity may grow as O(logn)  (Theo­
            rem 182, p. 279) and in the second case this is impossible (Theorem 188, p. 287); 
            finally, randomness with respect to computable partial martingales does not imply 
            Mises-Kolmogorov-Loveland randomness, since in the first case the complexity of 
            prefixes may be sublinear  (Theorem  190,  p.  290)  while in the second case this is 
            not possible (Theorem 193, p. 293).
                Therefore in each column all the arrows are irreversible (since the consequences 
            in the right column are different for each notion in the left column;  similarly for 
            the right column), so indeed no other implications can be added to our table.
                               CHAPTER 10
            Inequalities for entropy,  complexity, and size
                       10.1.  Introduction and summary
           The first publication of Kolmogorov in which the definition of complexity was 
        given  is  the  paper  “Three  approaches  to  the  quantitative  definition  of informa­
        tion”  [78].  The three approaches mentioned there are combinatorial, probabilistic, 
        and algorithmic.
           The algorithmic approach measures the amount of information in a message 
        by  its  Kolmogorov complexity  (as we  call  it  now—of course,  this  name was  not 
        used  by  Kolmogorov  himself).  Using  the  probabilistic  approach,  we  consider  a 
        message as one of the possible values of some random variable, and we measure the 
        Shannon entropy of this random variable.  But Kolmogorov started the paper with 
        the combinatorial approach, making the following (trivial) observation:  If there are 
        N different messages that can be transmitted, we need log N bits to specify which 
        of the messages is transmitted.  (If we need to guess one of N objects, we need to 
        ask logiV yes-or-no questions.)
           We have already mentioned some results that relate these three approaches.  For 
        example, Theorem 8 (p. 19) related the combinatorial and algorithmic approaches 
        and refined the following (absurd, if understood literally) statement,  “a string x has 
        complexity at most n if and only if x belongs to the set of at most 2n elements”. 
        Another example is that the results of Section 7.3 relate Kolmogorov complexity 
        and Shannon entropy.
           In this chapter, following [64,  157], we establish more formal connections be­
        tween the three approaches and restrict ourselves to a rather specific class of state­
        ments:  linear inequalities for entropy and complexity (and corresponding combina­
        torial statements).
           Let  x\,... ,xn  be binary  strings.  For every non-empty set  I  C  {1, 2,... ,n} 
        of indices,  consider the tuple xj  made of all X{  with i  €  I.  We are interested in 
        the Kolmogorov complexity of this tuple.  For example, for n = 3, we have seven 
        possible tuples, and we get a list of seven complexities:
              C(xi), C(x2), C(x3), C(xi,x2), C(xi, x3), C(x2, x3), C(xi,x2,x3).
        Let us give several examples of linear inequalities where these complexities appear:
             •  C(x\,x2) ^ C(xi) + C(x2) + O(logiV);
             •  C(xi,x2,x3) ^ C(xi) + C(x2,x3) + O(logiV);
             •  C(xi,x2,x3) + C(xi) ^ C(xi,x2) + C(xi,x3) + O(logiV);
             •  2C(xi,x2,x3)  ^ C(xi,x3) + C(x2,x3) + C(xux2) + O(logiV)
        (we assume here that x\,x2,x3 are strings of length at most N).
                                   313
          314        10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY  AND SIZE
             In general, linear inequality for complexities has the form
                                X>C(*7) ^ O(logAZ'), 
                                 i
          where the sum is taken over all non-empty subsets of {1,..., n}.  The coefficients А/ 
          may be positive,  negative,  or zeros;  we assume that all strings X{  have length at 
          most N, and the constant in О-notation does not depend on N (but may depend 
          on n and the inequality chosen).
             Which of these inequalities  are true?  More formally,  we are looking for the 
          tuples of coefficients A/ such that
                                 ^  XIC(xI) ^ clog N
                                  I
          for some c, every N, and all strings aq,..., xn of length at most N.
             This question is still wide open, and only some partial results are known.
             First of all,  this question is not specific to algorithmic information theory,  as 
          shown by A. Romashchenko who proved that such an inequality is true if and only 
          if the  inequality  for  Shannon  entropies  with the  same  coefficients  is  true,  where 
          strings X{ are replaced by random variables £/  (with arbitrary joint distribution),
                                    I
          where £/ is a random variable made from & with i € I (in other words, projection 
          of the random vector (£i,... ,£n) on /-coordinates).
             The implication in one direction is an easy consequence of the result  proven 
          in  Section  7.3:  Theorem  147  (p.  228)  says  that  entropy  is  an  expected value of 
          complexity and any linear inequality that  is true for complexities should be also 
          true for their expectations (with no error term, since the ratio 0(\ogN)/N tends 
          to 0 as N -» oo).
             More precisely, let &  be a random variable with values in some finite set X{. 
          Then the value of a random vector £ = (£i,..., £n) can be represented by a column 
         of height n.  To apply Theorem 147, consider N independent variables distributed 
          as £.  Together they form a random variable that we denote by ÇN.  Its values are 
         matrices having N columns and n rows.  Theorem 147 says that the expected value 
         of the complexity of this matrix is NH(£) + О (log N) (we spoke there about prefix 
         complexity and had A as a condition,  but with 0 (log7V)-precision this does not 
         matter).
             We can consider this matrix as a tuple of rows:  the zth row is a string of length 
         N over the alphabet Xi.  We can apply Theorem 147 not only to the entire matrix, 
         but also to selected rows indexed by i G /, where I is some subset of {1, 2,..., n}. 
         The expected complexity of this part of the matrix is NH(£i) + 0(logA/^).
             If the inequality
                                          s: O(logiV)
                                 I
         is  true  for all tuples x\,... ,xn,  it  can be applied to the rows of our matrix.  So, 
         taking the averages, we get
                               £A,JVtf(6)<0(logiV).
                                I
                         10.1.  INTRODUCTION AND  SUMMARY       315
         The left-hand side is
                                          JV,
         so this is possible only if
                               ^TA/tffôKO,
                                I
         as we claimed.
           The other direction is much less trivial.  We want to show that if the inequality 
         is true for entropies, it should be true for complexities with logarithmic error.  Here 
         some strings x\,..., xn  are given,  and we need to  construct  a family of random 
         variables whose entropies (and the entropies of their combinations) are close to the 
         complexities so we can apply the inequality for entropies.  This can be done by a 
         typization trick suggested by A. Romashchenko: We consider the set of all tuples of 
         strings x\ ,..., x'n whose complexities and conditional complexities are bounded by 
         the corresponding complexities of xi,... ,xn, and take a random element of this set 
         (with uniform distribution).  See below Section 10.6 (Theorem 212) for the details.
           Further results are related (in different ways) with the combinatorial interpre­
         tation of inequalities.  Let us start with a simple inequality
                        C(xi,x2) ^ C(xi) + C(x2) + O(logiV)
         and try to  understand its  combinatorial meaning.  Let X\  and X2  be finite sets 
         whose elements x\  G X\  and x2  G X2  are considered as messages.  Assume that 
         we have a set A c X\  x X2 whose elements are possible pairs of messages.  Then 
         we  have  \A\  possibilities  for  the  pair  (here  \A\  stands  for  the  cardinality  of A). 
         For the first component the number of possibilities is equal to the size of the first 
         projection of A (the set of x\  such that  (xi,x2)  G A for some x2).  Denoting this 
         number by m(l) and a similar number for the other coordinate by m(2), we can 
         write a combinatorial version of the same inequality,
                           log \ A\  ^ logm(l) + logm(2)
         or
                                |A| < m(l)m(2)
         (the size of a set is bounded by the product of sizes of its projections, an evident 
         observation).
           To  see  a  less  trivial  example,  consider  the  inequality  for  complexities  from 
         Theorem 26 (a similar inequality for entropies is considered in Problem 230):
               2C(xi,x2,x3) ^ C(xi,x2) + C(xi,x3) + C(x2,x3) + O(logiV).
         Using our analogy, we guess that for an arbitrary subset A of the Cartesian product 
         X\ x X2 x X3 the following inequality holds:
                    21og|A| ^ logm(l,2) + logm(l, 3) + logm(2,3),
         where m(i,j) is the number of elements in the projection of A onto X{ x Xj  (see 
         Figure 30) or
                           \A\2  < m(l, 2)m(l, 3)m(2,3).
         And indeed this is true; moreover, this inequality can be deduced from the inequality 
         about complexities using the following simple argument.  For an integer N consider 
         the set AN; we represent elements (xi,x2,x3) G A as columns of height 3, so every
             316            10.  INEQUALITIES  FOR  ENTROPY,  COMPLEXITY  AND  SIZE
             element  of AN  can be represented as a matrix of width N and height  3.  There 
             are  \A\N  matrices in AN, so among them there is a matrix of complexity at least 
             log |A|N = Л/Tog A.  Such a matrix is a triple of strings (x2,x2,x3) of length N (its 
             rows), and we can apply the inequality for complexities
                       2C(xl,x2,x3) < C(xi,x2) + C(xi,x3) + C(x2,x3) + O(logiV).
             Note that the complexities in the right-hand side are bounded.  For example, the 
             pair (x\,x2), a matrix of width N and height 2, is an N-tuple of its columns.  Each 
             column is an element of the projection of A onto coordinates (1,2), so the matrix 
             is  a string of length N over an alphabet of size m(l,2).  Therefore its complexity 
             (given N and A) is bounded by iVTogm(l, 2) + 0(1).  The set A does not depend 
             on N, and the complexity of N is 0(log iV), so we get
                    N log \A\ < iVlog m(l, 2) + N logm(l, 3) + iVlog m(2,3) + O(logiV), 
             which is possible for arbitrarily large N only if
                               2 log\A\ < logm(l, 2) + logm(l, 3) + logm(2, 3).
                                                   2
                                        Figure 30.  Three projections
                  281 Prove the same inequality for sets starting with the inequality
                              2#(6,6,6) < H (b , 6) + H (b , b ) + tf (a  b )
             for entropies.
                 (Hint:  Consider a triple uniformly distributed in A, and recall that the entropy 
             of every random variable is bounded by the log-size of its range.)
                  282 Give a direct proof of the same inequality without using complexities or
             entropies.
                 (Hint:  It can be derived from the inequality of Theorem 164.)
                 This inequality can be used to show that the number of triangles in a graph 
             with V edges is 0(VL5).
                 A similar argument can be applied to an arbitrary linear inequality for com­
             plexities that  has only one term in the left-hand side  (an inequality of this type
                                         10.1.  INTRODUCTION AND  SUMMARY                               317
              says  that  some  complexity  is  bounded  by  a positive  linear  combination of some 
              other complexities).  One can also extend this argument to inequalities that have 
              conditional complexities in the right-hand side.  For example, the inequality
                                           C{xi,x2) < C(xi) + C(x2 \xi)
              corresponds to the (evident) combinatorial statement
                                               m(l, 2) < m( 1) • m(211)
              valid for an arbitrary set A C X\  x X2, where by m(l,2) we mean the number of 
              elements in A, by m(l) we mean the number of elements in the first projection of 
              A, and by m(211) we mean the maximal size of all sections of A obtained when the 
              first coordinate is fixed.
                  Let us explain informally why this inequality is a natural combinatorial coun­
              terpart for the inequality about complexities.  The “combinatorial amount of infor­
              mation”  in x\  is logm(l); for a fixed value of X\  we have at most m{211) possible 
              values of x2, so the amount of information in x2 given x\  is (from the combinato­
              rial viewpoint) bounded by logm(2| 1).  And the amount of information in a pair 
              (x\, x2)  G A is measured as log A = logm(l,2).
                   283 Show that every linear inequality of the form L < R, where L and R are
              positive linear combinations of (conditional or unconditional)  complexities and L 
              has only one term, corresponds to a (true) combinatorial inequality.
                  We can describe completely the inequalities of this type that do not include 
              conditional complexities.  Consider an inequality
              (*)                    C{x i,. • • ,xn) < ^    XjC(xi) + О (log A/"),
                                                         i
              where all the coefficients  in the right-hand side are non-negative and the sum is 
              taken over non-empty I except for I — {1,2,..., n}.  (We can assume without loss 
              of generality that the left-hand side includes all X{.  Then if some Xj is missing there, 
              we can delete Xj  everywhere in the right-hand side.  In other words, replacing Xj 
              by the empty string, we get a stronger inequality.)
                  Theorem 204.  The inequality (*) holds if and only if for every г e {1,2,..., n} 
              the sum of coefficients in the right-hand side for the terms including Xi is at least 1.
                  P r o o f.  If the sum of coefficients is less than 1, the inequality is false even for 
              the case when all strings except хг are empty.
                  Assume now that for all i the sum of coefficients near terms that contain X{ is 
              at least 1.  Represent each term as a sum of conditional probabilities; for example,
                                                 C(x 1,x2,x3,... ,xn)
              now becomes
                         C(x i) + C(x2 \xi) + C(x3 |xi,x2) H------h C(xn\xi,... ,xn_i).
              In all the terms (in the left- and in the right-hand sides) we use the same ordering 
              (increasing indices,  as in this example).  Look at the terms C(x{ | • • • )  with some 
              conditions  in  the  left-hand  side  and  in  the  right-hand  side.  On  the  left  we  use 
              all preceding variables as conditions, and on the right some terms may have fewer 
              conditions.  But  the  complexity  may  only  increase  when  a condition  is  deleted, 
              and it  remains to recall that  the sum of coefficients in the right-hand side is  at 
              least 1.                                                                                   □
             318           10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY  AND  SIZE
                  284  Prove that for prefix complexity the inequalities from the previous the­
             orem are true with 0 (l)-precision (better than 0 (logn)-precision that we usually 
             have for strings of length n).
                 (Hint:  The argument above shows that this inequality is a linear combination of 
             basic inequalities, and the latter are true with 0 (Imprecision for prefix complexity 
             (Theorem 70,  p.  110).  Indeed,  if we  (temporarily)  redefine prefix complexity as 
             K(u\v) = K(u,v) — K(v),  then the inequalities of type  K(z\x,y)  ^  K(z)  are 
            reduced to basic inequalities.)
                 However, we want to understand the combinatorial meaning of arbitrary linear 
            inequalities (and not only those with one term in the left-hand side).  To understand 
            the problem, let us consider an example:  the basic inequality
                         C(xi) + C(xi,x2,x3) < C(xl,x2) + C(xi,x3) + 0(\ogN).
            The naive idea is to write a combinatorial inequality in the same way as before and 
            hope that for every A C X\ x X2 x X3 the following inequality holds:
                                    m(l) • m(l, 2,3) ^ m(l, 2) • m(l, 3).
            This is not the case.  This inequality is indeed true for every parallelepiped axbxc 
            where m(l) = a, m( 1, 2,3) = abc, m( 1, 2)  -  ab, and m(l, 3) — ac.  But if we add to 
            this parallelepiped another one, a' x lxl, where a'     a, then the values of m(l, 2), 
            m(l,3), and m(l,2,3) remain almost unchanged while m(l) increases significantly, 
            so the inequality may become false.
                 Another example is to consider the reverse inequality for the complexity of a
            pair
                                   C(xi) + C(x2 \x\) < C(x\,x2) + 0(1).
            What statement is its combinatorial translation?  Again,  the naive attempt is to 
            consider the inequality
                                          m(l) • m(211) < m(l, 2),
            but it does not work—the ratio m(l,2)/m(l) is an average size of a (non-empty) 
            section, and this average size can be much smaller than the maximal size m(2| 1).
                 So we now see the problem.  What can be done? There are several possibilities. 
            First,  one  may  consider  not  arbitrary  sets  but  some  special  (uniform  or  almost 
            uniform) sets, where this problem does not appear.  The other approach is to find a 
            better combinatorial translation of the inequalities.  Both approaches are considered 
            below; we start with the first one (uniform sets).
                                           10.2.  Uniform sets
                Let us recall the notation used.  Let A C X\ x • ■ ■ x Xn be a non-empty subset 
            of a Cartesian product of n finite sets X\,..., Xn.  For every set I C {1,...,n} of 
            indices, we consider the projection of A onto corresponding coordinates, which is a 
            subset of Пiei Xi-  The size of this projection is denoted by ша(1).  We consider 
            not  only projections,  but  also their sections.  Let  I and  J be two disjoint sets of 
            indices.  Let us fix /-coordinates in some way (by selection of a point in       -^"0 
            and consider the set of all /-coordinates of points in A with given /-coordinates. 
            So for every point in ПieI X{ we get some subset of YljejXj-  The maximal size 
            of these subsets is denoted by m^(J \ /)•  (If the set A is clear from the context, we 
            omit the subscript A in this notation.)
                                                                    10.2.   UNIFORM SETS                                                      319
                         We assume (as usual)  that m(0 )  =  1  and m(0 1J)  =  1  for every J.  On the 
                   other hand, we define m(I\ 0 ) as m(I).  For example, consider a set A C X\  x X2 
                   (see Figure 31).
                                                             2
                                        Figure 31.  A two-dimensional set and its parameters
                         Then 7Пл({1}) is the cardinality of the projection of A onto a horizontal axis, 
                   mA({2}) is the size of the vertical projection, mA({2} | {1}) is the maximal size of 
                   the vertical section, and mA({ 1} | {2}) is the maximal size of the horizontal section. 
                   The total number of elements in the set is mA({l,2}).
                         We can write the (obvious) inequality
                                                                m(l, 2) ^ m{ 1) ■ m{2 | 1)
                   (we omit the subscript  A  and curly brackets for brevity).  Indeed,  each of m{ 1) 
                   vertical sections contains at most m(2| 1) elements.
                         For an n-dimensional set a similar inequality says that
                              m(l, 2,... ,n) ^ m{ 1) • m(2 11) • m(311, 2) • ... ■ m(n| 1,2,..., n — 1).
                   It  is  true  for  the  same  reasons:  for  each  of m(l)  possible values  of the  first  co­
                   ordinate, there are at most m(2|l) values of the second coordinate;  for each pair 
                   there are at most m(3|l,2) values of the third coordinate;  and so on.  The same 
                   inequality is true for every permutation (k\,..., kn) of (1,2,..., n):
                         m(ki,. ..,kn)^ m(ki) ■ m(k2 \ki) ■ m(k3 \ki,k2) • ... ■ m(kn\ki,... ,fcn_i),
                   the left-hand side remains the same (the cardinality of A).
                         Now we are ready to give the definition of a uniform set.  A set A is uniform 
                   if all these inequalities (for all n! permutations of the set of indices) are equalities. 
                   The simplest example of a uniform set is a (combinatorial) parallelepiped, i.e., the 
                   set A\  x • • • x An for some Ai c Xi.
                         However,  there are other examples of uniform sets.  For example,  Figure 32 
                   shows a uniform sets of six elements (all non-zero sections have two elements and 
                   both projections have three).
                         Let J, J, К be disjoint sets of indices.  Then for arbitrary A the following in­
                   equality holds:
                                                      m(J U K\I) ^ m(J\I) ■ m(K\I U J)
                320               10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY AND  SIZE
                                                                    •   • 
                                                                •       • 
                                                                •   •
                                                   Figure 32.  A uniform set
                (for each combination of /-coordinates there are at most m(J\I) combinations of 
                /-coordinates,  and for each of them there are at most m(K\I U J)  combinations 
                for //-coordinates).
                     We can use this inequality to prove the inequality considered earlier,
                     m(fa,..., kn) ^ m(fa) ■ m(fa\fa) ■ m(fa | fa, fa) • ... • m(kn \ fa,..., kn-i)
                (which is evident anyhow, but let us continue the argument,  it will soon become 
                clear why we do this) by grouping factors in the right-hand side.  For example, the 
                product
                                                 m(fa I fa, fa) • m(fa I fa, fa, fa)
                is at least
                                                          m( fa,fa\fa,fa),
               then the product
                                                   m{ fa\fa) • m(fa, fa\fa, fa) 
               can be replaced (without increase) by
                                                          m^fa, fa, fa I fa),
               and so on, until we get the left-hand side.  For a uniform set, therefore,  all these 
               inequalities are equalities (since the first and last terms in the chain of inequalities 
               are equal).  Now we conclude that for uniform sets the inequality
                                             m(J U K\I) ^ m(J 11) • m(K | / U J)
               turns into  an equality for all I, J, K.  Indeed,  we can find a chain of inequalities 
               where this inequality appears.  The equation
                                             m(J U К 11) — m(J 11) • m(K \ I U J)
               can,  therefore,  be  considered  as  an  (equivalent)  definition  of uniform  sets.  We 
               require it to be true for every disjoint set I, J, К of indices.
                      285  Following this argument, give a complete proof that the property above
               is an equivalent definition of uniform sets.
                      286 Prove that a projection of a uniform set on every subset of coordinates 
               is again a uniform set.
                      287 Prove that every section of a uniform set  (we fix some coordinate and
               consider the set of possible values of all other coordinates) is again a uniform set.
                     Uniform sets are important as sources of random variables.  Consider a uniform 
               set  A  C  Xi  X  • ■ •  X  Xn  and  a random  point  that  is  uniformly  distributed  in A. 
               Its projection onto the ith coordinate is a random variable with values in Xf, we 
               denote this variable by &.
                                   10.3.  CONSTRUCTION  OF  A  UNIFORM  SET                   321
                 Theorem 205.  A set A is uniform if and only if for every I — {«i,..., г^}  the 
             random variable £/ = (£^,... , £ifc)  has uniform distribution on its range.
                 P r o o f.  Let I be some set of indices, and let J be its complement (the difference 
             between {1,2,... ,n} and I).  Then the equality
                                       m(l, 2,... ,n) = m(I) • m(J\I)
             means that that  the average size of a (non-empty)  section obtained by fixing I- 
             coordinates,  i.e.,  m(l, 2,...,n)/m{I),  is equal to its maximal size m(J\I), so all 
             the  sections  have  the  same  size.  And  this  means  that  all  the  values  of £/  are 
             équiprobable.
                 On the other hand, assume that for some set A for every set I of indices all the 
             values of £/ are équiprobable.  In particular, for I = {1,..., n — 1} we get that all 
             (non-empty) sections obtained by fixing the first n — 1 coordinates have the same 
             size, so
             (*)          m(l, 2 ,... ,n) = m(n | 1, 2,..., n — 1) • m(l, 2 ,... ,n — I).
             Moreover, since the random variable  (£i,£2, • • • 5£n-i)  is uniformly distributed in 
             the projection of the set A to coordinates 1,2,..., n — 1, we get the same picture 
             for this projection:  £1, ... ,£n-i  are random variables obtained by projection of a 
             point uniformly distributed in this set.  Using induction, we may assume that this 
            set is uniform.  Then the equation (*) can be continued:
            m(l,2, ...,n)
                = m(n 11, 2,..., n — 1) • m(n — 111,2,..., n — 2) • ... • m(3 [1,2) • m(211) • m(l).
            The same argument can be applied for every ordering of coordinates, so we conclude 
            that A is uniform according to our definition.                                     □
                 C o r o lla r y .  For the random variables £1, • • • ,£n  constructed in this way, the 
             entropy of the tuple £/  equals log m(I)  (for every I C {1,2,..., n}).
                 So we conclude that the following statement is true:
                 Theorem  206.  Every linear inequality that is true for entropies is also true 
            for the log-sizes of projections for uniform sets.
                 For example, if А С X\ x X2 x A3 is uniform, then the inequality
                                     m(l) ■ m(l, 2,3) ^ m(l, 2) ■ m(l, 3)
            holds, since it corresponds to the basic inequalities for complexities and entropies 
             (Theorem 24).  Note that  (as we have discussed)  this inequality is false for some 
            non-uniform sets.
                 In  the  next  section,  following  [37],  we  prove  a  reverse  statement.  Starting 
            from a tuple of random variables, we find a uniform set for which the log-sizes of 
            projections are (almost) proportional to the entropies of the corresponding groups 
            of variables.
                                  10.3.  Construction of a uniform set
                 Assume that we have some tuple of (dependent) random variables 771,..., rjn\ all 
            of them have finite range.  We want to construct a uniform set A whose projection
             322           10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY  AND SIZE
             sizes (their logarithms) are proportional to the entropies of ту.  The ideal situation 
             is
                                              logmA{I) - H(ry)
             for all I C  {1,... , n}.  Then we would conclude immediately that every inequality 
             that is true for the (log-sizes of the) projections of uniform sets is true for arbitrary 
             random variables.
                 Of course, this requirement is too restrictive:  The entropies in general are not 
             logarithms of integer numbers.  But  if we  are interested  in  linear  inequalities,  it 
             is  enough if the log-sizes  are proportional  to entropies,  or  approximately  propor­
             tional to them.  To achieve this,  we consider first the case when the probabilities 
             of all values of (ry,... ,уп)  are rational.  This is enough to conclude that the lin­
             ear inequalities for entropies and log-sizes are the same (the rational numbers are 
             dense, and every distribution can be approximated by a distribution with rational 
             probabilities, while the entropy is a continuous function of the distribution).
                 Assume that  a tuple of random variables ту,..., rjn  is  given.  Each  value of 
             this  tuple  is  a column of height  n.  Each column has  (we  assume)  some rational 
             probability.  Let iV be a common denominator for all the probabilities.  Consider a 
             matrix with N columns where the columns appear with given probabilities, so our 
             tuple can be obtained by taking at random a column from this matrix.
                 Let us consider the rows of this matrix.  The ith. row is a string of length N 
             over an alphabet whose letters are possible values of    Denote the set of all strings 
             of length N in this alphabet by X{.  (The length N is fixed, so we do not include 
             N in the notation.)  Then the entire matrix is an element of X\ x • • • x Xn.
                 Consider now all matrices that can be obtained from this one by a permutation 
             of columns.  In other words, consider all matrices of width N where the frequencies 
             of all the columns are exactly the same as in our matrix,  i.e., they correspond to 
             the distribution of the tuple ry,... ,rjn.  We get some set U C X\  x  • • • x Xn.  Let 
             us show that this is a uniform set with the required sizes of all the projections.
                 First, let us note that every element of U is obtained from the original table by 
             some permutation of columns.  If we take a random permutation (all N\ permuta­
             tions are équiprobable), the probability of getting a given element of a set U does 
             not  depend on the element.  (Indeed,  the number of permutations that give this 
             element is equal to the number of permutations that keep this element unchanged, 
             and this depends only on the multiplicities of different columns and not on their 
             positions.)
                 This property remains true if we delete some rows from the table.  Therefore, 
             the projection of a random point in U on an arbitrary subset of coordinates is also 
             uniformly distributed, so the set U is uniform.
                 Now we need to estimate the sizes of projections.  First, let us find the size of 
             the set  itself.  Assume that the matrix has m different columns that appear in it 
            with probabilities q\,... ,qm.  Then the number of matrices that can be obtained 
             by permutations of columns is equal to
                                                      N\
                                           (qiN)\(q2N)\---(qmN)V 
             and its logarithm is (by Stirling’s formula)
                                         Nh(qi,...,qm) + 0(logAO,
                                       10.4.  UNIFORM  SETS  AND  ORBITS                       323
             where  h(q\,..., qm)  =         log<ft)  is  the  Shannon  entropy  of a  random  vari­
             able  that  has  m  values  with  probabilities  qi,. ■ ■ ,qm,  i.e.,  the  Shannon  entropy 
             of {rji,. ■ ■ ,r]n).  So  the  log-size  of U  is  (approximately)  N  times  bigger than the 
             entropy of (?7i,..., r]n), and the same can be said about every projection of the set 
             (and the corresponding projection of the random variable).
                 If a linear inequality is true for log-sizes of the projections of all uniform sets, 
             it will be true for the set U.  Increasing N (we can multiply N by an integer factor) 
             and taking the limit (as N —> oo), we conclude that the same inequality is true for 
             all random variables with rational probabilities.  By continuity it is true for every 
             probability distribution, and we get the following result  [37]:
                 Theorem 207 (Chan and Yeung).  Every linear inequality that is true for log- 
             sizes of the projections of uniform sets is also true for entropies of arbitrary random 
             variables.
                                      10.4.  Uniform sets and orbits
                 Let us think again about the construction of the previous section.  How do we 
             get a uniform set?  Usually uniformity is a byproduct of some algebraic structure on 
             the objects considered.  Such a structure indeed can be found in our construction.
                 Namely, we have a permutation group S'/v and its action on the columns of the 
             matrix.  The uniform set is an orbit of some point (and contains all the matrices with 
             the same frequencies).  This is a special case of the following situation.  Consider 
            some finite group G and some actions of G on finite sets X\...., Xn.  Together they 
            define an action of G on Xi x • • • x Xn.  Consider and arbitrary point (aq,..., xn) 
            Xi X ••• X Xn and its orbit U (for this action).
                 Theorem 208.  The set U is a uniform subset of Xi  x • ■ • x Xn.
                 Proof.  Let us consider all elements of G as équiprobable.  Consider the action 
            of a random element of G on the point x =  {aq,... , æm);  the image is a random 
            variable whose range is the orbit of G.  All the values of these random variables are 
            équiprobable.  Indeed, the elements of G that map a given point ж to a given point 
            у form a coset of a stabilizer subgroup of the point x  (which contains the group 
            elements that map x to itself), and all the cosets have the same size.
                 The same is true for every subset of the set of indices, so a random element of 
            U has the same chances of being projected onto all points (of the projection of U)\ 
            thus the set U is uniform.                                                          □
                 How can we rewrite the inequalities for the sizes of projections in terms of the 
            size of the group and its subgroups?  The orbit U has size  |G|/|Sj where S is the 
            stabilizer of the point x.  This stabilizer  (for our action)  is the intersection of the 
            stabilizers of aq,... ,xn.  The same can be said for every set I of indices:  the size 
            of the projection of U on indices {UU2,...} is the ratio |G|/|5ij П Si2 П • • • | where 
            Sj  is the stabilizer of Xj.  Note that we can go in the other direction:  an arbitrary 
            subgroup H of G is the stabilizer of some point in the action of G on the cosets of 
            H.  In this way every inequality for the projections of uniform sets translates to an 
            inequality about the sizes of subgroups and their intersections.
               324              10.  INEQUALITIES  FOR  ENTROPY,  COMPLEXITY  AND  SIZE
                    For example, the inequality m(l, 2) ^ m(l) -m(2) gives the following inequality 
               that is valid for arbitrary subgroups Hi and H2 of an arbitrary finite group G:
                                                        |G|          |G|    JG|_
                                                    \HlC\H2\ "  |H,|        \H2\
               or |H1n F 2|^ |H 1|-|H 2|/|G|.
                    A more interesting inequality m(l,2,3)2 < m(l, 2)m(l, 3)m(2,3) gives the in­
               equality
                                |Я, П H2 П Я3|2 > \Hi n Я2| • |Я, П Я3| • |Я2 П Я3|/|С|
               for an arbitrary finite group G and its subgroups Hi, H2, H3.
                    The  proof of Theorem  207  shows  that  the  reverse  statement  is  true:  every 
               inequality for the size of a group and its subgroups  (that  contains the products 
               of them with some exponents)  can be translated into an inequality for arbitrary 
               random variables, since we may approximate a random variable by the action of 
               the permutation group.  We get the following surprising result [37]:
                    Theorem 209  (Chan and Yeung).  Every linear equality for the entropies of 
               random variables translates into an inequality for the sizes of the group and its 
               subgroups,  and vice versa.
                                               10.5.  Almost uniform sets
                    Recall that a set A C Xi x • • • x Xn is uniform if the inequality
                    m(ki,... ,kn) ^ m(ki) • m(k2 \ h) ■ m(k3 \ ki, k2) • ... • m(kn \ ki,..., kn-i)
               (that is true for all sets) turns out to be an equality for every permutation k\,..., kn 
               of {1,2,..., n}.  Now consider a weaker requirement and say that A is c-uniform if 
               the ratio between two sides of this inequality is bounded by c (for every permuta­
               tion).  So 1-uniform sets are uniform sets as defined above.
                    Many properties of uniform sets  are still  true  (up  to  some  error  factor)  for 
               almost uniform sets.
                    Theorem 210.  Let A be a c-uniform set.
                    (a)  Let I, J, К be disjoint sets of indices.  The right-hand side in the inequality
                                           m(JuK\I) < m(J\I) -m(K\IUJ)
               (it is true for every set A) exceeds the left-hand side at most by a factor c.
                    (b)  The projection of A onto an arbitrary set of coordinates is a c-uniform set.
                    (c)  Let A'  be a subset of A that contains at least an e-fraction of elements of 
               A.  Then A' is cje-uniform.
                    (d)  Let £ be a random variable uniformly distributed in A, and let I be a set of 
               coordinates.  Then its projection £/  on I-coordinates has entropy between log m(I) — 
               loge and logm(I).
                    (e)  Let I  and J  be  disjoint sets  of coordinates.  Then  i7(£j|£/)  is  between 
               log m(J 11) — log c and log m(J\I).
                    Proof,  (a) We can group the factors in the right-hand side of the inequality 
                    m(ki,...,kn) < m(ki) • m(k2 \ h) ■ m(k3 \ ki, k2) ■ ... • m(kn \ kx,..., kn-i) 
               using the inequality
                                           m(J\I) -m(K\IUJ) ^ m(JUK\I)
                                                               10.5.   ALMOST UNIFORM  SETS                                                     325
                   for some /, J, and K.  The product decreases at each step, and at the end we get 
                   the  left-hand side.  If the  initial  inequality was  an equality  up to  a c-factor,  the 
                   same is true for the inequalities used at each step.  And the ordering of coordinates 
                   can be chosen in such a way that this process goes through a given triple I, J, К.
                          (b) By assumption
                       m(n| 1,... ,n — 1) • m(n — 111,... ,n — 2) ■                             ■ m(2| 1) • m(l) < cm(l,... ,n),
                   and this inequality can be continued
                                           cm(l,..., n) < cm(n| 1,..., n — 1) • m(l,..., n — 1).
                   Cancelling the terms m(n| 1,... ,n — 1)  in both sides,  we see that the projection 
                   onto 1, 2,..., n — 1 is c-uniform (and we can do the same for every group of n — 1 
                   coordinates).  Then we can delete one more coordinate, etc.
                          (c) The maximal sizes of the sections for a subset do not exceed the correspond­
                   ing sizes for the entire set, and the size of the entire set is multiplied by e, so the 
                   constant c can be replaced by cje.
                          (d) The random variable £/ has m(I) different values, so its entropy is bounded 
                   by log m(I).  On the other hand, let J = {1, 2,..., n} \ I.  Then
                                                          m(I) ■ m(J\I) ^ cm(l, 2,... ,n),
                   so every value of £/ has probability at most
                                                          m(J|/)/m(l, 2,..., n) < c/m(/),
                   and the entropy is at least logm(I) — loge.
                         (e)  Let us compare the equality
                       я(б,...,£„)
                                = Я(^п|^,...^п_1) + Я(^п_1|6,...^п-2) + -" + Я(б1б) + Я(6)
                   and the inequality
                                        logm(l,... n)
                                               ^ logm(n11,...,n — 1) + logm(n — 111,... ,n — 2)
                                                   -I------h logm(2| 1) + logm(l).
                   The left-hand sides are the same since £ is uniformly distributed in A.  Each entropy 
                   term in the first inequality is bounded by the corresponding logarithm in the second 
                   inequality (the conditional entropy is the average of entropies for all possible values 
                   of the  condition,  and each entropy  in this  average  is bounded by the log-size of 
                   the  range).  Since  A  is  c-uniform,  both sides  of the  inequality  differ  at  most  by 
                   loge.  Therefore,  each entropy can differ from the corresponding log-size at most 
                   by log c.  By grouping the terms in these two sums (after a suitable permutation of 
                   coordinates), we can prove the inequality
                                                log m{J 11) -  log c < H(Çj I £/) < log m(J \ I) 
                   for an arbitrary disjoint set of indices I and J.                                                                             □
                         An immediate corollary of statements (d) and (e) is that if some linear inequal­
                   ity is true for entropies, it is also true for log-sizes of corresponding projections and 
                   sections of a c-uniform set with error term at most A loge, where A is the sum of 
                   absolute values of all coefficients.  (Increasing c, we allow sets that are less uniform, 
                   so the error term increases.)
            326          10.  INEQUALITIES FOR ENTROPY, COMPLEXITY AND SIZE
                This property will be used in the next section when we start from a tuple of 
            strings,  construct  a c-uniform  set,  and  then  consider  the  corresponding  random 
            variables.
                                       10.6.  Typization trick
                The following theorem starts from an arbitrary tuple of strings xi,... ,xn and 
            constructs an almost uniform set A in some Cartesian product X\ x X2 x • • • x Xn 
            of finite sets Xi such that \ogm(J\I) « C(xj\xj) with logarithmic precision:
                Theorem 211 (A. Romashchenko).  For every n there exists a constant d such 
            that the following holds: For every N > 1 and for every tuple x\,... ,xn of strings of 
            complexity at most N there exist finite sets X\,..., Xn and an Nd-uniform subset 
            A C X\ x • • • x Xn such that
                                   |logm(J|/) -  C(xj\xj)\ ^dlogN 
           for all disjoint subsets I,J C {1,..., n).
                Note that the sets Xi are not really relevant here:  We can speak instead about 
            a finite set of arbitrary n-tuples with the required size of projections and sections. 
            Note also that a strange condition N > 1 is needed since for N = 1 the bound Nc 
            does not grow as c increases.
               The proof uses the notion of a complexity vector for a tuple of strings.  For-a 
           given tuple Xi,..., xn the complexity vector is the list of all complexities C(xj \xj) 
           for all pairs (/, J) of disjoint subsets of the set of indices {1,..., n}.  Note that the 
           length of this vector is exponential in n:  There are 2n — 1 unconditional complexities 
            (where J — 0 ) and a lot of conditional ones.  Let us denote the complexity vector 
           for xi,... :xn by x(xi,.. .,x„).
               Proof.  Consider  (for  given  xi,...,xn)  the  set  A(xi,...,xn)  of  all  strings 
           yi,...,yn such that
                                     х(у 1, ■ • • ,Уп)  ^   x ( x i , . ..,x„)
           componentwise.  For example, for n — 1 this is the set of all strings of complexity 
           at most C(xi).  For n — 2, we consider all pairs (£1,2:2) such that
                                   C(yi) ^ C(x1),   C{y2) ^ C(x2),
                 C{yi,y2)<C{xi,x2),  C(yi\y2) ^ C(xi\x2),  C{y2 \y1) < C(x2 \x{).
           The set A(x 1,..., xn) is guaranteed to be non-empty—it contains {x\,..., xn).  In 
           fact, it contains about 2c('Xl,---,Xn^  elements.  Indeed, this is an upper bound since 
           the complexity of all elements of this set is bounded by C(x 1,... ,xn).  It remains 
           to show that this set cannot be much smaller.
               Indeed,  knowing the complexity vector x(xi,... ,xn),  we can enumerate the 
           elements of A(x 1,..., xn).  To specify the complexity vector, we need 0(log N) bits. 
           (Note that the number of components in the complexity vector depends only on n, 
           so we consider it as a constant even though this constant grows exponentially in n.) 
           So every element of A(x 1,... ,xn)  can be described by specifying (in addition to 
           the complexity vector) its ordinal number in the enumeration, and therefore every 
           element of A(xi,..., xn) has complexity
                                    log \A{x\,... ,xn)\ + 0(\ogN).
                                                 10.6.  TYPIZATION TRICK                                  327
              In particular,  this is true for the initial tuple  (xi,... ,xn),  so we get the required 
              bound for the cardinality.
                   Let us show now that the set A(xi,..., xn) is c-uniform for some constant c the 
              depends polynomially on N.  For that let us consider both sides of the inequality
                      m(l, 2,..., n) ^ ra( 1) ■ m(211) • m(311, 2) • ... ■ m(n | 1,2,..., n — 1).
              The logarithms of the factors in the right-hand side do not exceed the corresponding 
              complexities:  m( 1) ^ 2c^Xl\ since by construction we have C(y\) < C(x\) for every 
              tuple (yi,... ,yn) G A(x\,... ,xn)  (to be exact, one should write 2c(Xl)+1, but all 
              our estimates have logarithmic precision anyway).  For the same reasons we have 
              m(2|l)  ^ 2c^X2^Xl>)  and so on.  We conclude that the logarithm of the right-hand 
              side does not exceed
                     C(x i) + C(x2 \xi) + C(x3 \xi,x2) H------1- C(xn \xi,.. • ,xn_i) + 0(1),
              and this  sum is  equal  to  C(xi,... ,xn) + 0(\ogN).  But  we know  also  that  the 
              logarithm of the left-hand side is at least C(xi,..., xn) — О (log N), so the difference 
              between logarithms is O(logiV) and both sides differ at most by a polynomial (in 
              N) factor.  As a byproduct we conclude that
                              C{xi Ixi,... ,x»_i) = logm(i 11, • • • ,i -  1) + O(logiV), 
              and a similar argument (with the permuted indices and grouped terms) shows that 
                                         C(xj\xi) = log m(J 11) + O(logiV) 
              for every disjoint I and J.                                                                  □
                   The construction used in the proof may be called a typization trick:  starting 
              from a tuple, we construct the set where this tuple is typical.
                   Now it is easy to finish the proof of the promised result:
                   Theorem 212 (A. Romashchenko).  Every linear inequality
                                                     i
              that is true for arbitrary random variables £i,...,£n>              a ^s0   true for strings  of 
              complexity at most N with О (log N) -precision,
                                               £>/*■(6) « 0(logJV).
                                                 I
                   Here the constant hidden in О (log AT) depends on n (and grows exponentially 
              as n increases) but not on the strings Xi,..., xn.
                   PROOF.  The reverse implication (every inequality that is true for complexities 
              is also true for entropies) was proven in Section 10.1.
                   Now  we  are  ready  to  go  in  the  other  direction.      Assume  that  this  linear 
              inequality  is  true  for  entropies.   Consider  arbitrary  strings  xi,...,xn  and the 
              set A = A(xi,...,xn)  from the previous theorem.  Consider a random variable 
              (£i,... ,£n)  that is uniformly distributed in A.  Since the set A is jVc-uniform, the 
              entropies  are  О (log iV)-close  to  the sizes  of the corresponding sections,  as Theo­
              rem 210 says.  On the other hand, the log-sizes logm(/| J) (even for the conditional 
              case, though we need this only for J = 0 ) coincide with corresponding complexities 
              with 0(logiV)-precision (Theorem 211).                                                       □
        328      10.  INEQUALITIES  FOR ENTROPY, COMPLEXITY AND SIZE
          The linear inequalities (of the type considered) are universal formulae of some 
        language, so a natural question arises:  Is a similar result true for more complicated 
        statements,  say,  for  V3-formulas  of the  same  language  (defined  in  some  natural 
        way)?  The answer turns out to be negative, as shown in [141].
                 10.7.  Combinatorial interpretation:  Examples
          Let us recall the main idea of the combinatorial interpretation:  The statement 
        “x has complexity at most n”  is translated as  “x belongs to the set of at most 2n 
        elements”.1  (Since the complexity is defined up to 0(1), we do not specify whether 
        we consider strict or non-strict  inequalities.)  In this way we get a combinatorial 
        translation of a binary relation C(x) < n, so all statements should be reformulated 
        in terms of this binary relation.  Here are some examples:
            •  The inequality C(x) ^ C(y) means that for every n the statement C(y) < 
              n implies C(x) < n.
            •  The inequality C(x) ^ 2C(y) can be rephrased as follows:  for every n the 
              inequality C(y) < n implies C(x) < 2n.
            •  For the inequality C(z)  ^ C(x) + C(y), the following translation can be 
              used:  for every и and v, if C(x) < и and C(y) < v, then C{z) < u + v.
        Using the last example as a guideline, let us try to invent a combinatorial translation 
        of the inequality C(x,y) ^ C(x) + C(y).  As we have said, it means that C(x) <_u 
        and C(y) < v imply C(x, y) < u + v.  So we want to say something like “if x belongs 
        to some set of size at most 2U, and у belongs to some set of size at most 2V, then 
        we can construct a set of size at most 2U+V that is guaranteed to contain  (x,y)”. 
        Indeed,  we can consider the  Cartesian product of two given sets,  and its size is 
        indeed bounded by 2U+V, so we get a true (though trivial) statement.
          To get a combinatorial version of the inequality C(x,y) ^ C(x) + C(y |x), we 
        need to translate the statement C(y |x)  < v.  This can be done as follows:  (x,y) 
        belongs to some set whose sections (for every fixed x) have size at most 2V.  In this 
        way we again get a true (but trivial) combinatorial statement.
          We get a much more interesting situation when we consider the reverse inequal­
        ity.  We can try to rewrite C(z) > C(x)+C(y) as follows:  If C(x) ^ и and C(y) > v, 
        then C(z) ^ и + v.  But our approach is asymmetric:  We know what it means in 
        combinatorial terms when C is small, not large (the reason for this asymmetry is 
        that we can enumerate strings of small complexity, but not of large complexity). 
        So we need to consider a dual reformulation:  If it is not true that C(x) < и and it 
        is not true that C(y) < v, then it is not true that C(z) < u + v.  In other words, if 
        C(z) < u + v, then C{x) < и or C{y) < v.
          Let us now try to invent a combinatorial translation of the inequality
                          C(x,y) ^ C(x) + C(y |x).
        Using the trick described, we get the following statement.  If a pair  (x, y)  belongs 
        to  a given set of size  at  most  2U+V,  then either x  belongs to some set of size at 
        most 2U or (x, y)  belongs to some set of pairs whose sections (for every fixed first 
        coordinate) are of size at most 2V.
             course this should not be understood literally:  every x belongs to a singleton.  We will 
        see how this is interpreted in the examples below.
                                            10.7.   COMBINATORIAL INTERPRETATION:  EXAMPLES                                                  329
                         A more precise formulation of the statement would be for every u, v and for 
                   every set A of pairs that has at most 2U+V elements, there exist
                              •  a set В of size at most 2U\
                              •  a set  C of pairs that contains at most 2V  pairs with the same first com­
                                 ponent
                   such that for every (x, y) € A either x G В or (x, y) € C (or both).
                         In fact the proof of the formula for the complexity of a pair used exactly this 
                   approach:  For a given x and y, we looked at how many pairs with the same x have 
                   small complexity.  If there are only few of them,  then C(y |x) was small  (now we 
                   say that the pair  (x,y) belongs to C); if there are many of those pairs, then C(x) 
                   is small  (because this can happen only for few values of x).  (Now we say that x 
                   belongs to B.)
                          288  Translate the arguments used to prove the formula for the complexity of
                   a pair to give a formal proof of the combinatorial statement in question.
                         Now consider an inequality where both sides have more than one term:
                                              C{xx) + ^(х^хг^з) < C(xi,x2) + C(x2,x3).
                   (We called it a basic inequality; it is true with an O(logiV) error term for strings 
                  of complexity at most N.)
                         In  terms  of a  binary  relation  C(x)  <  n,  this  inequality  can  be  restated  as 
                  follows:
                                 If C(xi, X2) < a, C(xi, X3)  < 6, and a + b — p + q, then at least 
                                 one of the inequalities C(x 1) < p and C(x 1,Х2,хз) < q holds.
                   (For the sake of brevit)', we omit all the details about 0(log iV)-precision.)
                         The natural combinatorial translation of this statement looks as follows:
                                 If A C X\ x A2 x A3, тл( 1,2) < 2“, m^( 1,3) ^ 2Ь, and a + b — 
                                p + q, then there exist B,C С A1XA2XA3 such that A C BuC, 
                                 niß( 1 ) ^ 2P, and mc(l,2,3) ^ 2q.
                         Eliminating the variables a and b, we can rewrite this statement  (in a multi­
                  plicative version):
                                 If A C X\ xX‘2 x A3 and тл( 1, 2)-тл(1,3) = l-V for some /, V >
                                 0, then the set A can be covered by two sets В and C such that 
                                 T7iß(l) < I and mc(l,2,3) < V.
                         In geometric terms, if some set A has small projections 011 the planes 1, 2 and
                   1,3,  its length in direction 1  (the size of the projection on the first coordinate) and 
                  its volume (the cardinality of A) can both be large.  But we can split A into two 
                  parts В and C in such a way that В has small length and C has small volume.
                         (Recalling the example with two parallelepipeds, we see that in this example 
                  В could be the large one and C can be the small one.)
                         Of course, these heuristic arguments are not proofs.  But in fact the last state­
                  ment is indeed true (though not completely trivial).  Here is its proof.
                         Consider the projection of the set  A onto the  1, 2-plane;  this projection is  a 
                  subset of Ai x A2, and we denote it by A12.  For every x G Ai consider the section 
                  of this projection (pairs with first component x).  Let n2{;x) be the cardinality of 
                  this section.  Then
                                                            m(l,2) = |Ai2| = ^ n 2(x)
                               10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY  AND  SIZE
               330
               (the size of the set is the sum of sizes of all its sections).  Similarly,
                                              m(l,3) = |Ai3| -  ^2n3(x).
                                                                    X
              The length m^( 1) is the number of non-zero terms in these sums,  and m(l,2,3) 
              has an upper bound,
                                           m(l, 2,3) = \A\ ^ ^ n 2(x)n3(x).
                                                                 X
              One can assume without loss of generality that this inequality is an equality:  we 
              can add missing elements to A leaving A’s projections unchanged.
                   Now we need to split A into two parts В and C, and we need to care about the 
              length of В (its 1-projection) and the size of C.  For a given length of В we need to 
              cover a maximal number of points, so we consider I maximal sections (that make 
              n2(x)n3(x) maximal) and put all their elements in B.  The rest is C.
                   It  remains to prove that the cardinality of C does not exceed  |Ai2| ■ |Ai3|/7. 
              How can we do this?  We have bounds for the sizes of two 2D-projections of C: 
              They do not exceed |^Ti21 and |Ai3|.  We know also that all sections of C (for every 
              X  £ X\)  have  area at  most  Si,  where  Si  is  the  size  of the  Zth  section  of A  (in 
              decreasing order).  Consider the inequality
                               2C{xi,x2,x3) < C(xi,x2) + C(xi,x3) + C(x2,x3 \xi).
              It is easy to prove.  Rewrite all terms of the form C(xi,...) as C(xi) + C(... laq). 
              The left-hand side contains only one term, so we already know that it implies the 
              combinatorial statement
                                      m(l, 2,3)2 < m(l, 2) • m{ 1,3) • m(2,311).
              Therefore
                                                 |C7|2  <  |Л12| • И 13| • S,.
              It remains to show that
                                                    о  .  IA12I • |4i3|
                                                     '          /2
              Recall that В consists of I rectangles, and each of them has size at least Si.  The sum 
              of widths n2(x) of these rectangles is at most  |^4i21,  and the sum of their heights 
              n3(x)  is  at  most  |Ai3|.  The  average width  is then bounded  by  |Ai2|//,  and the 
              average height is bounded by |Ai3|/7.  To finish the proof, note that if S ^ афг for 
              all i = 1,2,..., /, then
                                            e ^ ai + • • • + uz   bi + • • • + bi 
                                                        I                I
              (the logarithm function is concave).
                   So our guess (the combinatorial statement that is similar to the basic inequality 
              about complexities) turns out to be true.
                          10.8.  Combinatorial interpretation:  The general case
                   After all these examples let us consider the general case.  Assume that an arbi­
              trary inequality for complexities is given.  Split the negative and positive coefficients, 
              and get the inequality with positive coefficients in both sides
                                     10.8.    COMBINATORIAL INTERPRETATION: THE GENERAL CASE                                                 331
                   (the  sums  in  the  left-hand  side  and  the  right-hand  side  are  disjoint:  all  7’s  are 
                   different from all J ’s, and the coefficients А/ and ßj are positive).
                         How can we translate  this  inequality  into  a  combinatorial  statement?  The 
                   examples above suggest the following translation:
                                 Consider a set A c X\ x • • • x Xn.  Let n7 be arbitrary positive 
                                 numbers such that Пмл' =
                                                                i                  j
                                 Then A can be covered by some Bj such that
                                                                       mB,(I) < n7.
                   Unfortunately, we can prove this statement only with logarithmic factors (though 
                   one can hope that the stronger statement, say with 0(l)-factors, is also true).  Here 
                   is the weaker version:
                                 For some constant d, for every X\,... ,Xn, for every finite A c 
                                 X\ x • • • x Xn and for every family of positive numbers n7 (one 
                                 number for each I in the left-hand side) such that
                                                              Пмл'=
                                                                i                  j
                                 there is a cover of A by sets Bj such that 
                                                               mBl(I) <             •  (log|i4|)d.
                         Theorem 213.  This statement is true for some coefficients А/  and ßj if and 
                   only if
                                                    Y  AiG(x,) < Y                             + O(logJV)
                  for all N and for all strings aq,..., xn of complexity at most N.
                         PROOF.  Assume that the inequality for complexities is true.  Let us show how 
                   a given set A can be split into parts of the required size.  First of all, we may assume 
                  without loss of generality that the elements of A are tuples of strings (x\,..., xn), 
                   and the length of all strings is bounded by N = log \A\ (we have enough strings for 
                  that).
                         Let us assume for now that the set A is simple (it has complexity O(logiV)). 
                  Then all its projections are simple,  so the complexities of every element of some 
                  projection is bounded by the logarithm of its size (with 0(log iV)-precision).  Then 
                   for every element  (aq,..., xn) G A, we have
                                                          C(xj) ^ logm^(J) + O(logiV)
                  for all J.  Adding all these inequalities with coefficients ßj, we conclude that
                                             ^2 ßjC(xj) ^ log ( l \m A( j y f  +O(l0gJV).
                                               J                           '   J                '
                  Now we use the inequality for complexity (which we assume to be true)  and the 
                   condition for nj, and we conclude that
                              ^2 ^iC(xi) < bg ( П  n ) 1 ) + °(loS N) = ^2 Xi loS ni + О (log N)
                                I                         ^  I         '                          I
             332           10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY  AND  SIZE
             for  (xi,... ,xn)  G A.  Therefore,  for  each  element  of A  at  least  one  term  in the 
             left-hand side is upper-bounded by the corresponding term in the right-hand side: 
             For every (aq...., xn) G A there exists I such that
                                           C(xj) ^ nj + O(logJV),
             i.e., xj belongs to the set of all objects of complexity at most nj + О (log N).  This 
             set  (more precisely, its intersection with A) can be now used as Bj.
                 This ends the proof for the case when A is simple.  The result can be extended 
             to  arbitrary A by a standard trick.  For each N consider all possible subsets A C 
             X\ X • • • X Xn where all Xj consist of strings of length at most N.  Among all these 
             A, we consider the “worst”  one (for given coefficients Xj and (ij), i.e., the set that 
             has the worst ratio of the sizes of the left-  and  right-hand  sides for optimal Bj. 
             This set can be found by an exhaustive search (for given N and given coefficients 
             Лi,ßj),  so  it  is simple,  and we can apply the argument  above.  Since it  was the 
             most difficult set, the same statement is true for all other sets.
                 In fact, this argument has some flaw.  We do not assume that the coefficients 
             Xj  and (ij  are rational  (they may even be non-computable reals).  However,  it is 
             enough to know them with precision 1/iV, since all logarithms of cardinalities are 
             bounded by N, and this brings only 0(1) total error.  And we need only O(logiV) 
             bits to specify the coefficients with that precision.
                 So we have proven one direction of the statement of the theorem.  It remains 
             to show that if the statement about the cover is true, then the inequalities for the 
             complexities are also true.  This can be done with the same typization trick.
                 Consider an arbitrary tuple aq,..., xn of strings.  Each has complexity at most 
             N.  Assume that  the  inequality  for  complexities  is  false  and  the  left-hand  side 
             significantly (by more than O(logiV)) exceeds the right-hand side.  The typization 
             trick gives us an almost uniform set A = A(xi,..., xn).  Let us define nj.  Decrease 
             all C(xj) by the same quantity in such a way that the left-hand side becomes equal 
             to the right-hand side.  Since logmq(J) does not exceed C(xj), we can apply our 
             assumption.  In this way we get a cover of A by sets Bj whose /-projections have 
             at most 2n,+°(log N') elements.  This is significantly smaller than the corresponding 
             projection of the set  A;  it  contains about  2C^X’^  elements.  (Recall that we have 
             decreased nj significantly.)  Since A is almost uniform, every part that generates a 
             small fraction in one of its projections is a small part of A itself, so the sets Bj (the 
             number of these sets is fixed) cannot cover A entirely.                            □
                  289 Make all the  estimates  in this  argument  precise  (instead  of speaking
             about  “significant increase”,  “small fraction”, and so on).
                  290  Show that Theorem 213 and its proof can be generalized to the inequal­
             ities that contain conditional entropies (not only unconditional ones)
                             10.9.  One more combinatorial interpretation
                 The last argument in the previous section suggests another combinatorial inter­
             pretation for inequalities.  It looks somewhat less natural, but it is easy to formulate 
             since now we do not need to separate the positive and negative coefficients in the 
             inequalityand treat them differently.
                           10.9.  ONE  MORE  COMBINATORIAL INTERPRETATION             333
                Let us consider arbitrary (positive or negative) coefficients Л/ and the following 
            combinatorial statement:
                    For some constant d, for every tuple of finite set X\,..., Xn and 
                    for every A c X\ x • • • x Xn, one can represent A as a union of 
                    at most (log |A|)d sets, and for each of these sets the inequality
                                        Y[m{I)Xj < (log|A|)d
                    is  true for the projections of the set.  (The sets may have non­
                    empty intersections.)
               Theorem 214.  Consider a tuple of coefficients Aj.  The combinatorial state­
            ment above is true for these A/  if and only if
                                       ^A,C(x,)<0(logJV)
           for every N and for all strings x\,..., xn of complexity at most N.
               Proof.  Assume first that the combinatorial statement is true, and let us prove 
            the inequality for complexities.  Consider arbitrary strings x* of complexity at most 
            N.  Using the typization trick, we find a set A = A(x 1}..., xn) of similar objects; 
            its log-size log |A| is a polynomial in N.
               We apply  the  assumption  to  A  and  conclude  that  it  can  be  represented  as 
            a  union  of  a  polynomial  (in  log|A|,  therefore,  also  in  N)  number  of sets  with 
            required  properties  (inequality  for  the  sizes  of the  projections).  Let  В  be  the 
            biggest of these sets.  The set В is a polynomial fraction of A, and A is c-uniform 
           for a polynomially large c; therefore, В itself is c-uniform for some larger (but still 
           polynomially large) value of c.  The log-sizes of A- and ^-projections differ at most 
           by  O(logiV).  Therefore,  the inequality  for  В  implies  the  same inequality  for  A 
            (with 0 (log iV)-precision); we know that in this case the inequality for complexities 
           of Xi holds.
               To prove the reverse implication, we need the following lemma.
               Lemma.  Every set A c X\  x • • ■ x Xn can be represented as a union of poly­
            nomially (in N = log |A|)  many parts where each part is a c-uniform set for some 
           polynomially (in N) large value of c.  (The parts do not need to be disjoint.)
               Note that this lemma mentions neither Kolmogorov complexity nor inequalities. 
           Still it implies the result we want.  Indeed, the inequality for complexities is also true 
           for Shannon entropies of an arbitrary tuple of random variables.  In particular, this 
           inequality is true for a random variable uniformly distributed in one of the parts. 
           Since the parts are uniform, for each of them the entropy of each projection of this 
           variable is equal to the log-size of this projection of the set (with O(logc) = O(logn) 
           precision), so we get the desired inequality for the sizes of the projections.
               It  remains  to  prove  the  lemma.  There  are  several  proofs;  interestingly,  the 
           simplest proof uses Kolmogorov complexity and proceeds as follows.  Without loss 
           of generality we may assume that the elements (xi,..., xn) € A are tuples of binary 
           strings.  For each x € A consider the set of all  (yi,..., yn)  G A whose complexity 
           vector  (the list  of all  conditional  complexities)  relative  to  A  (i.e.,  A  is  added  as 
           a condition to all the complexities) is bounded coordinate-wise by the complexity 
           vector for x (also with A as a condition).
               Note the following two changes:  (1) before we considered all tuples y, while now 
           we consider only the elements of A;  (2) now we add A as a condition everywhere.
              334            10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY AND SIZE
                  Each of the subsets constructed in this way is determined by the complexity 
              vector of X, so we get only polynomially many sets.  It remains to show that each 
              of these sets is c-uniform for some polynomially (in N) large value of c.
                  This  can  be  done  as  before:  The  number  of elements  in  such  a  set  is  not 
              significantly smaller than 2c(Xl'—'Xn\A\ and the log-sizes of the sections are upper 
              bounded by conditional complexities, so we can use the same argument.                     □
                  It  is  interesting to find a purely combinatorial proof of this lemma that does 
              not use Kolmogorov complexity (recall that the statement does not mention Kol­
              mogorov complexity). This is not straightforward even for the two-dimensional case 
              (when A is a set of pairs).  Assume that some finite set A C N2 is given.  We said 
              that A is almost uniform if
                                m(l, 2) « m(l)m(211),  m(l, 2) « m(2)m(l 12).
              In  other  words  the  average  size  of  a  (non-empty)  vertical  section,  i.e., 
              m(l, 2)/m(l),  should  be  not  much  less  than  the  maximal  size  m(2|l),  and  the 
              same should be true for the horizontal sections.
                  How can we try to achieve this?  We can split the set A into parts classifying 
              the vertical sections according to their size  (say,  up to factor 2).  Each part then 
              has maximal size not greater than 2 times the average size (or even minimal size). 
              In this way we take care of vertical sections, but if after that we classify horizontal 
              sections in the same way, then we lose the property in the vertical direction.
                  So  what  could we do?  Let us first  note that  it  is  enough to  find  an almost 
              uniform subset of A that is not too small, i.e., it contains at least some polynomial 
              fraction  of the  original  set.  If we  know  how  to  do  this,  we  can  then  apply  the 
             same argument to the rest of A, and so on.  If at each step we separate at least an 
             e-fraction, then after 1/e steps we decrease the cardinality of A approximately by 
             factor e = 2.71828 • • •, so after a polynomial number of steps we have fewer than 1 
             element (i.e., nothing remains).
                  So how can we get a (not very small) part that is uniform in both directions? 
             Let  us  make  a vertical  classification  and  then  take  the  biggest  part.  We  forget 
             about the other parts, split this biggest part in the horizontal direction, and again 
             take the biggest part.  It  remains to  note that  this  biggest  part  is still vertically 
             uniform; see the proof of Theorem 210(c).
                  This argument can be generalized for arbitrary dimension.  Its advantage (com­
             pared to the complexity argument above) is that we get parts where non-uniformity 
             is bounded by a polynomial in the log-size of the part (and not of the entire set).
                  We can improve the statement even more and guarantee that non-uniformity 
             of all the parts is bounded by some constant that depends on the dimension n but 
             not on the size of the set.  It is done in [3] in the following way.
                  For every partition (into disjoint parts)  let us define its weight in such a way 
             that a minimal weight partition (it exists because the number of possible partitions 
             is finite) would satisfy all the requirements.
                  The weight of the partition is the sum of the weights of its elements, and the 
             weight of an element x that belongs to some part X is defined as
                                           ^logmx(H|A) - dlog|X|,
                                           а ,в
                                 10.9.  ONE  MORE  COMBINATORIAL INTERPRETATION                           335
              where the sum is taken over all pairs of disjoint subsets А, В C {1, 2, ,  n}, and d 
              is some constant factor to be chosen later.  Note that the sum also includes log \X\ 
              (obtained for A = 0 , В = {1, 2,..., n}).  All the elements of some part have equal 
              weights.
                   Let us show that for large enough d the number of parts in the minimal weight 
              partition is small.  Namely, we show that the total weight decreases when we com­
              bine two parts with almost equal parameters into one part.  “Almost equal” means 
              that the values of log mx (В | A) for the two part differ at most by 1 (for every A and 
              В).  Indeed,  the value of mx{B\A) for the combined part is at most three times 
              bigger than the same value for each of the parts, and the value of \X\ is multiplied 
              at  least  by  factor  1.5.  For  large  enough d the increase  in  |X|  will  outweigh the 
              possible decrease in all the mx(B\A).  Note that the suitable value of d depends 
              only on the number of terms in the sum  (and the latter is determined by n and 
              does not depend on the size of the set).
                   So  let  us  assume  that  d  is  chosen  in  this  way.   Now we classify  the  parts 
              according to the integer parts of \ogmx{B\A) for all A and B.  As we have seen, 
              for every tuple of integers there is at most one part,  and the number of possible 
              tuples  is  bounded  by  a polynomial  in  log|A|,  and  the  latter is  bounded  by  the 
              log-size of the set to be partitioned, so we get the desired bound for the number of 
              parts.
                   It remains to show that in minimal weight partitions, all the parts are almost 
              uniform.  To achieve this, we show that a non-uniform part can be split in such a 
              way that the total weight decreases.  While splitting some part, we do not change 
              the weights of the elements of the other parts, so we can concentrate on the weights 
              inside the non-uniform part.  Consider the formula that defines the weight, i.e.,
                                             ^T\ogmx(B\ A) - dlog|A|.
                                             а ,в
              When the part is split, for each of its points all the terms in this expression (both 
              with plus and minus signs) decrease.  We need that the decrease in the plus-part is 
              bigger that in the minus-part.  The latter can be computed easily:  If a part contains 
              m elements and is split into two parts of size pm and qm (where p + q = 1), then 
              the subtracted term (for all m elements altogether) decreases by dmh(p,q), where 
              h(p, q) — v{— logp) + <?(— logg) is the Shannon entropy of a random variable with 
              probabilities p and q,  and it does not exceed 1.  So the decrease  (per element)  in 
              the negative part is at most d.
                   If the part  (that we try to split)  is very non-uniform,  there exist sets A and 
              В such that m(A U В) significantly exceeds m(A) • m(B| A).  This means that the 
              AuB-projection has В-sections of very different sizes, and the maximal one exceeds 
              the average one by some large factor I.  We then split this projection (and therefore 
              the  entire  part)  into  two  pieces,  using  the  geometric  mean  between  the  average 
              and maximal size as a threshold.  For the piece with small sections the maximal 
              size is now y/l times smaller.  On the other hand, the large sections form at most 
              l/\//-fraction of all the sections (Chebyshev inequality), so for the other piece the 
              size of the А-projection also decreases at least by factor y/l.
                   So  the splitting reduces  at  least  one positive  term  (for  both pieces)  at  least 
              by logy/l (and other positive terms do not increase, as we have mentioned).  So if 
              I  is  large enough  (log y/l >  d),  the total weight decreases.  We conclude that the
             336            10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY AND  SIZE
             partition of minimal weight has c-uniform parts for some constant c that is (as well 
             as  d  and  I)  determined  by  n  (though  c  increases  fast  as  a function  of n).  This 
             finishes the alternative proof of the lemma that uses weights instead of Kolmogorov 
             complexities.
                           10.10.  The inequalities for two and three strings
                 As  we  have  seen,  there  exists  some  class  of linear  inequalities  that  can  be 
             defined in many equivalent ways, using entropies, complexities, sizes of projections 
             of uniform sets,  sizes of subgroups,  etc.  This  is  indeed  remarkable,  but  what  is 
             this class?  Can we give some explicit description of it?  Unfortunately, it remains a 
             wide open problem, and this class is known only for the simple case n ^ 3.  Let us 
             describe the answer.
                 For n = 1 the situation is trivial.  For n = 2 we have the inequalities
               оайКЯй.Ы,  0^Я(6КЯКЬЙ),
             which (in other words) say that all three quantities
                                         tf(ftlïi),       16), Щ г-Ь )
             are non-negative.  On the other hand, it is easy to see that these three quantities 
             may be arbitrary  non-negative  real  numbers.  Take  three  independent  variables 
             a, ß, 7 with arbitrary non-negative entropies, and let
                                             £i = (a,/?),£> = {ßi l)-
             Then we have #(£i|6 ) = H(a), /(£i:6 ) = H(ß), and #(&|£i)  =  H{i).  So 
             the three inequalities listed above are necessary and sufficient for a triple of real 
             numbers to be equal to
                                             Я «1),Я (& ),Я (|„6)
             for some random variables 0  and £2-  So we do not need other inequalities since any 
             other inequality is a consequence of these three.  Note also that linear programming 
             guarantees that  every consequence is  a non-negative linear combination of these 
             three inequalities.
                 In fact, this statement is not just the description of all true linear inequalities 
             for entropies (complexities, etc.)  in the case n — 2:  We have shown also that every 
             triple of reals that satisfies our inequalities can appear as an entropy triple.  Let us 
             repeat this statement in geometric language.
                 Each n-tuple of random variables determines a point that is a tuple of 2n — 1 
             values of entropies (for n = 2 we get points in three-dimensional space).  Consider 
             all these points  (for all n-tuples of random variables).  We get some set E in the 
             corresponding vector space.  The linear inequalities for entropies are closed half­
             spaces that contain E.  In the case n = 2, as we have seen, the set E is exactly the 
             intersection of half-spaces corresponding to the inequalities listed above.
                 In the general case the intersection of all the half-spaces containing some set 
             can be bigger than the set itself.  For example, this happens if the set is not convex 
             or not closed.  So even if we know all the inequalities for entropies, we have only 
             some partial  information  about  E.  On the  other  hand,  the  inequalities  are  the 
             most interesting part, since the equivalence is proven only for inequalities.  The sets 
             themselves are different (e.g., the sizes of projections are logarithms of integers, for 
             Kolmogorov complexity everything is defined only up to some 0(l)-term, etc.).
                                10.11.  DIMENSIONS  AND  INGLETON’S  INEQUALITY                  337
                 Now let  us  consider  the  case  n  =  3.  Here  we  have  a  subset  in  M7.  The 
             coordinates  correspond to  seven  non-empty subsets  of the three-element  set.  It 
             is  convenient  to  perform a linear transformation of this space and consider other 
             coordinates a\,..., 07, as described in Figure 5 (p. 49) for the case of Kolmogorov 
             complexities.  In these coordinates our inequalities just mean that all a*, except for 
             the central part a5  (that we denoted by /(£1 :^2:^з))5 are non-negative,2  and this 
             central part is non-negative if we add 02, 04 or üq to it.  We can say that £ С M7 is 
             a subset of a set F defined by these inequalities.
                 It is easy to see that F is a set of non-negative linear combinations of finitely 
             many generators.  These generators can be chosen in the following way.  First, we 
             let some a-L be 1 while all others are zeros.  In this way we get seven vectors.  This 
             is  not  enough:  we add one more where as  =  — 1,  02  =     — а% =  1,  and all the
             other ai  are zeros.  All these vectors belong to F\  let us check that they generate 
             the entire F.  Indeed, take some point in F.  The value of as may be negative, but 
             its absolute value does not exceed 02, 04, and üq, so we take our special vector with 
             coefficient  |as|  and then adjust all the other coordinates as needed.
                 Now it is clear that there are no other inequalities for entropies.  Indeed,  all 
             the  generating  vectors  belong  to  £  (can  be  implemented  as  entropies);  the  last 
             vector corresponds to independent £1  and £2  uniformly distributed in  {0,1}  and 
             £3 = £1 + £2  (mod 2).  Every true inequality is true for the generators, so it is true 
             for the entire F (and is a consequence of basic inequalities).
                  291  Show that the set £ (for n — 3) is not convex.  For example, for the last 
             generator e the vector Ae belongs to £ if and only if A is the logarithm of some 
             positive integer.
                  292 Show that  (for arbitrary n)  the set £ is closed under addition:  If two
             vectors e, e' £       belong to £, then their sum e + e' also belongs to £.
                 (Hint:  Consider two tuples that give e and e', and combine independent copies 
             of them.)
                  293  Prove that the closure of the set £ is convex (for arbitrary n)
                 (Hint:  If e and e'  belong to £,  then ke + le'  belongs to E for arbitrary non­
             negative integers к and I.  So it is enough to multiply a vector in £ by a positive 
             real.  We know how to multiply it by an integer factor; we can also (approximately) 
             multiply  it  by  a small  positive  real  using the  following trick—take our variables 
             with some small probability e, otherwise use fixed dummy values.)
                             10.11.  Dimensions and Ingleton’s inequality
                 In the previous section we described all true linear inequalities for the entropies 
             of two and three linear variables.  (Moreover, for n = 2, we described the set £ itself, 
             not only the dual set of linear inequalities that are true for all elements of £.)  For 
             n = 4, we do not have such a description; let us describe what is known.
                 Recall that we consider an n-tuple of random variables £ = £1,..., £n; by £/ (for 
             some set I C {1,..., n} of indices) we denote the tuple of variables & with i £ I.  We 
             consider the entropies H(£/) of these subtuples.  The conditional entropies H(£/ | £j) 
             are linear combinations of unconditional ones, so we do not need to consider them.
                 2In fact we used the notation I(x 1 :X2 :хз) for strings Х1,Ж2,жз and not for random variables, 
             but the definition for variables is the same.
            338          10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY  AND  SIZE
                Each tuple £ corresponds  to  a point  in M2"“1;  the coordinates of this point 
            are  H(£i)  for  all  non-empty  I.  These  points  (taken for  all  tuples  £)  form  a set 
            that we have denoted by £.  As we noted  (Problems 291,  292,  and 293),  this set 
            is  not  necessarily convex,  but its closure is a convex cone  (for each two points in 
            the closure, all their non-negative linear combinations also belong to the closure). 
            Then we switched from £ to its dual set, the set of all linear inequalities that are 
            true for all points in £.  Geometrically speaking,  we consider all half-spaces that 
            contain £.  If we take the intersection of all these half-spaces, we get the minimal 
            closed convex cone containing £ (standard result in linear programming theory).
                The following basic inequalities are guaranteed to be true for all points in £:
                                  H(£i)  ^       0                for every I,
                                  #(£/)  <       # (O)            for еуегУ I CJ,
                             + # (Ouj)  <  # (£/) + #  (O)        for every I, J.
            We have already used the name “basic inequality”  for the inequality
                             Я(6) + tf (6,6,£з) < Я(6,6) + Я(6, £з),
            that corresponds to the case I = {1,2}, J — {1,3} (the case of arbitrary I and J can 
            be reduced to this special case by grouping).  Now for convenience the inequalities 
            of the first two types are also called basic inequalities.
               To summarize:  the set £ is contained in the polyhedral cone defined by basic 
            inequalities.  For n — 2,  the set £ coincides with this cone;  for n = 3, this is not 
            true,  but  at  least  £ is dense in this cone.  For n = 4,  even this weaker statement 
            becomes false.
               How can we describe the convex cone defined by basic inequalities?  Each set 
            defined by  a finite  family  of linear  inequalities  is  generated by  its  extreme  rays, 
            and there are finitely many extreme rays.  If all of those rays intersect £ (as it was 
            earlier), we could conclude that all the inequalities for entropies are consequences 
            of basic inequalities.
               For n — 4, this is not the case, but it is still instructive to look at the extreme 
            rays.  They can be found (see  [64]);  most of them correspond to points in £, but 
           there are some others called “special” rays.  All special rays are the same up to the 
            renaming of variables, so we show here only one:
                                Я(6) = я(6) = ЯКО = Я(£4) = 2 га,
                                           Я(| 1.6) =4n,
                     ЯК, Кз) = Я К, К4) = Я (|2Кз) = Н((2,Ь) = Я « 3,£0 = Зп,
                    ЯКьККз) = ЯК„6К4) = ЯК,КзК4) = Я(6КзК4) = 4п,
                                        ЯК1К2К3К4) = 4га.
           In other words, each string has complexity 2n; all the strings together, as well as all 
           triples, have complexity 4n; and all pairs have complexity 3n, except for one special 
           pair that has complexity 4n.  (Here n is a non-negative factor that parametrizes the 
           ray.)
               It  is  not  easy  to  understand the  informal  meaning of these  conditions.  One 
           can draw a picture,  but  the picture for four strings  is  rather  complicated.  One 
           may note that £1  and £2  can be exchanged,  as well  as £3  and £4.  One  can also 
           draw the pictures for triples (Figure 33).  Trying to construct random variables or
                                                                     10.11.  DIMENSIONS  AND  INGLETON'S  INEQUALITY                                                                                                 339
                                                                               ft                   ft                                               f t    [ f t ]
                                                                         Figure 33.  Complexity pictures for triples
                            strings that  correspond  to  this  picture,  we  come  to  the  following problem.  The 
                            right picture hints that £3  and £4  have n bits of common information that is also 
                            included in £1  and £2-  On the other hand,  the left picture shows that £1  and £2 
                            do  not  have  common  information  (they have  zero  in  the  intersection  part).  Of 
                            course, it is not a formal contradiction, since we have not specified what we mean 
                            by  “common information”.  But  indeed we will show later that £ь£2)£з,  and £4 
                            with these complexities do not exist.
                                     If some extremal rays to not  intersect £,  it  may happen that some other in­
                            equalities  (in addition to the basic ones)  are true for complexities.  How can one 
                            find them?  Maybe we know all the extreme rays of our cone, and it remains to find 
                            the faces of this cone to get the new inequalities?  One can indeed find the faces of 
                            the cone generated by all non-special rays.  In addition to basic inequalities we get 
                            one more (up to renaming of the variables):
                                                                   Щз : ft К  /(ft : ft I ft ) + /(ft : ft I ft ) +  A ft : ft ).
                           We have rewritten this inequality in terms of conditional entropies to make it more 
                            understandable.  In terms of unconditional complexities we get the inequality
                                                                   12 + 3 + 4 + 134 + 234 < 13 + 23 + 14 + 24 + 34
                            (we write only the indices to make it short; for example, 134 stands for FT(£i, £3, £4)), 
                           which looks even more mysterious).
                                     It  turns out  that this inequality is well known in matroid theory;  it is called 
                            Ingleton’s inequality for the dimensions of subspaces of vector spaces:
                                     Theorem 215.  Let H\, H2, H3, H4 be finite-dimensional subspaces of some 
                            vector space.  Then
                            dim(#i+#2) + dim#3 + dim #4 + dmfiHi+Hs + Hi) + dim(#2+#3 + # 4)
                                    < dim (#4 + # 3)+dim(#2 + #3 ) + dim(#4 + #4 )+dim(#2 + #4 )+dim(#3 + Hfi).
                                     Before  proving  this  theorem,  let  us  elaborate  upon  the  connection  between 
                           entropies and dimensions.  Let F be a finite field, and let X be a finite-dimensional 
                           space over F.  For each subspace  Y  С  I ,   we consider  a random variable.  The 
                           probability space is the space of all linear functionals X —> F; the random variable 
                           £y  (corresponding to  the  subspace  Y)  maps  every functional  in  the  probability 
                           space to  its restriction on Y.  (Less formally,  for each subspace Y we consider a 
                           random variable that is a restriction of a random functional on Y.)  The values of 
                           £y are elements of the space Y* (dual to У), and they all have the same probability, 
                           so the entropy of £y equals dim У • log |F|.
             340            10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY AND  SIZE
                 Note that  all  the  random  variables      are  defined  on  the  same  probability 
             space, so we may consider the joint distribution of a pair (£y , £ z )  for two subspaces 
             Y and Z.  What is the entropy of this pair?  The restrictions of a functional on Y 
             and Z determine its restriction on Y + Z = {y + z\y £ Y, z £ Z} and vice versa. 
             So the entropy of the pair equals dim(T + Z) • log |F|.
                 This observation immediately gives the following corollary:
                 Theorem 216.  Every inequality that is true for entropies of random variables 
             and their tuples is also true for the dimensions of finite-dimensional subspaces of a 
             vector space over a finite field if the entropy of the tuple is replaced by the dimension 
             of the sum of corresponding subspaces.
                  294  Show that a similar statement is also true for finite-dimensional vector 
             spaces over M and over C.
                 (Hint:  Since  the  M-dimension  of a  space  over  C  is  twice  as  big  as  its  K- 
             dimension, it is enough to consider M.  We assume that a scalar product is defined 
             and consider the random variables that are projections of a random point in the 
             unit ball on the subspaces; the projections are rounded up to e-precision for some 
             small E.  The resulting variables do not have entropies exactly proportional to the 
             dimensions,  since the projection of the random point of the ball is not uniformly 
             distributed and the projections on X and Y determine the projection onto X + Y 
             only up to some precision, so we have finitely many possibilities, etc.  Still the main 
             terms are proportional to dimensions as e —> 0.)
                 If we want to generalize this result to arbitrary infinite fields,  a more compli­
             cated argument is needed.  First of all, the statement about the existence of a tuple 
             of subspaces with prescribed dimensions  (and the dimensions of their sums)  can 
             be translated into the language of matrices—it says that there exists a matrix of 
             a certain size where some minors are zeros while some other minors are not.  So if 
             some tuple of dimensions is possible for a field F, it is also possible for all its ex­
             tensions.  Thus we may consider only algebraically closed fields.  The algebraically 
             closed fields of some characteristic are elementarily equivalent to each other, so we 
             can choose an appropriate field:  C for characteristic zero was already discussed, and 
             we can choose the algebraic closure of Z/pZ for prime characteristic p.  If for this 
             field a tuple of dimensions can be implemented, it can be implemented by matrices 
             with algebraic elements,  so there exist a finite extension that contains all needed 
             elements,  and we again reduce the statement to the case of finite fields  (already 
             considered).
                  295 Provide the details for this argument.
                 Note that the conversion of subspaces into random variables is a rather general 
             way of constructing tuples of variables with required entropies (and therefore points 
             in S)—all the points in 8 that we have seen are constructed in this way.  Only in 
             the next section,  speaking about conditionally independent variables,  will we see 
             examples of an essentially different type.
                 Proof.  Now we finally prove Ingleton’s inequality for dimensions.  It cannot 
             be derived directly from Theorem 216, since it is not true for entropies of arbitrary 
             random variables.  So we need to prepare ourselves by establishing more connections 
             between entropies and dimensions.
                              10.11.  DIMENSIONS  AND  INGLETON’S  INEQUALITY               341
                 Recall  that  in  the  inequalities  for  entropies  the  conditional  entropy  H(a \ß) 
             appeared as a shortcut for H(a,ß) — H(ß).  This expression translates into dimen­
            sions  as  dim(A + B) — dim#.  In other words,  it  is the dimension  of the image 
            of the subspace A under linear mapping with kernel B, and this dimension equals 
            dim A — dim(A П В).  Similarly,  I(a:ß)  means  H(a) + H(ß) — H(a,ß),  and  it 
            corresponds  to  dim A + dim# — dim(A + В)  and  is  equal  to  dim(A П В).  Fi­
            nally,  I{ot:ß\'))  is equal to H(a I7) + H(ß I7) — H(a,ß I7), and it corresponds to 
            dim A/C+dimB/C—dim(A + B)/C, where X/C is the image of X under the linear 
            mapping that has kernel C.  Note that the latter expression cannot  be rewritten 
            as dim(A П B)/C—the image of Ап В under the mapping with kernel C can be 
            smaller than the intersection of images of A and В under the same mapping.  (This 
            happens, for example, if А, В, C are three different one-dimensional subspaces of a 
            two-dimensional space.)
                Ingleton’s inequality for the dimension of subspaces can now be rewritten as
                          dim(A П В) ^ I (A : В \ С) + I (A : B \ D) + dim(C П D),
            where I(A:B\C) stands for the dimension of the intersection of the images of A 
            and В under the mapping with kernel C.  Denote А П В by X\ it is enough to show 
            that
                               dimX ^ dimX/C + dimX/D + dim(C П D),
            since dim X/C ^ I (A : В \ C) (the image of the intersection is contained in the inter­
            section of the images, and can be even smaller).  The latter inequality corresponds 
            to an easy inequality for entropies
            and it remains to use Theorem 216.                                               □
                 296 Prove the inequality #(£) ^ #(£|7) -t- H(£\ö) + /(7 :5) that we used.
                (Hint:  Using the picture (or a simple computation), we note that 
                        л  (Î) + Я «  I ■7 , Ö) + Ih : 51Î) = H(i I ■7 ) + H(i IS) + J(7 : «), 
            so this inequality is the sum of basic inequalities.)
                 297 A careful reader would note that our proof of Ingleton’s inequality works
            only  for  vector  spaces  over  some  finite  field,  unless  we  use  some  rather  obscure 
            tricks for the case of an infinite field (see the discussion above).  How can we avoid 
            these tricks?
                (Hint:  The choice of the field was important for converting the inequality for 
            entropies into an inequality for dimensions.  But this inequality for entropies was a 
            combination of basic inequalities, and basic inequalities for dimensions are true for 
            arbitrary field.)
                 298  We know that the inequalities for entropies can be translated into in­
            equalities for  the sizes  of subgroups.  Show that  Ingleton’s  inequality  under this 
            translation becomes true for subgroups of an abelian group.
                (Hint:  Follow the proof of Ingleton’s inequality  using the sum of subgroups 
            instead of the intersection of subspaces  (in the abelian case the sum of subgroups 
            is a subgroup itself).)
                As a byproduct of our arguments we obtain the following interesting observa­
            tion:
             342           10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY  AND  SIZE
                  299 Every true linear inequality for dimensions that involves only four sub­
             spaces is a consequence of the basic inequalities and Ingleton’s inequality.
                 (Hint:  As we  have  mentioned,  all  the  extreme rays  for the  cone  defined  by 
             basic inequalities,  except for the special cases mentioned, can be implemented by 
             subspaces, not only by random variables.  Consider the convex cone generated by 
             these  (non-special)  rays.  One can check by  a  (long)  computation that  the faces 
             of this cone are given by basic inequalities and Ingleton’s inequality  (for different 
             orderings of variables).)
                  300 Formulate and prove a similar statement for four finite subgroups of an
             abelian group.
                         10.12.  Conditionally independent random variables
                 We have mentioned several times that Ingleton’s inequality may be false for 
             random variables.  Moreover, in this section we show an example in which the right- 
             hand side of this equality is zero while its left-hand side is positive.  A useful tool 
             here is the notion of conditional independence;  see  [153,  112]  for more advanced 
             applications of this tool.
                 Let a,ß, 7 be three variables defined on the same probability space.  We say 
             that a and ß are independent given 7 if /(a:/3|7) = 0.  It is easy to check that this 
             condition is equivalent to the following statement:  For every value 70 of 7 that has 
             non-zero probability, the conditional distributions of a and ß under the condition 
             7 =  7o  are independent.
                  301  Prove this statement.
                 (Hint:  I(a:ß I7) is an average (taken over all 70 with corresponding probabil­
             ities)  of the mutual information between corresponding conditional distributions.)
                 Abusing slightly the terminology, we say that two random variables a and ß 
             defined on the same probability space are conditionally independent if one can find 
             two other random variables 7 and 6 defined on the same probability space or on 
             its more fine-grained version (where elementary events are split into smaller ones) 
             such that
                    •  7 and 6 are independent;
                    •  a and ß are independent given 7;
                    •  a and ß are independent given 5.
                 We are allowed to split the elementary events in the probability space, so the 
             conditional independence property is now a property of the joint distribution of a 
             and /3, and it does not depend on the space where a and ß are defined.  (As usual, 
             we consider random variables with finitely many values.)
                 The three conditions in this definition mean that three terms in the right-hand 
             side of Ingleton’s inequality are zeros.  It remains to construct  an example where 
             the left-hand side is positive nevertheless:
                 THEOREM  217.  There  exist  conditionally  independent  random  variables  that 
             are not independent.
                 PROOF.  We need to provide an example of a quadruple of random variables 
             a, ß, 7,6 that satisfies the requirements stated in the definition of conditional inde­
             pendence, but where a and ß are dependent.  The following example was suggested 
             by Romashchenko.  Each of the four variables has values 0 and 1.  The variables 7
                                    10.13.  NON-SHANNON  INEQUALITIES                     343
            and 6 are independent and uniformly distributed, so each of four possible combina­
            tions has probability 1/4.
                It remains to define a and ß.  It is done as follows:  If 7 = 5, then the common 
            value of 7 and Ö is at the same time the value of a and ß  (so they are equal).  If 
            7 Ф 6, the joint distribution of a and ß (for two cases 7 = 1, Ö = 0 and 7 = 0,6 = 1) 
            is defined as follows:
                                                   0     1
                                              0   1/8   3/8
                                              1   3/8   1/8
            For a fixed value (say) 7 = 0 the conditional distribution of a and ß is the average 
            of this matrix and the matrix
                                                    0   1
                                                0   1 0
                                                1   0 0
            The average is
                                                  0      1
                                             0   9/16   3/16
                                             1   3/16   1/16
            and we get the joint distribution of two independent variables; each is equal to zero 
            with probability 3/4.  On the other hand, the joint distribution of a and ß is the 
            average of all the four matrices (for four possible conditions) and is equal to
                                                  0      1
                                            F
                                            I1   5/16  3/16
                                                 3/16  5/16
            so a and ß are dependent.                                                      □
                Let us summarize what we know about £ for the case n = 4 at this moment. We 
            started with a polyhedral cone defined by basic inequalities.  It is an upper bound 
            for  £.  We  stated  (without  providing  details  of the  corresponding  computation) 
            that the extreme rays of this cone are of two types:  non-special ones and special 
            ones.  For non-special ones it is easy to show that they belong to E, and we get a 
            lower bound for £\  the cone generated by these non-special rays.  One can check 
            (again a computation is needed) that this cone can be equivalently defined as the 
            set  of points  that  satisfy  the  basic  inequalities  plus  Ingleton’s  inequalities.  The 
            last  theorem provides an example of a point in £ that does not satisfy Ingleton’s 
            inequality, so this lower bound is not exact (for n = 4).
                In the next section we will see that the upper bound is not exact either  (for 
            n — 4).
                                  10.13.  Non-Shannon inequalities
                The  inequalities  that  are  not  linear  combinations  of basic  inequalities  were 
            founded in [222,  223];  see  [113]  for more details.  They are called  non-Shannon 
            inequalities.  Currently many such inequalities are known; however, their nature is 
            not well understood yet.  We consider only one example, the inequality from [113] 
            it is probably the most intuitive among them.
              344             10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY AND SIZE
                                                                  ß
                                   Figure  34.  The additional term W(a,ß,e)
                  Theorem 218.  For every quintuple of random variables a,ß,j,S,£ the follow­
              ing inequality holds:
                I(or.ß) < I(a:/?|7) + I(a:ß\S) + IfriS) + I(a:ß\e) + I(cr.e\ß) + I(ß:e\a).
                  This inequality looks frightening; nevertheless, some comments could be useful 
              (at least, for memorizing this inequality).  The right-hand side consists of two parts. 
              The first three terms, without e,  are exactly the same as in Ingleton’s inequality. 
              If there were no other terms in the right-hand side, we would get exactly Ingleton’s 
              inequality,  which is  false,  so  we  add other terms  to  make the  inequality weaker. 
              Namely, we add
                                  W(a,ß,e) = I(a:ß\e) + I(a:e\ß)+I(ß:e\a).
                  This additional term (see Figure 34) contains £ while the rest of the inequality 
              does  not.  One can say that  Ingleton’s  inequality  may  be  false,  but  the  error is 
              bounded by infe W{a,ß,e)\ this (mysterious) quantity depends only on a and ß.
                  One more observation:  we can use the same variable  (let us call it £)  as a, 
              ß,  and £  (here we really mean the same variable,  not the identically distributed 
              variable).  Then W(a,ß,e) — 0, and we get the inequality
              (*)                       Щ()^НШ+НШ6) + 1(т.6)
              that we have seen while proving Ingleton’s inequality for the dimensions of vector 
              spaces.
                  We can also derive the following conditional inequality from Theorem 218.  If 
              W(a,ß,e) = 0 for some e, then
                                     I (a :ß)^I{a:ß\4) + I{a:ß\6) + J(7 : S)
              for all 7 and 5.  Using this conditional inequality, we see that the special ray from 
              Section  10.11  does  not  belong to  E  (let  a =  £3,  ß  =  £4,  7  =  £ —      S  =  £2). 
              However, this ray satisfies all basic inequalities, so the inequality of Theorem 218 
              cannot be derived from basic inequalities.
                  This conditional inequality can be proven directly by applying the inequality 
              (*) to the random variable £ provided by the next theorem (applied to a, ß, and e).
                  Theorem  219.  IfW(a,ß, 7 )  =  0,  the  random  variables  a,/3,7  have  “fully 
              extractable  common information”  in  the following  sense.  There  exists  a  random 
              variable £  such that
                                         H((\a) = H((\ß) = H(t\'1)=0,
                                                                             =   0.
                                    10.13.  NON-SHANNON  INEQUALITIES                      345
                P r o o f.  Consider a three-dimensional table for the joint probability distribu­
            tion of a, ß,7.  Our assumption says that every two-dimensional section of this table 
            (some coordinate is fixed) has rank 1 (if it has rank 0, then the corresponding value 
            appears with probability zero and can be eliminated).
                First  let  us  assume  that  all  the  elements  in  the  three-dimensional  table  are 
            positive;  it this case the three variables a, /5,7 are independent.  Indeed,  consider 
            all one-dimensional sections that are parallel to the first coordinate, and fill a two- 
            dimensional table (indexed by other coordinates) with these vectors.  All the vec­
            tors  are  non-zero  (and  even  have  all  non-zero  coordinates),  and  the  assumption 
            guarantees that  they are proportional in each row and each column of this two- 
            dimensional table.  So all the vectors are proportional, therefore all the orthogonal 
            two-dimensional sections are proportional.  By the assumption, these section have 
            rank 1, and this finishes the proof for the case of an everywhere positive table.
                Now consider the general case.  In this case we will show that the table has 
            a  block  diagonal  structure,  i.e.,  it  may  be  split  into  blocks,  where  each  block  is 
            a  combinatorial  parallelepiped  (product  of three  sets  of indices),  projections  of 
            different blocks onto the coordinate axes are disjoint, and the table has zeros outside 
            the blocks (and positive values inside all blocks).
                We can apply the above-mentioned argument to each block to get independence 
            inside each block.  This finishes the proof,  since we can use the block number as 
            £;  the variable £ is a function of each of the variables a, ß,7, and when £ is given 
            (=the block is fixed), variables a ,/5,7 become independent.
                So it remains to prove the block diagonal property.  Consider the set of positions 
            that contain strictly positive elements.  This set has the following property:  If two 
            opposite corners of a rectangle (parallel to the axes) contain positive elements,  then 
            the two other comers contain positive elements.  (Otherwise, the determinant of the 
            corresponding 2x2 matrix is not zero, and the rank is greater than 1.)  Now, using 
            this property,  let us find bigger and bigger blocks in the table.  We start with an 
            one-element block.  At each step we look at whether there exists a positive element 
            in a place that falls into the block along at least one coordinate.  If yes, it is easy 
            to see (using the property above) that the block can be extended into a bigger one 
            with all non-zero elements.  In this way we get a maximal block that is separated 
            from other non-zero elements along each coordinate, and we then apply the same 
            argument to the rest of the table.                                             □
                 302 Provide the missing details for this argument.
                 303 (a)  Prove  the  following  statement  (it  is  sometimes  called  the  double 
            Markov property lemma):  if I(ß : 7 1 a) = 0 and I (a: 7 1 ß) = 0, there exists a random 
            variable £ such that H(£\a) — 0, H(£ \ß) — 0, and I((a,ß): 7 ^) = 0.
                (b) Derive Theorem 219 from this statement.
                (Hint for (b):  For Theorem 219 one can use the same variable £ as in (a).  One 
            can prove that the variables a and 7 are independent when £ is known using the 
            independence of the pair aß and 7.  (A similar argument shows that ß and 7 are 
            independent when £ is known.)  In addition, we know that a and ß are independent 
            when £ is known.  One can derive from this that a and ß are independent when £ 
            is known and that £ is a function of 7.  How can we do this?  For the first claim, 
            draw a diagram with three variables aß,  7,  and £  and  look  at  the  regions  that 
            contain zeros.  This diagram shows that the mutual information between £ and aß
                             10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY  AND SIZE
              346
              is not less than the mutual information between 7 and aß.  On the other hand, the 
              condition W(a,ß, 7) = 0 implies that the mutual information between aß and 7 
              equals I(a:ß:7) = I(a:ß).  Also, the conditions H(£\a) = #(£|/3) — 0 imply that 
              the mutual information between aß and £ equals I (a :/?:£) (see again the diagram). 
              Therefore,  I(a:ß)  ^  /(cc :/?:£)  ^  I(a:ß:7)  =  I(a:ß),  and  all  inequalities  here 
              become equalities.  The equality  between  the two  first  terms  means  that  a  and 
             ß  are  independent  when £  is  given.  Finally,  the  entropy  #(£ 17)  is  bounded  by 
              #(£|q!,7) + #(£|/?,7) + I(a:ß I7) and all three terms in the last sum are zeros.)
                  In [154] a Kolmogorov complexity version of Theorem 219 is proven.  Assume 
             that a, b, c are strings,  and three quantities I(a:b\c), I(a:c\b),  and I(b:c\a)  are 
             small,  e.g.,  are  bounded  by  0(log(|a| + |b| + |c|)).  Then  there  exists  a string  d 
             such  that  the  conditional  complexities  (7(d|a),  C(d\b),  C(d\c),  as  well  as  the 
             mutual information I(a:b\d), I(b:c\d), and I(a:c\d)  are also small  (bounded by 
             0 (log(|o| + \b\ + |c|))).
                  Artificial independence.  We considered some special cases of Theorem 218. 
             Now we will prove Theorem 208 in the general case using some trick  (by making 
             some variables independent).
                  Proof.  Let us split the variables in the inequality into three groups:
                                            (1)  a.ßi    (2)4,*;  (3) e.
             Note that in our inequality (2)-variables never appear in the same tuple with (3)- 
             variables, though variables of both groups are used together with (Invariables.  So 
             without loss of generality we may assume that the pair (7, S)  and e are independent 
             given  (a,ß).  Indeed,  consider  a different joint  distribution  for  all  the  variables, 
             obtained  in  the  following  way:  First  we  generate  values  of  (a,ß)  according  to 
             the existing distribution,  and then we independently generate values of (7, S)  and 
             e  according to their  (existing)  conditional distributions given  (a,ß).  Indeed,  for
             (l)  + (2)-variables the distribution remains the same, so the entropies do not change; 
             the same is true for (1) + (3)-variables.
                  Knowing this,  we  see  that  it  is  enough  to  prove  a  weaker  inequality  (with 
             additional terms in the right-hand side):
                             I (a :ß)^I(a:ß\'y) + I(a:ß\S) + /(7 : Ô) + W(a, ß, e)
                                        + /((7, S):e\(a,ß))
                                        + I('y:e\(a,ß))
                                        + I(6:e\(a,ß)).
             Indeed, if the pair  (7, S)  is independent with e given (a,ß), then the same in true 
             for its components 7 and 5, so the last three terms vanish after making the groups 
             artificially independent  (and the other terms remain the same).
                                    10.13.  NON-SHANNON  INEQUALITIES                      347
                The latter inequality is in fact the sum of eight basic inequalities:
                                           I({a,ß):e |7 ,£) ^ 0,
                                               I(a:ß |e,7) ^ 0,
                                               I(a:ß\e,6) ^ 0,
                                                 I(T.ô\e) ^ 0,
                                                 I(T-e\a) > 0,
                                                 I{l'-£\ß) > о,
                                                 I(S:e I a)  ^  0,
                                                 I(6:e\ß) ^ 0.
            There is no problem to check this:  we just need to express all the mutual information 
            in terms of entropies of tuples, and we get the same inequalities after canceling the 
            opposite terms.  Still it remains unclear why this happens and how one can invent 
            such a trick.                                                                  □
                One may say that our non-Shannon inequality, while not being a positive linear 
            combination of basic inequalities, still follows from them in a more general sense. 
            In fact, we have discovered a general deduction rule for entropy inequalities:  if we 
            manage to split the variables in some inequality into three groups in such a way 
            that the second and third group never meet, then this inequality can be derived 
            from a  (generally)  weaker  inequality where the mutual  information  between the 
            variables of the second and third group (conditional to all the variables of the first 
            group) is added.
                This rule can be used to prove many other non-Shannon inequalities for en­
            tropies  (it  is  known that  one can get  in this way infinitely many inequalities for 
            four variables, and each of them is not a positive linear combination of others).
                Deleting the unique information.  There is one more tool that can be used 
            to  derive  new  inequalities  for entropies.  It  is  based  on  the  following  Ahlswede- 
            Körner theorem [2] that we state here without proof.  (See [113, Lemma 5] for the 
            proof.)
                Theorem 220.  Let a,ß,e be random variables  with some joint  distribution. 
            Consider n  independent  copies  of this  triple,  and  denote  them by ai,ßi,£{  (i  = 
            1,..., n). Let
                           A = (ax,...,an),B = {ßi,... ,ßn),E = (el5...,en).
            Then there exist a random variable E',  defined on the same space as A,B,E such 
            that all seven entropies on the diagram for the triple А, В, E'  are the same  (up to 
            o(n))  as for the triple А, В , E,  except for one, H(E' | A, В), that is now o(n) instead 
            of H(E\A,B).
                Informally speaking, E' is like E but does not contain any information that is 
            missing in A, B.  A similar statement is true for an arbitrary number of random 
            variables  ai,...,otk  (with  some joint  distribution).  Let  us  take  n  independent 
            samples from this distribution  and denote the resulting variables by  A\,..., Ak ■ 
            One can “delete” the information that is unique for Ak (is missing in A\,..., Ak-1) 
            and replace  Ak  by  some  A'k  in such  a way  that  all  the  regions on  the  diagram
                       348                       10.  INEQUALITIES  FOR  ENTROPY.  COMPLEXITY  AND  SIZE
                       k e e p   t h e ir   s iz e   ( w it h   o (7 r )- p r e c is io n )  e x c e p t   fo r   t h e   o n e :    the conditional entropy
                       H(A'k\Ai,..., A -i) is  now   o{n).
                                304 Prove this statement for the case к = 2.
                              Note that it is important here that we deal with n independent copies and allow 
                       o{n) errors.  It is not possible to get such a result without that.  Indeed, let a and 
                       £ be dependent uniformly distributed binary variables.  Then one cannot construct 
                       a variable e'  that has Н{е'\а)  = 0  (i.e.,  is a function of a)  and has the required 
                       entropy.
                              Now let us explain how the Ahlswede-Körner theorem can be used to prove 
                       Theorem 218.  Let a,/5,7, 5, e be some random variables (on the same probability 
                       space).  First  of all,  let  us  prove  the  inequality  of Theorem  218  with  the  addi­
                       tional term 3H(s | a, ß)  in the right-hand side.  It can be obtained by adding the 
                       inequalities
                                                             H(eb)^H(e\c,) + H(e\ß) + I(a-.ß\1), 
                                                              H(e IÄK H(e I a) + H(e \ 0) + I(a : ß \ S), 
                                                                  Н{е)^Н{еЬ) + Н(е\5) + 1(т.0),
                       and the equality
                                      I {a : ß) + 2 H{e \ a) + 2 H{e | ß) = H{e) + W{a, ß, e) + 3H{e\ a, ß).
                       We have already seen the third inequality  (Problem  296,  p.  341).  The first  two 
                       inequalities  follow  from  its  conditional version.  All  three inequalities  are combi­
                       nations of basic inequalities.  The equality is easy to check using the diagram for 
                       c*,ß,£.
                              Now  the  Ahlswede-Körner  theorem  allows  us  to  get  rid  of the  undesirable 
                      term 3H(£\a, ß).  Consider n independent tuples cq, /?i,7t,£t,£i  (for i =  l,...,n) 
                      of variables with the same distribution,  and random variables A —  (cq,... ,an), 
                       В = {ßi                             С = (7i,...,7n), D = (5Ь...,5П),  E  =  (fi, ..., en).  The
                      entropies of variables А, В, C, D, Е and their combinations are n times bigger than 
                      the entropies of the original variables and their combinations.  Now we can apply the 
                      Ahlswede-Körner theorem to random variables a, 
                                                                                                                      ß, £ and get a random variable 
                      E'  defined  on  the  same  space  as  the  variables  A, B,C, D, E.  For  A,B,C,D,E' 
                      we write the inequality with the additional term 3H(E' \A, B)  in the right-hand 
                      side  (that  we  have just  proven).  This  additional  term  is  o(n)  as  stated  by  the 
                      Ahlswede-Körner theorem.  Then we replace E' by E in all other terms.  We claim 
                      that  all the terms then change only by o(n).  Indeed,  for the terms that contain 
                      C or D, nothing changes as these terms do not contain E'  (here we use the same 
                      property of our inequality as in the other proof of Theorem 218).  And terms that 
                      do not contain C or D can be represented as sums of regions on the diagram for 
                      A,B,E' different  from H(E' \A, B).  Therefore the inequality  remains true with 
                      o(n)-precision after the replacement.  It remains to divide by n and note that we 
                      get the desired inequality as n —> oo.
                              It is instructive to compare these two tricks (making variables artificially inde­
                      pendent and applying the Ahlswede-Körner theorem).  In both cases we have mod­
                      ified the joint distribution of the variables A, B, C, D, E.  (We can apply artificial 
                      independence to the n-times-sampled variables, it does not make any difference.) 
                      In  the  first  case  we  kept  that  joint  distributions  for  А, В, E  and  for  A, B, C, D
                                   10.13.  NON-SHANNON  INEQUALITIES                    349
            unchanged.  In the second case we kept unchanged only the joint distribution for 
            A,B,C,D.  Then we killed the term I(CD:E\AB) in the first case, and the term 
            H{E\AB) in the second case, while keeping the other terms (almost) unchanged.
                                CHAPTER 11
                         Common information
                  11.1.  Incompressible representations of strings
           Is  “the information in a string”  material?  Though this question sounds quite 
        informal, the following example gives an idea of what we are asking.  Assume that 
        we are given a string x whose Kolmogorov complexity is n and thus x “has n bits of 
        information”.  Can we divide that information into two equal parts, as if those bits 
        were pebbles?  This question may be formulated quite formally:  Are there strings 
        x\  and X2, each of complexity nj2, such that C(x\ \ x) ~ 0 and C(x2 |x) ~ 0 (i.e., 
        X\,X2  do not  have any new information compared with x)  and C(x\x\,x2)  ~ 0 
         (i.e., no information is lost)?  It is natural to understand the approximate equalities 
        as equalities holding with accuracy O(logn).  We will soon see that such x\ and X2 
        indeed exist.
           It is convenient to use the following notion in this context.  We say that strings 
        x  and  y are equivalent  with  accuracy c,  or just  c-equivalent,  if C(x\y)  ^  c  and 
        C(y I x) ^ c.  Of course this relation is not an equivalence relation.  If x is equivalent 
        to y, and y is equivalent to z, both with accuracy c, then we can prove only that x 
        is equivalent to 2 with accuracy 2c + O (loge).  (An alternative approach would be 
        to consider sequences xo, x\,..., Xi,... of strings where the length of Xi is bounded 
        by  a polynomial  in i,  instead  of individual  strings;  then  we  may  call  sequences 
        x0,xb ... and j/о, г/i,..., equivalent if C(xi\yi) = O(logi) and С(у{\х{) = O(logi). 
        In this way we get a true equivalence relation on sequences of strings.)
           The complexities of c-equivalent strings are almost the same—they differ by at 
        most 0(c) and even c + O (loge).  More generally, if we replace a string by another 
        string that is c-equivalent to the original one,  then all the complexities involving 
        that string change by at most 0(c).  For example I(x:y\z) changes by at most 0(c) 
        after replacing each of the strings x, y, z  (and even all of them at the same time) 
        by a c-equivalent string.
           Using this notion, we can now formulate our first observation:
           Theorem  221.  For every string x  there is  a string x'  of length C(x) that is 
        О (log C(x))-equivalent to x.  The string x' is incompressible,  that is,  its complexity 
         differs from its length by at most 0 (logO(x)).
           P roof.  Let x' be (some) shortest description of x.  Then its length is C{x) and 
        its complexity differs from C{x) by at most a constant.  As we can algorithmically 
        transform x'  into x,  we have C(x \x')  =  0(1).  On the other hand,  the following 
        inequalities
                       C(x) ^ C{x,x') < C(x') < l{x') = C(x)
        hold up to a constant additive term.  As the leftmost and the rightmost terms in 
        these inequalities coincide, they are equalities.  In particular, C{x,x') ~ C{x).  The
                                   351
         352                11.  COMMON  INFORMATION
         theorem on the complexity of the pair implies  that  C{x' \ x)  æ  0 with  accuracy 
         0 (logC(x)).                                           □
            As a corollary we obtain a positive answer to the above question:  Replace the 
         given string by its shortest description x', and let x\  and X2 be two halves of x'.
            305 Verify that all the requirements are fulfilled.
            3Ö6 Assume that C(y\x) = n.  Show that there is an (intermediate) 2 such 
         that C(z |x) æ nj2 and C(y\z) æn/2 with accuracy 0(logC(x, y)).
           In these examples information bits behave as something material.  A similar 
         thing happens when we deal with information in a string and some part  of this 
         information.  Let us explain what we mean by this.
           Assume that some strings x and y are given such that C(y |x)  ~ 0  (“all the 
         information in y is a part of the information from x”).  Then there is an incom­
         pressible string x' that is equivalent to x, and some prefix y' of x' that is equivalent 
         to y.  (This implies that y' is an incompressible string of length about C{y).)  More 
         specifically, the following holds:
           Theorem  222.  For every two  strings x  and y  there  exist  strings x'  and y' 
         that are equivalent to x and y (respectively) with accuracy 0(C(y\x) -flog C(x,y)), 
         such that y'  is a prefix of x'  and both x'  and y'  are incompressible  ( with the same 
         accuracy).
           Proof.  Let y'  be a shortest  description of y.  Then y'  is  an incompressible 
         string of length C(y) and y' is equivalent to y.
           Let z' be a shortest description of x conditional to y.  Then z' is an incompress­
         ible string of length C{x\y).  Knowing y'  and z',  we can find y and then find x. 
         Therefore the complexity of the pair y' ,z' is at least C(x,y).  On the other hand, 
        the total length of strings y'  and z'  equals  C(y) + C(x\y)  « C(x,y).  Hence the 
        string x' = y'z' is incompressible.
           As we have seen,  C'(xlx')  æ 0.  It remains to show that  C{x'\x)  ~ 0.  Since 
         C'(xlx')  «  0,  we have  C(x,x')  «  C{x')  æ  C(x,y).  On the other hand,  we have 
         C(x,y) ~ C(x) with accuracy 0(C(y |x)).  Hence C(x,x') ~ C(x) and the Kolmo- 
        gorov-Levin theorem implies that C{x'\x) ~ 0.           □
           By this theorem we can think of every two strings x,y with C(y\x) « 0 as a 
        string consisting of C(x) almost material bits and its prefix of length C(y).
                 11.2.  Representing mutual information as a string
           Is there an analog of Theorem 222 for arbitrary two strings x, yl  Recall that 
         any two strings x,y can be characterized by their complexities C(x), C{y) and the 
        complexity C(x, y) of the pair.  These values determine both conditional complexi­
        ties (with logarithmic accuracy) and the mutual information
                          C(x\y) = C(x, y) -  C(y),
                          C(y |x) = C{x,y) -  C(x),
                          I(x.y) — C(x) + C(y) - C(x, y)
         (cf.  Figure 3 on p. 46).  We have seen that sometimes Figure 3 can be understood 
        almost literally—this happens when x and y are overlapping substrings of a random 
        string.  One can conjecture that this holds in the general case.
                          11.2.  REPRESENTING  MUTUAL  INFORMATION  AS  A  STRING          353
                 C onjecture.  For every two strings x and y there is an incompressible string 
            и of length C(x,y)  that is equivalent (with logarithmic accuracy)  to the pair (x,y), 
            such that the C(x)-bit prefix of и is equivalent to x and the C(y)-bit suffix of и is 
             equivalent to у  (with the same accuracy).
                However this conjecture is wrong.  To see this, notice that the conjecture implies 
            that for every two strings x and у there exists a string z (the common part of the 
            said prefix and suffix) such that
                                             C(z\x) — 0,
                                             C(z\y) = 0,
                                               C(z) = I{x:y)
            (up to logarithmic error terms).  Informally, these equalities mean that the string 
            z represents the common information in x and y.  We will show that for some pair 
            x, у there is no such z.
                 307  Show that if for some strings x and у such a 2 exists, then the conjecture
            is true for these x and y.
                There are several counterexamples to the conjecture.  The simplest counterex­
            ample  (from Muchnik’s paper  [134])  is the following one.  We will construct two 
            strings  x  and  у  of complexity  2n  that  have  n  bits  of mutual  information  (with 
            logarithmic precision).  Thus the complexity of the pair  (x,y) will be close to 3n. 
            Additionally, there will be no string z of complexity n such that C(z | x) and C(z | y) 
            are negligible (with accuracy O(logn)).
                How can we do this?  Let  us first  rewrite the  latter two  conditions for  z  as 
            C{x\z)  = n and C(y\z) — n.  Thus we are looking for strings x and у both of 
            complexity 2n that have n bits of mutual information and for which there is no 
            string z such that C(z) и n, C(x\z) « n, and C(y\z) « n.  The following theorem 
            guarantees the existence of strings x and у with these properties and even with a 
            stronger property:  There is no z such that C(z), C(x\z), and C(y\z) are less than 
            l.ln.
                Theorem 223.  For every n there are strings x and у such that
                   C(x) — 2n + O(logn), C(y) = 2n + O(logn), I(x:y) = n + O(logn), 
            and such that there is no z  of complexity less  than 1.1 n  with C(x\z)  <  1.1 n and 
            C(y I z) < 1.1 n.
                P r o o f.  Let  us  show  that  there  exists  a  pair  x,y  of strings,  each  of length 
            2n + 2, such that
                   •  C(x) ^ 2n,
                   •  C(y) ^ 2n,
                   •  C(x, y) ^ 3n, and
                   •  there is no z with C(z) < l.ln, C(x\z) < l.ln, C(y\z) < l.ln.
            Indeed,  the  first  condition is violated by less than quarter of all  pairs  (the total 
            number of x’s is 22n+2, and only 22n of them have complexity less than 2n).  Sim­
            ilarly,  the second condition is violated by less than quarter of all pairs.  The third 
            condition is violated by less than 23n pairs, which is a negligible quantity compared 
            to the total number 24n+4 of all pairs.  Finally, the fourth condition is violated by 
            less than 2l ln x 2l ln pairs for every fixed z.  As there are less than 2l ln different
            354                        11.  COMMON  INFORMATION
            strings z, the total number of such pairs is less than 3.3n, which is again negligible 
            compared to the total number of all pairs.
                So there are many pairs satisfying all the conditions.  Let x, y be the first pair. 
            To specify this pair we need to know n and the following three lists:
                  •  the list of all strings of complexity less than 2n,
                  •  the list of all pairs of strings of complexity less than 3n, and
                  •  the list of all pairs of strings  (u,v) with C(u) < l.ln and C(v\u) < l.ln.
                Recall that the complexity of the list of all strings of complexity less than к is
            at most к + 0 (log&)  (the list can be specified by the number к and the size of the 
            list, which is less than 2k and thus can be identified by к bits).
                Therefore, the list of all strings of complexity less than 2n has complexity at 
            most 2n + O(logn).  For the same reasons the complexity of the second list is less 
            than 3n + O(logn).
               A similar argument can be applied to the list of all pairs of strings (u,v) such 
            that C(u) < l.ln and C(v \ u) < l.ln.  The complexity of this list is at most 2.2n + 
            O(logn).  Indeed, we can find this list from n and the number of such pairs, which 
            is less than 22-2n.
               What is the joint  complexity of three lists,  i.e.,  the complexity of the triple 
            made of these three lists?  Is is much less than the sum of their complexities and is 
            bounded by our maximal bounds, i.e., 3n (with accuracy O(logn)).  Indeed, given 
            n, each list can be specified by its size.  Moreover, we only need to know the sum of 
            the sizes.  Indeed, we can generate elements from all three lists in parallel until we 
            obtain the specified total number of elements.  Once we get that many elements, 
           we know all the three lists.  It remains to notice that the sum of any three binary 
            numbers is at most two bits longer than the maximal number.  Therefore the joint 
            complexity of the three lists is at most 3n + O(logn).
               Thus,  the complexity of the pair x, у is at most 3n + O(logn).  On the other 
           hand,  its  complexity is  at  least  3n  by  construction.  Again  by  construction,  the 
           complexity of each of x, у is at least 2n.  As the length of both x, у is 2n + 2, the 
           complexity of each of them is at most 2n + 0(1).  Finally, by construction there is 
           no z with C(z) < l.ln, C(x\z) < l.ln, C{y\z) < l.ln.                         □
               It is clear that the bound can be improved; let us make more precise estimates. 
           Assume that we want to construct strings x and у of complexity 2n (both) with 
           mutual information n, but there should be no string z such that
                             C(z) < a,  C(x\z) < ß,    and  C(y\z)<'y.
           What are the conditions on a, ß, and 7 that make our construction possible?  The 
           list of all pairs u,v such that C(u) < a and C(v\u) < ß has complexity a + ß, so 
           we need the condition a + ß <  3n.  In the same way,  the condition a + 7  <  3n 
           appears.  Finally,  to prove the existence of a pair we need to know that less than 
           24n pairs are prohibited, so we add the condition a + ß + 7 < 4n.  These conditions 
           are sufficient to construct a pair with the required properties.
               Moreover,  we  can  prohibit  simultaneously  all  the  pairs  for  different  triples 
           a, yd, 7  that  satisfy  these  inequalities.  There  are  0(n3)  triples,  so  if we  require 
           the inequalities to be true with an 0 (logn)-margin, we still have pairs that are not 
           prohibited; the complexity overhead needed to merge polynomially many enumer­
           ations is also O(logn).  So we get the following statement:
                          11.2.  REPRESENTING  MUTUAL  INFORMATION  AS  A  STRING          355
                Theorem 224.  For every n there exist strings x, y of complexity 2n +  O(logn) 
            such that C(x,y) = 3n + 0(logn),  and for every z at least one of the following three 
            inequalities is true:
                (a)  C(z) + C(x\z) ^ 3n — O(logn);
                (b)  C(z) + C(y\z) ^ 3n — O(logn);
                (c)  (7(2) + (7(:r | z) + C{y\z) ^ 4  n — O(logn).
                To be completely formal, we state the following:  For some c and for all n there 
            exist  strings  x  and  y  whose complexities deviate from 2n by at most clogn,  the 
            complexity of the pair deviates from 3n by at most clogn, and for every string 2 
            one of the inequalities (a)-(c) is true with clogn in the right-hand side in place of 
            O(logn).
                The pair constructed in this theorem is the worst-case pair from the viewpoint 
            of common information.  This vague statement can be made precise in the following 
            way.  For each pair x,y let us consider the set C(x,y) C N3 of triples (a,/3, 7) such 
            that there exists a string 2 that makes three conditions
                               C(z) < a,  C(x\z) < ß,     and  C(y\z)<'y
            true.  The  set  C(x,y)  is  upwards  closed  (obviously).  Note  that  C(x,y)  is  not 
            determined by complexities of x, y, and the pair x, y:  We have seen two pairs that 
            that both have C{x) = C(y)  =  2n and C(x,y)  =  3n but have different  C(x,y). 
            As we will see,  these examples are extreme points:  we get a maximal set C(x,y) 
            if x and y are (respectively) 2n-bit prefix and 2n-bit suffix of some random 3n-bit 
            string, and we get a minimal C(x,y) for the pair provided by Theorem 224.
                Let us explain why this happens.  Assume that x and y are strings of complexity 
            2n with mutual information n (as usual, we allow the deviation of order O(logn) 
            without saying this explicitly).  Every triple  (a,/5,7)  G  C(x,y) should satisfy the 
            obvious inequalities
                               a + ß ^ 2n,   a + 7 ^ 2n,    a + ß + 7 ^ 3n
            (since C(x)  <  C(z) + C(x\z)  and so on).  This means that C(x,y) is a subset of 
            the set Cm of all triples (a, /5,7) satisfying these three inequalities.  (Again we omit 
            O(logn) terms that are needed for the exact statement.)
                How can we represent this set of triples in an intuitive way?  For each ß and 
            7 there is some threshold ao(ß,j)—the triples with a >      belong to Cm and the 
            triples  with  a < ao  do  not  belong to  Cm-  The graph of the function  (/3,7) 
            ao(/3,7)  is  the  boundary  of Cm-  It  has  three  faces  (corresponding to  the three 
            inequalities),  and can be represented on the plane by drawing for every a the set 
            of points where ao(/3,7) = a (i.e., the level line of the function ao).  This line is а 
            boundary of the horizontal (fixed a) section of Cm-  See Figure 35.
                Using this picture,  it  is easy to check that for our first example  (overlapping 
            factors  of a random string)  the set  C(x,y)  achieves  its  upper bound  Cm-  How 
            should we choose 2 for given a,/3,7 in (7m?  When a < n, we let 2 be a part of 
            the overlap;  adding 2 as the condition then decreases the complexities of both x 
            and у by a.  If a > n, we let 2 be the overlap plus some parts of x and у (in some 
            proportion; the change in the proportion gives different points on a line with slope 
            — 1 on the picture.
            356                         11.  COMMON  INFORMATION
                    Figure  35.  The set Cm                 Figure 36.  The set Cm
                Theorem 224 provides an example of a pair (x, y) with smaller C(x, y).  Indeed, 
            it gives a pair where C(x, y) is contained in the union of the sets
                               a + ß ^ 3n,   a + 7 ^ 3n,   a + ß + 7 > 4n
            that correspond to inequalities (a)-(c).  Intersecting Cm with this union, we get a 
            smaller set that is called Cm in the sequel; it is shown in Figure 36.
                In  fact,  Cm  coincides with C(x,y)  for this pair,  and  Cm  C  C(x,y)  for every 
            pair x, y where x and y have complexity 2n and the pair has complexity 3n.
                To check this, we need to find a suitable z for every point (a,/3,7) G Cm•  It-is 
            enough to do this for all minimal triples in Cm  (since C(x,y)  is upwards closed). 
            The points on the lines with slope  —1  (Figure 36)  correspond to a string z that 
            combines  part  of x  with part  of y  in  some  proportion.  For  example,  the  point 
            (1.5n, 1.5n) corresponds to z that combines n/2 bits from the shortest description 
            of x and n/2 bits from the shortest description of y.  The point  (n, n) corresponds 
            to  a  string  z  of  length  2n  that  combines  n  bits  from  each  of the  two  shortest 
            descriptions.  The point  (n + h, h)  (where 0 ^ h ^ n) corresponds to a string z of 
            length 2n — h that is a prefix of the shortest description of y.  Then C(y\z) is the 
            number of the remaining bits,  i.e.,  h.  On the other hand,  C(x\z)  is bounded by 
            C(x\y)  (i.e., n) plus C(y\z)  (i.e.,  h).  Finally, the points (/1,0)  (where 0 < h < n) 
            correspond to strings z of complexity at most 3n — h that contain all y and n — h 
            bits of the shortest description of x given y.  So we get the following statement:
                Theorem  225.  For every pair (x,y) of strings  that have  complexity 2n  and 
            mutual information n, the set C(x,y) is  (with logarithmic precision)  between the 
            lower bound Cm and the upper bound Cm ; both bounds are achieved for some pairs.
                As soon as the set C(x,y)  is known for a pair  (x,y),  we can formally derive 
            some properties of this pair.  Here is one example:
                Theorem  226.  Assume  that C(x,y) — Cm (as it happens for the pair con­
            structed in  Theorem 224).  Then for every z the inequality
                                       C(z) ^ 2C(z\x) + 2C(z\y)
            holds.
                Before proving this inequality, let us comment on its meaning.  It says that only 
            strings z of small complexity can be simple relative to x and y at the same time. 
            Note that if one can extract common information from x and г/, then this common
                      11.3.  THE  COMBINATORIAL  MEANING  OF  COMMON  INFORMATION        357
            information  z  is  simple  relative  to  x  and  y,  so  this  inequality  is  a  quantitative 
            statement that says that common information cannot be extracted.
                P r o o f.  We may assume without loss of generality that C(z)  =   0{n)  (if the 
            complexity of z is big, then C(z\x) and C(z\y) are greater than C(z)f2).
                Let  us  rewrite  the  inequality  in  terms  of the  quantities  that  appear  in  the 
            definition of C(x,y):  it may be rewritten as
                              C(z) < 2C(x, z) -  2C(x) + 2C(y, z) - 2C(y),
            and then
                     C(z) ^ 2C(z) + 2C(x I z) -  2C(x) + 2C(z) + 2C(y \ z) - 2C(y),
            i.e.,
                             2C{x) + 2C{y) ^ 3C(z) + 2C(x \ z) + 2C(y \ z).
            The left-hand side equals 8n; to check that the right-hand side cannot be less, we 
            consider each line on Figure 36 (and it is enough to consider points with a minimal 
            sum of coordinates).                                                          □
                 308  Prove that for small values of C(z) one can get a better bound 
                                       C(z) ^ C(z\x) + C(z\y), 
            but in general the constant 2 cannot be improved.
                    11.3.  The combinatorial meaning of common information
                Theorem 224 gives us an example of a pair of strings that do not have  (ex­
            tractable)  common information in the strongest possible sense.  Still it  does not 
            explain why this happens,  what properties of x  and y  make mutual information 
            non-extractable.  This is a rather informal question, and we do not know any state­
            ment that answers it completely.  Still some observations can be made.
                What does it mean that for given x  and y  there exists a string z such that 
            C{z) < o;,  C{x\z)  < ß,  and C{y\z) < 7?  Let us denote by Um(z)  the set of all 
            strings whose conditional complexity given z is less than m.  The size of this set is 
            about 2m.  Our condition means that the pair  (x,y)  is covered by one of the sets 
            Uß(z) x U*y(z)\ there are at most 0(2“) sets of this type (one for each z).
                Therefore, the condition  (o;,/?,7)  £ C(x,y)  means that the pair  (x,y)  is cov­
            ered by a union of 2“ combinatorial rectangles of size 2^ x 27.  (A combinatorial 
            rectangle is a product of two arbitrary sets.)  On the other hand, if (x, y)  is covered 
            by an enumerable family of 2“ combinatorial rectangles of size 2^  x 27,  then the 
            triple  (o;, ß, 7)  (plus the complexity of the enumeration algorithm and logarithmic 
            overhead)  belongs to C(x,y)\  Let z be the ordinal number of the rectangle that 
            covers  (x,y).  So one can say that the set C{x,y) is determined if we know which 
            (simple enumerable) families of combinatorial rectangles cover the pair (x,y).
                Now it is clear how one can construct an example of a pair without common 
            information—find a set that is difficult to cover by combinatorial rectangles,  and 
            take a random element of this set.  (This approach also was suggested by An. Much- 
            nik).
                It is convenient to identify the sets of pairs with binary relations, or bipartite 
            graphs—a pair (x, y)  is then an edge that connects vertex x  in the left part with 
            vertex y  in the right part.  The combinatorial rectangle is then the set of all edges 
            that connect some subset of the left part and some subset of the right part.
                                   358                                                                               11.  COMMON  INFORMATION
                                               There is a simple property of a bipartite graph that guarantees that it is hard to 
                                   cover this graph by combinatorial rectangles.  The graph should not contain cycles 
                                   of length 4  (there are no vertices a, b in one part and c, d in other part such that 
                                   all  four edges ac,ad,bc,bd are in the graph).  The following combinatorial lemma 
                                   shows that such a graph is difficult to cover:
                                               Lemma.  Consider a bipartite graph with I vertices in the left part and L vertices 
                                   in the  right part;  assume  that I  ^  L.  If this graph  does not have  cycles  of length 
                                   4,  then the density of edges in it  ( the number of edges divided by IL)  is bounded by 
                                   0  (  m    a   x  (  l  /  >   / Z    ,   1 / / ) ) .
                                              In other words, if we place stars in a rectangular table in such a way that no 
                                   four stars form a rectangle with horizontal and vertical sides, then the density of 
                                  stars is bounded either by 0 (l/smaller side) or by 0 (1/д/larger side).
                                              PROOF.  For each of I left vertices, consider the set of its right neighbors.  The 
                                  condition about cycles says that every two sets of this type (for two different left 
                                  vertices) have at most one common element.  The inclusion-exclusion formula then 
                                  allows us to give a lower bound for the size of the union of all these neighbor sets: 
                                  the sum of sizes of all sets (i.e., the total number of edges in the graph) minus the 
                                  number of all possible pairs, at most I2.  On the other hand, this union contains at 
                                  most L elements (the size of the right part).
                                              So we conclude that the total number of edges is bounded by L + I2, and the 
                                  density is bounded by l/l + l/L.  This gives the required bound if the first term 
                                  dominates the second one,  i.e.,  for  I  ^  \fh.  But  for I  ^  \/Z,  we get  the bound 
                                  0(l/L) which is not enough (we want 0 (l/>/Z)); this is OK if I = 0(y/L),  but I 
                                  can be bigger.
                                              To get the required bound, let us consider a part of the graph by choosing \fL 
                                  vertices  in the  left  part  (among I)  with maximal  number of neighbors.  Deleting 
                                  all other left vertices, we only increase the density, and for the reduced graph the 
                                  density is bounded by l/y/Z.  The lemma is proven.
                                              Now we need to find a graph without 4-cycles.  (Then the lemma guarantees 
                                  that  it  is  difficult  to  cover by  rectangles,  and  a random edge in this  graph gives 
                                  us  a pair  without  common information;  see  below.)  Here  is  a simple  geometric 
                                  construction.
                                              Consider some finite field F and a plane (a two-dimensional vector space) over F. 
                                  The left vertices are points in this plane; the right vertices are lines.  Edges connect 
                                  incident points and lines.  We do not have 4-cycles thanks to Euclid’s axiom:  for 
                                  two given points there is at most one line that goes through them.
                                              How many vertices and edges do we get? If the field contains about 2n elements, 
                                  then we have about 22n vertices on each side and about 23n edges (each line contains 
                                  about 2n points,  and for each point there is about 2n lines going through it).  So 
                                  for most edges in the graph, the complexities C(x) and C(y) are close to 2n, the 
                                  complexity of the pair is close to 2n, and mutual information I{x\y) is close to n.
                                                309 Show that I(x:y) = n + O(logn) for all edges xy in this graph whose 
                                  complexity exceeds 3n — O(logn).
                                              To finish the alternative proof of Theorem 223, let us see what fraction of the 
                                  edges (in this graph) can be covered by 2l ln rectangles of size 2l ln x 2l ln.  (We 
                                  again use  1.1  as a, ß,  and 7.)  We can apply the lemma above to each rectangle
                               11.3.  THE  COMBINATORIAL  MEANING  OF  COMMON  INFORMATION                                   359
                 and conclude that the density of edges is bounded by 2  0-55n, so the total number 
                 of edges covered by all the rectangles, is at most
                                         2i.in  ^  2l  ln  y         X  2 _ 0.55n     2^-75n       <2^n
                 So most of the edges remain uncovered (and most of the edges have required com­
                 plexities and mutual information), so a random edge in this graph with high prob­
                 ability provides an example required by Theorem 223.
                       310 Show that every edge xy whose complexity is close to 3n can be used as
                 an example for Theorem 223).
                      (Hint:  The set of covered edges can be enumerated by a simple algorithm.)
                      This construction gives a nice alternative proof of Theorem 223,  but there is 
                 one subtle point  in it:  we need to  know that  there exists a field of size close  to 
                 2n.  (Knowing that such a field exists, we can find it by a brute-force search, so we 
                 may assume that the field is simple given n.)  It is a classical (but not completely 
                 trivial) result in algebra and number theory.  The field of size 2n can be constructed 
                 by a degree n extension of a field with two elements consisting of all roots of the 
                polynomial x2  — x.  Also we can use the Bertrand postulate which guarantees that 
                for every N there is a prime number p between N and 2N and use residues modulo 
                p where p is a prime number close to 2n.
                      Now we have a concrete example of two strings with non-extractable mutual 
                information:  a random pair whose first term is a point, the second term is a line, 
                 and the point  and the line are incident.  To make this example more symmetric, 
                we may consider a projective plane instead of the affine one (the complexity does 
                not change much since infinite points form a negligible fraction of all points).  We 
                may also choose a random pair of orthogonal one-dimensional subspaces in a three- 
                dimensional space over a finite field (with scalar product X\y\ + Х2У2 + жзУз).
                      It would be nice to construct a similar example using spaces over M (points on 
                the sphere in R3).  Probably one can take some discrete subset of the sphere with 
                reasonable constant density, but there are some problems:  If x and у are close, then 
                there are many points that are almost orthogonal to both.
                      Of course, the coefficient 1.1 is not optimal.  The same argument can be tried 
                for arbitrary a,ß, 7 and is successful (the number of covered edges is less than 3n) 
                if they are not too large.  Using our lemma (exactly in the form it is stated), one 
                can get the following result.  The set C(x, y) for a random pair (incident point and 
                line)  is contained up to 0(logn)-precision in the set S shown in Figure 37.  (The 
                value a = 3n corresponds to the origin.)  For /3 ^ 7  the set  5 is defined by the
                                                         Figure 37.  The set S
        360               11.  COMMON  INFORMATION
        inequality
                        a + 7/2 + max{7/2, ß} > 3n,
        and for 7 ^ ß it is defined by the symmetric inequality
                        a + ß/2 + max{/3/2,7} > 3n.
        We can also intersect this set with Cm  (since C(x,y)  C  Cm)-  In this way we get 
        an even more complicated picture (not shown here).
          Unlike our previous examples, we have here only an upper bound for the set 
        C(x, y)\ the set itself remains unknown.  It may happen that C(x, y) for this example 
        depends on the choice of the finite  field F.  This  looks weird,  but  the following 
        problem shows that it is not so unbelievable as it could seem at first.
          I 311  Assume that F is a field of cardinality q2  (then q = pk for some prime 
        p).  Let  (x,y)  be a random pair of an incident point and line on a plane over F. 
        Prove that C(x,y) contains the triple  (1.5n,n,n) where n = log |F|  = 2 logg.  As 
        usual,  we ignore logarithmic terms.  (This point  is on the boundary of the set S 
        (see Figure 37).  We do not know whether the same is true for an arbitrary field.)
          (Hint:  The field F contains a subfield G of size q.  Every element in F has the 
        form t + sa, where t, s £ G, and a is some fixed element in F.  Then we can split the 
        pairs of an incident point and line into g3 classes, where each class contains at most 
        g3  pairs and involves at most g2 points and g2 lines.  To get this classification, we 
        consider lines of the form y — kx + b (vertical lines are non-random) and represent 
        the coefficients in this equation as к = /  + ra and b = h + sa, where /, r,h,s £ G. 
        If a point (x, y) is on this line, let us represent x as x = g + ta.  Then y = fg + h+ 
        ( ft+gr + s)a+rta2.  We may fix r, t, s in q3 different ways; each way corresponds to 
        a class of pairs.  Each class involves q2 lines (and this is OK) and g3 points (which is 
        not OK). To decrease the number of involved points, we can use the following trick. 
        Let us represent the coefficients of each line as к — f + ra, b — h + (s — ft)a, and 
        its points as (g + ta, fg + h + (gr + s)o; + rta2), where all coefficients /, g, h, s, r, t 
        are from G.  Now, fixing r, t, s, we get a set that involves q2 lines (parametrized by 
        f, h) and q2 points (parametrized by g, fg + h).)
           312 Prove that we get the same set C(x,y) for all randomly chosen pairs (x, y)
        in the line-point graph, up to О (log n)-precision: To decide whether a triple (a, ß, 7) 
        belongs to this set,  we need to know only the maximal possible number of edges 
        in a combinatorial rectangle of size 2^ x 27.  Namely, it is necessary and sufficient 
        that this number times 2a exceeds the total number of edges (up to a polynomial 
        factor).
          (Hint (Razenshteyn [151]):  For every two edges there exists an affine bijection 
        of the plane that maps the first edge into the second one.  If there is a rectangle with 
        a large number of edges, we may cover the graph by several random images of this 
        rectangle.  We need an additional factor of polynomial size to cover all the edges 
        with positive probability.  If all the rectangles are small, there are not enough edges 
        to cover the entire graph (or a significant part of it) by small number of them.)
          As in the previous section, the upper bound for C(x,y) implies an inequality 
        that bounds the unconditional complexity of 2 in terms of conditional complexities 
        (given x and y):
          Theorem  227.  Let x,y be a random pair of an incident line  and point in a 
        plane  over a finite field  of size  about 2n.  Then for  every  string z  the following
                     11.3.  THE  COMBINATORIAL  MEANING  OF  COMMON  INFORMATION       361
            inequality holds:
                                C(z) ^ 2C(z\x) + 2C(z\y) + O(logn).
                Proof.  As before, we need to prove that
                                          8n ^ 3a + 2ß + 2j
            for  all  (a,ß,j)  G  C(x,y).  It  is  enough to  check  this inequality for  all  points  in 
            S П Cm-  We may assume without  loss of generality that ß  <  7.  If ß exceeds 
            7/2,  then the required inequality is obtained as follows.  Multiply the inequality 
            <2 + 7/2 + /? ^ 3n (from the definition of S) by 2 and add the inequality <2 + 7 ^ 2n 
            (from the definition of Cm)-  Otherwise (if ß ^ 7/2), we sum up the inequalities 
            <2 + 7 > 3n (definition of 5), <2 + ß ^ 2n, and <2 + ß + 7 ^ 3n (both taken from the 
            definition of См)-                                                          □
                So  now  we  have  two  constructions  of a pair  without  (extractable)  common 
            information.  The first  construction  gives  better  bounds  for the  set  C(x<y).  So 
            what are the advantages of the second one?  The most important (though informal) 
            advantage is that we have found a combinatorial reason (a graph that is difficult to 
            cover by rectangles) and a simple sufficient condition for this (no cycles of length 4).
                A more formal advantage is that in the second construction we get a stochastic 
            example—our pair is an element of maximal complexity in a simple set.  Probably 
            the first construction does not give us a stochastic example.  However, we may prove 
            the existence of stochastic pairs with minimal C(x,y):  A probabilistic method can 
            be  used  to  prove  the  existence  of a  graph  that  is  maximally  hard  to  cover  by 
            combinatorial rectangles (see [151]).
               Another advantage of the second construction is that it can be used to prove 
            a  bit  stronger  statement  using  oracle  complexity.  In  fact  we  have  shown  that 
            for every oracle A there exist two strings x and y of complexity (without oracle) 
            2n  and  mutual  information  (also  without  oracle)  n  such  that  there  is  no  string 
            2  with  CA(z)  <  l.ln,  CA(x\z)  <  1.1?г,  and  CA(y\z)  <  l.ln.  Indeed,  even  a 
            very powerful oracle still defines some combinatorial rectangles, they cover only a 
            negligible fraction of the edges,  and it  remains to select  an uncovered  (and non­
            simple) edge.  (Of course, the resulting pair depends on the oracle, since every pair 
            is simple relative to some oracle.)
                313  State and prove a similar result with additional condition и (a string of 
            unlimited complexity) instead of the oracle.
               What happens with common  (extractable)  information  in  two  strings  if we 
            add some oracle?  Some results in this direction are obtained in [138]; it is assumed 
            there that the oracle is independent with a pair x, у (so the complexities and mutual 
            information remain unchanged).  Evidently, if some 2 is the common information, 
           then adding the oracle does not  destroy this.  It  can  be shown that  the reverse 
           statement  is  true  (however,  the existing proof does not  give our  usual  O(logn)- 
            precision but a much weaker result).
               We can apply similar combinatorial techniques to other algebraic constructions. 
            For example, we may consider a pair of orthogonal one-dimensional subspaces in a 
           four-dimensional space or,  almost equivalently  (if we ignore infinitely far points), 
            a random pair (a point in a three-dimensional space and a two-dimensional affine 
           plane that goes through this point).  One cannot apply the lemma about 4-cycles 
            any more (there are 4-cycles—for any two one-dimensional subspaces one can find
        362             11.  COMMON  INFORMATION
        a two-dimensional space orthogonal to both and two one-dimensional subspaces in 
        it).  Still there are only few one-dimensional subspaces in a two-dimensional space, 
       so we still can apply a similar argument based on the inclusion-exclusion formula. 
        (See the proof of Theorem 5 in [39] for details.)
          There are other similar examples.  One can consider a pair of two affine lines in 
       a three-dimensional space that have a common point.  Another series of examples 
       can be obtained in the following way.  Fix some integer n ^ 3 and some integers 
       k, I  such that  0  <  к < I < n.  Then in the n-dimensional space over a finite field 
       consider  a random pair  (/c-dimensional  subspace,  /-dimensional subspace)  where 
       the first subspace is contained in the second one.  Romashchenko has shown [153], 
       that one cannot extract common information from this pair.  The proof uses the 
       following remark:  making a short random walk in the resulting bipartite graph, we 
       get an almost uniform distribution.
            11.4.  Conditional independence and common information
          In this section we consider one more (quite mysterious) way to obtain strings 
       that have mutual information but no extractable common information [112, 153]. 
       Let us start by recalling the inequality of Problem 296 (p. 341):
                     H(t) ^ H(t\a) + H(t\ß) + I(or.ß),
       or, better to say, the corresponding inequality for complexities
                     C(z) < C(z IX) + C(z I у) + I(x : у)
       (as usual, we omit the logarithmic terms).  If I(x:y) = 0, this inequality implies an 
       upper bound for unconditional complexity in terms of conditional ones,
                        C(z) < C(z \x) + C(z\y).
       There is no surprise here—if there is no mutual information, there is no common 
       information to extract.  It seems that we are not getting anywhere.  But a similar 
       bound can be obtained for the case when x and у are  conditionally independent 
       relative to  two  independent  strings,  i.e.,  if there exist  strings  и  and v  such that 
       I(x:y\u)  —  0,  I(x:y\v) —  0,  and I(u:v)  =  0.  Namely,  the following inequality 
       holds:
          Theorem 228.
              C(z) ^ 2C(z I x) + 2C(z I y) + I(x:y\u) + I(x:y\v) + I(u:v) 
       for arbitrary strings x, y, z, u, v  (with 0(\ogC(x, у, и, z, v))-precision).
          This inequality is a consequence of the previous one and Ingleton’s inequality
                    I(x:y) ^ I(x:y\u) + I(x:y\v) + I(u:v),
       but, of course, Ingleton’s inequality is not always true.  So we need to proceed in a 
       different order.
          PROOF.  Let us consider again the inequality
                     C(z) ^ C(z |w) + C(z I v) + I(u:v)
                          11.4.  CONDITIONAL INDEPENDENCE AND  COMMON  INFORMATION                      363
              and  then  bound  C(z\u)  and  C(z\v)  using  the  relativized  versions  of the  same 
              inequality:
                                    C(z\u) ^ C(z\x,u) + C(z\y,u) + I(x:y\u),
                                    C(z\v) < C(z \x,v) + C(z\y,v) + I(x:y\v).
              After that it remains to note that we decrease the complexity when adding another 
              condition:  C(z\x, u) ^ C(z \ x).                                                          □
                   This  theorem  can  be  used  to  construct  strings  with  non-extract able  mutual 
              information.  Consider conditionally independent variables that are not independent 
              (Theorem 217, p. 342):  there exist a and ß that are independent given 7 and also 
              independent given <5, where 7 and 5 are independent variables, while a and ß are 
              not independent.
                   Now we may consider N independent trials of this quadruple and collect the 
              outcomes of a into string x, and outcomes of ß into y.  These strings x and y with 
              high probability will have significant mutual information that cannot be extracted. 
              To see this, let us collect the outcomes of 7 and 5 into и and v, and apply the last 
              inequality to these four strings.  (Note that to construct x and y, we need to make 
              N samples of a and ß alone; the variables 7 and 5 are needed only for the proof of 
              non-extract ability.)
                   For technical reasons,  to get a better error term  (our usual logarithmic term 
              instead  of a square  root  that  appears  when  we  compare  the  frequency  and  the 
              probability), one should consider not the independent trials,  but trials with fixed 
              frequencies.  Let us explain what this means.
                   Consider a quadruple of strings x, y, u, v of length N where the frequencies of all 
              quadruples of letters are equal to the probabilities for the outcomes of (a,ß, 7,5). 
              This means that x is a string in the alphabet that is the range of a, у is a string 
              whose alphabet is the range of ß, etc., and the number of positions i — 1,..., N, 
              where strings x, y, u, v have letters a, b, c, d, respectively, is equal to
                                      Pr[o: = a, ß = 6,7 = c, (5 = d\ ■ N + 0(1)
              (we need to add 0(1) for rounding—the product of N and the probability is not 
              always an integer).
                  As we know from Section 7.3 (Theorem 146), for most quadruples of strings with 
              these frequencies the complexities of these strings and their combinations deviate 
              from N x (the corresponding entropies) only by O(logAf).1  In this way we get a 
              quadruple of strings x, y, u, v such that
                      I(x:y I и) — О (log Af),    I(x:y\v) = O(logAf),       I(u:v) — 0(log N), 
              and at the same time
                                           I(x:y) = Nl(or.ß) + О (log AT),
              while  (according to our assumption)  I(a:ß) ф 0.  It remains to use Theorem 228 
              to conclude that for every z the inequality
                                             C(z)^2C(z\x) + 2C(z\y)
                   1To see this we should recall the proof of Theorem 146 where we estimated how many strings 
              with given frequencies exist.
          364                   11.  COMMON  INFORMATION
          holds with 0 (log 7V)-precision, while the complexities of x and y and their mutual 
          information is proportional to  N  (with the same precision)  with non-zero coeffi­
          cients.
             In this way we get one more construction of strings with non-extractable mutual 
          information.  They are stochastic (as in the geometric construction), and we (a bit 
          mysteriously) avoided any combinatorial considerations.
              314  We have already seen (Theorem 217, p. 342) that there exist conditionally 
          independent random variables a and ß,  both having the uniform distribution in 
          {0,1},  such that Pr[a = ß\  = 5/8.  Prove that this statement remains true if we 
          replace 5/8 by arbitrary c G [3/8, 5/8].
             However, if c is close to 0 or 1, a similar statement is not true:  one cannot add 
          7 and Ö to make these two variables conditionally independent.  Nevertheless,  for 
          eveiy c G (0,1) we can still prove that the common information is not extractable 
          (for most of the strings obtained by N trials for these a and ß).  We split this result 
          into the following two problems.
              315  Consider an arbitrary c G  (0,1).  Prove that there exist finite chains of 
          random variables ao, ai,..., otk and ßo,ßi,  .., ßk defined on some common prob-
          ability space such that
               •  ao  and ßo are uniformly distributed in {0,1};
               •  Pr[a0 = ßo] - c;
               •  O.Q  and ßo  are independent given an ;
               •  ao  and ßo are independent given ß\ ;
               •  a i  and ßi  are independent given 0:2;
               •  a\  and ßi  are independent given Д2;
               •  ak- 1  and ßk- 1  are independent given ak',
               •  ak-i  and ßk-i  are independent given ßk',
               •  ak and ßk are independent.
             (Hint:  If this statement is true for some c,  it  is also true for c' —  (c2 + l)/2. 
          To show this,  we may apply the construction used to prove Theorem 217,  using 
          the c-construction for 7  and  Ö.  One should  correct  the distributions  of the pair 
          a,ß under the conditions 7  =  0,  Ö —  1  and 7  =  1,5  =  0 in  such  a way that  a 
          and ß become independent given 7 and given Ö.  Finally,  one should check that 
          every number between 1/2 and 1 can be obtained from some number in (1/2,5/8) 
          by several iterations of the function с и   (c2 + l)/2.  To prove the statement for 
          c < 1/ 2, we invert one of the variables (say, a).)
             316      Assume that  a  and ß  satisfy  the  statement  of the  previous  problem. 
          Consider the set  of pairs of binary strings x,y of length n where the frequencies 
          of all  pairs  follow  the  distribution  for  (a,ß).  Prove  that  a random  (maximally 
          complex)  pair  in  this  set  has  significant  (proportional  to  n)  mutual  information 
          but no extractable common information.  More precisely, for every г the inequality 
          K(z)
               ^ 0 (K(z I x) + K{z I y) + log n) holds (the constant in О-notation may depend 
          on the length к of the chain).
             The solution of Problem 315 can be generalized for non-binary alphabets and 
          non-uniform distributions.  There is a simple criterion to decide whether the random 
          variables a, ß  (their common distribution)  satisfy the statement  of Problem  315. 
          This happens if and only if one cannot permute the rows and columns of the matrix
                    11.4.  CONDITIONAL INDEPENDENCE AND COMMON  INFORMATION      365
           (that defines the common distribution) in such a way that a matrix of the form
           appears (here the two zeros stand for zero blocks).  In this case the strings obtained 
           by n trials of a and ß have no extractable common information.  On the other hand, 
           if such a permutation (that produces a matrix of this form) exists, some part of the 
           mutual information is extractable (indeed, both variables determine the number of 
           the block they are in).  This argument (see [112] for the details) gives an alternative 
           proof of a criterion first obtained by Gåcs and Körner [59].
                                               CHAPTER 12
                   Multisource algorithmic information theory
                               12.1.  Information transmission requests
                 Multisource information theory deals with information transmission in a net­
             work.  Such a network includes information sources (one or many), the destinations 
             (one or many) where information should be delivered, and channels that are used 
             for transmission;  some  (or all)  channels may have limited capacity.  The classical 
             Shannon approach considers sources as random variables and is well developed.  It 
             tries to find conditions that make some information transmission requests feasible.
                 Similar questions could (and should) be asked for algorithmic information the­
             ory.  Let us explain this setting more formally, following [179].  Consider a directed 
            graph whose edges are channels and nodes are processors.  Some nodes (called in­
            put nodes)  get outside information;  this information should be processed  (in the 
            nodes)  and  transmitted  (via the  edges)  into  some  other  nodes,  finally  reaching 
             output nodes.
                 More formally,  an  information transmission  request  consists of the following 
            parts:
                    •  a finite acyclic directed graph;
                    •  a set of input nodes;
                    •  an input string for each input node;
                    •  a set of output nodes;
                    •  a (desired)  output string for each output node;
                    •  a non-negative integer capacity for each edge (the value +oo is also allowed 
                      and means unlimited capacity).
            To fulfill  this request,  one should write on each edge e some string whose length 
            does not exceed the capacity of edge e, in such a way that
                                            C(X|Y1,...,yt)«0
            for every node г that has incoming strings Yf,...,     and for every outgoing string 
            X in this node.  Here by incoming strings for a node we mean the strings written on 
            incoming edges and the input string for the node (if it is an input node); similarly, 
            outgoing strings are strings written on outgoing edges,  and the output string for 
            this node (if it is an output node).
                 Informally speaking, this condition means that the nodes can only process the 
            incoming information and cannot create (a non-negligible amount of)  new infor­
            mation.  As usual, the approximate equality sign (~) means that the complexity in 
            question is 0(logN), where N is the total length of all input and output strings in 
            the request.  So in fact we consider not one request but a sequence of requests with 
            increasing values of N.
                                                     367
         368        12.  MULTISOURCE ALGORITHMIC INFORMATION THEORY
                                      1
                                       к I
                                      В
                 Figure 38.  The simplest information transmission request
            We are interested in the conditions that make a request (actually, a sequence 
         of requests) fulfillable.
            Consider a network that has two nodes and one edge (Figure 38).  Let us agree 
         that all edges are directed top-down, so the direction arrows are omitted.  The top 
         node is an input node and has input string A; the bottom node is an output node 
         and has output string B.  The channel has capacity k.
            In other words, for given strings A and В we are looking for a string X (trans­
         mitted message) such that
                        C(X\A)^0,  C(B |X)«0,  l(X)^k.
         Obviously, it is possible only if C(B \ A) ~ 0 and C(B) ^ к (the latter inequality is 
         understood also with logarithmic precision).  On the other hand, these conditions 
         are also sufficient because we may use the shortest description for B as X.
            To express this evident idea formally, we (unfortunately) need a rather obscure 
         statement.  Let An  and Bn  be sequences of strings,  and let kn  be  a sequence of 
         integers.  Assume  that  the  lengths  of An  and  Bn  as  well  as  the  integer  kn  are 
         bounded by a polynomial in n.  Then the following two properties are equivalent:
            (1)  there  exists  a  sequence  of strings  Xn  such  that  l(Xn)  ^  kn + O(logn), 
         C(Xn\An) = O(logn), and C(Bn\Xn) = O(logn);
            (2)  C(Bn I An) = O(logn) and C(Bn) O n  + O(logn).
            This equivalence follows from two (rather trivial) remarks.  First,
                    C(B I A) ^ C(B IX) + C(X I A) + О (log C(A, B,X)),
                       C(B) ^ l(X) + C(B IX) + О (log C(B IX)) 
         for all strings A,B,X (so (1) implies (2)).  Second,
                for every A, В and к there exists a string X such that 
                     l(X) ^ C(B),C(X\A) ^ C(B\A) + 0(\ogC(B)), 
                and C(B\X) = 0(1)
         (let X be the shortest program for B) and therefore (1) follows from (2).
            For the case A = B, the statement has clear intuitive meaning:  a string A can 
         be transmitted through a communication channel if and only if its complexity does 
         not exceed the capacity of the channel.
            Let us now switch to more interesting examples.
                            12.2.  Conditional encoding
            In the following request we want to transmit some string A assuming that both 
         the sender and the receiver know some string В  (Figure 39).  We need to encode 
         A by a k-bit string, send this string down, and then decode A back; both encoder
                            12.3.  CONDITIONAL CODES:  MUCHNIK’S THEOREM              369
                                                    В
                                              A
                                              к
                                              A
                       Figure 39.  Encoding and decoding A when В is known
            and  decoder have  access to  В  (the capacity  of the edges shown  as  solid  lines  is 
            unlimited, so we may assume without loss of generality that these edges carry the 
            entire string B).
               This request can be fulfilled if and only if C(A\B)  <  k.  Indeed, the decoder 
           knows В (or some derivative of B) and к additional bits, so it can generate A only 
           if C(A\B) ^ k.  On the other hand, if C(A\B) < к, then the shortest description 
           of A given В has at most к bits and can be sent over a restricted channel, while 
           two  other  unrestricted  channels  transmit  B.  Note  that  the  complexity  of this 
           shortest  description relative to the pair  (A, В)  is  bounded by a logarithm of the 
           total complexity of A and В, since knowing the length of this description we can 
           try all strings of this length in parallel until we find some description.
                317       The last  argument  shows  only  (but  this  is  enough  for  us)  that  some 
           shortest description has logarithmic complexity given A and B.  Prove that each of 
           them has logarithmic complexity since there is only 0 (1) of them.
               (Hint:  It can be proven in the same way as in Problem 40 on p. 40.)
                318  Give the exact statement of the last criterion  (for the network in Fig­
           ure 39) in terms of sequences A& and Вк, and prove this statement.
                          12.3.  Conditional codes:  Muchnik’s theorem
               This section  is  devoted  to  a remarkable  result  of Muchnik  [135].  It  can  be 
           considered as an algorithmic counterpart of a well-known Slepian-Wolf theorem in 
           Shannon information theory.  In the language of the previous section,  Muchnik’s 
           result  says  that  one  does  not  need  to  use  В  while  encoding  A  (Figure  39),  and 
           this edge may be deleted (Figure 40), and the condition when the request can be 
           fulfilled remains the same.
               This condition is C(A\B)  <  k,  and it remains necessary for obvious reasons 
           (the graph is smaller).  It remains sufficient too; here is the exact statement.
                                             A
                                              к
                                             A
                  FIGURE 40.  Sending A when decoder knows В:  Muchnik’s theorem
        370      12.  MULTISOURCE ALGORITHMIC INFORMATION THEORY
          Theorem 229.  Let A and В be arbitrary strings of complexity at most n.  Then 
        there exists a string X  of length at most C(A\B) + O(logn)  such that C(X\ A) = 
        O(logn)  and C(A\B,X) = O(logn).
          The hidden constant in O(logn) does not depend on n, A, B.
          This statement can be reformulated:  For every A and В there exists a program 
        that transforms В to A, has logarithmic complexity given A, and has unconditional 
        complexity C(A | B) (up to logarithmic precision).  In other words, the additional re­
        striction saying that the program should be simple relative to A, increases the min­
        imal possible (unconditional) complexity of the program only by 0(logC(A, B)).
          P r o o f.  Assume that the string A has complexity a.  Replace A by its (shortest) 
        description of length a.  This replacement changes the values of C(A \ В), C(X | A), 
        and C(A\B,X) only by O(logn).  So we may assume without loss of generality 
        that A has length a.  (Complexity of A remains close to a, but this does not matter 
        for us.)
          Assume that the conditional complexity C(A\B) equals m.  The idea of the 
        proof can be explained as follows.  Consider some hash function x '■ ®a ~that 
        computes an m-bit hash value (fingerprint) for every а-bit string.
          For a given string В we have about 2m strings Z of length a such that C(Z \ В) ^
        m.  Let Sß  C Ba be the set of these strings.  According to our assumption, A is one 
        of the elements of Sß.
          Imagine that we are extremely lucky, and all the strings in Sß  have different 
        hash values.  Then every string P £ Sß can be uniquely reconstructed if we know 
        x(P)  and В  (the function x 1S assumed to be fixed).  So we can use x{A)  as X 
        in the statement of the theorem.  It has correct length, is simple relative to A (we 
        assume that x 1S simple),  and,  together with B,  allows us to reconstruct  A—we 
        have to enumerate Sß until we find a string with the correct hash value.
          Of course this is too good to be true.  For every hash function x,  if a > m, 
        there are at least 2a~m strings that have the same hash value (and for simple x we 
        can find many simple strings with the same hash values, and they will be in Sß for 
        every В), so we cannot hope to be so lucky.
          We need to modify our plan and consider for every Z £ Ma several (poly(n)) 
        hash values instead of one.  Instead of a hash function, we consider now a bipartite 
        graph £cBax Bm where each left vertex Z has at most poly(n) right neighbors. 
        These neighbors are called fingerprints of Z.
          Proving the theorem, we look for X among the fingerprints of A.  This guar­
        antees that  C(X\A) —  O(logn),  assuming that graph E is simple  (has O(logn) 
        complexity).  Indeed,  to specify X when A is known,  it  is enough to specify the 
        ordinal number of X among the fingerprints of A.
          If, for a given A £ Sß, one of its fingerprints X determines A inside Sß uniquely 
        (no other strings in Sß have X among their fingerprints), then we can reconstruct 
        A by enumerating Sß and waiting for a string that has X among its fingerprints. 
        In  this  case  C(A\B,X)  =  O(logn);  we  assume  here that  E  is simple,  i.e.,  has 
        complexity O(logn).
          Moreover,  the same complexity bound holds if there are polynomially many 
        (in  n)  strings  in  Sß  that  have  X  among their  fingerprints.  We  need to specify 
        additionally  the  ordinal  number  of A  among  these  strings  (in  the  order  of the 
        enumeration of Sß), and this requires additionally 0 (logn)-bits.
                            12.3.  CONDITIONAL CODES:  MUCHNIK’S THEOREM              371
                So  our  goal  is  now  that  A has a right  neighbor  that has few  (polynomially 
            many)  left neighbors.  Here, speaking about neighbors, we consider the restriction 
            of E onto    as a bipartite graph with left part Sb and right part ®m.  Note that 
            this restricted graph has about 2m vertices both in the left and in the right parts.
               In other words,  a vertex in the right part is bad if it has many neighbors on 
            the left;  a vertex in the left part is bad if all its right neighbors are bad.  We need 
            A to be not bad.  How can we achieve this?
               The fraction of bad right vertices is small:  The number of edges is bounded by 
            the size of the left part times the maximal degree of left vertices, and each bad right 
            vertex creates a lot of edges.  More precisely, we can ensure that this fraction is at 
            most  l/p{n) for a given polynomial p (the threshold for the number of neighbors 
            for bad vertices is determined by p and increases as p increases).
               We want to prove then that the fraction of bad left vertices is also small.  To 
            achieve this, we assume that E has the following expander-like property:  For every 
            subset T  in the  left part,  the set  of all right neighbors  of T-vertices  is  at least as 
            big  as  T  itself  Having  this  property,  we  consider  the  set  T  of bad  vertices  in 
            the left part;  all their right neighbors are bad by definition, so the number of left 
            bad vertices is bounded by the number of the right bad vertices.  (Note that this 
           expander property of E remains valid if E is restricted on Sb-)
               It remains to explain where we get a (simple) graph E with this property and 
           what we do if A falls into a (small) set of bad vertices.
               It is well known that the existence of expander-like graphs can be usually proved 
           by a rather simple probabilistic arguments.  To construct them explicitly is quite a 
           different story; it is a very interesting and complicated business where great progress 
           has been made during the last decades.  However, we can avoid these complications 
           using the following simple trick:  After the existence of a graph with some property 
           is proven, we can generate all the graphs until we find some with this property.  We 
           assume that the property is decidable, so this is an algorithmic procedure (which 
           takes  a  long  time,  but  we  have  no  time  bounds).  The  first  suitable  graph  has 
           logarithmic complexity, since to organize the search we need to know only the sizes 
           of sets (we may assume the sizes are powers of 2).  In this way we convert the pure 
           existence proof into a simple object with the property we need.
               The last bit of the proof is why A cannot be bad.  Note that bad right strings can 
           be enumerated effectively when В (as well as E) is known.  As we find new elements 
           in Sb, we generate new left vertices in the restricted graph and new bad strings in 
           the right part.  Since E is known, we can also enumerate left bad strings.  So every 
           bad string can be specified (given В) by its ordinal number in the enumeration of 
           bad left strings, and the number of these strings is significantly less than 2m.  And 
           we assumed that the complexity of A given В is m, this was our definition of m. 
           So A cannot be bad.  (Also we need 0(logn)-bits to specify m, a, and E, but, as 
           we will see, the gap is enough to get a contradiction.)
               We have described the proof in the top-down mode.  Now let us describe the 
           details of the  argument  in the bottom-up direction.  First,  we need  an existence 
           proof for expander-like graphs.
               Lemma.  Let a and m be positive integers,  and let a ^ m.  Then there exists a 
            bipartite graph E сШа x ®m  where each vertex in the left side has degree at most 
           a + m + 2,  with the following property.  For every set T e l“ with at most 2m~1
        372      12.  MULTISOURCE ALGORITHMIC INFORMATION THEORY
        elements, the set E(T)  of all neighbors of all elements in T has more elements than 
        T itself.
          PROOF.  Let  us  prove  that  a random  graph  has  this  property  with  positive 
        probability.  Speaking about random graph, we mean that for every left vertex we 
        randomly select a+m+2 neighbors on the right.  This choice is made independently 
        (for different points and for different neighbors, so some neighbors of a given vertex 
        may coincide, and we actually get fewer than a + m + 2 neighbors).
          What does it mean that the property stated in the lemma is false?  In this case 
        we have some non-empty subset T of the left part and some subset U of the right 
        part, such that \U\ — |T|, but all neighbors of T-elements belong to U.  We compute 
        the probability of this event for fixed T and U and then show that the sum of these 
        probabilities over all T and U is less than 1.
          Assume that T and U are fixed, and let t be their cardinality.  By assumption, 
        t  <  2rn~1, so the probability for a random right vertex to get inside U is at most 
        1/ 2.  The probability that this happens t(a + m + 2) times for t elements of T (we 
        make a + m + 2 trials for each vertex in T) is at most 2~t('a+Tn+2K
          Let us sum these probabilities first over all pairs of T and U of given size t.  The 
        number of different T is at most  (2°)*  (we select one of 2° elements t times,  and 
        the possible coincidence and permutations make the number of sets even smaller); 
        the number of different U is at most (2Tn)t.  So the sum over sets of size t does not 
        exceed
                        turrit  _ 2 at . 2-*(ш+а+2)  __(1/ 4)*
        It remains to note the the sum XX1/4)*  (over all t ^  1) is less than 1  (it is equal 
        to 1/3).  The lemma is proven.
          Denote by ЕШ)а the first  (in some natural order)  graph that satisfies the re­
        quirements of this lemma.  Its complexity is at most 2 log а + 0(1)  (it is enough to 
        specify a and m using two halves of a 2 log а-bit string).
          For a given string В and for given m and a, consider the set Sb of а-bit strings 
        that have complexity at most m with condition B.  Consider the restriction of the 
        graph Em,a to Sb-  We get a graph with at most 2m+1 • (a + m + 2 ) < а2ш+3 edges 
        (each of at most 2m+1  left vertices has at most a + m + 2 neighbors).  Call a right 
        vertex bad if it has at least a4 neighbors.  Then the number of bad right vertices is 
        at most к = 2ш+3/а3.
          A left vertex (a string in Sb) is now called bad if all its right neighbors are bad. 
        The properties of Em^a guarantee that number of bad left vertices also is bounded 
        by к — 2171+3/a3.  Indeed, we may assume that a is large enough and к < 2Tn~1.  If 
        there are k + 1 bad left vertices, then the expander property of the graph guarantees 
        that the number of their neighbors is at least к + 1.  All they are bad (due to the 
        definition of bad left vertex).
          The bad left  vertices  can be enumerated  (given m,  a  and  B),  so  every bad 
        string can be specified by giving its ordinal number in the enumeration,  i.e.,  by 
        m — 31oga + 0 (1) bits, so
                      C(P | B, m, a) < m — 3 log a + 0(1)
        for every bad string P.  We see that the gap is larger than 2 log a needed to specify 
        m and a, so all the bad strings have conditional complexity (relative to B) less than 
        m, and A cannot appear among them.
                       12.4.  COMBINATORIAL INTERPRETATION OF MUCHNIK’S THEOREM                 373
                 Therefore, A has a good (not bad) right neighbor X.  Then
                                        C{X\A)      (3 + e) log a + 0(1)
             (to specify X we provide m, a, and the ordinal number of X among the neighbors 
             of A;  we need some e > 0 to take care of pair encoding).  The length of X is m, 
             i.e.,  C(A\B).  Finally, C{A\B,X) does not exceed (6 + e)loga (we need 4 log a to 
             specify the ordinal number of X among a4 left neighbors of X, and 2 log a is needed 
             to specify m and a; again we use some e to cover pair encoding).
                 The Muchnik theorem is proven.                                                  □
                  319        Show  that  we  have  proven  a  bit  stronger  property  that  we  claimed: 
             We assumed that the complexity of В is bounded by n,  but we never used this 
             assumption.
                 Let  us  recall  that  all  the  inequalities  in  Muchnik’s  theorem  are  true  with 
             0(logn)-precision,  where  n  is  the  maximal  complexity  of  A  and  B.   Can  we 
             strengthen the claim by requiring 0 (logm)-precision for m equal to maximal con­
             ditional complexities C(A\B) and C(B\A)?  It turns out that this is not possible; 
             see [205, Section 5]  (we do not reproduce the proof here).
                     12.4.  Combinatorial interpretation of Muchnik’s theorem
                 Many results about Kolmogorov complexity have some combinatorial counter­
             part, an equivalent statement of purely combinatorial nature that does not mention 
             Kolmogorov complexity.  In many cases this statement is about the existence of a 
             winning strategy in some game.  (See [132] about this effect in general computabil­
             ity theory;  [136] considers the special case of Kolmogorov complexity.)
                 Theorem  229  (proven  in  the  previous  section)  also  has  some  combinatorial 
             counterpart.  For given values of a,  b, m  (we assume that m ^ a), we consider a 
             two-player game.  The players are called Mathematician (M) and Adversary (A). 
             The game also has some parameter c (that corresponds to the constant in O(logn)- 
             notation; see below).
                 M may select for every а-bit string A at most c(a + b)c strings of length m; she 
             declares these strings as  “simple relative to A”.  Also for each pair of strings В  (of 
             length b) and X (of length m) she may select at most c(a + b)c strings of length a 
             and declare them as “simple relative to В, X”.
                 A may select for each fe-bit string В at most 2Tn strings of length a and declare 
             them as “simple relative to В”.
                 Each player  may  make the  next  move  (i.e.,  declare  more strings  as  simple) 
             at  any moment  (whatever the opponent does), seeing the moves of the opponent 
             made earlier.  We get a game that is essentially finite—the declared strings cannot 
             be taken back,  so the game reaches some limit  position.  However,  watching the 
             game,  we may not know whether this limit position is reached, since the players 
             keep right to make moves even if they do not exercise this right.
                 The limit position of the game determines the winner as follows:
                      M wins if for every string В of length b and for every string A 
                      of length a declared (by A) simple relative to H, there exists a 
                      string X of length m that is declared (by M) simple relative to 
                      A,  such that  A  is  declared  (again by  M)  simple relative to  В 
                      and X.
       374      12.  MULTISOURCE  ALGORITHMIC INFORMATION THEORY
          Now we can formulate a combinatorial equivalent of Theorem 229.
          Theorem  230.  There exists  a constant c such that for every positive integer 
       a, b, m with m ^ a the described game with parameters a, b, m, and c has a winning 
       strategy for M.
          Let us show that this combinatorial statement is indeed equivalent to Theo­
       rem 229.  Assume that it is true for some c.  Consider a blind adversary that does 
       not look at M’s moves and for each В declares as simple all strings of length a that 
       have conditional complexity (given В) less than m.  This behavior is algorithmic, 
       and the algorithm is determined by a, b, m.  The winning strategy for M can be 
       found by a brute-force search (the game is essentially finite,  and we assume that 
       such  a strategy  exists),  so  we  may  assume  that  the  winning  strategy  is  simple. 
       So  the strings  declared by  M  as simple  are  indeed simple,  i.e.,  they  have  small 
       (conditional)  complexity.  Indeed, to specify such a string, one may specify its or­
       dinal  number in the list  of strings  declared simple  (for  a given  condition),  using 
       loge + clog(a + b)  bits,  and  also  specify  a,  6,  m  (additional  0 (log(a + b))  bits). 
       So we get the statement of Theorem 229.  (A technical comment:  The factor c in 
       c(a + b)c is needed for small values of c and corresponds to the 0 (l)-term in the 
       complexity bounds that should be added to О (logn) for the case when n — 1 and 
       logn — 0.)
          In the other direction, assume that the statement of Theorem 229 is true with 
       some constant  d  in  О (logn).  We want to prove that  for sufficiently large c the 
       combinatorial statement is true.  Assume it is not the case and for every c there 
       exist a, b, m for which A can win the game.  This strategy (together with a, b, c) can 
       be found by a search.  So if this strategy declares some A as simple with respect 
       to  B,  then indeed the conditional complexity C(A\B)  is small—it is bounded by 
       m + 0(C(c)).  We get a contradiction if A plays this strategy against the following 
       blind strategy for M:  Declare X as simple for A if C(X\ A) < clog(a + b) + loge, 
       and declare A simple for В, X if C(A \ В, X) < clog(a + b) + loge.
          Playing this strategy, M does not violate the quantitative restrictions (on the 
       number of simple strings).  To get  the desired contradiction,  it  remains  to  show 
       that  M wins the game.  Let A, В be strings of lengths a,  b,  and assume that A 
       is  declared simple for В  (by A).  Theorem 229 says that there exists a string X' 
       of length C(A\B) + c'log(a + b) for which the statement of that theorem is true. 
       Since C(A IB) is bounded by m + 0(C'(c)), the string X' is only slightly longer that 
       m.  Let X be the first m bits of X'.  The complexities C{X'\A) and C(A\B,X') 
       are  small  as  Theorem  229  says,  and  the  number  of discarded  bits  is  also  small. 
       Therefore the  complexities  C(X\A)  and  C(A | В, X)  are  also  small,  and for the 
       right choice of c they are less than clog(a + b) + loge, so M wins.
          Let is provide the necessary bounds.  The conditional complexity C(X \ A) is at 
       most
                         d log(a + b) + O(logm)
       (the complexity of X'  given A plus the length of the prefix-free encoding of m). 
       The conditional complexity C(A\B,X) does not exceed the sum
                         d log(a + b) + 0(C(c))
       (the complexity of A given В, X plus the length of the discarded suffix of X').  Now 
       we see that one can choose c of the form 2г in such a way that both sums do not
                                    12.5.  A  DIGRESSION:  ON-LINE  MATCHING                    375
             exceed clog(a + b) + loge (since c is a power of 2, the complexity of c that appears 
             in the upper bound for C(A\B, X) is much smaller than loge).
                 So we have shown that the combinatorial statement of Theorem 230 is indeed 
             equivalent to its complexity counterpart, Theorem 229 (and is true, since we proved 
             the latter).
                 In fact, the combinatorial translation is possible not only for the statement of 
             Theorem 229, but also for its proof.  During the game, M does not exercise her right 
             to declare new Л-simple strings during the game; she declares all the neighbors of 
                (according to the expander graph) at once.  Then a string A is declared simple 
             A
             given X, В if (1)  A is a neighbor of X,  (2)  A has been declared simple given В 
             by A,  (3) at the moment of this declaration the number of neighbors of X among 
             the strings that  are already declared simple given В  is small.  In this way M  is 
             able to serve,  for each B,  most of the strings that are declared simple for this B. 
             The remaining strings  (a small fraction of all strings declared simple for  В)  are 
             forwarded to the next level of service where the same strategy is used, but for m 
             that is smaller by 1, and so on.  Finally the number of strings declared simple for 
             A is bounded by a sum of a sequence where each term is twice smaller than the 
             previous one, so the number of simple strings is multiplied by 2 (not a problem).
                 The ability to declare all the strings at once also has some algorithmic conse­
             quences:
                  320       Prove that for every string A of length n there exists a string X of length 
             C(A), such that C(A\ X) = O(logn), and total conditional complexity of X given 
             A (i.e., the minimal complexity of a total program that maps A to X) is O(logn). 
             Show that one cannot replace both the complexities C(A | X) and C(X \ A) by total 
             conditional complexities.
                 (Hint:  To prove the second part, one may use the existence of non-stochastic 
             strings; see Chapter 14 about algorithmic statistics.)
                 This problem shows that Muchnik’s argument gives us something non-trivial 
             even for empty B.  For non-empty В we can get a version of Muchnik’s theorem 
             where C(X\A) is replaced by total conditional complexity (and other conditional 
             complexities are understood in the usual way);  one needs to assume additionally 
             that the length of A (not only its complexity) does not exceed n.
                                 12.5.  A digression:  On-line matching
                 In  this  section  we  modify  the  combinatorial  proof of Theorem  229  to  get  a 
             stronger (and simpler) combinatorial statement.
                 Consider some bipartite graph E С A x В with left part A and right part B. 
             Given a subset A'  c  A,  one can look for a matching that selects for each vertex 
             a £ A1 some 5-neighbor in such a way that no vertices in В are selected twice.  In 
             other words, one can try to find a bijection defined on A', and this bijection should 
             be a subset of E.  One can consider an on-line version of the same task.  Assume 
             that the vertices in A are given one by one, and we need to select a neighbor for 
             the next vertex not knowing which vertices follow.
                 More formally, we say that the graph E С A x В allows on-line matching of size 
             к if there is a strategy that selects distinct neighbors for к vertices in A provided 
             sequentially by the adversary.  (The game consists of к moves:  at each move the 
             adversary select a vertex in A not used before.  In response, we have to select some 
             5-neighbor of this vertex.  We win if all the selected neighbors are different.)
             376           12.  MULTISOURCE ALGORITHMIC INFORMATION THEORY
                 Note that  this  definition  is  non-symmetric—the  adversary selects  A-vert ices 
             and we have to select 5-vertices.  It is clear from the definition that the property 
             of the  graph  “to  allow  on-line  matching  of size  к”  is  in  PSPACE,  i.e.,  can  be 
             checked in polynomial space.  (It would be interesting to get better upper bounds 
             for complexity or some lower bounds.)
                 Here is the result about on-line matching that somehow explains the combina­
             torial root for Muchnik’s theorem.
                 Theorem 231.  For some constant c and for all a and m such that a ^ m, there 
             exists a bipartite graph with 2 a vertices in the left part and 2mac vertices in the right 
            part and degree at most ac for every left vertex that allows on-line matching of size 
             2m.
                 In  other  words,  one  can  find  a  graph  with  arbitrary  sizes  of left  and  right 
            parts that has small (polynomial) left degree and allows on-line matching of almost 
            maximal size (close to the size of the right part, up to a polynomial factor).
                 Before proving Theorem 231,  let us explain how it implies Theorem 229.  As 
            before,  let  us  replace  A  by  its  shortest  description  of length a.  Let  m  be equal 
            to  C(A\B) + 1.  Theorem 231 then guarantees the existence of a bipartite graph 
            with 2a left vertices and ac2m right vertices and left degree at most ac that allows 
            on-line matching of size 2m.  This is a computable property, so the first graph (in 
            some order)  with this property has small complexity (logarithmic in a).  Fix this 
            graph and some computable strategy that wins the on-line matching game.  For a 
            given 5, let us enumerate a-bit strings that have conditional complexity less than 
            m given B.  There are at most 2m of them, and A is among them.  Applying the 
            matching strategy, we find some right neighbor for each of these strings, including 
            A.  Now let X be the selected neighbor of A.  This is the string we looked for. 
            Indeed, X (as well as all right neighbors of A) has small complexity given A, since 
            A has only few neighbors.  On the other hand, knowing В and X (as well as a and 
            m that determine the graph and the strategy)  we can find A—start the process 
            described above and wait until X is assigned to some vertex.
                 PROOF.  Now let us prove Theorem 231.  It is enough to prove a weaker state­
            ment that allows the matching strategy to skip some elements (whichever it wants), 
            but not more than half of them (at most 2m~1  elements).  Indeed, if such a weak 
            matching strategy exists, we can start a similar process for skipped elements for­
            warding them to another matching strategy (with m decreased by 1 and a corre­
            sponding graph); the elements skipped by this strategy are forwarded to the third 
            one,  etc.  In this way we get  a full matching in the graph whose left part is the 
            same as before, and the right part is a disjoint union of the right parts of all used 
            graphs (for m, m — 1, m — 2,... up to zero, where the matching task is trivial).
                It  remains  to  note  that  this  weaker property  is  guaranteed  if the graph has 
            the  expansion property used  in the proof of Muchnik’s  theorem.  The matching 
            algorithm is straightforward:  If a vertex has neighbors not used before, select one 
            of them; if not, skip the vertex.  Let us show that this strategy serves at least half 
            of the vertices (at least 2m~l ones).  If it serves less, then at the right part we have 
            used less than 2m~1  vertices.  On the other hand,  for each skipped vertex all its 
            neighbors are used (this was the reason to skip it).  So we get more than 2m~1  left 
            vertices  (skipped ones)  whose neighbors are all in the set of used right vertices of 
            size less than 2m~1.  This is impossible due to the expansion property.            □
                       12.6.  INFORMATION  DISTANCE AND  SIMULTANEOUS  ENCODING        377
                It  would  be interesting to compare this argument with the proof of Slcpian- 
            Wolf theorem and find some  “common denominator”,  a combinatorial fact  that 
            implies both Muchnik and Slepian-Wolf theorems.  It would be also interesting to 
            find another proof of Theorem 231  (e.g., a direct probabilistic argument, or, even 
            better, some explicit construction of the graph).
                One can also  prove  Muchnik’s  theorem  using  another  famous  combinatorial 
            tool,  randomness extractors.  This idea was suggested  (in a different context and 
            before Muchnik’s paper) in [23], and it was applied to Muchnik’s theorem in [145]. 
            The advantage of this  approach  is that  one  can  use  known  explicit  randomness 
            extractors  and  other  known  techniques  (e.g.,  pseudo-randomness  generators,  as 
            suggested by Romashchenko) to prove the space-bounded version of Muchnik’s the­
            orem [143,  144].  (Let us mention that resource-bounded Kolmogorov complexity 
            and its relations with computational complexity theory is an important topic that 
            is outside the scope of our book.)
                     12.6.  Information distance and simultaneous encoding
               Now we consider a request  (Figure 41) where the capacity of the dotted line 
            is bounded by k, and the k-bit string transmitted along this channel must contain 
            enough information to transform A to В and vice versa.
                Obviously, it is possible only if C{A\ В) < к and C(B | A) ^ к (with logarithmic 
            precision, as in all our considerations).  Indeed, the left output node receives A (or 
            some string derived from A) and X (or some string derived from X ) and produces 
            B, so C(B\ A) ^ k.  A symmetric argument shows that C(A\B) < k.
               We get a necessary condition for the feasibility of this request:
                                     max(C(A\B),C(B\A))  <   A;
            (as  usual,  logarithmic  terms  are  omitted).  It  was  shown  by  Bennett,  Gacs,  Li, 
            Vitånyi, and Zurek [9] that this necessary condition is at the same time sufficient. 
            Here is the exact statement of their result:
               Theorem 232.  Let A, В be strings such that C{A\B) < к and C(B\A) < k. 
            Then there exists a string X of length к such that
              C{A\B,X) = 0{logk),  C{B\A,X) = O(logfc),  and  C(X\A, B) — 0(\ogk).
               Proof.  Consider  all  pairs  (A,B)  of  strings  such  that  C(A\B)  <  к  and 
            C(B I A) < к at the same time.  We get an enumerable binary relation on strings; 
            all its vertical  (A is fixed)  and horizontal  (В is fixed) sections contain at most 2k
                   Figure 41.  Bennett-Gacs-Li-Vitanyi-Zurek information request
             378            12.  MULTISOURCE  ALGORITHMIC  INFORMATION THEORY
             elements.  In other words, we get an (infinite) bipartite graph, and the degree of all 
             vertices (in both parts) is bounded by 2 k.
                 Now we want to split all the pairs into 2k+1  (or fewer) classes in such a way 
             that each class is a one-to-one correspondence (it does not have pairs on the same 
             vertical or the same horizontal line).  In graph terms, we color all the edges using 
             2 k+ 1  colors in such a way that every two edges that have a common endpoint have 
             different colors.
                 This is easy—each new edge is colored by a first  (in some order)  color that 
             was not used for other edges with common endpoints.  Since fewer than 2k edges 
             may share each of the endpoints, we always have a free color.  (In other words, we 
             number the classes by 0 • • • 2 k+l  — 1,  and  each new pair is put  in  a class that  is 
             allowed, i.e., it does not contain pairs with the same first or second coordinates.)
                 We described an infinite process that depends on к but not on A, B.  At some 
             moment it assigns some color (class number) to the pair  (A, В)  (edge A-В).  Let 
             X be this color (=its number in binary).  It has at most к + 1 bits instead of к but 
             one additional bit does not matter with our precision.  Knowing A, k,  and X, we 
             can find В:  run the process described,  and wait for an edge starting at A that is 
             colored by X ; its other endpoint is B.  For the same reason, knowing B, k, and X, 
             we can compute A.  Finally, C(X | A, B, k) = 0(1):  knowing k, A, and B, we wait 
             until the edge A-В gets some color; this color is A.                                 □
                  321        Prove a stronger statement about bipartite graphs.  Assume that a finite 
             bipartite graph is given,  and all vertices  (in both parts)  have degree at  most  N. 
             Prove that one can color its edges using N colors in such a way that the edges with 
             common endpoint have different colors.  Why can we not use this fact in the proof 
             of Theorem 232 (and need to prove a weaker version from scratch)?
                 (Hint:  We may assume that the degree is exactly N, and then apply the Ford- 
             Fulkerson  argument  (max-flow=min-cut)  or the Hall theorem.  However,  all this 
             does not help us since we obtain the graph edges sequentially and have to assign 
             colors on-line.)
                 One may also note that the color in the last proof can be encoded by a (k + 1)- 
             bit string, so it determines k, and we do not need to specify к separately.  So the 
             complexities C(A\B, X)  and  C(B\A,X)  are in fact 0(1),  not  0(\ogk).  To put 
             the same observation in programming terms, for every two strings A and В there 
             exists a program of complexity max(C(A | B), C(B \ A)) + 0(1) that maps A to В 
             and В to A.  Indeed, consider a program that knows the color of the edge A-В (it 
             is used as a constant in the program) and waits for an edge having this color and 
             being incident to the input vertex.  (Note that a logarithmic amount of information 
             is  enough to distinguish  A  and  В;  e.g.,  one may specify the number of positions 
             where A and В differ.)
                  322        Let A and В be two independent random n-bit strings (i.e., C(A) « n, 
             C(B)  æ  n,  and  C((A,B))  æ  2n).  Give  an  explicit  example of a string X  that 
             satisfies the requirements of Theorem 232.
                 (Answer:  Take the bit-wise XOR of A and В.)
                 For the case when the complexity C(A \ В) and C(B \ A) are different, the fol­
             lowing refinement of Theorem 232 is possible.  Assume, for example, that C( A \ B) 
             is bigger.  Then the string X can be split into two parts:  the first part is the infor­
             mation needed to transform A into В, it has length C(B\A); the rest has length
                             12.7.  CONDITIONAL  CODES  FOR TWO  CONDITIONS              379
            C(A\B) — C(B\A),  and together with the first  part  makes possible  the reverse 
            transformation.
                Here is the formal statement:
                Theorem 233.  Assume that C(A \B) < k, C(B\ A) < I, and к > I.  Then there 
            exists a k-bit string X  such that C(X\A,B) — 0(\ogk), C(A\B,X)  = O(logZc), 
            and C(B\A,X') = O(logfc)  where X' is an l-bit prefix of X.
                P r o o f.  Let  us  use  the same trick with  2l+1  colors,  still  requiring that  each 
            left  vertex has different colors of adjacent edges,  but on the right side up to 2 k~l 
            adjacent edges of the same color are allowed.  Then the color X of edge A-В allows 
            us to find В given A, and in the other direction we need the color plus the number of 
            edge A-В in the enumeration of all £?-edges of color X (as they are generated).  □
                 323       Prove the statement of Theorem 233 in the form used in [9]:  Under the 
            assumptions of Theorem 233, there exist a string Y of length к — I and a string X 
            of length I such that C(B,Y \ A,X) = 0(\ogk) and C(A\B,Y, X) = 0(logk).
                            12.7.  Conditional codes for two conditions
                Let us now consider an information request that in some sense generalizes the 
            two last examples (Figure 42).
                For the case A = В we get the request from Section  12.3  (in two symmetric 
            copies).  If we let C — (A, В), we get essentially the same request as in Section 12.6 
            (in each of the output vertices one string is known, and to restore the pair means 
            to restore the other one).
                It is easy to state the necessary conditions for this request to be fulfillable:
                                     C{C\A) < к,     C(C\B) < k.
            Muchnik [135] has shown that these conditions in fact are also sufficient.  Here is 
            the exact statement:
                Theorem 234.  Let A, B, and C be arbitrary strings of complexity at most n, 
            and let к be a positive integer such that C(C\ A) < к and C(C\B) ^ k.  Then there 
            exists  a string X  of length at most к + O(logn)  such that C(X\C)  =  O(logn), 
            C(C\A,X) = O(logn),  and C{C\B,X) = O(logn).
                In programming terms this statement can be rephrased as follows.  For every 
            three strings А, В, C there exists a program of complexity at most
                                   max(C(C I A), C{C \ B)) + O(logn)
                                           A              В
                                                 c
                                                 к
                                           C♦            ♦C
                            Figure 42.  Restoring C when A ox В are given
                12.  MULTISOURCE  ALGORITHMIC  INFORMATION THEORY
        380
                Figure 43.  Additional edges that do not help much
       that is (logarithmically) simple given (7, and it maps each of the two strings A and 
       В to C.  (As before,  the program lias some additional part that distinguishes A 
       from B; this part has logarithmic complexity.)
          Note  that  this  statement  remains  non-trivial  even  if we  do  not  require  the 
       program be simple given C:  no other proof even of this weak version is known.  In 
       other words, the problem does not look simpler if we add two additional edges, as 
       shown in Figure 43.
          As before, we can also refine the statement of Theorem 234 for the case when 
       conditional complexities are different:
          Theorem 235.  Let A, B,  and C be strings of complexity at most n,  and-let 
       к ^ I be positive integers such that C(C\ A) ^ к and C(C\B) ^ I.  Then there exists 
       a string X of length к such that C{X\C) = O(logn), C(C\A, X) = O(logn),  and 
       C(C\B,X') = O(logn), where X' is the l-bit prefix of X.
          324 How can one formulate this  statement  in  terms  of some  information
       request?
          (Hint:  X is sent  along an edge of capacity k,  and another edge of capacity I 
       extends the first edge.)
          All these statements are proven by Muchnik [135].  We reproduce the proof of 
       Theorem 235 given in his paper.
          Proof.  We use the same idea:  The string X is one of the (few)  “fingerprints” 
       of C.  However, the argument needs to be changed.  Even for the simple case к = I, 
       we have a problem.  One can find the fingerprint X such that C(C\ A, X) is small, 
       as well as some other fingerprint X' such that C(C\B,X') is small, but we need 
       the same X for both cases.  How can we achieve this?
          We may consider fingerprints X that generate only few collisions both in Sa 
       and Sß  (here Sa  and Sb stand for the set of strings that are simple relative to A 
       and B, respectively).  Indeed those universal fingerprints exist  (most of the right 
       vertices have this property, since Sa U Sb is only twice bigger than each of Sa and 
       Sb)-  The expansion property now guarantees that for most strings in Sa  and for 
       most strings  in  Sb  there  exists  a universal  fingerprint.  But  then  we  run  into  a 
       problem.  We would like to say that the remaining strings have small complexity 
       since  there  are only a few of them,  and we can generate them—but to generate 
       them we need to know both A and B, and we have only one of these two strings as 
       a condition....
          What can we do?  Let us consider A and В separately, but let us require that 
       C has not only one good fingerprint  (neighbor) but that most of the neighbors of
                                                                                                            12.7.  CONDITIONAL CODES FOR TWO  CONDITIONS                                                                                                                                                                                          381
                                            C are good.  If this can be achieved for A and В separately, then some fingerprint 
                                            will be good simultaneously for A and B.
                                                           So we need to change many things, starting from the expander-type property 
                                            we use.  Let E с P x Q be a bipartite graph with left part P and right part Q. 
                                            Now we require that for every small enough U  C  Q, the set of x G  P such that 
                                            most neighbors of x are in U is small.  In other words,  in our previous argument 
                                            a vertex in P was bad for us if all its neighbors are bad in Q\ now it is enough if 
                                            most neighbors are bad in Q.
                                                           Moreover, to adapt our argument to the case when C(C\A) and C(C\B) are 
                                            different, we consider not only fingerprints but also their prefixes.  So the statement 
                                            about the existence of an expander-like graph is now as follows (by [u)m we denote 
                                            the m-bit prefix of и) :
                                                           Lemma.  Let n  and N  be positive integers,  and let e  >  0  be a real number. 
                                            Assume that
                                                                                                                                                                n2N+2n+l£N/2  <   !
                                             Then there exists a family of N mappings
                                                                                                                                                            Xi,...,XN:Bn ^® n
                                            with the following property.  For every m  G {1,... ,n}  and for every non-empty 
                                            subset U C В771  of size at most e2m,  the number of x G ®n  such that
                                                                                                           [Xi(x)]m € U  for at least half of i G {1,..., N} 
                                            is less than \U\  (the cardinality ofU).
                                                           (Some comments:  Instead of a graph with left-degree N, we consider a family 
                                           of N mappings, so we allow multiple edges (xi(x) — Xj(x)  for i ф j).  Now both 
                                           arguments and values of Xi have the same size n, but the statement speaks about 
                                           m-bit prefixes for all m ^ n.)
                                                           Proof.  As usual,  let  us consider randomly chosen Xi> •••■>Xn  (for all i  and 
                                           x the values Xi(x)  are independent and uniformly distributed)  and show that the 
                                           probability of violating the statement is less than  1.  Let us get an upper bound 
                                           for this probability.  For each m ^  n, for each t  < e2m,  and for each pair of sets 
                                           T C l"   and U C Bm both having cardinality t, we consider the following event. 
                                            For each x G T, at least half of the values [Xi{x)\m  (for i — 1,... ,n)  are in U.  We 
                                           need an upper bound for the probability of this event.
                                                          For a fixed iG T  the probability of the event  “at least half of x-neighbors is 
                                           in  U”  is  at  most  2NeN^2:  There are at most  2N  subsets X  C  {1,2,..., AT}  that 
                                           contain at least N/2 elements, and for each X the probability that all Xi(x) axe in 
                                            U (for all i  G  X)  is  at  most eNl2 .  (Recall that  [Xi(x)]m  is uniformly distributed 
                                           in Bm and U occupies only an e-fraction of Bm.)  This event should happen for all 
                                           iG T  (this gives exponent t).  In this way we get the following bound for the total 
                                           probability (that should be less than 1):
                                                                                                                           t i           e2Tn
                                                                                                                       E E  E                                                                                   E  (2We"/2)L
                                                                                                                      77i=l  t= 1  TcBn,|T|=i  UCBm
                                           The number of different T is bounded by 2nt (the number of sequences of t elements 
                                           of Bn).  For the same reason the number of different U is bounded by 2mt.  For the
            382           12.  MULTISOURCE  ALGORITHMIC  INFORMATION  THEORY
            entire sum we get an upper bound
                                         n  e2m
                                        E E 21 n 21 m 2^t N £ Nt / 2
                                        ^   ^ ( 2n2m2N£N/2Y.
                                        m= 1  1=1
            The internal  sum  is  a  geometric  sequence.  Our  assumptions  guarantee  that  its 
            common ratio is less than 1/2,  so the sum is bounded by 2 times the first term, 
            and this term does not depend on t.  So we get the upper bound
                                 2П ■ (2N+n+m£N/2) ^ n2N+2n+l£N/2^
            and it is less than 1 according to our assumption.  The lemma is proven.
                We will use this lemma for e = 1/8.  In this case the condition can be rewritten
            as
                                           n2N+2n+i < 8n / 2
            or
                                      logn + N + 2n + 1 < 2>N/2.
            We can let N — 6n, and this guarantees that the condition of the lemma is true for 
            all sufficiently large n.
                Now we are ready to continue the proof of Theorem 235.  As before, we replace 
            C by its  shortest  (unconditional)  description,  so  we  assume  that  C  is  an  n-bit 
            string.  (The complexity of A and В is irrelevant; in particular, these complexities 
            may exceed n.)  Let us apply the lemma with N = 6n and e = 1/8; it provides 6n 
            mappings Хь • • • ,Хбп:         with specified properties.  As usual, we can take
            the first  (in some natural ordering) family with these properties (for a given value 
            of n), so we may assume that the complexity of the family x?: is O(logn).
                Assume that C(C\ A) = к and C(C\B) = I.  (The assumption only guarantees 
            the inequalities  C(C\A)  ^  к  and C{C\B)  ^  I.  But  we can decrease  к  and I  if 
            needed, and the statement becomes only stronger.)  Taking к (or I) first bits of the 
            hash values, we get N (= 6n) mappings Bn —>     (the same for I).  These families 
            define bipartite graphs in Шп x Шк and  x Ш1 where each left vertex has degree N 
            (including multiple edges).  Then we restrict these graphs on Sa and Sb, where Sa 
            consists of n-bit strings that have complexity at most к given A, and Sb  consists 
            of n-bit strings that have complexity at most I given B.  In Шк we note bad vertices 
            that have more than nc neighbors in Sa ; in Ш1 we note bad vertices that have more 
            than nc neighbors in Sb-  (The value of a sufficiently large constant c will be chosen 
            later.)
                In both cases the number of bad vertices in bounded by
                               2N ■ 2k/nc (for Efc) and 2N ■ 2l/nc(îor В4),
            since (a) the degree of a bad vertex exceeds nc; (b) the total number of edges in the 
            restricted graphs is bounded by |£/i| • N and \Sb \ • N, respectively; (c)  |£/i| < 2 • 2 k 
            and \SB\ < 2 -2l.
                Now, implementing our plan, we say that a vertex in Sa is bad if at least half 
            of its neighbors in the graph in Sa  x Шк  (multiple edges are counted several times) 
            are bad.  The lemma guarantees that the number of bad vertices in Sa is less than 
            2N • 2k/nc (we assume that c is large enough, so the bound for the number of bad
                              12.8.  INFORMATION  FLOW  AND  NETWORK  CUTS               383
            vertices is less than e2 k, where e = 1/8; recall than N = 6n).  Since the bad vertices 
            in  Sa  can be enumerated  (given n,  k,  and A), the conditional complexity of each 
            of them given A is at most
                           log(2iV • 2k/nc) + O(logn) < к — clogn + О (log гг).
            Therefore,  for  large  enough  c  all  bad  vertices  have  conditional  complexity  (with 
            condition  A)  less  than  k,  and  C  is  not  bad  (its  complexity  is  exactly  k).  This 
            means that most values [xi{Ç)]k (more than Nj2) are good in Шк.
                A similar argument for the other graph shows that most of the values [Xi{C)\i 
            are good in Bh  Therefore there exists i which leads to good vertices in both cases. 
            Then X  = [Xi{C)]k  and X' = [xi{C)\i  have the required properties.          □
                 325  State  and prove  a similar  result  for  three  conditions  (or  polynomially
            many conditions).
                After  these  remarkable  results  are  proven,  one  may  want  to  go  farther  and 
            ask:  Is it possible to find for a given A one string of length к that can be used to 
            reconstruct A starting from m'bitrary В such that C(A\B) < к?  It is easy to see, 
            however, that it is too good to be true.
                Let к = nj2, and let AT be a string of length nj2 that satisfies this property 
            (i.e., C(A IX, B) æ 0 for every В such that C(A \ В) < n/2).  Then C(A \ X) should 
            be at most n/2, since we can take the half of the n-bit description of A for B.  Now 
            we can take X for B\ then the complexity of the pair (X, B) is at most n/2, and A 
            cannot be reconstructed.
                 326       Show that not only is one fingerprint not enough, but any fixed number 
            of fingerprints is not enough.
                (Hint:  Assume  that  d  strings  are  enough.  We  can  assume  without  loss  of 
            generality that all these strings are incompressible.  For some i, let us concatenate 
            г-bit  prefixes  of all  fingerprints  and  denote  this  string  by  Bi.  For  some  i  —  io 
            the conditional complexity C(A\Bi) is close to n/2 since it decreases continuously 
            from n to some value not exceeding n/2 (the latter because it is true even for one 
            fingerprint).  This io is at least n/2/c; none of the fingerprints can serve Bi0, since 
            each  of them  has  some  common  information  with  Bi0  and  the  total  amount  of 
            information is not enough.)
                             12.8.  Information flow and network cuts
                We have considered several types of information requests; for each type we have 
            found necessary and sufficient conditions for the request to be fulfillable.  In all our 
            examples these  conditions  can  be  obtained  in  some  uniform  way  using  network 
            cuts.  Let us  describe this  technique explicitly.  Consider  an  information  request 
            (a  directed  acyclic  graph  with  capacities,  input  and  output  vertices,  and  input 
            and output strings).  Let us formulate a necessary condition for this request to be 
            fulfillable.
                Choose some cut of the request graph, i.e., some set I of the graph nodes.  We 
            are  interested  in  the  information  flow  that  goes inside  I.  Consider  all the graph 
            edges that cross I in this direction (start outside I and end in I).  If there is some 
            unlimited capacity edge among them, we do not get any non-trivial conditions for 
            our /.  Assume that this is not the case and that all capacities u\,..., Uk are finite. 
            Let Vi,..., Vi  be the input strings for all input vertices in /,  and let W\,..., Wm
            384           12.  MULTISOURCE  ALGORITHMIC  INFORMATION  THEORY
                    Figure 44.  A cut for Bennett-Gâcs-Li-Vitânyi-Zurek theorem
            be the output strings for all vertices in 7.  Then the following condition is necessary 
            for the request to be fulfillable:
                              C(WU. ..,wm\vl,...,vl)^ul + --- + uk.
            (As always, all the inequalities are understood with logarithmic precision).  Indeed, 
            if we know all V\,..., Vi and also all the messages along the edges going into 7 from 
            outside,  we  can  reconstruct  (with  logarithmic  advice)  all  the  outgoing  messages 
            for  all vertices in 7,  including W\,..., Wm.  This can be done by considering the 
            vertices of 7  in  a topologically sorted  ordering  (the starting point  of every edge 
            should be considered before its endpoint; recall that the graph is acyclic according 
            to our assumption).
                In  fact  this  is just  the  standard  argument  about  flows  and  cuts,  adapted to 
            information flow.  Let us explain how this general scheme gives the necessary con­
            ditions for the requests in one of our previous examples.  Consider the request from 
            Section 12.6, and let 7 be the set of three vertices inside the dotted line (Figure 44).
                The incoming information consists of A and some string of length к (along the 
            edge with capacity k).  Two other edges of the graph go in the opposite direction 
            (recall  that  all  edges  are  assumed  to  be  in  the  top-down  direction).  So  we  get 
            exactly the condition C(B | A)  ^ к that we used.
                327  Show  that  for  all  our  examples  the  conditions  we  considered  can  be
            obtained as cut-flow conditions for suitable chosen cuts.
                                  12.9.  Networks with one source
                In  the  previous  section  we  explained  a  general  method  to  obtain  necessary 
            conditions for the information requests to be fulfillable.  A natural question arises: 
            Are they (taken for all possible cuts) sufficient?  In all previous examples this was 
            indeed the case.  In general, as we will see later in this chapter, this is not true.  In 
            this section we show that it is true for the special case where there exists one input 
            node with input string A and several output nodes with (the same) output strings 
            A.  In other words,  if we want to transmit some information without change and 
            have only one source node, the cut-flow conditions are not only necessary but also 
            sufficient.
                For Shannon information theory this problem was studied in  [1,  104].  Our 
            argument follows the scheme used there (with some changes needed to adapt it to 
            the complexity framework).
                                   12.9.  NETWORKS  WITH  ONE  SOURCE                    385
                Let us start with an example.  Assume that we want to transmit a string A of 
            length 2к to three destination nodes  (Figure 45);  all the channels have unlimited 
            capacity except for the first three that have capacity k.  Can we achieve this?
                                                 A
                                                  /о
                                               A A A
                              Figure 45.  Splitting information into pieces
                There is no problem to deliver A into each destination separately.  For example, 
            to deliver it into the left destination vertex, we split it into two halves A\  and A2, 
            each containing к bits,  and send these halves using two left channels.  They have 
            capacity k, so this is possible.  (The third channel is useless for this destination.)
                The same trick can be used for two other destinations.  The problem appears 
            when we want to send A to all three destinations at the same time.  For this we 
            would like to cut A into  “three halves”  in such a way that every two of them are 
            enough to reconstruct A.  A standard secret sharing scheme can be used—we send 
            strings Ai, A2 and A\ ® A2 (bitwise XOR, or sum modulo 2) along three channels.
                It turns out that one may do something similar in the general case and prove 
            the following result:
                Theorem 236.  Consider an information transmission request with integer ca­
            pacities,  a  single  input vertex  and an input string A  of length n,  and the same 
            output strings A in several places.  Assume that all the cut-flow conditions are true: 
            For every J that does not contain the input vertex and contains at least one output 
            vertex,  the sum of capacities of all incoming edges  ( that start outside J and arrive 
            to  J)  is  at  least n.  Then  this  request is fulfillable  with О (log n) -precision:  One 
            can find strings for all edges in such a way that for every vertex the conditional 
            complexity of outgoing information given the incoming information is O(logn).
                (The constant hidden in O(logn) depends on the graph but not on n, capacities 
            and A.)
                Proof.  Consider first the case when there is only one output node t.  In this 
            case we need to send n bits of information from source node s to the destination 
            node t along the edges.  Let us imagine that each bit is packed into an envelope 
            (the position is also written inside the envelope).  We get n envelopes in the source 
            node.  We want to bring them to the output node with the restriction that if an 
            edge has capacity k, at most к envelopes could be carried along this edge.
                This envelope-moving problem is solvable due to the Ford-Fulkerson theorem 
            (min-cut equals max-flow (see, e.g., [45], since all capacities are integers, an integer 
            flow exists).  Now we write on each edge the bits that were carried along this edge. 
            More precisely, Ford and Fulkerson provide a set of envelopes for each edge; consider
            386           12.  M ULTISOURCE  ALGORITHMIC  INFORMATION  THEORY
            the numbers (positions) of bits sent in these envelopes, in non-decreasing order, and 
            write on the edge the subsequence of bits in these positions.
                Let  us show that bounds for conditional complexities are satisfied.  Consider 
            some node and the output and input strings for this node.  The output strings are 
            made by combining the bits obtained from the input string.  The scheme (which 
            bits go where) does not depend on A and can be computed if n is known, so this 
            scheme has complexity O(logn), and it is enough to know it to transform incoming 
            strings into outgoing strings.
                This finishes the proof for the case of one output node.
                To prove the same result in the general case, we use random linear codes.  In the 
            previous argument we just moved bits from incoming strings to outgoing strings. 
            Now we use a more general tool—we apply some linear operator in each vertex. 
            Imagine for now that bits are elements of the two-element field F2  (it consists of 0 
            and  1,  and 1 + 1  = 0).  Then /-bit strings are vectors in the /-dimensional vector 
            space over this field.
                Consider a node that has incoming edges of capacities i\,..., ip, and outgoing 
            edges of capacities           (We replace  all  infinite  capacities  by  n:  since  we
            send only  n  bits,  we  never  will  need  to  send  more  than  n  bits  along  an  edge.) 
            Then the linear transformation in the node can be specified by a matrix of size 
            (ii + • • • + jq)  X  (?’i + • • • + ip),  and we multiply the incoming bits vector by this 
            matrix to get the outgoing bits vector.  Note that we send exactly к bits along an 
            edge of capacity к (even if this edge looks useless).
                (It  is  easy to see that our solution for the three outputs example has exactly 
            this form.)
                Assume that for some vertex such a transformation matrix is chosen.  Then we 
            get for each output some linear transformation (from input n-dimensional space to 
            output n-dimensional space)  over the field F2.  We want to make all these linear 
            operators  invertible.  This  would  guarantee  that  the  output  string  contains  full 
            information (up to a fixed linear transformation)  about the input string.  So if we 
            can find some matrices in the nodes that make all the input-output transformations 
            invertible, we are done.
                There is  one subtle  point  here:  The  mere  existence  of good  transformation 
            matrices for all nodes is not enough.  We need these matrices to be simple.  However, 
            a standard trick helps.  If good matrices exist, we can use the first example in some 
            ordering, and it has complexity O(logn); recall that the graph is fixed.
                A more serious problem is that in some cases it is not possible to make these 
            matrices invertible for each output.
                328       Consider the information request from Figure 46.  Assume that the input 
            string  consists  of two  bits  and  the  capacities  of all  edges  are  equal  to  1.  Show 
            that  no  linear  transformations  in  the  nodes  make  all six  input-output  mappings 
            invertible.
                (Hint:  There are only three non-linear functionals on two-dimensional space, 
            so two of the intermediate vertices will carry the same information.)
                Note that  for  each  output  it  is  possible  to  find  transformations  in  the  ver­
            tices that make the transformation for this output invertible; indeed, we have seen 
            that we do not even need arbitrary linear transformations—repackaging the bits 
            (from envelopes to envelopes) is enough.  The problem is that we cannot make the 
            transformation for all output nodes invertible simultaneously.
                                   12.9.  NETWORKS  WITH  ONE  SOURCE                   387
                                                  A
                                  Figure 46.  Field F2 is not enough.
                Let us change our setting and consider elements of an arbitrary field F instead 
            of bits  (elements of F2).  The input is now a vector from Fn ;  an edge of capacity 
            к  carries  an  element  of Fk,  and  transformations  in  the  nodes  are  F-linear,  i.e., 
            determined by matrices with elements from F.
                Let us show that for a large enough field F one can find the transformations 
            in the nodes that make all the output mappings invertible at the same time.  Let 
            us consider the matrix elements as F-valued variables.  Then the elements of the 
            resulting input-output matrices are polynomials in these variables.  The determi­
            nants of these matrices are also  polynomials.  The degree of a determinant  as a 
            polynomial in matrix elements is bounded by nE, where E is the number of edges 
            in the graph (going from input to output, we increase the degree of the polynomial 
            by 1, and by computing the determinant we multiply the degree at most by n).  So 
            for each output we have a polynomial of a limited degree (the determinant of the 
            corresponding matrix), and we know that this polynomial is not equal to 0 (since 
            we can make the matrix invertible for each output separately).  It remains to use 
            the following simple algebraic result:
                Lemma.  A polynomial of degree d inm variables or over a field F is either equal 
            to 0  (has zero coefficients)  or is equal to zero in a random point with probability at 
            most d/\F\.
                Here by degree we mean total degree (exponents for all variable are added), |F| 
            stands for the cardinality of F, and the probability is taken over uniform distribu­
            tion in Fm.
                Proof.  We use induction over m.  For m = 1 the claim says that the number of 
            roots of a univariate polynomial is bounded by its degree (factorization argument). 
            For  m  >  1,  we  represent  the  given  polynomial  as  a polynomial  in  one  variable 
            whose coefficients are polynomials in the remaining variables.  Let d\ be the degree 
            of this univariate polynomial (the maximal exponent for the selected variable), and 
            let  d2  be  the  degree  of its  leading coefficient  (as  a polynomial  in  the  remaining 
            variables).  This leading coefficient may be zero or not depending on the values of 
            the remaining variables.  The probability for it to be zero is bounded by d2/\F\ 
            due  to  the  induction  assumption,  and  if the  leading  coefficient  is  not  zero,  the 
            probability to bump into a root of the non-zero univariate polynomial of degree d\ 
            is bounded by d\/\F\.  In total we get (0Î2 +d\)/\F\ ^ d/\F\.  The lemma is proven.
             388             12.  M ULTISOURCE  ALGORITHMIC  INFORMATION  THEORY
                  Now we note that if the probability of getting a zero determinant at each given 
             output is less  than  1/ (number  of output  nodes),  then there exist some values of 
             the variables that  make  all  the  determinants  non-zero  simultaneously.  We  have 
             the degree bounded by nE,  the number of outputs is also bounded by E, so the 
             condition nE2  <  |F|  is enough to guarantee the existence of matrices that make 
             all input-output transformations invertible.  (And these matrices can be found by 
             search, so they are simple matrices with this property.)
                  How can we use all this in a situation where we have n bits (and not n elements 
             of a big finite field)?  As usual for coding theory,  let us split the n-bit string into 
             blocks of some size k.  We get (approximately) n/k blocks and interpret each block 
             as an element of a field of size 2 k (such a field exists for all /с, as explained in algebra 
             textbooks).  If it turns out that the number n and all capacities are multiples of k, 
             and that 2 k > (n/k)E2, then we are done.
                  In general, we need some adjustments.  First we choose some value of к such 
             that  2k  >  nE2  (we  ignore  the  1/k  factor,  but  it  is  in  our  favor).  This  gives  us 
             к = O(logn).  Then we round n and the capacities making them multiples of к (n 
             is rounded downwards and capacities are rounded upwards, so the inequalities for 
             cuts remain true).  The rounding error is O(logn), so it does not matter with our 
             precision, and it remains to use the statement we proved.                                □
                   329 Using random linear transformations, construct a probabilistic algorithm 
             that finds the maximal flow in a directed acyclic graph with integer capacities.
                  (Hint:  For a given n we may find whether a flow of size n exists by assigning 
             random matrices to each vertex and checking whether the resulting n x n matrix 
             is invertible.)
                  In  the  rest  of the  chapter we consider examples of the opposite type,  where 
             cut-flow conditions are only necessary, but not sufficient.
                        12.10.  Common information as an information request
                  We have already considered one example where necessary cut-flow conditions 
             turn out to be insufficient—this was the common information problem from Chap­
             ter 11.  In Section 11.2 we asked (for given strings x, у and for given integers a, ß, 7) 
             whether there exists a string 2: such that
                                     C{z) < a,    C{x I z) < ß,    C{y I z) < 7 .
             This problem,  considered up to logarithmic precision,  can be reformulated as an 
             information transmission request  (Figure 47).  Indeed,  if a string 2: with required 
             properties exists, it can be sent along the middle edge (of capacity a);  two other 
             edges should transmit conditional descriptions of x and у given 2:.  (Both 2: and these 
             conditional descriptions can be found by search with logarithmic advice if x and у 
             are given, so there is no new information in the top vertex.)  O11 the other hand, if 
             this information transmission request is fulfillable, then the string 2: sent along the 
             middle edge satisfies the required inequalities (with logarithmic precision).
                  The cut-flow conditions give the inequalities
                             C(x)^oc + ß,  C(y)^a + 7,  C{x, y) ^ a + ß + 7,
             and we have seen in Chapter 11 many different examples of strings x and у where 
             these inequalities turn out to be insufficient for the existence of string 2:  (common 
             information) with required properties.
                                       12.11.  SIMPLIFYING  A  PROGRAM                        389
                                      12.11.  Simplifying a program
                 In the previous section we have seen an example of an information request that 
             can be not fulfillable even if all the cut-flow conditions are satisfied.  This request is 
             rather complicated, and it is interesting to find a simpler example.  In this section 
            we mention one of these examples—it turns out that the statement of Theorem 229 
             is quite close to the boundary line, and just a slightly more general setting makes 
             the cut-flow conditions insufficient.
                 Consider an information request suggested by Vyugin (Figure 48).  The differ­
            ence with Muchnik’s theorem is that now instead of reconstructing one of the input 
            strings  (P in the current notation), we have to obtain some third string (В).  The 
            cut-flow conditions for this problem are C(B\A) ^ к and C(B\A,P) = 0 (consid­
            ered with logarithmic precision).  They are (as always) necessary, but one can show 
            that they are not sufficient.
                 This request can be described informally as “simplification of a program”.  Since 
            C(B\A,P)  =  0,  the string  P  can  be  considered  as  a program  (or  information 
            sufficient for a program)  that transforms A into B\  however, the complexity of P 
            is bigger than strictly necessary, i.e., it exceeds к =  C(B \ A).  Can we find another 
            program, of minimal possible complexity C(B\A), that transforms A to В and at 
            the same time is a  “simplification”  of the first one  (i.e.,  has no  new information 
            compared to Р)?
                 The detailed  explanation of the  negative  answer  (even several  explanations, 
            using game, probabilistic, and combinatorial arguments) is given in [140].
                                 Figure 47.  Common information request
                                                  P 
                                                  к 
                                                  В
                                  FIGURE 48.  Simplification of a program
        390      12.  MULTISOURCE ALGORITHMIC  INFORMATION  THEORY
                               A
                               P 
                               A
                   Figure 49.  Two channels of bounded capacity
                      12.12.  Minimal sufficient statistics
          Tn this section we consider another request where cut-flow conditions are not 
        sufficient  (Figure 49).  Here the output string is again (like in Muchnik’s theorem) 
        one of the input strings, but now both capacities are limited.  In addition, we allow 
        use of the full information about В when encoding A.
           This problem is related to the notion of minimal sufficient statistics in probabil­
        ity theory.  Let us explain this connection (though it is not important for the proofs, 
        so  one may skip these explanations).  Consider a pair of two random variables 9 
        and X with some joint distribution.  The variable 9 is considered a parameter, and 
        for each value of 9 we consider the conditional distribution of X.  For example, we 
        may first choose 9 uniformly distributed in [0,1], and then choose an n-bit string X 
        according to the Bernoulli distribution on n-bit strings with parameter 9.  In this 
        way we get a joint distribution of 9 and X.
           Assume that we observe X in a pair (9, X) generated according to this distri­
        bution, and want to guess 9.  (As usual, we assume that some a priori distribution 
        on the space [0,1] of parameters is given.)  Not all information in X is really useful 
        for that—it is enough to know how many ones are among the outcomes, and it does 
        not matter what their positions are.  More formally,  the random variable N(X), 
        the number of ones in X , extracts all information about 9 that is in X,
                           I(N(X):9) = I(X:9).
        For an arbitrary function N the left-hand side does not exceed the right-hand side; 
        the functions N that transform this inequality into an equality are called sufficient 
        statistics.  The same condition can be formulated in a different way:  9 and X and 
        independent given N(X).  One more reformulation is H(9\N(X)) = H(9\X).
           By definition, the random variable X itself is a sufficient statistic; our example 
        shows that it may contain a lot of irrelevant information.  A sufficient statistic is 
        called a minimal sufficient statistic if it is a function of all other sufficient statistics. 
        For the random variables with finitely many values, a minimal sufficient statistic 
        always exists (and is unique up to permutations):  one should identify those values 
        of X that lead to the same conditional distributions on 9.  The minimal sufficient 
        statistic has minimal entropy among all sufficient statistics.
           330  Assume that all values of 9 have positive probabilities.  Prove that the 
        notion of sufficient  statistics depends only on the values of conditional probabil­
        ities  P[X  =  X  I  9  =  t]  for  all  pairs  x,t  (so  the  distribution  for  9  itself is  not 
        important).
                                   12.12.  MINIMAL  SUFFICIENT STATISTICS                    391
                 (Hint:  For each x consider a vector containing all P[X = x \ в = t] for all values 
             t of the random variable в.  Then iV is a sufficient statistic if and only if every two 
             arguments with the same values of N correspond to proportional vectors.)
                 The search  for  minimal  sufficient  statistics  can  be  described  as  follows:  We 
             consider random variables X' such that H{X'\X) — 0  (i.e.,  functions of X)  and 
             select  those for which the value H{B\X')  is minimal.  Then we minimize H(X') 
             among the selected random variables.
                 Now we try to find an algorithmic information counterpart of this procedure. 
             Let us consider two strings A (which corresponds to в) and В (which corresponds 
            to  X).  We want to select some part В'  of information in  В for which C{A\B') 
            reaches  the  minimal possible value  C(A\B).  Among those  B'  we  then want  to 
            select one with minimal C(B') so it can be considered as an algorithmic version of 
            minimal sufficient statistics.
                 This setting can be explained in terms of information transmission using Fig­
            ure  49.  Here  B'  (a function  of В)  is  sent  via a channel  of capacity  g,  and  the 
            conditional  description  of A  given  B'  is  sent  via  a  channel  of capacity p.  The 
            minimal statistics problem can be now stated as follows—minimize q for minimal 
            possible p « C(A \ В).
                 Let us consider some more general questions.  For which p and q is the informa­
            tion transmission request  (for given A and В)  fulfillable?  The necessary cut-flow 
            conditions are (with logarithmic precision) C( A) ^ p + q and C( A | B) ^ p.
                 331 Find cuts that give these conditions.
                 For some pairs A, В these necessary conditions turn out to be sufficient.
                 332 Show that if A and В have extractable common information (e.g., A and 
            В are overlapping substrings of some incompressible string), then these necessary 
            conditions are also sufficient.
                 {Hint:  The condition C(A\B) ^ p guarantees that we can send the part of A 
            outside В along the left channel,  plus some other part of A,  and the rest can be 
            sent along the right channel—the condition C(A) ^ p + q guarantees that there is 
            enough capacity.)
                 However,  as we will see,  in the general case the necessary conditions are not 
            sufficient.  To provide an example in which this happens, let us fix the complexities 
            and conditional complexities of A and B.  We agree that A and В have complexity 
            2n, and the pair (A. В) has complexity 3n.  (So the conditional complexities C(A | В) 
            and C(B\ A) are about n.)  Figure 50 shows necessary conditions p + q ^ 2n and 
            p ^ n for this case.
                 Now let us try to find some values of p and q when the request is guaranteed 
            to  be  fulfillable  (whatever A  and  В  are,  assuming they have complexities as we 
            agreed).  We can send the string A along the left channel, so the request is feasible 
            for p — 2n and q = 0 (and, of course, for all bigger p and q).  Another possibility is 
            to send В completely along the right channel, so the request is feasible for p = n, 
            q = 2n (and for bigger p and q).  In this way we get two quadrants with vertices 
            (2n,0) and (n, 2n).  Moreover, if we delete (say, the last) к bits from В (which we 
            assume to be incompressible),  then the conditional complexity C(A\B)  increases 
            at most by к, so the request is feasible for q = 2n — k, p — n + k.  So in the dark 
            grey region on Figure 51  (denoted there by G) the request is always feasible.
                           12.  MULTISOURCE ALGORITHMIC INFORMATION THEORY
            392
                                            Q
                                        2 n
                                                                   V
                                Figure 50.  Necessary (cut-flow)  conditions
                As in the case of common information (Chapter 11), we may say that the profile 
            of a pair  (A, В)  (the set  of all pairs  (p, q)  that  make the request  feasible)  is not 
            determined by complexities and conditional complexities of A and В—for different 
            pairs with the same complexities the profiles could be different.  Problem 332 shows 
            that for some pairs the profile coincides with the upper bound provided by cut-flow 
            inequalities.  Theorem 237 shows that for some pairs the profile coincides with the 
            lower bound (it equals G; see Figure 51).
                However, we need to be careful to formulate the statement correctly.  One would 
            like to claim that for every B' that is simple relative to B, the pair (C(A \ B'), l(B')) 
            is in an 0(logn)-neighborhood of G, and the hidden constant in O(logn) does not 
            depend on n and B1.  However, the assumption “В' is simple relative to      needs to 
            be formulated in some exact way.  We should choose some threshold r and require 
            that  C(B'\B) < r.  As r increases,  the distance between  (C(A\B/),l(B/))  and G 
            may increase, and we should make an exact statement about it.  In fact, the distance 
            is bounded by O(r).  Here is the exact statement:
                Theorem 237.  For every n there exist strings A, В of complexity 2n+0(logn) 
            such that C(A,B)  =  3n + O(logn),  and for every В’  the pair (C(A \ B’), 1{B')) 
            belongs to the 0(logn + C(B' \ B))-neighborhood of the set G.
                                     Figure 51.  Sufficient conditions
                                     12.12.  MINIMAL  SUFFICIENT  STATISTICS                    393
                 Proof.  As in many other cases, we will prove this result using a game.1  First 
             we describe the game, then show the winning strategy, and finally we explain how 
             this  implies the statement of the theorem.
                 The game has parameter n.  We (being one of the players) may, for each 2n-bit 
             string B,  choose at most 2n strings of length 2n,  calling them  “simple given B”. 
             Our adversary, on the other hand, for each triple (p, q, r)  (taken from some set M 
             of admissible triples, see below) may do the following:
                    •  for every string В of length 2n, choose at most 2r strings of length q and 
                       call them  “r-simple given В” ;
                    •  for every string B' of length q, choose at most 2P strings of length 2n and 
                       call them  “p-simple given В   .
             This is done independently for each triple (p, q, r)  from M.  Imagine that we play 
             against  a  team.  For  each  (p, q, r)  there  is  a  team  member  who  makes  his  own 
             announcements (obeying his own cardinality restrictions,  as described),  but they 
             play as a team against us.
                 The adversary team includes two more members.  The first one may choose up 
             to 22'1“1  strings of length 2n  (i.e., not more than half) and call them  “bad”.  The 
             second one may for every string В of length 2n choose up to 2n-2 strings of length 
             2n (in a different way for each В) and call them “bad for this FT’.
                 Later we will play with the adversary who marks as “bad” all 2n-bit strings of 
             complexity less than 2n — 1, and for each В marks as “bad for В”  all 2n-bit strings 
             A such that C(A\B) < n — 2.  But the game rules do not say anything about this 
             specific choice of bad strings.  Note that we use threshold n — 2 (and not, say, n — 1) 
             since we need some reserve; see below.
                 Both we and the adversary make the declarations gradually—at any moment 
             each player may extend the lists of simple/bad strings (if the cardinality restrictions 
             are not violated by this extension).  The declarations cannot be retracted.  Since the 
             total number of positions is finite,  the game  (though being formally infinite)  has 
             some limit position,  but the players do not declare whether they will make more 
             moves or not.
                 The winner in the game is determined by the limit  position.  The adversary 
             wins if for every pair of 2n-bit strings A and B, where A is n-simple given B, and 
             A  and В  are not bad,  and A is not bad for B,  there exists  an admissible triple 
             (p, q, r)  and a q-bit string B' such that
                    •  B' is r-simple given B\
                    •  A is p-simple given B'.
                 According to our plan, we first show a simple computable strategy that wins this 
             game (for some set of admissible triples (p, q,r)), and we then derive the statement 
             of the theorem.  This strategy replies to each move of the adversary (made by some 
             team member who declares a new  “bad”  or a new  “simple”  string)  by one move. 
             We add (for some string В of length 2n) one n-simple string A of length 2n that 
             prevents the adversary from winning, if no new moves are made.  To achieve this, 
             we need the following:
                    •  the chosen strings A and В are not yet declared bad;
                    •  A is not yet declared bad for B\
                 1We have  already seen  several  game arguments;  for a survey  of game  techniques  for  Kol­
             mogorov complexity see [198,  136].
         394      12.  MULTISOURCE  ALGORITHMIC  INFORMATION THEORY
             •  there is no admissible triple (p, q, r) for which two things happen:  (1) there 
               exists sonie B' of length q that was declared r-simple given В by (p, q, r)- 
               player of the adversary’s team, and (2) at the same time A was declared 
               p-simple given B' by the same player.2
           Why is this possible?  At every step of the game there is at least 22n-1 strings 
        not declared bad.  If we select one of them, we may declare every string as n-simple 
        given В unless we exhausted the quota, i.e., we have already declared 2n strings to 
        be n-simple given this B.  But if this happens for all non-bad B,  this means that 
        we have already made 22n_1 • 2n = 23n_1  moves.  Recall that we make one move 
        after each move of the adversary, and we will see that the adversary cannot make 
        so many moves.  So we can choose some B.
           After В is chosen in such a way that we have not exhausted the quota for n- 
        simple strings given B, we start to select A.  Let us consider some admissible triple 
         (p, q, r)  and count the strings A that do not make the position winning because of 
        this triple.  There is at most 2T strings B' of length q declared as r-simple given B. 
        For each of these B' there is at most 2P strings of length 2n declared as p-simple 
        given B'  (for the same triple = by the same team member).  So we have to avoid 
        2p+r strings for each triple  (p, q, r)  from the set M of admissible triples.  In total 
        we have to avoid 2p+r • |M| strings.  Also we have to avoid strings that are declared 
        bad,  at most 22n_1,  as well as strings declared bad for В,  at most 2n ~ 2  of them. 
        So we can make the move we want, assuming that
        (*)              2p+r ■ \M\ + 22n~ 1 + 2n' 2 < 22"'.
           Now we need to count how many moves the adversary can make.  Each of M 
        players of his team takes care of one admissible triple  (p, q, r)  and makes at most 
        22n+r moves, declaring r-simple strings, and 2qJtp moves, declaring p-simple strings. 
        So the total number of moves for all players is bounded by
                          \M\ • (2max(2n+r) + 2max(<?+P))
        (maximal values of 2n + r an q+p are taken over all elements in M).  We should also 
        add 22n~1 moves that the adversary can make declaring bad strings and 2 2n -2n ~ 2 = 
        23n—2 moves he can make declaring bad strings for 22n strings of length 2n.  So we 
        can ensure the required upper bound for the number of the adversary’s moves if
                 \M\ • (2max(2n+r) -\- 2max(<?+p)) _|_ 22n_1 -\- 23n~2  < 23n_1
        Taking into  account  that  2k + 2/  is  close to  2max{fc,/},  it  is  easy  to  see  that  the 
        conditions  (*)  and  (**)  are guaranteed  to  be true if all the  triples  (p,q,r)  G  M 
        satisfy the inequalities
                           p + r < 2n — 3logn — 0 (1),
                          2n + r < 3n — 31ogn — 0(1), 
                           p + q < 3n — 31ogn — 0(1).
        Note that there are 0(n3) triples satisfying these inequalities, and to compensate 
        for the factor  |M|, we subtracted 31ogn + 0(1), the upper bound for log|M|.  In 
        these  inequalities we write  0(1),  but some small  value,  like  10,  will  work.  Now
           2 Note that  according to our  agreement  different  players on  the  adversary team  may  make 
        different declarations,  so the term  “r-simple”  has a different  meaning for the  {p, q, r)-player and
        (p1, q', r)-player  even  when  r  is  the  same  for  both.  (In  fact,  this  freedom  is  not  used  by  the 
        adversary, with whom we really play to prove the theorem.)
                                   12.12.  MINIMAL  SUFFICIENT STATISTICS                    395
            let M be the set of all triples that satisfy these inequalities: we know then that a 
            winning strategy in our game exists.  (Note that for small r we get the conditions 
            p < 2n and p + q < 3n that are true for points in G.)
                 Now we apply this winning strategy against  the following  blind  strategy for 
            the  adversary  (blind  means  that  he  does  not  look  at  our  moves).  He  declares 
            a  2n-bit  string  of complexity  less  than  2n —  1  to  be  bad;  for each  2n-bit  string 
            В  all  the  strings  that  have  conditional  complexity  less  than  n — 2  given  В  are 
            declared bad for B.  Also for each triple (p,q,r)  the corresponding player from the 
            adversary team declares strings of small conditional complexity as r- or p-simple 
            for the corresponding threshold r or p.
                 Then  both  players  follow  some  computable  strategies,  so  the  game  is  com­
            putable, and we need only to know n to start its simulation.  We cannot effectively 
            find when the limit state is reached,  but it will happen at some moment.  In this 
            state there is a pair (A, В)  of strings guaranteed by the winning condition.  Let us 
            show that these two strings satisfy the statement of the theorem.
                The length of both strings A and В is 2?г; the complexity is 271 + 0(1) (it cannot 
            be less since otherwise the string would be declared bad).  Since we declare at most 
            2n  different  strings  A  as  simple  given  В,  we  have  C{A\B)  ^  n + 0 (log77.).  On 
            the other hand, the complexity C(A \ B) cannot be less than n — 0(1), otherwise A 
            would be declared bad for B.
                Let  B'  be a ç-bit  string such  that  C(B'\B)  <  r.  We may assume that r  is 
            much smaller than n,  say,  r  <  n/2  (otherwise the  term  0 (r)  in  the  right-hand 
            side makes the statement of the theorem trivial).  So the inequality for 2n + r on 
            the  previous  page  (the  second  one  in  the  list  of three  inequalities  after  (**))  is 
            true.  Therefore,  for every p that satisfies the first  and the third inequalities,  the 
            complexity C(A\B')  exceeds p.  So for every pair  (p, q)  from the light-grey area 
            (Figure 51)  that  is 0(r + log7r)-far from O,  there is no B'  of length  q such that 
            C(B' IB) <C t and C{A I 7T ) <C p.  That is exactly what we needed to show.
                Theorem 237 is proven.                                                        □
                Let  us  now  give  another  proof of the  same  statement  using  a  probabilistic 
            argument.  It  follows almost  the same scheme as the game proof above,  but  has 
            several important differences.  First, we do not wait until our adversary makes his 
            moves, but we make all our moves at the beginning of the game.  Second, we do not 
            describe the winning moves explicitly—we just show that a random choice provides 
            a winning strategy with positive probability.
                Let  us  first  make  a  technical  remark:  Under  these  assumptions  we  may  as­
            sume without loss of generality that both players always fully use their quotas for 
            bad/simple strings (adding strings is always to the player’s advantage).
                Now we can start the argument, explaining its relation to the game version in 
            parentheses.  Our strategy is now represented by some mapping
                                            U : B2n x B M  B2n
            (the values of U (В, X ) for a given В and all possible X correspond to strings simple 
            for В in the game argument).  Assume that some finite set M of integer triples is 
            fixed and for each triple (p, q, r) e PI two mappings
                                           Vp.q,r : B2n xBr ^B ?
            396          12.  MULTISOURCE  ALGORITHMIC INFORMATION THEORY
            and
                                       Wp,q,r : Ш4 X Bp -+ B2n 
            are given.  Moreover, assume that a mapping
                                          S: B2n-2 -a B2”
            and another mapping
                                      T: B2n X Bn-21ogn   E2n 
            are given.  (The mappings Vp^q^r and Wp^q,r correspond to the moves of the (p, q, r)- 
            player on the adversary team.  Namely, strings Vp<q^T(B, X) for all possible X are 
            y-simple for В;  strings  Wp^q^T{B',X)  for all possible X  are p-simple for B'.  The 
            mappings  S' and T correspond to the moves of two additional players.  Namely, 
            S(X) are bad strings (there are 22n“2 of them), and T(B,X) are bad for В (there 
            are 2n-21ogn of them).  The bounds for the number of bad strings are now more 
           strict than in the game argument; this is needed to obtain the bounds below.)
               We say that a mapping U is covered by a quadruple V, W, S, T (which consists 
           of two families of mappings and two mappings) if for every string B G B2n and for 
           every string A that is equal to U(B,X) for some X G Bn  (for every string В and 
           for every string A that is declared by us as n-simple for В), the pair (A, В) satisfies 
           one of four conditions:
                (1)  В belongs to the range of S (i.e.,  В — S(Y) for some Y G B2n~2).  (The 
                    string В is declared bad by the adversary.)
                (2)  A belongs to the range of S (i.e., A = S(Y) for some Y G B2n~2).  (The 
                    string A is declared bad by the adversary.)
                (3)  A equals T(B,Y) for some Y G Bn-21ogn.  (The string A is declared bad 
                    for В by the adversary.)
                (4)  There exists a triple (p, q, r) G  M and some ç-bit string B' such that the 
                    following two conditions (a) and (b) are both true:
                     (a)  B' = Vp^q,r(B,Y) for some Y G Br ((p, ç, r)-player declared B' to be 
                        y-simple given B)\
                     (b)  A = Wp^ r(B', Z) for some Z G Bp ((p, ç, y)-player declared A to be 
                        p-simple given В').
               We will prove (under some conditions on the set M, see below) that there exists 
           some mapping U that is not covered by any quadruple V, W, S, T.  This proof will 
           use a probabilistic  argument:  For each quadruple we count how many mappings 
           are covered  by it  (i.e.,  we  compute the probability  for  a random mapping to be 
           covered by a given quadruple), then we multiply this probability by the number of 
           quadruples and show that the product is less than 1.
               The counting for one quadruple goes as follows.  Assume that  V. W, S, T are 
           fixed.  There are at  least  22n_1  strings В of length 2n that violate condition  (1). 
           For the set  U to be covered,  it is needed that for each of these В each of the 2n 
           values A = U(B,X)  (for all n-bit X) is covered by one of the conditions  (2)-(4). 
           We will show that for a given В  and X the probability of this event  is at  most 
           1/ 2.  Then, by independence, we conclude that for a random U the probability to 
           be covered is at most     (1/2)22"-1*2'1 = (1/2)23""1 
           Let us check the estimate for given В and X.  Unsuitable A include the following:
                  •  strings covered by (2), at most 22n-2 of them;
                  •  strings covered by (3), at most 2n-21ogn of them;
                                                 12.12.   MINIMAL SUFFICIENT STATISTICS                                           397
                           •  strings in WPtq^r(VPjq^r(B,Y), Z) covered by (4), at most 2r x 2P of them 
                              for each triple (p, q, r) € M.
                 In total we get
                                                      2лп~ л + 2n ~ * logn + 2  +р •  M 
                 unsuitable strings, and this number is bounded by 22n~l  (half of all strings) if
                 (*)                                      r + p + log\M\ < 2n — 3
                 for all  (p,q,r) G M (this is our first requirement for M).
                       Now we estimate the number of all quadruples V, W, S, T.  For given p, q, r there 
                 are at most
                                                             (29)з271><2Г = 2 я'2,2т'+г 
                 possibilities for VPiqjr and at most
                                                             (22n)2" X 2P       2 2 n-2q+p
                 possibilities for Wp^ r.  There are at most
                                                            ß2n^22n~2 — 22n22n~2
                 possibilities for 5 and at most
                                                      ^22n^22Tlx2n- 21o6n  _  2(23n/n) + l
                 possibilities  for  T.  The  first  two  bounds  appear with  exponent  \M\  (to  get  the 
                 bound  for  the  total  number  of possibilities  for  V  and  W)\  in  total  we  get  the 
                 following bound for the number of quadruples:
                                         2<j-22n+rx|M|  , 22n-2g+pX\M\ . 22n'22n~2 . 2(23n/n)+1
                 The binary logarithm of this number does not exceed
                                 q ■ 22n+r x |M| + 2n • 29+p x \M\ + 2n • 22n"2 + (23n/n) + 1, 
                 and this is smaller than 23n  (as we need to finish the proof) if 
                 (**)                            2n + r + logg + log\M\ < 3n — 0(1)
                 and
                 (***)                            q + p + logn + log\M\ < 3n — 0(1).
                 (We use that 2a + 2b is equal to 2max(a’6) up to an 0(l)-factor; two other conditions 
                 2n — 2 + log2n < 3n — 0(1) and log(23n/n + 1) < 3n — 0(1) are guaranteed to be 
                 true.)
                       All three conditions (*)-(***) are true if
                                                        p + r <2n — 31ogn — 0(1),
                                                       2n + r < 3n — 4 logn — 0(1), 
                                                        p + q < 3n — 41ogn — 0(1)
                 for all (p, q, r)  G M, since \M\ — 0(n3) in this case.  Let us now define M as the set 
                 of triples satisfying these three inequalities.  We know now that (for this M) there 
                 exists  a mapping U not  covered by any quadruple V, W, S, T.  Now the standard 
                 argument shows that there exists a mapping U of logarithmic complexity (first in 
                 some order) not covered by any quadruple.
                       Take this U and consider the following quadruple V, W,S, T.  Let  {5(-)}  (the 
                 range of 5)  be the set of all 2n-bit strings whose complexity is less than 2n — 2.
             398             12.  MULTISOURCE  ALGORITHMIC INFORMATION THEORY
             For every B € B2n let the set  {T(B, •)}  be the set of all 2?r-bit strings that have 
             conditional complexity given В less than n — 2 log n.  Let (for given p.q.r) the set 
             VPiQ,r(B,-)  be the set of all g-bit strings of conditional complexity  (given В)  less 
             than r.  And let (for given p, g, r) the set Wp,q,r(B', •) be the set of all 2n-bit strings 
             of conditional  complexity  (given  В')  less  than p.  (We specify only  the range of 
             these mappings,  the order can  be arbitrary.  Also,  the number of strings with a 
             given property may be less than the number of slots, so we fill the remaining slots 
             in an arbitrary way.)
                 We know that U is not covered by this quadruple.  This means that there exist 
             strings A and В of length 2n that do not satisfy any of the properties (l)-(4).  Then 
             C(A) = 2n + 0(1)  (because A has length 2n and cannot have smaller complexity, 
             otherwise it  would be covered by S).  For the same reasons C(B)  =  2n + 0(1). 
             The conditional complexity C(A\B) equals n + 0(logn):  it cannot be bigger since 
             A = U(B,X) for some string X of length n, and the complexity of U is O(logn), 
             and it cannot be smaller, since otherwise the property (3) would be true.  Finally, 
             there is no triple  (p,g,r)  in the set M such that C(B'\B) < r and C(A\B') < p, 
             otherwise (4) would be true.
                 The rest of the proof is the same as in the game argument,  and this finishes 
             the probabilistic proof.
                 Finally, one can provide a geometric construction that gives strings A, В with 
             the required property.  (Unlike the case of common information, here the geometric 
             construction gives almost the same complexity bound, not weaker ones.)
                 Consider the field with 2n elements (or a field of approximately this size, if we 
             want to consider integers modulo p for some prime p), and a two-dimensional plane 
             over this field.  Let (A, B) be a random pair that consists of a point and a line going 
             through this point.  Then we get complexities as required by Theorem 237.  Let us 
             show that such a pair has the required properties.
                 Assume that a string B' is given and
             (*)                  C(B' \B) ^ r,     C(B')^q,  C(A\B')^p
             for  some p, g, r.  We want to prove that  the pair  (p, g)  is  in  an  0(r) + 0(\ogn)- 
             neighborhood of the set G by showing that otherwise the pair  (A, B)  would have 
             smaller complexity.  Let  us estimate the number of pairs  (A, В)  such that  (*)  is 
             satisfied for some B'.  Each of the 2q strings B' determines two sets:
                     •  the set Ub>  of 2n-bit strings A such that C(A \ B' )      ;
                     •  the set Vb>  of 2n-bit strings В such that C(B'\B) ^ r.
             The set Ub' has cardinality 2P (in fact, 0(2P), but we ignore bounded factors).  The 
             set  Vb>  may have different sizes depending on the choice of В', but we know that 
             the family Vb<  for all B' covers the set B2n in at most 2r layers (for each В there 
             are at most 2r strings B' that are r-simple given B).
                 We want to show that the union of the combinatorial rectangles  Ub'  x  Vb> 
             over all B' covers only a small fraction of all pairs of incident point and lines.  To 
             bound the number of pairs covered by these rectangles, we use the same technique 
             as before:  the incidence graph does not have cycles of length 4,  so we can apply 
             the combinatorial lemma (p. 358).  Let us recall the statement of this lemma.  If a 
             rectangular table I x L has stars in some cells and one cannot find two rows and 
             two columns that have stars at all four intersections, then the total number of stars
                                   12.12.  MINIMAL  SUFFICIENT STATISTICS                    399
             is bounded by
                    •  0{L) for I ^ y/L-
                    •  0(lVZ) for I ^ y/L
             (this  is  the  bound  obtained  in  the  proof).  Now  we  have  to  consider  separately 
            the case of large and small  Vb>.  If Vb>  is  large  and  contains  more than  \J\Ub>\ 
             (i.e.,  more  than  2p/2)  elements,  then  the  number  of covered  pairs  for  this  B'  is 
            at  most  2p/2|Vß'|.  The sum over all B'  of this type is bounded by 0 (2p/222n2r) 
             (since the set of size 22n is covered by at most 2r  layers).  Now consider small Vb>'s 
            that contain at most  л/Щв11 elements.  For them Ub>  x  Vb>  covers at most 0(2P) 
            elements, and for 2q  different B'  we have in total 0 (2p+q)  pairs.
                 Therefore, if p + q < 2>n — O(logn) and (p/2) + 2n + r < 3n — О (log n), then a 
            random pair (A, В) cannot be served by any of B'.  (We should note also that the 
            set  of pairs that  are served can be enumerated if we know n,p,q,r,  i.e.,  O(logn) 
            bits of advice.)  The second inequality can be rewritten as p + 2r < 2n; it is a bit 
            worse than the bound p + r < 2n that appeared in our first argument, but we still 
            get the bound 0 (r), as the theorem claims.
                 This finishes the third proof of Theorem 237.
                 Remark.  The geometric proof provides a simple set of pairs where most of the 
            pairs satisfy the statement of the theorem.  So it gives a stochastic (in the sense of 
            Section 14.2) pair with required properties.
                The same result can be achieved by some modification of the second  (proba­
            bilistic) proof.  We have said that a mapping is covered if something is true for all 
            pairs (of certain type).  Let us weaken this restriction and say that U is covered if 
            the same condition is true at least for the half of the pairs.  To prove the existence 
            of U that is not covered, we can use the following (trivial)  probability bound.  If 
            each of 2 k independent events has probability less than 1/16, then more than 2 k~l 
            events happen with probability at most 22  • (1/16)2  /2 = 2“2  .  So we may replace 
            1/2 by 1/16 in the argument and continue the proof as before.  In this way we get 
            a simple set U where half of the elements have the required properties.
                 (Another approach is also possible.  Instead  of considering  S',  T and  all  the 
            admissible triples in parallel, one can prove that with high probability for a random 
            U the fraction of pairs when (4) is true is small.  These small fractions and small 
            probabilities are then added for all triples from M.)
                                  CHAPTER 13
                           Information and logic
                       13.1.  Problems, operations, complexity
            In this  chapter we define a problem  as an  arbitrary  (finite or infinite)  set  of 
         binary strings.  The elements of this set are called solutions to that problem.
            Why such strange terminology?  Generally speaking, having a problem, we need 
         to solve it, i.e., to find its solution.  We assume that a solution can be represented 
         by text (written in some formal language), i.e., by a binary string (assuming some 
         natural  encoding  is  used).  We  will  measure  the  amount  of information  in  the 
         solutions ignoring all other aspects, so we identify the problem with the set of its 
         solutions.
            By complexity of a problem A we mean the minimal complexity of its solutions, 
                             C(X) =  min{C(i) I  X G  X}.
         As usual, the empty set, i.e., the unsolvable problem, has complexity +oo.
            For example, the complexity of a singleton  {x}  is just the complexity of the 
         string x.  For a less trivial example, recall that in Section 1.2, we considered  (for 
         a given n)  the problem  “find a natural number  к  ^  n”.  The complexity of this 
         problem was denoted by С^(п).  Now we can say that we consider the problem
            n”, whose solutions are natural numbers к ^ n, and its complexity.  (Formally, 
         we have to speak about binary representations of those numbers.)
            Let  X  and  У  be  two  problems.  We  can  consider  the  problem  “solve  both 
         problems X and У”, as well as the problem  “choose one of the problems X and 
         Y and solve it”.  The solutions for the first problem ( “X and У” ) are pairs (w, v) 
         where и is a solution to X and v is a solution to Y.  The solutions to the second 
         problem (“X or Y”) are solutions to one of the problems X, Y plus a special tag 
         that  says which of the two problems we are trying to solve.  So we come to the 
         following formal definitions:
                        XAY = {[x,y] \xeX,yeY},
                        X У Y = {[0, ж] I ж G A} U {[1,у] I у G Y}.
         Since we want the problems to be sets of strings, we use some (computable one-to- 
         one) encoding [x,y] for the pair (x.y).
            At first glance, these definitions do not look interesting.  Indeed, the complexity 
         of {æ} A {y} is just the complexity of the pair (x, y), and the complexity of {x} V {y} 
         equals min(C(x),C(y)) + 0(1).  In general, the complexity C(X V Y) is equal to 
         min(C(A), С(У)) + 0(1) for every two problems X and Y (not only for singletons). 
         More interesting examples will appear later.
            The problem А Л У is called the conjunction of problems A and Y while the 
         problem A V У is called the disjunction of A and У.
                                      401
            402                        13.  INFORMATION  AND  LOGIC
                One may define disjunction in a different way.  Imagine a teacher who gives an 
            exam with two questions and says that for a passing grade it is enough to answer 
            one of them.  Assume that the teacher gets a student’s paper where both questions 
            are answered, but only one answer is correct.  Usually it is still enough to pass the 
            exam.  This corresponds to the following formal definition:
                                    XVY = {[x, y] I x G X or y G Y}.
            We call this operation pseudo-disjunction.  Its complexity is the same as for dis­
            junction (up to 0 (1), as usual), but these two problems are essentially different (see 
            below).
                 333 Prove that C(XvY) = C(X V Y) + 0(1).
                Yet another (intermediate) interpretation of disjunction will be the union of X 
            and Y ; in this version we are required to give a correct answer but are not obliged 
            to specify which question we are trying to answer.
                The conditional complexity C{y\x) can also be understood as the complexity 
            of some problem.  Informally, we consider the problem  “transform x to y".  More 
            formally,  this problem can be defined as {x}  —>  {г/},  where X —ï Y  (for any two 
            problems X and Y) is the set of all programs that convert every solution to X into 
            some solution to Y.
                Here we fix some programming language where programs (as well as their inputs 
            and outputs) are binary strings.  Let us denote by \p]{x) the output of program p on 
            input x.  If the computation of p on x does not terminate, then \p](x) is undefined. 
            We assume that the programming language is universal (every computable function 
            can be represented as a program).  Moreover, we assume that it allows a computable 
            translation from any other programming language (this property is called the Gödel 
            property, see [184]).  Then we let
                     X —> Y = {p I Vx (x G X  =Ф-  [p](x) is defined and [p](x) G Y)}.
            In fact we have already used this approach.  In section 6.4 we defined C(x     n) 
            as  the  minimal complexity of a program that  produces x on every input  к such 
            that к ^ n.  In our new notation, C(x     n)  = C({m G N | m ^ n) -» {x}).  Our 
            initial example, the complexity of {x} —> {y}, is equal to the conditional complexity 
            C(y |x) + 0 (1).
                Some theorems of Chapter 12 can be now stated in terms of problem complexity.
                For example, the solutions to  (x —> y) A (y —> x)  (we write  {x}  as x to sim­
            plify notation)  are pairs  [гг, г;]  where program и transforms x to у and program v 
            transforms у to x.  As we have seen,  the complexity of this problem is equal to 
            max(C(x|y), C(y\x)) + 0(logO(x,y)).
                Here is one more example.  The solutions for the problem (x —> z) A (y —> z) 
            are pairs of programs  [гг, v] such that и maps x to z and v maps у to 2.  We have 
            shown that the minimal possible complexity of such a pair is
                                max(C(z\x),C(z\y)) + 0(logC(x, y, z)).
                 334  Find the complexity of the problem а —> (b —ï c) where a, b, c are strings
            (singletons).
                (Hint:  This problem is equivalent to (a Ab)   c.)
                          13.2.  PROBLEM  COMPLEXITY AND  INTUITIONISTIC  LOGIC          403
                 335  Find the complexity of a A (b —» c)  (with logarithmic precision).
                (Answer:  C(a) + C(c\a, b).  Hint4.  Given a and a program that maps  [a. b]  to 
            c,  we  can convert  b to c.  In the opposite direction,  let  us add to the  (supposed) 
            answer the value of C(b\a).  Then we get C(a,b,c).  So it is enough to show that 
            the triple a, b, c can be reconstructed from the set of the following objects:  a, the 
            program that converts a to 6, and the program that converts b to c.)
                 336 Prove that the complexity of (x V y)     (zVy) is 0(1), but the reverse 
            implication (x\/y) —> (x V y) has the same complexity as x V y, up to an O(logn) 
            additive term, if x and y are strings of length at most n.
                (Hint:  Let p be a solution to  (xVy)  —>  (x V y).  We say that a pair  (u, v)  is 
            compatible with p if и and v are strings of length at most n and for all strings w of 
            length at most n both values [p]([w, w\) and [p]([tu, u]) are solutions touV r.  Then 
            for every pair (w, v) compatible with p, we have either и = x or v = y.)
                 337 Show that the problem
                                       ((zVy)    (x V y)) -> (xVy)
            has complexity O(logn) if x and у are strings of length at most n.
                The last  two  problems show  the difference  between disjunction  and  pseudo­
            disjunction.  They show, in particular, that problems xVx and x V x differ substan­
            tially (although their complexities are close).  The latter problem xVx is equivalent 
            to x.  On the other hand, the problem x\/x is not equivalent to x, as the complexity 
            of the problem  (x\/x)  —> x is close to the complexity of x itself and thus can be 
            arbitrarily large.  (In the next section, we will define formally what it means that 
            the two problems are equivalent.)
                Historical remarks.  The study of operations  Л, V, —>  on problems goes back 
            to Kolmogorov [76]  and Kleene [75].  The complexity of problems obtained from 
            singletons by these operations was studied in  [182]  and  [142].  The formula con­
            sidered in Problem 335 is from [142].  Problem 336, although inspired by [182], is 
            presumably new.
                        13.2.  Problem complexity and intuitionistic logic
                The problem X  —>  Y  has  the  following property:  If both  problems  X  and 
            X —»• Y are simple, then Y is simple, too.  Moreover,
                                      C{Y) < C(X) + C{X -+У).
            This  inequality  is  true  with  precision  0(logC(X))  (or  0(\ogC{X  —>  Y))).  It 
            generalizes the inequality
                                         C(y) < C(x) + C(yjx)
            (which is true for all strings x  and y,  with logarithmic precision).  Moreover,  we 
            can add Y in the left-hand side:
                                    C(X A Y)^ C{X) + C(X ^  Y).
            Note, however, that the reverse inequality is not true anymore (recall that reverse 
            inequality holds for singleton sets X, Y, i.e., for strings).
                 338  Find problems (sets) X and Y such that C(X A Y) is significantly less
            than C(X) + C(X      Y).
          404                   13.  INFORMATION  AND  LOGIC
             (Hint:  Let X be the set of all random (incompressible) strings of length n, and 
          let Y be the set of all random strings of length 2n.)
             As we have said, the idea to consider Л, V, —» as operations on problems (instead 
          of statements)  goes  back to  Kolmogorov  [76]  and Kleene  [75].  They used it  to 
          construct an interpretation for intuitionistic propositional calculus (IPC); see, e.g., 
          the textbook [200]  for more information about IPC.  In this chapter we consider 
          the  relation  between  provability  of a  formula  in  IPC  and  the  maximal  possible 
          complexity of problems generated by that formula.
             Let  Ф(p, q,...)  be a propositional formula with connectives Л, V, —> and with 
          variables p,q,....  Let X, Y,... be arbitrary problems (=sets of strings).  Substitute 
          X,Y,... for variables p, q,..., and let Ф(Х, Y,...) denote the resulting problem.
             There exists the following subtle problem regarding this definition.  Actually, 
          operations on problems depend on the choice of a pairing function x, y —> [x,y\ (con­
          junction and disjunction), on the choice of a programming language (implication), 
          and, finally, on the choice of the tags 0,  1  (disjunction).  However, this dependence 
          is quite weak:  Different choices lead to the problems whose complexities differ only 
          by 0 (1).
             More formally,  let  Ф(р, ç,...)  be  an  arbitrary  propositional  formula,  and  let 
          X, Y,...  be arbitrary problems  (sets).  Let  Ф'(Х, Y,...)  and  Ф"(Х, Y,...)  be two 
          problems obtained from Ф and X, Y,... by using different pairing functions ([x,y]', 
          [x,y]”),  different  programming languages  (U'(p,x)  =  [p]'(x),  U"(p,x)  =  \p]"(x))., 
          and, finally, different tags  (a',b' and a",b")  in the definition of disjunction.  Then 
          the difference between the complexities of problems Ф'(Х, Y,...) and Ф"(Х, Y,...) is 
          0(1).  (Recall that we assume that pairing functions are computable bijections and 
          programming languages are universal and have the Gödel property, in particular, 
          translation algorithms in both directions exist.)
             This claim is easily proved by induction.  More specifically, for every formula Ф 
          we construct by induction two computable functions:  ff2 that maps every solution 
          for the problem Ф'(Х, Y,...) to some solution for Ф"(Х, Y,...); and f21  that maPs 
          every solution for Ф"(Х, Y,...) to some solution for Ф'(Х, Y,...).
             For the propositional variable Ф both functions f ®2  and f21  are identity func­
          tions.  If Ф is  Ф Л 0,  and for Ф and 0 both functions are already constructed, we 
          define fi2  as follows.  The input string s is represented as s = [u, v]', then f^2  and 
          f ®2 are applied to и and v, respectively.  Finally, we apply the other pairing function 
          to the resulting strings.  The function f2l  is defined similarly.
             The case of disjunction is entirely similar.
             Now assume that Ф = Ф —> 0.  Then the function f ®2 can be defined as follows. 
          Consider the (computable) function V(s,b) = f^([s]'(f21(b))).  If s is a solution to 
          Ф'(Х, Y,...) —> 0'(X, Y,...)  and & is a solution to Ф"(Х, Y,...), then V(s, b) is a 
          solution to 0"(X, Y,...).  Consider Y as an interpreter of a programming language. 
          As U" has the Gödel property, there exists a translation algorithm that converts 
          Y-programs to [/"-programs.  Therefore there exists a total computable function 
          t:  £ —> £ such that  [t(s)]//(&) = V(s,b) for all s,b.  This function t can be used as
          гф
          J12-
              339 Provide details to this argument.
             Now we can relate the complexity of problems expressed by formulas to IPC: 
          If  a  formula  Ф(p,q...)  is  provable  in  IPC,  then  the  complexity  of the  problem
                             13.3.  SOME  FORMULAS  AND THEIR COMPLEXITY              405
            Ф(Х, Y,...) is bounded by a constant (depending on Ф but not on X,7,...).  More­
            over,  there exists  a string s  that  is  a solution to the problem  Ф(Х, Y,...)  for all 
            X, Y,....  This was shown essentially by Kleene by simple induction on the length of 
            the derivation of Ф(р, q...).  For example, assume that Ф (p,q...) is the IPC axiom 
           p —> (q —»■ p).  Then s is the program  “transform a given string x into a program 
           that outputs x for every input”.
                340  Complete this argument, and show that for each formula Ф(p, q...) prov­
           able in IPC there exists a string s that solves Ф(Х, Y,...) for all problems X, Y,....
               Surprisingly,  a kind of reverse statement is also true:  If a formula Ф(p, q,...) 
            without negations is not provable in IPC, then the complexity of Ф(Х, Y,...) is not 
           bounded (and grows linearly, as the following theorem states):
               Theorem  238.  Let Ф (ti,...,tk)  be a propositional formula with connectives 
           Л, V,—> (no negations and no logical constant _L ).  Assume that Ф  is not provable 
            in IPC.  Then there exists e > 0 and a sequence of finite non-empty sets X ™,..., X\? 
            (for n = 1,2,...) that contain only strings of length at most n, such that the com­
           plexity of Ф(Х^,..., X£) is at least en for all sufficiently large n.
               This is the main result of this chapter.  We will prove it modulo some result 
           about formulas that are not provable in IPC.
               Historical remark.  The first non-constant lower bound for Ф(Хр,... ,Х%) for 
           formulas that are not derivable in IPC was shown in [42].  The linear lower bound, 
           as in Theorem 238, is due to A. Chernov [38].
                           13.3.  Some formulas and their complexity
               In the proof of Theorem 238, we use as a tool some bounds for complexities 
           of problems obtained by substituting singletons in non-provable formulas.  Some of 
           these bounds are of independent interest (e.g., the bounds we already mentioned). 
           Let us start with more examples of this type.
               First, let us consider Peirce’s law,
                                         ((p   q)   p)   p.
           Peirce’s  law is  provable  in  classical  propositional  logic  (it  is  true  for  all  Boolean 
           values of its variables), but it cannot be proved in IPC. Thus, by Theorem 238, for 
           all n we can find non-empty sets X, Y of strings of length at most n such that the 
           complexity of the problem ((A —»• Y) —»• X)   X is higher than en.  However, it 
           turns out that the complexity of the problem  ((x —* y)  —* x)  —> x  (as usual,  we 
           write just x for singleton  {x})  is  O(logn)  for all strings x, y of length at most n 
           (Theorem 239 below).
               There is no contradiction here:  The complexity of ((A —»• Y)  —»• X)  —> X is 
           small  for  all  singletons  X  and  Y.  However,  it  may  be  large  for  arbitrary  finite 
           non-empty sets X and Y.
               Theorem 239.  The complexity of the problem ((x —> y) —> x) —> x is O(logn) 
           for all strings x,y of length at most n.
               PROOF.  It  is  enough  to  provide  an  algorithm that  gets  n  and  a solution  to 
           (x —> y) —> x and outputs x.  This algorithm works as follows.  Let p be a solution 
           to  (x  —>  y)  —>  x.  Let  5  denote  the  set  of all  strings  of length  at  most  n.  For 
           every total function r: 5 —*  5  (there are finitely many of them),  let us fix some
            406                        13.  INFORMATION  AND  LOGIC
            program lT that computes r.  We say that a pair (u,v) £ S x S is compatible with 
            p if \р]{1т) = и for all t: S' —> S such that t(u) — v.  By assumption, the pair (x, y) 
            is compatible with p.  Given p and n, we can enumerate all pairs compatible with 
            p.  We claim that the first component и of the first (in fact, every) compatible pair 
            (u,v)  is  equal to x.  (So we can find x given p and n,  as we promised.)  Indeed, 
            assume that u/i.  There exists a function т such that т(х) = у and t(u) — v. 
            Then p should produce both и and x for the input lT, a contradiction.           □
                A careful reader might notice that this argument is not entirely complete.  We 
            have used  implicitly  that  for  any  given  finite  function  т  (presented  as  the  table 
            of its  values)  one  may  effectively  find  its  program  lT.  This  is  a corollary of our 
            assumption about programming language (the list of values can be considered as a 
            program in some other language, and that program can be effectively translated to 
            our language).
                Note also that we could restrict ourselves to a smaller class of functions (e.g., 
            linear functions ax + b if S is enriched with a field structure).  We only need that 
            for every two different points, and for every two prescribed values in these points, 
            there is a function in the class that has the required values in those points.
                 341 Prove that  in  the  statement  of the  previous  theorem  we  can  replace
            O(logn) by 0(\ogk), where к — max(C (x), С (y)).
                (Hint:  Consider the shortest programs for x and у instead of x and у them­
            selves.  Then S can be replaced by the set of all strings of length at most k.  The 
            program lT works as follows:  for input и it searches for the first program p of length 
            at most к that produces u, applies т to p, and then decompresses the result.)
                This theorem, together with the inequality
                               C(Y) < C(X) + C(X —► Y) + 0(logC(X)),
            implies that C(x) < C((x —> y) —> x) + O(logn) for all strings x and у of length at 
            most n (and hence, C((x —> y) —> x) — C(x) + O(logn), as the reverse inequality 
            is trivial).
                It  is worth noting that there exist formulas A(p,q)  and B(p,q) such that the 
            complexity of B(x,y) never exceeds significantly the complexity of A(x,y)  (for all 
            strings x and y), but the implication A(x, y) —> B(x, y) has rather high complexity.
                An example of this kind is a pair of formulas  (x —> у)  —> у and x V y.  (By 
            the way,  they are classically equivalent.)  As we will show,  their complexities can 
            differ  at  most  by  O(logn)  for  n-bit  strings,  but  the  complexity  of the  problem 
            ((x —У у) —У у) —ï {x V y) could be as high as n.
                To show this, consider first the problem (x —> y) —> y.
                Theorem  240.  The complexity of the problem (x —>• y)  —»■  y  is equal to the 
            complexity of the problem х\/ y (up to О (log n)-additive terms, for strings x and у 
            of length at most n).
                (Recall that the complexity of x V у is min(C'(a:), C(y)) + 0(1).)
                Proof.  To prove this, we present  (1)  an algorithm that transforms every so­
            lution to ж V у into some solution to (x —> у) —> у, and (2) an algorithm that gets a 
            solution to (x —> у) —> у and O(logn)  bits of additional information and produces 
            some solution to x V y.
                             13.3.  SOME  FORMULAS  AND THEIR COMPLEXITY              407
               The first algorithm gets [0, x] or [1 ,y\ and should produce a program that maps 
            every solution to (x —>■ y) to y.  If the input is [1, y], we generate the program that 
            outputs y (without even reading its input).  If the input is  [0,x], we produce the 
            following program:  apply the solution to (x —>■ y) (given as input) to x, and output 
            the result y.
               The second algorithm is more interesting.  Given a solution to  (x —> y)  —>■ y, 
            a number n  (an upper bound for lengths of x and y),  and one additional bit of 
            information (see below), the algorithm outputs some solution to x V y.
               Let p be a given solution to (x —> y) -A y.  Let S stand for the set of all strings 
            of length at most n.  For every function r : S —> S, we can effectively find a program 
            Ij-  that computes r.  This time we say that a pair (u,v) £ S x S is compatible with 
           p if \p](lT) = v for all r such that t(u) = v.
               By definition, the pair (x, y) is compatible with p.  However, other pairs could 
            be compatible with p also.  The main point is that for every two compatible pairs 
            {u'jv')  and  (u,,,v"),  we  have  either  u'  =  u"  or  v'  —  v".  Indeed,  assume  that 
           и' Ф u".  Then there exists a function r such that t{u') = v' and r(u") — v".  By 
           definition p(lT) should be equal both to v' and v".  So v' = v" unless u' — u".
               Knowing p and n, we can enumerate all pairs compatible with p.  Consider the 
           first pair in this enumeration.  As we have shown, either и = x or v — y, but we do 
           not know which of these two cases happens.  This is why we need an additional bit: 
           we output и or v depending on the value of that bit.                        □
               This argument can be generalized to prove the following statement:
               Theorem 241.  The complexity of the problem (x —»• y)  -a  z  (with O(logn)- 
           precision for strings x, y, z of length at most n) coincides with the complexity of the 
           problem z V (x Л (y —> z)).
               As we have seen in Problem 335, the complexity of the latter problem is equal 
           to min(C(,z), C(x) + C(z |x, y)).
               PROOF.  It is enough to provide two algorithms.  The first one converts every 
           solution  for  the  second  problem  into  a solution  for  the  first  problem.  The  sec­
           ond algorithm gets a solution to the first problem  and additional 0(\ogn)-bits of 
           information, and it produces a solution to the second problem.
               We start with the first algorithm.  By definition a solution to the first problem 
           is  a  program  that  maps  every  solution  for  x  —>  у  to  z.  And  a solution  for  the 
           second problem, which is given to the algorithm, is either z or a pair (x, program 
           that converts у to z).  If it is z, we produce a program that maps everything to z. 
           And if it is x and a program p that converts у to z, then we output the following 
           program that is a solution to the first problem:  apply the given solution for x —> у 
           to x and get у; then apply p to у and get z; output z.
               The second algorithm gets a solution to the first problem, the number n and 
           one auxiliary bit of advice, and produces a solution to the second problem.
               Let p denote the given solution to  (x -A y) —> z.  Let  S stand for the set of 
           strings of length at most n.  For every function r : S —>■ S, we fix some program lT 
           that computes that function.  We say that a triple (u,v,w) £ Sx S xS is compatible 
           with p if \p](lT) = w for all r such that t(u) = v.
               By definition the triple  (x, y, z)  is compatible with p.  Given p and n, we can 
           enumerate all compatible triples; let (u,v,w) be the first triple in the order of this 
           enumeration.
             408                         13.  INFORMATION  AND  LOGIC
                 It may happen that w — z.  In this case we know z (if we get an advice bit that 
             says that this indeed the case).
                 If w ф z, then we can find both x and a solution to y —> z as follows.  First, 
             let  us  show that x — u.  Indeed,  if it were not the case, then there would exist a 
             function r that maps both x to y and и to v.  Both triples (x, y, z) and (и, v, w) are 
             compatible with p, therefore p(lT) would be equal to both w and z, contradicting 
             to the assumption w Ф z.
                 It  remains to show how to find z given у in the second case (w ф z).  This is 
             easy:  in general, the problem у —> z is easier than (x —> y) —> z, since every у can 
             be considered as a (constant) function that is a solution to x —» y.                □
                 Historical remark.  All the examples in this section are taken from [182] except 
             for Theorem 241, which was taken from [142].
                         13.4.  More examples and the proof of Theorem 238
                 Theorem 242.  The complexity of the problem ((x —» y)  —» y) —>  (x V y)  is 
             equal to min(C(x |y), C(y\x)) + O(logn) for strings x and у of length at most n.
                 In particular, if x, у are independent random strings of length n, the complexity 
             of this problem is close to n.
                 Proof.  First, it is easy to see that the complexity of ((x —> y) —> y) —> (x V y) 
             does not exceed C(y |x) + 0(1).  Indeed,  if p maps x to у and g is a solution .to 
             (x —> y) —> y, then y equals \p](q).
                 Now let  us  prove  that  the  complexity of ((x  —»  y)  —»  y)  —»  (x V y)  is  also 
             bounded by C(x\y) + O(logn).  It is enough to show that given a program p that 
             maps y to x, a solution q to  (x —> y) —> y and the number n, we can find x or y. 
             Consider again the set S of all strings of length at most n.  Call a pair (u, v) G S x S 
             compatible with q if q(lr) — v for all r: S —> S such that t(u) = v.  Obviously, the 
             pair (x, y) is compatible with q.
                 We have seen that any two pairs compatible with q have either the same first 
             components or the same second components.  This implies that either all compatible 
             pairs have first component x or all compatible pairs have second component y (or 
             both).  Indeed,  assume that there is a pair whose first  component x'  is different 
             from x.  Then its second component  is y,  so the pair is  (x',y).  For the sake of 
             contradiction,  assume that there is a pair whose second component y' is different 
             from y, and thus that pair is  (x,y').  Now the pairs  (x,y')  and (x',y) violate the 
             requirement mentioned above.
                 So let us assume that n, p,  and q are given.  We search for pairs compatible 
             with q until the first such pair (и, v)  is found.  Then we do two things in parallel: 
             (1)  we  look for other pairs compatible with q,  and  (2)  we run p on v  and verify 
             the equality p(v) = u.  One of these two things will happen:  if p(v) is undefined or 
            p(v) Ф u, then (и, v) ф (x, y) so another pair will appear.  If we find another pair 
             (u',v')  compatible with q,  then we know either x  (if v ф v', then и — x)  or y  (if 
             и ф и', then v = у).  And if we know that p(v) = u, we can be sure that и — x (if 
             и Ф x, then v = y, hence и = p(v) = p(y) — x).
                 It  remains to show that the complexity of ((x —> y) —> y) —> (x V y) cannot be 
             much less than min{C(y | x), C(x \ y)}.  We do this in the following way.  We present 
             a way to convert any program p that solves ((x —> y) —> y) —>  (x V y) into a pair 
             of programs (гх,гъ) such that either rq  maps x to у or Г2 maps у to x.  However,
                                             13.4.   EXAMPLES AND THE PROOF OF THEOREM  238                                                   409
                   there is no indication which of these two possibilities happens, so in fact we exhibit 
                   a solution to the problem
                                            (((.t -*y)^y)^ (X V y)) -»> {(x ->• y)V(y -> x)).
                   This is enough to get the required bound for the complexity.
                         Here is the idea.  Let p  be a solution to ((x —>■ y) —> y)  —>■ (x V y).  We need to 
                   convert either x  to y  or y  to x.  Both x  and y  can be used to construct a solution 
                   for (x  —> y) —> y;  indeed, y  can be converted into a program that maps everything 
                   to y,  and  x  can be used  to  convert  (x  —> y)  to  y.  Then we can apply p  to this 
                   solution and get a solution to ж V y, i.e., x  or y.  If it happens that we get the other 
                   string (not the one we started with), we succeed in transformation of x  to у  or vice 
                   versa.  But why may we hope to be so lucky?
                         We apply a tool from computability theory.  Fix a pair of disjoint coinputably 
                   enumerable sets А, В  C N that cannot be separated by a decidable (=computable) 
                   set.  The latter means that every decidable set that contains  A  has a non-empty 
                   intersection with В  (and vice versa).
                         For  every  natural  i  and  for  every two  strings  u,v,  we  consider  the  program 
                   qi(u,v)  that works as follows (its input s  is considered as a program):
                                 qi(u,v) on input s:
                                           i £ A: output v,
                                           i  £ B: output  [s](w);
                                           [s](u) — v:  output v — [s](w).
                   This means that g?(u, v)  enumerates A, В  and applies s to м in parallel,  waiting 
                   until one of the three events (listed before the colons) happens, and then performs 
                   the described action.  Note that the first and second conditions are disjoint  (since 
                   A  and В  are disjoint); the third condition is not disjoint with the first two, but the 
                   action is the same anyway.
                         The construction guarantees the following properties:
                         (1)  if i £ A,  then for every и  the program qi(u, y) is a solution to (x  —> y) —> y\
                         (2) if г £ В,  then for every v  the program qi(x, v) is a solution to (x  —>■ y) —> y.
                         (3)  for  every i the program qi(x, y)  is a solution  to (x  —»■ y)  —»■ у.
                   Therefore,  in  all three  cases \p](qi(u, v))  is a solution to  x V  у (by  our assumption
                   on p).
                         Now we can present programs rq  and Г2  and prove that either rq  maps x  to 
                   у  or Г2  maps у  to x.  The program rq  applies p  to qi(x,v)  for all i £ В  and for 
                   all strings v  in parallel, and waits until p  produces an output of the form [1, z]  for 
                   some z]  then rq  outputs 2.  Similarly, Г2  applies p  to qi(u,y)  for alii £ A  and for
                   all u,  and waits  until p  gives output  [0, z]  for some 2; then Г2  outputs 2.
                         The properties of qi  mentioned above guarantee the correctness of the output; 
                   it  remains to show that at least one of the events will happen.  Assume that it is 
                   not the case and [p](qi{x,v))  always starts with 0 (for all i £ В  and for all v) and 
                   \p](qi(u,y))  always starts with  1  (for all i  £  A  and for all u).  In particular,  this 
                   happens for v = у  and и = x.  Recall that  \p](qi(x,y))  is defined for all i,  and in 
                   this way we can compute a separator for A  and B,  a contradiction.                                                         □
                         The formula used in this theorem already looks quite complicated.  However, 
                   for Theorem 238 we need to go even farther and consider some generalizations of 
                   the problem ((x  —» £/) —» y)  —»• (x V y).
             410                           13.  INFORMATION  AND  LOGIC
                  Consider  arbitrary  strings              where  к  ^  2,  and  two  arbitrary  non­
             empty disjoint sets I.J c {1,..., к}.  Then consider the problem
                                              ((X -t Y)      Y)    Z,
             where X is the conjunction of singletons {uj} for i e I (i.e., the /с-tuple), Y is the 
             disjunction of all Uj for i € J, and Z is the disjunction of all singletons их,..., Uk- 
             For example, for к = 2, I = {1}, J = {2}, we get the problem ((щ                  щ) —>
             (ui V щ) considered above.  In another example we use different letters for the sake 
             of readability:
                            {{xi A x2 -»• yi V y2) -»■ У\ V y2) -»• xx V x2 V yi V y2 V z.
                  Theorem 243.  The complexity of the problem ((X —> Y) —> Y) —> Z is lower 
             bounded  (with 0(1)-precision)  by the minimal conditional complexity of щ  given 
             all other strings u i,...,
                  Proof.  This theorem generalizes Theorem 242,  and its proof is also similar. 
             We construct a tuple of к algorithms that has the following property.  For every 
             program p that solves  ((X  —ï Y ) —> Y)—ïZ, there exists rn such that the mth 
             algorithm, given p, reconstructs um from all other щ.
                  As  before,  the  mth  algorithm  uses  all  other  щ  (with  t  ф  m)  to  construct 
             solutions to  (X —y Y) —>• Y;  then p is applied to these solutions.  Note that if we 
             know all ut except um, we either know all strings in X (and can construct a solution 
             to X) or all strings in Y (and can construct a solution to T; in fact, we get several 
             solutions for Y, since Y is a disjunction of singletons, but this is not important).  It 
             may even happen that we can find solutions to both X and Y (e.g., if the missing 
             string appears neither in X nor in Y).  Having a solution to X or 7, we may (as 
             before) construct a solution to  (X —> Y)  —> Y and apply p to it.  If we are lucky 
             enough to get pair [£, ut\ with t — m as the output of p (recall that m is the number 
             of the missing string we want to reconstruct), we know urn.  (A technical note:  we 
             assume that solutions to Y and Z are pairs of the form [t,ut]; it is not exactly the 
             case since we have not defined disjunction of many problems and should consider 
             (■их V (112 V (• • • {uk- 1 V Uk) •••))) instead, but this is inessential.)
                 The only problem is to ensure that  (for some m)  this will indeed work,  i.e., 
             that we will obtain a pair [t, щ\ with t — m.  This is done again using computability 
             theory.  Instead of computably inseparable sets,  we now use a more general con­
             struction, a computable diagonal function d.  A function d from the set of natural 
             numbers to its subset S is called diagonal if for every partial computable function 
             и of the same type there exists i such that d(i)  ~ u(i).  (Here a ~ b means that 
             either both a and b are undefined or both are defined and a = b.)  A computable 
             diagonal function can be constructed as follows:  Let d{i)  be the value of the zth 
             computable function on i.
                 For S = {0,1}, i.e., for Boolean-valued functions, such a function is determined 
             by  two  disjoint  enumerable  sets  (preimages  of 0  and  1);  the  diagonal  property 
             implies that no total Boolean function can be different from d everywhere, so these 
             enumerable sets are inseparable.
                 In the proof, we will use a computable diagonal function from the set of natural 
             numbers to the set S = {1,..., A:}.  More specifically, for every к strings vx,..., Vk 
             and for every natural number i, we will construct a program qi(vx, • ■ ■ ,Vk) that has 
             the following properties:
                              13.4.  EXAMPLES  AND  THE  PROOF  OF  THEOREM  238              411
                 (1) for original strings щ ,... ,Uk and every i, the program qi(u\,..., мд,) solves 
             (A -> Y) -> У;
                 (2)  if d(i)  = m,  then qi(ui,..., щ) remains a solution to (X —> Y) —> Y even 
             after we replace um by any other string vm (i.e., ql(ul,..., um_], vm,um+b ..., uk) 
             is a solution to (X —> Y) —> Y for every vm ).
                 Let  us  finish  the  argument  assuming  that  we  can  construct  such  a program 
             qi{v i,... ,Vk).  The algorithm that reconstructs urn from the rest of the ut's works 
             as follows.  We start (in parallel) the computations of d(i) for all inputs i.  As soon 
             as an i with d(i) — m is found, we apply the given program p to all the solutions 
             to  (X —>• Y) —> Y of the form qi{u\,..., wm_i, *, um+i,..., мд.) where * stands for 
             arbitrary strings  (all these programs are solutions due to  (2)).  As soon as one of 
            p’s outputs is of type [m, *], we halt:  the second component of the resulting pair is 
             V>m-
                 As we said, property (2) implies the correctness of this algorithm (assuming it 
            terminates).  However we still need to show that this algorithm terminates for some 
            m (we do not wait forever).  To this end we use the diagonal property of d.  Assume 
            that for all m the computation does not terminate.  This means, in particular, that 
            this happens for <p(wb • • •, и к ) for all i.  Property (1) guarantees, however, that in 
            this case we apply p to a solution for (X —> Y) —> Y.  Therefore,  [p](çi(ui,..., мд)) 
            is defined and is a solution to Z for all i.  Thus the computations do not terminate 
            because the first component t of the output pairs never coincides with d(i) (for all i 
            such that d(i) is defined).  But this first component is a total computable function 
            of г, so we get a contradiction with the diagonal property.
                 It remains to construct the program qi(v\,..., v^) (for arbitrary г and for arbi­
            trary strings t»i,..., Vk).  Given s as input, this program does two things in parallel:
                 (1)  it runs the computation of d(i)\
                 (2)  it  uses  vt  for  t  G  I  to  construct  a potential solution to  A,  and  it  applies 
            s to that solution and checks whether the output of s equals [t) vt] for some t € J 
             (the  second  coordinate  coincides  with  the  the  tth  element  of the  original  tuple
                 As soon as one of these events happens (including the coincidence in (2)), the 
            program performs the following actions:
                    •  In case (1), when d{i) is defined and equals some m:
                        —  if m ^ /, we use vt  (with t G I) to construct a tentative solution to 
                           X and apply s to this solution;  the output  of s is returned  as  the 
                           output of qi(vi,..., Vk) on input s;
                        —  if m €  /  (and  therefore m ^  J),  we return the pair  [t,vt\  for some 
                           t  G  J  (to be precise,  let  us  agree that we use minimal t G  J);  note 
                           that in this case output does not depend on s.
                    •  In case (2), the program outputs the pair [t,vt] produced by s.
                Why does this work? If V\,...,vд, coincides with щ ,... ,Uk, and s is a solution 
            to  (X  —>  Y)  —>  Y,  then  the second  event  always  happens  unless the first  event 
            happens earlier; in both cases the output of дДиъ • • •, Wfc) is a solution to Z.  So in 
            this case the program qi(ui,... ,Uk) is a solution for ((A —ï Y) —> Y) —> Z.
                Now assume that d(i) = m, that each vt, except for (maybe) vrn, coincides with 
            ut,  and that s is a solution to (A —> Y) —>• Y .  In this case we do not know which 
            of the two parallel computations will stop first.  But  in both cases the output of 
                i,..., Vk) is guaranteed to be a solution to Z.  Indeed, in case (1) we do not use
            qi(v
                                             13.  INFORMATION  AND  LOGIC
              412
              the value of v.in at all.  In case (2) we do use um, but we have two reasons to believe 
              that the answer is correct (since the output of s coincides with [t. vt}), and one of 
              these two reasons still works.                                                             □
                   To prove Theorem 238, we need to make one more step and consider slightly 
              more general formulas.  Again let iq,..., Uk be a tuple of strings.  Consider several 
              (N) pairs of disjoint set of indices:
                                  h n jt = 0 ,                           1=1,...,N.
              For each I we define problems Xi and Yi as before, i.e., let Xi be the conjunction of 
              singletons щ for t G //, while Yi is the disjunction of singletons yt for t € J).  Finally, 
              let Z be the disjunction of all singletons Ui,... ,Uk-  Now consider the problem
                              ((№  —» n ) -» у,) л • • • л ((xN -> yn) -> yn)) -4 2 .
                   Theorem  244.  The complexity of this problem is not less than the minimal 
              conditional complexity of some ut relative to the tuple of all other iq,..., Uk  (with 
              0(1) -precision).
                   P r o o f.  The proof mainly repeats the previous argument.  For each I, for each 
              natural number г, and for each tuple v = (vi,... ,Vk) that coincides with щ,... ,Uk 
              except for one component, we construct a program qu(v) that solves the problem
                                                    {X^Yi)^Yh
              if d(i)  is defined and is equal to the coordinate where и and v differ, or if v = и (in 
              the latter case it is not important whether d(i)  is defined or not and what is the 
              value of d(i)).
                  For every m we consider the following algorithm that tries to reconstruct um 
              given any solution p to the problem
                              ((№  -4 U) -> П) Л • • • Л (№ v -4 Yn) -4 ВД)  -4 z
              and given all the other components of u.  We apply p to tuples (qu(v), ..., qNi(v)) 
              for all the tuples v that differ from и only in mth coordinate and all the i such that 
              d(i) = m.  If we are lucky and for some v,i the program p terminates and outputs 
              a pair [m, *] with the first component m, then the second component is um  (as we 
              wanted).
                  As  before,  the  diagonal  property  of d  is  needed  to  show  that  such  a  lucky 
              coincidence will indeed happen for some m.  More specifically, it will happen for v = 
              и and for i such that d(i) coincides with the first component of \p](qu(v), ■ ■ ■ > qNi(v)) 
              (by the diagonal property such an i does exist).                                           □
                  The propositional formulas used in this theorem are called critical implications. 
              Namely, a critical implication is a formula of the type
                             (((Pi —> Q\) —>■ Qi) Л • ■ • A ((P/v —> Q n ) —> Q n ))     P?
              where R is the disjunction of certain variables s\.... ,Sk, for each I formula Pi is a 
              conjunction of some of these variables and Qi is a disjunction of some other variables 
              (Pi and Qi have no common variables and are not empty).  Critical implications are 
              not provable in IPC; this can be easily shown using Kripke models and also follows 
              from  the  previous  theorem  (recall  that  provable  formulas  have  low  complexity). 
              It  turns out  that  critical  implications  are  universal non-provable  formulas.  More 
              precisely, the following statement is true:
                            13.5.  PROOF  OF  A  SIMILAR RESULT  VIA  KRIPKE  MODELS            413
                 Theorem  245.  Let Ф(£ь..., tm) be a propositional formula with connectives 
             Л, V, —» that is not provable in IPC.  Then there exists a number k,  propositional 
             formulas Ti,..., Tm using new variables si,..., s^  with connectives A  and V,  and 
             a critical implication J(s\,..., Sk) such that the formula
                                              *{Tu ...,Tm) ^ J
             is provable in IPC.
                 Note that formulas Ti do not use implication.
                 This result  (due to Yu. T. Medvedev) belongs to logic and proof theory, so we 
             do not include its proof in our book.  Interested readers may refer to the paper [42] 
             where the proof is given.  This statement will be used in the proof of Theorem 238.
                 P r o o f.  Let  Ф(t\,... ,tm)  be  a formula which  is  not  provable  in  IPC.  The­
             orem  245  guarantees  that  there  exist  a number  к  and  formulas T\,..., Tm  with 
             variables s\,..., Sk and a critical implication J(si,..., st) such that the formula
                                              Ф(ТЬ ..., Tm) —> J
             is provable in IPC.
                 Let us take к independent random strings щ,..., Uk of length n/c (the constant 
             c will be chosen later).  Let us substitute corresponding к singletons for variables 
             in T\...., Tm and denote the resulting non-empty sets by Xi,..., Xm.  Those sets 
             are finite,  since  formulas Ti  in Theorem  245  do not  contain  implication.  As the 
             implication Ф(Ть ... ,Tm) —> J is provable in IPC, the complexity of the problem
                                       Ф(ХЬ..., Xm) —> JK ,..., Uk)
             is  0(1).  By Theorem 244 the complexity of the problem J(wi,... ,Uk) is at least 
             n/c — 0(1).  Therefore,  the complexity of the problem Ф(Хь..., Xm)  is at least 
             n/c — 0(1), too.  This bound exceeds n/(2c) for all sufficiently large n.
                 Now we need to choose c in such a way that all the elements in Xi,..., Xm are 
            strings of length at most n.  Recall that every element of Xi,... ,ХШ is obtained 
             from singletons щ,..., ut by a fixed number of conjunctions and disjunctions.  This 
             means that we use pairing operations 0(1) times (starting from variables and tags 
             0,  1).  If we choose a pairing function in a natural way, it does not increase the size 
            of strings more than linearly,  so  an 0(l)-iteration of pairing operations will give 
            only a linear increase in size,  and for suitable  c we get  strings of length at most
             n.  It remains to note that the statement of the theorem is invariant and does not 
            depend on the choice of the pairing function.                                       □
                 Historical remark.  Theorem 242 is taken from [182].  Theorem 244 is due to 
            A. Chernov [38].
               13.5.  Proof of a result similar to Theorem 238 using Kripke models
                 In the proof of Theorem 238, we used the statement of Theorem 245 (without 
            proof).  Muchnik showed that a similar statement can be proved directly using only 
            the completeness theorem for Kripke models and IPC.
                 Theorem  246.  Let Ф(ti,...,tk)  be a propositional formula with connectives 
             Л, V, —>  not provable in IPC.  Then for every n  there exist problems X ”, ..., X£ 
             of complexity 0(n) such that the complexity of Ф(Х”,..., X£) is at least n for all 
             sufficiently large n.
         414                  13.  INFORMATION  AND  LOGIC
            This statement is weaker than Theorem 238 because now the sets Хг may be 
         (and actually will  be)  infinite,  and  only  their  complexity  (not  the  length  of the 
         elements)  is  0(n).  It  would  be  nice  to  find  a direct  proof of the  full  version  of 
         Theorem 238.
            PROOF.  Let a finite Kripke model be given such that Ф(^,... ,tk) Is false (in 
         the  root).  For  every  integer  n,  using  this  model,  we  construct  sets  Xx,...,X*,. 
         (We omit the superscript n since n is fixed in the sequel.)  Let  (K, ^) denote the 
         underlying Kripke structure (a finite partially ordered set having the least element, 
         which is called the root-, the elements of К  are called w orlds).
            Let us fix some set L of non-negative integers including zero; we assume that 
         every two different elements of L differ significantly  (say,  at least ten times).  We 
         also assume that all non-zero elements of L are much bigger than n.  The elements 
         of L are called lengths in the sequel (we will consider strings whose lengths are in 
         L).  The elements of L are split between worlds in such a way that every world и 
         gets infinitely many lengths, and this subset of L is decidable.  We assume also that 
         zero length is assigned to the root world.  The lengths assigned to a world и are 
         called и-lengths and all strings of such lengths are said to belong to the world u.
            It is easy to see that all these requirements can be satisfied (in many ways).  Now 
         we can explain the construction of the set X* that corresponds to the variable ti. 
         This set is a (disjoint) union of two parts.  The first part contains all random strings 
         from worlds where ti is true (in the Kripke model).  The second part (denoted by 
         C in the sequel) consists of all pairs  (x, у) where x and у are random strings from 
         incomparable (in К) worlds.  The second part is the same for all variables.
            Here we need a technical clarification.  A string is called random in this argu­
         ment if its complexity is not too small compared to its length.  Let us choose some 
         threshold and say that a string is random if its complexity is not less than length/10. 
         To be technically correct,  we should replace pairs  (x,y)  by their encodings  [x,y\, 
         and use tags to distinguish between elements of different types:
              С = {[x,y\ I x,y are random strings from incomparable worlds},
             Xi = {Ox \ U is true at some world v,  and x is a random string from v} 
                   U{ly\ye C}.
         Note that strings from different worlds have different lengths, so for every element 
         of Xi we can reconstruct where it came from.
            Now we prove by induction on the length of the formula Ф(£ь ..., tk) the prop­
         erties of the problem Ф(Х1?..., X^).  Essentially, we prove that this problem can be 
         described in the same way as was done for variables.  More precisely, let us consider 
         the set Хф defined as follows:  it is the set of all random strings from worlds where 
         Ф is true (in the model), plus all elements of C (added with the same precautions 
         as  before,  with  tags,  pairs,  etc.).  We  prove  that  the  problem  Ф(Х1?... ,Xfc)  is 
         algorithmically equivalent  to  the set  Хф.  This  means  that for  every  formula  Ф 
         there exist two computable functions f, g such that /  transforms every solution to 
         Ф(Хх,..., Xk) into some element of Хф and g transforms every element of Xq, into 
         some solution to Ф(Хх,..., Xk).
            The base of induction (Ф is a variable U) is trivial:  by definition Хф coincides 
         with Xi, so we let /  and g be the identity function.  For the induction step, we need 
         to prove the following equivalences:
                            13.5.  PROOF  OF  A  SIMILAR  RESULT  VIA  KRIPKE  MODELS            415
                  (A)  the set Хф V X© is algorithmically equivalent to the set ХфУ©;
                  (B)  the set Хф Л X© is algorithmically equivalent to the set ХфЛ©;
                  (C)  the set Хф —> X© is algorithmically equivalent to the set Хф_>©
                 We also use that the operations on problems are stable with respect to algo­
             rithmic equivalence.  This means that if U and V are algorithmically equivalent to 
             U'  and  V ,  respectively,  then the set  U V V  (U A V,  U —>  V)  is  algorithmically 
             equivalent to the set U' V V   (resp.  U' A V7,  U' —> V).  This is a straightforward 
             consequence of the definition.
                 (A)  By definition,  ХфУ©  is  the union of the sets Хф  and X©.  So,  given an 
             element of Хф  or X©,  it  is easy to produce an element of ХфУ©.  On the other 
             hand,  if we  get  an  element  in the  union of Хф  and  X©,  we  can find  out  which 
             world it comes from (because the lengths are different) and construct a solution to 
             Хф V X©.
                 (B) By definition, ХфЛ© is the intersection of the sets Хф and X©, and ХфАХ© 
             is their Cartesian product.  Having some element x in the intersection, we can easily 
             produce an element of the Cartesian product, namely,  [x,x].  On the other hand, 
             assume that we get some element [x, у] of the Cartesian product of Хф and X©.  If 
             at least one of x, у belongs to C (which can be decided looking at the structure of x 
             and y), then this element belongs to the intersection of Хф and X©  (both contain 
             C).  If not,  then x and у are random strings from the worlds where Ф and О are 
             true (in the model).  Looking at the lengths, we can find these two worlds, say, и 
             and v.  Now we distinguish two cases:
                 (1)  The worlds и and v are not comparable in K.  Then we produce the pair 
             [x, y] which belongs to C by definition (and therefore belongs to the intersection of 
             Хф and X©).
                 (2)  The  worlds  и  and  v  are  comparable,  e.g.,  и  precedes  v.  Then  (due  to 
             monotonicity)  the  formula  Ф  is  true  also  in  v,  so  the  element  у  belongs  to  the 
             intersection of Хф and X©.
                 (C)  This is the central part of the proof.  We have to show that,  given some 
             element of Хф_>©, we can find an element of Хф -©• X© and vice versa.  We start 
             with the first claim.
                 Assume that a string x 6 Хф_>©  is given.  We need to find some solution to 
             Хф —»■ X©.  In other words, given x and some element y G Хф, we need to find an 
             element of X©.  If either x or у belong to C, we output this element of C (recall 
             that C is a part of X©).  Now assume that both x and у do not belong to C.  Then 
             x is a random string from some world и where Ф —»■ 0 is true, and у is a random 
             string from some world v where Ф is true (and both worlds can be reconstructed 
             from the lengths of x and y).  Then three cases are possible:
                 (1)  If v  precedes u,  then Ф  is true in и also.  Therefore,  0  is true in и  (since 
             the implication Ф —> 0 is true in и).  So x belongs to X©  (and we may output x).
                 (2)  If и precedes v, then 0 is true in v, since both Ф is true in v and Ф —> 0 
             is true in v  (monotonicity).  Therefore у belongs to X©  (and we may output y).
                 (3) If и and v are incomparable, then the pair [x, y] belongs to C (and, therefore, 
             X©), so we output [u,v].
                 It  remains to show that,  given an element r of Хф  —> X©, we can construct 
             some element of Хф_>©.  So we assume that a program r is given that transforms 
             every string in Хф into some string in X©.
                     416                                             13.  INFORMATION  AND  LOGIC
                            If the formula Ф —>• 0 is true in the root world, the empty string A belongs to 
                     АГФ_>.© and we output Л.
                            Now let  us  assume  that  Ф  —>  0  is  false  in  the  root  world.  Then  (by  the 
                     definition of implication  in Kripke models)  there exists some world и where Ф is 
                     true and 0 is false.  Let us choose some big length / (much bigger than the length 
                     of r)  assigned to u.  We know that  program r on every random string of length 
                     /  terminates and produces some element of Xq.  However, we do not know which 
                     strings of length / are random and which are not.  So we apply r (concurrently) to 
                     all strings of length /.  If some string s appears more that 2Z/2 times as r’s output, 
                     then we can be sure that  s  belongs to Xq.  Indeed,  the number of non-random 
                     strings of length / is much smaller than 2Z/2, so s should be also an output of r for 
                     some random input.  Therefore, s belongs to Xq C Xp^©.
                            Let  us  try  to  prove  that  there  is  a string s  that  has  many  (more than  2Z/2) 
                     preimages.  Let x be a random string of length /,  and let s be the output of r on 
                     input x.  The complexity of s does not exceed (up to a logarithmic term) the sum 
                     of lengths of x and r, so it is significantly less than 21.  Moreover, s belongs to Xq, 
                     so  s equals either 0£ or  l[y,z]  where t,y,z are random strings of allowed lengths. 
                     Therefore, the lengths of t,y, z cannot exceed /.  (If these strings are longer than /, 
                     they should be much longer due to the choice of lengths and, being random, they 
                     should have large complexity compared to /.)  For t (if the output has form 0£) we 
                     know more:  the length of t should be strictly less than I, since t comes from a world 
                     where 0 is true, and this world differs from и (recall that 0 is false in и).  So the 
                     length of t does not exceed //10.  For у and z (if the output has form 1 [y, z]) we also 
                     have additional information:  у and z come from incomparable worlds, so they have 
                     different lengths.  Therefore, at most one of them can have length I, and the other 
                     one should be short:  either the length of у or the length of z does not exceed //10. 
                     If we knew that both у and z always have lengths at most //10, we could conclude 
                     that the set of all outputs of p on random inputs of length / has cardinality much 
                     less than 2Z/2, so some element has a lot of preimages (more than 2Z/2).
                            Since we cannot guarantee this  (that one of strings у and z can have length 
                     /),  this argument does not work.  Still we can save something from our reasoning. 
                     There  are  two  possibilities:  either  (1)  some  s  has  more  that  2Ч2  preimages,  or 
                     (2)  there exists у such that for more than  21!2  inputs x  of length /  the program 
                     r outputs an element of the form 1 [y,z\ or 1 [z,y\  for that у and some z of length 
                     /.   (The values of z can be different for different  inputs.)  In the second case the 
                     string у must be a random string from some world incomparable with u.  Indeed, 
                     at least one of the inputs that are transformed by r into 1 [y, z] or 1 [z, y] is random, 
                     so the output should belong to AT©.  That is,  у and z are random and belong to 
                     incomparable worlds.  The string z comes from world и (because z has length /); 
                     therefore у comes from some world incomparable with u.
                            Let us summarize our findings.  We apply r to all strings of length / and find 
                     either s or у with the properties described.  In the first case (some s has a lot of 
                     preimages) we know what to do:  s belongs to Xq С Хф_».©.  In the second case we 
                     obtain a random string у from some world v that is incomparable with u.  It may 
                     happen that  Ф  —>  0  is true in v;  then we are done.  If not,  there exists a world 
                     щ  above v where Ф is true and 0 is false.  We can repeat our argument for щ 
                     and get either some element of Xq or a random string from some worlds Vi that is 
                     incomparable with щ.  Note that v\ cannot be below v, since in this case it would
                    13.6.  COMPLEXITY  NOT REDUCIBLE TO COMPLEXITIES  OF TUPLES      417
            be below щ.  Therefore, either vi  is incomparable with v  (and we have a pair of 
           random strings from incomparable worlds) or is strictly above v.  In the latter case 
           either Ф —> 0 is true in v\, or we can repeat our arguments for tq  and so on.  This 
           process cannot  be infinite  since  model К is finite  and cannot contain an infinite 
           increasing sequence of worlds.
               We have finished our induction argument showing that the set Ф(^1,..., Xk) is 
           algorithmically equivalent to Хф.  By assumption, Ф is false in the root world, so all 
           strings in Х Ф have length much bigger than n, and the complexity of Ф(Ах,..., Xk) 
           exceeds n.  However, we need to guarantee also that complexities of all Xi are 0(n). 
           To achieve this, we need to adjust our argument:  We may assume without loss of 
           generality that К has maximal element where all variables are true (this does not 
           change the truth values in other worlds).  Also we may assume that length 2n is 
           assigned to this maximal world.  Then every random string of length 2n belongs to 
           all Xi.                                                                    □
               Historical remark.  The  proof of Theorem  246  (due  to  An.  A.  Muchnik)  is 
           published here for the first time.
                     13.6.  A problem whose complexity is not expressible 
                              in terms of the complexities of tuples
               We have found (with logarithmic accuracy)  complexities of several problems 
           that  can  be  obtained  from  singletons  using  the  operations  Л, V, —>.  The  reader 
           can  wrongly deduce  that  complexity of every problem obtained from  singletons 
           {x},{y},...  using  these  three  operations  can  be  expressed  through  complexities 
           of x,y,..., their pairs,  triples,  etc.  (with some decent, say logarithmic,  accuracy). 
           This is not  the case.  The problem  (x —> z) A (y —> z)  (whose complexity equals 
           the maximum of C(z\x) and C(z\y), with logarithmic accuracy) is already close 
           to the border of the area where this is possible.  It turns out that the complexity 
           of a slightly general problem  (a —> c) A (b —> d)  cannot be expressed through the 
           complexities  of a, b, c, d,  their  pairs,  triples,  etc.  This  is  stated  in  Theorem  247 
           below.
               We start with several simple observations on the complexity of this problem.
                342  Prove that for all strings a, b,c, d the following inequalities  hold  (with
           logarithmic accuracy) :
                           C({a —► c) A (6 —► d)) ^ C(c\a) + C(d\b),
                           C((a —> c) A (6 —> d)) ^ C(d\b, c) + C(c),
                           C((a —>■ c) A (& —>• d)) ^ C(c\a, d) + C(d),
                           C((a —> c) A (b —>• d)) ^ C(b,c,d\a) — C(b\a,c), 
                           C((a —> c) A (6 —d)) > C(a, c,d\b) -  C(a \b,d).
                343  Problem 342 establishes three upper bounds and two lower bounds for 
           the complexity of the problem (a —> c)A (b —> d).  Prove that there exists a sequence 
           of quadruples (an, &n, cn, dn)  of strings of lengths 0 (n) such that for all n each of 
           the upper bounds is larger than each of the above lower bounds by n — 0(1).
               (Hint:  Let  a  —  d  and  b  =  c where  a,b  are  independent  random  strings  of 
           length n.)
        418              13.  INFORMATION AND LOGIC
           Theorem  247.  For some positive 5  there  exist two sequences  of quadruples 
        än,bn,cn,dn  and an,bn,cn,dn  of strings of lengths 0 (n) such that the complexity 
        of the  problem  (än  —>■  cn)  Л  (bn  —>  dn)  exceeds  the  complexity  of the problem 
        (an —> c„)A (bn —>■ dn) at least by 5n.  On the other hand, the difference between the 
        complexities of strings än,bn,cn,dn  and the complexities of strings an,bn,cn,dn is 
        at most O(logn),  and the same thing holds not only for strings themselves but also 
        for all their pairs,  triples,  and the quadruple.
          Proof.  Geometric version.  We start with the following observation.  For the 
        quadruple än,bn,cn, dn (from the statement of the theorem) both lower bounds from 
        Problem 342 for complexity of (a —> c) A (b —> d)  must  be strict:  The difference 
        between the complexity of (ân —> cn) A (bn —>■ dn) and both lower bounds must be 
        more than 6n.  We will first exhibit such a quadruple.  Then we will find another 
        quadruple with  the  same  complexities  (of its  components,  their  pairs,  etc.),  for 
        which this difference is o(n).
          We use the same geometric arguments as in the construction of words whose 
        mutual  information  cannot  be represented  as  a string  (see  page  357).  Consider 
        the three-dimensional affine linear space over the field of cardinality 2n.  We will 
        consider lines and planes in that space.  The number of points in the space is 23n, 
        the number of lines is about 24n:  every line can be identified by its arbitrary two 
        different points, there are about 26n of pairs of different points and every line can 
        be represented in  about  22n  different ways  by  a pair  of its points.  The number 
        of planes is about 23n:  every plane can be identified by three different points, the 
        number of triples of different points is about  29n, and every plane has 22n points 
        and hence has about 26n triples of different points.  Let (a, b) be any random pair of 
        intersecting lines, c their common point, and d the (unique) plane containing both 
        lines à, b.
          Then C(ä, b) — 7n (with logarithmic accuracy).  It is not hard to check that for 
        the quadruple à, b, c, d both lower bounds from Problem 342 are equal to n (with 
        accuracy O(logn)).
          Let us show that
                         C((ä —>■ с) Л (b —>■ d)) ^ 1.5n
        (with precision O(logn)).
          Let 7 be a solution to the problem  (â —> c) A (b ->  d).  Then 7 is a pair of 
        programs {a, ß) such that a transforms ä into c and ß transforms b into d.  Let S 
        stand for the set of all pairs of different intersecting lines a, b such that the program 
        a transforms a into the common point of a and b and the program ß transforms b 
        into the plane containing both a and b.  For any given triple (a, ß, n) we are able 
        to generate all elements of S.  Since the pair  (à, b)  is in S, we can deduce that
                        7n ^ C(ä, b) < C(7) + log IS I
        (with accuracy O(logn)).  Thus it suffices to show the upper bound 0(25,5n)  for 
        the cardinality of S.  This bound is a direct corollary from the following lemma.
          Lemma.  Assume that we are given a pair of functions (/, g) such that the first 
        function maps every line to a point on that line and the second function maps every 
        line to a plane containing that line.  Let the set S consist of all pairs of lines (a, b) 
        such that the point f(a) belongs also to b, and the plane g(b) also contains a.  Then 
        S contains at most 0(25,5n) pairs.
                     13.6.  COMPLEXITY NOT REDUCIBLE TO COMPLEXITIES OF TUPLES            419
                Proof.  Directly from  the definition we can derive the upper  bound  \S\  — 
            0(26n).  Indeed, for every line b there are about 22n lines a in the plane g(b), thus 
            the  cardinality  of S  exceeds  the  number  of lines  (24n)  at  most  22n  times.  This 
            bound  can  also  be  derived  by  counting,  for  every  line  a,  the  number  of lines  b 
            passing through the point /(a).  Notice that in the first argument we did not take 
            into account that b should pass through the point /(a), and in the second argument 
            we did not take into account that the line a should lie on the plane g{b).
                We will modify the first argument as follows.  We used the fact that for every 
            line b there are at most 22n lines a such that the pair (a, b) is in S.  Now we will show 
            that  on average for every line b there are 0(21-5n) lines a such that the pair (a, b) 
            is in S.  To prove this, we will certainly take into account the condition /(a) G b. 
            (We could also modify the second argument and show that on average for every 
            line a the set S has 0(21-5n) pairs of the form (a, *).)
                To this end we partition S into slices.  Each slice is identified by a plane d and 
            consists of all pairs  (a, b)  with g{b) — d.  Thus both lines from all pairs from the 
            same slice lie on the same plane.  We will upper bound the cardinality of each slice, 
            and then we will sum up the obtained bounds.
                Let us fix a plane d and bound the number of pairs in the slice corresponding 
            to d.  To this end fix a point c on the plane d and let Ac denote the set of all lines a 
            on the plane d with /(a) = c.  Similarly, let Bc stand for the set of all lines b passing 
            through c with g(b) — d (the conditions imply that the line a passes through c and 
            the line b lies on d).  It is clear that the cardinality of the slice is at most
                                 EHciibc| < /ei^ei^i2
                                  с               V е          C
            (we have applied the Cauchy-Schwarz inequality).  It is easy to bound both sums in 
            the right-hand side of the displayed inequality, as each of them has a clear meaning. 
            More specifically, the sum Y^c I Ac 12 is proportional to the probability of the follow­
            ing event:  For a randomly chosen (w.r.t. the uniform probability distribution) pair 
            of lines (a', a") on the plane d, it holds that f{a') = f{a").  Indeed, this probability 
            equals the sum over all c of the probability of the intersection of independent events 
            f{a') = c and f{a") — c.  The probability of each of these two events is equal to the 
            ratio of  I Ac I  and the total number of lines on d, which is about 22n.  On the other 
            hand, the probability of event f(a') = /(a") is at most 2~n  (for every fixed a' the 
            probability of event f{a') = f{a") does not exceed the probability that a" passes 
            through the point /(a'), which is about 2~n).  Hence the sum      l^e|2 does not 
            exceed
                                           (22n)2 • 2~n = 23n.
            This inequality holds up to a constant factor.
                The second sum       | Bc |2  is  related  to  the average number of points shared 
            by independent lines 6', b" chosen at random (w.r.t. uniform distribution) from the 
            set  Md  which consists of all lines  b with g(b)  = d (thus all lines from  Md  lie on 
            the plane d).  Indeed,  the average cardinality of intersection of b'  and b"  is equal 
            to  the  sum,  over  all  points  c  G  d,  of probability  of the  event  c  G  b' П b".  This 
            event is the intersection of independent events c G b' and c G b".  The probability 
            of each  of these  events  equals  the  ratio  of  \BC\  and  | Md \ ■  Hence  the  average 
            number of common points in b' and b" is equal to the ratio of the sum   | Bc |2 to 
            the square of  | Add I •  On the other hand,  any two different lines have at most one
            420                      13.  INFORMATION AND LOGIC
            common point, and two coinciding lines have 2n common points.  The lines b' and 
            b"  coincide with probability  1 / 1 Md |,  thus the average number of common points 
            in b' and b" does not exceed 1 + 2n/1 Md |.  Hence
                 £ |B C|2<  \Md\2(l + 2n/\Md\)= \Md \2 +               (I Mi I  + 2n)2.
                  c
                Recall that the number of pairs (a, b) in the slice identified by the plane d is at 
            most       I Ac 12 J2c I Be 12 •  Therefore it does not exceed
                                y/23n(\Md \ + 2n )2 = 21,5n( I Md I  +2n)
            (up to a constant factor).
               It  remains to sum up the resulting upper bounds of slice’s cardinalities over 
            all d:
                       |S|  < 2l*nY,(.\Md\ +2n) = 215n( ^ \Md\+J22n
                                    d                    \   d         d
            The families Md are disjoint and hence the sum of their cardinalities is equal to the 
            total number of lines (about 24n).  The number of planes is about 23n and therefore 
            the second sum is also about 24n.  This completes the proofs of both the lemma 
            and of the lower bound for the complexity of the problem (a —> c) A (b —> d).
               To  complete the proof of the  theorem  it  remains  to  find  another  quadruple 
            (a, b, c, d) that has the same complexities as (a, b, c, d) and such that the complexity 
            of the problem (ä —> c) A (b —> d) is close to n.
               To  achieve this,  we pick a random word of length In and chop it into seven 
            pieces u,v,w,p,q,r, s,  each of length n.  Then let  ä — uvws, b — pqrs, c = ups, 
            d  =  vqs.  A  simple  counting  reveals  that  the  complexities  of words  from  both 
            quadruples, their pairs, etc., are equal to
                                C(a) = C(b) = An,   C(c) = C(d) = 3 n,
                                            C(a, b) = 7n,
                          C(a, c) = C(a, d) — C(b, c) — C(b, d) — C(c, d) — 5n,
                                      C(a, c, d) — C(b, c, d) — 6n,
                               C(a, b, c) = C(a, b, d) = C(a, b, c, d) = In.
           The complexity of the problem (ä —> c) A (b —> d) is close to n, since given bitwise 
           XOR of p and v we can transform ä to c, and b to d.
               Probabilistic version.  Again we start with exhibiting a quadruple än,bn,cn, dn 
           such that the difference between complexity of (än —> cn) A (bn —> dn)  and both 
           lower bounds from Problem 342 is linear in n.
               Fix a natural n.  We will find a quadruple of strings  (à, b, c, d), each of length 
           n and complexity close to n, such that complexities of all pairs of those strings are 
           close to  2n and complexities of all triples and of the quadruple itself are close to 
           3n.  This implies that both lower bounds from Problem 342 for that quadruple are 
           close to n.  Besides, the complexity of the problem (a —> c) A (b —> d) will be close 
           to 2n.
               On the top level the construction is the following.  Consider functions Q that 
           map triples of words of length n to words of length n.
                    13.6.  COMPLEXITY NOT REDUCIBLE TO COMPLEXITIES OF TUPLES      421
               Lemma.  For all sufficiently large n there is a function Q of complexity at most 
           logn + 0(1)  such that for at least half of triples (a, b, c)  of words of length n the 
           complexity of the problem (a -А с) Л (b —> Q(a, b, c))  is at least 2n — O(logn).
               Before proving the lemma, let us explain how it implies Theorem 247.  Let Q 
           be a function satisfying the lemma.  The number of triples (a, b, c) that satisfy the 
           inequality C((a -А с) Л (b —> Q(a,b,c)))  ^ 2n — O(logn)  is at least 23n“1.  Hence 
           there  is  such  a triple  with  complexity  at  least  3n — 1.  Let  (ä,b,c)  be  any such 
           triple,  and let d = Q(a,b,c).  Then the complexity of the quadruple (a,b,c, d) and 
           the complexity of the triple (â, b, c) are close to 3n, as claimed before.  This implies 
           that  all the pairs of strings a, b, c,  and the strings à, b, c themselves have claimed 
           complexities.
               Moreover,  the  triple  (b, c, d)  is  also  random.  Indeed,  the  complexity  of the 
           problem  (a  —>  с) Л (b  —»  d)  is  at  least  2n  and  at  the  same  time  it  is  bounded 
           from above by the sum C(c) + C(d\b, c)  < 2n.  Therefore both terms in the sum 
           should be close to n.  Thus the word d is independent from the pair  (b, c).  Since 
           the pair (b, c)  is random, the triple  (b, c, d)  is also random.  The only requirements 
           that are not guaranteed by the lemma are the randomness of the triple (a, b, d) and 
           the randomness of the triple (ä,c,d).  We will explain later how we will guarantee 
           them.
               Proof.  To prove the lemma,  we define a decidable property of functions Q 
           that guarantees the statement of the lemma.  Then we will show that for all large 
           enough n a randomly chosen function has that property with positive probability 
           (hence the property is not empty).  By an exhaustive search,  for any given n we 
           are  able  to  find  a  function  Q  with  that  property.  Hence  the  graph  of the  first 
           found such function Q can be computed from n and thus its complexity is less than 
           logn + 0(1).
               Let S denote the set of all words of length n.  Let M be a set of (total) functions 
           from S to S.  We say that the set M serves a quadruple (a, b, c, d) € S'4, if /(a) — c 
           and g(b)  — d for some pair  (/, g)  € M.  The property of a function  Q we spoke 
           about above is the following:
                   every  set  M  consisting  of  less  than  2k  pairs  of  functions 
                   (from S to S) serves less than 1/8 of the quadruples of the form 
                   (a,b,c,Q(a,b, c)).
           We will specify the parameter к later; it will be a little less than 2n.  The property 
           guarantees that for at least 7/8 of the triples (a, b, c) the complexity of the problem 
           (a —> с) Л (b —> Q(a,b,c)) is larger than k.  Indeed, every solution to the problem 
           (a  —>  с) Л (b  —>  d)  is  a  pair  of programs  (p, q)  with  [p](a)  =  c  and  [ç](b)  =  d. 
           For every pair  (p, q)  of programs of complexity less than к, we can extend in an 
           arbitrary way the mappings a ^  [p](a) and b ha [g](6) onto the entire set S.  We get 
           a set M of pairs of functions of cardinality less than 2k.  Hence M serves less than 
           23n-3  quadruples  (a,b,c,Q(a,b,c)).  On the other hand M serves all quadruples 
           (a, b, c, d) such that the complexity of the problem (a —> c) A (b —> d) is less than k. 
           (The reader certainly noticed that to prove the lemma we need the property with 
           the threshold 1/2 in place of 1/8.  It will become clear later why we have chosen 
           1/8 as the threshold.)
             422                          13.  INFORMATION AND LOGIC
                 Let Q be chosen with a uniform probability distribution among all functions 
             from S3 to S.  In other words,  the values Q(a,b,c)  are independent  (for different 
             triples  (a,b,c))  and uniformly distributed in S.  We have to choose the parameter 
             к = n — O(logn) so that with positive probability a random function Q has the 
             property specified above.
                 We fix first a set M consisting of 2k pairs of functions from 5 to 5 and bound 
             from above the probability that it serves more than 23n-3 quadruples of the form 
             (a, b, c, Q(a, b, c)).  To this end let us divide triples  (a,b,c)  into  “bad”  and  “good” 
             ones.  The  number  of bad  triples will  be  less  than  23n-4.  And  for  good  triples 
             (a, b, c) only a fraction of at most 1/32 of quadruples (a, b, c, d) will be served by M. 
             By the Chernoff inequality with high probability the number of served quadruples 
             (a, b, c, Q(a, b, c))  with good (a, b, c)  will be also less than 23n-4.
                 More specifically,  a triple  (a,b,c)  (and also the quadruple  (a,b,c,d))  is bad if 
             the number of (/, g) from M with /(a) — c is more than  \M |2“n+4 (this property 
             depends on a and c only).  Since for each pair (/, g) there are only 22n triples of the 
             form (a, 6,/(a)), the number of bad triples is less than  \M\  • 22n/(|M|2“n+4) = 
             23n-4; the remaining triples are good.
                 We claim that if к < 2n — 9, then for every good triple (a, b, c) the probability 
             of the event  “M serves the quadruple (a,b,c,Q(a,b,c))”  is less than 1/32.  Indeed, 
             if M serves the quadruple  (a,b,c,d),  then d falls into the set that consists of all 
             strings g(b)  such that  for some /   the pair  (/, g)  belongs to M and /(a) = c.  As 
             (a, b, c)  is good, this set has at most  | M 12-n+4 =  2k~ n+4 strings.  If к is chosen to 
             be less than 2n — 9, then this set contains a fraction at most 1/32 of all strings of 
             length n.
                 We will use now Chernoff inequality in the following form.  Assume that we 
             are given N independent events, and the probability of each event equals p.  Then 
             for  any  £  with  probability  at  least  1  — e“2e  N  the  number  of events  that  have 
             occurred is fewer than (p + e)N.  In particular, this holds for e — p:  the number of 
             events that have occurred is fewer than 2pN with probability at least 1 — e~2p N. 
             Obviously,  the same bound holds in the case when the probability of each event 
             is  at  most p  (maybe less  than p).  In our case each event  is specified by  a good 
             triple  (a,b,c)  and thus  (15/16)23n  <  N  <  23n:  the event occurs if M serves the 
             triple (a, b, c, Q(a, b, c)).  The probability of each event is at most p = 1/32.  By the 
             Chernoff bound with probability at least 1 — e“n^2        the number of good triples 
             (a, b, c)  such that M serves (a, b, c, Q(a, b, c))  is at most N/16 < 23n~4.
                 If  at  most  1/16  of good  quadruples  are  served  by  M,  then  (even  if all  bad 
             quadruples are served) the fraction of served quadruples is at most 1/16 + 1/16 = 
             1/8.  Hence M serves more than 1/8 of quadruples (a, b, c, Q{a, b, c)) with probabil­
             ity at most e-n^3
                 It remains to verify that the probability e“n^2    is less than 1 even after being 
             multiplied by the number of sets M of cardinality 2k.  The number of such sets is 
             fewer than the square of the number of functions from S' to S' (that number equals 
             2n2n ) raised to the power of 2k :
                                            22n2n2k _ ^2п+к+1оя n + 1
             Let us compare this value to the probability
                                                    2_n(23n)
                     13.6.  COMPLEXITY  NOT  REDUCIBLE TO  COMPLEXITIES  OF TUPLES       423
            of the event “M serves more than 1/8 of quadruples of the form (a, 6, c, Q(a, b, c))”. 
            The product of displayed numbers is equal to 2 raised to the power which is the dif­
            ference of two numbers:  2n+/c+logn+1 and f2(23n).  We need that the latter number 
            be bigger than the former one.  This happens when к = 2n — logn — 0(1).  (Recall 
            that the number of bad triples is small under the condition к ^ 2n — 9, thus all our 
            calculations remain valid.)  The lemma is proven.
                It remains to explain how to guarantee the randomness of the remaining triples 
            {ä, b, d)  and (â, c, d).  The simplest solution is to guarantee this in exactly the same 
            way we guaranteed the randomness of the triple (b,c,d).  That is, we will modify 
            the lemma by requiring that for half of triples  (a, b, c)  not only does the problem 
            (a —> c) A (b —> Q(a,b,c)) have large complexity but that the symmetric problems 
            (c —^ 6) A {a —У Q(a, b, c)) and (b —> a) A (c —>• Q(a, 6, c)) have large complexity also.
                Lemma.  For all sufficiently large n there is a function Q of complexity at most 
            logn + 0(1) such that for more than half of triples (a, b, c)  of words of length n the 
            complexity of each of the problems
             (a—>c) Л (b^Q(a, b, c)),  (c—>b) Л (a^Q(a, b, c)),  and (6—»а) Л (c^Q(a,b,c)) 
            is at least 2 n — О (logn).
                Proof.  Recall  that  the  previous  lemma  was  proven  by  the  probabilistic 
            method:  We have exhibited a property of a function and have shown that a ran­
            domly chosen function does not have that property with exponentially small prob­
            ability.  Now,  instead of one property of a function Q,  we have three symmetric 
            properties.  Each  of the three properties does not  hold with exponentially small 
            probability.  Thus for all large enough n there is a function Q that has all the three 
            properties.  For such a function the number of triples served in at least one of the 
            three ways is at most 1/8 + 1 /8 + 1/8 < 1/2.  The lemma is proven.
                It remains to exhibit another quadruple (a, 6, c, d) of strings that have the same 
            complexities  (as well as their pairs,  triples,  and  the quadruple)  as  (â, b, c, d)  and 
            such that the complexity of the problem C((a —> c) A (b —> d)) is much less than 2n. 
            To this end pick a random string of length 3n and chop it into three parts a, b, c, 
            each of length n.  Then let d = a 0 b ® c.  Given a 0 c, we can transform a to c and 
            b to d.  Hence C((a —> c) A (b —> d)) ^ n  (up to an additive constant).     □
                344       Show that the complexity of the problem (p V q) —> (г V s) also cannot be 
            expressed through complexities of p, q, r, s, their pairs, triples, and the quadruple.
                {Hint:  Let p = a, q = b, r = ac, s = bd where  (a, 6, c, d)  is either of the two 
            quadruples used in the above proof (say,  in the first one).  The complexity of the 
            resulting problem depends on which of the two quadruples we have chosen.  On the 
            other hand, the complexities of p, q, r, s, their pairs, triples, and the quadruple does 
            not depend on this choice.)
                It is instructive to compare the geometric proof with the probabilistic one.  The 
            geometric proof is more constructive than the probabilistic one:  the first quadru­
            ple  is  identified  more  explicitly  in  the  geometric  proof than  in  the  probabilistic 
            one.  On the other hand, in the probabilistic proof, the complexity of the problem 
            (a —>• с) Л (b —>• d) equals the upper bounds from Problem 342, which are all equal 
            to 2n.  For the quadruple (à —> c) A (b —> d) from the geometric proof, the upper 
            bounds from Problem 342 are still equal to 2n; however, we were able to show only
                       13.  INFORMATION  AND  LOGIC
       424
       the lower bound of 1.5n for the complexity of the problem (ä —> c) A (b —> d).  We 
       do not know whether a better lower bound holds.
          Historical remark.  Theorem 247 was established in [142].
                                                  CHAPTER 14
                                       Algorithmic statistics
                            14.1.  The framework and randomness deficiency
                  Generally speaking, mathematical statistics deals with the following problem: 
             there are some experimental data, and we look for a reasonable theory that explains 
             these data (is consistent with these data).  It turns out that the notion of complexity 
             is helpful in understanding this problem.  This is a topic of algorithmic statistics.1
                  Consider the following (simplified) example.  A  “black box”, switched on, has 
             produced  a sequence  of bits,  say,  of length  106.    (This  sequence  could  also  be 
             considered as a number between 0 and 21-000,000 — 1.)  What information about the 
             internal structure of the black box could we get by analyzing this sequence?  Or, 
             at least, what conjectures about this internal structure look compatible with these 
             data?
                  Classical statistics is not well suited to this situation.  If we had information 
             from several independent copies of our device, or if we could switch on the device 
             many times (and have good reason to believe that the results are independent), or 
             if we had some probabilistic distribution that depends on a parameter and needed 
             to choose the most suitable value of this parameter—in all these cases the statistic 
             would know what to do.  But if our experiment cannot be repeated (which is not 
             uncommon in practice, by the way) and we have no a priori information about the 
             family of possible distributions, statistics does not tell us what to do.  Indeed, we 
             have a set of all 21,000,000 possible outcomes, and no structure on this set, so what 
             can we say about one specific outcome?
                  Common sense nevertheless supports some conclusions even in this case.  For 
             example, if our device produced 106 zeros, then many people would think that the 
             device is indeed very simple and can produce only zeros.  Similarly, if the sequence 
             was 010101 •• •  (alternating zeros  and ones),  people would  probably  believe that 
             the black box is a simple mechanism of a flip-flop type.  And if the sequence had 
             no visible regularities, people would probably think that the device is some kind of 
             random bit generator.  So the conclusions could be quite different, and it would be 
             interesting to give some more formal support for our common sense reasoning.
                  In the first example (a zero string) the “explanation” (hypothesis) is a singleton: 
             we  think  that  perhaps  the  device  can  produce  only  this  string.  In  the  second 
             example (and in all similar situations when the device produces a binary string x of 
             a very small complexity) the same explanation looks reasonable:  we believe that the 
             device is made just for producing this specific string x.  So the set of possibilities
                  1An alternative short introduction to this topic can be found  in  [201]  (without proofs).  A 
             more detailed  exposition that  contains  some  material  of this  chapter  but  puts  it  in  a different 
             perspective can be found in a recent survey paper [202].
                                                        425
                       14.  ALGORITHMIC STATISTICS
       426
       is  a singleton  {x}.  On  the  other hand,  in the  third  example  (a random-looking 
       sequence) the  “explanation set”  is the set of all strings.
          There are some intermediate examples.  Imagine that our device produced a 
       sequence of length  106 where the first 500,000 bits are zeros and the second half 
       is  a  random-looking sequence of length  500,000 without  any visible regularities. 
       Then we may guess that the device first produces 500,000 zeros and then switches 
       to another mode and produces 500,000 random bits.  Here the explanation set has 
       cardinality 2500,000  and consists of all strings of length  1,000,000 that start with 
       500,000 zeros.
          The general framework that covers all our examples, can be explained as follows: 
       given a string x, we suggest some finite set A that contains x and can be considered 
       as a reasonable explanation for x.  What do we mean by  “reasonable”?  Here are 
       two natural requirements:
            •  the set A should be simple  (its Kolmogorov complexity C(A) should be 
             small);
            •  the string x should be a “typical”  element of A.
          More specifically, Kolmogorov complexity C(A) of a finite set A is the complex­
       ity of the list of its elements (written in some fixed order, e.g., sorted in alphabetic 
       order, and encoded by a binary string).  It does not depend on the specific ordering 
       (lexicographical or any other computable total ordering) and on the encoding (up 
       to a constant).
          The notion of a “typical representative of a set” can also be made more precise 
       using Kolmogorov complexity.  Recall that if a set A consists of N elements, then 
       the conditional complexity C(x\A) of every x in A does not exceed logiV + 0(1) 
       (each element  can be described by its ordinal number in A—assuming that A is 
       known).  For most x in A the complexity C(x\ A) is close to logiV, since only very 
       few elements have smaller complexity.  Informally speaking, an element x is typical 
       in A if d(x I A) is negligible.
          Let us reformulate this in the following way.  Consider a finite set A, an element 
       x e A, and the difference
                       d(x\ A) = log|A| — C{x\ A).
       As we have seen, this difference is non-negative (up to 0(1)).  We call it the ran­
       domness deficiency of x as an element of A.  Note that we do not use this formula 
       to define d(x \ A) if x is not in A; in this case d(x | A) is undefined.  (It is also natural 
       to let d(x I A) be +oo when x £ A, since in this case the explanation A is completely 
       unsuitable for x.)
          An element x is typical in A if d(x \ A) is negligible.
          345   Prove that for a given A the probability of the event “a randomly chosen 
       element x 6 A has deficiency greater than k”  does not exceed 2~k.
          (Here probability means just the fraction of elements with given property in A.) 
       In fact,  to make this statement true, we need to replace log|A|  by  [bgAJ;  since 
       complexity is defined up to a constant anyway, we are not that pedantic.
          Let  us  note also that the  function d  (with two  arguments x and A)  is lower 
       semicomputable  (enumerable from below):  We can effectively provide more and 
       more precise lower bounds for it, but we cannot say when its value was achieved. 
       (Indeed, function C is upper semicomputable.)
                               14.1.  THE  FRAMEWORK  AND  RANDOMNESS  DEFICIENCY                        427
                    346         Assume that a function 5(x\A)  is given,  where x  is a string and  A is 
              a  set  containing  that  string  and  5  has  the  following  properties:  (a)  5  is  lower 
              semicomputable;  (b)  for  every  finite  set  A  and  for  every  natural  number  к  the 
              fraction of strings in A with S(x \ A) > к is less than 2~k.  Then S(x \ A) < d(x \ A) + 
              0(1).
                   This statement is a direct corollary of a similar statement for conditional Kol­
              mogorov complexity (see Theorem 19 on p. 36).  Its meaning is the following.  There 
              are different opinions about which elements of a given set are typical and which are 
              not.  That is, there exist different methods to measure non-typicality.  Assume that 
              we normalize each method so that, after normalization, in each set the fraction of 
              /с-non-typical  element  is  less  than  2 ~k.   Assume  also  that  we  can  reveal  non­
              typicality of a given string in a given set provided we have enough time for that 
              (that time can be quite long and not bounded by any total computable function). 
              Then there is the best such method in the sense that the deficiency it reveals is not 
              less than the deficiency revealed by any other method (up to an additive constant).
                   Randomness deficiency in a finite set is similar to randomness deficiency of an 
              infinite  sequence  with  respect  to  a probability  measure  (see  Section  3.5).  More 
              specifically, it is similar to the maximal probability bounded randomness test.  One 
              can also define an analogue of an expectationally bounded randomness test.
                    347         Let the prefix randomness deficiency of a string x in a finite set A be 
              defined as dp(x\ A) = log2 \A\ — K(x\ A).  Show that dp(x\ A) is a maximal lower 
              semicomputable function S of x and A such that (1/|A|) J2xeA2<5(z|A) is at most 1
              for all finite sets A.
                   (Hint:  Recall that prefix complexity coincides with the negative logarithm of 
              the a priori probability.)
                   Thus a finite set A is considered a good explanation for x if it is simple and 
              the randomness deficiency d(x\ A) of x in A is small.  Those strings having such an 
              explanation are called stochastic.  Are there non-stochastic strings?  This question 
              will be answered in the next section.
                   Notice that we consider only statistical hypotheses that are uniform distribu­
              tions over finite sets.  In a more general framework one can consider also arbitrary 
              probability distributions over strings (say, with finite supports and rational values 
              to avoid technical problems).  For such distributions the randomness deficiency of 
              a string x with respect to a distribution P is defined as — log2 P(x) — C(x \ P)  (if 
              P(x)  =  0,  then the deficiency  is  infinite:  for such strings  x  the hypothesis  P  is 
              completely unsatisfactory).
                  For uniform distributions (all elements of a finite set A have probability 1/|A|), 
              the generalized definition of randomness deficiency coincides with the previous one. 
              Notice that the general case is not very different from the case of uniform distribu­
              tions:
                   348         Assume that x is a string of length n and P is a probability distribution 
              (not necessarily uniform)  of complexity к such that the randomness deficiency of 
              x with respect to P is at most I.  Then there is a set A of complexity at most к + 
              0(log(/ + n))  containing x  such  that  the  randomness  deficiency  of x  in  A  is  at 
              most I + 0(log(/ + n)).
                   (Hint:  Let  A  =  {y  \  P(y)  ^ p]  where p is the probability of x with respect 
              to P rounded to the nearest integer power of 2.)
        428              14.  ALGORITHMIC STATISTICS
          This problem explains why we are considering uniform distributions only.  Let 
        us stress that in the definition of Kolmogorov complexity of a finite set of strings 
        we consider the set as a finite object represented by the list of all its elements in the 
        lexicographical order.  An alternative approach is to measure the complexity of a set 
        as the minimal length of a program enumerating the set.  With this approach the 
        definition of stochastic strings becomes trivial:  all strings  are stochastic.  Indeed 
        for  every  string  x  of complexity  к  one  can  consider  the  set  Sk  of all  strings  of 
        complexity at most  к as an explanation for x.  It has 0(2k)  elements and hence 
        the  randomness  deficiency of x  in  Sk  is  negligible.  On  the  other  hand,  we  can 
        enumerate this set given к and hence Sk can be enumerated by a program of length 
        log к + 0(1).  However, intuitively Sk is not a good  “explanation”  for x.
          In  the  case  of general  probability  distributions  (not  only  uniform),  we  also 
        consider a distribution as a finite object represented by the list of all pairs (x, P(x)) 
        for x in the support of P and arranged lexicographically.  This is why we need the 
        support to be finite and the values to be rational.  Alternatively, we could consider 
        infinite supports and uniformly computable values—in that case the explanation 
        would be a program computing the function x i+ P{x).  It is essential that we do 
        not allow lower semicomputable semimeasures represented by programs that lower 
        semicompute them.  If we did, then any string would obtain a perfect explanation— 
        the maximal lower semicomputable semimeasure.
          Historical remark.  The first definition of randomness deficiency was given by 
        Kolmogorov, who used the formula log|A| — C(x).  The formula log|A| — C(x\A) 
        used throughout the book is due to [60]  (note that in [60] the prefix complexity is 
        used instead of the plain one, the difference is 0(log(deficiency))).  Kolmogorov’s 
        randomness deficiency log |A| — C(x) is less than or equal to the randomness defi­
        ciency log I A\ — C(x I A), and they differ by at most C(A).  The two deficiencies may 
        differ  that  much,  e.g.,  for  A —  {x}.  Perhaps Kolmogorov was interested only in 
        sets A with negligible complexity, in which case these two deficiencies are close.  For 
        sets with large complexity the expression log |A| — C(x)  may have large negative 
        value and hardly makes any sense.
                         14.2.  Stochastic objects
          A string x is called (a, /3)-stochastic if there is a finite set A containing x with 
        C(A) ^ a and d(x\A) ^ ß.
          A natural question arises.  Consider all strings x of length n and consider a 
        and ß of order О (log/г) or o(n), making the complexity of explanations for x much 
        smaller than the length of x.  For such a,ß,  are there non-stochastic strings  (i.e., 
        “non-explainable”  objects)?  An affirmative answer to this question is provided by 
        the following theorem.
          Theorem 248.  Assume that 2a + ß < n — O(logn).  Then there is a string of 
        length n that is not (a, ß)-stochastic.
          (The accurate statement is that there is a c such that for all large enough n 
        and  all  a,ß  with  2a + ß  <  n — clogn there  is  a string  of length n  that  is  not 
        (a, /3)-stochastic.)
          PROOF.  Consider the list of all finite sets of complexity at most a.  The Kol­
        mogorov complexity of this list is at most a + O(loga) = a + O(logn)  (see p. 25).
                                            14.2.  STOCHASTIC  OBJECTS                            429
             Ignoring additive error terms of order O(logn)  (here and also further) we will as­
             sume that the complexity of the list is less than a.
                  Remove from the list all sets of cardinality more than 2a+l3.  The Kolmogorov 
             complexity of the resulting list is also less than a.  By construction it has at most 
             2a sets and each of them has at most 2a+@ elements.  Thus the union of all sets in 
             the list has less than 22a+l3 < 2n strings.  Hence there is a string of length n that 
             does not appear in any set from the list.  Let t be the lexicographically first such 
             string.  Its complexity is at most a, as it can be found given n and the list.
                  Let us show that this string (denoted by t in the sequel) is not (a, /3)-stochastic. 
             Indeed,  assume that it is contained in some set A of complexity at most a.  The 
             cardinality  of A  exceeds  2Q+/3  since  all  smaller  sets  were  taken  into  account  by 
             construction.  Therefore
                       d(t I A) — log #A -  C(t I A) > (a + ß) — C{t) > (a + ß) — a ^ ß
             (one should also add a reserve of size clogn to compensate for logarithmic terms 
             that we ignore).                                                                      □
                 In the other direction we have the following trivial bound:
                  Theorem 249.  If a + ß > n + O(logn),  all the strings of length n are (a,ß)- 
             stochastic.
                 P roof.  Indeed, we can split all n-bit strings into 2a sets of size 2^.          □
                 As we will see later, the reality is closer to this bound than to the bound of the 
             previous theorem.  See Problem 365 on p. 449.
                 It is natural to ask how often non-stochastic objects appear.  For example, what 
             is the fraction of non-stochastic objects among all n-bit strings?  It is immediately 
             clear that this fraction does not exceed 2"^:  Let A be the set of all n-bit strings, 
             and note that strings with deficiency ß or more form only a 2“^-fraction of A.
                 On the other hand, if 2a + ß       n, we can extend the reasoning used to prove 
             Theorem 248.  Namely,  for some h we consider all sets of complexity at most a 
             and cardinality at most 2a+@+h.  Then we take the first 2h elements not covered 
             by these sets; it is possible if 2a + ß + h < n.  The complexity of those elements is 
             bounded by a + /i, so its deficiency in any set of size greater than 2a+@+h exceeds ß. 
             These arguments (with 0(logn)-corrections needed) prove the following statement:
                 Theorem 250.  If2a + ß < n — O(logn),  then the fraction of n-bit strings that 
             are not (a, ß)-stochastic is at least 2 ~2a~P~°^ogn\
                 Instead of a fraction of non-stochastic strings (i.e., the probability of obtaining 
             such a string by tossing a fair coin), one can ask about their total a priori prob­
             ability  (i.e.,  the probability of obtaining such a string by a universal randomized 
             algorithm).  More formally, let m(x) be the discrete a priori probability of x as de­
             fined in Chapter 4:  m(:r) = 2~K(xï+°(1h  Then we consider the sum of m(:r) over 
             all x  of length n that  are not  (a, /5)-stochastic.  The following theorem estimates 
             this sum:
                 Theorem 251.  If2a + ß<n — O(logn)  and a < ß — O(logn),  then this sum 
             equals 2 ~a+° ^ n\
                 P roof.  We need to prove both lower and upper bounds for this sum.  The lower 
             bound easily follows from the proof of Theorem 248.  Indeed, a non-stochastic string
             430                          14.  ALGORITHMIC STATISTICS
             constructed in that proof had complexity a and therefore its a priori probability is 
             2 ~a  (as usual, we ignore O(logn) corrections needed, now in the exponent).
                 To get an upper bound, consider the sum of m(x) over all strings of length n. 
             That sum is a real number ш ^ 1.  Let ш be the number represented by first a bits 
             in the binary representation of w.
                 Consider the following measure P on strings  of length n associated with Q. 
             Start  lower  semicomputation of m(x)  for  all  strings  x  of length  n  and  continue 
             until the sum of all obtained lower bounds for m{x) reaches Q.  Let P{x)  be the 
             lower bound for m(x) we get at that time.  If Q and n are given, we can compute 
             P(x) for all x of length n.  Therefore the complexity of P is at most a.  The sum of 
             differences between m(x) and P(x) over all strings of length n is bounded by 2~a.
                 As we saw in Problem 348, one can use arbitrary finite probabilistic distribution 
             in the definition of stochasticity (with an 0(logn)-change in the parameters), not 
             only the uniform ones.  It remains to be shown that the total a priori probability 
             of all strings x that have d(x\P) > ß is bounded by 2~a.  Indeed, for those strings 
             we have
                                           log P(x) — C(x IP) > ß.
             The complexity of P is bounded by a and therefore C{x) exceeds C(x\P) at most 
             by a.  Thus we have
                                          — logP(x) — C(x) > ß — a.
             We ignore 0(logn)-terms, so we can replace plain complexity by prefix complexity:
                                          — log P(x) — К(x) > ß — a.
             Prefix complexity can be defined in terms of a priori probability, so we get
                                           log(m(x)/P(x)) > ß — a
             for  all  x  that  have  deficiency  exceeding  ß  with  respect  to  P.  By  assumption, 
             a  <  ß  with  some  safety  margin  (enough  to  compensate  all  the  simplifications 
             we made),  so  we  may  assume that  for  all  those  x  we  have  P(x)  <  m(x)/2,  or 
             (m(x) — P(x)) > m(x)/2.  Recall that the sum of m(x) — P(x) over all x of length 
             n does not exceed 2~Q by construction of ш.  Hence the sum of m(x) over all strings 
             of deficiency (with respect to P) exceeding ß is at most 2”a+1, and this is what we 
             wanted to prove.                                                                    □
                 The notion of a stochastic object can be considered as a finite analog of the 
             notion  of an  ML-random  sequence with respect  to  a computable  measure.  The 
             following problem expresses this similarity in more formal terms.
                  349        Assume that  a sequence ш  is  ML-random with respect  to  some  com­
             putable  measure.  Prove that  for  all  n  the  n-bit  prefix  of the  sequence  ш  is  an 
             (O(logn), 0(logn))-stochastic string.  (Hint:  Use  Problem  348.)  Conclude  that 
             there is an infinite sequence that is not ML-random with respect to any computable 
             measure.  (Hint:  Adding a short prefix does not affect non-stochasticity.)
                 Historical remarks.  The first definition of (a, /3)-stochasticity was given by Kol­
             mogorov (the authors learned it from his talk given in 1981 [83], but most probably 
             it was formulated earlier in 1970s; the definition appeared in print in [174]).  Kol­
             mogorov and Shen ([174]) used the formula log |A| —C(x) for randomness deficiency.
                 The existence of non-stochastic objects (Theorem 248) was noted in [174].  The 
             first estimates of the a priori measure for the set of non-stochastic objects appeared 
             in [210].  The first tight bound 2  Q for the a priori measure of (a, /3)-non-stochastic
                           14.3.  TWO-PART  DESCRIPTIONS        431
         objects is due to Muchnik  [139, Theorem 10.10], who established it for all  (a,/3) 
         with  3a + ß  <  n.  Both  papers  [210]  and  [139]  used  the  Kolmogorov  formula 
         log \A\ — C(x) for randomness deficiency.
           Theorem 251 appears to be new.  Note that this theorem and Muchnik’s re­
         sult use incomparable assumptions on the parameters a, ß.  Besides, Theorem 251 
         estimates the a priori measure of a larger set than Muchnik’s result.
                          14.3.  Two-part descriptions
           There is another natural way to estimate the quality of statistical hypotheses. 
         Let us start with the following remark.  If a string x belongs to some finite set A, 
         we can specify x in two steps:
              •  first, we specify A;
              •  then we specify the ordinal number of x in A (in some natural ordering, 
               say, the lexicographic one).
         Therefore, we get C(x) ^ C(A) + log фА for every element x of an arbitrary finite 
         set A (again with logarithmic precision).
           There can be many two-part descriptions of the same string x (with different 
         sets A).  Which of them are better?  Naturally, we would like to make both parts 
         smaller  (by  finding  a simpler  and  smaller set  A):  if we  can  decrease  one of the 
         parameters while not increasing the other one, this is an improvement.  But which 
         is better:  simple A or small complex A?  We can compare the lengths of the resulting 
         two-part descriptions and choose a set A which gives the shorter one.  This approach 
         is often called the Minimum Description Length principle (MDL).
           The following simple observation shows that we can move the information from 
         the first part of the description into its second part (leaving the total length almost 
         unchanged).  In this way we make the set  smaller  (the price we pay is that  its 
         complexity increases).
           Theorem 252.  Let x be a string, and let A be a finite set that contains x.  Let 
         i  be a non-negative integer such that i < log фА.  Then there exists a finite set A' 
         containing x such that # A! < фА/2г andC(A') < C(A) +i + 0(logmin{i, C(A)}).
           Proof.  List all the elements of A in some  (say,  lexicographic)  order.  Then 
         split the list into 2* parts (first фА/2г elements, next фА/2г elements etc.; we omit 
         evident precautions for the case when фА is not a multiple of 2г).  Then let A' be 
         the part with x.  To specify A',  it  is enough to specify A and the part  number, 
         which requires at most i bits.  (The logarithmic term at the end is needed to form 
         a pair of these two descriptions;  it is enough to specify the length of the shorter 
         description.)                                          □
           We will use the following convenient (though non-standard) terminology:  a set 
         A is called а (к * I)-description  (of every its element) if C(A) ^ к and log фА ^ I. 
         Theorem 252 can now be formulated as follows:  if some x has a (k* ^-description, 
         then for every i 6 [0,1] it also has ((k + i + 0(logmin(i, к))) * (I — i))-description.
           For  a  given  string  x  let  us  consider  the  set  Px  of all  pairs  (k,l)  such  that  x 
         has а  (к * ^-description,  i.e.,  there exists a set  A containing x with C(A) ^ к  and 
         log фА ^ 1.  Obviously,  this set  is  closed  upwnrds  and  contains W4th each point  all 
         points  on  the  right  (with  the  bigger  к)  and  on  the  top  (with  bigger  I).  The  last 
         theorem says  that  we  can  also  move  down-right  adding  (г, —i)  (with  logarithmic 
         precision).
        432              14.  ALGORITHMIC STATISTICS
                          Figure 52.  The set Px
          We will see that  movement  in  the opposite direction is not  always  possible. 
        So,  having two-part descriptions with the same total length, we should prefer the 
        one with the bigger set (since it always can be converted into others, but not vice 
        versa).
          Let us look again at the set Px for some n-bit string x\ see Figure 52.  It contains 
        the  point  (0,n)  that  corresponds  to  A  =  Bn,  the  set  of all  n-bit  strings  (with 
        logarithmic precision).  On the other side the set Px  contains the point  (C(x),0) 
        that  corresponds  to  the singleton  A —  {x}.  The boundary of Px  is some curve 
        connecting these two points, and this curve never gets into the triangle k + s ^ C(x) 
        and always goes down (when moving from left to right) with slope at least —1 or 
        more, as Theorem 252 says.
          This picture raises a natural question:  Which boundary curves are possible and 
        which are not? Is it possible, for example, that the boundary goes along the dotted 
        line  on  Figure  52?  The  answer is  positive:  take  a random string of the  desired 
        complexity and add trailing zeros to achieve the desired length.  Then the point 
        (0, C(x))  (the left end of the dotted line) corresponds to the set A of all strings of 
        the same length having the same trailing zeros.  We know that the boundary curve 
        cannot  go  down slower than with slope  —1  and that  it  should end  at  (C(x),0), 
        therefore it follows the dotted line (with logarithmic precision).
          There is a more difficult question:  Is it possible that the boundary curve starts 
        from (0, n) and goes with the slope —1 to the very end and then goes down rapidly 
        to  (C(x),0)?  (See Figure 53.)  Such a string x,  informally speaking,  would have 
        essentially only two types of statistical explanations:  a set of all strings of length n 
        (and its parts obtained by Theorem 252) and the exact description, the singleton 
        {x}.
           350   Show that such x is not  (a, ß)-stochastic if a, ß are smaller than C(x) 
        and n — 2C(x), respectively.
          It  turns out  that  not  only are these two opposite cases possible,  but  also  all 
        intermediate  curves  are  possible  (assuming  they  have  a  bounded  slope  and  are 
        simple enough), if we allow a logarithmic deviation from the prescribed curve.
                       14.3.  TWO-PART  DESCRIPTIONS  433
                     logsize
              Figure 53.  Two opposite possibilities for a boundary curve
          Theorem 253.  Let к < n be two integers, and let to > t\ > • • • > tk be a strictly 
       decreasing sequence of integers such that to ^ n and tk — 0; let m be the complexity 
       of this sequence.  Then there exists a string x of complexity к + О (log n) + 0(m) and 
       length n+0(logn)+0(m) for which the boundary curve of Px coincides with the line 
       (0, to)-(l,ti)— ■ ■ -(k,tk)  with O(logn) + О(m)-precision:  the distance between the 
       set Px and the set T = {{i,j) \ (i < k) => (j > t{)} is bounded by O(logn) + 0(m).
          (We say  that  the  distance  between  two  sets  P  and  Q  is  at  most  e  if  P  is 
       contained in e-neighborhood of Q and vice versa.)
          P r o o f.  For  every  i  in  the  range  0 ■■■ к  we  list  all  the  sets  of complexity  at 
       most i and size at most 2ti.  For a given i the union of all these sets is denoted by 
       Si.  It  contains at most  2l+ti  elements.  (Here and later we omit constant factors 
       and factors polynomial in n when estimating cardinalities,  since they correspond 
       to  O(logn)  additive  terms  for  lengths  and  complexities.)  Since  the  sequence  t{ 
       strictly  decreases  (this  corresponds  to  slope  —1  in  the  picture),  the  sums  i + U 
       do not increase, therefore each Si  has at most  2to — 2n elements.  Therefore, the 
       union of all Si also has at most 2n elements (up to a polynomial factor, see above). 
       Therefore, we can find a string of length n  (actually n + O(logn))  that does not 
       belong to any Si.  Let x be a first such string in some order (e.g., in lexicographic 
       order).
          By construction, the set Px lies above the curve determined by ti.  So we need 
       to estimate the complexity of x and prove that Px follows the curve (i.e., that T is 
       contained in the neighborhood of Px).
          Let us start with the upper bound for the complexity of x.  The list of all objects 
       of complexity at most к plus the full table of their complexities have complexity 
       k + O(logfc), since it is enough to know к and the number of terminating programs 
       of length at most к.  Except for this list, we need to know the sequence to,... ,tk 
       whose complexity is m.
          For the lower bound, the complexity of x cannot be less than к since all the 
       singletons of this complexity were excluded (via Tk).
          It remains to be shown that for every i ^ к we can put x into a set A of com­
       plexity i (or slightly bigger) and size 2li  (or slightly bigger).  For this we enumerate
         434               14.  ALGORITHMIC STATISTICS
         a sequence of sets of correct size and show that one of the sets will have the required 
         properties.  If this sequence of sets is not very long, the complexity of its elements 
         is bounded.  Here are the details.
           We start by taking the first 2tj strings of length n as our first set A.  Then we 
         start enumerating all finite sets of complexity at most j and of size at most 2*-»  for 
         all j — 0,..., k, and get an enumeration of all Sj.  Recall that x is the first element 
         that does not belong to all such Sj. So, when a new set of complexity at most j and 
         of size at most 2tj appears, all its elements are included in Sj and removed from A. 
         Until all elements of A are deleted, we have nothing to worry about, since A covers 
         the minimal remaining element.  If (and when)  all elements of A are deleted,  we 
         replace A by a new set that consists of first 2li undeleted (yet) strings of length n. 
         Then we wait again until all the elements of this new A are deleted.  If (and when) 
         this happens, we take 2li first undeleted elements as new A, etc.
           The construction guarantees the correct size of the sets and that one of them 
         covers x (the minimal non-deleted element).  It remains to estimate the complexity 
         of the sets we construct in this way.
           First, to start the process that generates these sets, we need to know the length 
         n  (actually something logarithmically close to n)  and the sequence to,...,tk-  In 
         total we need m + O(logn) bits.  To specify each version of A, we need to add its 
         version number.  So we need to show that the number of different A’s that appear 
         in the process is at most 2г or slightly bigger.
           A new set A is created when all the elements of the old A are deleted.  Let us 
         distinguish two types of changes of A: the first changes after a new set of complexity 
        j appears with j ^ i and the remaining changes.  The changes of the first type can 
         happen only 0 (2г) times since there are at most 0 (2г) sets of complexity at most 
         i.  Thus it suffices to bound the number of changes of the second type.  For those 
         changes all the elements of A are removed due to elements of Sj with j > i.  We 
         have at most 2J+L elements in Sj. Since tj +j ^ U + i, the total number of deleted 
        elements only slightly exceeds 2ti+l, and each set A consists of 2li elements, so we 
        get about 2г changes of A.                              □
            351 Prove that we cannot strengthen Theorem 253 by requiring the distance 
         between the sets Px and T be O(logn) (and not O(logn) + 0{m)).
           (Hint:  The number of strings of length n + O(logn) is much smaller than the 
         number of sets T that satisfy the conditions of the theorem.)
            352 Prove that there is no algorithm that, given any x, will find the boundary
        of the set Px with accuracy 0(\ogl(x)).
           Stronger results on non-computability of the boundary of Px can be found in 
        the paper [203].
           Theorem 253 shows that the value of the complexity C(x) does not completely 
        describe the properties of x\ different strings x of the same complexity can have dif­
        ferent boundary curves of Px.  This curve can be considered an infinite-dimensional 
        characterization of x.
           To understand this characteristic better, the following notation is useful.  The 
        classification  of strings  according  to  their  complexity  can  be  represented  by  an 
        increasing sequence  of sets  So  C  S\  C  S2 ■ ■ ■,  where  Si  is  the  set  of all  strings 
        having complexity at most i.  The sets Si are enumerable (uniformly in г); the size 
        of Si is 0(2г).
                       14.3.  TWO-PART DESCRIPTIONS   435
          Now, instead of this linear classification, we have a two-dimensional family Sij 
       where Sij is the union of all finite sets A with C(A) ^ i and log фА ^ j (these sets 
       were called the (î*j)-descriptions of their elements).  We get a two-dimensional table 
       formed by Sij; note that it is monotone along both coordinates, i.e., Sij increases 
       when i or j increases.  Theorem 252 says that this table is (almost) increasing along 
       the diagonal:       Sij C Si+k^j — k'
       (As usual, we ignore logarithmic corrections:  one should write
                         Sij C *S'i+A:+0(logk),j — к
       instead.)
          To understand better the meaning of this two-dimensional stratification, let us 
       look at the equivalent definitions of Sij. As usual, we ignore the logarithmic terms 
       and consider as identical two families S and S' if Sij C S'i+0^ogl^ j+o(\ogi) where 
       I = i + j.
          By an  enumerated list  in the following theorem we mean  an algorithm  that 
       (from time to  time)  emits  binary strings  (perhaps,  with repetitions);  the length 
       of such a list  is defined  as the number of strings emitted  (each string is counted 
       as many times as it was emitted).  Condition (c) assumes that the algorithm can 
       produce strings in groups of arbitrary size (different groups produced by the same 
       algorithm may have different sizes).
          Theorem  254.  The following properties of a string x  are  equivalent in this 
       sense (each of them implies the others with logarithmic change in the parameters):
          (a)  x  belongs to Sij  (has an (i * j)-description);
          (b)  there exists a simple (—of complexity 0(log(i + j))) enumerated list of size 
       at most 2*+J  where x  appears  (for the first time)  at least 2J  steps before the end of 
       the list;
          (c)  there exists a simple  (=of complexity 0(log(i + j))) enumerated list of size 
       at most 2*+J  that includes x where strings are produced in at most 2l groups;
          (d)  in every simple (—of complexity 0(log(i+j))) enumerated list that includes 
       all the strings of complexity at most i + j,  the string x appears (for the first time) 
       at least 2J  steps before the end of the list.
          P r o o f.  To  show that  (a)  implies  (c),  assume that  (a)  is true.  Enumerate all 
       sets of complexity at most i and of size at most  2J .  When a new set appears,  it 
       forms a new group added to the list.  In this way we get at most 2l groups of size at 
       most 2J , so the total length of the enumerated list is at most 2*+J .  The complexity 
       of the enumeration algorithm is logarithmic since only i and j should be specified.
          To get  (b)  from  (a),  we should  modify the construction slightly and  add  2J 
       arbitrary elements after each portion.  The total number of elements increases then 
       by 2l+J  and is still acceptable.
          On the other hand, (b) easily implies (a):  we need to split the list in groups of 
       size 2J .  Then we get at most 2г groups, and only 2J  last elements are left outside 
       the groups.  Therefore, x is covered by some group.  Each group is determined by its 
       ordinal number and therefore has complexity i  (plus logarithmic term that covers 
       the complexity of the list).
          To get  (a)  from  (c),  we split each group into pieces of size 2J  (except for one 
       last piece that can be smaller).  The number of full pieces is at most 2г, since the 
       length of the list is at most 2*+J .  The same is true for the number of non-full pieces.
            436                         14.  ALGORITHMIC STATISTICS
            So every piece can be specified by its ordinal number,  so its complexity does not 
            exceed i.
                So the properties (a)-(c) are equivalent (modulo logarithmic change in parame­
            ters), and it remains to show that they are equivalent to (d).  Evidently, (d) implies 
            (b), so it is enough to show that (a) implies (d).
                So let us assume that x is an element of some finite set A that has complexity 
            at most i and size at most    .  All elements of A have complexity at most i + j + 
            0(log(i + j)).  As usual, we ignore the logarithmic term and hope that the reader 
            can make the necessary corrections.
                Assume also that an enumerated list is given that includes all the strings of 
            complexity at most i + j.  We want to show that x will appear in this list not too 
            close to the end and at least    strings will follow it.  Knowing the set A, we may 
            perform the enumeration until all the elements of A appear in the list.  Let В be the 
            part of the list enumerated at that moment.  The set В is a finite set of complexity 
            at  most  i  (since  it  is  determined  by  A  and  the  enumerating algorithm,  which is 
            assumed to be simple).  Now consider the (lexicographically) first 2J strings outside 
            B.  Each of these strings is determined by В (of complexity i) and ordinal number 
            (at most j bits), so they have complexity at most i + j.  And all these strings should 
            appear in the enumeration after x.                                               □
                One could say that we have introduced an additional classification of strings of 
            complexity at most I by measuring the distance to the end of the list.  In terms of 
            our two-dimensional stratification, we can speak of an increasing sequence of sets 
            Sij on the diagonal i+j — I.  (Strictly speaking, the increasing sequence is obtained 
            only after logarithmic corrections.)  Random strings of length n ^ I — 0(\ogl) (i.e., 
            the strings of length n and complexity n) are at the beginning of this classification, 
            having (I * 0)-descriptions.  At the other end we have (few) strings that have only 
            (0 * ^-descriptions.
                 353        Show that  all  strings  at  the  end  of the  enumerated  list  of strings  of 
            complexity at most n (that are followed only by poly(n) strings) are almost equal 
            in the sense that the conditional complexity of one of them given the other one is 
            O(logn).
                One might say that the difference between I and the logarithm of the number of 
            strings after x in the enumerated list of all strings of complexity at most I measures 
            how strange x is.  (The equivalence of (b)  and  (d)  guarantees that this measure 
            does not  depend significantly on the choice of enumeration.)  Random strings of 
            length at most I — O(logZ) are not strange at all, while the strings that are close to 
            the end of the list, have maximal strangeness (close to I).  But one should keep in 
            mind the following:
                   •  The strangeness of a given string x of complexity к  (that is determined 
                     by its position in the enumerated list of all strings of complexity at most 
                     к)  can decrease significantly if we consider the same x as an element of 
                     the  list  of all  strings  of complexity  at  most  I  for  some  I  >  k.  In  fact, 
                     each string x determines a function that maps I  ^  C(x)  to the number 
                     of strings after x in the enumeration of strings of complexity at most I. 
                     It is essentially the same curve we considered before (the boundary curve 
                     for Px) but transformed into other coordinates:  for every I we look at the 
                     moment when the diagonal line i + j — I gets inside Px.
                       14.3.  TWO-PART  DESCRIPTIONS  437
            •  The strangeness of strings x and y can be very different even if C(x\ у) « 0 
             and C(y\x)  ~ 0 at the same time.  (Indeed,  if Z  >  C(x) + 0(\ogC(x)), 
             then the shortest description for a string x is random and is not strange 
             even if x were.)
               However,  if x  and y  correspond to each other under a simple com­
             putable bijection, this is not possible (see the next problem).
          354   Assume that x and y correspond to each other under a bijection computed 
       by a program of complexity t.  Prove that if x G Sij, then y G Si+o(t)j-
          Recall that there is a simple computable bijection that maps a string x to a 
       string y if and only if the total complexity of each of those strings conditional to 
       the other one is negligible (see Problem 31 on p. 36).
          By very similar arguments as those used to prove Theorem 254, we can show 
       that kn (and also mn from Theorem 15 (p. 25)) for different n are closely related:
          355   Prove that for all n' < n the string kn>  (i.e., the binary expansion of the 
       number kn> ) is equivalent to the length n' prefix of the string kn.  (Two strings x,y 
       are called equivalent if both conditional complexities C(x | y), C(y \ x) are O(logn)). 
       Show that strings mn have a similar property.
          (Hint:  (See[203].)  For kn  we have to show that  given any number T larger 
       than B(n — s) we are able to find all strings of complexity at most n except fewer 
       than 2s  such strings,  and the other way around.  Given such a T,  start  an enu­
       meration of strings of complexity at most n and output them in portions of size 
       2s.  After T steps  all the  complete portions will  appear.  Indeed,  the  number of 
       steps needed to output all complete portions can be computed from the number of 
       complete portions which has at most n — s bits.  The number of remaining strings 
       is fewer than 2s.  In the opposite direction, given a list of strings of complexity at 
       most n except fewer than 2s such strings, we again start an enumeration of strings 
       of complexity at most n and wait until all the given strings appear in that enumer­
       ation.  Let T denote the number of steps when it happens.  Then any number t > T 
       has complexity at least n — s.  Indeed, if C(t) < n — s, then consider 2s first strings 
       outside the list.  Each of them has complexity at most n, a contradiction.  For mn 
       the arguments are entirely similar.)
          The next result generalizes the statement of Problem 39 on p. 40:  If a string x 
       has many descriptions of size k, it has shorter descriptions.  Now we speak about 
       (i * ^-descriptions of x, i.e., finite sets containing x that have complexity at most 
       i  and cardinality at most 2À
          Theorem 255.  Assume that a string x has at least 2k sets as (i*j)-descriptions. 
       Then x has some (i * (j — k))-description and even some ((г — к) * j)-description.
          In this statement we omit (as usual) the logarithmic error terms (the parameters 
       should  be  increased  by  0(log(? + j + k))).  The word  “even”  reminds  us  about 
       Theorem 252 that allows us to convert (г—fc)*j-descriptions to i*(j—^-descriptions.
          PROOF.  The first (simpler) statement is an easy consequence of the arguments 
       used in the proof of Theorem 254.  Let us enumerate all sets A of complexity at 
       most i and size at most  2J  and see which strings belong to 2k  or more sets  (are 
       covered with multiplicity at least 2k).  We have at most 2*+J)2k such elements, i.e., 
       2г+j-k^ ancj these elements can be enumerated in at most 2l groups (each new set A 
       may create one new group).  So it remains to recall statement (c) of Theorem 254.
             438                         14.  ALGORITHMIC  STATISTICS
                 To get a stronger second statement, we need to decrease the number of groups 
             in  this  argument  to  2г~к  (keeping the number  of elements approximately at  the 
             same level).  It can be done as follows.  Again we enumerate sets of complexity at 
             most i and size at most 2J and look at the strings that are covered many times.  But 
             now we also consider the strings that  are covered with multiplicity 2k~x  (half of 
             the full multiplicity considered before); we call them candidates.  When an element 
             with full multiplicity appears, we output this element together with all candidates 
             that exist at that moment.
                 In this way we may output elements that will never reach the full multiplicity, 
             but this is not a problem since the total number of emitted elements can increase at 
             most twice compared to our count.  The advantage is that the number of groups is 
             now much smaller:  after all candidates are emitted, we need at least 2fc_1 new sets 
             to  get  a new element with full multiplicity  (its  multiplicity should  increase from 
             2 k ~ 1  to 2 k).                                                                   □
                 This result has the following important corollary:
                 Theorem 256.  If a string x has an (i*j)~ description A such that C(A\x) ^ k, 
             then x has also an (i * (j — k))-description and even an ((г — к) * j)-description.
                 Again we omit the logarithmic corrections needed for the exact formulation.
                 PROOF.  Knowing  x  and  the  values  of г  and  j  (the  latter  information  is  of 
             logarithmic  size),  we  can  enumerate  all  (i * ^-descriptions  of x.  Therefore,  the 
             complexity of each (г * j)-description given x does not exceed the logarithm of the 
             number of descriptions, and if there is an (i*j)-description A with large C(A \ x), this 
             means that there are many descriptions, and we can apply the previous theorem.  □
                 This statement shows that the descriptions with optimal parameters  (on the 
             boundary of Px for a given x) are simple relative to x.  Which, intuitively speaking, 
             is not surprising at all:  If a description contains some irrelevant information  (not 
             related to x), it hardly could be optimal.
                 Historical remarks.  The idea of considering two-part descriptions with optimal 
            parameters goes back to Kolmogorov.  Theorem 252 was mentioned by Kolmogorov 
             in  his  talk  in  1974  [82].  It  appeared  in  print  in  [60,  178].  Possible  shapes  of 
            the set Px  (Theorem 253) were found in [203].  The enumerations of all objects of 
             bounded complexity and their relation to two-part descriptions were studied in [60, 
             Section III, Е].  Theorem 254,  although inspired by  [60]  and  [203],  is presumably 
            new.  Theorems 255 and 256 appeared in [203].
                                  14.4.  Hypotheses of restricted type
                 In  this  section  we  consider  the  restricted  case:  the  sets  (considered  as  de­
            scriptions, or statistical hypotheses) are taken from some family A that is fixed in 
             advance.  (Elements of A are finite sets of binary strings.)  Informally speaking, this 
            means that we have some a priori information about the black box that produces a 
            given string:  This string is obtained by a random choice in one of the Л-sets, but 
            we do not know in which one.
                 Before we had no restrictions  (the family A was the family of all finite sets). 
            It turns out that the results obtained so far can be extended (with weaker bounds) 
            to other families that satisfy some natural conditions.  Let us formulate these con­
            ditions.
                                  14.4.  HYPOTHESES  OF  RESTRICTED  TYPE                  439
                 (1)  The family A is enumerable.  This means that there exists an algorithm 
            that prints elements of A as lists, with some separators (saying where one element 
            of A ends and another one begins).
                (2)  For every n the family A contains the set Bn of all n-bit strings.
                (3) The exists some polynomial p with the following property:  for every A G A, 
            for every natural n, and for every natural c < #A the set of all n-bit strings in A 
            can be covered by at most p(n) ■ фА/с sets of cardinality at most c from A.
                For a string x we denote by      the set of pairs  (i,j)  such that x has  (i * j)~ 
            description  that belongs  to A.  The set    is  a subset  of Px  defined  earlier;  the 
            bigger A is, the bigger is P^.  The full set Px is P^  for the family A that contains 
            all finite sets.
                Assume that the family A has properties (l)-(3).  Then for every string x the 
            set P£ has properties close to the properties of Px proved earlier.  Namely, for every 
            string x of length n the following is true:
                   •  The set P^ contains a pair that is 0(logn)-close to (0,n).  Indeed, prop­
                     erty  (2)  guarantees  that  the  family  A  contains  the  set  Bn  that  is  an 
                      (O(logn) * n)-description of x.
                   •  The set P^ contains a pair that is 0(l)-close to  (C(æ),0).  Indeed, con­
                     dition  (3)  applied to c = 1 and A = Bn says that every singleton belongs 
                     to A, therefore each string has a ((C(x) + 0(1)) * 0)-description.
                   •  The adaptation of Theorem 252 is true:  if (i.j)  G P^, then
                                      (г + к + O(logn), j — к) e P
                     for every к ^ j.  (Recall that n is the length of x.)  Indeed, assume that 
                     x has an  (i * j)-description A G A.  For a given к we enumerate A until 
                     we find a family of p(n)2k sets of size 2~кфА  (or less)  in A that covers 
                     all  strings  of length n  in  A.  Such  a family  exists  due  to  (3),  and p  is 
                     the polynomial from  (3).  The complexity of the set  that  covers x does 
                     not  exceed  г + к + 0(logn + log A:),  since this  set  is  determined  by  A, 
                     n,  к  and  the  ordinal  number  of the  set  in  the  cover.  We  may  assume 
                     without loss of generality that  к  ^ n,  otherwise  {x}  can be used as an 
                     ((г + к + O(logn)) * (j — A;))-description of x.  So the term 0(log k) can be 
                     omitted.
                Example.  Consider  the  family  A  formed  by  all  balls  in  Hamming’s  sense, 
            i.e.,  the  sets  By^r  — {x  |  l(x) — l(y),d(x,y)  <  r}  (here  l(u)  is  the  length  of 
            binary string и and d(x, y) is the Hamming distance between two strings x and у 
            of the same length).  The parameter r is called the radius of the ball and у is its 
            center.  Informally speaking, this means that the experimental data were obtained 
            by changing at most r bits in some string у (and all possible changes are equally 
            probable).  This  assumption  could  be reasonable  if some string у  is sent  via an 
            unreliable channel.  Both parameters у and r are not known to us in advance.
                 356  Prove  that  for  r  ^  n  the  set  ®n  of n-bit  strings  can  be  covered  by 
            poly(n)2n/V Hamming balls of radius r, where N stands for the cardinality of such 
            a ball (i.e.,V = l+n+•■•+ (;)).
                (Hint:  Consider  N  balls  of radius  r  whose  centers  are  randomly  chosen  in 
            Bn.  For a given x,  the probability of not  being covered  by  any of them equals 
            (1 — V/2n)N < e~VJV/2".  For N = n In 2 • 2n/V this upper bound is 2~n, so for this 
            N the probability of leaving some x uncovered is less than 1.)
        440              14.  ALGORITHMIC  STATISTICS
           357 Prove that this family (of all Hamming balls) satisfies conditions (l)-(3)
        above.
           (Hint for (3):  Let A be a ball of radius a, and let c be a number less than #A. 
        We need to cover A by balls of cardinality c or less.  Without loss of generality we 
        may assume that a ^ n/2.  Indeed, if a > n/2, then we can cover A by two balls 
        Ao, A\ of radius n/2 (the set of all n-bit strings can be covered by two balls of radius 
        n/2, whose centers are the all-zero sequence and all-one sequence).  Assuming that 
        the statement holds for Aq and A\, we cover both Ao  and A\  and then join the 
        obtained families of balls.  As the cardinality of both Ao,Ai  is not more than that 
        of A, we are done.
          Let b be the maximal integer in the interval 0 • • • n/2 such that the cardinality 
        |H|  of a ball of radius  b does not  exceed c.  We will  cover A  by Hamming balls 
        of radius b.  When we increase the radius of the ball by one,  its size increases at 
        most n + 1 times.  Therefore, \B\ ^ c/(n + 1), and it suffices to cover A by at most 
        poly(n)|A|/|F?| balls of radius b.
          Cover all the strings that are at distance at most b from the center of A by one 
        ball of radius b that has the same center as A.  Partition the remaining points into 
        spheres of radii d — b + 1,..., a:  the sphere of radius d consists of all strings  at 
        Hamming distance exactly d from the center of A.  As the number of those spheres 
        is at  most n,  it suffices,  for every d €  (b, n/2],  to cover a sphere of radius d by at 
        most poly(n)|S'|/|H| balls of radius b.
          Fix d and a sphere S of radius d 6  (b, n/2].  We will show that for some f  a 
        small family of balls whose centers are at distance /  from the center of S covers S. 
        Let /  be the solution to the equation b + f(l — 2b/n) = d rounded to the nearest 
        integer.  Consider any ball В of radius b whose center is a distance /  from the center 
        of S.
          We claim that a fraction at least 1/ poly(n) of points in В belong to S.  Indeed, 
        let  X  and у denote the centers of S and В,  respectively.  Let P denote the set of 
        all indexes i from 1 to n where у coincides with x  (i.e., Xi = yi), and let Q stand 
        for the complement of P.  Choose a set of (b/n)\P\  indexes from P and another 
        set  of (b/n)\Q\  indexes from Q.  Then flip the bits of у with chosen indexes.  The 
        resulting string y'  is  at  distance  (b/n)\P\ + (b/n)\Q\  — b from у  and  at  distance 
        f  — (b/n)f + (n—f)(b/n) — d from x.  Thus y' belongs to the intersection of В and S. 
        The number of strings y' that can be obtained in this way equals (f^ n^) ((n-^b/n)) • 
        Up to a factor poly(n) this number equals
                  2fh(b/n,l — b/n) + (n—f)h(b/n,l—b/n)      ^nh(b/n,l—b/n)
        On the other hand, the cardinality |i?| of a ball of radius b is equal to this number 
        as well, up to a factor poly(n).
          Thus every ball В of radius b with center at distance /  from x covers at least 
        \B\/poly(n) of points from S.  Choose such a ball В at random.  All points z € S 
        have the same probability of being covered by B.  As each ball В covers \B\ / poly(n) 
        of points  from  S,  this  probability  is  at  least  |^|/(|*5'| poly(77.)).  Hence  there  is  a 
        polynomial p such that p(n)\S\/\B\ random balls of radius b with centers at distance 
        /  from x cover S with positive probability.
           358  Consider the family A that consists of all Hamming balls.  Prove that 
        there exists a string x for which the set P'£■ is much smaller than the set Px.  (The
                                      14.4.  HYPOTHESES  OF  RESTRICTED  TYPE                          441
              exact statement is for some positive e and for all sufficiently large n there exists a 
              string X of length n such that the distance between            and Px exceeds en.)
                   (Hint:  Fix some a in  (0,1/2)  and let  V be the cardinality of the Hamming 
              ball of radius an.  Find a set E of cardinality N — 2n/V such that every Hamming 
              ball of radius an contains at most n points from E.  (This property is related to 
              list  decoding  in  coding  theory.  The  existence  of such  a set  can  be  proved  by  a 
              probabilistic argument:  N randomly chosen n-bit strings have this property with 
              positive probability.  Indeed,  the probability of a random point  being in E is an 
              inverse of the number of points, so the distribution is close to Poisson distribution 
              with parameter 1, and tails decrease much faster than 2~n needed.)  Since E can be 
              found by an exhaustive search, we can assume that its complexity is О (logn) and 
              ignore it (and other O(logn)-terms) in the sequel.  Now let x be a random element 
              in E, i.e., a string x G E of complexity about log фЕ.  The complexity of a ball A of 
              radius an that contains x is at least C(x), since knowing such a ball and an ordinal 
              number of x in A D E, we can find x.  Therefore x does not have (log фЕ, log V)- 
              descriptions in A.  On the other hand, x does have a (0, log #  .^-description if we do 
              not require it to be in A ; the set E is such a description.  The point (log фЕ, log V) 
              is above the line C(A) + logфА — logфЕ, so P£ is significantly smaller than Px.)
                   359 Describe the set P^ for x constructed in the preceding problem.
                  (Hint:  The border of the set P     consists of a vertical segment C(A) = n—log V, 
              where log фА ^ log V, and the segment of slope —1 defined by C(A) + log фА — n, 
             where log V ^ log#A)
                  Let A be a family that has properties (l)-(3).  We now prove a (weaker) version 
             of Theorem 253 where the precision is only 0(\Jn logn) instead of О (logn).  Note 
             that with this precision the term 0(m) in Theorem 253 (which is proportional to 
             the complexity of the boundary curve) is not needed.  Indeed, if we draw a curve on 
             a cell paper with cell size 0 (yjn) or larger, the curve goes through 0 (^/n) cells and 
             can be described by O(yjn) bits, so we may assume without loss of generality that 
             the complexity of the curve (the sequence U in the statement below) is 0 (\/n).
                  Theorem  257.  Let к  ^  n  be  two  integers,  and  let to  >  ti  >  • • •  >  tk  be  a 
              strictly  decreasing sequence  of integers  such  that to  ^  n  and t-k  —  0.  Then there 
              exists  a string x  of complexity к + 0 (\/nlogn)  and length n + O(logn) for which 
              the  distance  between the set P a n d  the set T = {(г, j)  \  (i ^ k)     (j  ^ ti)}  is  at 
              most 0 (\Jnlogn).
                  Proof.  The proof is similar to the proof of Theorem 253.  Let us first recall 
             that  proof.  We consider the string x that is the lexicographically first string  (of 
             suitable length n’) that is not covered by any bad set, i.e., by any set of complexity 
             at  most i and size at most  2J,  where the pair  (г, j)  is at the boundary of the set 
             T.  The length n7 is chosen in such a way that the total number of strings in all 
             bad sets is strictly less than 2n .  On the other hand, we need good sets that cover 
             x.  For every boundary point (i,j) we construct a set A,j that contains x and has 
             complexity close to i and size 2À  The set A.j  is constructed in several attempts. 
             Initially Aij  is the set  of lexicographically first  2J  strings of length n'.  Then we 
             enumerate bad sets and delete all their elements from A,j-  At some step, A,j may 
             become empty.  We then fill it with 2J  lexicographically first strings that are not 
             in the bad sets (at the moment).  By construction the final A j  contains the first 
             x that is not in a bad set  (since it is the case all the time).  And the set A,j  can
        442              14.  ALGORITHMIC  STATISTICS
        be described by the number of changes (plus some small information describing the 
        process as a whole and the value of j).  So it  is crucial to  have an upper bound 
        for the number of changes.  How do we get this bound?  We note that when Aij 
        becomes empty,  it is filled again,  and all the new elements should be covered by 
        bad sets before the new change could happen.  Two types of bad sets may appear: 
        small ones (of size less than 2? ) and large ones (of size at least 2J ).  The slope of the 
        boundary line for T guarantees that the total number of elements in all small bad 
        sets does not exceed 2г+?  (up to a poly(n)-factor), so they may make Aij  empty 
        only 2г times.  And the number of large bad sets is 0(2г), since the complexity of 
        each is bounded by i.  (More precisely, we count separately the number of changes 
        for Aij that are first changes after a large bad set appears, and the number of other 
        changes.)
          Can we use the same argument in the new situation? We can generate bad sets 
        as before and have the same bounds for their sizes and the total number of their 
        elements.  So the length n' of x can be the same (in fact,  almost the same, as we 
        will need now that the union of all bad sets is less than half of all strings of length 
        n'\  see  below).  Note that we now may enumerate only bad sets in A, since A is 
        enumerable, but we do not even need this condition.  What we cannot do is let Aij 
        be the set of the first non-deleted elements:  we need Aij to be a set from A.
          So we now go in the other direction.  Instead of choosing x first and then finding 
        a suitable good Aij  that contains x,  we construct the sets Aij  G A that change 
        in time in such a way that (1) their intersection always contains some non-deleted 
        element (an element that is not yet covered by bad sets) and (2) each Aij has not 
        too many versions.  The non-deleted element in their intersection (in the final state) 
        is then chosen as x.
          Unfortunately, we cannot do this for all points (г, j) along the boundary curve. 
        (This explains the loss of precision in the statement of the theorem.)  Instead, we 
        construct good sets only for some values of j.  These values go down from n to 0 
        with step  л/nTögn.  We select  N =  \Jnf logn points  (z'i, ji), • •., (гдг, Jn)  on the 
        boundary of T; the first coordinates ii,... An form a non-decreasing sequence, and 
        the second coordinates j i,..., Jn split the range n ■ ■ • 0 into (almost) equal intervals 
        (ji  = n, jjH  =0).  Then we construct good sets of sizes at most 2J1,..., 2JJV, and 
        denote them by A\,..., An-  All these sets belong to the family A.  We also let Ao 
        be the set of all strings of length n' — n + O(logn);  the choice of the constant in 
        O(logn) will be discussed later.
          Let us first  describe the construction of A i,... ,An  assuming that the set of 
        deleted elements is fixed.  (Then we discuss what to do when more elements are 
        deleted.)  We construct As  inductively  (first Ai,  then A<i  etc.).  As we have said, 
        #AS ^ 2Js  (in particular, An is a singleton), and we keep track of the ratio
              (the number of non-deleted strings in Ao П A\ П • • • П As)/2Js.
        For  s  —  0  this  ratio  is  at  least  1/2;  this  is  obtained  by  a suitable  choice  of n' 
        (the  union of all bad  sets should cover at  most half of all n'-bit  strings).  When 
        constructing the next As,  we ensure that this ratio decreases only by a poly(n)- 
        factor.  How?  Assume that As_i  is already constructed;  its size is at most 2Js~1. 
        Condition (3) for A guarantees that As_i can be covered by Л-sets of size at most 
        2Js, and we need about 2Js-1-J'5  covering sets (up to a poly(n)-factor).  Now we let 
        As be the covering set that contains the maximal number of non-deleted elements 
        in Aq П • • • П As-\.  The ratio can decrease only by the same poly(n)-factor.  In this
                                    14.4.  HYPOTHESES  OF RESTRICTED TYPE                        443
             way we get
                    (the number of non-deleted strings in Ao П A\ П • ■ ■ П As) ^ a  >s2Ja'/2,
             where a stands for the poly(n)-factor mentioned above.2
                 Up to now we assumed that the set of deleted elements is fixed.  What happens 
             when more strings are deleted?  The number of the non-deleted elements in Aq П 
             •••rij4s can decrease, and at some point and for some s it can become less than the 
             declared threshold vs = a"s2J“/2.  Then we can find minimal s where this happens 
             and rebuild all the sets As, .As+i,... (for As the threshold is not crossed due to the 
             minimality of s).  In this way we update the sets As from time to time, replacing 
             them (and all the consequent ones) by new versions when needed.
                 The problem with this construction is that the number of updates  (different 
             versions of each As)  can be too big.  Imagine that after an update some element 
             is  deleted,  and  the  threshold  is  crossed  again.  Then  a new  update  is  necessary, 
             and after this update the next deletion can trigger a new update, etc.  To keep the 
             number of updates reasonable, we will ensure that after the update for all the new 
             sets Ai  (starting from As)  the number of non-deleted elements in Aq П • • • П A\,  is 
             twice bigger than the threshold щ = a~l2v /2.  This can be achieved if we make the 
             factor a twice as big:  since for As_\  we have not crossed the threshold, for As we 
             can guarantee the inequality with additional factor 2.
                 Now let  us prove the bound for the number of updates for some As.  These 
             updates  can  be of two  types:  first,  when  As  itself starts  the  update  (being  the 
             minimal s where the threshold is crossed); second, when the update is induced by 
             one of the previous sets.  Let  us estimate the number of the updates of the first 
             type.  This update happens when the number of non-deleted elements  (that was 
             at least 2us immediately after the previous update of any kind) becomes less than 
             vs.  This means that at least us elements were deleted.  How can this happen?  One 
             possibility is that a new bad set of complexity at most is  (a large bad set) appears 
             after the last update.  This can happen at most 0(2îs)-times, since there are at most 
             0(2?)-objects of complexity at most i.  The other possibility is the accumulation 
             of elements deleted due to small bad sets, of complexity at least is  and of size at 
             most 2Js.  The total number of such elements is bounded by nO(2îs+j4), since the 
             sum ii + ji  may only decrease as I increases.  So the number of updates of As not 
             caused by large bad sets is bounded by
                 nO(2is+j*)/4      0 {п21«+э°)    0 (nas2is) = 2is+N°(lo&n') — 2*s+°(v/nlogrd
                                     a~s2i°
             (recall that s < N,  a = poly(n), and N « ^Jn/ logn).  This bound remains valid 
             if we take into account the induced updates (when the threshold is crossed for the 
             preceding sets:  there are at most N ^ n these sets, and an additional factor n is 
             absorbed by O-notation).
                 We conclude that all the versions of As have complexity at most
                                               is + 0 (y/n\ogn),
             since each of them can be described by the version number plus the parameters 
             of the  generating  process  (we  need  to  know  n  and  the  boundary  curve,  whose
                 2Note that for the values of s close to N , the right-hand side can be less than 1; the inequality 
             then claims just  the existence of non-deleted elements.  The induction step  is still  possible:  the 
             non-deleted element is contained in one of the covering sets.
             444                         14.  ALGORITHMIC  STATISTICS
             complexity is  0 (y /n )  according to  our  assumption,  see  the  discussion  before the 
             statement of the theorem).  The same is true for the final version.  It remains to 
             take X in the intersection of the final As.  (Recall that An is a singleton, so the final 
             An is  {a:}.)  Indeed,  by construction, this x has no bad  (г * j)-descriptions where 
             (i,j) is on the boundary of T.  On the other hand, x has good descriptions that are 
             0 (\/^logn)-close to  this  boundary  and  whose vertical coordinates  are  y/n log n- 
             apart.  (Recall that the slope of the boundary guarantees that horizontal distance 
             is less than the vertical distance.)  Therefore the position of the boundary curve for 
             P'£■  is determined with precision 0 {y/n\ogn), as required.3                       □
                 Remark.  In this proof we may use bad sets not only from A.  Therefore, the 
             set P® is close to T for every family В that contains A, and it is not even needed 
             that В satisfies requirements (l)-(3) itself.
                  360 Provide the missing details in this argument.
                  36Ï (1)  Let  i   be  a string  of length  n  and  let  r  be  a natural  number  not 
             exceeding nj2.  By CT(x) we denote the minimal (plain) complexity of a string у 
             of the same length n that differs from x in at most r positions.  Prove that  (with 
             О (logn) precision) the value of Cr(x) is the minimal i such that x has (i*logF(r))- 
             description that is a Hamming ball.  (Here V(r)  is the cardinality of a Hamming 
             ball of radius r in B71.)
                 (2)  Describe  all  the  possible shapes  of the function  Cr{x)  as a function of r 
             (that appear for different x) with precision 0 (y /n  logn).
                 (Hint:  For every x in Bn we have C q(x)  = C(x) and Cn(x) = O(logn).  Also 
             we have
                               0 ^ Ca(x) -  Cb{x) ^ log{V{b)/V{a)) + O(logn) 
             for every a < b ^ nj2.  On the other hand, for every к ^ n and for every function 
             t:  {0, 1,..., n/2} such that
             t(0) = k, t(n/2) — 0 and 0 ^ t(a) — t(b) ^ log(V(b)/V (a)) for every a < b ^ n/2, 
             there exists a string x of length n and complexity k+0(y/n logn) such that Ca(x) = 
             t(a) + 0 (y/n logn) for all a = 0,1,... n/2.)
                 We can again look at  the error-correcting codes:  If a  (Kolmogorov-)  simple 
             set  of codewords  has  distance d,  then  for  a codeword  x  in  this  set  the  function 
             Cr(x) does not significantly decrease when r increases from 0 to d/2  (indeed, the 
             codeword can be reconstructed from the approximate version of it).
                 Complexity measure  Cr(x)  was  introduced  in  the  paper  [69].  In  [54],  this 
             notion  was  generalized  to  conditional  complexity.  There  are  two  natural  gener­
             alizations,  uniform  and  non-uniform  ones.  The  uniform  conditional  complexity 
             CrS(x I y)  is  defined  as  the  minimal  length of a program that  given  any string y' 
             at Hamming distance at most s from у outputs a string x' at Hamming distance 
             at  most  r  from x.  It  is  important  that  x'  may depend  on  y'.  The  non-uniform 
             conditional complexity Crs(x\y) is defined as maxy/ mkv C(x' \ y') where x',y' are 
             at Hamming distance at most r, s from x, у, respectively.  The difference between 
             the uniform and the non-uniform definitions is the following.  In the non-uniform 
             definition the program to transform y' to x' may depend on y' while in the uniform
                 3Now we see why N was chosen to be  у/n / logn:  the bigger N   is,  the more points on the 
             curve we have,  but then the number of versions of the good sets and their complexity increases, 
            so we have some trade-offs.  The chosen value of N  balances these two sources of errors.
                                  14.4.  HYPOTHESES  OF  RESTRICTED  TYPE                  445
            definition the same short program must transform every y' to an x'.  This implies 
            that the non-uniform complexity cannot exceed the uniform one.  The non-uniform 
            complexity can by much less than the uniform one (see [54] for details).
                Theorem 254 provided a criterion saying whether a given string has а (г * j)- 
            description (unrestricted).  It is not clear whether similar criterion could be found 
            for an arbitrary class A of allowed descriptions.  On the other hand, Theorem 255 
            is  (with minimal changes) valid for an arbitrary enumerable family of descriptions; 
            see conditions (l)-(3) on p. 439.
                Theorem 258.  Let A be an enumerable family of finite sets.  Assume that x is 
            a string of length n that has at least 2k  different (i * j)-descriptions from A.  (Recall 
            that the (i* j)-description of x is a finite set of complexity at mosti and cardinality 
            at most 2J  containing x.)  Then x has some ((г — к) * j)-description from A.
                Therefore, if A satisfies also the requirement  (3), the string x in this theorem 
            also has an (г * (j — fc))-description.  (See above about the version of Theorem 252 
            for restricted descriptions.)
                As usual, these statements need logarithmic terms to be exact (this means that 
            0 (log(n+i+j+fc))-terms should be added to the description parameters).
                Proof.  Let us enumerate all (i * j)-descriptions from A, i.e., finite sets that 
            belong to A,  and  have  cardinality  at  most  2J  and complexity  at  most  i.  For  a 
            fixed n, we start a selection process:  some of the generated descriptions are marked 
            (=selected) immediately after their generation.  This process should satisfy the fol­
            lowing requirements:  (1) at any moment every уг-bit string x that has at least 2k 
            descriptions  (among enumerated ones) belongs to one of the marked descriptions; 
            (2) the total number of marked sets does not exceed 2г~кр(п, k,i,j) for some poly­
            nomial p.  So we need to construct a selection strategy (of logarithmic complexity). 
            We present two proofs:  a probabilistic one and an explicit construction.
                Probabilistic proof.  First  we consider  a finite game  that  corresponds  to 
            our situation.  The game is played by two players, whose turn to move alternates. 
            Each player makes 2г  moves.  At each move the first player presents some set of 
            n-bit strings, and the second player replies saying whether it marks this set or not. 
            The second player loses,  if after some moves the number of marked sets exceeds 
            2г-/с+1(п _|_ i) in 2  (this specific value follows from the argument below) or if there 
            exists a string x that belongs to 2k sets of the first player but does not belong to 
            any marked set.
                Since this is a finite game with full information, one of the players has a winning 
            strategy.  We claim that the second player can win.  If it is not the case, the first 
            player has a winning strategy.  We get a contradiction by showing that the second 
            player has a probabilistic strategy that wins with positive probability against any 
            strategy of the first player.  So we assume that some (deterministic) strategy of the 
            first  player is fixed and consider the following simple probabilistic strategy of the 
            second player:  every set A presented by the first player is marked with probability 
            p = 2~k(n + 1) In2.
                The expected number of marked sets is р2г = 2г~к(п + 1) ln 2.  By Chebyshev’s 
            inequality,  the number of marked set exceeds the expectation by a factor 2 with 
            probability less than 1/2.  So it is enough to show that the second bad case (after 
            some move there exists x that belongs to 2k sets of the first player but does not 
            belong to any marked set) happens with probability at most 1/2.
             446                          14.  ALGORITHMIC STATISTICS
                 For that, it is enough to show that for every fixed x the probability of this bad 
             event is at most  2“^ +1l   The intuitive explanation is simple:  if x belongs to 2k 
             sets, the second player had (at least) 2k chances to mark a set containing x (when 
             these 2k sets were presented by the first player), and the probability of missing all 
             these chances is at most (1 — p)2  ; the choice of p guarantees that this probability 
             is less than l/2 ~(n+x\   Indeed, using the bound (1 — l/x)x < 1/e, it is easy to show 
             that
                                       (1 — p)2k  < e- ln2(n+l) — 2~(n+l).
                 A meticulous reader would say that this argument  is not technically correct 
             since the behavior of the first player (and the moment when the next set containing 
             x is produced) depends on the moves of the second player, so we do not have inde­
             pendent events with probability 1 — p each (as it is assumed in the computation).4 
             The formal argument  considers for each t the event Rt  “after some move of the 
             second player,  the string x belongs to at least t sets provided by the first player, 
             but it does not belong to any selected set”.  Then we prove by induction  (over t) 
             that the probability of Rt does not exceed (1 —p)l.  Indeed, it is easy to see that 
             Rt is a union of several disjoint subsets (depending on the events happening until 
             the first player provides t + 1st set containing x), and Rt+i  is obtained by taking a 
             (1 — p)-fraction in each of them.
                 Constructive proof.  We consider the same game, but now we allow more 
             sets  to  be  selected  (replacing  the  bound  2г~к+1(п +  l)ln2  by  a  bigger  bound 
             2l~ki2n\n2),  and  we  also  allow  the  second  player  to  select  sets  that  were  pro­
             duced earlier  (not  necessarily upon  the preceding move of the first player).  The 
             explicit winning strategy for the second players performs simultaneously i — к + log i 
             substrategies (indexed by the numbers log(2fc/2), log(2fc/2) + 1,..., i).
                 The substrategy number s wakes up once in 2s  moves  (when the number of 
             moves already made by the first player is a multiple of 2s).  It forms a family S that 
             consists of 2s  last sets produced by the first player, and the set T that consists of 
             all strings x covered by at least 2k/i sets from S.  Then it selects some elements in 
             S in such a way that all x G T are covered by one of the selected sets.  It is done 
             by a greedy algorithm:  first take a set from  S that  covers a maximal part of T, 
             then take the set that covers a maximal number of non-covered elements, etc.  How 
             many steps do we need to cover the entire T?  Let us show that
                                                 (i/2k)n2s In 2
             steps are enough.  Indeed, every element of T is covered by at least 2k/i sets from 
             S.  Therefore,  some set  from S covers at least  #T2k/(i2s)  elements,  i.e.,  2k~s/i- 
             fraction of T.  At the next step the non-covered part is multiplied by (1 — 2k~s/г) 
             again, and after m2s-fcln2 steps the number of non-covered elements is bounded 
             by
                                 # I ( l - 2fc-7 f r t l n 2  < 2n(l/e)nln2 -  1,
                 4The same problem appears if we observe a sequence of independent trials.  Each of them is 
             successful with probability p, and then we select some trials (before they are actually performed, 
             based  on the  information  obtained  so  far)  and  ask what  is  the  probability  of the event  “f  first 
             selected trials were all unsuccessful”.  This probability does not exceed (1 — p)4; it can be smaller 
             if the total number of selected trials  is fewer than t with positive probability.  This scheme was 
             considered by von Mises when he defined random sequences using selection rules.
                               14.5.  OPTIMALITY  AND  RANDOMNESS  DEFICIENCY                    447
             therefore all elements of T are covered.  (Instead of a greedy algorithm one may 
             use a probabilistic argument and show that randomly chosen in2s~k In 2 sets from 
             S cover T with positive probability;  however, our goal is to construct  an explicit 
             strategy.)
                 Anyway, the number of sets selected by a substrategy number s does not exceed 
                                         т28-к(\п2)21- 8 =in2i~k\n2,
             and we get at most i2n2l~k In 2 for all substrategies.
                 It  remains to prove that  after each move of the second player every string x 
             that belongs to 2k or more sets of the first player also belongs to some selected set. 
             For the tth move we consider the binary representation of t,
                                  t = 2Sl + 2S2 + • ■ •  ,  where si  > S2 > 1 • •  •
             Since x does not belong to the sets selected by substrategies number si, S2, ■.., the 
             multiplicity  of x  among the first  2Sl  sets  is  less  than  2k/i,  the  multiplicity  of x 
             among the next 2S2 sets is also less than 2k/i, etc.  For those j with 2Sj  < 2k/i, the 
             multiplicity of x among the respective portion of 2si sets is obviously less than 2k ji. 
             Therefore,  we conclude that the total multiplicity of x is less than i ■ 2k/i = 2k, 
             and the second player does not  need to  care about  x.  This finishes the explicit 
             construction of the winning strategy.
                 Now we can assume without loss of generality that the winning strategy has 
             complexity at most 0(log(n + к + i + j)).  (In the probabilistic argument we have 
             proved the existence of a winning strategy, but then we can perform the exhaustive 
             search until we find one; the first strategy found will have small complexity.)  Then 
             we use this simple strategy to play against the strategy of the second player which 
             enumerates all .Д-sets of complexity less than i and size 2^  (or less).  The selected 
             sets can be described by their ordinal numbers (among the selected sets), so their 
             complexity is bounded by i — к (with logarithmic precision).  Every string that has 
             2 k different (г*^-descriptions in A will also have one among the selected sets, and 
             that is what we need.                                                                □
                 As before (for arbitrary sets), this result implies that explanation with minimal 
             parameters are simple with respect to the explaining object:
                 Theorem 259.  Let A be an enumerable family of finite sets.  If a string x has 
             an (i * j)-description A € A such that C(A\x)  <  k,  then x  has  an ((г — к) * j)- 
             description in A.  If the family A satisfies condition (3)  on p.  439,  then x has also 
             an (i * (j — k))-description in A.
                 As usual, we omit the logarithmic corrections needed in the exact statement of 
             this result.
                 Historical remark.  All the results from this section, including non-trivial exer­
             cises,  are from  [204].  The probabilistic proof of Theorem 258 was independently 
             proposed by Michal Kouckÿ and Andrei Muchnik.
                              14.5.  Optimality and randomness deficiency
                 We have considered two ways to measure how bad a finite A is as an explanation 
             for a given object x:  the first is the randomness deficiency that was defined as
                                         d{x\A) = log # A — C{x I A) ;
          448                  14.  ALGORITHMIC STATISTICS
          the second one, which can be called the optimality deficiency and is defined as
                             ö(x\A) =log#A + C(A)-C(x),
          shows how far the two-part description of x using A is from the optimum.  How are 
          these two numbers related?  First let us make an easy observation.
             Theorem 260.  The randomness deficiency of a string x of a finite set A does 
          not exceed its optimality deficiency (with logarithmic precision,  as usual; here l(x) 
          stands for the length of x) :
                              d(x\ A) < 5{x\ A) + 0(log/(x)).
             Proof.  We need to prove that
                   logФА -  C(x\A) < logфА + C(A) -  C(x) + 0(logl(x)).
          Canceling the term log фА, we get an inequality
                           C(x) ^ C(A) + C(x\A) + 0(logl(x)).
          Its right-hand side is the complexity of the pair (x, A) with accuracy О (log C(x, A)), 
          and it is larger than C(x) with accuracy О (log C(x | A)).  Note that the bound we are 
          proving should hold with 0(log/(x))-precision,  and 0(logC'(x| A))  — 0(log/(x)).
                                                                         □
             This argument shows that the difference between these two deficiencies is close 
          to C(x, A) — C(x), i.e., to C(A\x) with precision 0(\ogl(x) + logC(A)), and this is 
          0(log/(x)) if C(A) = 0(C(x)).  (There is no sense in considering the explanations 
          that are much more complex than the object they try to explain, so we will always 
          assume that C(A) = 0(C(x)).)
             It is easy to give an example of a hypothesis whose optimality deficiency exceeds 
          significantly its randomness deficiency.  Let x be a random string of length n, and 
          let  В  be  the  set  of all  strings  of length  n  plus  some  random  string  у  of length 
          n — 1  that  is  independent  of x.  Then  C(B\x)  is close to  n,  and the  optimality 
          deficiency is about n, while the randomness deficiency is still small (including у in 
          the set of all strings of length n does not much change the randomness deficiency 
          of x in that set).  In this example, the hypothesis В looks bad from the intuitive 
          viewpoint:  It contains an irrelevant element у which has nothing in common with 
          the x that we try to explain.  Eliminating this y, we improve the hypothesis and 
          make its optimality deficiency close to its randomness deficiency (which is small in 
          both cases).
             Recall that we have proved Theorem 256 which shows that the situation in this 
          example is general:  If for a given hypothesis В for a string x the difference between 
          the optimality deficiency 5{x\B)  and randomness deficiency d(x\B) is large (this 
          difference is about C(B\x), as we have seen), then one can find another hypothesis 
          A of the same size and of the same (and even smaller by C(B\x)) complexity such 
          that S(x\ A) does not exceed d(x\B).
             Therefore, the question whether for a given string x there exists a set A with 
          C(A)  <  a  and  d(x\A)  <  ß  (asked  in  the  definition  of  (a, /3)-stochasticity),  is 
          equivalent  (with logarithmic precision)  to the question of whether there exists a 
          set  A  with  C(A)  <  a  and  S(x\A)  <  ß.  That  is,  the set  Px  contains  the  same 
          information about x as the set Qx of pairs  (a, ß)  for which x is  (a,/3)-stochastic, 
          but using different coordinates.
                             14.5.  OPTIMALITY  AND  RANDOMNESS  DEFICIENCY              449
                 362  Let  X  be  an n-bit  string of complexity  k.  Show that  the set  Px  (see
            Theorem 253) determines for which a and ß the string x is (a, /3)-stochastic:  this 
            happens iff the pair  (a, C(x) — a + ß)  is  in  Px  or  a  >  C(x)  (with  logarithmic 
            accuracy).
                 363  Prove the claim from p. 429:  the first inequality of Theorem 249 can be 
            replaced by a weaker inequality a + ß < n — O(logn).
                [Hint:  Consider the first string of length n that has no a*(n~ a) descriptions 
            (to be precise we need to subtract O(logn) from the parameters).  Its complexity 
            is close to a.  The previous problem implies that x is not (a, /3)-stochastic.)
                 364 Prove that  if a + ß  <  n — O(logn),  then  the  fraction  of non~(a, ß)- 
            stochastic strings is at least 2 ~a~^~°^ogn\
                (Hint:  Consider the first  2n~a~13 strings of length n  (in lexicographic order) 
            that do not have [a * (n — a))-descriptions (we omit logarithmic corrections in the 
            parameters).  Each of them has complexity at least a and at most a + n — a — ß = 
            n — ß.  The latter implies that for every x in this set the point  (a, C(x) — a + ß) 
            does not belong to Px.)
                 365  Prove that the first inequality of Theorem 251 can be replaced by the
            weaker inequality a + ß < n — O(logn).
                (Hint:  The proof of the upper bound remains almost the same:  the a priori 
            probability of a string provided by Problem 363 is at least 2~a.  The proof of the 
            lower bound used only the inequality a < ß — O(logn).)
                 366 For every x consider the set Qx of all pairs (a,ß) such that x is (a,ß)- 
            stochastic.  Characterize possible behaviors of Qx.
                [Hint:  Let x be an n-bit string of complexity k.  Then the set  Qx  is upward 
            closed (i.e.,  (a,ß)  G  Qx  implies  (a', ft)  G  Qx  for all a'  ^ a, ß'  ^ ß)  and contains 
            pairs (0, n — k) and (к, 0) with logarithmic precision (this means that Qx contains 
            some pairs (0(logn),n — к + O(logn)) and (k + 0(1), 0)).  On the other hand, let 
            к and n be some numbers, к ^ n, and let so,..., s& be a sequence of integers such 
            that n — к ^ so ^ si  ^    ^     = 0.  Let m be the complexity of this sequence.
            Then there exists a string x of length n and complexity к + O(logn) + 0(m) such 
            that Qx is O(logn) + O(m) close to the set S = {(a, ß) | (a ^ k) => (ß ^ sa)}.)
                367  Assume that for a string x and some a there exists a hypothesis that
            achieves minimal randomness deficiency among hypotheses of complexity at most 
            a, and its optimality deficiency exceeds its randomness deficiency by 7.  Then the 
            boundary of Px contains a segment of slope —1 that covers the interval (a — 7, a) 
            on the horizontal axis.
                (Hint:  Use the stronger statement of Theorem 256.)
                368 Let A be a family of finite sets that satisfies conditions (l)-(3) on p. 439. 
            Prove that for any x and any a ^ C(x) the following are equivalent with logarithmic 
            precision:
                   •  there exists a set A G A of complexity at most a with d(x | A) ^ ß;
                   •  there exists a set A G A of complexity at most a with S(x | A) ^ ß\
                   •  the point  (a, C(x) — a + ß) belongs to P^.
                369 Let A be an arbitrary family of finite sets enumerated by program p. 
            Prove that for every x of length at most n the following statements are equivalent
            450                       14.  ALGORITHMIC  STATISTICS
            up to an 0(C(p) + logC'(vl) + logn + log log #A)-change in the parameters:
                  •  there exists a set A G A such that d(x\ A) ^ ß\
                  •  there exists a set A E A such that S(x\ A) ^ ß.
                Historical remarks.  The existence of strings of length n and complexity about к 
            that are not (к, n — к + 0(logn))-stochastic was first proved in [60, Theorem IV.2]. 
            The study of possible shapes of the set Qx was initiated by V. V’yugin [211,  212] 
            using direct arguments (and not the relation between Qx and Px).  The descriptions 
            of possible shapes of Qx with accuracy О (logn) (Problem 366) is due to [203], where 
            reduction to the set Px is used.  Problems 367, 368, and 369 go back to [203, 204].
                                     14.6.  Minimal hypotheses
               Fix a string x.  We have associated with x the set  Px  consisting of all pairs 
            (a,ß) such that x has an (a*/^-description.  Those descriptions were considered as 
            “statistical hypotheses to explain x”.  What do they look like?  It turns out that we 
            can identify a more or less explicit class of models such that every model reduces in 
            a sense to a model from that class.  This class arises from the proof of Theorem 254.
               Let I be some number greater than C(x).  Then the list of all strings of com­
            plexity at most I contains x.  Fix some enumeration of this list (an algorithm that 
            generates all these strings; each appears only once).  We assume that this algorithm 
            is simple:  its complexity is O(logZ).  Let Ni  be the number of elements in the list. 
            Consider the binary representation of Ni, i.e., the sum
                          Ni = 2Sl + 2S2 + • • • + 2s*,  where s\  > S2 > ■ • ■ > st.
            According to this decomposition, we may split the list itself into groups:  first 2Sl 
            elements, next 2S2 elements, etc.  The string x belongs to one of these groups.  This 
           group  (the corresponding finite set)  can be considered  as a hypothesis  for x.  In 
           this way we get a family of models for x:  each I > C(x) produces some hypothesis, 
           denoted BXii in the sequel.
               The following two  theorems  prove  the  promised  properties  of these  models. 
           First,  they are  minimal,  i.e.,  they lie on the border of the set  Px.  Second,  each 
           model for x reduces in a sense to one of them.
               Theorem 261.  Assume that x belongs to the part Bxj  of size 2s  in this con­
            struction.  Then this part is an ((I — s) * s)-description of x  and the point (I — s, s) 
            is  on  the  boundary of Px.  (As usual,  the exact statement needs a logarithmic cor­
            rection:  this part is an ((I — s + 0 (\ogl)) *s)-description of x and the corresponding 
           point is in the О (log /)-neighborhood of the boundary of Px.)
               Proof.  To specify this part, it is enough to know its size and the number of 
           elements enumerated before it,  i.e.,  it  is  enough to  know  s,  I  and  all  bits  of Ni 
           except s last bits (i.e., I — s bits).  Also we need to know the enumerating algorithm 
           itself, but it has logarithmic complexity (as we assumed).  Therefore the complexity 
           of the part is I — s + O(logZ), and the number of elements is 2s, as we have claimed.
               If the point (I — s,s) were far from the boundary and were in Px together with 
           more than logarithmic neighborhood,  then the string x would have much better 
           two-part descriptions  (with the same or even smaller total length and with larger 
           size),  so Theorem 254(d) would imply that the string x appears in the list earlier 
            (more than 2s  elements follow x in the enumeration),  which is impossible in our 
           construction.                                                                □
                                             14.6.  MINIMAL  HYPOTHESES                               451
                  The next result explains in which sense these descriptions are universal.  Let 
              X  be  an  arbitrary string,  and let  A  be some finite set  that  contains x.  Let  I  be 
              the maximal complexity of the elements of A.  As before, let us split the strings of 
             complexity at most I (there are Ni of them) into parts corresponding to ones in the 
             binary representation of TV/.  Let В be the part that contains x,  and let 2s  be its 
             size.
                  Theorem 262.  The hypothesis В = Bxy  (considered as an explanation for x) 
              is not worse than A in terms of complexity and optimality deficiency:
                  (a)  C{B)KC{A) + 0{\ogl);
                  (b)  5{x IB) ^ S(x I A) + O(logZ);
                  (c)  C(B\A) ^ О (log/)  (the hypothesis В is simple given A).
                  Proof.  Knowing A and /, we can enumerate all strings of complexity at most I 
             until we see all the elements of A.  At that moment the string x already appears, and 
             it belongs to the part of size 2s, so there are only О(2s) strings yet to be discovered 
              (from this part and the smaller parts).  Therefore, we know N[ with precision О(2s), 
             and therefore we know its first I — s bits (with O(l)-advice).  And this information, 
             together with I and s, determines B.  Therefore,  C(B\A)  ^  O(logZ),  so we have 
             proved (c) and therefore (a).
                  The statement (b) follows directly from the construction.  Indeed, if C(A) = a 
             and log фА — ß, then all the strings in A have an (a*/3)-description and complexity 
             at most a + ß + O(loga), so their maximal complexity I does not exceed a + ß + 
             O(loga).  The two-part description we have constructed is an ((/ — s)*s)-description 
             (as the previous theorem shows), so its total length and optimality deficiency do 
             not exceed those of A.                                                                    □
                  The  relation  between  parameters  of descriptions  A  and  В  is  illustrated  by 
             Figure 54:  the dot corresponds to the parameters of A,  and the gray area shows 
             the possible parameters of B.
                  What happens if the initial hypothesis A is already on the boundary of Px? 
             Does it mean that В has the same parameters as A?  Generally, no:  the model В 
             may lie on the dashed part of the boundary of the grey area shown in Figure 54. 
             (It is not possible that В is inside the grey area, since in this case A will correspond 
             to the internal point of Px.)
                  In other words,  assume that the boundary of Px consists of vertical lines and 
             non-vertical  lines  with  slope  —1.   Then the left-upper  endpoints  of non-vertical
                                                  ß
                                                                     a
                  Figure 54.  The parameters of the hypothesis A and its simplification В
              452                           14.  ALGORITHMIC  STATISTICS
              segments correspond to the hypotheses of described type (since for such A the grey 
              area where В resides has only one common point with Px).
                  Notice that  the information that  is  contained in these  hypotheses,  does not 
              really depend on x:  the hypothesis В contains the same information as the (I — s)- 
              bit  prefix of the string JV).  As we have seen in Problem 355  (p. 437),  this prefix 
              can be replaced by Ni-S,  which has the same information as the first  I — s bits 
              of Chaitin’s    number.  Thus the larger the complexity of our model is, the more 
              information about       it  has.  This  is  discouraging,  since  the  number     does not 
              depend on x.
                  It  might  be that  other parameters  (than complexity and cardinality)  help to 
              distinguish  models  of the same size and complexity,  as explanations for x.  The 
              paper  [199]  suggests  one such parameter,  namely the total complexity A condi­
              tional  to  x.  In  all  our  examples  intuitively  right  models  for  x  have  small  total 
              complexity conditional to x.  On the other hand, one can show that models from 
              the universal family from Theorem 261 have large total complexity conditional to 
              some  of their  members.  We omit  the  proof of this  claim,  which  may  be  found 
              in  [199].
                  Note also that this observation (saying that different hypotheses contain almost 
              the same information) is applicable only to hypotheses of our special type and not 
              to  arbitrary hypotheses on the boundary of Px,  as the following example shows. 
              Let x be a random n-bit string.  Consider two hypotheses:  the set of n-bit strings 
              у  that  have the same first  half as x and the set  of n-bit strings у that  have the 
              same second half as x.  Both hypotheses have small optimality deficiency, but the 
              information contained in them is completely different.  (This does not contradict 
              our results above, since the set of all n-bit strings as В has better parameters than 
              both.)
                  Historical remarks.  Cutting the list of all strings of complexity at most к into 
              portions  according to the binary expansion of JV*  was  introduced in  [60],  where 
              it  was noticed that for к = C(x) we obtain in this way a model for x with small 
             optimality deficiency.  Later in  [203]  models of this type were considered also for 
              к > C(x), and Theorems 261 and 262 were proven.
                                           14.7.  A bit of philosophy
                  There are several philosophical questions related to the task of finding a good 
             two-part description for a given string x.  For instance, we can let x be the sequence 
             of all observations about the world made by mankind (encoded in binary) and then 
             consider scientific theories as models A for x.  Among those theories we want do 
             identify the right  ones.  Our criteria are the simplicity of the theory in question 
              (measured  by  the  Kolmogorov  complexity  of A—the  less  the  complexity  is  the 
             better), and the  “concreteness”  or the  “explanatory capability”  (measured by the 
             size of A—the less the size is, the more concrete the model is, hence the better). 
              One can also recall the ancient philosopher Occam and his razor  ( “entities must not 
             be multiplied beyond necessity”), which advises choosing the simplest explanation. 
              Or we can look for a scientific theory A such that the randomness deficiency of the 
             data x with respect to A is small (“a good theory should explain all the regularities 
             in the data” ).
                  There  are  also  more  practical  issues  related  to  algorithmic  statistics.  Kol­
             mogorov complexity can be considered as a theory of “ultimate compression” :  the
                                       14.7.  A  BIT  OF  PHILOSOPHY                    453
            complexity of a string x is the lower bound for its compressed size for compressors 
            without loss of information.  The closer to this bound the compressed size is the 
            better the compression method is (for files from a practically important family of 
            files).
                This applies to lossy data compression.  What about loss compression?  Nowa­
            days  many  compression  techniques  are  used  that  discard  certain  not  important 
            parts of the information that is being encoded.  Such methods allow us to decrease 
            the compressed size below Kolmogorov complexity.
                For  instance,  assume  that  we  are given  an  old  phonograph  record  that  has 
            scratches in random places on the record.  These scratches produce peaks on the 
            waveform of the sound (the two-dimensional plot of sound pressure as a function of 
            time).  Thus the original information has been distorted.  Due to this distortion the 
            Kolmogorov complexity of the record has been much increased (if there are many 
            scratches).  However,  if we care only about the general impression of playing the 
            record, the exact spots of the scratches are not important.  It is enough to store in 
            the compressed file only the general character of the scratches.
                In  other words,  our  phonograph record is  an element  of a large  family that 
            consists  of all  the  records with about  the same number of scratches of the same 
            type.  In this way we obtain a two-part description of the record:  the first part is 
            the description of this set (the clean record and statistical parameters of the noise) 
            and the second part identifies the exact spots of the scratches.  If our method of 
            compression discards the second part, then after decompression we will get another 
            record.  That  record  will  be  obtained  from  the  original  clean  record  by  adding 
            another noise with the same statistical parameters.  One can hope that the audience 
            will not notice the change.  Besides, if the decompressing program does not add any 
            noise  at  all to  the  clean record,  thus  “de-noising”  the record,  then we obtain an 
            even  better  result  (unless  of course  we  are  interested  in  listening  to  an  ancient 
            phonograph instead of listening to music).
               The statement of Problem 369 can be interpreted as follows using this analogy. 
            Assume that a string x was obtained from an unknown string y of the same length 
            by  adding a noise.  That  is,  for some  known natural number r the string x was 
            obtained by a random sampling in a radius-r Hamming ball with the center y.  We 
           want to de-noise x and to this end we are looking for a Hamming ball of radius r 
            that provides the minimal length two-part description for x (that is, the Hamming 
            ball of minimal complexity).  Assume that we have succeeded and such a ball is 
            found.  With high probability the randomness deficiency of x in the original ball 
            is  small.  By  Problem  369  (for the family of all  Hamming balls of radius r)  the 
            randomness deficiency of x in the ball we have found is small as well.  Thus the 
           second part in the found two-part description for x has no useful information.  In 
           other words,  the center of the ball we have found is a de-noised version of x  (in 
            particular, we have also removed the noise present in y).
               Here is another example of lossy compression via Kolmogorov complexity.  Kol­
            mogorov complexity of a high-resolution picture of a sand-dune is very large, as it 
            identifies  the locations of all individual grains of sand,  which are random.  For a 
           person who looks at that picture, the picture is just a typical element of the set of 
            all similar pictures, where the sand-dune is at the same place, has the same form, 
            and consists of the sand of the same type,  while individual sand grains may oc­
           cupy arbitrary spots.  If our compressor stores only the description of this large set
         454                 14.  ALGORITHMIC  STATISTICS
         and the decompressing program finds any typical element of that set, the person 
         contemplating the picture will hardly notice any difference.
            We should remember that this is just an analogy and we should not expect that 
         mathematical theorems on Kolmogorov complexity of two-part descriptions will be 
         directly applied in practice.  One of the reasons for that is our ignoring the compu­
         tational complexity of decompressing programs and ignoring compressing programs 
         at all.  It might be that it is this ignoring that implies paradoxical independence of 
         some minimal models on the string x mentioned earlier.
                                   APPENDIX  1
              Complexity and foundations of probability
            In  this section there are no theorems and no proofs.  Instead,  we discuss the 
         foundations of probability theory (the connection between probability theory as a 
         part of mathematics, and its applications to the real world), especially the role of 
         the algorithmic information theory, following [180].
                            Probability theory paradox
            One often describes the natural sciences framework as follows:  A hypothesis is 
         used to predict something, and the prediction is then checked against the observed 
         actual behavior of the system.  If there is a contradiction, the hypothesis needs to 
         be changed.
            Can we include probability theory in this framework?  A statistical hypothesis 
         (say, the assumption of a fair coin) should be then checked against the experimental 
         data (results of coin tossing)  and rejected if some discrepancy is found.  However, 
         there  is  an  obvious  problem:  The  fair  coin  assumption  says  that  in  a series  of, 
         say,  1000 coin tossings  all of the  21000  possible outcomes  (all  21000  bit  strings of 
         length 1000) have the same probability 2“1000.  How can we say that some of them 
         contradict the assumption while other do not?
            The same paradox can be explained in a different way.  Consider a casino that 
         wants to outsource the task of card shuffling to  a special factory that  produced 
         shrink-wrapped  well-shuffled  decks  of cards.  This  factory  would  need  a quality 
         control department.  It looks at the deck before shipping it to the customer, blocks 
         some badly shuffled decks, and approves some others as well shuffled.  But how is 
         it possible if all n! orderings of n cards have the same probability?
            Here is a modernized version of the same paradox.  Imagine that a company 
         that runs a multiple-choice test for millions of students decided to make for each 
         participant  an  individual  version  of the  test  by  random  permutation  of possible 
         answers to each question.  Imagine that in one of the copies all the correct answers 
         turn out to be labeled as  “A”. Should they discard this copy?
                               Current best practice
            Whatever the philosophers say, statisticians have to perform their duties.  Let 
         us try to provide a description of their current best practice (see [194,  175,  180]).
            A.  How a statistical hypothesis is  applied.  First of all, we have to admit that 
         probability theory makes no predictions but only gives recommendations:  If the 
         probability  (computed  on  the  basis  of the  statistical  hypothesis)  of an  event  A 
         is  much  smaller  than  the  probability  of an  event  B,  then  the  possibility  of the 
         event В  must  be  taken  into  consideration  to  a greater  extent  than  the possibility 
         of the event A (assuming the consequences are equally grave).  For example, if the
                                       455
            456            1.  COMPLEXITY  AND  FOUNDATIONS  OF  PROBABILITY
            probability of A is smaller than the probability of being killed on the street by a 
            meteorite, we usually ignore A completely (since we have to ignore event В anyway 
            in our everyday life).
                Borel [22, pp. 232-233] describes this principle as follows:
                     ... Il y a à Paris moins d’un million d’hommes adultes ; El’s jour­
                     naux rapportent chaque jour des accidents ou incidents bizarres 
                     arrivés à l’un d’eux ; la vie serait impossible si chacun craignait 
                     continuellement pour lui-même toutes les aventures qu’on peut 
                     lire  dans  le  faits  divers  cela revient  à dire  qu’on  doit  négliger 
                     pratiquement les probabilités inférieures à un millionième.  (...)
                         Souvent la peur d’un mal fait tomber dans un pire.
                         Pour savoir  distinguer  le  pire,  il  est  bon  de  connaître  les 
                     probabilités des diverses éventualités— 1 2 
                B. How a statistical hypothesis is tested.  Here we cannot say naively that if we 
            observe some event that has negligible probability according to our hypothesis, we 
            reject this hypothesis.  Indeed, this would mean that any 1000-bit sequence of the 
            outcomes would make the fair coin assumption rejected (since this specific sequence 
            has negligible probability 2"1000).
                Here algorithmic information theory comes into play:  We reject the hypoth­
            esis if we observe a simple event that has negligible probability according to this 
            hypothesis.  For example,  if coin tossing produces a thousand tails,  this event  is 
            simple and has negligible probability,  so we do not believe the coin is fair.  Both 
            conditions  (simple and negligible probability)  are important:  the event  “the first 
            bit is a tail”  is simple but has probability 1/2, so it does not discredit the coin.  On 
            the other hand, every sequence of outcomes has negligible probability 2“1000, but 
            if it is not simple, its appearance does not discredit the fair coin assumption.
                Often both parts of this scheme are combined into a statement  “events with 
            small probabilities do not happen”.  For example, Borel writes:  “.. .je suis arrivé 
            à la conclusion qu’on ne devrait pas craindre d’employer le mot de certitude pour 
            désigner une probabilité qui différé de l’unité d’une quantité suffisamment petite” 
            ([22,  p.  5]).2  Sometimes  this  statement  is  called  the  “Cournot  principle”.  But 
            we prefer to distinguish between these two stages, because for the hypothesis test­
            ing the existence of a simple description of an event with negligible probability is 
            important,  and for application of the hypothesis it seems unimportant.  (We can 
            expect, however, that events interesting to us have simple descriptions because of 
            their interest.)
                          Simple events and events specified in advance
                Unfortunately, this scheme remains not very precise:  the Kolmogorov complex­
            ity of an object x (defined as the minimal length of the program that produces x) 
            depends on the choice of programming language.  We need also to fix some way to
                1 Fewer than a million people live in Paris.  Newspapers daily inform us about the strange 
            events or accidents that happen to some of them.  Our life would be impossible if we were afraid 
            of all  adventures  we read about.  So one can say that from a practical viewpoint we can ignore 
            events with probability less that one millionth. . .  Often  by  trying  to  avoid something  bad we  are 
            confronted  with  even  w orse...  To  avoid  this,  it  is  good  to  know  the  probabilities  of different 
            events.
                21 came to the conclusion that one must not be afraid to use the word certainty to describe 
            a probability that falls short of unity by a sufficiently small quantity.
                            SIMPLE  EVENTS  AND  EVENTS  SPECIFIED  IN  ADVANCE             457
            describe the events in question.  Both choices lead only to an 0(l)-change asymp­
            totically; however, strictly speaking, due to this uncertainty we cannot say that one 
            event has smaller complexity than the other one.  (The word “negligible” is also not 
            very precise.)  On the other hand,  the scheme described, while very vague, seems 
            to be the best approximation to the current practice.
                 One of the possible ways to eliminate complexity in this picture is to say that a 
            hypothesis is discredited if we observe a very improbable event that was specified in 
             advance (before the experiment).  Here we come to the following question.  Imagine 
            that you make some experiment and get a sequence of a thousand bits that looks 
            random at first.  Then somebody comes and says,  “Look, if we consider every third 
            bit  in  this  sequence,  the  zeros  and  ones  alternate.”  Will you  still believe  in  the 
            fair coin hypothesis?  Probably not, even if you haven’t thought about this event 
            before while looking at the sequence:  the event is so simple that one  could think 
            about it.  In fact, one may consider the union of all simple events that have small 
            probability, and it still has small probability (if the bound for the complexity of a 
            simple event is small compared to the number of coin tossings involved, which is a 
            reasonable condition anyway).  And this union can be considered as specified before 
            the experiment (e.g., it is described in this book).
                On the other hand, if the sequence repeats some other sequence observed earlier, 
            we probably will not believe it is obtained by coin tossing even if this earlier sequence 
            had high complexity.  One may explain this opinion saying the the entire sequence 
            of observations is simple since it contains repetitions; however, the first observation 
            may not  be covered  by  any probabilistic  assumption.  This  could  be  taken into 
            account by considering the conditional complexity of the event (with respect to all 
            information available before the experiment).
                The  conclusion  is  that  we  may  remove  one  problematic  requirement  (being 
            simple  in  some  vague  sense)  and  replace  it  by  another  problematic  one  (being 
            specified before the observation).  Borel comments on the situation  [
            112]:                                                                 21,  pp.  111- 
                     Disons un mot de la réflexion de Bertrand relativement au tri­
                     angle équilatéral que formeraient  trois étoiles ;  elle se  rattache 
                     à  la  question  du  nombre  rond.  Si  l’on  considère  un  nombre 
                     pris  au  hasard entre  1.000.000 et  2.000.000 la probabilité pour 
                     qu’il soit  égal  à  1.342.517 est  égale à un millionième;  la prob­
                     abilité pour qu’il soit égal à 1.500.000 est aussi égale à un mil­
                     lionième.  On  considérera  cependant  volontiers  cette  dernière 
                     éventualité comme moins probable que la première ;  cela tient 
                     à  ce  qu’on  ne  se  représente  jamais  individuellement  un  nom­
                     bre tel que 1.542.317 ;  on le regarde comme le type de nombres 
                     d’apparences analogues et si,  en le transcrivant,  on modifie un 
                     chiffre, on s’en aperçoit à peine et l’on ne distingue pas 1.324.519 
                     de 1.324.517 :  le lecteur a besoin de faire un effort pour s’assurer 
                     que les quatre nombres écrits dans le lignes précédentes sont tous 
                     différents.
                          Lorsque l’on a observé un nombre tel que le précédent comme 
                     évaluation d’un angle en dixiémes de secondes centésimales, on 
                     ne songe pas à se poser la question de savoir qu’elle était la prob­
                     abilité pour que cet angle fût précisément égal a 13°42/51/(7  car
                 458                  1.  C O M PLEXITY  AND  FOUNDATIONS  OF  PROBABILITY
                             on ne se serait jamais posé cette question précise avant d’avoir 
                             mesuré  l’angle.  Il  faut  bien  que  cet  angle  ait  une  valeur  et, 
                             qu’elle que soit sa valeur à un dixième de seconde près, on pour­
                             rait, après l’avoir mesurée, dire que la probabilité a priori, pour 
                             que  cette  valeur  soit  précisément  telle  qu’elle  est,  est  un  dix- 
                             millionième, et que c’est là un fait bien extraordinaire.  {...)
                                   La question est de savoir si l’on doit faire ces mêmes réserves 
                             dans le cas où l’on constate qu’un des angles du triangle formé 
                             par trois étoiles  a une valeur  remarquable  et  est,  par exemple, 
                             égal  à  l’angle  du  triangle  équilatéral  {...)  ou  à un  demi-angle 
                             droit  {...)  Voici  ce  que  l’on  peut  dire  à  se  sujet :  on  doit  se 
                             défier beaucoup de la tendance que l’on a à regarder comme re­
                             marquable une circonstance que l’on n’avait pas précisée  avant 
                             l’expérience,  car  le  nombre  des  circonstances  qui  peuvent  ap­
                             paraître comme remarquables,  à divers points de vue,  est  très 
                             considérable.3
                                                         Frequency approach
                      The most natural and common explanation of the notion of probability says 
                that  probability  is  the  limit  value  of  frequencies  observed  when  the  number  of 
                repetitions tends to infinity.  (This  approach was advocated as the only possible 
                basis for probability theory by Richard von Mises.)
                      However,  we  cannot  observe  infinite  sequences,  so  the  actual  application  of 
                this  definition  should  somehow  deal  with  finite  number  of repetitions.  And  for 
                a  finite  number  of repetitions  our  claim  is  not  so  strong:  We  do  not  guarantee 
                that frequency of tails for a fair coin is  exactly  1/2.  We say only that it is highly 
                 improbable that it deviates significantly from 1/2.  Since the words highly improbable 
                need  to  be  interpreted,  this  leads  to  some  kind  of logical  circle  that  makes  the 
                frequency approach much less convincing; to get out of this logical circle we need 
                some version of the Cournot principle.
                      3Let  us comment on Bertrand’s observation  (about an equilateral triangle formed by three 
                stars); it is related to the idea of a “round number”.  Consider a random integer between 1 000000 
                and 2 000 000.  The probability that it is equal to 1 342 517 is one over million; the probability that 
                it  is  equal  to  1  500  000,  is  also one over  million.  However,  the second  event  is often considered 
                as  something  less  likely  than  the  first  one.    This  is  because  nobody  considers  individually  a 
                number like 1 542 317.  It is considered as an example of some type of numbers, and if we change 
                accidentally one digit when copying such a number, it is hardly noticeable:  1  324 519 looks very 
                similar to 1  324 517.  A special effort is needed to check that the four numbers mentioned above 
                are different.
                      When a number like this appears as an angle measured in centesimal seconds, we do not ask 
                ourselves  what  is the  probability  that  this  angle is  exactly  13°42/51/(7  because we  never would 
                be interested in such a question before the measurement.  Of course, the angle should have some 
                value,  and  whatever  this  value  is  (up  to  a tenth of a second),  we  may  measure it  and say that 
                the  a  priori  probability  to  get  this  value  is  one  in  ten  million,  so  an  extraordinary  event  has 
                happened...
                      The question  is  whether the same reservations  apply  if one of the  angles  formed  by  three 
                starts has a remarkable value, for example,  is equal to the angle in the equilateral triangle...  or 
                the half of the  right  angle...  What  can we say about  that?  One should  try  hard to avoid  the 
                temptation to consider some event not fixed  before the  experiment, as a remarkable one, because 
                a lot of events could look remarkable from some viewpoint.
                                 ARE  “REAL-LIFE”  SEQUENCES  COMPLEX?                   459
                Technically, the frequency approach can be related to the principles explained 
            above.  Indeed,  the event  “the number of tails in  1000 000 coin tossings deviates 
            from 500000 more than by 100 000” has a simple description and very small prob­
            ability, so we reject the fair coin assumption if such an event happens (and ignore 
            the dangers related to this event if we accept the fair coin assumption).  In this way 
            the belief that frequency should be close to probability (if the statistical hypothesis 
            is chosen correctly) can be treated as the consequence of the principles explained 
            above.
                                   Dynamical and statistical laws
                We have described how probability theory is usually applied.  But the funda­
            mental question remains:  Probability theory describes (to some extent) the behav­
            ior of a symmetric coin or die and turns out to be practically useful in many cases. 
            But is it  a new law of nature or some consequence of the known dynamical laws 
            of classical mechanics?  Can we somehow prove that a symmetric die indeed has 
            the same probabilities for all faces (if the starting point is high enough and initial 
            linear and rotation speeds are high enough)?
                Since it is not clear what kind of “proof” we would like to have, let us put the 
            question in a more practical way.  Assume that we have a die that is not symmetric 
            and we know exactly the position of its center of gravity.  Can we use the laws of 
            mechanics to find the probabilities of different outcomes?
                It  seems  that  this  is  possible,  at  least  in  principle.  The  laws  of mechanics 
            determine the behavior of a die (and therefore the outcome) if we know the initial 
            point in the phase space (initial position and velocity) precisely.  The phase space, 
            therefore, is split into six parts that correspond to six outcomes.  In this sense there 
            is  no  uncertainty or probabilities up to  now.  But  these six parts are well mixed 
            since very small modifications affect the result, so if we consider a small (but not 
            very small) part of the phase space around the initial conditions and any probability 
            distribution on this part whose density does not change drastically,  the measures 
            of the six parts will follow the same proportion.
                The last sentence can be transformed into a rigorous mathematical statement 
            if we  introduce specific  assumptions  about  the  size  of the starting  region  in  the 
            phase space and variations of the density of the probability distribution on it.  It 
            then can be proved.  Probably it is a rather difficult mathematical problem not yet 
            solved,  but  at  least  theoretically  the  laws  of mechanics  allow  us  to  compute the 
            probabilities of different outcomes for a non-symmetric die.
                                Are  “real-life”  sequences complex?
                The argument in the preceding section would not convince a philosophically 
            minded person.  Well,  we can  (in principle)  compute some numbers that can be 
            interpreted as probabilities of the outcomes for a die, and if we do not need to fix the 
            distribution on the initial condition,  it is enough to assume that this distribution 
            is  smooth  enough.  But  still  we  speak  about  probability  distributions  that  are 
            somehow externally imposed in addition to dynamical laws.
                Essentially the same question can be reformulated as follows.  Make  106  coin 
            tosses and try to compress the resulting sequence of zeros and ones by a standard 
            compression program, say, gzip.  (Technically, you need first to convert a bit se­
            quence into a byte sequence.)  Repeat this experiment (coin tossing plus gzipping)
                 1.  COMPLEXITY AND FOUNDATIONS OF PROBABILITY
       460
       as many times as you want, and this will never give you more than 1% compression. 
        (Such a compression is possible for less than a 2“10000-fraction of all sequences.) 
       This statement deserves to be called a law of nature:  it can be checked experimen­
       tally in the same way as other laws.  So the question is,  Does this law of nature 
       follow from dynamical laws we know?
          To see where the problem is, it is convenient to simplify the situation.  Imagine 
       for a while that we have discrete time, phase space is [0,1), and the dynamical law 
       is
                  X i—^ T(x) =  if 2a; < 1  then 2x else 2x — 1.
       So  we  get  a sequence of states  xq,x\  =  T(xo),X2  = T(xi),...;  at  each step we 
       observe where the current state is—writing 0 if xn  is in  [0,1/ 2)  and  1  if xn  is in 
        (1/ 2. 1).
          This transformation T has the mixing property we spoke about:  If for some 
       large t we look at the set of points that after t iterations are in the left half of the 
       interval, we see that it is just the set of reals where tth bit of the binary representa­
       tion is zero, and these reals occupy about a half in every (not too short) interval.  In 
       other words, we see that a sequence of bits obtained is just the binary representa­
       tion of the initial condition.  So our process just reveals the initial condition bit by 
       bit, and any statement about the resulting bit sequence (e.g., its incompressibility) 
       is just a statement about the initial condition.
          So what?  Do we need to add to the dynamical laws just one more metaphys­
       ical law saying that the world was created at a random (incompressible) state? 
       Indeed, algorithmic transformations (including dynamical laws) cannot increase sig­
       nificantly the Kolmogorov complexity of the state, so if objects of high complexity 
       exist in the  (otherwise deterministic, as we assume for now) real world now, they 
       should  be there  at  the  very  beginning.  (Note  that  it  is  difficult  to  explain  the 
       randomness observed saying that we just observe the world at random time or in 
       a random place.  The number of bits needed to encode the time and place in the 
       world is not enough to explain an incompressible string of length,  say  106,  if we 
       use standard estimates for the size and age of the world.  The logarithms of the 
       ratios of the maximal and minimal lengths (or time intervals) that exist in nature 
       are  negligible  compared  to  106,  and  therefore  the  position  in  space-time  cannot 
       determine a string of this complexity.)
          Should we conclude then that instead of playing dice (as Einstein could put it), 
       God provided “concentrated randomness”  (a state of high Kolmogorov complexity) 
       while creating the world?
         Randomness as ignorance:  Blum-Micali-Yao pseudo-randomness
          This discussion becomes too philosophical to continue it seriously.  However, 
       there are important  mathematical results that could influence the opinion of the 
       philosophers  discussing  the  notions  of probability  and  randomness  if they  knew 
       these results.  In this book we did not touch complexity with bounded resources 
       (an important but not well-studied topic) and instead stayed in the realm of general 
       computability theory, but we cannot avoid this topic when discussing the philosoph­
       ical aspects of the notion of probability.
          This result  is the existence of pseudo-random number generators  (as defined 
       by Blum, Micali and Yao; they are standard tools in computational cryptography; 
       see,  e.g.,  the  Goldreich  textbook  [61]).  Their  existence  has  been  proven  using
                      A  DIGRESSION:  THERMODYNAMICS  461
       some complexity assumptions (the existence of one-way functions) that are widely 
       believed though not yet proven.
          Let us explain what a pseudo-random number generator (in the Blum-Micali- 
       Yao sense) is.  Here we use rather vague terms and oversimplify the matter, but there 
       is rigorous mathematics behind it.  Imagine a simple and fast algorithmic procedure 
       that gets a seed, a binary string of moderate size, say, 1000 bits, and produces a very 
       long sequence of bits out of it, say, of length 1010.  By necessity the output string 
       has small complexity compared to its length (complexity is bounded by the seed size 
       plus the length of the processing program,  which we assume to be rather short). 
       However,  it  may  happen  that  the  output  sequences  will  be  “indistinguishable” 
       from truly random sequences of length  1010,  and in this case the transformation 
       procedure is called a pseudo-random number generator.
          It  sounds like a contradiction:  as we have said,  output  sequences have small 
       Kolmogorov complexity,  and  this  property  distinguishes  them  from  most  of the 
       sequences of length 1010.  So how they can be indistinguishable?  The explanation is 
       that the difference becomes obvious only when we know the seed used for producing 
       the sequence, but there is no way to find out what seed is by looking at the sequence 
       itself.  The formal statement is quite technical, but its idea is simple:  Consider any 
       simple test that looks at a 1010-bit string and says yes or no (by whatever reason; 
       any simple and fast program could be a test).  Then consider two ratios:  (1) the 
       fraction of bit strings of length 1010 that pass the test (among all bit strings of this 
       length)  and (2)  the fraction of seeds that lead to a 1010-bit string that passes the 
       test (among all seeds).  The pseudo-random number generator property guarantees 
       that these two numbers are very close.
          This implies that if some test rejects most of the pseudo-random strings (pro­
       duced by the generator), then it would also reject most of the strings of the same 
       length, so there is no way to find out whether somebody has given us random or 
       pseudo-random strings.
          In  a  more  vague  language,  this  example shows  us  that  randomness  may  be 
       in  the  eye  of the  beholder,  i.e.,  the  randomness  of an  observed  sequence  could 
       be the consequence of our limited computational abilities which prevent  us from 
       discovering non-randomness.  (However,  if somebody shows us the seed,  our eyes 
       are immediately opened, and we see that the sequence has very small complexity.)
          So we should not exclude the possibility that the world is governed by simple 
       dynamical laws and its initial state can be also described by several thousands of 
       bits.  In this case “true” randomness does not exist in the world, and every sequence 
       of 106 coin tossings that happened or will happen in the foreseeable future produces 
       a string that has Kolmogorov complexity much smaller than its length.  However, 
       a computationally limited observer (like ourselves) would never discover this fact.
                     A digression:  Thermodynamics
          The connection  between  statistical  and  dynamical  laws  was  discussed  a  lot 
       in the context of thermodynamics while discussing the second law.  However, one 
       should be very careful with exact definition and statements.  For example, it is often 
       said that the second law of thermodynamics cannot be derived from dynamical laws 
       because they are time-reversible while the second law is not.  On the other hand, 
       it  is  often said that the second law has many equivalent formulations,  and one of 
       them claims that the perpetual motion machine of the second kind is impossible,
        462       1.  COMPLEXITY  AND  FOUNDATIONS  OF  PROBABILITY
        i.e.,  no  device can operate on  a cycle to receive heat  from a single reservoir  and 
        produce a net amount of work.
          However,  as  Nikita  Markaryan  explained  (personal  communication),  in  this 
        formulation the second law of thermodynamics is a consequence of dynamic laws. 
        Here is  a sketch  of this  argument.  Imagine that  a perpetual motion  machine of 
        the second kind exists.  Assume this machine is attached to a long cylinder that 
        contains warm gas.  Fluctuations of gas pressure provide a heat exchange between 
        gas and machine.  On the other side the machine has a rotating spindle and a rope 
        to lift some weight (due to rotation).
                            gas          machine
                                                t□
          When the machine works,  the gas temperature  (energy)  goes down  and the 
        weight goes up.  This is not enough to call the machine a perpetual motion machine 
        of the second kind  (indeed, it can contain some amount of cold substance to cool' 
        the gas and some spring to lift the weight).  So we assume that the rotation angle 
        (and the height change) can be made arbitrarily large by increasing the amount of 
        the gas and the length of the cylinder.  We also need to specify the initial conditions 
        of the gas;  here the natural requirement is that the machine works (as described) 
        for most initial conditions (according to the natural probability distribution in the 
        gas phase space).
          Why is such a machine impossible? The phase space of the entire system can be 
        considered as a product of two components:  the phase space of the machine itself 
        and the phase space of the gas.  The components interact,  and the total energy 
        is  constant.  Since the machine itself has some fixed number of components,  the 
        dimension of its component (or the number of degrees of freedom in the machine) 
        is negligible compared to the dimension of the gas component (resp. the number of 
        degrees of freedom in the gas).  The phase space of the gas is split into layers corre­
        sponding to different levels of energy; the higher the energy is, the more volume in 
        the phase space is used.  This dependence outweighs the similar dependence for the 
        machine since the gas has many more degrees of freedom.  Since the transformation 
        of the phase space of the entire system is measure-preserving, it is impossible that 
        a trajectory started from a large set with high probability ends in a small set:  the 
        probability of this event does not exceed the ratio of a measures of destination and 
        source sets in the phase space.  So the machine that  (with high probability) cools 
        the gas in a random state and produces mechanical energy (=is a perpetual mobile 
        of the second kind) is impossible.
          This argument is. quite informal and ignores many important points.  For ex­
        ample, the measure on the phase space of the entire system is not exactly a product 
        of measures on the gas and machine coordinates;  the source set of the trajectory 
        can have small measure if the initial state of the machine is fixed with very high 
        precision, etc.  (The latter case does not contradict the laws of thermodynamics:  if
                           ANOTHER DIGRESSION:  QUANTUM MECHANICS              463
           the machine uses a fixed amount of cooling substance of very low temperature, the 
           amount of work produced can be very large.)  But at least these informal arguments 
           make plausible that dynamic laws make impossible the perpetual motion machine 
           of the second kind (if the latter is defined properly).
                          Another digression:  Quantum mechanics
              Another physics  topic  often  discussed  is  quantum  mechanics  as  a source  of 
           randomness.  There were many philosophical debates around quantum mechanics. 
           However, it seems that the relation between quantum mechanical models and obser­
           vations resembles the situation with probability theory and statistical mechanics. 
           The difference is that in quantum mechanics the model assigns amplitudes (instead 
           of probabilities)  to  different  outcomes  (or events).  The  amplitudes  are  complex 
           numbers and the quantum Cournot principle says that if the  (absolute value)  of 
           the amplitude of event A is smaller than for event B, then the possibility of event В 
           must be taken into consideration to a greater extent than the possibility of event A 
           (assuming the consequences are equally grave).  Again this  implies  that  we  can 
           (practically) ignore events with very small amplitudes.
              The interpretation of the square of amplitude as probability can be then derived 
           is the same way as in the case of the frequency approach.  If a system is made of N 
           independent identical systems with two outcomes 0 and 1 and the outcome 1 has 
           amplitude z in each system, then for the entire system the amplitude of the event 
           “the number of l’s among the outcomes deviates significantly from N\z\2”  is very 
          small (it is just the classical law of large numbers in disguise).
              One can then try to analyze measurement devices from the quantum mechan­
          ical  viewpoint  and  prove  (using  the  same  quantum  Cournot  principle)  that  the 
          frequency of some outcome of measurement is close to the square of the length of 
          the projection of the initial state to a corresponding subspace outside some event 
          of small amplitude, etc.
                                     APPENDIX 2
                   Four algorithmic faces of randomness
                                     V.  U spensky
                 This appendix is a translation of the brochure “Four algorithmic 
                 faces of randomness”  (2nd corrected edition, MCCME Publish­
                 ers,  Moscow, 2009; the first edition was published in 2006) that 
                 is  based  on  a  lecture  delivered  by  Uspensky  during  the  sum­
                 mer school  “Modern Mathematics”  (Dubna near Moscow, Rus­
                 sia,  July  23,  2005).  The terminology used in this brochure1  is 
                 somewhat different  from that  used in the rest  of the book;  in 
                 particular, the terms chaotic, typical, and unpredictable are used 
                 to stress specific properties of random objects that appear in the 
                 corresponding definition.  Chaoticness means that the complex­
                 ity is high  (no regularities can be used to give a short descrip­
                 tion);  typicalness  is  based on measure theory;  unpredictability 
                 guarantees that no strategy can win in a prediction game against 
                 this  sequence.  There  are rigorous definitions for  these notions 
                 that can be considered possible definitions of true randomness. 
                 And it is remarkable that natural definitions of chaoticness and 
                 typicalness turn out to be equivalent (Levin-Schnorr theorem).
                                     Introduction
             If somebody tells us that  she tossed  a  “fair”  coin  twenty times  and got  the 
          string
          (I)                     10001011101111010000 
          (where 0 and 1 denote head and tail), or the string
          (II)                   01111011001101110001,
          this would not surprise us.  However, if somebody claims to obtain
          (III)                  00000000000000000000 
          or
          (IV)                   01010101010101010101,
          we start to doubt that the experiment was really performed in a proper way.  But 
          why?
             1The same terminology was  approved  by  Kolmogorov and  used  in  the  opening  talk  “Al­
          gorithms  and  randomness”  at  the  First  World  Congress  of the  Bernoulli  Society  (written  by 
          Kolmogorov and Uspensky, delivered by Uspensky), and in [84,  208,  194,  139]
                                         465
        466        2.  FOUR ALGORITHMIC FACES OF  RANDOMNESS
           Somehow the strings  (I)  and  (II)  are perceived  as  “random”  while  (III)  and 
        (IV) are not.
          But what does it  mean to be  “perceived  as  random”?  Classical probability 
        theory  says  nothing  about  this  natural  question.  Sometimes  they  say  that  the 
        outcomes (III) and (IV) have very small probability 2~ 20 to appear in a fair coin 
        tossing, so the chances to get them are less than one in a million.  Still, (I) and (II) 
        have exactly the same probability!
          Let us start with three important remarks.
            •  First, the intuitive idea of randomness depends on the assumed probability 
              distribution.  If the coin is very asymmetric and one side is much heavier, 
              or  if  it  is  tossed  in  a  very  special  way,  (III)  or  (IV)  may  not  surprise 
              us.  So,  for simplicity,  we will speak mostly about  fair coin tossing,  i.e., 
              independent trials with success probability 1/2.
            •  Second,  the  intuitive  idea of randomness has  sense only  if the string is 
              long enough.  It would be stupid to ask which of four strings 00, 01,  10, 
              11 looks more random than the others.
            •  Finally, there is no sharp boundary between (intuitively) random and non- 
              random strings.  Indeed,  changing one bit  in a random string,  we get a 
              string that  is random,  too.  But  in several steps we can obtain  (III)  or 
              (IV)  from any string.  This well-known effect is sometimes called  “heap 
              paradox”.
          So,  trying  to  define  randomness,  one  should  consider  very  long  strings,  or, 
        even better, infinite bit sequences (in general infinite objects are  “approximations 
        from above”  for large finite objects).  For infinite sequences one may try to draw a 
        meaningful sharp division between random and non-random objects, i.e., to define 
        rigorously a mathematical notion of a random  bit sequence.  In this survey we 
        describe several attempts to provide such a definition, made by different authors. 
        However,  a general disclaimer is needed:  for all practical purposes only finite se­
        quences (strings) matter, so these definitions are necessarily far from “real life”.  In 
        fact, even very long finite sequences never appear in real life, so it is hard to extend 
        our intuition of randomness even to long finite strings.  This said, we now switch to 
        mathematical definitions.
          Let us start with some useful notation and terminology.
          We consider finite bit strings, i.e., finite sequences of zeros and ones.  (They are 
        also called  binary words.)  A string x = x \.... ,xn  has length n,  denoted also by 
        \x\?  A string may have zero length, i.e., may contain no bits; it is then called an 
        empty string and is denoted by A.
          The set of all binary strings is denoted by H.  The set of all infinite bit sequences 
        is  denoted by fi.  An infinite sequence ai, й2, аз, •..  has finite string ai, a2,..., an 
        as  its  n-bit prefix.  For every string x  we  consider  the set  Qx  C  fl of all infinite 
        sequences that have prefix x.  This set is called a ball, and the volume of this ball 
        is defined as 2“^   and denoted by v(x).2 3
          Each sequence from Q is considered as a record of an (infinite) coin tossing.  Let 
        us repeat that for now we assume that the coin is fair.  Mathematically speaking, it
          2 We used the notation l{x) for the length of x in the main part of the book.
          3In the rest of the book we call Clx  an interval,  not a ball,  and speak about its length,  not 
        volume.
                           INTRODUCTION               467
       means that we consider a uniform probability distribution on ft where for each ball 
       Qx the probability to get an element of Qx is equal to its volume.
          Our goal is to specify a well-defined subset of Q that could be considered as 
       the set of all random sequences.  Traditional probability theory cannot help here; 
       even the question can hardly be stated in its language.  In a paradoxical way, the 
       notion  of algorithm  helps.  It  may  sound  strange:  the  notion  of randomness  is 
       defined in terms of the notion of algorithm, which is a deterministic procedure that 
       has nothing to do with randomness,  but it is the case.  All known definitions of 
       randomness for individual objects  (in our case—individual binary sequences)  are 
       based on the theory of algorithms in some way.
          We may start by trying to  identify a characteristic  property that  intuitively 
       should be possessed by all random sequences, and then use this property (specified 
       rigorously) as a formal definition of randomness.
          So, what properties could be reasonably expected from a randomly chosen bit 
       sequence?
          First of all, the limit frequency should exist in such a sequence.  For the simplest 
       case of a fair coin this means that the fraction of zeros (as well as the fraction of ones) 
       in the n-bit prefix of the sequence should converge to 1/2 as n goes to infinity.  This 
       property can be called frequency stability.  Moreover, the same property should hold 
       not only for the sequence itself, but also for every its reasonably chosen subsequence.
          Second,  a randomly chosen sequence is expected to be  chaotic.  This means 
       that  it  has  a  complex structure  and  cannot  have  a  reasonable  description.  The 
       psychological difference between the perception of strings  (I),  (II)  and  (III),  (IV) 
       can be explained,  as Kolmogorov suggested,  by the fact that strings  (I)  and  (II) 
       have no short description while (III) and (IV) have a regular structure and can be 
       described easily.
          Third, a randomly chosen sequence should be typical, in the sense that it be­
       longs to any reasonable majority.
          Finally,  it should be unpredictable.  This means that making bets against this 
       sequence,  trying to guess its  terms,  we cannot win systematically,  and  no clever 
       strategy could help us.
          Of course, these wordings are vague.  One should specify the meaning of word 
       “reasonable”  that  occurs  in  the  explanations  of frequency stability,  chaoticness, 
       and  typicalness,  as  well  of the  words  “description”  and  “strategy”.  Theory  of 
       algorithms can be used to convert these descriptions into formal definitions, and we 
       get four rigorously defined properties:  frequency stability,  chaoticness,  typicalness, 
       and unpredictability.  Each of them can be considered as some  “algorithmic face of 
       randomness”  and can to some extent pretend to be a mathematical definition of 
       randomness.  In this way we get four well-defined classes of sequences that could 
       compete for the title of the  “true class of random sequences”  though each has its 
       strong and weak points.
          In the following exposition our goal is two-fold:  (1) to give rigorous definitions 
       for  the  four  properties  mentioned  above  and  therefore  to  define  four  classes  of 
       sequences;  (2) to state (currently known) relations between these properties (and, 
       therefore, between the corresponding classes of sequences).
               468                   2.  FOUR  ALGORITHMIC  FACES  OF  RANDOMNESS
                               Face one:  Frequency stability and stochasticness
                    The idea to define the notion of an individual random sequence goes back to 
               Richard von Mises, a well-known German mathematician; it seems that he was the 
               first  who  tried  to  give  such a definition.  This  happened in  the beginning of the 
               twentieth century, in 1919.  At least it was he who suggested a reasonable approach 
               to this definition (though he did not give a rigorous mathematical one).
                    Von Mises started by requiring frequency stability,  i.e.,  the existence of limit 
               frequency: the fraction of ones among the first n terms should converge (for the case 
               of fair coin) to 1/2 as n tends to infinity.  Of course, this property is not sufficient. 
               For example, this is true for the (definitely non-random) sequence
                                                   0,1,0,1,0,1,0,1,....
               Evidently,  we should  require that  not  only the sequence  itself,  but  also  its  sub­
               sequences satisfy the frequency stability property.  But we cannot expect  all the 
               subsequences to be stable in this sense:  indeed, even a perfectly random sequence 
               has a zero subsequence, we may select just the terms that are equal to zero.  So we 
               have to restrict  ourselves and consider only  “reasonable chosen”,  or  “admissible” 
               subsequences.
                    It  is  nice  to  consider  any  subsequence  of a  given  sequence  as  the  result  of 
               selection procedure applied to the terms of the original sequence:  the subsequence 
               consists just of those terms which are selected.  Any selection procedure is based on 
               some selection rule.  To obtain a reasonable, or admissible, subsequence, one needs 
               to use a reasonable (admissible) selection rule.  For example, a reasonable selection 
               rule may select  all  terms ai  where г  is  a prime  number,  or all  terms that  follow 
               zeros  (i.e.,  all  terms  a^+i  such  that     =  0).  In  this  way we get  two  admissible 
               subsequences.
                   Kolmogorov at some point suggested the name stochastic for a sequence whose 
               admissible subsequences all have the frequency stability property.
                   The scheme suggested by von Mises was rather vague; it was turned to a rig­
              orous definition of randomness when the theory of algorithms was developed.  One 
              of its inventors,  an American mathematician Alonzo Church suggested in 1940 to 
              define the admissible selection rule as algorithms of special type.  The sequences 
              where all Church-admissible subsequences satisfy the frequency stability property 
               are called  Church stochastic sequences.4  This definition, however, looks too broad: 
              for example, there exists a Church stochastic sequence that becomes non-Church- 
              stochastic after a computable permutation of its terms.5
                   In  1963 Kolmogorov modified the definition given by Church and suggested a 
              broader class of admissible selection rules, thus defining a broader (in fact, strictly 
              broader)  class of admissible subsequences.  In particular,  Kolmogorov’s definition 
              does not  require that the selected terms keep the ordering they had in the orig­
              inal  sequence.  A  corresponding class of sequences,  called  Kolmogorov stochastic 
               sequences,6 appears:  they are sequences such that all Kolmogorov-admissible sub- 
              sequences satisfy  the  frequency  stability  property.  By  definition,  this  class  is  a
                   4In the main part of the book they are called M ises-Church random sequences.
                   5See Theorem 203(d), p. 307.
                   6They are called M ises-Kolmogorov  random  sequences  in the main part of the book.  The 
              most standard name used nowadays is Kolmogorov-Loveland stochastic sequences.
                       FACE ONE:  FREQUENCY STABILITY AND STOCHASTICNESS          469
           subclass (in fact, a proper subclass) of the class of Church stochastic sequences.  In 
           the sequel we denote the class of stochastic sequences by S.
               Soon  it  turned  out  that  the  class  S  was  also  too  broad.  For  example,  one 
           may construct a Kolmogorov stochastic sequence where each prefix has more zeros 
           than ones.7  It contradicts our intuition (supported by some theorems of probability 
           theory:  a one-dimensional random walk returns to the starting point with proba­
           bility 1).  So even the strictest version of the von Mises approach currently known 
           does not provide an intuitively satisfactory notion of randomness,  though it is an 
           interesting object to study that reflects some aspects of randomness.
               To be precise,  let us reproduce the definitions suggested by Church and Kol­
           mogorov.  In both cases we define some class of admissible selection rules used to 
           form subsequences of a given sequence.
               Imagine that the terms of the sequence (zeros and ones) are written on paper 
           cards that are put on the table, face down, so we do not see what is written on the 
           cards.  Our goal is to select some of the cards and form another sequence made of the 
           bits on the selected cards.  This subsequence (in the case of Kolmogorov’s definition 
           this term is used in a broad sense, the order of terms in the subsequence may differ 
           from their order in the original sequence) is called an admissible subsequence.  An 
           admissible selection rule is  an algorithm that  decides on each step  (1)  which bit 
           should be revealed (corresponding card turned over) next and (2) whether this bit 
           should be included in the subsequence or not.  The algorithm has access to the bits 
           already revealed (those bits form its input).  It may well happen that the algorithm 
           selects only finitely many bits (it may hang or reveal more and more bits without 
           selecting any of them), in this case we say that no admissible subsequence is formed. 
           (Anyway, the frequency stability property makes sense only for infinite sequences.) 
           If for every admissible selection rule we get a sequence that satisfies the frequency 
           stability property, the original sequence is called stochastic.
               To give a more precise description, let us recall some terminology.  A function is 
           called computable if there is an algorithm that computes this function.  This means, 
           for some function /, that (1) the algorithm terminates on every input x such that 
           f(x)  is defined,  and produces f(x),  and  (2)  the algorithm does not terminate on 
           all inputs where /  is undefined.
               Assume that  a sequence a\,a2,...  is given,  so the nth card contains bit  an. 
           A Church admissible selection rule is an arbitrary computable function G defined 
           on all binary strings and has  True and False as values.  The cards are turned over 
           sequentially (first the card that carries a\, then a2, etc.);  before the next card is 
           turned over, the selection rule decides whether that card is selected or not.  This 
           is  done in the following way.  Assume that n cards,  carrying bits ai,... ,an, have 
           been turned over.  If G(ai,..., an) equals  True, then the next card, carrying an+1, 
           is included in the subsequence; otherwise, it is not.  At the first step we include a\ 
           in the subsequence depending on the value of G(A).  In other words, the selected 
           subsequence consists of terms
                                      ®n( 1)) ®n(2)) ®n(3)) • • • )
           where  n(l),n(2),n(3),...  are  all  numbers  n  such  that  G{a\,..., an-i)  =  True, 
           assuming that there are infinitely many numbers with this property.  Otherwise, we 
           get a finite sequence, and it is not considered as admissible subsequence.
              7 See Theorem 203(b), p. 307.
                      470                               2.  FOUR  ALGORITHMIC  FACES  OF  RANDOMNESS
                              This was Church’s definition.  Before we explain Kolmorogov’s version, let us 
                      explain what we mean by a generalized subsequence of some sequence ai, <22,....  It 
                      is a sequence of the form
                                                                           Q'<p( 1) ) ^<£>(2) ) ■ • • )          ) • • • )
                      where
                                                                               * < 3 => Ф ) ^  4>(j)-
                      In the usual definition of subsequence the last condition is stronger:  we require that 
                      subsequence is monotone, i.e., <p(i) < <p(j) for i < j.
                              Each Kolmogorov admissible selection rule attempts to select some generalized 
                      subsequence of the given sequence.  Here we say “attempts” since this attempt may 
                      be unsuccessful:  in this case instead of an infinite subsequence we get a tuple (finite 
                      sequence) that consists of some terms taken from the original sequence.  We say that 
                      our original sequence is Kolmogorov stochastic if all infinite subsequences obtained 
                      from it by Kolmogorov admissible rules have the frequency stability property.
                              It remains to explain what is a Kolmogorov admissible selection rule.  To specify 
                      such a rule, we consider two computable functions F and G.  The first one (F) is 
                      used to construct some intermediate generalized subsequence; the final subsequence 
                      is a (monotone) subsequence of that intermediate sequence.  Both functions F and 
                      G are defined on  (some)  binary strings,  so their domains  are subsets  of E  (may 
                      be, different ones).  The values of F are positive integers, and the values of G are 
                      Boolean values  True  and  False.  We start  by constructing a sequence of natural 
                      numbers
                                    77(1)         .F(A),  n(2)              .Е(а.п(1)),  ...,  n(k T 1)                     F{an(ip • • • > ^n(k))-
                      This construction is stopped and gives a finite sequence in the following three cases:
                                   •  the value F(an(i),..., an^ )  is undefined;
                                   •  the value G(an^ , ..., an^ )  is undefined;
                                   •  the value F(an^ , . . .,an^ )  coincides with one of the n(l),..., n(k).
                      If none of these three events happens, we get an infinite sequence of indices
                                                                               n(l),n(2),n(3),...,
                      and a generalized subsequence an^ , an(2), an(3)j • • • •  Now, and this is the last step, 
                      we select  a  (monotone)  subsequence  of these  subsequence  by choosing  all terms 
                      an(fc)  such that G(an(i),..., an(*._i)) —  True, in the order of increasing k.
                                                                          Face two:  Chaoticness
                             Let  us  return  to  strings  (I)-(IV)  that  we  started  with.  According  to  Kol­
                      mogorov’s explanation, strings (I) and (II) look random because they are complex, 
                      while (III) and (IV) look non-random because they are simple.  It seems that intu­
                      itively we expect the result of a random process be complex, and we suspect some 
                      cheating when it turns out to be simple.
                             There are many ways to compare objects around us: we can distinguish big and 
                      small objects, or heavy and light objects.  Also we can speak about complex and 
                      simple objects.  In the 1960s Kolmogorov8 observed that mathematics can be used
                             8Kolmogorov’s paper of 1965  [78]  became most well  known,  but  he was  not  alone:  many 
                      people  independently  came to similar  ideas.  As  Kolmogorov notes  in  his  paper  [79],  the  first 
                      publication  in  this  direction  was  written  by  Ray  Solomonoff  [187];  Gregory  Chaitin  [28]  also 
                      developed this idea a bit later.
                                           FACE TWO:  CHAOTICNESS                                471
             for  such  a  classification.  Now  the corresponding mathematical  theory is  usually 
             called Kolmogorov complexity theory.
                 The main idea is simple and natural:  complexity of an object can be mea­
             sured  by  the  length  of  its  shortest  description.  Each  object  has  a  long 
             description, however a complex object cannot have a short description.
                 Let Y be the set of all objects we consider,  and let X be a set of all possible 
             descriptions  of those  objects.  Let  us  recall  that  \x\  stands  for  the  length  of x. 
             According to what we said, the complexity of an object y, denoted by Comp(t/), is 
             defined by the formula
                                 Comply) = min{|x| : x is a description of y}.
                                               X
             If an object y has no description at all, its complexity is infinite (the minimum of 
             the empty set is defined as infinity).
                 Of course, we need some uniform way to measure the length of a description, it 
             would be unfair to say that something can be easily described in Chinese because 
             only  one  glyph  is  needed,  and  has  only  a  complicated  English  description  that 
             consists of several dozen letters.  So we assume that all descriptions are presented 
             as binary strings.  In other words, we assume in the sequel that X = E.
                 The set  of all  pairs  (x,y)  where  x  describes  y,  can  be  called  a  language  of 
             descriptions or a description language.  Note that (for some description language) 
             some object y may have many descriptions.  We may also consider description lan­
             guages where the same x can describe several objects.  For example, the expression 
             “a string of zeros” can be considered as a description of all such strings, and we may 
             even consider an expression  “a bit string”  as a description of all binary strings.9
                 What  has  been  discussed  above  was  a  preparation  for  the  following  formal 
             definition.  Consider an arbitrary subset E in the Cartesian product E x Y, called 
             a description language.  If (x, y)  G E,  we say that the string x is a description of 
             the object y.  The complexity Compfî of an object y with respect to the description 
             language E is defined as
                                      Comp E(y) = min {| ж I :  (x,y) G E}.
                                                     X
             (Again, the minimum of the empty set is infinite.)
                 For a language E — ExY where every string ж is a description of every object 
             y, the complexity of all objects equals zero, since the empty string is a description 
             of every object.  Such a description language is formally allowed but will not appear 
             in the classes of description languages considered in the sequel.
                 Imagine two description languages with the following property: to get a descrip­
             tion of some object у for the second language, we take its description for the first 
             language and repeat it twice.  Evidently the second description language is worse, 
             since it provides descriptions that are twice as long, and we want the descriptions 
             to be short.
                 Formally speaking, we say that a description language A is not worse than a 
             description language В and write A ^ B, if there exists some constant c such that 
             CompA(t/) < CompB(y) + c for all y.
                 Consider natural languages as description languages.  Assume that for any pair 
             of natural languages there is a translation algorithm that converts any given text
                 9However, we should not go too far in this direction; otherwise, the notion of complexity will 
             be trivial.
        472        2.  FOUR  ALGORITHMIC  FACES  OF  RANDOMNESS
        in  the  first  language  into  an  equivalent  text  in  the  second  language.  We  then 
        can conclude that  description  language  corresponding to  the second  language  is 
        not worse than that corresponding to the first language.  For example, a Turkish- 
        language description of an object may consist of two parts:  a Japanese-language 
        description and a Japanese-Turkish translation algorithm.  In this way we get a 
        Turkish description that is longer than a Japanese description at most by a constant 
        (the length of the Japanese-Turkish translation algorithm).  This constant does not 
        depend on the choice of the object described.  Taking the shortest possible Japanese 
        description, we conclude that the Turkish language is not worse than the Japanese 
        language if we consider both as description languages.
          Let us call a language family any family of description languages.  Having some 
        language family C,  we may ask whether there exists an optimal language in this 
        family.  A language A from L is  optimal  (for C)  if it is not worse than any other 
        description language in the family, i.e., if
                            (VR e C) (A < B).
          An optimal description language,  if it exists for some family,  should be used 
        to  measure complexity.  The complexity of an object with respect  to some fixed 
        optimal  description  language  can  be  called  algorithmic  entropy  of this  object.10 
        Entropy  is  the  final  version  of the  measure  of complexity  (when some family of 
        description languages is fixed).
          For some language families one can prove the existence of an optimal description 
        language.  For those families the notion of entropy is well defined.  The statements 
        of this type are usually called Solomonoff-Kolmogorov theorems,  since they were 
        first to discover such statements.
          A given family may contain (and usually contains)  many optimal description 
        languages.  Each of them gives some entropy function.  However,  due to the op­
        timality definition, every two entropies (corresponding to two optimal description 
        languages for some family) differ by at most an additive constant.  In other words, 
        if A and В are two optimal description languages in the family C, then there exists 
        a constant c such that
                         I Comply) -  Compß(y) I < c
        for all y.
          Remark.  Of course, one can rightfully complain that the notion of entropy 
        that pretends to be a complexity measure for individual objects is still defined only 
        up to some bounded additive term, and one would like to select some true entropy 
        function among different ones.  However, attempts of this type have not succeeded 
        up to now.
          We use the letter К to denote algorithmic entropy  (as a tribute to  Kolmo­
        gorov)11  and  sometimes  add  another  letter  to  specify  the  family  of description 
        languages used.  If K'  and K"  are two entropy functions for the same family of 
        description languages, then
                             I К' -  К" I  < c
        (as we have noted).
          10In the main part of the book we keep the name complexity for this notion, and we use the 
        word entropy for Shannon entropy only.
          11 In the main part of the book the letter К  is used for prefix version of complexity (entropy).
                                                 FACE TWO:  CHAOTICNESS                                       473
                    Kolmogorov not only gave a definition of algorithmic entropy, but also realized 
               its  connection  with  randomness.  He  observed  that  for  a  random  sequence  the 
               entropy of its n-bit prefix grows fast as n tends to infinity.  Notice that a random 
               sequence can start with, say, a million zeros, and the entropy of this prefix is very 
               low, but asymptotically it still grows fast.
                    When speaking about prefixes of binary sequences, we use binary strings (such 
               as  (I),  (II),  (III),  (IV))  as  objects  whose  complexity is  measured.  So  we  assume 
               that Y = E in the sequel.
                    If a description language contains a pair  (2,2),  this  means that  2  is  its own 
               description.  Consider a description language D that consists of all such pairs; this 
               D can be called a diagonal language (as mathematicians would say); linguists could 
               call  it  an  antonymous  description language.  Evidently,  Comply)  =  \y\.  Let us 
               consider only language families that include D (the family of monotone description 
               languages defined in the sequel, has this property).  Then for every entropy function 
               К for this family there exists some c such that
                                                       K(v) < \y\+c
               for  all  y.  So,  up to  an  additive  constant,  the  maximal possible value of entropy 
               for an n-bit string is n.  Kolmogorov conjectured that for a random sequence this 
               upper bound for its 71-bit prefixes is tight  (again up to a constant).  This is how 
               Kolmogorov interpreted the chaoticness property.
                    So let us fix some language family (that contains an optimal language), and let 
               К be one of the corresponding entropy functions.  A sequence
                                                      t t l   ■>  & 2  5  •  •  •  1  O '111  •  •  •
               is then called chaotic if there exists a constant c such that
                                                 K(ai, a2,.. •, an) > n -  c
               for all n.  Evidently, this definition does not depend on the choice of specific entropy 
               function in the family, but may depend on the choice of the family.
                   It turned out that for some natural language family the notion of chaoticness 
               defined in this way gives a reasonable formalization of the intuitive idea of random­
               ness.
                   In Kolmogorov complexity theory the relations between descriptions and ob­
              jects have an algorithmic nature.  Following Kolmogorov, we restrict ourselves to 
               enumerable12 sets.  The notion of an enumerable set is one of the main notions in 
              the theory of computability (and in mathematics in general).  It can be explained 
               intuitively in the following way.  Imagine a printing device that prints binary strings 
              sequentially;  printed strings are separated by spaces.  The time intervals between 
              printing consecutive strings may be arbitrary  (but each string should be printed 
              completely without delays, and infinite sequences of bits are not allowed).  It may 
              happen that  the  device hangs  (and  does  not  print  anything)  after  finitely  many 
              strings  have been printed,  then the set  of strings  printed by the device is finite. 
              In  particular,  the  device  may  print  nothing  at  all,  then we  get  an  empty set  of 
              output strings.  For such a device, the set of all printed strings is enumerable—and 
              every enumerable set can be obtained in this way, if the device is equipped with a
                   12What we call  enumerable is usually called  computably enumerable, or recursively enumer­
               able.  The word enumerable usually refers to countable sets.  In our exposition,  we use the term 
               enumerable sets to refer to computably enumerable sets; see footnote 13 below.
            474               2.  FOUR ALGORITHMIC  FACES OF  RANDOMNESS
            suitable program.  For example, for every formal theory (like set theory, or formal 
            arithmetic) the set of all theorems (provable statements) is enumerable.  The intro­
            duction of a formal computational model or of a general notion of a formal theory 
            falls beyond our scope.  However we will describe the notion of an enumerable set 
            in more detail.
                Let us start with countable sets.  This term is used in two different ways.  One 
            more narrow definition says that countable sets are those sets for which there exists 
            a one-to-one correspondence with the set N of all natural numbers.  The other more 
            liberal definition says that countable sets are those sets for which there exists a one- 
            to-one correspondence with some initial segment of N.  Here by initial segment of N 
            we mean a subset M of N that is downward-closed, i.e., every natural number that 
            is smaller than some element of M also belongs to M.  For example, the entire N 
            and the empty set 0 are both initial segments of N, and all finite sets are countable 
            in this more liberal interpretation.  We use this interpretation; then one can say that 
            a set is countable if it is either empty or can be represented as a set of terms of an 
            infinite sequence.  For example, the finite set {a, 6, c} is the set of terms of infinite 
            sequence a, 6, c, c, c, c,....  If we  additionally require that  this  infinite sequence is 
            computable, we get the definition of an enumerable set.  It remains to explain what 
            a computable sequence is.
                A sequence w\, w2,..., wn,... is called computable if there exists an algorithm 
            that for any given n computes its nth term wn.  One may say that the notion of a 
            computable sequence is an effective (algorithmic) version of the notion of sequence, 
            and the notion of an enumerable set  is  an  effective  (algorithmic)  version  of the 
            notion of a countable set.13  Let us repeat the definition:  a set is enumerable if it 
            is  empty or it is a set of terms of some computable sequence.
                All the description languages we consider are subsets of E x E and therefore are 
            all countable.  Kolmogorov suggested considering enumerable description languages 
            only.  The final step in the definition of chaoticness was made by Leonid Levin, a 
            student of Kolmogorov; in 1973 he published a paper in which a class of monotone 
            description languages was introduced, and the corresponding notion of chaoticness 
            was studied.14  Let us provide the corresponding definitions.
                We say that strings и and v are  compatible  and write и  ~ v  if one of these 
            strings is a prefix of the other one.
                A description language E is called monotone if E is enumerable and the fol­
            lowing requirement is satisfied:
                         {(xi,yi) e E к {x2,y2) € E к (xi « x2)) =>• (yi « y2).
            It can be shown that there exists a monotone description language that is optimal 
            for the family of monotone description languages.  So the notion of entropy for this 
            family is well defined;  the corresponding entropy function is called the  monotone 
            entropy15 and is denoted by KM.
                13To stress the difference between algorithmic and non-algorithmic notions, enumerable sets 
            are usually called recursively  enumerable or  computably  enumerable  (computable functions were 
            traditionally called  “recursive functions”  for historical reasons).  The word  “enumerable”  is often 
            used as a synonym for  “countable”.
                14A similar notion was introduced by Claus-Peter Schnorr in his publication of 1972; see the 
            footnote on p. 482.
                15In the main part of the book this function is called monotone  complexity, it is defined in 
            Section 6.2.
                        FACE THREE:  TYPICALNESS      475
          A sequence that  is chaotic  for monotone description  languages  is called just 
        chaotic in the sequel.16  The chaoticness requirement can be written as follows:
                     3c Vn    Й2,..., an)  > n — c).
          We denote the class of all chaotic sequences by C.
          It  seems that the definition of chaoticness is a good approximation to the in­
       tuitive notion of randomness.  There are two reasons for this.
          First, every chaotic sequences satisfies the standard laws of probability theory 
        (such as the strong law of large numbers, the law of iterated logarithm etc.).
          Second, the class C of chaotic sequences coincides with another natural candi­
       date for the randomness definition, the class T of typical sequences (see below):
                             C = T.
       One could even use the names typical-chaotic or  chaotic-typical for the sequences 
       in  C  (=T)  and denote this class by CT or TC.  This class is a proper subclass 
       of the class S of all Kolmogorov stochastic sequences (as we said, the definition of 
       stochasticity seems to be too liberal to reflect our intuition of randomness):
                          TC C S,  ТС ф S.
                        Face three:  Typicalness
          What do we mean by saying that some object is  “typical”  for some category? 
       This means that it belongs to every reasonable majority of objects selected from 
       this category.  For example,  a typical human being has height less than 2 meters 
       (i.e.,  belongs  to  the majority of people who have height  less than  2 meters),  has 
       age at least 3 (i.e., belongs to the majority of people who are at least 3 years old), 
       etc.  The adjective  “reasonable”  is important here, since every object x is doomed 
       to fall outside the overwhelming majority of objects that differ from x.
          Our intuition says that every random object is typical.  But how can we clarify 
       the  latter  notion?  Let  us  give  a mathematical  definition of typicalness  for  a bit 
       sequence  (assuming the uniform distribution on infinite bit sequences that corre­
       sponds to a fair coin tossing).  As we have said,  for that we need to specify what 
       an  “overwhelming majority”  is in the set of all sequences and when that majority 
       is  “reasonable”.  Then the class of typical sequences is defined as the intersection 
       of all reasonable overwhelming majorities.
          A set of sequences forms an overwhelming majority if its complement is small, 
       so we need to define the notion of a small set.  Using the language of probability 
       theory, we can say that some set Q is small if the event “randomly chosen sequence 
       is in Q”  has probability zero.  In terms of measure theory small sets are just sets of 
       measure 0.  However, we want to have a more explicit definition.  It can be given in 
       the following way.
          A set Q is small if it can be covered by a countable family of balls whose total 
       volume is arbitrarily small.  In other terms, Q is small if for every natural m there 
       exists a sequence of binary strings
                        (x(l),x(2 ),...,z(n),...)
          16Since this property is equivalent to Martin-Löf randomness  (called typicalness in this ap­
       pendix), we do not use a different name in the main text of the book.
                              2.  FOUR ALGORITHMIC FACES OF RANDOMNESS
            47C
            such that
                                             Q C
                                    £ vM»)) = £ 2Hx(",i<_L-
                                      n            n
            Evidently, each sequence forms a small set (a singleton), so the intersection of all sets 
            with small complements is empty, and we need to define “reasonable overwhelming 
            majority”  in a more restrictive way.
                This can be done by considering the following effective version of the definition 
            of a small set.
                First, we require the sequence (x(l),x(2),..., x(n),...)  in the definition to be 
            computable.  In other words, some algorithm should compute x{n) given n as input.
                Second,  we require not only the computability of this sequence,  but  uniform 
            computability:  the sequence (x(l), x(2),..., x(n),...) with required properties can 
            be constructed by some algorithm given m.  We need to explain what it means: 
            this  sequence  is  an  infinite  object,  and  algorithms  deal  with  finite  objects  only. 
            We require that there exists some algorithm that, given m, produces an algorithm 
            (=a program) that computes some sequence (x(l), x(2), ..., x(n),...) with required 
            properties.17
                These two changes in the definition of a small set give us a definition of a more 
            restricted notion, that of an effectively small set.18  The complements of effectively 
            small sets could be called effectively large sets.  Now the intersection of all effectively 
            large sets is not empty; moreover, this intersection itself is an effectively large set. 
            This smallest effectively large set is our goal:  we denote it by T and call it the set 
            of all typical sequences.
                Typical sequences are usually called Martin-Löf random sequences, since this 
            definition  was  suggested  (as  a definition of randomness)  in  1966  by  Per  Martin- 
            Löf, an eminent Swedish mathematician, who in 1964 and 1965 studied at Moscow 
            University under the supervision of Kolmogorov.
                As we have said already, the class T of all typical sequences coincides with the 
            class C of all chaotic sequences,
                                                T = C,
            and the elements of this class can be called  chaotic-typical or  typical-chaotic se­
            quences (and the class may be denoted by CT or TC).
                As we have already mentioned,
                                          CT c S,  CT Ф s.
                                    Face four:  Unpredictability
                Any random sequence is unpredictable in the following sense:  if we know the 
            values of some its terms, it does not give us any information about the terms not 
            revealed yet.  So if a Casino prepares a random sequence and then allows a Player 
            to  make bets on the values of the terms she does not  know,  the  Casino  is safe; 
            more precisely, there is no strategy for the Player that allows her to make Casino 
            bankrupt independent of the initial amount of money Casino has.
                17An equivalent definition requires that, given m  and n, an algorithm computes the nth term 
            of a sequence that satisfies the requirements for the given m.
                18In the main part of the book those sets are called effectively null sets.
                                    FACE FOUR:  UNPREDICTABILITY                      477
               In other words, we define the unpredictability of some sequence in terms of a 
            game where Casino uses that sequence and Player makes bets against that sequence, 
            i.e.,  on  the values of terms of that sequence not yet revealed.  Player and Casino 
            initially have some amount of money.  Casino also has some bit sequence, and Player 
            does not know it.  Player can then make bets about some bits of that sequence, not 
            necessarily in the monotone order and not necessarily about all bits; some terms of 
           the sequence may be skipped.
               We can imagine that bits are written on cards that lie on an infinite table face 
           down,  so  Player  does not see the bits:  she sees only an infinite sequence of card 
           backs.  At each move, Player points to some card, makes a prediction about the bit 
           on that card and declares the amount of her bet.  Then the card is turned over.  If 
           the prediction is correct,  Casino pays that amount to Player;  if the prediction is 
           wrong, Player loses her money (i.e., pays that amount to Casino).  Player wins if 
           she managed to make Casino bankrupt.  Of course, if Player has unlimited credit 
           resources, she can always win by doubling the bets until her guess becomes correct. 
           But we assume that Player has no credit line, so the amount of the bet should not 
           exceed her current capital.
               A sequence is called predictable if there is a strategy for Player that allows her 
           to win against that sequence.  This means that for the arbitrarily large initial capital 
           of Casino, Casino will nevertheless become bankrupt if Player uses this strategy.  A 
           sequence is called unpredictable if it is not predictable.
               More formally the game may be described as follows.  We consider an infinite 
           sequence of zeros and ones:
                                         a = (ai, й2, аз,...).
           At each move Player creates a triple
                                               (п,г,и),
           where
                                 ne N,  г G {0,1},   u G Q,  v ^ 0;
           here,  as  usual,  N  is  the  set  of natural  numbers,19  and  Q  is  the  set  of rational 
           numbers.  The meaning of this triple is the following:  n is the number of the bit 
           on which the bet is made, i is the predicted value of that bit, and the non-negative 
           rational number v is the amount of the bet.  The moves are performed sequentially, 
           starting from the first one;  the triple that represents the kth move is denoted by 
            (n(k),i(k),v(k)).  (More formally, moves are triples of the described form.)
               Player’s capital before the kth. move is denoted by V(k — 1).  Without loss of 
           generality we may assume that the initial capital of Player equals 1, i.e., P(0) = 1.
               After each move, Player’s capital changes according to the following rules:
                  •  if i(k) = an(fc)  (Player made a correct guess), then 
                    V(k) = V(k-l)+v(k);
                              n(k)  (Player made an incorrect guess), then 
                  •  if i{k) Ф a
                    V{k) = V{k-l)-v{k).
               Two additional remarks are needed.
               19Sometimes 0 is considered as a natural number (logicians and computability experts usually 
           do this), sometimes not—in this appendix we follow the second convention and do not consider 0 
           as a natural number.
              478                  2.  FOUR  ALGORITHMIC  FACES  OF  RANDOMNESS
                   First, moves may be valid or invalid, and the game continues only if the move 
              is valid.  By definition, a valid move should satisfy two requirements:
                     1)  the number of the bit on which bet is made is valid:  this means that this 
                        bit was not used earlier, i.e., n(k) does not appear among n(l),, n(k —
                        i);
                     2)  the bet itself is valid:  its size is less than the current capital, i.e., v(k) < 
                        V{k - 1).
                  The game stops when Player makes an incorrect move.  In this case she keeps 
              the current capital forever, and cannot win.
                  It  is  also  possible that  Player  refrains  from  making any move  (she may even 
              refrain from making the first move); in this case she also keeps the current capital 
              forever  and  cannot  win.  However,  we  do  not  say  in  this  case  that  the  game  is 
              stopped.  Player  can  think  for  an  arbitrarily  long  time  before  making  her  next 
              move; the time for thinking is not limited, so it is possible that she thinks forever, 
              i.e.,  never makes any move.  While thinking, the capital remains unchanged, so in 
              this case the capital remains unchanged forever.  We do not say, however, that the 
              game is stopped, since Player never explicitly declares that she will not make any 
              move.  So  three scenarios  are possible:  (1)  Player  makes  infinitely  many  moves; 
              (2)  Player attempts to make an invalid move and the game is stopped;  (3) Player 
              at some point starts thinking but never makes a move.
                  Of course, this is only an illustration, and the formal definition goes as follows. 
              By definition, Player wins against the sequence a if
                                                   sup V(k) = +00, 
                                                    к
              i.e.,
                                                  VW 3k V{k) > W.
              This means that Player can cause the bankruptcy of Casino independently of its 
              initial capital.  This is possible only if game is infinite, that is, at each turn Player 
              makes a valid move.
                  The game is described now,  and we define the notion of strategy.  A strategy 
              is a rule that tells Player what she should do, i.e., prescribes the next move based 
              on the history of the game.  The strategy is not  required to be total,  its output 
              may be undefined because Player makes no move:  the strategy produces an output 
              exactly in the cases when Player makes some move.  The input to the strategy is 
              the history of the game, that is, the sequence of all the moves made so far and the 
              values of the bits revealed so far.  (One could add to the history the information 
              about the capital at every moment, but this is redundant, since this information 
              can be easily computed.)
                  Here is the history before the kth move can be represented as a table:
                                             n(l)    n(2)         n(k — 1)
                                             г(1)    i( 2)        i(k -  1)
                                             v{l)    v(2)         v(k — 1)
                                            ®n( 1)  O'n(2)         an(k-1)
              (for к = 1 the table is empty).
                  A strategy therefore is a function that maps every table of this kind to a move 
              (n, i, v), or it may be undefined (on some tables).  Here “table of this kind” means an
                                                 FACE FOUR:  UNPREDICTABILITY                                         479
                arbitrary table with positive integers in the first row, non-negative rational numbers 
                in the third row, and bits in the second and fourth rows.
                     Assume that we are given a strategy and a table that can appear during the 
                game of that strategy against some sequence.  Then the first three rows of that table 
                can be uniquely reconstructed from the last row.  Indeed, we reconstruct the first 
                move (n(l), i(l), г>(1))  applying the strategy to the empty table.  Then  (assuming 
                that the fourth row is known) we know the history of the game before the second 
                move, i.e., the table
                                                                  n(l)
                                                                  i( 1)
                                                                  u(l)
                                                                 ®n( 1) •
                Then we apply the strategy again to find the second move  (n(2), i(2),v{2))  and 
                hence the table
                                                             n(l)     n(2)
                                                             i(l)      H 2)
                                                             u(l)     «(2)
                                                             ®n( 1)   ^n( 2)
                and so on.
                     So,  when defining strategies,  we may assume that only the fourth line of the 
                table is given to the strategy.  This line is a binary string (an element of E).  Given 
                a binary string,  the strategy may have no  output  or provide the next  move,  an 
                element  of N x  {0,1}  x  Q+,  as  an  output.  (Here  Q+  stands  for  the  set  of all 
                non-negative rational numbers.)
                     So we can now give the final definition of a strategy:  it is a partial mapping of 
                type
                                                      E ->• N x {0,1} x <Q>+.
                     We are interested in strategies that are computable, i.e., that can be computed 
                by an algorithm.  Let us specify what that means.  Assume that an algorithm A 
               gets  elements  of a set  X  as  input  and produces  elements  of a set  Y  as  output. 
                Consider the subset  of X  that  consists of all  inputs  for  which  A  provides some 
               output, and the function from this subset of X to Y that maps each input value 
               to the corresponding output value.  We say that A computes that function, and a 
               function is computable if some algorithm computes it.
                     We will consider strategies that are computable in this sense.  (If the algorithm 
               does  not  terminate  for  an  input  history,  then  the  strategy  is  undefined  on  that 
               history, in which case we may imagine that Player is thinking about her move but 
               never comes to any decision.)
                     We say that a sequence a is predictable if there exists a computable strategy 
               that wins against a (i.e., Player wins if she uses this strategy against a).  Otherwise, 
               a is unpredictable.20  The class of all unpredictable sequences is denoted by U.
                     It  is  known  that  every  unpredictable  sequence  is  Kolmogorov  stochastic  (it 
               belongs to the class S) and that every typical-chaotic sequence is unpredictable:
                                                            CT C U C S.
                     20In the main text unpredictable sequences are called  “Kolmogorov-Loveland random”; see 
               the discussion on p. 310.
          480            2.  FOUR ALGORITHMIC FACES OF  RANDOMNESS
             It  is  also  known that  the class of Kolmogorov stochastic sequences is signifi­
          cantly larger than the class of unpredictable sequences:
                                        S + U.
             But the question whether the classes of chaotic (=typical)  and unpredictable 
          sequences coincide, is still open:
                                     CT-U.
             This  is  an  important  problem;  several  people  tried  to  solve  it  but  got  only 
          partial results.
             Strategies  that  avoid  invalid  moves.  Defining  unpredictable  sequences, 
          we may restrict ourselves to strategies that never make invalid moves.  Indeed, we 
          can modify an algorithm A that computes the winning strategy, and get another 
          algorithm В that does not terminate when A attempts to make an invalid move. 
          One has to check whether the move is valid, and this can be done algorithmically: 
          knowing the input for A,  we reconstruct  the history of the game,  including the 
          numbers of bits revealed and the current  capital of Player,  so we can check the 
          validity of the move recommended by A and cancel an invalid attempt.21
                  Generalization for arbitrary computable distributions
             Up to now we considered only the case of uniform distribution on the space 
          fl  of binary sequences;  all the main  ideas  can be illustrated  in this special case. 
          Now, to complete the picture, we consider the general case of arbitrary computable 
          probability distribution on fl.  (See the definition below.)  Let us make some com­
          ments for readers who are not yet familiar with the general notion of a probability 
          distribution {measure).
             We say that a set M is equipped with a measure p if (1) some class of subsets of 
          M is chosen and its elements are called measurable subsets-, (2) for each measurable 
          subset A some number p{A) is chosen and this number is called the measure of A. 
          There are some requirements (axioms of measure theory); we do not go into detail 
          here  and  note  only  that  this  requirement  implies  the  following  fact:  any  finite 
          or countable union of disjoint  measurable subsets is measurable and its measure 
          is  equal  to  the  sum  of the  measures  of the  parts.  For probability  measures,  or 
          probability  distributions,  we  require  also  that  p{M)  =  1.  The  intuitive  meaning 
          of p{A) is the probability of the event  “a randomly chosen element of M belongs 
          to Ä \
             A  measure  on  Q.  is  determined  by  the  measures  of balls.  For  the  uniform 
          distribution (and only for it) we have
                                (VzeE) МПх) = 2"м ).
          It  corresponds  to  the  case where  zeros  and  ones  are  équiprobable  and  trials  are 
          independent.  A slightly more general case is Bernoulli  distribution,  also called a 
          binomial distribution.  Here the trials are also independent,  but  in each trial the 
          probabilities of 1 and 0 are p and 1 —p, respectively.  This number p is a parameter;
             21A  more complicated  argument  shows that  the class  of unpredictable sequences  does  not 
          change if we consider only total computable strategies,  i.e.,  the strategies defined  on  all inputs; 
          see the discussion on p. 310.
                        GENERALIZATION  FOR ARBITRARY COMPUTABLE DISTRIBUTIONS                      481
              for p = 1/2, we get uniform distribution.  Formally,  for the Bernoulli distribution 
             with parameter p we have
             where к is the number of ones in x.
                  The next step is to consider quasi-Bernoulli distributions where trials are still 
             independent,  but  the  probability  of success  may  depend  on  the  number  of the 
             trial:  in the kth trial the outcome 1 appears with probability p(k).  More formally, 
             consider a sequence of reals
                                  P = (pt1)»^2). • • -,p(k),...),   0 < p(k) < 1.
             Then the quasi-Bernoulli distribution with parameter p is defined by the formula
                                                             71
                                                  pifix) — 1 1  q ,
                                                            ;-i
             where r./ = p(i) if ж* = 1 and г* = 1 — p(i) if Xi = 0.  If p = (p,p,... ,p,...), we get 
             Bernoulli distributions as a special case.
                  In this section we show how the definitions of stochasticness, chaoticness, typ­
             icalness, and unpredictability can be extended to the case of arbitrary computable 
             probability distribution p (see the definition below).  Let us tell in advance that for 
             this more general case the same relationships hold:
                                          сад = ты  с и м  c sm , 
                                                   s(aO Ф и м
             (the last inequality is true assuming that all balls have positive measure).
                  Here C(p), T(p), U(/r), S(p) denote (respectively) the classes of chaotic, typ­
             ical,  unpredictable,  and Kolmogorov stochastic sequences with respect to the dis­
             tribution p\  these classes are defined below.  Our old classes now can be written 
             as C = C(?7), T = T(rj), U = U(?7), and S = S(77) for the uniform distribution rj 
             on ST
                  Let us warn the reader that this section is addressed to people who like gen­
             eralizations.  It is a bit more difficult than the previous exposition.  Moreover, our 
             task, that is the search for a natural definition of randomness, is less clear for gen­
             eral distributions.  The intuitive meaning of an individual random sequence as a 
             plausible outcome of some natural physical process like coin tossing becomes less 
             and less clear as we switch from the simple example of fair coin tossing and the 
             uniform distribution to more and more general classes of distributions.
                  Computable measures  (distributions).  One may attempt to call a mea­
             sure on Q computable if there exists an algorithm that for each binary string x 
             computes a measure p(£lx) of the ball Qx.  However, we have to be cautious:  the 
             output of an algorithm may be an integer or rational number (to be more precise, 
             its name or representation as a string over a finite alphabet), and we cannot name 
             all  the  real  numbers  since  we  have  only  countably  many  names.  So  we  require 
             that the algorithm computes not a real number (the measure of the ball) but its 
             approximation.
                  Here is the definition.  A measure p is computable if there exists an algorithm 
             that for any given pair (a binary string x, a positive rational e) computes a rational 
             number that differs from p(flx) at most by e.
        482        2.  FOUR  ALGORITHMIC  FACES  OF  RANDOMNESS
          One could add a requirement that there is an algorithm that for a given x says 
        whether the equality p(Qx)  — 0 holds or not.  That  requirement gives a strictly 
        smaller class of measures that are called strongly computable measures in the sequel.
          An  important  subclass  of the  class  of computable  measures  is  the  class  of 
        computable-rational measures where the measure of each ball 0 is a rational number 
        that can be computed (the corresponding fraction presented) given x.  Note that it 
        is not the same as a computable measure whose values (on balls) are rational num­
        bers:  in the latter case we are only able to provide arbitrarily close approximations 
        to the rational number which is the measure of the ball, and this is not enough to 
        produce this number entirely (as a fraction of two integers).
          Recalling that probability distributions are those measures for which the mea­
        sure of Q equals  1,  we may speak about computable probability distributions on 
        Q,.  Many definitions and statements about randomness for the uniform distribution 
        can be generalized naturally to arbitrary computable probability distributions.  In 
        particular,  one can prove a general version of Martin-Löf’s theorem  (saying that 
        the  intersection  of all  effectively  large  sets  is  an  effectively  large  set  itself),  and 
        Levin’s theorem (saying that typicalness is equivalent to chaoticness defined using 
        monotone complexity).22
          Stochasticness.  Recall  our  notation:  the  kth  term  of some  sequence  e  is 
        denoted by ek or (to avoid subscripts) by e(k).
          For the case of uniform distribution,  stochasticness was understood as global 
        frequency stability,  i.e.,  the stability of frequencies in all admissible subsequences. 
        Those subsequences were obtained by application of Kolmogorov-admissible selec­
        tion rules.  For the general case of an arbitrary computable measure this scheme 
        remains the same, but frequency stability should be replaced by some more general 
        property derived from the strong law of large numbers in probability theory.
          For the Bernoulli distribution with parameter p 6 (0,1) the definition is clear: 
        we require that every admissible subsequence has the frequency stability property 
                    p.  In other words, for every admissible subsequence the fraction 
        with limit frequency 
        of ones in its n-bit prefixes tends to p as n —> oo.  We also treat the cases p = 0 and 
        p —  1  in  a special way:  only the sequence that contains only zeros  (respectively, 
        ones) is stochastic.
          Can we consider an even more general case of non-Bernoulli distribution?  This 
        definitely goes beyond the original idea of von Mises:  he tries to define the notion 
        of probability as limit frequency in random sequences.  Still one can try to follow 
        this path, starting with quasi-Bernoulli sequences.
          One could not expect the existence of limit frequency in the subsequences of 
        a  quasi-Bernoulli  sequence  (and  different  subsequences  may  have  different  limit 
        frequencies even if they exist).  So the stochasticity requirement should take into 
        account the selection rules  (which terms were selected).  But  first  let  us exclude 
        the  case when  a bit  appears  that  has probability  zero:  we declare a sequence a 
        non-stochastic if there exists some к such that a(k) = 0 and p(k) = 1, or a(k) = 1 
        and p(k) = 0.  Assuming this does not happen, we call a sequence a stochastic with
          22In the main part of the book this result is called the  “Levin-Schnorr theorem”; Schnorr’s 
        paper  was  published  earlier  and  considered  some  special  notion  of complexity  called  “process 
        complexity”.  It can differ significantly from monotone complexity (see the section about history 
        below and the bibliography at the end of this appendix), but the underlying ideas are similar and 
        the proof for one of them can be easily adapted for the other one.
                                   GENERALIZATION FOR ARBITRARY COMPUTABLE DISTRIBUTIONS                                                           483
                    respect to a selection rule © if its generalized subsequence
                                                         b =                 a(m2),... ,a(mk),...),
                   obtained by this rule, satisfies the following requirement taken from the strong law 
                   of large numbers for quasi-Bernoulli distributions:
                                              a{mx) + ----V a{mk) _ p{mi) + • • • + p{mk)
                                                              к                                    к
                   as к —> oo.  Now we can define a stochastic sequence with respect to a given quasi- 
                   Bernoulli measure by requiring that for every Kolmogorov-admissible selection rule 
                   that  produces  an  infinite  generalized  subsequence  this  subsequence  is  stochastic 
                   with respect to this rule.
                          Remark.  By definition,  generalized subsequences are always infinite,  so the 
                   word  “infinite”  in the last sentence can be omitted.  However,  we use it to stress 
                   that we really are interested only in the infinite sequences, not tuples.
                          Now we want to extend the notion of Kolmogorov-stochasticness to a wider 
                   class of probability distribution.  First, let us introduce some notation.
                          Let n(l), n(2),..., n(k) be some natural numbers, and let г(1), i(2),..., i(k) be 
                   some bits.  By A1^ '                        we denote the set of all sequences a G Г2 such that
                   (*)                            ®n(l) — *(!)>  ®n(2) —                        ■ i  Q’n(k)  — i{k~).
                   The ratio                                               ип(1)- ■■■> n(k), m\ 
                                                                               l),...,i(fc), 1    )
                   is denoted in the sequel by
                                                                  ß m n(l), ..., n(k)\
                                                                        1     i{1),  • • •,  i{k)J
                   since it  is  the conditional probability of the event  “the mth  term of a equals  1” 
                   under condition (*).  That probability is undefined when the denominator equals 
                   zero.
                          Let us fix an arbitrarily sequence a e fi and some Kolmogorov-admissible se­
                   lection rule 0.  Our goal is to define the notion  “a is stochastic with respect to 0 ”. 
                   Recall that © was applied to select a subsequence of a in two steps.  First, we select 
                   an auxiliary generalized subsequence c; then the resulting subsequence is obtained 
                   by omitting some terms in c.  More precisely,
                                                            C      (®n( 1) ; ®n(2) ) • • • j Q‘n(k) )•••))
                   where the number n(k) is computed algorithmically given the tuple
                                                                 (®n( 1) i ®n(2) ) •  •  •  j Q"n(k—l)) •
                   Then,  using the same tuple as input,  the rule ©  decides whether the term an^.) 
                   should be included in the final subsequence b.  Therefore,
                                                   b = (a(n(ki)),a(n(k2)),...,a{n{kj)),...).
                   At both stages it may happen that 0 (the corresponding algorithm) does not pro­
                   duce any output (the number in the first case, and the decision bit in the second 
                   case).  Then b is finite and we do not require anything,  hence the sequence a is
             484               2.  FOUR ALGORITHMIC FACES OF  RANDOMNESS
             declared to be stochastic with respect to 0.  But if b is infinite, then some require­
            ment should be fulfilled to make a stochastic with respect to 0.  Let us describe 
            that requirement.
                 By rj we denote the conditional probability
                              ß n(kj)      n{ 1),     n(2),         n(kj - 1) \
                                         a(n(l)), a(n(2)), ..., a(n(kj - 1 ))J'
            Consider the difference
                       s  _ r 1+r2H------b rj    a(n(ki)) + a(n(k2)) H------b a(n(kj))
                      Oj —          :                              :                 •
                                    3                             J
            Here ôj is defined only if all rq,..., rj  are defined.
                 We say that b satisfies the strong law of large numbers if all ôj are defined and 
            Sj  —> 0 as j —> oo.  A sequence a is then called stochastic with respect to 0 if the 
            generalized subsequence obtained from a according to 0 satisfies the strong law of 
            large numbers.
                Finally,  a sequence a is called  Kolmogorov stochastic with respect to a given 
            probability distribution if a is stochastic with respect to every Kolmogorov-admis­
            sible selection rule that selects an infinite generalized subsequence from a.
                This definition by itself does not use the computability of the measure.  How­
            ever,  to  compare it  with other  randomness notions,  we need  to assume that  the 
            measure (the probability distribution in question) is computable.
                 Chaoticness.  A sequence a = (a\, a2, аз,...) is chaotic with respect to a com­
            putable measure ß if there exists a constant c such that
                               KM(ai,a2, • ■ •, an) > -  logß(ß
            for all n (here log stands, as usual, for the binary logarithm).
                For  arbitrary  computable measures,  the motivation for  this definition  is  the 
            same as it was for the uniform measure.  One can prove that for every computable 
            measure ß there exists some c such that
                                         KM(x) < -  log ß(üx) + c
            for all strings X.  Informally speaking, for every computable measure ß we can find 
            some monotone description language that fits that measure in the following sense: 
            it  provides short  descriptions  for strings  x  that  have big values of ß(ftx)  (as the 
            inequality above specifies).  The sequence is chaotic if those descriptions cannot be 
            significantly shortened (more than by a constant).
                Typicalness.  The definition of typicalness can be naturally extended to arbi­
            trary measures:  we used the volume (=the uniform measure) of balls when defining 
            small sets, and now we should use their measure instead.
                As before, we start by defining effectively small sets.  A set Q C 0, is effectively 
            small with respect to measure ß if there exists an algorithm A with the following 
            property.  Given  any positive integer m and input,  the  algorithm A produces as 
            output  an algorithm for the computing a sequence  (x(l), x(2),... ,x(n),...)  such
                              GENERALIZATION  FOR ARBITRARY COMPUTABLE DISTRIBUTIONS                                       485
                that
                                                              Q   C      ^x(n) )
                                                                      11
                                                             П
                Then a set  is  considered  effectively  large  with  respect  to  p  if its  complement  is 
                effectively small.
                      For every computable measure p the following Martin-Löf theorem holds:  the 
                union of all effectively small sets is effectively small, and therefore the intersection 
                of all  effectively  large  sets  is  effectively  large.  This  result  provides  the  smallest 
                effectively  large  set  which  is  called  the  constructive  support  of measure  p.  The 
                elements of this constructive support are called typical with respect to p, so the set 
                T(p) is defined as the constructive support of the distribution p.
                      Unpredictability.  Let us explain how the definition of unpredictability (given 
                above for the uniform distribution) should be changed for the case of arbitrary dis­
                tributions.  Two changes are necessary for that:  some auxiliary factor (that equals 
                1 for the uniform distribution and was therefore omitted), and some additional rule 
                that tells us when to stop the game (for the uniform distribution it is not needed 
                since the corresponding situation cannot happen).
                      The payoff for bets depends on the probability distribution.  If Player makes a 
                wrong guess, her bet is lost, i.e., the capital decreases by the size of the bet.  But if 
                she makes a correct guess, the increase is proportional to the bet, and the coefficient 
                depends on the probability of the correctly predicted outcome.  The coefficient is 
                large if this outcome has small probability, and is small if it has large probability. 
                For uniform distribution the probability is always 1/2 and the coefficient is always 1. 
                The exact  value of the  coefficient  for  an  arbitrary  distribution  is  determined  as 
                follows.
                      Recall that ад-  denotes the kth. term of a sequence a; similarly,  alk  is the kth 
                term of a', etc.  Player’s j th move is a triple (n(j), i(j), v(j)).
                     Let a be the sequence used by Casino for the game.  Let
                                A(k — 1) = ja' G Г2: a'n^  = an^  for all j — 1,2 ..., к - 1}
                (so A(0) = Г2) and
                                        Ai{k) = ja' G A(k — 1) : a'n^  = i} for i = 0,1.
                This notation makes sense if all the numbers n{l) appearing in it are defined.  Note 
                that
                (1)                               Г2 = A(0) D A{ 1) D A(2) D • • •  ,
                ( 2)                        1 = p{A{0)) ÿ д(Л(1)) ÿ ß(A(2)) > • • • .
                If Player’s fcth guess was correct, then
                (3)                               i(k)     dn(fc))    -^i(fc)(^0 — ^(^)i
                otherwise
                (4)                             i(k) ф an{k),       Ai_i^(k) = A(k).
          486             2.  FOUR  ALGORITHMIC  FACES  OF  RANDOMNESS
          Note also that
           (5)                    A{k-l) = A0{k)UAl{k).
              If i(k)  = ап(^)  (i.e.,  the kth guess was correct),  Player’s capital increases ac­
          cording to the formula
          (6)               V{k) = V(k - 1) + v{k) • ß{Ai(k){k))
          This  formula guarantees  that  the  game  is  fair,  i.e.,  the  expected  change  of the 
          capital  at  the  fcth  step  equals  zero.  However,  an  unpleasant  surprise  is  possible 
          when we apply this rule:  the value р(Ацкфк)) in the denominator may be equal 
          to 0.  In this case (which was not possible for the uniform distribution nor for any 
          positive  distribution  where  all  balls  have  positive  measures)  a special  additional 
          stopping rule is used.
              Additional stopping rule.  This is used when it happens (for the first time) 
          that p,(A(k)) — 0 (cf. equation (2) above).  Assume that p(A(k — 1)) ф 0, p(A(k)) — 
          0.  The last move made was the kth move, when Player made a prediction i(k).  If 
          the prediction turns out to be correct (i.e., i(k) = an^), then the game is stopped 
          and Player’s capital  is  declared to  be infinite  V(k)  =  +oo,  and Player wins  the 
          game.  If the prediction turns to be incorrect, i.e.,  i(k) ф an(/ф  then the game is 
          also stopped, but in this case the capital of Player remains unchanged (and fixed), 
          so Player does not win the game.
              This rule takes care of the problem of a zero denominator in (6).  Indeed,  (6) 
          is applied only if i(k) = an(ky  In this case А^кфк) = A(k), according to (3).  So if 
          we get a zero denominator, it means that p(A(k)) = 0.  But in this case we apply 
          the additional stopping rule instead of (6).  (Or we could say that we apply (6) and 
          declare that we get +oo when dividing positive number ß{Ai_i^(k)) by zero.)
              The definitions of a strategy, a computable strategy, a strategy that performs 
          only valid moves remain (up to these changes) the same as for the special case of 
          the uniform distribution.
                                 History and bibliography
              We print the numbers in italic to distinguish them from the references in the 
          main list of references.
           [1]  A. Kolmogorov, V. Uspensky.  “Algorithms and randomness.” SIAM  J.  Theory Probab.  A ppi, 
              v.  32  (198),  p.  389-412.  Translated  with annoying errors  (for  instance,  everywhere instead 
              of the  correct  translation  “recursively  enumerable”  an  incorrect  translation  “countable”  is 
              used);  better  translation  can  be  found  in:  Yu.  V.  Prokhorov,  V.  V.  Sazonov,  eds.,  Proc. 
              1st  W orld  Congress  of the  Bernoulli  Society  ( Tashkent  1986), v.  1,  Probability  Theory  and 
              Appi, VNU Science Press, Utrecht,  1987, p. 3-55.
           [2]  V. Uspensky,  A. Semenov.  Algorithms:  main ideas  and  applications.  Kluwer Academic Pub­
              lishers,  1993, 269 pp.
           [5]  V.  Uspensky,  A.  Semenov,  A.  Shen.  “Can an individual sequence of zeros and ones be ran­
             dom?”  Russian Math.  Surveys, v. 45(1),  1990,  121-189.
           [4]  V. Uspensky, A. Shen.  “Relations between varieties of Kolmogorov complexities.”  M athemat­
              ical Systems  Theory, v.  29 (3),  1996, p.  271-292.
           [5]  An.  Muchnik,  A.  Semenov, V.  Uspensky.  “Mathematical metaphysics of randomness,”  The­
              oretical  Com puter Science, v.  207,  1998, p.  263-317.
           [6 ]  A. Shen,  “On relations between different algorithmic definitions of randomness,”  Soviet Math. 
              Dokl., v. 38  (2),  1989, 316-319.
                                         HISTORY  AND  BIBLIOGRAPHY                              487
              [7]  V. V ’yiigin,  “Algorithmic entropy (complexity) of finite objects and its application to defining 
                 randomness and amount of information,”  Selecta M athematica (formerly Sovietica), v. 13(4), 
                 1994, p.  357-389.
              [S]  M. Li, P. Vitânyi. An Introduction to Kolmogorov Complexity and Its Applications., Springer- 
                 Verlag,  1993, xx+546 pp., 38 illustrations; third ed., 2008, xxiii+790 pp., 50 illustrations.
                 Of course, this list is not complete in any sense.  However, in these publications 
             (especially in [5]) one can find further references to get a more complete picture.  In 
              2 ],  in Section 2.6  (Applications to probability theory) different definitions of ran­
             [
             dom sequence are given (pp. 166-178).  Note that the terminology in [2] is different; 
             what we call chaotic sequences is called there Kolmogorov random sequences, what 
             we  call  typical  sequences  is  called  there  Martin-Löf  random  sequences;  Church 
             stochastic  sequences  are  called  there  Mises-Church  random sequences,  and  Kol­
             mogorov stochastic sequences are called there Mises-Kolmogorov-Loveland random 
             sequences,  and in  [5]  they are called Kolmogorov-Loveland stochastic sequences. 
             D. Loveland independently discovered this class later, in 1966, while Kolmogorov’s 
             paper appeared in 1963.  The unpredictable sequences  (as defined by us)  do not 
             appear in [2 ] since they were introduced only later (in 1998, see [5]).
                 An example of a Church stochastic sequence that becomes not Church stochas­
             tic  after a computable permutation of its terms was published by D. Loveland in 
             1966.  That  example  is  important  not  only  because it  shows  a flaw  in  Church’s 
             definition, but also because it stresses an important property of randomness that is 
             intuitively obvious but was not taken into account earlier:  conservation after every 
             computable permutation.
                 Description complexity theory, i.e., the theory of complexity of objects, should 
             not be mixed with computational complexity theory, i.e., the theory of complexity 
             of computations.  Description  complexity theory  forms  the  basis  for  algorithmic 
             information theory.  Both theories,  closely related,  were founded  by Kolmogorov 
             in  his  seminar talks  at  Lomonosov  Moscow  State University in the beginning of 
             the 1960s; Kolmogorov’s main goal was to create a new foundation for information 
             theory based on the idea that the more complex an object is,  the greater is the 
             information carried by that object.  That new foundation should avoid the notion 
             of probability replacing it by the notion of algorithm,  and also should be applied 
             to  the definition of an individual random object.  In his  1969 paper  (the English 
             version was published in 1968) Kolmogorov wrote:
                        (1)  Basic information theory concepts must and can be founded 
                            without recourse to probability theory, and in such a man­
                            ner that  “entropy”  and  “mutual information”  concepts are 
                            applicable to individual values.
                       (2)  Thus introduced, information theory concepts can form the 
                            basis  of the  term  random,  which  naturally  suggests  that 
                            randomness is the absence of regularities.23
                 The idea of measuring the complexity of an object by the length of its short­
             est description was proposed by Kolmogorov in his paper of 1965;24  a year earlier 
             similar ideas were published in the U.S. by Ray Solomonoff (Kolmogorov learned
                 23The published English version of this paper says  “random is the absence of periodicity”, 
             but this is evidently a translation error, and we correct the text following the Russian version.
                 24See item [78] in the main list of references.
        488          2.  FOUR  ALGORITHMIC FACES  OF  RANDOMNESS
        about Solomonoff’s work when publishing his  1969 paper,25  and he cited it).  So 
        we  called  the  statement  about  existence  of an  optimal  description  language  the 
        Solomonoff-Kolmogorov theorem.  At the same  time  (the middle of 1960s)  Kol­
        mogorov suggested in his seminar talks that the growth of complexity of prefixes 
        can be used to define randomness for individual infinite sequences.  However, the 
        family of description languages introduced by Kolmogorov turned out to be unsuit­
        able for this,  and (as we have said before)  a suitable family was found in 1973 by 
        Leonid Levin who defined the notion of monotone entropy.
           Typical sequences were defined  (and called  “random”)  by Per Martin-Löf in 
        1966, as we have said earlier.
           The existence of a Kolmogorov stochastic sequence that is not typical (= not 
        chaotic) was proven by Alexander Shen (see [6] or [2, Section 6.2.4]).26
           Let  К be one of the entropy functions  (many of them were studied,  includ­
        ing plain,  a priori,  monotone,  process, prefix, and decision entropies;  the versions 
        mentioned are different in the sense that the difference between any two of these 
        entropy functions is not bounded).  We may try to define chaotic sequences (with 
        respect to the uniform distribution) using К by requiring that
                        3c Mn (K(ai, a2, a3,..., an) > n -  c).
        (Just for the record:  for plain and decision entropy no sequences with this property 
        exist,  and for four other versions we get a definition that is equivalent to typical­
        ness.)  The equivalence of chaoticness for monotone entropy and typicalness was 
        shown by Levin in the same paper where monotone entropy was introduced.  In­
        dependently Claus-Peter Schnorr in his 1973 paper27  (the conference version was 
        published in 1972) introduced another version of entropy, process entropy (Schnorr 
        used  the  name  “process  complexity”)  and  proved  (by  a  similar  argument)  that 
        the corresponding notion of chaoticness is equivalent to typicalness.  Process en­
        tropy and monotone entropy differ significantly (their difference is unbounded, as 
        Vladimir Vyugin showed in [7]); later Schnorr switched to monotone entropy, and 
        the equivalence between chaoticness based on monotone entropy and typicalness is 
        sometimes called the Levin-Schnorr theorem.
           Prefix entropy was introduced by Levin in his Ph.D. thesis submitted in 1971, 
        but the thesis was rejected28 and the definition was published only in 1974.29  Later 
        Gregory J. Chaitin independently discovered the same definition (see his paper “A 
        theory of program size formally identical to  information theory”,  Journal  of the 
        Association of Computing Machinery,  1975, v.  22,  no.  3,  329-340)  where he also 
        introduced chaoticness definition using prefix entropy and claimed (without proof) 
        that  this  version  of chaoticness  is  equivalent  to  typicalness;  the  proof was  first 
        published in Vyugin’s paper  [7,  Corollary 3.2].  Prefix entropy can be defined as
           25See item [79]  in the main list of references.
           26The  main idea of this  proof was  invented  by  M.  van  Lambalgen  for  monotone selection 
        rules and can be easily generalized to non-monotone ones.  — A.  Shen.
           27See item [169] in the main list of references.
           28Levin was a USSR citizen.  The rejection of his thesis, having been approved by Kolmogorov 
        who was the thesis advisor and all the reviewers, took place for political reasons.  He emigrated 
        in  1978 and earned a Ph.D. at the Massachusetts Institute of Technology (MIT)  in 1979.
           29See  item  [94]  in  the  main  list  of references,  where  the  prefix  entropy  was  called  prefix 
        complexity, we use the same name in the main part of this book.
                       HISTORY AND  BIBLIOGRAPHY      489
       entropy for the family of prefix description languages.  A set E is a prefix description 
        language if E is enumerable and the following condition holds:
                ((xi,yi) e E & (x2,y2) G E & (xi « x2)) => (yi = y2).
          Note also that the term  “complexity”  is normally used for what we call  “en­
       tropy”  (i.e., complexity with respect to an optimal description language).
          Unpredictable sequences (as defined above) appeared (spring 1991) in the joint 
       talk  “Randomness and Lawlessness”  given by Andrei Muchnik,  Alexey Semenov, 
       and Vladimir Uspensky at the conference in California devoted to the foundations 
       of randomness  (March 4-7,  Institute for Mathematical Studies in the Social Sci­
       ences, Stanford University).  The paper [5] published in 1998 is based on that talk, 
       and  it  contained  the  results  about  relations  between  unpredictability  and  other 
       randomness notions.30
          Note  that  the  definition  of unpredictability  given  in  the  present  exposition 
       slightly differs from the definition in [5].  Namely, in [5] the bet was called valid if 
       a weaker inequality v(k)  ^  V(k — 1) holds, while we require the strict inequality 
       v(k)  <  V(k —  1).  Both  definitions  are  equivalent  (i.e.,  lead  to  the  same  class 
       of unpredictable sequences),  but  still our current  definition looks somehow more 
       thoughtful.  There  are  two  reasons  to  prefer  the  new  version.  First,  the  game 
       looks more natural:  if Player bets all her capital and makes a wrong guess,  then 
       no money is left and the rest of the game is trivial  (only zero bets are possible). 
       Second, we need strict inequality to make the game realistic from the algorithmic 
       viewpoint for arbitrarily computable measures (only computable-rational measures 
       were considered in [5]).  Indeed, before a bet is made, Player should check that the 
       bet  is  valid.  She  can check the strict  inequality v(k)  <  V(k — 1)  before making 
       the bet (checking algorithm terminates and confirms the inequality if it holds, and 
       does not terminate otherwise), but one cannot construct a similar algorithm for the 
       inequality v(k) ^ V(k — 1) and the arbitrary computable measure.
          The game approach to randomness was mentioned already by von Mises who 
       spoke about the non-existence of a winning strategy (without formal definitions) 
       when playing against Casino.  Later several formal definitions were suggested, but 
       the version from [5]  (with a cosmetic change mentioned above) seems to be more 
       adequate.  Indeed, in the previous versions either the computability requirement for 
       the strategy was replaced by a requirement of another kind (still of an algorithmic 
       nature, but less natural) or the resulting class of sequences was known to be different 
       from the class of chaotic-typical sequences.  For the definition from [5] there is still 
       some hope that it is equivalent to chaoticness and typicalness;  if it is indeed the 
       case,  this equivalence will be another reason to believe that this class  (of chaotic- 
       typical sequences)  is  a good  approximation for our intuitive  notion of a random 
       sequence.
          30For the case when the bets are made from left to right, as the sequence terms appears, the 
       game approach to randomness and the corresponding notion of a martingale was introduced in 
       the 1930s by Jean Ville [206] as an alternative to von Mises’ approach.
                                                     Bibliography
                 [1]  Ahlswede R., Cai N., Li S. R., Yeung R., Network Information Flow, IEEE  Transactions  on 
                     Information  Theory, v. 46 (2000), no. 4,  1204-1216.
                 [2]  Ahlswede R., Körner J., On common information and related characteristics of correlated in­
                     formation sources, Preprint, Presented at the 7th Prague Conference on Information Theory 
                     (1974).
                 [3]  Alon N., Newman I., Shen A., Tardos G., Vereshchagin N., Partitioning multi-dimensional 
                     sets in a small number of “uniform” parts, European Journal of Combinatorics, v. 28 (2007), 
                     134-144. Preliminary version, Electronic  Colloquium on  Computational Complexity, TR05- 
                     095  (2005) eccc.hpi-web.de.
                 [4]  Andreev M., Busy beavers and Kolmogorov complexity, Pursuit of the  Universal, Proceed­
                     ings of the 12th Computability in Europe conference, CiE2016, Paris, France, June 27-July 
                     1,  2016,195-204.
                 [5]  Bauwens B., Plain and prefix complexity characterizations of 2-randomness:  simple proofs, 
                     Archive for Mathematical Logic, v.  54 (2015), no. 5, 615-629. arXiv: 1310.5230 (2013).
                 [6]  Bauwens B., Shen A., Complexity of complexity and strings with maximal plain and prefix 
                     Kolmogorov complexity, Journal of Symbolic  Logic, v. 79 (2014), issue 2, 620-632.
                 [7]  Bauwens B.,  Shen A., Takahashi H.,  Conditional probabilities  and van  Lambalgen theorem 
                     revisited, arXiv: 1607.04240 (2016).
                 [8]  Becher V.,  Figueira S.,  and  Picchi  R.,  Turing unpublished  algorithm for normal numbers, 
                     Theoretical  Computer Science, v. 377 (2007), no.  1-3,  126-138.
                 [9]  Bennett C. H., Gåcs P., Li M., Vitånyi P. M. B., and Zurek W., Information distance, IEEE 
                     Trans.  Information  Theory,  v.  44  (1998),  no.  4,  1407-1423.  Preliminary version,  Thermo­
                     dynamics of computation and information distance,  Proc.  25th  ACM  Symp.  on  Theory  of 
                     Computing (STOC 1993), p. 21-30.
                [10]  Bienvenu L.,  Game-theoretic  characterization of randomness:  unpredictability and stochas- 
                     ticity,  Ph.D.  thesis, University of Marseille,  2008.
                [11]  Bienvenu L., Desfontaines D., Shen A., Generic algorithms for halting problem and optimal 
                     machines revisited,  Logical Methods  in  Computer Science, v.  12  (2:1), 2016,  1-29.  See also 
                     arXiv:1503.00731.pdf.
                [12]  Bienvenu  L.,   Downey  R.,  Kolmogorov  complexity  and  Solovay  functions,  Elec­
                     tronic  Proc.  26th  Symp.  on  Theoretical  Aspects  of  Computer  Science  (STACS  2009), 
                     stacs2009.informatik.uni-freiburg.de/proceedings.php
                [13]  Bienvenu L., Gåcs P., Hoyrup M., Rojas C., and Shen A., Algorithmic tests and randomness 
                     with respect  to a class of measures,  Proc.  of the  Steklov  Institute  of M athematics,  v.  274 
                     (2011), 41-102. See also arXiv: 1103.1529v2.
                [14]  Bienvenu  L.,   Hölzl  R.,   Porter  C.,    Shafer  P.,   Randomness  and  semi-measures, 
                     arXiv: 1310.5133v2.
                [15]  Bienvenu  L.,  Hoyrup  M.,  Shen  A.,  Layerwise  computability  and  image  randomness, 
                     arXiv:1607.04232.
                [16]  Bienvenu L., Muchnik An., Shen A., Vereshchagin N.K., Limit complexities revisited, Theory 
                     of Computing Systems, v. 47 (2010), no. 3, 720-736. See also arXiv:0802.2833. A corrected 
                     version which includes also simplified proofs of stronger results:  arXiv: 1204.0201
                [17]  Bienvenu L.,  SablikM.,  The  dynamics  of cellular  automata  in  shift-invariant  topologies, 
                     Proc.  11th  Conference  on  Developments  in  language  theory  (DLT  2007),  Lecture  Notes 
                     in Computer Science, v. 4588, 84-95.
                [18]  Bienvenu  L.,  Shafer  G.,  Shen  A.,  On  the  history of martingales  in  the study  of random­
                     ness,  Electronic  Journal for  H istory  of Probability  and Statistics, v.  5  (2009),  no.  1,  1-40,
                                                                491
                                       BIBLIOGRAPHY
          492
               www.jehps.net/juin2009/BienvenuShaferShen.pdf A  more detailed  exposition,  Bienvenu 
               L.,  Shen A., Algorithmic information theory and martingales, arXiv:0906.2614.
           [19]  Bienvenu  L.,  Shen  A.,  Random  semicomputable  reals  revisited,  Computation,  Physics 
               and  Beyound,  Lecture  Notes  in  Computer  Science,  v.  7160,  2012,  31-45.  See  also 
               arXiv:1110.5028.
           [20]  Bienvenu L.,  Shen A.,  K-trivial,  К-low and MLR-low sequences:  a tutorial.  Fields  of logic 
               and computation, II. Essays dedicated to Yuri Gurevich on the occasion of his 75th birthday, 
               Lecture Notes in Computer Science, 9300 (2015), p.  1-23. See also:  arXiv: 1407.4259.
           [21]  Borei E.,  Le  hasard,  Paris,  Librairie Félix Alcan,  1920.
           [22]  Borei E., Probabilité et certitude, Presses Universitaires de France, 1961. English translation: 
               Borei E.,  Probability  and  certainty, Walker,  1963.
           [23]  Buhrman H., Fortnow L., Laplante S., Resource-bounded Kolmogorov complexity revisited, 
               SIAM Journal on  Computing, v. 31 (2002), no. 3, p. 887-905.
           [24]  Buhrman H., Fortnow L., Newman I., Vereshchagin N., Increasing Kolmogorov complexity, 
               Proc.  22nd  Symp.  on  Theoretical  Aspects  of  Com puter  Science  (STACS  2005),  Lecture 
               Notes in Computer Science v. 3404, 412-421. Preliminary version, Electronic Colloquium on 
               Computational Complexity, TR04-081 (2004).
           [25]  Calude C. S.,  Inform ation  and  randomness:  an  algorithmic perspective,  2nd ed.,  Springer- 
               Verlag, 2002 (first edition,  1994), 450 pp.  ISBN 3-540-43466-6.
           [26]  Calude  C. S.,  Hertling  P.,  Khoussainov  B.,  Wang,  Y.,  Recursively  enumerable  reals  and 
               Chaitin  Omega numbers,  Proc.  15th  Symp.  on  Theoretical  Aspects  of  Com puter  Science 
               (STACS 1998), Lecture Notes in Computer Science v.  1373, 596-606.
           [27]  Calude C. S., Staiger L. and Terwijn S., On partial randomness, Annals of Pure and Applied 
               Logic, v.  138 (2006), no.  1-3, 20-30.
           [28]  Chaitin  G. J.,  On the  length  of programs  for  computing binary sequences,  Journal  of the 
               ACM, v.  13 (1966), no. 4, 547-569.
           [29]  Chaitin G. J.,  On the length of programs for computing binary sequences:  statistical con­
              siderations,  Journal of the  ACM , v.  16 (1969), no.  1,  145-159.
           [30]  Chaitin  G.J.,  Computational  complexity  and  GödePs  incompleteness  theorem,  ACM  
               SIGACT News, no. 9 (1971), 11-12.
           [31]  Chaitin  G. J.,  Information-theoretic  limitations  of formal  systems,  Journal  of  the  ACM , 
              v.  21  (1974), no.  3, 403-424.
           [32]  Chaitin G. J., A theory of program size formally identical to information theory,  Journal of 
               the  ACM, v. 22 (1975), no. 3, 329-340.
           [33]  Chaitin G. J., Information-theoretic characterizations of recursive infinite strings,  Theoreti­
               cal  C om puter Science, v.  2  (1976), issue 1, 45-48.
           [34]  Chaitin G. J., Incompleteness theorems for random reals, Advances in Applied M athematics, 
              v.  8  (1987),  119-146.
           [35]  Chaitin  G.J.,  Algorithm ic  inform ation  theory,  Cambridge  University  Press,  1987.  Third 
              printing,  1990.
           [36]  Champernowne D.G., The construction of decimals normal in the scale of ten,  Journal  of 
               the  London Mathematical Society, v. 8 (1933), 254-260.
           [37]  Chan T. H., Yeung R. W., On a relation between information inequalities and group theory, 
              IEEE Transactions on Information  Theory, v. 48 (2002), no. 7,  1992-1995.
           [38]  Chernov A.V., Complexity of sets obtained as values of propositional formulas, M athematical 
              Notes, v.  75,  issue  1  (January 2004),  131-139.
           [39]  Chernov A., Muchnik An. A., Romashchenko A., Shen A., Vereshchagin N. K., Upper semi­
              lattice of binary strings with the relation  “x is simple conditional to y”,  Theoretical  Com­
              puter Science, v. 271 (2002), no.  1-2, 69-95. Preliminary version,  Proc.  1 4 th IEEE  Confer­
               ence  on  Computational  Complexity (CCC 1999),  114-122.
           [40]  Chernov A., Hutter M., Schmidhuber J., Algorithmic complexity bounds on future prediction 
              errors, Information and Computation, v. 205 (2007), 242-261. DOI 10.1016/j.ic.2006.10.004. 
              See also arXiv:cs/0701120.
           [41]  Chernov A., Shen A., Vereshchagin N., Vovk V., On-line Probability, Complexity and Ran­
              domness, Proc.  19th  Conference on Algorithmic Learning  Theory (ALT 2008), 138-153.
           [42]  Chernov A., Skvortsov D., Skvortsova E., and Vereshchagin N., Variants of realizability for 
              propositional  formulas and the logic of the weak law of excluded middle,  Proc.  of Steklov
                               BIBLIOGRAPHY                   493
           Institute  of M athematics,  v.  242  (2003),  67-85.  Preliminary version,  Proc.  16th  Workshop 
           on  Computer Science Logic (CSL 2002), Lecture Notes in Computer Science v. 2471, 74-88.
         [43]  Chung  F. R. K.,  Graham  R. L.,  Frankl  P.,  Shearer  J.B.,  Some  intersection  theorems  for 
           ordered sets and graphs, Journal of Combinatorial  Theory,  A, v. 43 (1986), 23-37.
         [44]  Church  A.,  On  the  concept  of a random  sequence,  Bull.  Amer.  Math.  Soc,  v.  46  (1940), 
           no.  2,  130-135.
         [45]  Cormen T. H.,  Leiserson  C. E.,  Rivest,  R. L.,  Stein  C.,  Introduction  to  Algorithms,  3  ed., 
           Cambridge, MIT Press, 2009.
         [46]  Daley R.P., Minimal-program complexity of pseudo-recursive and pseudo-random sequences, 
           Mathematical Systems  Theory (now Theory of Computing Systems), v. 9 (1975), no.  1, 83- 
           94.
         [47]  Dawid  A. P.,  de  Rooij  S.,  Shafer  G.,  Shen  A.,  Vereshchagin  N. K.,  and  Vovk  V.,  Insur­
           ing against loss of evidence in game-theoretic probability,  Statistics  &  Probability  Letters, 
           v.  81  (2011), no.  1,  157-162. See alsoarXiv:1005.1811.
         [48]  Day,  A.,  Increasing the gap between descriptional complexity and algorithmic probability, 
           Proc.  2 4 th  IE EE  Conference  on  Com putational  Com plexity  (CCC 2009), 263-273. A more 
           detailed exposition, homepages .mes. vuw. ac .nz/"adam/papers/dayjnonotone_a_priori .pdf.
         [491  Downey R., Hirschfeldt D., Algorithmic randomness and complexity, Springer-Verlag, 2010, 
           855 pp.  ISBN 978-0387955674.
         [50]  Downey R.,  Hirschfeldt D.,  Nies A.,  TerwijnS.,  Calibrating  randomness.,  The  Bulletin  of 
           Symbolic Logic, v.  12 (2006), no. 3, 411-491.
         [51]  Durand  B.,  Levin  L.,  Shen  A.,  Complex tilings,  Journal  of Symbolic  Logic,  v.  73  (2007), 
           no.  2,  593-613. See also:  arXiv:cs/0107008.
         [52]  Durand D., Shen A., and Vereshchagin N., Descriptive complexity of computable sequences, 
           Theoretical Computer Science, v. 171 (2001), 47-58. Preliminary versions, Proc.  16th Symp. 
           on  Theoretical  Aspects  of  Com puter  Science  (STACS  1999),  Lecture  Notes  in  Computer 
           Science v.  1563,  p.  153-162,  Electronic  Colloquium  on  Computational  Complexity, TR01- 
           087 (2001).
         [53]  Durand B.  and Vereshchagin N.,  Kolmogorov-Loveland stochasticity for finite strings,  In­
           formation Processing Letters, v. 91  (2004), 263-269.
         [54]  Fortnow L., Lee T., Vereshchagin N., Kolmogorov complexity with error, Proc.  23rd Symp. 
           Theoretical Aspects of Computer Science (STACS 2006), Lecture Notes in Computer Science 
           v. 3884, 137-148. Preliminary version, Electronic Colloquium on Computational Complexity, 
           TR04-080 (2004).
         [55]  Gâcs  P.,  On  the  symmetry  of algorithmic  information,  Soviet  Math.  Dokl,  v.  15  (1974), 
           no.  5,  1477-1480.
         [56]  Gâcs P., Exact expressions for some randomness test, Zeitschrift für Math.  Logik und Grund­
           lagen d.  M ath., v.  26  (1980), 385-394.
         [57]  Gâcs P., On the relation between descriptional complexity and algorithmic probability,  The­
           oretical  Com puter Science,  1983,  v.  22,  71-93.  Preliminary version,  Proc.  22nd  Symp.  on 
           Foundations  of Computer Science (FOCS 1981), 296-303.
         [58]  Gâcs  P.,  Every sequence is reducible to a random one,  Information  and  Control  (now In­
           formation and  Computation), v. 70 (1986), no. 2-3,  186-192.
         [59]  Gâcs, P., and Körner, J., Common information is far less than mutual information, Problems 
           of Control  and Information  Theory, v. 2 (1973), no. 2,  149-162.
         [60]  Gâcs P., Tromp J., Vitånyi P.M.B., Algorithmic statistics, IEEE  Transactions  on Informa­
           tion  Theory, v.  47, no.  6,  2001,  2443-2463.
         [61]  Goldreich O., Foundations of Cryptography.  V.  1.  Basic  Tools, Cambridge University Press, 
           Cambridge, 2007.
         [62]  Gorbunov  K.  Yu.,  On  a  complexity  of the  formula  A V В  =>  C,  Theoretical  Computer 
           Science, v.  207 (1998), no.  2, 383-386.
         [63]  Halmos P. R.,  Measure  Theory, N.Y.: Van Nostrand,  1950. 292 pp.
         [64]  Hammer D., Romashchenko A., Shen A., Vereshchagin N., Inequalities for Shannon entropies 
           and Kolmogorov complexities, Journal of Computer and System Sciences, v. 60 (2000), 442- 
           464. Preliminary version, Proc.  12th IEEE Conference on Computational Complexity (CCC 
           1997),  13-23.
         [65]  Hammer D., Shen A., A strange application of Kolmogorov complexity, Theory of Computing 
           Systems, v. 31  (1998), no.  1,  1-4.
          494                          BIBLIOGRAPHY
           [66]  Hoeffding,  W.,  Probability inequalities  for sums of bounded  random variables,  Journal  of 
               the  American Statistical Association, v. 58, issue 301  (March 1963),  13-30.
           [67]  Hölzl R.,  Kräling T., Merkle W., Time-bounded Kolmogorov complexity and Solovay func­
               tions,  Proc.  34th  Symp.  M athematical  Foundations  of  Com puter  Science  (MFCS  2009), 
               Lecture Notes in Computer Science v. 5734, 392-402.
           [68]  Hutter,  M.  Universal  Artificial  Intelligence.  Sequential  Decisions  Based  on  Algorithmic 
               Probability. ISBN 3-540-22139-5. Springer, 2005. 278 pp.
           [69]  Impagliazzo R.,  Shaltiel  R.,  and Wigderson A.,  Extractors and pseudo-random generators 
               with optimal seed length, Proceedings of the 32nd ACM  Symp.  on the  Theory of Computing 
               (STOC 2000), 1-10.
           [70]  Kakutani S.,  On equivalence of infinite product measures,  Annals  of M athem atics,  Second 
               Series, v.  49  (1948), no.  1,  214-224.
           [71]  Kalinina  E.,  Prefix-free  and  prefix-correct  complexities  with  compound  conditions,  Proc. 
               5th  Computer Science  Symp.  in  Russia  (CSR 2010),  Lecture Notes in  Computer  Science 
               v.  6072,  259-265.
           [72]  Karpovich P., Monotone complexity of a pair, Proc.  5th  Com puter Science Symp.  in Russia 
               (CSR 2010), Lecture Notes in Computer Science v. 6072, 266-275.
           [73]  Kjos-Hanssen B., The probability distribution as a computational resource for randomness 
               testing,  Open  access  Journal  of Logic  and  Analysis,  v.2  (2010),  logicandanalysis.org/ 
               index.php/jla/article/view/78.
           [74]  Kjos-Hanssen B.,  Merkle W., Stephan F.,  Kolmogorov complexity and the recursion theo­
               rem,  Transactions  of the  Am erican  M athematical  Society,  v.  363  (2010),  5465-5480.  Pre­
               liminary version,  Proc.  23rd  Symp.  on  Theoretical  Aspects  of  Com puter  Science  (STACS 
               2006), Lecture Notes in Computer Science v. 3884,  149-161.
           [75]  Kleene  S.C.,  On  the  interpretation  of intuitionistic  number  theory,  Journal  of Symbolic 
               Logic, v.  10  (1945),  109-124.
           [76]  Kolmogoroff A., Zur Deutung der intuitionistishen Logik, M athematische Zeitschrift, Bd. 35 
               (1932), H.  1,  S.  58-65.
           [77]  Kolmogorov A. N., On tables of random numbers, Sankhyä,  The Indian Journal of Statistics, 
               Ser.  A,  v.  25  (1963), no.  4, p.  369-376. Reprinted in  Theoretical  Com puter Science, v.  207 
               (1998), no.  2, 387-395.
           [78]  Kolmogorov A. N., Three approaches to the quantitative definition of information, Problems 
               Inform.  Transmission, v.  1  (1965), no.  1,  1-7.
           [79]  Kolmogorov A. N., Logical basis for information theory and probability theory, IEEE Trans. 
               Inform.  Theory, v.  14 (1968), 662-664.
           [80]  Kolmogorov  A.N.,  Combinatorial  foundations  of information  theory  and  the  calculus  of 
              probabilities  [a  talk  at  the  International  Mathematical  Congress  (Nice,  1970)],  Russian 
               Mathematical Surveys, v. 38 (1983) no. 4, 29-40.
           [81]  Kolmogorov A.N., Fomin S.V.  Introductory Real Analysis,  Englewood Cliff:  Prentice-Hall, 
               1970. 403 pp.
           [82]  Kolmogorov A.N.,  Talk  at  the  Information Theory  Symposium  in Tallinn,  Estonia  (then 
              USSR), 1974.
           [83]  Kolmogorov A.N., Talk at the seminar at Moscow State University Mathematics Department 
               (Logic Division), 26 November 1981.  [The definition of (a, /3)-stochasticity was given in this 
              talk, and the question about the fraction of non-stochastic objects was asked.]
           [84]  Kolmogorov  A. N.  and  Uspensky  V. A.,  Algorithms  and  randomness,  SIAM   J.  Theory 
               Probab.  Appl. v. 32 (1987) p. 389-412 [with annoying translation errors31]. Without annoy­
              ing translation errors:  Prokhorov Yu. V. and Sazonov V. V., Eds., Proc.  1st  W orld  Congress 
               of the  Bernoulli  Society  (Tashkent  1986),  v.  1:  Probab.  Theory and Appl.,  VNU  Science 
              Press,  Utrecht  1987, 3-55.
           [85]  Kolmogorov i kibemetika [Kolmogorov and cybernetics, in Russian]. A collection of papers, 
              edited  by  D.A.  Pospelov  and  Ya.I.  Fet.  Novosibirsk:  IVM  MG  SO  RAN,  2001.  (Voprosy 
              istorii informatiki [The history of computer science], 2) The transcript of Kolmogorov’s talk
              31 Some of those errors drastically distort  meaning;  e.g.,  the Russian word  “perechislimyi”, 
          which  should  be  translated  as  “enumerable”  or  “recursively  enumerable”,  was  translated  as 
          “countable”.
                               BIBLIOGRAPHY                   495
            in the Institute of Philosophy of Russian Academy of Sciences (April 23,  1965) is on 118— 
            137.  It  is  also available  (October 2014)  as http://cshistory.nsu.ru/?int=VIEW&el=1832& 
            t emp1=INTERFACE.
         [86]  Kucera  A.,  Measure,  Il^-classes  and  complete  extensions  of PA,  In:  Ebbinghaus  H.-D., 
            Müller G.H. and Sacks G. E. (Eds.), Recursion  Theory  Week (Oberwolfach,  1984), Lecture 
            Notes in Mathematics, v.  1141  (1985), 245-259.
         [87]  Kucera A.,  Slaman T.,  Randomness and recursive enumerability,  SIAM  Journal  on  Com­
            puting, v. 31  (2001), no.  1,  199-211.
         [88]  Kuipers L., Niederreiter H.,  Uniform  distribution  of sequences, Wiley-Interscience,  1949.
         [89]  Kummer M., On the complexity of random strings, Proc.  13th Symp.  on Theoretical Aspects 
            of Computer Science (STACS 1996), Lecture Notes in Computer Science v.  1046, 25-36.
         [90]  van Lambalgen M.,  Random sequences, Ph. D. Thesis, University of Amsterdam,  1987.
         [91]  de  Leeuw  K.,  Moore  E.F.,  Shannon  С. E.,  and  Shapiro  N.,  Computability  by  probabilis­
            tic  machines.  In  Autom ata  Studies,  С.  E.  Shannon  and  J.  McCarthey  (Eds.),  Princeton 
            University Press, Princeton, New Jersey,  1956,  183-212.
         [92]  Levin L. A., Some  theorems on the  algorithmic  approach to probability theory and informa­
            tion  theory  (1971  dissertation directed  by A. N.  Kolmogorov;  turned down as required by 
            the Soviet authorities despite unanimously positive reviews). English translation published 
            later in Annals  of Pure  and Applied Logic, v.  162  (2010), p.  224-235. The original Russian 
            version of the thesis is available as http://www.cs.bu.edu/fac/lnd/dvi/diss/l-dis.pdf.
         [93]  Levin L.A.,  On the notion of a random sequence,  Soviet  Math.  Dokl.,  v.  14  (1973),  1413- 
            1416.
         [94]  Levin L. A., Laws of information conservation (nongrowth) and aspects of the foundation of 
            probability theory,  Problems  of Inform ation  Transmission, v.  10 (1974), 206-210.
         [95]  Levin L. A., Various measures of complexity for finite objects (axiomatic description), Soviet 
            Math.  Dokl., v.  17 (1976), 522-526.
         [96]  Levin L. A.,  On the principle of conservation of information in intuitionistic  mathematics, 
            Soviet Math.  Dokl., v.  17 (1976), no. 2, 601-605.
         [97]  Levin L.A., Uniform tests of randomness, Soviet Math.  Dokl., v.  17 (1976), no.  2, 337-340.
         [98]  Levin L.A.,  On a concrete method of assigning complexity measures,  Soviet  Math.  Dokl., 
            v.  18  (1977), no.  3,  727-731.
         [99]  Levin  L. A.,  A  concept of independence with application in various fields of mathematics, 
            MIT Technical Report, MIT/LCS/TR-235, 1980, 21 pp.
        [100]  Levin L. A., Randomness conservation inequalities:  information and independence in math­
            ematical theories, Information and  Control, v. 61  (1984), no.  1-2,  15-37.
        [101]  Levin L. A., Vyugin V. V.,  Invariant properties of informational bulks,  Proc.  6 th Symp.  on 
            Mathematical Foundations of Computer Science (MFCS 1977), Lecture Notes in Computer 
            Science v.  153, 359-364.
        [102]  Levin  L. A.,  Forbidden  information,  2002,  8  pp.,  arXiv:cs/0203029.  Preliminary  version, 
            Proc.  43th  IEEE Symp.  on Foundations of Computer Science (FOCS 2002), 761-768.
        [103]  Li M., Vitånyi P.,  An Introduction to  Kolmogorov  complexity and its  applications, 3rd ed., 
            Springer, 2008  (1 ed.,  1993; 2 ed.,  1997), xxiii+790 pp. ISBN 978-0-387-49820-1.
        [104]  Li S.R., Yeung R. W.,  Cai N., Linear network coding,  IEEE  Transactions  on  Information 
            Theory, v. 49 (2003), no. 2, 371-381.
        [105]  Loomis L. H., Whitney H., An inequality related to the isoperimetric inequality,  Bulletin of 
            the  American Mathematical Society, v. 55 (1949), 961-962.
        [106]  Loveland D. W., A new interpretation of von Mises’ concept of a random sequence, Z.  Math. 
            Logik und Grundlagen d.  Math., v.  12 (1966), 279-294.
        [107]  Loveland D. W., The Kleene hierarchy classification of recursively random sequences,  Trans. 
            Amer.  Math.  Soc., v.  125  (1966), 497-510.
        [108]  Loveland D. W., A Variant of the Kolmogorov concept of complexity, Information and Con­
            trol  (now  Inform ation and  C om putation), v.  15 (1969), 510-526.
        [109]  Loveland D. W., On minimal-program complexity measures, Proc.  1st ACM  Symp.  on  The­
            ory  of Computing (STOC 1969), 61-65.
        [110]  Lutz  J.,  Dimension  in  complexity  classes,  SIAM   Journal  on  Computing,  v.  32  (2003), 
            1236-1259. Preliminary version, Proc.  15th IEEE Conference on Computational Complexity 
            (CCC 2000), p.  158-169.
               496                                      BIBLIOGRAPHY
               [111]  Lutz J. H., The dimensions of individual strings and sequences, Information and  Computa­
                     tion, v.  187 (2003), no.  1, 49-79. Preliminary version, Gales and the constructive dimension 
                     of individual sequences, Proc.  27th  Colloquium on Automata,  Languages,  and Programming 
                     (ICALP 2000), Lecture Notes in Computer Science v.  1853, 902-913.
               [112]  Makarychev  K.,  Makarychev  Yu.,  Chain  independence  and  common  information.  IEEE 
                     Transactions on Information  Theory, v. 58 (2012), no. 8, 5279-5286. See also Conditionally 
                     independent random variables, arXiv: cs/0510029.
               [113]  Makarychev K., Makarychev Yu., Romashchenko A., Vereshchagin N., A new class of non- 
                     Shannon-type inequalities for entropies,  Communications in Information and Systems, v. 2 
                     (2002), no. 2,  147-166.
               [114]  Manin   Yu. I.   Vychislimoe    i   nevychillimoe    (Computable     and   non-computable), 
                     Moscow: Sovetskoe radio (Soviet radio),  1980,  128 pp.  (Russian)
               [115]  Martin-Löf P.,  The definition  of random sequences,  Information  and  Control  (now  Infor­
                     mation and  Computation), v. 9 (1966), 602-619.
               [116]  Martin-Löf  P.,  Algorithmen  und  zufällige  Folgen,  Vier  Vorträge  von  Per  Martin-Löf 
                     (Stockholm) gehalten am Mathematischen Institut der Universität Erlangen-Nürnberg, Als 
                     Manuskript vervielfältigt, Erlangen, 1966, 61 pp. See alsowww.probabilityandfinance.com/ 
                     misc/erlangen.pdf.
               [117]  Martin-Löf P.,  Complexity oscillations in infinite binary sequences,  Z.  W ahrscheinlichkeit­
                     stheorie venu.  Geb., v.  19 (1971), 225-230.
               [118]  Mayordomo E., A Kolmogorov complexity characterization of constructive Hausdorff dimen­
                     sion,  Inform ation Processing Letters, v. 84 (2002), no.  1,  1-3.
               [119]  Merkle W.  The Kolmogorov-Loveland stochastic sequences are not closed under selecting 
                     subsequences, Journal of Symbolic Logic, v. 68 (2003), 1362-1376. Preliminary version, Proc. 
                     29th Colloquium on Automata,  Languages,  and Programming (ICALP 2002), Lecture Notes 
                     in Computer Science v. 2380, 390-400.
               [120]  Merkle W., The complexity of stochastic sequences, Proc.  18th IEEE  Conference  on  Com­
                     putational  Complexity (CCC 2003), 230-235.
               [121]  Miller  J.,  Every  2-random  real  is  Kolmogorov  random,  Journal  of Symbolic  Logic,  v.  69 
                     (2004), no. 3, 907-913.
               [122]  Miller  J.,  Contrasting plain  and prefix-free  Kolmogorov complexity,  (preprint  from  2006), 
                    www.math.wise.edu/"jmiller/Notes/contrasting.pdf.
               [123]  Miller J., Yu L., On initial segment complexity and degrees of randomness,  Transactions  of 
                     the  A m erican  M athem atical Society, v.  360  (2008), no. 6, 3193-3210.
               [124]  Miller J., The К-Degrees, low for К  degrees, and weakly low for К  sets. Notre Dame Journal 
                     of Formal Logic, v. 50, no. 4 (2009), 381-391.
               [125]  Miller J., Two notes on subshifts, Proceedings of the Am erican M athematical Society, v. 140 
                     (2012), no.  5,  1617-1622. See also www.math.wise.edu/"jmiller/Papers/subshifts.pdf
               [126]  von  Mises  R.,  Grundlagen  der  Wahrscheinlichkeitsrechnung,  M athematische  Zeitschrift, 
                     Bd.  5  (1919),  S.  52-99.  Reprinted in  Selected  Papers  of Richard  von  M ises.  Volume  Two. 
                     Probability  and Statistics, General. American Mathematical Society,  1964. 57-106.
               [127]  von  Mises  R.,  W ahrscheinlichkeit,  Statistik  und  Wahrheit,  Wien:  Springer-Verlag,  1928, 
                     189 pp.
               [128]  von  Mises  R.,  On  the  foundations  of probability  and  statistics,  Annals  of  M athematical 
                     Statistics, v.  12 (1941), 191-205. Reprinted in Selected Papers of Richard von M ises.  Volume 
                     Two.  Probability and Statistics,  General. American Mathematical Society,  1964, 340-355.
               [129]  von Mises R., Doob J. L., Discussion of papers on probability theory, Annals of M athematical 
                     Statistics,  v.  12  (1941),  p.  215-217.  Reprinted  in  Selected  Papers  of Richard  von  M ises. 
                     Volume  Two.  Probability  and  Statistics,  General.  American  Mathematical  Society,  1964, 
                    356-359.
               [130]  Moser R.A.,  A constructive proof of the Lovasz Local Lemma,  Proc.  41st ACM  Symp.  on 
                     Theory of Computing (STOC 2009), 343-350. See also arXiv:0810.4812.
               [131]  Moser R.A., Tardos G.,  A constructive proof of the general Lovasz Local Lemma,  Journal 
                     of the  ACM, v.57 (2010), no. 2,  11.1-11.15. See also arXiv:0903.0544.
               [132]  Muchnik  An. A.,  On  the  basic  structures  of the  descriptive  theory  of algorithms,  Soviet 
                     Math.  D okl, v.  32 (1985), no.  3, 671-674.
               [133]  Muchnik An. A., Lower limits on frequencies in computable sequences and relativized a priori 
                    probability,  SIAM   Theory Probab.  Appl., v.  32  (1987), 513-514.
                                                      BIBLIOGRAPHY                                          497
              [134]  Muchnik  An. A.,  On  common  information,  Theoretical  Computer  Science,  v.  207,  no.  2 
                    (1998), 319-328.
              [135]  Muchnik An. A.  Conditional complexity and codes,  Theoretical  Computer Science,  v.  271 
                    (2002),  no.  1-2,  97-109.  Preliminary version,  Muchnik  A.,  Semenov A.,  Multi-conditional 
                    descriptions and codes in Kolmogorov complexity, Electronic  Colloquium on Computational 
                    Complexity, TR00-015 (2000).
              [136]  Muchnik  An. A.,  Mezhirov  I.,  Shen  A.,  Vereshchagin  N.K.,  Game  interpretation  of Kol­
                    mogorov complexity, 2010, arXiv:1003.4712.
              [137]  Muchnik  An. A.,  Positselsky  S.E.,  Kolmogorov  entropy  in  the  context  of  computability 
                    theory,  Theoretical  Com puter Science, v.  271  (2002), no.  1-2,  15-35.
              [138]  Muchnik An. A., Romashchenko A., Stability of properties of Kolmogorov complexity under 
                    relativization.  Problems  of Inform ation  Transmission,  v.  46,  no.  1  (2010),  38-61.  Prelimi­
                    nary version, Muchnik An. A., Romashchenko A., Random oracle does not help extract the 
                    mutual information, Proc.  33rd Symp.  on Mathematical Foundations  of Computer Science 
                    (MFCS 2008), Lecture Notes in Computer Science v. 5162, 527-538.
              [139]  Muchnik An. A., Semenov A. L., Uspensky V. A., Mathematical metaphysics of randomness, 
                    Theoretical  Com puter Science, v.  207, no. 2  (1998), 263-317.
              [140]  Muchnik An. A.,  Shen A.,  Ustinov M.,  Vereshchagin N.K.,  Vyugin M.,  Non-reducible de­
                    scriptions  for  conditional  Kolmogorov  complexity,  Theoretical  Com puter  Science,  v.  384 
                    (2007),  no.  1,  77-86.  Preliminary versions,  Muchnik  An. A.,  Shen  A.,  Vereshchagin  N. K., 
                    Vyugin  M.,  Non-reducible  descriptions  for  conditional  Kolmogorov complexity,  Proc.  3rd 
                    Conference on  Theory and Applications of Models of Computation (TAMC 2006), Lecture 
                    Notes in Computer Science v. 3959, 308-317 and Electronic  Colloquium  on  Computational 
                    Complexity TR04-054 (2004).
              [141]  Muchnik  An. A.,  Vereshchagin  N.K.,  Shannon  entropy vs.  Kolmogorov  complexity,  Proc. 
                    1st  Computer Science  Symp.  in  Russia  (CSR 2006),  Lecture Notes  in  Computer Science 
                    v.  3967, 281-291.
              [142]  Muchnik An. A.,  Vereshchagin  N.K.,  On joint conditional  complexity  (entropy),  Proc.  of 
                    the  Steklov  Institute  of M athem atics,  v.  274  (2011),  90-104.  Preliminary versions,  Much­
                    nik  An. A.,  Vereshchagin  N.K.,  Logical  operations  and  Kolmogorov  complexity.  II.  Proc. 
                    16th IEEE Conference on Computational Complexity (CCC 2001), 256-265 and Electronic 
                    Colloquium  on  Computational  Complexity TR01-089 (2001).
              [143]  Musatov  D.,  Improving  the  space-bounded  version  of  Muchnik’s  conditional  complexity 
                    theorem via “naive”  derandomization, Proc.  6 th  Computer Science Symp.  in Russia (CSR 
                    2011), Lecture Notes in Computer Science v. 6651, 64-76.
              [144]  Musatov D., Space-bounded Kolmogorov extractors, Proc.  7th  Computer Science Symp.  in 
                    Russia (CSR 2012), Lecture Notes in Computer Science v. 7353, 266-277.
              [145]  Musatov D.,  Romashchenko A.,  Shen A.,  Variations on Muchnik’s conditional complexity 
                    theorem,  Theory  Comput.  Syst.,  v.  49  (2011),  no.  2,  227-245.  Preliminary version,  Proc. 
                    4th  Computer Science  Symp.  in  Russia  (CSR 2009),  Lecture Notes in  Computer Science 
                    v.  5675,  250-262.
              [146]  Niederreiter H., A combinatorial problem for vector spaces over finite fields, Discrete M ath­
                    ematics, v.  96  (1991), no.  3, 221-228.
              [147]  Nies A.,  Com putability  and randomness, Oxford University Press, 2009, 420 pp. ISBN 978- 
                    0-19-923076-1
              [148]  Nies A., Stephan F., Terwijn S., Randomness, relativization and Turing degrees,  Journal of 
                    Symbolic Logic, v.  70 (2005), no.  2, 515-535.
              [149]  Novikov G.,  Relations  between randomness deficiencies, 2016, arXiv: 1608.08246.
              [150]  Rado, T., On non-computable functions, Bell System   Technical Journal, v. 41, issue 3, May 
                    1962, 877-884.
              [151]  Razenshteyn I., Common information revisited, 2012, arXiv: 1104.3207.
              [152]  Reimann   J.,    Computability   and   fractal   dimension,    PhD  thesis,     Ruprecht-
                    Karls   Universität  Heidelberg,   2004   (URN:  urn:nbn:de:bsz: 16-opus-55430),       see
                    www.ub.uni-heidelberg.de/archiv/5543.
              [153]  Romashchenko A., Pairs of words with nonmaterializable mutual information,  Problems  of 
                    Information  Transmission, v. 36 (2000), no.  1, 3-20.
              [154]  Romashchenko A.,  Extracting the mutual information for a triple of binary strings,  Proc. 
                    18th IEEE Conference on  Computational Complexity (CCC 2003), 221-235.
                 498                                            BIBLIOGRAPHY
                 [155]  Romashchenko  A.,  Rumyantsev  A.,  Shen  A.,  Zametki  po  teorii  kodirovaniya  (Notes  on 
                        coding theory), Moscow:  MCCME, 2011, 80 pp.  (Russian)
                 [156]  Romashchenko A.,  Shen A., Topological arguments for Kolmogorov complexity,  Theory  of 
                        Computing Systems, v. 56, 2015, issue 3, 513-526.
                 [157]  Romashchenko A., Shen A., Vereshchagin N., Combinatorial interpretation of Kolmogorov 
                       complexity,  Theoretical  Com puter  Science,  v.  271  (2002),  no.  1-2,  111-123.  Preliminary 
                       versions, Proc.  15th IEEE Conference on Computational Complexity (CCC 2000), 131-137, 
                       Electronic  Colloquium  on  Computational  Complexity TR00-026 (2000).
                 [158]  Rumyantsev A., Kolmogorov complexity, Lovasz Local Lemma and critical exponents, Proc. 
                       2nd  Computer Science  Symp.  in  Russia  (CSR 2007),  Lecture Notes in Computer Science 
                       v.  4649,  349-355. See also arXiv: 1009.4995.
                 [159]  Rumyantsev  A.,        Infinite    computable  version  of  Lovasz  Local  Lemma,                   2010, 
                       arXiv:1012.0557.
                 [160]  Rumyantsev  A.,  Ushakov  M.,  Forbidden  substrings,  Kolmogorov  complexity  and  almost 
                       periodic sequences, Proc.  23rd Symp.  on  Theoretical Aspects  of Com puter Science (STACS 
                       2006), Lecture Notes in Computer Science v. 3884, 396-407. See also arXiv: 1009.4455.
                 [161]  Schmidt  W.,  On  normal  numbers,  Pacific  Journal  of  M athematics,  v.  10  (1960),  no.  2, 
                       661-672.
                 [162]  Schnorr С. P., Eine Bemerkung zum Begriff der zufälligen Folge, Zeitschrift für  W ahrschein­
                       lichkeitstheorie  und  Verw.  Gebiete, v.  14 (1969), 27-35.
                 [163]  Schnorr C.P., Uber die Definition effektiver Zufallstests, Teil I,  Zeitschrift für  W ahrschein­
                       lichkeitstheorie und  Verw.  Gebiete, v.  15  (1970), 297-312.
                [164]  Schnorr С. P., Uber die Definition effektiver Zufallstests, Teil II, Zeitschrift für  W ahrschein­
                       lichkeitstheorie  und  Verw.  Gebiete, v.  15  (1970), 313-328.
                [165]  Schnorr C.P., Klassifikation der Zufallsgesetze nach Komplexität und Ordnung,  Zeitschrift 
                       für  Wahrscheinlichkeitstheorie und  Verw.  Gebiete, v.  16  (1970),  1-21.
                [166]  Schnorr  C.P.,  Zufälligkeit  und  W ahrscheinlichkeit.  Eine  algorithmische  Begründung  der 
                        Wahrscheinlichkeitstheorie,  Lecture  Notes  in  Mathematics,  v.  218,  iv+212  pp.,  Springer, 
                       1971.
                [167]  Schnorr  С. P.,  A  unified  approach  to  the  definition  of  random  sequences,  M athematical 
                       Systems  Theory (now  Theory of Computing Systems), v. 5 (1971), no. 3, 246-258.
                [168]  Schnorr С. P, Optimal Gödel numberings, Proc.  IFIP congress  71  (1971), v.  1, 56-58.
                [169]  Schnorr  С. P.,  Process  complexity  and  effective  random  tests,  Journal  of  Com puter  and 
                       System Sciences, v. 7 (1973), 376-388. Preliminary version, Proc. 4th ACM  Symp.  on Theory 
                       of Computing (STOC 1972), 168-176.
                [170]  Schnorr С. P., Optimal enumerations and optimal Gödel numberings, Mathematical Systems 
                       Theory (now  Theory of Computing System s), v. 8 (1975), no. 2,  182-191.
                [171]  Shafer  G.,  Shen  A.,  Vereshchagin  N.K.,  Vovk  V.,  Test  martingales,  Bayes  factors,  and 
                       p-values,  Statistical  Science v.  26 (2011), no.  1, 84-101. see also arXiv:0912.4269.
                [172]  Shafer G., Vovk V.,  Probability and finance:  it's  only  a game!  New York:  Wiley,  2001.
                [173]  Shen A.,  Aksiomaticheskoe opisanie ponyatiya entropii konechnogo objekta (An axiomatic 
                       description of the notion of entropy of finite objects), Logic and foundations of mathem atics. 
                       Abstracts of the 8 th All-union conference  “Logic and methodology of science”, Palanga, Sept 
                       26-28, 1982, Vilnius,  1982,  104-105.  (Russian)
                [174]  Shen  A.,  The concept  of (a,/3)-stochasticity in  the  Kolmogorov sense,  and  its  properties. 
                       Soviet Math.  Dokl., v. 28, no.  1,  1983, 295-299
                [175]  Shen  A.,  К  logicheskim  osnovam  primenenia teorii  veroyatnostei  (On  logical  foundations 
                       of applications  of probability  theory),  Workshop  on  Sem iotic  aspects  of form alization  of 
                       intellectual  activities,  Telavi,  Oct.  29-N ov.  6 ,  1983,  144-146.  (Russian)
                [176]  Shen  A.,  Algorithmic variants  of the  notion of entropy,  Soviet  Math.  Dokl.,  v.  29  (1984), 
                       no.  3, 569-573.
                [177]  Shen A., On relations between different algorithmic definitions of randomness, Soviet Math. 
                       Dokl., v.  38  (1989), no.  2,  316-319.
                [178]  Shen  A.,  Discussion  on  Kolmogorov  Complexity  and  Statistical  Analysis,  The  Com puter 
                       Journal, v.  42, no. 4,  1999, 340-342.
                [179]  Shen A.,  Multisource algorithmic information theory,  Proc.  3rd  Conference  on  Theory  and 
                       Applications  of Models  of Computation (TAMC 2006), Lecture Notes in Computer Science 
                       v.  3959, 327-338.
                               BIBLIOGRAPHY                   499
        [180]  Shen A., Algorithmic information theory and foundations of probability, Proc.  3rd  Workshop 
            on  Reachability Problems  (2009), Lecture Notes in Computer Science v. 5797, 26-34.
        [181]  Shen A., Around Kolmogorov complexity:  basic notions and results.  Measures of Complex­
            ity.  Festschrift for  Alexey  Chervonenkis,  edited  by V.  Vovk,  H.  Papadoupoulos,  A.  Gam- 
            merman,  Springer,  2015,  75-116.  See  also  arXiv: 1504.04955  (2015),  a  revised  version  of 
            Algorithmic information theory and Kolmogorov complexity, lecture notes, http://www.it. 
            uu.se/research/publications/reports/2000-034.
        [182]  Shen A.  Autom atic  Kolmogorov  complexity  and norm ality  revisited, 2017, arXiv.org/pdf/ 
            170109060.pdf.
        [183]  Shen, A. and Vereshchagin, N., Logical operations and Kolmogorov complexity,  Theoretical 
            Computer Science,  v.271  (2002)  pp.125-129.  Preliminary  version:  Electronic  Colloquium 
            Computational Complexity, TR01-088(2001).
        [184]  Shen  A.  and  Vereshchagin  NK.  Computable  functions,  American  Mathematical  Society, 
            Student Mathematical Library, vol.  19, 2003.
        [185]  Sipser M.,  Introduction to  the  theory  of computation, PWS Publishing,  1996.
        [186]  Slepian D., Wolf J. K., Noiseless coding of correlated information sources, IEEE Transactions 
            on  Information  Theory, v. IT-19 (1973), no. 4, 471-480.
        [187]  Solomonoff R. J.,  A  formal  theory  of inductive  inference,  part  1,  part  2,  Inform ation  and 
            Control (now Information  and  Computation), v. 7 (1964),  1-22, 224-254.
        [188]  Solovay R., Draft of a paper  (or series  of papers)  on  Chaitin’s work, done for the most part 
            during Sept.-Dee.  1974, unpublished (but available to many people in the field).
        [189]  Solovay R. M., On Random R.E. Sets. In:  A.I. Arruda, N.C.A. da Costa, R. Chaqui (Eds.), 
            Non-classical  logics,  model  theory  and  computability,  North-Holland,  Amsterdam,  1977, 
            283-307.
        [190]  Tadaki K., A generalization of Chaitin’s halting probability S7 and halting self-similar sets, 
            Hokkaido mathematical journal, v. 31 (2002), no. 1, 219-253. See also arXiv :nlin/0212001.
        [191]  Takahashi H.,  On a definition of random sequences with respect to conditional probability, 
            Information  and  Computation, v. 206 (2008), no.  12,  1375-1382.
        [192]  Takahashi H., Algorithmic randomness and monotone complexity on product space,  Infor­
            mation and Computation, v. 209 (2011), no. 2, 183-197, dx.doi.org/10.1016/j.ic.2010.10.003. 
            See also arXiv.org:0910.5076.
        [193]  Uspensky  V.,  Semenov  A.,  Algorithms:  main  ideas  and  applications,  Kluwer  Academic 
            Publishers,  1993, 269 pp.
        [194]  Uspensky V. A., Semenov A. L., Shen’ A. Kh., Can an individual sequence of zeros and ones 
            be random?  Russian Math.  Surveys, v. 45 (1990), no.  1,  121-189.
        [195]  Uspensky V. A.,  Shen A.,  Relations between varieties of Kolmogorov complexities,  M athe­
            matical Systems  Theory (now  Theory of Computing System s), v. 29 (1996), no. 3, 271-292.
        [196]  Vereshchagin N.K. Kolmogorov complexity conditional to large integers.  Theoretical  Com­
            puter Science, v. 271 (2002), no. 1-2, 59-67. Preliminary version, Electronic  Colloquium on 
            Computational Complexity TR01-086 (2001).
        [197]  Vereshchagin N., Kolmogorov complexity of enumerating finite sets, Information Processing 
            Letters, v.  103 (2007), 34-39. Preliminary version, Electronic  Colloquium on  Computational 
            Complexity TR04-030 (2004).
        [198]  Vereshchagin  N. K.,  Kolmogorov  complexity  and  games,  Bulletin  of  the  EATCS,  v.  94 
            (2008), 43-75.
        [199]  Vereshchagin N., Minimal sufficient statistic revisited, Proc.  5th  Conference on  Computabil­
            ity  in  Europe  (CiE 2009), Lecture Notes in Computer Science v. 5635, 478-487.
        [200]  Vereshchagin  N.K.,  Shen  A.,  Lektsii  po  matematicheskoi  logike  i  teorii  algorit- 
            mov,  Chast’  2,  Yazyki  i  ischisleniya  (Lectures  in  m athem atical  logic  and  computabil­
            ity  theory,  P art  2,  Languages  and  Calculi),  3rd  edition,  Moscow:  MCCME,  2008, 
           ftp :/ / ftp .mccme.ru/users/shen/logic/f irstord.
        [201]  Vereshchagin  N.,  Shen  A.,  Algorithmic  statistics  revisited,  M easures  of  Complexity. 
            Festschrift for Alexey Chervonenkis, edited by V. Vovk, H. Papadoupoulos, A. Gammerman, 
           Springer, 2015, 235-252. See also arXiv: 1504.04950 (2015)
        [202]  Vereshchagin N., Shen A., Algorithmic statistics: forty years later, arXiv: 1607.08077 (2016)
        [203]  Vereshchagin N. K, and Vitånyi P. M. B, Kolmogorov’s structure functions with an applica­
           tion to the foundations of model selection, IEEE Transactions on Information  Theory, v. 50
          500                          BIBLIOGRAPHY
               (2004), no.  12, 3265-3290. Preliminary version, Proc.  43th IEEE Symp.  on Foundations  of 
               Computer Science (FOCS 2002), 751-760.
          [204]  Vereshchagin  N. K,  and  Vitånyi  P. M. B,  Rate distortion  and  denoising of individual  data 
               using  Kolmogorov complexity,  IEEE  Transactions  on  Information  Theory,  v.  56  (2010), 
               no.  7, 3438-3454, see also arXiv:cs/0411014 (2004). See also arXiv:cs/0411014 (2004).
          [205]  Vereshchagin  N.  and  Vyugin  M.,  Independent  minimum length programs to translate be­
               tween  given  strings,  Theoretical  Com puter  Science,  v.  271  (2002),  131-143.  Preliminary 
               version,  Electronic  Colloquium  on  Computational  Complexity TR00-035 (2000).
          [206]  Ville J.,  Etude  critique  de  la  notion  de  collectif, Gauthier-Villars,  1939.  (Monographies des 
               probabilités. Calcul des probabilités et ses applications. Publiée sous la direction de M. Émile 
               Borel.  Fascicule III.)
          [207]  Vovk V.G.,  On a randomness criterion,  Soviet  M athematics  Doklady,  v.  35  (1987),  no.  3, 
               656—660. See also:  http://www.vovk.net/criterion.pdf
          [208]  Vovk V.G., The law of the iterated logarithm for random Kolmogorov, or chaotic, sequences, 
               Theory  of Probability  and its  Applications, v.  32 (1987), no. 3, 413-425.
          [209]  Vovk V., V ’yiigin V.V., On the empirical validity of Bayesian method, Journal of the Royal 
               Statistical Society, B, v.  55  (1993), 253-266.
          [210]  V’yugin V.V., Nonstochastic objects,  Problems  of Information  Transmission, v.  21,  no.  2, 
               1985, 77-83.
          [211]  V.V. V’yiigin, On the defect of randomness of a finite object with respect to measures with 
              given complexity bounds,  SIAM   Theory  of Probability and Its  Applications, v.  32,  issue 3, 
               1987, 508-512.
          [212]  V.V. V’yugin, Algorithmic complexity and stochastic properties of finite binary sequences, 
               The  Computer Journal, v. 42, no. 4, 1999, 294-317.
          [213]  V’yugin  V.V.,  Algorithmic  entropy  (complexity)  of finite  objects  and  its  applications  to 
              defining randomness and amount of information,  Selecta  M athematica form erly  Sovietica, 
              v.  13(4),  1994, 357-389.
          [214]  V’yugin  V.V.,  Ergodic  theorems  for  individual  random  sequences,  Theoretical  Com puter 
               Science, v.  207 (1998), no. 2, 343-361.
          [215]  V’yugin V.V.,  Non-stochastic infinite and finite sequences,  Theoretical  Com puter Science, 
              v.  207 (1998), no.  2,  363-382.
          [216]  Wald A., Sur la notion de collectif dans le calcul des probabilités (On the notion of collective 
              in  probability  theory),  présentée  par  M.  Émile  Borel.  Com ptes  rendus,  v.  202,  180-183 
               (séance du 20 janvier 1936).
          [217]  Wald A., Die Wiederspruchsfreiheit des Kollektivbegriffes der Wahrscheinlichkeitsrechnung, 
              Ergebnisse  eines  m atem atischen Kolloquiums, v. 8  (1937), 38-72. Reprinted in Menger K., 
              Ergebnisse  eines  M athematischen Kolloquiums, Springer, Wien, New York,  1998.
          [218]  Wall D.D.,  Normal Numbers, PhD thesis, University of California, Berkeley CA,  1949.
          [219]  Wigderson,  A.,  Randomness  extractors—applications  and  constructions,  Proc.  29th 
               Conference  on  Foundations  of  Software  Technology  and  Theoretical  Computer  Sci­
               ence  (FSTTCS 2009), DOI 10.4230/LIPIcs.FSTTCS.2009.2340, drops.dagstuhl.de/opus/ 
              volltexte/2009/2340/.
          [220]  Yeung, R. W., A  First course in inform ation theory, Kluwer Academic / Plenum Publishers, 
              2002.
          [221]  Zaslavsky,  I.  D.;  Tseitin,  G.  S.  O  singulyarnykh  pokrytiyakh  i svyazannykh s nimi svoist- 
              vah  konstruktivnykh  funktsii  (Singular  coverings  and  properties  of constructive  functions 
              connected with them),  Trudy Mat.  Inst.  Steklov, v. 67 (1962) 458-502.  (Russian)
          [222]  Zhang  Z.,  Yeung  R.W.,  A  non-Shannon-type  conditional  information  inequality,  IEEE 
               Transactions  on Information  Theory, v. 43 (1997), 1982-1986.
          [223]  Zhang Z., Yeung R.W., On characterization of entropy function via information inequalities, 
              IEEE Transactions  on Information  Theory, v. 44 (1998), 1440-1450.
          [224]  Zimand M., Guest Column:  Possibilities and Impossibilities in Kolmogorov complexity ex­
              traction, SIG A CT News, December 2010.
          [225]  Zvonkin  AK.,  Levin  L. A.,  The  complexity  of finite  objects  and  the  development  of the 
              concepts  of information  and  randomness  by  means  of the  theory  of algorithms,  Russian 
              Math.  Surveys, v. 25 (1970), no. 6, 83-124.
                                                                  Name Index
                   Ahlswede, Rudolf F., 347, 348, 384                                  Day, Adam, 133, 134 
                   Andreev, Mikhail, xiv                                               Dektyarev, Mikhail, xiv 
                   Arslanov, Marat, 28                                                 Doob, Joseph Leo, 271, 274, 282 
                   Arzumanyan, Vitaly, xiv                                             Downey, Rodney Graham, xi, 29, 165, 180 
                   Asarin, Eugene, xiv                                                 Durand, Bruno, xiv, 43, 296 
                   Azuma, Kazuoki, 302, 304                                            Einstein, Albert, 460 
                   Baire, René-Louis,  178                                             Ershov, Yuri,  197, 201 
                   Bauwens, Bruno, xiv, 41, 112, 188                                   Euclid of Alexandria, 233 
                   Becher, Veronica, 266                                               Fano, Robert Mario, 224 
                   Bennett, Charles H., 377                                            Figueira, Santiago, 266 
                   Bernoulli, Jacob,  11, 55, 66, 92,  176, 238,                       Fomin, Sergey, 54 
                         245, 264, 276, 279, 291, 297, 390, 480                        Ford, Lester Randolf, Jr., 378, 385 
                   Berry, G.G., 9                                                      Fortnow, Lance Jeremy, 40, 252, 377, 444 
                   Bertrand, Joseph Louis François, 359, 457,                          Frankl, Peter, 225 
                         458                                                           Fulkerson, Delbert Ray, 378, 385 
                   Besicovitch, Abram, 259                                             Gâcs, Péter, xiv, 41, 65, 74,  133,  134,  151, 
                   Bienvenu, Laurent, xiv, 74, 84,  148,  158,                               155,  180,  188,  284,  365, 377, 428, 438, 
                         164,  165,  179,  188, 259, 283                                     450, 452 
                   Blum, Manuel, 460                                                   Gibbs, Josiah Willard, 215 
                   Borel, Félix Edouard Justin Emile, 53, 57,                          Gödel, Kurt,  13, 35, 202, 204, 402 
                         162, 456, 457                                                 Goldreich, Oded, 460 
                   Buhrman, Harry Matthijs, xiv, 40, 377                               Graham, Ronald Lewis, 225 
                   Cai,  Ning,  384                                                    Hall, Philipp,  378 
                   Calude, Cristian Sorin, xi, xiv,  158                               Halmos, Paul Richard, 54 
                   Cantelli, Francesco Paolo, 57,  162                                 Hammer, Daniel, 12, 313, 338 
                   Cantor, Georg Ferdinand Ludwig Philipp,                             Hamming, Richard Wesley, 21, 439 
                         53,  68,  72,  73,  135,  150,  172,  175,  189,              Hardy, Godfrey Harold, 240 
                         258, 259, 271, 300                                            Hausdorff, Felix,  172-176, 240, 259, 284, 
                   Cauchy, Augustin-Louis, 257, 419                                          286
                   Chaitin, Gregory, xi,  13, 22, 42, 84, 94,  106,                    Hertling, Peter,  158 
                         157,  172,  283, 452, 470                                     Hirschfeldt, Dennis R., xi, xiv, 29,  180 
                   Champernowne, David Gawen, 266                                      Hoeffding, Wassily, 302, 304 
                   Chan, Terence, 323                                                  Hölzl, Rupert, xiv,  165,  188 
                   Chelnokov, Georgy, xiv                                              Hoyrup, Mathieu, xiv, 74,  182 
                                                                                       Huffman, David A., 216 
                   Chernov, Alexey, xiv, 200, 362, 413                                 Hutter, Marcus,  10, 200 
                   Chung, Fan R. K., 225                                               Impagliazzo, Russel, 444 
                   Church, Alonzo, 264, 269, 280, 283, 287,                            Ingleton, Aubrey William, 337, 339, 
                         292, 295, 304, 305, 487                                             341-344, 362 
                   Cormen, Thomas H., 385                                              Jordan, Marie Ennemond Camill, 70 
                   Cournot, Antoine Augustin, 456, 463                                 Kakutani, Shizuo, 297, 299 
                   Daley, Robert P., 287, 289, 292, 293, 296,                          Kalinina, Elena, xiv, 41, 208 
                         304, 305, 308, 309, 311                                       Karpovich, Pavel, xiv,  198 
                   Dawid, Alexander Philip, 274                                        Kepler, Johannes,  10
                                                                               501
                                                        NAME INDEX
               502
               Khinchin, Alexander, 240                             Mises, Richard von,  12, 67, 261, 262, 264, 
               Khodyrev, Alexander, xiv,  173                           269,  280,  283,  287,  291,  292,  295, 304, 
               Khoussainov, Bakhadyr,  158                              305, 446, 458, 468, 487 
               Kjos-Hanssen, Bj0rn, 28, 65                          Moore, Edward Forrest, 209 
               Kleene, Stephen Cole, 404                            Moser, Robin A., 252
              Kolmogorov Andrei, xi, xiii,  1, 3, 34, 37,           Muchnik, Andrei, v, xiv,  101, 184, 250, 293, 
                   54,  145,  267,  271,  291,  297,  313, 404,         353, 357, 358, 369, 373, 379, 380, 389, 
                   467-469                                              413, 431, 489 
              König, Julius, 243                                    Musatov, Daniil, xiv, 377 
              Körner, Janos, 347, 348, 365                          Newman, Ilan, 40 
              Kouckÿ, Michal, xiv                                   Niederreiter, Harald,  198 
              Kraft, Leon G., 94, 214, 217, 283                     Nies, André, xi, xiv, 29,  157,  180 
              Kräling, Thorsten,  165                              Novikov, Gleb, xiv, 74 
              Kripke, Saul Aaron, 412                               Ockham (Occam), William of, 10, 452 
              Kucera, Antonin,  158,  164,  180                    Peirce,  Charles Sanders, 405 
              Kuipers, Lauwerens, 265                              Picchi, Rafael, 266 
              Kullback, Solomon, 215, 276                          Podolskii, Vladimir, xiv 
              Kumok, Akim, xiv                                     Porter, Christopher R., xiv,  188 
              Kurtz, Stewart A., 70, 270, 284, 309                 Positselsky, Semen, xiv,  101 
              Lambalgen, Michiel van,  185, 293, 297, 488          Post, Emil Leon, 22 
              Laplante, Sophie, 377                                Pritykin,  Yuri, xiv 
              Lee, Troy, 444                                       Ptolemaios, Klaudios,  10 
              Leeuw, Karel de, 209                                 Radö, Tibor, 25 
              Leibler, Richard, 215, 276                           Raskin, Mikhail, xiv 
              Leiserson, Charles E., 385                           Razenshteyn, Ilya, xiv, 360, 361 
              Levin, Leonid, xi, xiii, xiv, 37, 65, 74, 84,        Reimann, Jan, xiv,  173 
                   106,  112,  125,  144,  158,  160,  169,  173,  Rivest, Ronald Linn, 385 
                   176,  179, 229, 230, 279, 283, 300, 465,        Rojas, Cristobal, 74
                   488                                             Romashchenko, Andrei, xiv, 35, 313, 314, 
              Li,  Ming, xi, xiii,  15, 236, 377                        338, 342, 346, 362, 377 
              Li,  Shuo-Yen Robert, 384                            Rooij, Steven de, 274 
              Littlewood, John Edensor, 240                        Rumyantsev, Andrei, xiv,  149, 242, 244,
              Loomis, Lynn Harold, 225, 255                             248
              Lovåsz, Laszlo, 244-246                              Rute, Jason, xiv 
              Loveland, Donald, 42, 194, 267, 268, 291,            Sablik,  Mathieu, 259 
                   296,  297, 305, 307, 487                        Salnikov, Sergey, xiv 
              Lutz, Jack H.,  174, 285                             Savchik, Alexey, xiv 
              Makarychev, Konstantin, xiv, 244, 343, 365           Savin, Arseny, xiv 
              Makarychev, Yuri, xiv, 343, 365                      Schmidhuber, Jürgen, 200 
              Makhlin, Anton, xiv                                  Schmidt, Wolfgang M., 265 
              Manin, Yuri, 21                                      Schnorr, Claus-Peter, xi, 69,  125,  144,  160, 
              Markaryan, Nikita, 462                                    169,  173, 202, 229, 230,  266,  278, 279, 
              Martin-Löf, Per, xi, 9, 44, 53, 54, 60, 65,               282,  283, 300, 465, 474, 488 
                   71,  125,  146,  157,  180, 229,  239,  264,    Schwarz, Hermann Amandus, 257, 419 
                   269,  292, 297, 430, 475                        Scott, Dana,  197, 201 
              Mayordomo, Elvira, 174                               Semenov, Alexey, xiii, 250, 489 
              McMillan, Brockway, 217, 222                         Shafer,  Glenn, 84,  148, 274, 277 
              Medvedev, Yuri, 413                                  Shafer, Paul,  188 
                                                                   Shaltiel,  Ronen, 444 
              Merkle, Wolfgang, xiv,  28, 165, 281, 282,           Shannon, Claude Elwood, xi, 7, 56, 209,
                   287, 291, 308                                        213, 214, 225, 226, 229, 231, 313, 323, 
              Meyer, Albert Ronald da Silva, 42                         343, 369, 384 
              Mezhirov, Ilya, xiv, 373                             Shapiro, Norman Z., 209 
              Micali, Silvio, 460                                  Shearer, James B., 225 
              Miller, Joseph S., xiv,  101,  102,  151,  154,      Simpson, Steven, xiv 
                   157,  243                                       Sipser,  Michael, xiv, 236 
              Milovanov Alexey, xiv                                Skvortsov, Dmitry Pavlovich, 413 
              Minasyan, Alexander, xiv                             Skvortsova,  Elena Zelikovna, 413
                            NAME INDEX                503
       Slaman, Theodore A.,  158,  164 
       Slepian, David, 369 
       Solomonoff, Ray, xi, xii, 3, 34, 470 
       Solovay, Robert Martin, 64,  102,  112,  113, 
          158, 208 
       Soprunov, Sergey, xiv 
       Stein,  Clifford,  385 
       Steinhaus, Hugo, 240 
       Stephan, Prank, 28,  157 
       Tadaki, Kohtaro,  173 
       Takahashi, Hayato,  188 
       Tarasov, Sergey, xiv 
       Tardos, Gabor, 252 
       Terwijn, Sebastiaan A.,  157 
       Tromp, John T., 428, 438, 450, 452 
       Tseitin, Grigory, 64
       Turing, Alan Mathison,  12, 25, 27, 44, 85, 
          172,  177,  180,  189,  234,  235,  266 
       Ushakov, Maxim, xiv, 242 
       Ustinov, Michael, xiv, 389 
       Ville,  Jean-André, 267, 268, 271, 273, 278, 
          310, 489
       Vitånyi, Paul Michael Bela, xi, xiii, xiv,  15, 
          236, 377, 428, 438, 450, 452 
       Vorovyov, Sergey, xiv 
       Vovk, Vladimir, xiv, 239, 274, 277, 297 
       Vyalyi, Mikhail, xiv 
       Vyugin, Mikhail, 389 
       Vyugin, Vladimir, xiv,  180, 430, 450, 488 
       Wald, Abraham, 262, 268, 287 
       Wall, Donald Dines, 265 
       Wang, Yongge, 158 
       Whitney, Hassler, 225, 255 
       Wigderson, Avi,  121, 444 
       Wolf, Jack K., 369 
       Yao, Andrew Chi-Chih, 460 
       Yeung, Raymond W., 323, 343, 384 
       Yu, Liang,  151,  154 
       Zaslavsky, Igor, 64 
       Zhang, Zhen, 343 
       Zimand, Marius,  121 
       Zurek, Wojciech H., 377 
       Zvonkin, Alexander, xiii,  176, 179
       Ville,  Jean-André , 269
                                                          Subject Index
              1-  random sequence,  157                                         m(i), 81 
              2-  random sequence, 157                                          m(x\z), 103
              B(n), 22, 25
              BB(n), 24, 25                                                     a priori complexity,  122,  133,  148 
              BP(n), 169                                                        a priori probability, 81, 96,  122 
              C(x),  15                                                            conditional,  103 
              C(x,y), 31                                                           continuous,  122 
              CA(x), 104                                                           discrete,  122,  157 
              C 23                                                              a priori randomness deficiency,  177 
              Я(0, 217                                                          absolutely normal real, 265 
              H(x),  15                                                         admissible selection rule, 469 
              I(x-.y), 45, 352                                                  Ahlswede-Körner theorem, 347 
              I(x:y:z), 50                                                      algorithm
              K(x),  15                                                            probabilistic,  75 
              K(x), 82                                                          algorithmic entropy, 472 
              K(x\z),  103                                                      almost uniform set, 324, 326 
              KA(x), 104                                                        amount of information, 5 
              KA (x),  122                                                         algorithmic approach, 313 
              KM(x), 130                                                           combinatorial approach, 313 
              KR(x), 193                                                           probabilistic approach, 313 
              C(x\y), 34                                                        amount of information in x about y, 45 
              Я-low set, 203                                                    ample excess lemma,  151 
              C(x\\A), 204                                                      arithmetic coding,  146 
              A, empty string, 2                                                Arslanov theorem, 28 
              П,  11,  157,  158,  160,  171                                    artificial independence, 346 
              Г2  number, 452                                                   atomless measure,  176
              -,  1
              a-gales, 285                                                      balanced sequence, 262 
              а-null set,  172,  173                                            ball,  466
              F, the space of partial functions, 201                            basic function, 72 
              O',  27                                                           basic inequality, 47, 223, 318, 338 
              4i,  158                                                          Bernoulli distribution, 480 
              4c,  160                                                          Bernoulli measure,  11, 55, 66 
              cr-additivity,  53                                                Bertrand postulate, 359 
              <j-algebra, 53                                                    Besicovitch distance, 259 
              a(x),  122                                                        binary words, 466 
              bin(n), 2                                                         bipartite graph, 357, 371 
              c-equivalence, 351                                                black box, 425 
              dE, 73                                                            blind randomness, 65 
              dp, 73                                                            Borel set,  53
              /o-space, 201                                                     Borel-Cantelli lemma, 57 
              /o-spaces,  197                                                   busy beaver, function,  25
              /(x), the length of the string x, 2 
              m-reduction, 28                                                   Cantor set,  172
                                                                        505
                506                                      SUBJECT INDEX
                Cantor space, 53,  172, 258                             complexity of a problem, 401 
                Cauchy-Schwarz inequality, 257                          complexity vector, 326, 333 
                Chaitin number Г2,  157,  158,  160,  171              computable function,  1,  15, 469 
               chaotic sequence, 473, 475, 484, 488                    computable mapping, 91,  193, 201 
               chaoticness, 467                                        computable martingale, 278 
               characteristic sequence, 67,  179                       computable measure, 60, 61 
               characteristic sequence of the set, 203                 computable number, 60 
               Chebyshev inequality,  101, 445                         computable sequence, 42 
               Church admissible selection rule, 469                   computable series,  159 
               Church stochastic sequence, 468                         computable set, 21 
               CNF, 249                                                condition, 34
               со-enumerable property, 209                             conditional probability,  275 
               code, 213                                               conditional complexity, 7, 200, 402 
                  average length, 214                                     prefix,  102 
                  injective,  213                                         relativized, 204 
                  prefix,  213                                         conditional decompressor, 34 
                  prefix-free, 213                                     conditional entropy, 219 
                  uniquely decodable, 213                              conditional independence, 51, 223, 342, 362 
               codes for two conditions, 379                           conditional probability, 219 
               codewords, 213                                          conjunction, 401
               combinatorial amount of information, 317                constructive support of measure, 485 
               combinatorial interpretation, 332                       continuity of measure, 54 
               combinatorial interpretation of inequalities,           continuous mapping,  193 
                    328                                                   E —» E,  127 
               common information, 351, 388                            coset, 323
                 combinatorial meaning, 357                            Cournot principle, 456 
                 relativized,  104                                     criterion of Martin-Löf randomness,  125, 
               compatible strings,  197, 474                                146,  164 
               completeness deficiency,  160                              plain complexity,  151 
               complex subsequences, 251                                  Solovay,  162 
               complexity,  198, 471                                   critical implication, 412 
                 a priori,  122                                        cut, 383
                 axiomatic definition,  19                             decidable set, 21 
                 combinatorial interpretation, 328                     decompressor, 1,  15,  129,  193 
                 conditional, 7, 34,  200, 402                           for functions, 201 
                    relativized, 204                                     optimal, 83,  130,  195, 200 
                 decision,  193,  194                                    prefix-free, 91, 98 
                    conditional, 200                                     prefix-stable, 91,  131 
                 Kolmogorov, 2, 4                                      description,  15, 34, 471 
                 monotone, 9,  130,  132, 227                            self-delimiting, 82 
                    conditional, 200                                     shortest, 352 
                 of a pair, 352                                        description language, 471 
                 of finite object,  16                                 description mode,  1,  15,  193,  198 
                 of functions, 201                                       optimal, 3,  15 
                 of large numbers, 22                                  discrete a priori probability,  157 
                 of natural numbers,  16                               disjunction, 401 
                 of pairs, 31, 48, 97, 318                             distance
                    prefix, 85,  105                                     Besicovitch, 259 
                 of texts,  145                                        distance between sequences, 53 
                 of triples,  32, 48, 318                              distance Kullback-Leibler, 215 
                 plain,  15                                            distribution 
                 prefix, 83,  96                                         marginal,  187 
                    conditional,  102, 200                             Doob inequality,  271 
                    of pairs, 97                                       Doob theorem, 274
                    of triples,  106,  109 
                 relativized,  27, 203, 361                            effective Hausdorff dimension,  174 
                 total, 35, 375, 437                                   effectively null set, 58, 59,  147 
                 with respect to description mode,  15                 effectively open set, 70,  178
                                                     SUBJECT INDEX                                       507
              empty string, 2, 466                                group, 323, 341 
              encoding, 213                                       group action, 323
              entropy, 314                                        Hall theorem, 378 
                algorithmic, 472                                  halting probability,  76 
                conditional, 217, 219                             halting problem, 24, 27 
                monotone (algorithmic), 474                       Hamming ball, 439 
                of a pair,  217,  218                               cardinality, 441 
                prefix (algorithmic), 488                         Hamming distance, 439 
                Shannon, 7, 56, 213, 214, 217, 226                hash function, 370 
              enumerable family of sets,  17                      Hausdorff dimension,  172, 284 
              enumerable set,  17, 77, 473                          effective,  174,  286 
              equivalence with accuracy c, 351                    Huffman code, 217 
              everywhere dense set,  178                          hypotheses, 438
              expander, 371, 381                                  hypotheses of restricted type, 438 
              expectation-bounded randomness                      hypothesis 
                  deficiency, 73                                    minimal, 450
              explanation for a string, 426 
              extension of measure, 54                            image measure,  181 
              extractor,  121, 377                                implication 
              family of sets                                        critical, 412 
                enumerable, 17                                    incompressible string, 8, 43, 351 
              fingerprint,  370, 380                              independence
              finite automaton, 236                                 conditional, 51, 342, 362 
              finite field,  386                                    of random variables,  221 
              forbidden sequence, 248, 249                          of strings, 48,  51 
              forbidden string, 247                               inequality
              Ford-Fulkerson theorem, 385                           Azuma-Hoeffding, 302, 304 
              Ford-Fulkerson theorem, 378                           basic,  223, 318,  338 
              formula                                               Cauchy-Schwarz, 257 
                propositional, 404                                  Chebyshev,  101, 445 
              fractals,  172                                        Doob, 271 
              frequency,  11, 209                                   Fano, 224
                lower, 209                                          for complexities, 313 
              frequency of a letter, 214                            Gibbs, 215 
              frequency stability,  261, 467                        Ingleton, 339, 362 
              frequency stability axiom, 261                        Kolmogorov, 271 
              function                                              Kraft, 214
                basic,  72                                          Kraft-McMillan, 217 
                computable,  1,  15, 469                            non-Shannon, 343 
                hash, 370                                        information 
                lower semicomputable, 278                           mutual, 45 
                prefix-free,  83, 86                             information common for three strings, 50 
                prefix-stable, 82                                information flow, 384 
                  with respect to the first argument,  102       Ingleton inequality, 339, 362 
                Solovay,  165,  166                              injective code, 213 
                upper semicomputable, 19                         input node, 367 
              game, 373, 375, 393, 445                           input tape, 86 
              game argument, 41                                  inseparable sets, 28 
              generalized subsequence, 470                       interval,  53
              generic sequence, 70,  178                         intuitionistic logic, 404 
              Gibbs inequality,  215                             IPC, 404
              graph, 357                                         König lemma, 243 
                bipartite, 357, 371                              Kakutani theorem, 297 
                cut,  383                                        Kollektiv,  12, 261,  309 
                expander, 371                                    Kolmogorov admissible selection rule, 470 
                matching, 375                                    Kolmogorov complexity, 2, 4 
                random, 372                                        conditional, 34
                508                                   SUBJECT INDEX
                  monotone, 130                                    marginal distribution,  187 
                  of pairs,  31, 48                                Markov chain, 51, 223 
                  of triples,  32,  48                             Martin-Löf random sequence, 476 
                  prefix,  83                                      martingale, 271, 276, 310 
                  relativized,  203                                   computable,  174, 278, 279 
                Kolmogorov complexity                                 lower semicomputable, 278 
                  plain,  15                                          partial,  290
                Kolmogorov inequality, 271                           strongly winning on a sequence, 273 
                Kolmogorov stochastic sequence, 468                  winning on a sequence, 273, 290 
                Kolmogorov stochastic sequence, 470, 475,            with respect to distribution, 274 
                    479, 483, 484                                  matching, 375 
                Kolmogorov-Levin theorem, 37, 39                     on-line, 375 
                Kraft inequality,  214                             mathematical statistics, 425 
                Kraft-Chaitin lemma, 94                            matroid, 339
                Kraft-McMillan inequality, 217                     maximal semimeasure, 79 
                Kripke model, 412, 413                             McMillan inequality, 217, 222 
                Kullback-Leibler distance, 215, 276                MDL (minimal description length), 431 
                Kurtz randomness, 70                               measurable set, 54 
                Lambalgen theorem, 185                             measure, 54,  172 
                language recognized by the automaton, 236            atomless,  176 
                law of iterated logarithm, 239, 269                  Bernoulli,  11, 55, 66,  75 
                law of large numbers, 55, 65                         computable, 60, 61,  119 
                  for variable probabilities, 302                    uniform, 55 
                  strong,  179, 303, 304                           Miller-Yu theorem, 154 
                layerwise computable mapping,  182                 minimal hypothesis, 450 
                lemma                                              minimal sufficient statistics, 390 
                  ample excess,  151                               ML-random point, 68 
                  Borel-Cantelli, 57                               ML-random real, 68 
                  Levin, 241                                       ML-randomness, 63 
                  Lovåsz, 252                                      modulus of convergence, 22 
                length of a string, 466                            Moivre-Laplace theorem, 230 
                Levin lemma, 241                                   monotone complexity, 9,  130,  132 
                Levin-Schnorr theorem,  146, 148                   monotone machine, 128 
                Lipschitz mapping, 258                             Muchnik theorem, 369 
                Lipschitz property,  16                              combinatorial version, 374 
                local lemma, Lovåsz, 246                           Muchnik’s theorem
                Lovåsz lemma, 245                                    combinatorial interpretation, 374 
                Lovåsz local lemma, 246                            mutual information, 45, 222, 352
                low for Martin-Löf randomness,  180                non-computability of complexity, 9 
                low set,  180                                      non-Shannon inequality, 343 
                lower bounds for complexity, 9                     non-stochastic string, 428 
                lower graph of mapping,  127                       normal real, 265 
                lower semicomputable, 72                           normal sequence, 265 
                lower semicomputable function,  116                not worse, comparison of description 
                lower semicomputable martingale, 278                    modes, 2 
                lower semicomputable real, 68, 76,  158            null set,  11,  54,  263 
                lower semicomputable semimeasure, 79               number
                lower semicomputable series, 80                      Solovay complete,  160 
                map                                                numbering, 202 
                  computable,  15                                    computable, 202 
                mapping                                              Gödel, 202 
                  computable, 91,  127,  193, 201                    optimal, 202
                  continuous, 90,  193                             Occam’s razor, 10, 452 
                  continuous E —> E, 127                           open subset, 53
                  covering,  396                                   optimal conditional decompressor, 34 
                  Lipschitz, 258                                   optimal decompressor, 83,  130, 200 
                  lower graph,  127                                  prefix-free,  169
                                                      SUBJECT INDEX                                         509
                 prefix-stable,  103                                  computable, 284 
              optimal description language, 472                       computably, 311 
              optimal description mode, 3,  15                        deficiency, 8 
              optimal numbering, 202                                  Kurtz, 284, 310, 311 
              optimality deficiency, 448                              Martin-Löf, 63, 310, 476 
              oracle,  27,  202                                       Mises-Church, 284, 311 
                 O',  203                                             Mises-Church-Daley, 305, 311 
                 non-comp utable,  180                                Mises-Kolmogorov-Loveland, 291, 309, 
              orbit of the point, 323                                   311
              output node, 367                                        of real numbers,  157 
              paradox                                                 partial-computably, 311 
                 Berry’s, 9                                           Schnorr, 69, 282 
                 heap, 466                                         randomness deficiency, 8, 71,  146,  182, 426, 
              partial martingale, 290                                   447 
              Peirce’s law, 405                                       a priori,  177
              Peirce,  Charles Sanders (1839-1914), 405               expectation-bounded, 73,  150 
              plane Kolmogorov complexity,  15                        probability-bounded,  73 
              prefix, 466                                          randomness extractor, 121 
              prefix code,  213                                    randomness test 
              prefix complexity, 83,  148                             Martin-Löf,  71 
                of pairs,  105                                        probability-bounded, 72 
                of triples,  106,  109                             real
              prefix randomness deficiency, 427                       absolutely normal, 265 
                prefix, 427                                           normal in base b, 265 
              prefix-free code,  213                               regular function, 60 
              prefix-free encoding, 31                             relation
              prefix-free function, 83, 86                           enumerable, 377 
              prefix-stable function, 82                           relativization, 47,  103, 202,  223 
              probabilistic algorithm, 75,  115                       inequalities, 223 
              probabilistic argument, 395                          robust program, 128
              probability                                          Schnorr effectively null set, 69 
                a priori,  81                                      Schnorr random sequence, 283 
                conditional, 219, 275                              Schnorr randomness, 282 
                of a letter,  214                                  Scott domain, 201 
                of the event, 54                                   secret sharing, 385 
                von Mises, 261                                     selection rule, 261,  262,  287 
              probability bounded randomness test, 72                Church-Daley admissible, 287 
              probability distribution                               Church-admissible, 264 
                computable, 480                                      Kolmogorov-Loveland admissible, 291 
              probability measure, 176                             self-delimited input, 85 
              probability-bounded randomness deficiency,           self-delimiting description, 82 
                   73                                              self-delimiting program, 3 
              problem, 401                                         semimartingale, 278 
              profile of a pair of strings, 392                    semimeasure, 92,  105, 279 
              proper sequence,  177                                  continuous,  116 
              propositional formula, 404                             lower semicomputable, 79 
              pseudo-disjunction, 403                                maximal, 79 
              pseudo-randomness generator, 377                       on the binary tree,  116 
              PSPACE, 376                                            simple,  118 
              random graph, 372                                      universal,  122 
              random sequence, 58, 261, 466                        semimeasures
              random string, 8,  102                                 lower semicomputable, 122 
              random variable                                      separable set, 27 
                conditional independence, 342                      sequence
                independence, 221                                    1-        random, 157
              randomness                                             2- random,  157 
                blind,  65                                           balanced, 262
              510                                   SUBJECT INDEX
                characteristic, 67                               Solomonoff-Kolmogorov theorem, 3, 15, 34
                Church stochastic, 264                           Solovay complete number,  160
                computably random, 279, 284, 311                 Solovay function,  168
                convergence speed,  158                          Solovay reduction,  160
                forbidden, 248                                  solution to a problem, 401
                generic, 70,  178                               space of partial functions, 201
                Kolmogorov-Loveland random, 310                 stabilizer subgroup, 323
                Kurtz random, 284, 310, 311                      Stirling’s approximation, 226
                lower semicomputable, 77                        stochastic sequence, 482
                Martin-Löf random, 63, 229, 270, 310            stochastic string, 428
                  criterion,  125                               strategy, 479
                Martin-L’of random, 430                         string
                Mises-Church random, 264, 266, 270,                forbidden,  247 
                  284,  311                                        incompressible, 8, 27, 351 
                Mises-Church-Daley random, 296, 305,               random, 8 
                  311                                              stochastic, 428 
                Mises-Kolmogorov-Loveland random,               strong law of large numbers,  11, 238, 276 
                  291,  296,  305,  307,  309, 311              sufficient statistics, 390 
                normal, 265                                     suffix code, 213 
                partial-computably random, 290, 311             supermartingale, 278 
                proper,  177                                    symmetry of information, 45
                random, 65, 466                                 the quantity of information 
                Schnorr random, 69, 266, 283                       in X about y, 7 
                typical,  63                                    theorem
                unpredictable, 476                                 Ahlswede-Körner, 347 
                weakly 1-generic,  178                             Arslanov, 28 
              set                                                  Baire,  178
                a-null,  172,  173                                 Chan-Yeung, 323, 324 
                c-uniform, 324                                     Doob, 274 
                r- separable, 27                                   Ford-Fulkerson, 378 
                almost uniform, 324, 326                           Gödel incompleteness,  13 
                Borel, 53                                         Hall,  378 
                Cantor, 53                                        Kakutani, 297 
                computable, 21                                    Kolmogorov-Levin, 37 
                decidable, 21                                     Kolmogorov-Solomonoff, 34 
                effectively null,  58,  59,  147                  Lambalgen,  185 
                effectively open,  70,  178                       Levin-Schnorr,  11,  173 
                enumerable, 17, 77, 473                           Martin-Löf, 482, 485 
                everywhere dense,  178                            Romashchenko, 314, 326, 327 
                low,  180                                         Shannon coding, 231 
                measurable, 54                                    Shannon on perfect cryptosystems, 225 
                null,  54,  263                                   Slepian-Wolf, 377 
                open, 53, 89                                      Solomonoff-Kolmogorov, 3 
                prefix-free, 98                                 total conditional complexity, 375 
                Schnorr effectively null, 69                    total conditional complexity, 35, 437 
                simple, 22,  118                                transitivity,  258 
                Turing complete, 27                             Turing completeness, 27 
                uniform, 318, 319                               Turing degree, 177 
              Shannon entropy, 7, 56, 213, 214, 217, 314        Turing equivalence,  172,  177 
              shortest description, 352                         Turing machine,  12, 85, 233 
              simple set, 22                                      crossing sequence, 234 
              simple family of sets,  118                       Turing reduction, 27 
              simple semimeasure,  118                          typical representative of a set, 426 
              simple set,  118                                  typical sequence, 63, 475, 476 
              singleton,  174                                   typicalness, 467 
              Slepian-Wolf theorem, 369, 377                    typization, 326, 327, 332
              slow convergence in the Solovay sense,  165
              slowly converging series,  169                    uniform measure, 55
                                  SUBJECT INDEX                    511
         uniform probability distribution, 467 
         uniform set, 318, 319 
          construction, 322 
         uniform tests of randomness, 65 
         universal continuous semimeasure, 122 
         universal enumerable set, 203 
         universal probabilistic machine 
          halting probability,  157 
         unpredictability, 467
         unpredictable sequence, 476, 477, 479, 485, 
            489
         upper graph of function,  19 
         upper semicomputable function, 19
         Ville example, 269
         volume of a ball, 466
         volume of a three-dimensional body, 257
         weakly 1-generic sequence, 178 
         word
          c-equivalence, 351
         XOR, 385
        Selected Published Titles in This Series
        222  Alexandru Buium,  Foundations of Arithmetic Differential Geometry, 2017 
        221  Dennis Gaitsgory and Nick Rozenblyum,  A Study in Derived Algebraic Geometry, 
           2017
        220  A.  Shen, V.  A.  Uspensky, and N. Vereshchagin,  Kolmogorov Complexity and 
           Algorithmic Randomness, 2017 
        219  Richard Evan Schwartz,  The Projective Heat Map, 2017
        218  Tushar Das, David Simmons, and Mariusz Urbanski,  Geometry and Dynamics in 
           Gromov Hyperbolic Metric Spaces, 2017 
        217  Benoit Fresse,  Homotopy of Operads and Grothendieck-Teichmiiller Groups, 2017 
        216  Frederick W.  Gehring,  Gaven J.  Martin, and Bruce P.  Palka,  An Introduction to 
           the Theory of Higher-Dimensional Quasiconformal Mappings, 2017 
        215  Robert Bieri and Ralph Strebel,  On Groups of PL-homeomorphisms of the Real Line, 
           2016
        214  Jared Speck,  Shock Formation in Small-Data Solutions to 3D Quasilinear Wave 
           Equations, 2016
        213  Harold G. Diamond and Wen-Bin Zhang (Cheung Man Ping),  Beurling 
           Generalized Numbers, 2016 
        212  Pandelis Dodos and Vassilis Kanellopoulos,  Ramsey Theory for Product Spaces, 
           2016
        211  Charlotte Hardouin, Jacques Sauloy, and Michael F.  Singer,  Galois Theories of 
           Linear Difference Equations: An Introduction, 2016 
        210  Jason P.  Bell,  Dragos  Ghioca,  and Thomas J. Tucker,  The Dynamical 
           Mordell-Lang Conjecture, 2016 
        209  Steve Y.  Oudot,  Persistence Theory: From Quiver Representations to Data Analysis, 
           2015
        208  Peter S.  Ozsvåth, Andrås I.  Stipsicz, and Zoltån Szabo,  Grid Homology for Knots 
           and Links, 2015
        207  Vladimir I.  Bogachev,  Nicolai V.  Krylov, Michael Röckner, and Stanislav V.
           Shaposhnikov,  Fokker-Planck-Kolmogorov Equations, 2015 
        206  Bennett  Chow, Sun-Chin Chu, David Glickenstein, Christine Guenther, James 
           Isenberg, Tom Ivey, Dan Knopf, Peng Lu, Feng Luo, and Lei Ni,  The Ricci Flow: 
           Techniques and Applications:  Part IV: Long-Time Solutions and Related Topics, 2015 
        205  Pavel Etingof, Shlomo Gelaki, Dmitri Nikshych, and Victor Ostrik,  Tensor 
           Categories, 2015
        204  Victor M.  Buchstaber and Taras E. Panov,  Toric Topology, 2015 
        203  Donald Yau and Mark W. Johnson,  A Foundation for PROPs, Algebras, and 
           Modules, 2015
        202  Shiri Artstein-Avidan, Apostolos Giannopoulos, and Vitali D.  Milman,
           Asymptotic Geometric Analysis, Part I, 2015 
        201  Christopher L. Douglas, John Francis,  André G.  Henriques, and Michael A.
           Hill,  Editors,  Topological Modular Forms, 2014 
        200  Nikolai Nadirashvili, Vladimir Tkachev, and Serge Vladu$,  Nonlinear Elliptic 
           Equations and Nonassociative Algebras, 2014 
        199  Dmitry S.  Kaliuzhnyi-Verbovetskyi and Victor Vinnikov,  Foundations of Free 
           Noncommutative Function Theory, 2014 
        198  Jörg Jahnel,  Brauer Groups, Tamagawa Measures, and Rational Points on Algebraic 
           Varieties, 2014
        197  Richard Evan Schwartz,  The Octagonal PETs, 2014
                   For a complete list of titles in this series, visit the 
                AMS Bookstore at www.ams.org/bookstore/survseries/.
