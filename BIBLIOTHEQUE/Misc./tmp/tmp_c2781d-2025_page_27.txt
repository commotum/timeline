                     Preprint, Under Review.
                                   ThesystempromptprovidedtotheagentinPOGS
                       You are an AI agent designed to navigate the Partially Observable
                       Graph Search (POGS) environment. Your primary objective is to find
                       and reach a specific target node.
                       The following are the only valid actions you can take in the game:
                       {list(range(env.num nodes))}
                       In a moment I will present you with an observation containing:
                      - Adjacency list showing the neighbors of all currently visible nodes
                      - Your current node position
                      - The target node you need to reach
                       The graph has {env.num nodes} nodes and is partially observable,
                       meaning you can only see connections within a k-nearest neighbor
                       radius of your current position.  In this episode k={env.k nearest}.
                       Your action should be a single integer representing the label of the
                       node you want to travel to. This node must be directly connected to
                       your current node.
                       PLAY
                     Prompt 21: The system prompt for POGS. This prompt guides the AI agent by defining its objective
                     (to find and reach a specific target node in the POGS environment), outlining the valid actions (moving
                     to an adjacent node), and detailing the structure of observations (adjacency list of visible nodes,
                     current position, and target node).
                     Figure 6: Planning frequency affects exploration in POGS. Intermediate planning frequency yields
                     higher success rates and efficiency, while ’always-plan’ agents show increased backtracking (Cnoise).
                     but also to a higher backtrack count, with the ’always-plan’ agent performing the most backtracking.
                     Notably, this excessive backtracking correlates with a reduced success rate, as ’always-plan’ agents
                     exhibit lower performance compared to those planning less frequently. This suggests that planning
                     too often causes the agent to continually change its mind, ultimately hindering its ability to efficiently
                     navigate the graph and reach the target.
                     In contrast, agents that never plan also exhibit a high backtrack count, underscoring the value of
                     having a plan. Planning appears to make the agent’s behaviour more consistent and less erratic, which
                     in turn leads to higher success rates. Overall, these results highlight the importance of balanced
                     planning: planning too frequently or not at all hinders performance, while moderate planning
                     improves efficiency and success.
                     C APPENDIX
                     C.1 SUPERVISED FINE-TUNING
                     FortheSupervisedFine-Tuning(SFT)phase,wegeneratedadatasetof1,024trajectoriesusingLlama
                     3.3 70B instruct. To ensure data diversity, we employed 16 distinct planning prompts (detailed in
                                                         27
