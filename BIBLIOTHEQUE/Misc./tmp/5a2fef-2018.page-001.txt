                                Relational recurrent neural networks
                                     α              α             α        αβ                α
                        AdamSantoro* ,RyanFaulkner* ,DavidRaposo* ,JackRae ,MikeChrzanowski ,
                                     α            α            α              α                αβ
                      ThéophaneWeber ,DaanWierstra ,OriolVinyals ,RazvanPascanu ,TimothyLillicrap
                                                    *Equal Contribution
                                                       αDeepMind
                                                  London, United Kingdom
                                    βCoMPLEX,ComputerScience,UniversityCollege London
                                                  London, United Kingdom
                                 {adamsantoro; rfaulk; draposo; jwrae; chrzanowskim;
                              theophane; weirstra; vinyals; razp; countzero}@google.com
                                                      Abstract
                            Memory-basedneural networks model temporal data by leveraging an ability to
                            rememberinformation for long periods. It is unclear, however, whether they also
                            have an ability to perform complex relational reasoning with the information they
                            remember. Here, we ﬁrst conﬁrm our intuitions that standard memory architectures
                            maystruggle at tasks that heavily involve an understanding of the ways in which
                            entities are connected – i.e., tasks involving relational reasoning. We then improve
                            uponthese deﬁcits by using a new memory module – a Relational Memory Core
                           (RMC)–whichemploysmulti-headdot product attention to allow memories to
                            interact. Finally, we test the RMC on a suite of tasks that may proﬁt from more
                            capable relational reasoning across sequential information, and show large gains
                            in RL domains (e.g. Mini PacMan), program evaluation, and language modeling,
                            achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and
                            GigaWorddatasets.
                     1  Introduction
       arXiv:1806.01822v2  [cs.LG]  28 Jun 2018Humansusesophisticated memory systems to access and reason about important information regard-
                     less of when it was initially perceived [1, 2]. In neural network research many successful approaches
                     to modeling sequential data also use memory systems, such as LSTMs [3] and memory-augmented
                     neural networks generally [4–7]. Bolstered by augmented memory capacities, bounded computational
                     costs over time, and an ability to deal with vanishing gradients, these networks learn to correlate
                     events across time to be proﬁcient at storing and retrieving information.
                     Here we propose that it is fruitful to consider memory interactions along with storage and retrieval.
                     Although current models can learn to compartmentalize and relate distributed, vectorized memories,
                     they are not biased towards doing so explicitly. We hypothesize that such a bias may allow a model
                     to better understand how memories are related, and hence may give it a better capacity for relational
                     reasoning over time. We begin by demonstrating that current models do indeed struggle in this
                     domain by developing a toy task to stress relational reasoning of sequential information. Using a new
                     Relational Memory Core (RMC), which uses multi-head dot product attention to allow memories to
                     interact with each other, we solve and analyze this toy problem. We then apply the RMC to a suite
                     of tasks that may proﬁt from more explicit memory-memory interactions, and hence, a potentially
                     Preprint. Work in progress.
