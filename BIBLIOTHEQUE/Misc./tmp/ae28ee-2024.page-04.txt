                               Published as a conference paper at ICLR 2024
                                                                                        Table 1: Average and maximum numbers char-
                                           flask (11)              django (850)         acterizing different attributes of a SWE-bench
                                   matplotlib (184)                                     task instance. Statistics are micro-averages cal-
                                      pylint (57)                                       culated without grouping by repository.
                                   pytest (119)
                                  requests (44)                                                                             Mean      Max
                                scikit-learn (229)                      astropy (95)      Issue Text    Length (Words)      195.1     4477
                                       seaborn (22)                    xarray (110)
                                          sphinx (187)                                    Codebase      #Files (non-test)   3,010    5,890
                                                                sympy (386)                             #Lines (non-test)   438K     886K
                               Figure 3:     Distribution of SWE-bench tasks                            #Lines edited         32.8    5888
                                                                                          Gold Patch    #Files edited          1.7      31
                               (in parenthesis) across 12 open source GitHub                            #Func. edited            3      36
                               repositories that each contains the source code            Tests         #Fail to Pass          9.1    1633
                               for a popular, widely downloaded PyPI package.                           #Total              120.8     9459
                               are not capable of following the detailed instructions to generate repository-wide code edits, and
                               typically output placeholder responses or unrelated code. To better evaluate the capabilities of these
                               models, we perform supervised fine-tuning on the 7 billion- and 13 billion-parameter CodeLlama-
                               Python models. The resulting models are specialized repository editors that can run on consumer
                               hardware and resolve GitHub issues.
                               Training data. We follow our data collection procedure and collect 19,000 issue-PR pairs from an
                               additional 37 popular Python package repositories. In contrast to Section 2.1, we do not require
                               that pull requests contribute test changes. This allows us to create a much larger training set to use
                               for supervised fine-tuning. To eliminate the risk of data contamination, the set of repositories in the
                               training data is disjoint from those included in the evaluation benchmark.
                               Trainingdetails. Given the instructions, an issue text from GitHub and the relevant code files as the
                               prompt,wefinetuneSWE-Llamatogeneratethepatchthatsolvedthegivenissue(the“goldpatch”).
                               Formemoryefficiency,wefine-tuneonlytheweightsoftheattentionsublayerusingLoRAHuetal.
                               (2022), and exclude training sequences with more than 30,000 tokens, reducing the effective size of
                               the training corpus to 10,000 instances. More details are provided in Appendix B.
                               4    EXPERIMENTAL SETUP
                               In this section we explain how inputs are constructed to run SWE-bench evaluation. In addition, we
                               review the models that we evaluate in this work.
                               4.1   RETRIEVAL-BASED APPROACH
                               SWE-bench instances provide an issue description and a codebase as input to the model. While
                               issues descriptions are usually short (195 words on average as shown in Table 1), codebases consist
                               of many more tokens (438K lines on average) than can typically be fit into an LMs context window.
                               Thenthequestion remains of exactly how to choose the relevant context to provide to the model?
                               To address this issue for our baselines, we simply use a generic retrieval system to select the files
                               to insert as context. In particular, we evaluate models under two relevant context settings: 1) sparse
                               retrieval and 2) an oracle retrieval.
                               Sparse retrieval. Dense retrieval methods are ill-suited to our setting due to very long key and
                               querylengths, and especially the unusual setting of retrieving code documents with natural language
                               queries. Therefore, we choosetouseBM25retrieval(Robertsonetal.,2009)toretrieverelevantfiles
                               to provide as context for each task instance. We experiment with three different maximum context
                               limits, and simply retrieve as many files as fits within the specified limit. We evaluate each model
                               on all limits that fit within its context window and report the best performance. From observation,
                               models perform best on the shortest context window, as shown in Table 2.
                               “Oracle” retrieval. For analysis purposes we also consider a setting where we “retrieve” the files
                               edited by the reference patch that solved the issue on GitHub. This “oracle” setting is less realistic,
                                                                                     4
