                                 Extending Context Window of Large Language Models from a
                                                                    Distributional Perspective
                                                              1*                     1*                           1                            1
                                       Yingsheng Wu ,YuxuanGu ,XiaochengFeng ,WeihongZhong ,
                                                                        2                   2                       2                1
                                                  Dongliang Xu , Qing Yang , Hongtao Liu, Bing Qin
                                                         1Harbin Institute of Technology, Harbin, China
                                                    2DuXiaoman(Beijing) Science Technology Co., Ltd.
                                                     {yswu,yxgu,xcfeng,whzhong,qinb}@ir.hit.edu.cn
                                                 {xudongliang,yangqing,liuhongtao}@duxiaoman.com
                                                 Abstract                                                       /2                              /2
                            Scaling the rotary position embedding (RoPE)
                            has become a common method for extending
                            the context window of RoPE-based large lan-                          0                               0
                            guage models (LLMs). However, existing scal-                                    10                               10
                                                                                                         20                              20
                            ing methodsoftenrelyonempiricalapproaches                                30                              30
                            and lack a profound understanding of the in-
                            ternal distribution within RoPE, resulting in                                     3 /2                            3 /2
                            suboptimal performance in extending the con-                                                       (a)
                            text window length. In this paper, we pro-                                          /2                              /2
                            pose to optimize the context window extend-
                            ing task from the view of rotary angle distribu-
                            tion. Specifically, we first estimate the distri-
                            bution of the rotary angles within the model                         0                               0
                            and analyze the extent to which length ex-                                      10                               10
                            tension perturbs this distribution. Then, we                                 20                              20
                                                                                                     30                              30
                            present a novel extension strategy that min-
                            imizes the disturbance between rotary angle                                       3 /2                            3 /2
                            distributions to maintain consistency with the                                                    (b)
                            pre-training phase, enhancing the model’s ca-                              Pre-trained Distribution        Interpolated Distribution
                            pability to generalize to longer sequences. Ex-                            Extrapolated Distribution
                            perimental results compared to the strong base-                    Figure 1: Rotary angle distributions of extrapolation
                            line methods demonstrate that our approach                         and interpolation methods in two different dimensions,
                            reduces by up to 72% of the distributional                         compared with the pre-trained angle distribution. (a) In
                            disturbance when extending LLaMA2’s con-                           onedimension,theextrapolatedrotaryangledistribution
                            text window to 8k, and reduces by up to 32%                        fits more closely with the pre-trained distribution. (b)
                           when extending to 16k. On the LongBench-                            In another dimension, the interpolated distribution fits
                            E benchmark, our method achieves an aver-                          better with the pre-trained distribution.
                            age improvement of up to 4.33% over exist-
                            ing state-of-the-art methods. Furthermore, our
                            method maintains the model’s performance on
                            the Hugging Face Open LLM benchmark af-                            2024), modeling arbitrarily long textual sequences
                            ter context window extension, with only an                         remains a significant challenge. On the one hand,
                            average performance fluctuation ranging from                       LLMstrainedonshortsequences often encounter
                           -0.12 to +0.22. Our code is available at https:                     out-of-distribution (OOD) issues when applied to
                           //github.com/1180301012/DPRoPE.                                     the longer ones (Liu et al., 2023). On the other
                                                                                               hand, training an LLM with extremely long con-
                      1 Introduction                                                           text windows (i.e., the maximal sequence length)
                      Given the remarkable capabilities of transformer-                        fromscratchisexpensiveandinefficient. Currently,
                      based large language models (LLMs) in addressing                         the most popular approach is pre-training a large
                      a wide range of natural language processing tasks                        language model, such as LLaMA, Qwen2 (Tou-
                      (OpenAI,2023;Touvronetal.,2023a,b;Jiangetal.,                            vron et al., 2023a,b; Team, 2024), with a limited
                                                                                               context window and the rotary position embedding
                          * Equal Contribution                                                 (RoPE, Su et al. (2021)). During the inference
                                                                                         7288
                               Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 7288–7301
                                                   November12-16,2024©2024AssociationforComputational Linguistics
