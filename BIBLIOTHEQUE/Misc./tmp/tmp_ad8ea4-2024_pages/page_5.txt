                         Base             Model        Context     Evaluation Context Length          Average
                         LLM              Name         Window       0-4k      4-8k       8k+      Avg.    Avg.
                                                                                                               >4k
                                         Original         4k        27.69     26.24     25.79     26.57    26.02
                                         PI(s=2)          8k        28.21     26.90     26.79     27.30    26.85
                     LLaMA2-7B           PI(s=4)          16k       29.46     29.53     27.59     28.87    28.56
                                       YaRN(s=2)          8k        27.99     27.01     26.93     27.31    26.97
                                       YaRN(s=4)          16k       27.92     29.19     28.85     28.65    29.02
                                      CLEX(ms=16)         64k       25.22     28.87     28.62     27.57    28.75
                                        Ours(s=2)         8k        28.24     27.78     27.43     27.82    27.61
                                        Ours(s=4)         16k       29.98     30.30     30.09     30.12    30.20
                                         Original         4k        26.97     26.05     26.27     26.43    26.16
                                         PI(s=2)          8k        31.43     30.95     29.74     30.71    30.35
                     LLaMA2-13B          PI(s=4)          16k       30.80     31.33     30.86     30.99    31.10
                                       YaRN(s=2)          8k        31.00     30.42     30.07     30.50    30.25
                                       YaRN(s=4)          16k       31.59     31.35     29.89     30.94    30.62
                                      CLEX(ms=16)         64k       29.84     30.22     30.22     30.09    30.22
                                        Ours(s=2)         8k        31.64     31.40     30.43     31.16    30.91
                                        Ours(s=4)         16k       31.58     32.29     31.15     31.67    31.72
                Table 1: Comparative performance analysis of various context window extension methods on the Longbench-E
                benchmark. Avg. denotes the average score across all lengths, while Avg.>4k represents the average score for
                lengths exceeding the pre-training length. The scaling factor of CLEX (Chen et al., 2024) is dynamic, "ms" denotes
                the maximumscaling factor, and we set the maximum scaling factor to 16 in accordance with the settings of Chen
                et al. (2024).
                embedding as follows:                                extension of RoPE-based LLMs while maintaining
                       (θ         i   E          i   I               their original short-context capabilities.
                          i   if D (P ′,P ) > D (P ′,P )+t
                  ˆ       s          L    L          L   L
                  θi =   θ    otherwise,                             4.1   Experimental Details
                          i
                                                               (9)   Wevalidate the effectiveness of our method on the
                where t is a threshold to determine the extension    trending LLaMA2 (Touvron et al., 2023b) model,
                strategy when the disturbance scores Di(PE ,P )
                                                            ′   L    including 7B and 13B parameter models. All mod-
                                                           L
                and Di(PI ,P ) are very close. As demonstrated
                           ′   L                                     els are trained on a subset of PG19 (Rae et al.,
                          L
                in eq. (9), for the ith dimension, we employ linear  2020) datasets. For s = 2, models are fine-tuned
                                         ′
                interpolation with s = L /L, when its disturbance
                                   i                                 for 1000 steps with a global batch size of 64 and
                score is much smaller. Otherwise, direct extrapola-  max length of 8192. For s = 4, models are fine-
                tion is a preferred choice for this dimension.       tuned for 500 steps with a global batch size of 64
                   It’s worth noting that our approach is a pre-     andamaxlengthof16384. Wesetthedefaultvalue
                execution strategy that does not add any time or     of b in eq. (5) to 360. By adjusting the value of t in
                calculation cost during the inference phase as long  eq. (9), we set the default number of interpolated
                as the extension length L′ is provided. Besides,     dimensionsto80for8kextensionandto64for16k
                since we only modify the value of θ, any advanced    extension. See more details in appendix B.1.
                method that influences the attention mechanism,
                such as FlashAttention (Dao et al., 2022; Dao,       4.2   LongContextEvaluation
                2023), is still compatible.                          Toevaluate the model’s capabilities on real-world
                4 Experiments                                        long context tasks with an extended context win-
                                                                     dow. We utilize the Longbench-E benchmark (Bai
                In this section, we evaluate our distribution-based  et al., 2023), which is specifically designed for
                method on both long- and short-context bench-        evaluating models with long context window. The
                marks. Theresultsshowthatmodelsemployingour          Longbench-E benchmark consists of 13 diverse
                method outperform existing context window exten-     tasks, with the average length of most tasks rang-
                sion methods, indicating a better context window     ing from 5k to 15k. Furthermore, Bai et al. (2023)
                                                                 7292
