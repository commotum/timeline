                                      Shtetl-Optimized
                                                      The Blog of Scott Aaronson
                                     If you take nothing else from this blog: quantum computers won't
                                    solve hard problems instantly by just trying all solutions in parallel.
                 « 6.893 Philosophy and Theoretical Computer Science
                                                                           In Defense of Kolmogorov Complexity »
                 The First Law of Complexodynamics
                 A  few  weeks  ago,  I  had  the  pleasure  of  attending  FQXi’s  Setting  Time  Aright
                 conference,  part  of  which  took  place  on  a  cruise  from  Bergen,  Norway  to
                 Copenhagen, Denmark.  (Why aren’t theoretical computer science conferences ever
                 held on cruises?  If nothing else, it certainly cuts down on attendees sneaking away
                 from  the  conference  venue.)    This  conference  brought  together  physicists,
                 cosmologists, philosophers, biologists, psychologists, and (for some strange reason)
                 one quantum complexity blogger to pontiﬁcate about the existence, directionality,
                 and nature of time.  If you want to know more about the conference, check out Sean
                 Carroll’s Cosmic Variance posts here and here.
                 Sean also delivered the opening talk of the conference, during which (among other
                 things) he asked a beautiful question: why does “complexity” or “interestingness” of
                 physical systems seem to increase with time and then hit a maximum and decrease,
                 in contrast to the entropy, which of course increases monotonically?
                 My purpose, in this post, is to sketch a possible answer to Sean’s question, drawing
                 on  concepts  from  Kolmogorov  complexity.    If  this  answer  has  been  suggested
                 before, I’m sure someone will let me know in the comments section.
                 First, some background: we all know the Second Law, which says that the entropy of
                 any closed system tends to increase with time until it reaches a maximum value. 
                 Here “entropy” is slippery to deﬁne—we’ll come back to that later—but somehow
                 measures how “random” or “generic” or “disordered” a system is.  As Sean points out
                 in his wonderful book From Eternity to Here, the Second Law is almost a tautology:
                 how could a system not tend to evolve to more “generic” conﬁgurations?  if it didn’t,
                 those  conﬁgurations  wouldn’t  be  generic!    So  the  real  question  is  not  why  the
                 entropy is increasing, but why it was ever low to begin with.  In other words, why did
                 the universe’s initial state at the big bang contain so much order for the universe’s
                 subsequent evolution to destroy?  I won’t address that celebrated mystery in this
                 post, but will simply take the low entropy of the initial state as given.
                 The  point  that  interests  us  is  this:  even  though  isolated  physical  systems  get
                 monotonically more entropic, they don’t get monotonically more “complicated” or
                 “interesting.”  Sean didn’t deﬁne what he meant by “complicated” or “interesting”
                 here—indeed, deﬁning those concepts was part of his challenge—but he illustrated
                 what he had in mind with the example of a cof㘶ee cup.  Shamelessly ripping of㘶 his
                 slides:
                                     Entropyincreases.
                   Complexityfirstincreases,thendecreases.
                        lowentropy        mediumentropy         highentropy
                      lowcomplexity       highcomplexity      lowcomplexity
                 Entropy increases monotonically from left to right, but intuitively, the “complexity”
                 seems highest in the middle picture: the one with all the tendrils of milk.  And same
                 is true for the whole universe: shortly after the big bang, the universe was basically
                 just a low-entropy soup of high-energy particles.  A googol years from now, after
                 the last black holes have sputtered away in bursts of Hawking radiation, the universe
                 will  basically  be  just  a  high-entropy soup of low-energy particles.  But today, in
                 between, the universe contains interesting structures such as galaxies and brains
                 and hot-dog-shaped novelty vehicles.  We see the pattern:
                 In  answering  Sean’s  provocative  question  (whether  there’s  some  “law  of
                 complexodynamics” that would explain his graph), it seems to me that the challenge
                 is twofold:
                    1.  Come up with a plausible formal deﬁnition of “complexity.”
                    2.  Prove that the “complexity,” so deﬁned, is large at intermediate times in natural
                        model systems, despite being close to zero at the initial time and close to zero at
                        late times.
                 To clarify: it’s not hard to explain, at least at a handwaving level, why the complexity
                 should be close to zero at the initial time.  It’s because we assumed the entropy is
                 close to zero, and entropy plausibly gives an upper bound on complexity.  Nor is it
                 hard to explain why the complexity should be close to zero at late times: it’s because
                 the system reaches equilibrium (i.e., something resembling the uniform distribution
                 over  all  possible  states),  which  we’re  essentially  deﬁning  to  be  simple.    At
                 intermediate  times,  neither  of  those  constraints  is  operative,  and  therefore  the
                 complexity could become large.  But does it become large?  How large?  How could
                 we predict?  And what kind of “complexity” are we talking about, anyway?
                 After thinking on and of㘶 about these questions, I now conjecture that they can be
                 answered  using  a  notion  called  sophistication  from  the  theory  of  Kolmogorov
                 complexity.  Recall that the Kolmogorov complexity of a string x is the length of the
                 shortest computer program that outputs x (in some Turing-universal programming
                 language—the exact choice can be shown not to matter much).  Sophistication is a
                 more … well, sophisticated concept, but we’ll get to that later.
                 As a ﬁrst step, let’s use Kolmogorov complexity to deﬁne entropy.  Already it’s not
                 quite obvious how to do that.  If you start, say, a cellular automaton, or a system of
                 billiard balls, in some simple initial conﬁguration, and then let it evolve for a while
                 according to dynamical laws, visually it will look like the entropy is going up.  But if
                 the system happens to be deterministic, then mathematically, its state can always be
                 speciﬁed by giving (1) the initial state, and (2) the number of steps t it’s been run
                 for.  The former takes a constant number of bits to specify (independent of t), while
                 the latter takes log(t) bits.  It follows that, if we use Kolmogorov complexity as our
                 stand-in for entropy, then the entropy can increase at most logarithmically with t—
                 much slower than the linear or polynomial increase that we’d intuitively expect.
                 There  are  at  least  two  ways  to  solve  this  problem.    The  ﬁrst  is  to  consider
                 probabilistic systems, rather than deterministic ones.  In the probabilistic case, the
                 Kolmogorov complexity really does increase at a polynomial rate, as you’d expect.
                   The  second  solution  is  to  replace  the  Kolmogorov  complexity  by  the  resource-
                 bounded Kolmogorov complexity: the length of the shortest computer program that
                 outputs the state in a short amount of time (or the size of the smallest, say, depth-3
                 circuit that outputs the state—for present purposes, it doesn’t even matter much
                 what kind of resource bound we impose, as long as the bound is severe enough).
                  Even though there’s a computer program only log(t) bits long to compute the state
                 of the system after t time steps, that program will typically use an amount of time
                 that grows with t (or even faster), so if we rule out su㘠陦ciently complex programs, we
                 can again get our program size to increase with t at a polynomial rate.
                 OK, that was entropy.  What about the thing Sean was calling “complexity”—which, to
                 avoid  confusion  with  other  kinds  of  complexity,  from  now  on  I’m  going  to  call
                 “complextropy”?  For this, we’re going to need a cluster of related ideas that go
                 under names like sophistication, Kolmogorov structure functions, and algorithmic
                 statistics.    The  backstory  is  that,  in  the  1970s  (after  introducing  Kolmogorov
                 complexity),  Kolmogorov made an observation that was closely related to Sean’s
                 observation  above.    A  uniformly  random  string,  he  said,  has  close-to-maximal
                 Kolmogorov complexity, but it’s also  one  of  the  least  “complex”  or  “interesting”
                 strings imaginable.  After all, we can describe essentially everything you’d ever want
                 to know about the string by saying “it’s random”!  But is there a way to formalize that
                 intuition?  Indeed there is.
                 First, given a set S of n-bit strings, let K(S) be the number of bits in the shortest
                 computer program that outputs the elements of S and then halts.  Also, given such a
                 set S and an element x of S, let K(x|S) be the length of the shortest program that
                 outputs  x,  given  an  oracle  for  testing  membership  in  S.    Then  we  can  let  the
                 sophistication of x, or Soph(x), be the smallest possible value of K(S), over all sets S
                 such that
                    1.  x∈S and
                    2.  K(x|S) ≥ log (|S|) – c, for some constant c.  (In other words, one can distill all the
                                      2
                        “nonrandom” information in x just by saying that x belongs that S.)
                 Intuitively, Soph(x) is the length of the shortest computer program that describes,
                 not necessarily x itself, but a set S of which x is a “random” or “generic” member.  To
                 illustrate, any string x with small Kolmogorov complexity has small sophistication,
                 since we can let S be the singleton set {x}.  However, a uniformly-random string also
                                                                                           n
                 has small sophistication, since we can let S be the set {0,1}  of all n-bit strings.  In
                 fact, the question arises of whether there are any sophisticated strings!  Apparently,
                 after Kolmogorov raised this question in the early 1980s, it was answered in the
                 a㘠陦rmative  by  Alexander  Shen  (for  more,  see  this  paper by Gács, Tromp, and
                 Vitányi).    The  construction  is  via  a  diagonalization  argument  that’s  a  bit  too
                 complicated to ﬁt in this blog post.
                 But  what  does  any  of  this  have  to  do  with  cof㘶ee  cups?    Well,  at  ﬁrst  glance,
                 sophistication seems to have exactly the properties that we were looking for in a
                 “complextropy” measure: it’s small for both simple strings and uniformly random
                 strings,  but  large  for  strings  in  a  weird  third  category  of  “neither  simple  nor
                 random.”  Unfortunately, as we deﬁned it above, sophistication still doesn’t do the
                 job.  For deterministic systems, the problem is the same as the one pointed out
                 earlier for Kolmogorov complexity: we can always describe the system’s state after t
                 time steps by specifying the initial state, the transition rule, and t.  Therefore the
                 sophistication can never exceed log(t)+c.  Even for probabilistic systems, though, we
                 can specify the set S(t) of all possible states after t time steps by specifying the initial
                 state,  the  probabilistic  transition  rule,  and  t.    And,  at  least  assuming  that  the
                 probability distribution over S(t) is uniform, by a simple counting argument the state
                 after  t  steps  will  almost  always  be  a  “generic”  element  of  S(t).    So  again,  the
                 sophistication  will  almost  never  exceed  log(t)+c.    (If  the  distribution  over  S(t)  is
                 nonuniform, then some technical further arguments are needed, which I omit.)
                 How can we ﬁx this problem?  I think the key is to bring computational resource
                 bounds into the picture.  (We already saw a hint of this in the discussion of entropy.) 
                 In  particular,  suppose  we  deﬁne  the  complextropy  of  an  n-bit  string  x  to  be
                 something like the following:
                     the number of bits in the shortest computer program that runs in n log(n) time,
                     and that outputs a nearly-uniform sample from a set S such that (i) x∈S, and (ii)
                     any  computer program that outputs x in n log(n) time, given an oracle that
                     provides independent, uniform samples from S, has at least log (|S|)-c bits, for
                                                                                                     2
                     some constant c.
                 Here n log(n) is just intended as a concrete example of a complexity bound: one
                 could replace it with some other time bound, or a restriction to (say) constant-depth
                 circuits or some other weak model of computation.  The motivation for the deﬁnition
                 is that we want some “complextropy” measure that will assign a value close to 0 to
                 the ﬁrst and third cof㘶ee cups in the picture, but a large value to the second cof㘶ee
                 cup.  And thus we consider the length of the shortest e㘠陦cient computer program
                 that  outputs,  not  necessarily  the  target  string  x  itself,  but  a  sample  from  a
                 probability distribution D such that x is not e㘠陦ciently compressible with respect to
                 D.  (In other words, x looks to any e㘠陦cient algorithm like a “random” or “generic”
                 sample from D.)
                 Note that it’s essential for this deﬁnition that we imposed a computational e㘠陦ciency
                 requirement in two places: on the sampling algorithm, and also on the algorithm that
                 reconstructs x given the sampling oracle.  Without the ﬁrst e㘠陦ciency constraint, the
                 complextropy could never exceed log(t)+c by the previous argument.  Meanwhile,
                 without the second e㘠陦ciency constraint, the complextropy would increase, but then
                 it would probably keep right on increasing, for the following reason: a time-bounded
                 sampling algorithm wouldn’t be able to sample from exactly the right set S, only a
                 reasonable  facsimile  thereof,  and  a  reconstruction  algorithm  with  unlimited  time
                 could probably then use special properties of the target string x to reconstruct x
                 with fewer than log2(|S|)-c bits.
                 But as long as we remember to put computational e㘠陦ciency requirements on both
                 algorithms,  I  conjecture  that  the  complextropy  will  satisfy  the  “First  Law  of
                 Complexodynamics,” exhibiting exactly the behavior that Sean wants: small for the
                 initial  state,  large  for  intermediate  states,  then  small  again  once  the  mixing  has
                 ﬁnished.  I don’t yet know how to prove this conjecture.  But crucially, it’s not a
                 hopelessly open-ended question that one tosses out just to show how wide-ranging
                 one’s thoughts are, but a relatively-bounded question about which actual theorems
                 could be proved and actual papers published.
                 If you want to do so, the ﬁrst step will be to “instantiate” everything I said above with
                 a  particular  model  system and particular resource constraints.  One good choice
                 could be a discretized “cof㘶ee cup,” consisting of a 2D array of black and white pixels
                 (the  “cof㘶ee”  and  “milk”),  which  are  initially  in  separated  components  and  then
                 subject to random nearest-neighbor mixing dynamics.  (E.g., at each time step, we
                 pick an adjacent cof㘶ee pixel and milk pixel uniformly at random, and swap the two.)
                   Can  we  show  that  for  such  a  system,  the  complextropy  becomes  large  at
                 intermediate  times  (intuitively,  because  of  the  need  to  specify  the  irregular
                 boundaries  between  the  regions  of  all-black  pixels,  all-white  pixels,  and  mixed
                 black-and-white pixels)?
                 One  could  try  to  show  such  a  statement  either  theoretically  or  empirically.
                  Theoretically, I have no idea where to begin in proving it, despite a clear intuition
                 that such a statement should hold: let me toss it out as a wonderful (I think) open
                 problem!  At an empirical level, one could simply try to plot the complextropy in
                 some  simulated  system,  like  the  discrete  cof㘶ee  cup,  and  show  that  it  has  the
                 predicted  small-large-small  behavior.      One  obvious  di㘠陦culty  here  is  that  the
                 complextropy, under any deﬁnition like the one I gave, is almost certainly going to
                 be  intractable  to  compute  or  even  approximate.    However,  one  could  try  to  get
                 around that problem the same way many others have, in empirical research inspired
                 by Kolmogorov complexity: namely, by using something you can compute (e.g., the
                 size of a gzip compressed ﬁle) as a rough-and-ready substitute for something you
                 can’t  compute  (e.g.,  the  Kolmogorov  complexity  K(x)).    In  the  interest  of  a  full
                 disclosure, a wonderful MIT undergrad, Lauren Oullette, recently started a research
                 project with me where she’s trying to do exactly that.  So hopefully, by the end of the
                 semester, we’ll be able to answer Sean’s question at least at a physics level of rigor!
                  Answering the question at a math/CS level of rigor could take a while longer.
                 PS (unrelated). Are neutrinos traveling faster than light?  See this xkcd strip (which
                 does what I was trying to do in the Deolalikar af㘶air, but better).
                      Like 10         Post               Follow           Share 10
                  
                  This entry was posted on Friday, September 23rd, 2011 at 1:53 pm and is filed under Complexity,
                  Physics for Doofuses. You can follow any responses to this entry through the RSS 2.0 feed. Both
                  comments and pings are currently closed.
                 127 Responses to “The First Law of Complexodynamics”
                  Alejandro Weinstein Says:
                  Comment #1 September 23rd, 2011 at 2:23 pm
                  What     about     Logical     Depth,     as    deﬁned      by    Charles     H.    Bennett     (see
                  http://en.wikipedia.org/wiki/Logical_depth and http://bit.ly/nh0bra for more details)? It seems
                  to me that it is also a good proxy for complexity.
                  Complexodynamics Says:
                  Comment #2 September 23rd, 2011 at 2:44 pm
                  As you demonstrate, the ﬁrst law of complexodynamics is: complexity is inversely proportional
                  to reason. This is also known as the KISS rule: Keep It Simple, Stupid.
                  OF COURSE complexity/interestingness peaks in the middle of an evolution, because that’s
                  when  the  system  is  EVOLVING.  Complexity/interestingness  is  maximized  when  things  are
                  changing. Initial state: delta = 0, boring. Final state: equilibrium, boring. Intermediate states:
                  that’s where all the interesting stuf㘶 is happening. The monotonic Second Law is the same thing
                  as   the   monotonic  passage  of  time.  Entropy  only  reaches  its  maximum,  and
                  complexity/interestingness returns to its minimum, when time stops and all evolutions are
                  static. See the cof㘶ee cup slide.
                  M. Alaggan Says:
                  Comment #3 September 23rd, 2011 at 2:55 pm
                  Consider the 2^n state space of some 3SAT equation, where each vector represents either
                  {satisﬁable, not satisﬁable}. Take a 3SAT instance which maximizes the complextropy of such
                  state space (over all state spaces of 3SAT instances). Maybe this could be useful to prove some
                  lower bounds on SAT.
                  Jair Says:
                  Comment #4 September 23rd, 2011 at 2:56 pm
                  Fascinating  stuf㘶,  Scott!  I  wonder  if  strings  of  human  language  would  fall  into  the  “high-
                  complextropy” category. It would be interesting to test this empirically once a good model is
                  developed.
                  Scott Says:
                  Comment #5 September 23rd, 2011 at 3:49 pm
                  Complexodynamics #2:
                  OF COURSE complexity/interestingness peaks in the middle of an evolution, because that’s
                  when the system is EVOLVING.
                  Yeah, I completely agree that that’s the intuition! The hard part is to come up with some
                  formal, quantitative measure of “complexity/interestingness” that matches that intuition. When
                  you actually try to do that, you run into lots of issues that I don’t think are nearly as obvious,
                  and which are what this post was about.
                  Sean Carroll Says:
                  Comment #6 September 23rd, 2011 at 3:49 pm
                  Thanks for the plug, Scott! And thanks even more for thinking about the problem. (Although
                  now I’ll have to do a less-superﬁcial job on the blog post I was planning myself…)
                  I’ll  certainly  need  to  think  more  carefully  about  the  careful  deﬁnitions  you’ve  suggested,
                  although we’re certainly thinking along similar lines. (I even thought of using ﬁle-compression
                  algorithms.) It would deﬁnitely be fun to have an actual argument for a clearly-stated principle.
                  There does seem to be one dif㘶erence of approach that may or may not be important. Namely,
                  I’m convinced of the importance of coarse-graining, which you don’t seem to bring into play at
                  all. Note that I did actually propose an (admittedly informal) deﬁnition of “complexity” on slide
                  15:
                  http://www.slideshare.net/seanmcarroll/setting-time-aright
                  Namely, “the Kolmogorov complexity of the description of each *macrostate* of the system.”
                  Obviously that relies on a ﬁxed coarse-graining supplied ahead of time, to partition the state
                  space into macrostate equivalence classes. This makes some people nervous because it seems
                  arbitrary, but to me it’s both legitimate and crucial (at least I suspect so). In the real world, we
                  can stare as much as we want at that class of cream and cof㘶ee — we’re not going to end up
                  specifying the microstate by measuring the position and momentum of every single molecule.
                  Our eyes just don’t work that way. We have available only certain macroscopic features of the
                  system, and that is ultimately where the coarse-graining comes from.
                  That may or may not be a minor point, I’m not sure. Certainly the most interesting questions
                  are the ones you identiﬁed, to which I have no good answers — in what way does complexity
                  develop, at what speeds, etc.
                  Sean Carroll Says:
                  Comment #7 September 23rd, 2011 at 3:52 pm
                  Actually I should say two more things.
                  First,  while  “Complexodynamics” is certainly right that there’s a sense in which complexity
                  must go up and then down in this case, it’s far from clear how much it goes up and in what
                  way. In the cof㘶ee cup, we could imagine completely uniform dif㘶usion, which would keep the
                  complexity pretty low. In fact, that probably happens in an isolated cof㘶ee cup; for this picture I
                  reached in and poked it with a spoon. But there are other systems in which the complexity goes
                  up appreciably, like the universe. Any would-be law better handle this dif㘶erence.
                  Second, Raissa D’Souza (who studies real-world complex networks) pointed out to me at the
                  conference that it’s likely that an honest graph of complexity vs. time isn’t nearly that smooth,
                  as complexity can grow and then crash and grow again. Something else that a proper law of
                  nature better be able to accommodate.
                  Scott Says:
                  Comment #8 September 23rd, 2011 at 3:59 pm
                  Thanks, Sean! The di㘠陦culty is that I couldn’t ﬁgure out how to formalize the concept of a
                  “macrostate” in any satisfactory way. However, I completely agree that one needs, if not that
                  concept itself, then something else that plays the same role (since otherwise the entropy will
                  essentially  never  go  up,  as  I  said  in  the  post)!  In  my  deﬁnition,  the  restriction  to  e㘠陦cient
                  sampling algorithms is what plays the role that coarse-graining might play in a more physics-
                  based argument (i.e., it’s the thing that prevents us from saying that the entropy is small
                  because of detailed regularities in the microstate that no one could ever notice in practice).
                  Scott Says:
                  Comment #9 September 23rd, 2011 at 4:13 pm
                  Alejandro Weinstein #1:
                  What about Logical Depth, as deﬁned by Charles H. Bennett
                  Great question! I’ve been extremely interested in Bennett’s logical depth measure for a while,
                  and I considered discussing it in the post, ultimately deciding against.
                  The bottom line is that I think logical depth is not the right measure for this job, because in
                  contrast to sophistication, I don’t see any intuitive reason why the depth should become large
                  at intermediate times in (say) the cof㘶ee cup example!
                  Recall that the logical depth of a string x is (essentially) the number of time steps taken by the
                  shortest program that outputs x. Now, to describe the state of a cof㘶ee cup with little tendrils
                  of milk that are developing in some probabilistic way, it seems to me that we want to describe
                  the  boundaries  of  the  all-milk  region,  the  all-cof㘶ee  region,  and  the  regions  with  various
                  mixtures  of  the  two.  Once  we’ve  speciﬁed  those  boundaries,  an  algorithm  can  output  a
                  microstate for the cof㘶ee cup that’s macroscopically indistinguishable from the observed one,
                  by sampling cof㘶ee elements with independent probability p and milk elements with probability
                  1-p in those regions that have been speciﬁed to have a (p,1-p) mixture.
                  Now, the above will probably be a sophisticated/complextropic description in the sense of my
                  post, since specifying the boundaries of the milk tendrils might require many bits. But it won’t
                  be a deep description: once you’ve speciﬁed the boundaries, actually sampling a microstate can
                  be done in nearly-linear time! Therefore the depth need not become large at any point during
                  the mixing.
                  If the above argument is wrong, I’ll be extremely grateful to anyone who can explain why.
                  Foster Boondoggle Says:
                  Comment #10 September 23rd, 2011 at 4:28 pm
                  I think this is a case of selection bias. We pay attention to emergent complex phenomena and
                  give them a name. We name stars, not amorphous dust clouds. We notice galaxies, not dif㘶use
                  intergalactic  gas.  We  notice  the  cloud  that  looks  like  a  duck.  Some  processes  produce
                  complexity, some don’t. We just get interested in the ones that do.
                  Entropy always wins in the end, so eventually the emergent complex system decays.
                  It’s not so clear that there’s anything to explain.
                  @Sean – Isn’t your coarse-graining the same as Scott’s deﬁnition of S – the least complex set
                  within which x is ef㘶ectively random?
                  Henry Says:
                  Comment #11 September 23rd, 2011 at 4:37 pm
                  Re: Scott #9:
                  I’m   not    sure   I   understand  your  argument  correctly,  but  couldn’t  also  the
                  conﬁguration/arrangement of the mixed boundaries be itself logically deep? There could be a
                  huge number of dif㘶erent (p_i,1 – p_i)’s in the mixing zone, and the shortest sampling program
                  might require extra time that’s dependent on the number of dif㘶erent mixing zones.
                  Scott Says:
                  Comment #12 September 23rd, 2011 at 4:41 pm
                  “Foster Boondoggle” #10: What’s interesting is that, in denying that there’s anything to explain,
                  you threw around phrases like “emergent complex system,” simply assuming that people would
                  know what’s meant by them! But, as I tried to explain in the post, deﬁning those concepts
                  rigorously enough that we can prove theorems about them is the main challenge here!
                  To put it dif㘶erently: can you write a computer program that takes as input a raw bit string, and
                  that decides whether or not that string encodes “emergent complex behavior”? How do you do
                  it?
                  The task might sound hopeless, but in the post I proposed a class of programs to do exactly
                  that:  namely,  programs that calculate the “complextropy” as deﬁned in terms of resource-
                  bounded  Kolmogorov  complexity.  You’re  more  than  welcome  to  criticize  my  proposal  or
                  suggest a better one, but you can’t do so while blithely using words for which the entire
                  problem is to deﬁne those words! 
                  Scott Says:
                  Comment #13 September 23rd, 2011 at 4:46 pm
                  Henry #11: Yes, it’s possible that the logical depth could become large at intermediate times,
                  or that it could do so in some model systems / cellular automata but not in others. I just don’t
                  see an inherent reason for that to happen: if it did happen, it would seem almost like an
                  accident! Maybe I’m wrong though.
                  Sampson Brass Says:
                  Comment #14 September 23rd, 2011 at 6:16 pm
                  Interesting article, Scott. But I must say I’m surprised you attended such a fantastically self-
                  indulgent conference as Setting Time Aright. Even though it seems to have been somewhat
                  fruitful  for  you,  the  monumental  waste  and  extravagance  of  such  an  enterprise  should  be
                  deeply repugnant to any morally sophisticated person–which I have always considered you to
                  be.
                  Scott Says:
                  Comment #15 September 23rd, 2011 at 6:35 pm
                  “Sampson Brass” #14:
                  the monumental waste and extravagance of such an enterprise should be deeply repugnant to
                  any morally sophisticated person
                  WTF?  It’s  not  like  any  public  money  was  spent  on  this—just  FQXi’s  (i.e.,  the  Templeton
                  Foundation’s). If they want to pay to send me on a nice cruise, I’m morally obligated to say no?
                  Can you explain why?
                  Sampson Brass Says:
                  Comment #16 September 23rd, 2011 at 6:56 pm
                  Gee, Scott, I just meant that regardless of where the money came from, it shouldn’t have been
                  spent as it was, on a luxurious cruise through waters far from the homes of many of the
                  participants. Sort of like how an argument can be made that it is immoral (irrespective of all
                  other factors) for a rich person to spend his or her own money on, say, a $500,000 automobile.
                  We’re talking about indulgence, waste, extravagance; I don’t see how that can be justiﬁed in a
                  world where so many people have so little. Though for the record I’ll bet the cruise was a lot of
                  fun and I’m not sure I would have had the moral courage to refuse an invitation had it been
                  tendered me!
                  Luca Says:
                  Comment #17 September 23rd, 2011 at 7:24 pm
                  One (very simple) way to think about the notion of “coarseness” in the cof㘶ee example would be
                  to look at all possible scales. For each size parameter, divide the cup into cubes of that size,
                  and  average  the  milk-cof㘶ee  content  in  each  cube.  Now  look  at  the  time  vs  Kolmogorov
                  complexity graph for the “pixellated” cup of cof㘶ee that you get at this scale. At very small
                  scale, the graph will be the increase in entropy from the 2nd law of thermodynamic (assuming
                  that the process is randomized, or assuming that the process is in some sense pseudorandom
                  and that we are using time-bounded Kolmogorov complexity); at a scale large enough that the
                  cup is a single cell, the Kolmogorov complexity is constant.
                  If you consider the whole 3-dimensional scale-time-complexity graph, however, there will be
                  some “bump” at middle scales.
                  I don’t know what would be good computational deﬁnitions that would allow a more abstract
                  treatment in which one would see the same qualitative phenomenon.
                  Notion of samplability and distinguishability seem related enough: if, after averaging over a
                  certain scale, you have small descriptive complexity, then you have a sampler that, in each cell,
                  will simply produce a random mix of cof㘶ee and milk according to the relative density, and this
                  will be indistinguishable from “reality” by an observer that is not able to resolve what happens
                  within cells.
                  Scott Says:
                  Comment #18 September 23rd, 2011 at 7:31 pm
                  Sampson #16:
                  Except that, uhh, the “far from the homes of many of the participants” part is no dif㘶erent than
                  any academic conference anywhere! By the triangle inequality, you can’t have an international
                  conference without it being far from someone‘s home…           Furthermore, only 2 out of 6 nights
                  were spent on the ship, and I really doubt that this conference was signiﬁcantly more expensive
                  than an ordinary conference in a hotel (those are quite expensive as well). And the ship was the
                  National Geographic Explorer, which is not exactly a typical luxury ship.
                  So,  I  think  it  boils  to  a  question  raised  in  the  Ask  Me  Anything  post:  should  all academic
                  conferences should be cancelled? But if so, why start with academic conferences? Wouldn’t it be
                  better ﬁrst to request a halt to ordinary vacations?
                  Incidentally, this happens to have been the ﬁrst time in my life on a cruise of any kind. I’ve
                  never been to the Caribbean or Hawaii, and in fact I haven’t gotten on a plane “purely” for
                  vacation purposes in the last decade. Can you say the same? How much money do you spend
                  on “extravagances”? However much it is, shame on you! 
                  Foster Boondoggle Says:
                  Comment #19 September 23rd, 2011 at 7:42 pm
                  Hi Scott – I didn’t mean to dis your post. I found it quite interesting. But I was responding to the
                  opening question: “why does ‘complexity’ or ‘interestingness’ of physical systems seem to
                  increase with time and then hit a maximum and decrease?” My point was just that it’s only
                  those systems that we study. For lots of other systems this wouldn’t be true.
                  Take your cellular automata test case, except instead of using an updating rule that leads to
                  simple dif㘶usion, randomize also over the rules and initial states. Most of those systems will be
                  utterly boring – e.g., all the black pixels will disappear, or the system will remain chaotic and
                  random at all times. We won’t talk about them because there’s nothing much to say. But one of
                  the systems will be Conway’s Life, which will be initially chaotic, but then evolve through some
                  interesting  intermediate  state  with  gliders  and  perhaps  other  more  complicated  structures,
                  before eventually settling down to blinkers, blocks and other entropic debris. That’s the system
                  we pay attention to.
                  Responding to the opening question: it’s not true in general. It is for some small subset of
                  dynamical systems and initial conditions, and those are the ones we talk about.
                  Rick Says:
                  Comment #20 September 23rd, 2011 at 7:55 pm
                  Scott,
                  Re: Samson
                  Don’t even bother trying to defend this to someone who’s obviously not ever going to accept it.
                  What’s the point?
                  If Samson feels so strongly about this type of thing, he can certainty make sure he never does
                  anything so morally corrupt.
                  And, if it’s really a problem for him, he can stop following your blog and spend his valuable
                  time dong something else.
                  What is this guy? A monk?
                  Have some fun, relax, be creative and continue to come up with great things, which you seem
                  to have a knack for
                  By  the  way,  I  hope  this  was  a  family  af㘶air,  although  I  know  it’s  hard  to  organize  this
                  sometimes.
                  Sean Carroll Says:
                  Comment #21 September 23rd, 2011 at 7:56 pm
                  I have to back Sampson up on this one, I’m afraid. We didn’t make it public, but the invited
                  speakers  were  treated  to  lavish  caviar  breakfasts  and  complementary  between-session
                  footrubs. Which was ﬁne, but I thought the hot and cold running champagne was a bit morally
                  reprehensible.
                  Also, Scott, I notice that you have a tendency to tell “jokes” during your talks. This squanders
                  valuable time that could be used to convey information rather than simply being enjoyable, and
                  furthers your reputation as a moral monster. And here I thought better of you.
                  Scott Says:
                  Comment #22 September 23rd, 2011 at 7:59 pm
                  Foster  #19:  I  agree  that  there  are  plenty  of  systems  that  never  do  anything  “complex”  or
                  “interesting,” no matter how long you let them run. On the other hand, this is a rare case where
                  I might need to side with Wolfram       : it seems to me that “complex behavior” is the rule rather
                  than the exception. Crucially, in using the word “complex” here I don’t mean anything as grand
                  as life  or  intelligence:  all  I  mean  is  behavior  (like  that  of  Sean’s  second  cof㘶ee  photo)  that
                  doesn’t manifestly fall into one of the two categories of
                  (1) simple and predictable or
                  (2) completely entropic and random.
                  You mention that “most” cellular automaton rules lead to something boring, but it seems to me
                  that that’s an artifact of restricting the search space to a tiny set of rules (e.g., 1-dimensional,
                  2-state,  nearest-neighbor).  As  you  increase  any  of  those  parameters—e.g.,  the  number  of
                  colors per pixel—I conjecture that the “interesting” rules will come to asymptotically dominate,
                  and moreover that they’ll do so extremely quickly. It would be great to prove that, if it hasn’t
                  been done already.
                  Chris W. Says:
                  Comment #23 September 23rd, 2011 at 8:22 pm
                  Foster,
                  As you you seem to be hinting (and thinking), the question is implicitly anthropic. That is, as
                  observing participants in this universe, we ﬁnd that the universe evolves this way because, if it
                  didn’t, we never would have been part of it. To the extent that we imagine ourselves as external
                  observers, we still ﬁnd only this kind of universe worth talking about, because it is complex
                  enough to accommodate us as observing participants (actively observing subsystems). That is,
                  it resembles the universe in which we actually ﬁnd ourselves, and which manifestly sustains our
                  existence, at least for a while.
                  That  sounds like  another  attempt  to  write  of㘶  the  question  as  mostly  vacuous,  but  I  have
                  something else in mind. Such a universe is one in which we can think and do science. That is,
                  for a signiﬁcant period of time it sustains that peculiar combination of stability (reproducibility)
                  and dynamism (and diversity) in which it makes sense to pose scientiﬁc questions and attempt
                  to  answer  them.  In  particular,  it  prompts  us—again,  as  participating  subsystems—to  seek
                  stability  and  invariant  patterns  amidst  and  underlying  change,  and  gives  us  some  hope  of
                  identifying them before the clock runs out on our species.
                  Chris W. Says:
                  Comment #24 September 23rd, 2011 at 8:37 pm
                  [PS: Is there a Godwin’s Law for allusions to the Anthropic Principle?      ]
                  Sean Carroll Says:
                  Comment #25 September 23rd, 2011 at 8:50 pm
                  Scott’s  conjecture  in  #22  that  “interesting”  rules  asymptotically  dominate  as  CA  algorithms
                  become more complex is extremely interesting, if true. I actually don’t even have an intuition
                  either way. Does anyone think something along those lines could be provable?
                  Andy D Says:
                  Comment #26 September 23rd, 2011 at 9:18 pm
                  I know this isn’t the central issue, but as background for computer scientists, it would be very
                  nice to have a precise formulation of the 2nd Law *as a statement about Cellular Automaton
                  behavior*, as well as some sense of its scope.
                  For  which  CA  rules/initial  states  does  it  hold  true?  is  the  Law  provable?  Is  it  ultimately  a
                  tautology,  or  not?  Are  we  discussing  a  single  notion  of  “macro-states”/”coarse-graining”/”
                  pixellation”, or allowing for a more abstract setting? And so on.
                  I’ve read a few web pieces by Wolfram and others on this issue, but none that satisfy me. Can
                  anyone help out?
                  Carl Lumma Says:
                  Comment #27 September 23rd, 2011 at 9:31 pm
                  Like all life, we harvest energy from entropy gradients. This may be why the middle glass is
                  most interesting to us. We can’t do anything with the glass on the right, so there’s no need to
                  pay attention to it. The glass on the left is easy – I can shake it and generate electricity. But
                  competition for such easy gradients pushes us to ever harder ones, and we evolved reward
                  systems to help us.
                  This suggests invoking something like Landauer’s principle. Deﬁne complextropy for dynamic
                  systems sampled in time. At each sample, observe the system and predict its next state. The
                  bits you get right are counted toward your booty. Run your algorithm for free but in place, so
                  you spend energy resetting registers and so forth. Complextropy could be t/(b-c), where b is
                  the number of bits in your booty, c is the number you erased (energy cost), and t the number of
                  samples. It will be negative for the glass on the right, small for the glass on the left, and large
                  for the glass in the middle. Buy as much memory as you like for best performance, as long as it
                  is small compared to b.
                  There may be a way to extend this to static examples like an *image* of a cof㘶ee cup. But
                  starting from such examples may be trappy. They look interesting, perhaps only because we’ve
                  learned to quickly recognize the dynamic systems they’re part of.
                  rrtucci Says:
                  Comment #28 September 23rd, 2011 at 9:36 pm
                  Let P(c|r) be the probability density of color c at position r. \sum_c P(c|r) = 1. Let \mu([a,b]) =
                  b-a. Generalize \mu(.) to a proper measure.
                  Can’t you use the following non-sophisticated measure of sophistication?
                  \mu( range(P(c|r)) )
                  (it’s zero initially and ﬁnally but not in between. You can average over all positions r if you want
                  a bulk measure )
                  rrtucci Says:
                  Comment #29 September 23rd, 2011 at 9:39 pm
                  Addendum: by range(.), I meant the range of the function P(.|r) over all c at ﬁxed r
                  Justin Dove Says:
                  Comment #30 September 23rd, 2011 at 9:59 pm
                  Since no one has opened this can of worms, I’ll make the ﬁrst neutrino comment.
                  I don’t think the situation is as analogous to the Deolalikar case as people would think. I think
                  this  is  actually  far  more  probable of being valid (not that I am saying anything about how
                  absolutely likely or not it is that it is valid). The ﬁrst reason, is that you can’t ignore the fact
                  that it is a post ﬁve sigma result, hell it’s post six sigma. Now, that doesn’t mean there’s not
                  some unseen systematic errors, but these guys seem to be pretty good at what they do, and it
                  appears that they have been working their asses of㘶 trying to ﬁnd any potential systematic
                  errors. In their arXiv paper they are careful to never use evocative “superluminal” language and
                  express an extremely conservative viewpoint on the matter. Again, this doesn’t mean it’s not a
                  systematic error, but if it is it would have to be slipping by a large number of very skeptical
                  skilled professionals that are actively and sincerely hunting for it.
                  The next thing is that contrary to the popsci (and even some legit sci) word ﬂoating around,
                  this does not violate SR! SR works ﬁne with superluminal particles, but they open the possibility
                  of retrocasual signals and either the mass must become imaginary OR both the energy and
                  momentum must become imaginary. All of these things seem ridiculous, but given some of the
                  radical revolutions physics has seen, its not entirely out of the question. Even with superluminal
                  particles, retrocasual signals could end up being restricted in other ways, or perhaps a new
                  picture of causality would be needed. In fact, there is already some theory results out there that
                  show certain tachyons can’t be used to signal FTL.
                  Of the other implications the least ridiculous (though still ridiculous) of these is the imaginary
