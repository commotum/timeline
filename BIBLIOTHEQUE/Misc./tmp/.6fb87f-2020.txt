                     DeeBERT:DynamicEarlyExitingforAcceleratingBERTInference
                                 1,2                 1,2             1                1,2                   1,2
                          Ji Xin   , Raphael Tang      , Jaejun Lee , Yaoliang Yu       , and JimmyLin
                             1David R. Cheriton School of Computer Science, University of Waterloo
                                              2Vector Institute for Artiﬁcial Intelligence
                      {ji.xin,r33tang,j474lee,yaoliang.yu,jimmylin}@uwaterloo.ca
                                     Abstract
                     Large-scale pre-trained language models such
                     as BERT have brought signiﬁcant improve-
                     ments to NLP applications.  However, they
                     are also notorious for being slow in inference,
                     which makes them difﬁcult to deploy in real-     Figure 1: DeeBERT model overview. Grey blocks are
                     time applications. We propose a simple but ef-   transformer layers, orange circles are classiﬁcation lay-
                     fective method, DeeBERT,toaccelerateBERT         ers (off-ramps), and blue arrows represent inference
                     inference. Our approach allows samples to        samples exiting at different layers.
                     exit earlier without passing through the entire
                     model. Experiments show that DeeBERT is
                     able to save up to ∼40% inference time with      hypothesize that, for BERT, features provided by
                     minimal degradation in model quality. Fur-       the intermediate transformer layers may sufﬁce to
                     ther analyses show different behaviors in the    classify some input samples.
                     BERTtransformer layers and also reveal their        DeeBERTaccelerates BERT inference by insert-
                     redundancy.  Our work provides new ideas         ing extra classiﬁcation layers (which we refer to
                     to efﬁciently apply deep transformer-based
                     models to downstream tasks. Code is avail-       as off-ramps) between each transformer layer of
                     able at https://github.com/castorini/            BERT(Figure 1). All transformer layers and off-
                     DeeBERT.                                         rampsarejointly ﬁne-tuned on a given downstream
                 1   Introduction                                     dataset. At inference time, after a sample goes
                                                                      through a transformer layer, it is passed to the fol-
                 Large-scale pre-trained language models such as      lowing off-ramp. If the off-ramp is conﬁdent of
                 ELMo(Peters et al., 2018), GPT (Radford et al.,      the prediction, the result is returned; otherwise, the
                 2019), BERT (Devlin et al., 2019), XLNet (Yang       sample is sent to the next transformer layer.
                 et al., 2019), and RoBERTa (Liu et al., 2019) have      In this paper, we conduct experiments on BERT
                 brought signiﬁcant improvements to natural lan-      and RoBERTa with six GLUE datasets, showing
                 guageprocessing (NLP) applications. Despite their    that DeeBERT is capable of accelerating model in-
                 power, they are notorious for being enormous in      ference by up to ∼40%withminimalmodelquality
        arXiv:2004.12993v1  [cs.CL]  27 Apr 2020size and slow in both training and inference. Theirdegradation on downstream tasks. Further analy-
                 long inference latencies present challenges to de-   ses reveal interesting patterns in the models’ trans-
                 ployment in real-time applications and hardware-     former layers, as well as redundancy in both BERT
                 constrained edge devices such as mobile phones       and RoBERTa.
                 and smart watches.                                   2   Related Work
                   To accelerate inference for BERT, we propose
                 DeeBERT:DynamicearlyexitingforBERT.The               BERT and RoBERTa are large-scale pre-trained
                 inspiration comes from a well-known observa-         language models based on transformers (Vaswani
                 tion in the computer vision community: in deep       et al., 2017). Despite their groundbreaking power,
                 convolutional neural networks, higher layers typi-   there have been many papers trying to examine and
                 cally produce more detailed and ﬁner-grained fea-    exploit their over-parameterization. Michel et al.
                 tures (Zeiler and Fergus, 2014). Therefore, we       (2019) and Voita et al. (2019) analyze redundancy
                  in attention heads. Q-BERT (Shen et al., 2019)          Algorithm 1 DeeBERTInference (Input: x)
                  uses quantization to compress BERT, and Layer-            for i = 1 to n do
                  Drop (Fan et al., 2019) uses group regularization            z =f(x;θ)
                                                                                i     i
                  to enable structured pruning at inference time. On           if entropy(zi) < S then
                  the knowledge distillation side, TinyBERT (Jiao                 return zi
                  et al., 2019) and DistilBERT (Sanh et al., 2019)             endif
                  both distill BERT into a smaller transformer-based        endfor
                  model, and Tang et al. (2019) distill BERT into           return zn
                  even smaller non-transformer-based models.
                    Our work is inspired by Cambazoglu et al.
                  (2010), Teerapittayanon et al. (2017), and Huang        3.2   DeeBERTatInference
                  et al. (2018), but mainly differs from previous work      The way DeeBERT works at inference time is
                  in that we focus on improving model efﬁciency           showninAlgorithm1. Wequantify an off-ramp’s
                  with minimal quality degradation.                       conﬁdenceinitsprediction using the entropy of the
                  3   Early Exit for BERT inference                       output probability distribution zi. When an input
                                                                          sample x arrives at an off-ramp, the off-ramp com-
                  DeeBERT modiﬁes ﬁne-tuning and inference of             pares the entropy of its output distribution z with a
                  BERTmodels,leavingpre-training unchanged. It                                                        i
                                                                          preset threshold S to determine whether the sample
                  adds one off-ramp for each transformer layer. An        should be returned here or sent to the next trans-
                  inference sample can exit earlier at an off-ramp,       former layer.
                  without going through the rest of the transformer         It is clear from both intuition and experimenta-
                  layers. The last off-ramp is the classiﬁcation layer    tion that a larger S leads to a faster but less accurate
                  of the original BERT model.                             model, and a smaller S leads to a more accurate
                  3.1   DeeBERTatFine-Tuning                              but slower one. In our experiments, we choose S
                  We start with a pre-trained BERT model with n           based on this principle.
                  transformer layers and add n off-ramps to it. For         Wealso explored using ensembles of multiple
                  ﬁne-tuning on a downstream task, the loss function      layers instead of a single layer for the off-ramp,
                  of the ith off-ramp is                                  but this does not bring signiﬁcant improvements.
                                                                          Thereason is that predictions from different layers
                      Li(D;θ) = 1        X H(y,fi(x;θ)),           (1)    are usually highly correlated, and a wrong predic-
                                   |D|                                    tion is unlikely to be “ﬁxed” by the other layers.
                                       (x,y)∈D                            Therefore, westicktothesimpleyetefﬁcientsingle
                  where D is the ﬁne-tuning training set, θ is the        output layer strategy.
                  collection of all parameters, (x,y) is the feature–
                  label pair of a sample, H is the cross-entropy loss     4   Experiments
                                                        th
                  function, and fi is the output of the i off-ramp.
                    Thenetworkis ﬁne-tuned in two stages:                 4.1   Experimental Setup
                  1. Updatetheembeddinglayer,alltransformerlay-           WeapplyDeeBERTtobothBERTandRoBERTa,
                     ers, and the last off-ramp with the loss function    and conduct experiments on six classiﬁcation
                     L . This stage is identical to BERT ﬁne-tuning       datasets from the GLUE benchmark (Wang et al.,
                       n                                                  2018): SST-2, MRPC, QNLI, RTE, QQP, and
                     in the original paper (Devlin et al., 2019).
                                                                          MNLI. Our implementation of DeeBERT is
                  2. Freeze all parameters ﬁne-tuned in the ﬁrst          adapted from the HuggingFace Transformers Li-
                     stage, and then update all but the last off-         brary (Wolf et al., 2019). Inference runtime mea-
                                                  P
                     rampwiththeloss function        n−1Li. The rea-      surementsareperformedonasingleNVIDIATesla
                                                     i=1
                     son for freezing parameters of transformer lay-      P100 graphics card.     Hyperparameters such as
                     ers is to keep the optimal output quality for the    hidden-state size, learning rate, ﬁne-tune epoch,
                     last off-ramp; otherwise, transformer layers are     and batch size are kept unchanged from the library.
                     no longer optimized solely for the last off-ramp,    There is no early stopping and the checkpoint after
                     generally worsening its quality.                     full ﬁne-tuning is chosen.
                                   SST-2         MRPC           QNLI           RTE           QQP          MNLI-(m/mm)
                                 Acc Time       F   Time     Acc Time       Acc Time      F    Time        Acc      Time
                                                 1                                         1
                                                                 BERT-base
                   Baseline      93.6 36.72s   88.2 34.77s   91.0 111.44s  69.9 61.26s   71.4 145min    83.9/83.0 202.84s
                   DistilBERT   −1.4 −40%     −1.1 −40%     −2.6 −40%      −9.4 −40%     −1.1 −40%        −4.5     −40%
                                −0.2 −21%     −0.3 −14%     −0.1 −15%      −0.4 −9%      −0.0 −24%      −0.0/−0.1 −14%
                   DeeBERT      −0.6 −40%     −1.3 −31%     −0.7 −29%      −0.6 −11%     −0.1 −39%      −0.8/−0.7 −25%
                                −2.1 −47%     −3.0 −44%     −3.1 −44%      −3.2 −33%     −2.0 −49%      −3.9/−3.8 −37%
                                                               RoBERTa-base
                   Baseline      94.3 36.73s   90.4 35.24s   92.4 112.96s  67.5 60.14s   71.8 152min    87.0/86.3 198.52s
                   LayerDrop    −1.8 −50%       -     -       -      -       -     -       -     -        −4.1     −50%
                                +0.1 −26%     +0.1 −25%     −0.1 −25%      −0.6 −32%     +0.1 −32%      −0.0/−0.0 −19%
                   DeeBERT      −0.0 −33%     +0.2 −28%     −0.5 −30%      −0.4 −33%     −0.0 −39%      −0.1/−0.3 −23%
                                −1.8 −44%     −1.1 −38%     −2.5 −39%      −1.1 −35%     −0.6 −44%      −3.9/−4.1 −29%
                 Table 1: Comparison between baseline (original BERT/RoBERTa), DeeBERT, and other acceleration methods.
                 LayerDroponlyreports results on SST-2 and MNLI. Time savings of DistilBERT and LayerDrop are estimated by
                 reported model size reduction.
                 4.2   MainResults                                         to drop gradually. The turning point typically
                 Wevary DeeBERT’s quality–efﬁciency trade-off              comes earlier for BERT than for RoBERTa,
                 by setting different entropy thresholds S, and com-       but after the turning point, the performance of
                 pare the results with other baselines in Table 1.         RoBERTadropsfasterthanforBERT.Therea-
                 Modelquality is measured on the test set, and the         son for this will be discussed in Section 4.4.
                 results are provided by the GLUE evaluation server.    • Occasionally, we observe spikes in the curves,
                 Efﬁciency is quantiﬁed with wall-clock inference          e.g., RoBERTa in SST-2, and both BERT and
                 runtime1 on the entire test set, where samples are
                 fed into the model one by one. For each run of Dee-       RoBERTainRTE.Weattributethistopossible
                 BERTonadataset,wechoosethreeentropythresh-                regularization brought by early exiting and thus
                 olds S based on quality–efﬁciency trade-offs on the       smaller effective model sizes, i.e., in some cases,
                 development set, aiming to demonstrate two cases:         using all transformer layers may not be as good
                 (1) the maximumruntimesavingswithminimalper-              as using only some of them.
                 formance drop (< 0.5%), and (2) the runtime sav-       ComparedwithotherBERTacceleration methods,
                 ings with moderate performance drop (2% − 4%).         DeeBERThasthefollowingtwoadvantages:
                 ChosenS values differ for each dataset.
                    Wealsovisualize the trade-off in Figure 2. Each
                 curve is drawn by interpolating a number of points,    • Instead of producing a ﬁxed-size smaller model
                 each of which corresponds to a different threshold        like DistilBERT (Sanh et al., 2019), Dee-
                 S. Since this only involves a comparison between          BERT produces a series of options for faster
                 different settings of DeeBERT, runtimeismeasured          inference, which users have the ﬂexibility to
                 on the development set.                                   choose from, according to their demands.
                    FromTable1andFigure2,weobservethefol-               • Unlike DistilBERT and LayerDrop (Fan et al.,
                 lowing patterns:                                          2019), DeeBERT does not require further pre-
                 • Despite differences in baseline performance,            training of the transformer model, which is much
                    both models show similar patterns on all               moretime-consuming than ﬁne-tuning.
                    datasets: the performance (accuracy/F score)
                                                            1
                    stays (mostly) the same until runtime saving        4.3   Expected Savings
                    reaches a certain turning point, and then starts    Asthemeasurementofruntimemightnotbestable,
                    1This includes both CPU and GPU runtime.            we propose another metric to capture efﬁciency,
                                base: SST-2          92.5      base: MRPC                         base: SST-2          92.5      base: MRPC
                                                                                        90                                       BERT
                       90                            90.0                                                              90.0      RoBERTa
                       85                            87.5                               80                             87.5
                      Accuracy (%)80                 F1 Score (%)85.0                   Accuracy (%)                  F1 Score (%)85.0
                                BERT                            BERT                    70                BERT
                       75       RoBERTa              82.5       RoBERTa                                   RoBERTa      82.5
                         0     25     50     75          0     25    50     75                      5        10                    5        10
                             Runtime Savings (%)            Runtime Savings (%)                    Exit Layer                     Exit Layer
                                 base: QNLI                     base: RTE                         base: QNLI                     base: RTE
                       90                            70                                 90                             70
                       80                            65                                 80                             65
                                                     60                                                                60
                      Accuracy (%)70                 Accuracy (%)55                     Accuracy (%)70                Accuracy (%)55
                                BERT                          BERT                                        BERT                          BERT
                       60       RoBERTa              50       RoBERTa                   60                RoBERTa      50               RoBERTa
                         0     25     50     75        0      25     50    75                       5        10                   5         10
                             Runtime Savings (%)           Runtime Savings (%)                     Exit Layer                    Exit Layer
                       90        base: QQP                     base: MNLI               90        base: QQP                     base: MNLI
                                                     85                BERT
                       80                                              RoBERTa          80                             80
                                                     80
                                                                                        70                             70
                       70                            75
                                                     70                                                                60
                      Accuracy (%)60                 Accuracy (%)                       F1 Score (%)60                Accuracy (%)
                                BERT                 65                                                   BERT         50               BERT
                       50       RoBERTa                                                 50                RoBERTa                       RoBERTa
                                                     60                                                                40
                         0     25     50     75        0      25     50    75                       5        10                   5         10
                             Runtime Savings (%)           Runtime Savings (%)                     Exit Layer                    Exit Layer
                    Figure 2: DeeBERT quality and efﬁciency trade-offs                Figure 4: Accuracy of each off-ramp for BERT-base
                    for BERT-base and RoBERTa-base models.                            and RoBERTa-base.
                                base: SST-2                    base: MRPC
                       80       BERT                 80        BERT                   measured saving in Figure 3. Overall, the curves
                       70       RoBERTa              70        RoBERTa
                       60                            60                               show a linear relationship between expected sav-
                       50                            50                               ings and measured savings, indicating that our re-
                       40                            40
                       30                            30                               ported runtime is a stable measurement of Dee-
                       20                            20                               BERT’sefﬁciency.
                      Measured Savings (%)10         Measured Savings (%)10
                        0                             0
                          0 102030405060708090           0 102030405060708090         4.4    Layerwise Analyses
                            Expected Savings (%)           Expected Savings (%)
                    Figure 3: Comparison between expected saving (x-                  In order to understand the effect of applying Dee-
                    axis) and actual measured saving (y-axis), using BERT-            BERTtobothmodels,weconductfurther analyses
                    base and RoBERTa-base models.                                     oneachoff-ramplayer. Experiments in this section
                                                                                      are also performed on the development set.
                    called expected saving, deﬁned as                                 Output Performance by Layer.                For each off-
                                             P                                        ramp, we force all samples in the development
                                                n    i ×N
                                       1−Pi=1               i ,              (2)      set to exit here, measure the output quality, and
                                                n   n×N
                                                i=1         i                         visualize the results in Figure 4.
                    where n is the number of layers and Ni is the num-                   From the ﬁgure, we notice the difference be-
                    ber of samples exiting at layer i. Intuitively, ex-               tween BERTandRoBERTa. Theoutputquality of
                    pected saving is the fraction of transformer layer                BERTimprovesatarelatively stable rate as the in-
                    execution saved by using early exiting. The ad-                   dex of the exit off-ramp increases. The output qual-
                    vantage of this metric is that it remains invariant               ity of RoBERTa, on the other hand, stays almost
                    betweendifferent runs and can be analytically com-                unchanged (or even worsens) for a few layers, then
                    puted. For validation, we compare this metric with                rapidly improves, and reaches a saturation point be-
                                    large: SST-2                       large: SST-2                100%      BERT-base: SST-2        100%     RoBERTa-base: SST-2
                          95                                                                            S=0.01                             S=0.3
                                                                                                        Savings=25%                        Savings=34%
                          90                                90                                     50% AccDrop=0.2%                   50% AccDrop=0.1% 
                          85                                                                        0%                                 0%
                                                            80                                     100%                              100%
                          80                                                                            S=0.05                             S=0.55
                         Accuracy (%)                       Accuracy (%)                                Savings=34%                        Savings=55%
                          75        BERT                                        BERT               50% AccDrop=0.7%                   50% AccDrop=1.3% 
                                    RoBERTa                 70                  RoBERTa
                            0       25     50      75          0         10         20            Fraction of Dataset0%             Fraction of Dataset0%
                                 Runtime Savings (%)                    Exit Layer                 100%              S=0.4           100% S=0.6
                                     large: MRPC                       large: MRPC                                   Savings=61%           Savings=61%
                          92.5                              92                                     50%               AccDrop=5.8%     50% AccDrop=3.7% 
                                                                       BERT
                          90.0                              90         RoBERTa
                                                                                                    0%                                 0%
                                                            88                                           1   3   5   7   9   11             1   3   5   7   9  11
                          87.5                                                                                   Exit Layer                        Exit Layer
                                                            86
                         F1 Score (%)85.0                   F1 Score (%)                         Figure6: NumberofoutputsamplesbylayerforBERT-
                                      BERT                  84                                   base and RoBERTa-base. Each plot represents a sepa-
                          82.5        RoBERTa               82                                   rate entropy threshold S.
                              0      25     50     75          0         10         20
                                 Runtime Savings (%)                    Exit Layer
                       Figure 5: Results for BERT-large and RoBERTa-large.                       5     Conclusions and Future Work
                                                                                                 We propose DeeBERT, an effective method that
                       fore BERT does. This provides an explanation for                          exploits redundancy in BERT models to achieve
                       the phenomenon mentioned in Section 4.2: on the                           better quality–efﬁciency trade-offs. Experiments
                       samedataset, RoBERTa often achieves more run-                             demonstrate its ability to accelerate BERT’s and
                       time savings while maintaining roughly the same                           RoBERTa’s inference by up to ∼40%, and also
                       output quality, but then quality drops faster after                       reveal interesting patterns of different transformer
                       reaching the turning point.                                               layers in BERT models.
                           We also show the results for BERT-large and                               There are a few interesting questions left unan-
                       RoBERTa-large in Figure 5. From the two plots                             swered in this paper, which would provide inter-
                       ontheright, we observe signs of redundancy that                           esting future research directions: (1) DeeBERT’s
                       both BERT-large and RoBERTa-large share: the                              training method, while maintaining good quality in
                       last several layers do not show much improvement                          the last off-ramp, reduces model capacity available
                       compared with the previous layers (performance                            for intermediate off-ramps; it would be important
                       even drops slightly in some cases). Such redun-                           to look for a method that achieves a better balance
                       dancy can also be seen in Figure 4.                                       between all off-ramps. (2) The reasons why some
                                                                                                                                                 2
                       NumberofExitingSamplesbyLayer. Wefur-                                     transformer layers appear redundant and why Dee-
                       ther show the fraction of samples exiting at each                         BERTconsiders some samples easier than others
                       off-ramp for a given entropy threshold in Figure 6.                       remain unknown; it would be interesting to fur-
                           Entropy threshold S = 0 is the baseline, and                          ther explore relationships between pre-training and
                       all samples exit at the last layer; as S increases,                       layer redundancy, samplecomplexityandexitlayer,
                       gradually more samples exit earlier. Apart from                           and related characteristics.
                       the obvious, we observe additional, interesting pat-                      Acknowledgment
                       terns: if a layer does not provide better-quality
                       output than previous layers, such as layer 11 in                          Wethank anonymous reviewers for their insight-
                       BERT-baseandlayers 2–4 and 6 in RoBERTa-base                              ful suggestions. We also gratefully acknowledge
                       (which can be seen in Figure 4, top left), it is typi-                    funding support from the Natural Sciences and En-
                       cally chosen by very few samples; popular layers                          gineering Research Council (NSERC) of Canada.
                       are typically those that substantially improve over                       Computational resources used in this work were
                       previous layers, such as layer 7 and 9 in RoBERTa-                        provided, in part, by the Province of Ontario, the
                       base. This shows that an entropy threshold is able                        GovernmentofCanadathroughCIFAR,andcom-
                       to choose the fastest off-ramp among those with                           panies sponsoring the Vector Institute.
                       comparable quality, and achieves a good trade-off                             2For example, the ﬁrst and last four layers of RoBERTa-
                       between quality and efﬁciency.                                            base on SST-2 (Figure 4, top left).
                      References                                                            ShengShen,ZhenDong,JiayuYe,LinjianMa,Zhewei
                      B. Barla Cambazoglu, Hugo Zaragoza, Olivier                               Yao, AmirGholami,MichaelW.Mahoney,andKurt
                         Chapelle, Jiang Chen, Ciya Liao, Zhaohui Zheng,                        Keutzer. 2019. Q-BERT: Hessian based ultra low
                         and Jon Degenhardt. 2010. Early exit optimizations                     precision quantization of BERT. arXiv:1909.05840.
                         for additive machine learned ranking systems. In                   Raphael Tang, Yao Lu, and Jimmy Lin. 2019. Natu-
                         Proceedings of the Third ACM International Con-                        ral language generation for effective knowledge dis-
                         ference on Web Search and Data Mining (WSDM                            tillation.  In Proceedings of the 2nd Workshop on
                         2010), pages 411–420, New York, New York.                              Deep Learning Approaches for Low-Resource NLP
                                                                                               (DeepLo 2019), pages 202–208, Hong Kong, China.
                      Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
                         Kristina Toutanova. 2019.          BERT: Pre-training of           Surat Teerapittayanon, Bradley McDanel, and Hsiang-
                         deep bidirectional transformers for language under-                    Tsung Kung. 2017.             BranchyNet:        Fast infer-
                         standing. In Proceedings of the 2019 Conference of                     ence via early exiting from deep neural networks.
                         the North American Chapter of the Association for                      arXiv:1709.01686.
                         Computational Linguistics: Human Language Tech-                    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
                         nologies, Volume 1 (Long and Short Papers), pages                      Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
                         4171–4186, Minneapolis, Minnesota.                                     Kaiser, and Illia Polosukhin. 2017. Attention is all
                      Angela Fan, Edouard Grave, and Armand Joulin. 2019.                       you need. In Advances in Neural Information Pro-
                         Reducing transformer depth on demand with struc-                       cessing Systems 30, pages 5998–6008.
                         tured dropout. arXiv:1909.11556.                                   Elena Voita, David Talbot, Fedor Moiseev, Rico Sen-
                      GaoHuang, Danlu Chen, Tianhong Li, Felix Wu, Lau-                         nrich, and Ivan Titov. 2019. Analyzing multi-head
                         rens van der Maaten, and Kilian Weinberger. 2018.                      self-attention: Specialized heads do the heavy lift-
                         Multi-scale dense networks for resource efﬁcient im-                   ing, the rest can be pruned. In Proceedings of the
                         age classiﬁcation. In Proceedings of the 6th Inter-                    57th Annual Meeting of the Association for Com-
                         national Conference on Learning Representations,                       putational Linguistics, pages 5797–5808, Florence,
                         Vancouver, BC, Canada.                                                 Italy.
                      Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,                     Alex Wang, Amanpreet Singh, Julian Michael, Fe-
                         Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.                          lix Hill, Omer Levy, and Samuel Bowman. 2018.
                         2019. TinyBERT: Distilling BERT for natural lan-                       GLUE: A multi-task benchmark and analysis plat-
                         guage understanding. arXiv:1909.10351.                                 form for natural language understanding.             In Pro-
                                                                                                ceedings of the 2018 EMNLP Workshop Black-
                      Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-                       boxNLP: Analyzing and Interpreting Neural Net-
                         dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,                          works for NLP, pages 353–355, Brussels, Belgium.
                         Luke Zettlemoyer, and Veselin Stoyanov. 2019.                      Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
                         RoBERTa: A robustly optimized BERT pretraining                         Chaumond, Clement Delangue, Anthony Moi, Pier-
                         approach. arXiv:1907.11692.                                            ric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-
                                                                                                icz, and Jamie Brew. 2019. HuggingFace’s Trans-
                      Paul Michel, Omer Levy, and Graham Neubig. 2019.                          formers: State-of-the-art natural language process-
                         Are sixteen heads really better than one?              In Ad-          ing. arXiv:1910.03771.
                         vances in Neural Information Processing Systems
                         32, pages 14014–14024.                                             Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-
                                                                                                bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.
                      Matthew Peters, Mark Neumann, Mohit Iyyer, Matt                           XLNet: generalized autoregressive pretraining for
                         Gardner, Christopher Clark, Kenton Lee, and Luke                       language understanding. arXiv:1906.08237.
                         Zettlemoyer. 2018. Deep contextualized word repre-                 Matthew D. Zeiler and Rob Fergus. 2014. Visualizing
                         sentations. In Proceedings of the 2018 Conference                      and understanding convolutional networks. In Pro-
                         of the North American Chapter of the Association                       ceedings of the 13th European Conference on Com-
                         for Computational Linguistics: Human Language                                                                                 ¨
                         Technologies, Volume 1 (Long Papers), pages 2227–                      puter Vision (ECCV 2014), pages 818–833, Zurich,
                         2237, New Orleans, Louisiana.                                          Switzerland.
                      Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
                         Dario Amodei, and Ilya Sutskever. 2019. Language
                         modelsareunsupervisedmultitasklearners. OpenAI
                         Blog.
                      Victor Sanh, Lysandre Debut, Julien Chaumond, and
                         Thomas Wolf. 2019.           DistilBERT, a distilled ver-
                         sion of BERT: smaller, faster, cheaper and lighter.
                         arXiv:1910.01108.
