                  A DetailsontheDataGeneration                              affirmation, self-correction, and context memory
                  Our dataset is generated encompassing a wide              tasks, models generate responses directly from the
                  range of 30 topics, which are medicine, health,           second turn of dialogue, utilizing the golden con-
                  history, science, technology, digital, automotive,        text from the first turn as historical dialogue in-
                  astronomy, geography, lifestyle, literature, politics,    formation. This approach is adopted due to the
                  physics, chemistry, biology, finance, stocks, law,        task characteristics, where the assessment of the
                  humanities, entertainment, music, gaming, fashion,        respective abilities begins from the second turn of
                  film and television, celebrities, sports, art, com-       dialogue. Whilemodelsneedtoutilizethedialogue
                  puter science, environment, and psychology. This          history from the first turn in subsequent turns, di-
                  variety ensures that our data spans a multitude of        rectly responding to the content of the first turn
                  diverse fields and areas of interest.                     lacks practical significance.
                     After generating preliminary data using GPT-4,         D Additionalresults utilizing
                  we manually filtered the data samples by human                 Qwen-72B-Chat
                  annotators to form the final dataset. The primary
                  criteria for curation are as follows:                     Weevaluated the top 5 models in our benchmark
                     1. Ensure that our dataset precisely adheres to        using an open-source Qwen-72B-Chat, and the re-
                  the data generation rules outlined for each specific      sults in Table 7 show that GPT-4 is still the most
                  task.                                                     powerful model and the rankings of GPT-4-Judge
                     2. Ensure that our dataset encompasses samples         and Qwen-72B-Judge are consistent. This also
                  from 30 different topics, with a minimum of 10            shows that Qwen-72B-Chat is a good alternative
                  distinct topics covered for each task.                    evaluator. It is open-source, free to use, and won’t
                     3. Remove similar dialogues with only varia-           be updated or taken down in the future.
                  tions in several keywords.                                E ModelDetails
                     4. Remove questions regarding real-time issues
                  (such as today’s weather) and those involving up-         Alldetailsabouttheevaluatedmodelsarepresented
                  to-date knowledge after 2022.                             in Table 8.
                     5. Removedialoguesthatcontaincommonsense
                  errors, offensive content, and any personal identity      F MoreCases
                  information.                                              Figures 35 to 45 show cases corresponding to each
                     Figures 8 to 20 show the prompts we utilize for        task, each reflecting the classical error of the model
                  data generation. When generating data for each            response. These cases show that our task design
                  task, we splice in a uniform initial prompt and           can accurately assess the corresponding ability of
                  a unique prompt for each task to ensure that the          LLMs.
                  generated data matches our ability and task require-
                  ments.                                                    G Fleiss’KappabetweenGPT-4and
                  B DetialsonDataStatistics                                      Humans
                  Table 6 demonstrates the statistics information for       To evaluate the agreement between humans and
                  each task as well as the overall statistics of MT-        GPT-4,wealsoprovideFleiss’Kappascore,which
                  Bench-101. Note that the number of words is cal-          is an inter-annotator agreement metric. Specifi-
                  culated from the golden context in the dataset.           cally, we compute (1) The Fleiss’ Kappa for the
                                                                            five raters; (2) The average Fleiss’ Kappa between
                  C DetailsonEvaluation                                     GPT-4 and each individual rater; (3) The Fleiss’
                                                                            KappabetweenGPT-4andthemajorityvoteof5
                  Figures 21 to 34 show the prompts we utilize for          humanannotators. (4) The Fleiss’ Kappa of GPT4
                  evaluation. For each task, we concatenate a uni-          and humans over all annotations. As shown in Ta-
                  form initial instruction, unique evaluation prompts       ble 9, the agreement between GPT-4 and humans
                  tailored to the specific task, and a consistent scor-     is still higher than that among humans.
                  ing format to ensure that the scoring criteria align
                  with our task requirements.
                     It’s also noteworthy that in format rephras-
                  ing, content rephrasing, anaphora resolution, self-
                                                                        7433
