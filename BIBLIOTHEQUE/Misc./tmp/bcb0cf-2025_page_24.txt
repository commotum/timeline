                    Published as a conference paper at ICLR 2025
                    phase aids in initializing the activations to more optimal states, which likely catalyzes faster and more stable
                    convergence during subsequent training.
                    E.3  IMPACT OF FIXED-POINT ITERATION ON DIFFERENT MODEL CONFIGURATIONS
                   The primary objective of this experiment is to rigorously assess the efficacy of incorporating Fixed-Point
                    Iteration (FPI) into various layers of our MIND model architecture. We focus on four distinct configurations
                    to perform this assessment (shown in Figure 1):
                        1. Model0: Astraightforward architecture comprised of Layer 1 → Layer 2 → Layer 3, devoid of FPI.
                        2. Model 1: Utilizes FPI exclusively in Layer 1.
                        3. Model 2: Employs FPI in both Layer 1 and Layer 2.
                        4. Model 3: Applies FPI across all layers (Layer 1 → Layer 2 → Layer 3).
                    Our hypothesis posits that the incorporation of FPI into an increasing number of layers will yield a com-
                    mensurate improvement in key performance metrics, notably test loss and accuracy. Specifically, we project
                    that Model 3 will exhibit superior performance relative to the other configurations, owing to the enhanced
                    complexity and optimization capabilities conferred by FPI. Note that all models are trained under identical
                    hyperparameter settings to ensure a fair comparison.
                    Figure 8 shows the frequency of test loss and accuracy across the four model configurations. The boxplots
                    provide a clear visualization of the performance differences, with Model 3 exhibiting the lowest test loss and
                    highest accuracy, as hypothesized. The inclusion of FPI in all layers allows for more effective optimization
                    and improved generalization capabilities. The progressive enhancement in performance from Model 0 to
                    Model 3 demonstrates the positive impact of FPI on the model’s learning capacity and ability to capture
                    complex patterns in the data.
                    Figure 8: Boxplots illustrating the distribution of test loss and accuracy across four distinct model configura-
                    tions in CIFAR-100. The configurations vary in the complexity and number of layers utilizing Fixed-Point
                    Iteration (FPI).
                    E.4  ROLE OF THE INTROSPECTION MODEL IN MIND MODEL’S PERFORMANCE AND ADAPTABILITY
                    Ourintrospectionnetworkalsoundergoesapre-trainingphaselastingthesamenumberofepochs. Wepreserve
                    the weights, as per Lillicrap et al. (2014), from these pre-training phases and use them as initialization points
                                                            24
