              Machine Super Intelligence
                  Doctoral Dissertation submitted to the
               Faculty of Informatics of the University of Lugano
             in partial fulﬁllment of the requirements for the degree of
                       Doctor of Philosophy
                         presented by
                       Shane Legg
                      under the supervision of
                    Prof. Dr. Marcus Hutter
                          June 2008
                Copyright © Shane Legg 2008
           This document is licensed under a Creative Commons
            Attribution-Share Alike 2.5 Switzerland License.
                                Dissertation Committee
            Prof. Dr. Marcus Hutter         Australian National University, Australia
            Prof. Dr. Jur¨ gen Schmidhuber                    IDSIA, Switzerland
                                           Technical University of Munich, Germany
            Prof. Dr. Fernando Pedone             University of Lugano, Switzerland
            Prof. Dr. Matthias Hauswirth          University of Lugano, Switzerland
            Prof. Dr. Marco Wiering             Utrecht University, The Netherlands
           Supervisor                                        PhD program director
           Prof. Dr. Marcus Hutter                       Prof. Dr. Fabio Crestani
           Contents
           Preface                                                                               i
               Thesis outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    iii
               Prerequisite knowledge . . . . . . . . . . . . . . . . . . . . . . . . . .      vi
               Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . .        vi
           1. Nature and Measurement of Intelligence                                            1
               1.1. Theories of intelligence . . . . . . . . . . . . . . . . . . . . . . .      3
               1.2. Deﬁnitions of human intelligence . . . . . . . . . . . . . . . . .          4
               1.3. Deﬁnitions of machine intelligence       . . . . . . . . . . . . . . . .    9
               1.4. Intelligence testing . . . . . . . . . . . . . . . . . . . . . . . . .     11
               1.5. Human intelligence tests . . . . . . . . . . . . . . . . . . . . . .       13
               1.6. Animal intelligence tests . . . . . . . . . . . . . . . . . . . . . .      15
               1.7. Machine intelligence tests . . . . . . . . . . . . . . . . . . . . .       16
               1.8. Conclusion      . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  22
           2. Universal Artiﬁcial Intelligence                                                 23
               2.1. Inductive inference . . . . . . . . . . . . . . . . . . . . . . . . .      23
               2.2. Bayes’ rule     . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25
               2.3. Binary sequence prediction        . . . . . . . . . . . . . . . . . . . .  27
               2.4. Solomonoﬀ’s prior and Kolmogorov complexity . . . . . . . . .              30
               2.5. Solomonoﬀ-Levin prior . . . . . . . . . . . . . . . . . . . . . . .        32
               2.6. Universal inference . . . . . . . . . . . . . . . . . . . . . . . . .      36
               2.7. Solomonoﬀ induction . . . . . . . . . . . . . . . . . . . . . . . .        38
               2.8. Agent-environment model . . . . . . . . . . . . . . . . . . . . .          39
               2.9. Optimal informed agents . . . . . . . . . . . . . . . . . . . . . .        43
               2.10. Universal AIXI agent . . . . . . . . . . . . . . . . . . . . . . . .      47
           3. Taxonomy of Environments                                                         53
               3.1. Passive environments . . . . . . . . . . . . . . . . . . . . . . . .       53
               3.2. Active environments . . . . . . . . . . . . . . . . . . . . . . . .        57
               3.3. Some common problem classes . . . . . . . . . . . . . . . . . .            60
               3.4. Ergodic MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . .         62
               3.5. Environments that admit self-optimising agents . . . . . . . . .           65
               3.6. Conclusion      . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  68
           4. Universal Intelligence Measure                                                   71
               4.1. A formal deﬁnition of machine intelligence . . . . . . . . . . . .         72
                  4.2. Universal intelligence of various agents . . . . . . . . . . . . . .         78
                  4.3. Properties of universal intelligence . . . . . . . . . . . . . . . .         83
                  4.4. Response to common criticisms . . . . . . . . . . . . . . . . . .            86
                  4.5. Conclusion      . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    93
              5. Limits of Computational Agents                                                    95
                  5.1. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . .        96
                  5.2. Prediction of computable sequences . . . . . . . . . . . . . . . .           98
                  5.3. Prediction of simple computable sequences . . . . . . . . . . . . 100
                  5.4. Complexity of prediction . . . . . . . . . . . . . . . . . . . . . . 102
                  5.5. Hard to predict sequences . . . . . . . . . . . . . . . . . . . . . 103
                  5.6. The limits of mathematical analysis         . . . . . . . . . . . . . . . 104
                  5.7. Conclusion      . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
              6. Temporal Diﬀerence Updating without a Learning Rate                              109
                  6.1. Temporal diﬀerence learning . . . . . . . . . . . . . . . . . . . . 110
                  6.2. Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
                  6.3. Estimating a small Markov process . . . . . . . . . . . . . . . . 115
                  6.4. A larger Markov process . . . . . . . . . . . . . . . . . . . . . . 117
                  6.5. Random Markov process . . . . . . . . . . . . . . . . . . . . . . 118
                  6.6. Non-stationary Markov process . . . . . . . . . . . . . . . . . . 119
                  6.7. Windy Gridworld . . . . . . . . . . . . . . . . . . . . . . . . . . 120
                  6.8. Conclusion      . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
              7. Discussion                                                                       125
                  7.1. Are super intelligent machines possible? . . . . . . . . . . . . . 126
                  7.2. How could intelligent machines be developed? . . . . . . . . . . 128
                  7.3. Is building intelligent machines a good idea? . . . . . . . . . . . 135
              A. Notation and Conventions                                                         139
              B. Ergodic MDPs admit self-optimising agents                                        143
                  B.1. Basic deﬁnitions      . . . . . . . . . . . . . . . . . . . . . . . . . . 143
                  B.2. Analysis of stationary Markov chains . . . . . . . . . . . . . . . 146
                  B.3. An optimal stationary policy . . . . . . . . . . . . . . . . . . . 152
                  B.4. Convergence of expected average value . . . . . . . . . . . . . . 155
              C. Deﬁnitions of Intelligence                                                       159
                  C.1. Collective deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . 159
                  C.2. Psychologist deﬁnitions       . . . . . . . . . . . . . . . . . . . . . . 161
                  C.3. AI researcher deﬁnitions . . . . . . . . . . . . . . . . . . . . . . 164
              Bibliography                                                                        167
              Index                                                                               179
          Mystics exult in mystery and want it to stay mysterious.
            Scientists exult in mystery for a diﬀerent reason:
                it gives them something to do.
              Richard Dawkins in The God Delusion
     Preface
     This thesis concerns the optimal behaviour of agents in unknown computable
     environments, also known as universal artiﬁcial intelligence. These theoretical
     agents are able to learn to perform optimally in many types of environments.
     Although they are able to optimally use prior information about the environ-
     mentif it is available, in many cases they also learn to perform optimally in the
     absence of such information. Moreover, these agents can be proven to upper
     bound the performance of general purpose computable agents. Clearly such
     agents are extremely powerful and general, hence the name universal artiﬁcial
     intelligence.
      That such agents can be mathematically deﬁned at all might come as a sur-
     prise to some. Surely then artiﬁcial intelligence has been solved? Not quite.
     The problem is that the theory behind these universal agents assumes inﬁnite
     computational resources. Although this greatly simpliﬁes the mathematical
     deﬁnitions and analysis, it also means that these models cannot be directly
     implemented as artiﬁcial intelligence algorithms. Eﬀorts have been made to
     scale these ideas down, however as yet none of these methods have produced
     practical algorithms that have been adopted by the mainstream. The main
     use of universal artiﬁcial intelligence theory thus far has been as a theoreti-
     cal tool with which to mathematically study the properties of machine super
     intelligence.
      The foundations of universal intelligence date back to the origins of philos-
     ophy and inductive inference. Universal artiﬁcial intelligence proper started
     with the work of Ray J. Solomonoﬀ in the 1960’s. Solomonoﬀ was considering
     the problem of predicting binary sequences. What he discovered was a for-
     mulation for an inductive inference system that can be proven to very rapidly
     learn to optimally predict any sequence that has a computable probability
     distribution. Not only is this theory astonishingly powerful, it also brings to-
     gether and elegantly formalises key philosophical principles behind inductive
     inference. Furthermore, by considering special cases of Solomonoﬀ’s model,
     one can recover well known statistical principles such as maximum likelihood,
     minimum description length and maximum entropy. This makes Solomonoﬀ’s
     model a kind of grand uniﬁed theory of inductive inference. Indeed, if it
     were not for its incomputability, the problem of induction might be considered
     solved. Whatever practical concerns one may have about Solomonoﬀ’s model,
     most would agree that it is nonetheless a beautiful blend of mathematics and
     philosophy.
                                       i
        PREFACE
          The main theoretical limitation of Solomonoﬀ induction is that it only
        addresses the problem of passive inductive learning, in particular sequence
        prediction. Whether the agent’s predictions are correct or not has no eﬀect on
        the future observed sequence. Thus the agent is passive in the sense that it
        is unable to inﬂuence the future. An example of this might be predicting the
        movement of the planets across the sky, or maybe the stock market, assuming
        that one is not wealthy enough to inﬂuence the market.
          In the more general active case the agent is able to take actions which
        may aﬀect the observed future. For example, an agent playing chess not only
        observes the other player, it is also able to make moves itself in order to
        increase its chances of winning the game. This is a very general setting in
        which seemingly any kind of goal directed problem can be framed. It is not
        necessary to assume, as is typically done in game theory, that the environment,
        in this case other player, plays optimally. We also do not assume that the
        behaviour of the environment is Markovian, as is typically done in control
        theory and reinforcement learning.
          In the late 1990’s Marcus Hutter extended Solomonoﬀ’s passive induction
        model to the active case by combining it with sequential decision theory. This
        produced a theory of universal agents, and in particular a universal agent for
        a very general class of interactive environments, known as the AIXI agent.
        Hutter was able to prove that the behaviour of universal agents converges to
        optimal in any setting where this is at all possible for a general agent, and
        that these agents are Pareto optimal in the sense that no agent can perform
        as well in all environments and strictly better in at least one. These are the
        strongest known results for a completely general purpose agent. Given that
        AIXI has such generality and extreme performance characteristics, it can be
        considered to be a theoretical model of a super intelligent agent.
          Unfortunately, even stronger results showing that AIXI converges to optimal
        behaviour rapidly, similar to Solomonoﬀ’s convergence result, have been shown
        to be impossible in some settings, and remain open questions in others. Indeed,
        many questions about universal artiﬁcial intelligence remain open. In part
        this is because the area is quite new with few people working in it, and partly
        because proving results about universal intelligent agents seems to be diﬃcult.
          The goal of this thesis is to explore some of the open issues surrounding
        universal artiﬁcial intelligence. In particular: In which settings the behaviour
        of universal agents converges to optimal, the way in which AIXI theory relates
        to the concept and deﬁnition of intelligence, the limitations that computable
        agents face when trying to approximate theoretical super intelligent agents
        such as AIXI, and ﬁnally some of the big picture implications of super intelli-
        gent machines and whether this is a topic that deserves greater study.
        ii
                                                                              PREFACE
           Thesis outline
           Much of the work presented in this thesis comes from prior publications. In
           some cases whole chapters are heavily based on prior publications, in other
           cases prior work is only mentioned in passing. Furthermore, while I wrote
           the text of the thesis, naturally not all of the ideas and work presented are
           myown. Besides the presented background material, many of the results and
           ideas in this thesis have been developed through collaboration with various
           colleagues, in particular my supervisor Marcus Hutter. This section outlines
           the contents of the thesis and also provides some guidance on the nature of
           my contribution to each chapter.
           1) Nature and Measurement of Intelligence.        Chapter 1 begins the thesis
           with the most fundamental question of all: What is intelligence? Amazingly,
           books and papers on artiﬁcial intelligence rarely delve into what intelligence
           actually is, or what artiﬁcial intelligence is trying to achieve.  When they
           do address the topic they usually just mention the Turing test and that the
           concept of intelligence is poorly deﬁned, before moving on to algorithms that
           presumably have this mysterious quality. As this thesis concerns theoretical
           models of systems that we claim to be extremely intelligent, we must ﬁrst ex-
           plore the diﬀerent tests and deﬁnitions of intelligence that have been proposed
           for humans, animals and machines. We draw from these an informal deﬁnition
           of intelligence that we will use throughout the rest of the thesis.
             This overview of the theory, deﬁnition and testing of intelligence is my own
           work. This chapter is based on (Legg and Hutter, 2007c), in particular the
           parts which built upon (Legg and Hutter 2007b; 2007a).
           2) Universal Artiﬁcial Intelligence.  At present AIXI is not widely known in
           academic circles, though it has captured the imagination of a community in-
           terested in new approaches to general purpose artiﬁcial intelligence, so called
           artiﬁcial general intelligence (AGI). However even within this community, it
           is clear that there is some confusion about AIXI and universal artiﬁcial intelli-
           gence. This may be attributable in part to the fact that current expositions of
           AIXI are diﬃcult for non-mathematicians to digest. As such, a less technical
           introduction to the subject would be helpful. Not only should this help clear
           up some misconceptions, it may also serve as an appetiser for the more techni-
           cal treatments that have been published by Hutter. Chapter 2 provides such
           an introduction. It starts with the basics of inductive inference and slowly
           builds up to the AIXI agent and its key theoretical properties.
             This introduction to universal artiﬁcial intelligence has not been published
           before, though small parts of it were derived from (Hutter et al., 2007)
           and (Legg, 1997). Section 2.6 is largely based on the material in (Hutter,
           2007a), and the sections that follow this on (Hutter, 2005).
                                                                                       iii
            PREFACE
            3) Optimality of AIXI.   Hutter has proven that universal agents converge
            to optimal behaviour in any environment where this is possible for a general
            agent. He further showed that the result holds for certain types of Markov
            decision processes, and claimed that this should generalise to related classes
            of environments. Formally deﬁning these environments and identifying the
            additional conditions for the convergence result to hold was left as an open
            problem. Indeed, it seems that nobody has ever documented the many abstract
            environment classes that are studied and formally shown how they are related
            to each other. In Chapter 3 we create such a taxonomy and identify the
            environment classes in which universal agents are able to learn to behave
            optimally. The diversity of these classes of environments adds weight to our
            claim that AIXI is super intelligent.
              Most of the classes of environments are well known, though their exact for-
            malisations as presented are my own. The proofs of the relationships between
            them and the resulting taxonomy of environment classes is my work. This
            chapter is largely based on (Legg and Hutter, 2004).
            4) Universal Intelligence Measure.  If AIXI really is an optimally intelligent
            machine, this suggests that we may be able to turn the problem around and
            use universal artiﬁcial intelligence theory to formally deﬁne a universal mea-
            sure of machine intelligence. In Chapter 4 we take the informal deﬁnition of
            intelligence from Chapter 1 and abstract and formalise it using ideas from
            the theory of universal artiﬁcial intelligence in Chapter 2. The result is an
            alternate characterisation of Hutter’s intelligence order relation. This gives us
            a formal deﬁnition of machine intelligence that we then compare with other
            formal deﬁnitions and tests of machine intelligence that have been proposed.
              The speciﬁc formulation of the universal intelligence measure is of my own
            creation. The chapter is largely based on (Legg and Hutter, 2007c), in partic-
            ular the parts of this paper which build upon (Legg and Hutter 2005b; 2006).
            5) Limits of Computational Agents.   Oneofthe key reasons for studying in-
            computable but elegant theoretical models, such as Solomonoﬀ induction and
            AIXI, is that it is hoped that these will someday guide us towards powerful
            computable models of artiﬁcial intelligence. Although there have been a num-
            ber of attempts at converting these universal theories into practical methods,
            the resulting methods have all been a mere shadow of their original founding
            theory. Is this because we have not yet seen how to properly convert these
            theories into practical algorithms, or are there more fundamental limitations
            at work?
              Chapter 5 explores this question mathematically. Speciﬁcally, it looks at
            the existence and nature of computable agents which are powerful and ex-
            tremely general. The results reveal a number of fundamental constraints on
            any endeavour to construct very general artiﬁcial intelligence algorithms.
            iv
                                                                        PREFACE
            The elementary results at the start of the chapter are already well known,
          nevertheless the proofs given are my own. The more signiﬁcant results towards
          the end are entirely original and are my own work. The chapter is based
          primarily on (Legg, 2006b) which built upon the results in (Legg, 2006a). The
          core results also appear with other related work in the book chapter (Legg
          et al., 2008).
          6) FundamentalTemporalDiﬀerenceLearning. Althoughderivingpractical
          theories based on universal artiﬁcial intelligence is problematic, there still exist
          manyopportunitiesfortheorytocontributetothedevelopmentofnewlearning
          techniques, albeit on a somewhat less grand scale. In Chapter 6 we derive an
          equation for temporal diﬀerence learning from statistical principles. We start
          with the variational principle and then bootstrap to produce an update-rule
          for discounted state value estimates. The resulting equation is similar to the
          standard equation for temporal diﬀerence learning with eligibility traces, so
          called TD(λ), however it lacks the parameter that speciﬁes the learning rate.
          In the place of this free parameter there is now an equation for the learning
          rate that is speciﬁc to each state transition. We experimentally test this new
          learning rule against TD(λ). Finally, we make some preliminary investigations
          into how to extend our new temporal diﬀerence algorithm to reinforcement
          learning.
            The derivation of the temporal diﬀerence learning rate comes from a col-
          lection of unpublished derivations by Hutter. I went through this collect of
          handwritten notes, checked the proofs and took out what seemed to be the
          most promising candidate for a new learning rule. The presented proof has
          some reworking for improved presentation. The implementation and testing of
          this update-rule is my own work, as is the extension to reinforcement learning
          bymergingitwithSarsa(λ) and Q(λ). These results were published in (Hutter
          and Legg, 2007).
          7) Discussion  The concluding discussion on the future development of ma-
          chine intelligence is my own. This has not been published before.
          Appendix A A description of the mathematical notation used.
          Appendix B A convergence proof for ergodic MDPs needed for key results in
          Chapter 2
          Appendix C This collection of deﬁnitions of intelligence, seemly the largest
          in existence, is my own work. This section of the appendix was based on (Legg
          and Hutter, 2007a).
            Some of my other publications which are only mentioned in passing in this
          thesis include (Smith et al., 1994; Legg, 1996; Cleary et al., 1996; Calude
                                                                                 v
            PREFACE
            et al., 2000; Legg et al., 2004; Legg and Hutter, 2005a; Hutter and Legg, 2006).
            Coverage of the research in this thesis in the popular scientiﬁc press includes
            New Scientist magazine (Graham-Rowe, 2005), Le Monde de l’intelligence
            (Fi´evet, 2005), as well as numerous blog and online newspaper articles.
            Prerequisite knowledge
            The thesis aims to be fairly self contained, however some knowledge of math-
            ematics, statistics and theoretical computer science is assumed. From math-
            ematics the reader should be familiar with linear algebra, calculus, basic set
            theory and logic. From statistics, basic probability theory and elementary
            distributions such as the uniform and binomial distributions. A knowledge of
            measuretheorywouldbebeneﬁcial, butis not essential. From theoretical com-
            puter science a knowledge of the basics such as Turing computation, universal
            Turing machines, incomputability and the halting problem are needed. The
            mathematical notation and conventions adopted are described in Appendix A.
            The reader may want to consult this before beginning Chapter 2 as this is
            where the mathematical material begins.
            Acknowledgements
            First and foremost I would like to thank my supervisor Marcus Hutter. Get-
            ting a PhD is a somewhat long process and I have appreciated his guidance
            throughout this endeavour. I am especially grateful for the way in which he
            has always gone through my work carefully and provided detailed feedback on
            where there was room for improvement. Not every graduate student receives
            such careful appraisal and guidance during this long voyage.
              Essentially all of the research contained in this thesis was carried out at the
            Dalle Molle Institute for Artiﬁcial Intelligence (IDSIA) near Lugano, Switzer-
            land. It has been a pleasure to work with such a talented group of people
            over the last 4 years. In particular I would like to thank Alexey Chernov for
            encouraging me to develop a few short proofs on the limits of computational
            prediction systems into a full length paper. For me, that was a turning point
            in my thesis.
              Aspecial thanks goes to my reading group: Jeﬀ Rose, Cyrus Hall, Giovanni
            Luca Ciampaglia, Katerina Barone-Adesi, Tom Schaul and Daan Wierstra.
            They went through most of my thesis ﬁnding typos and places where things
            were not well explained.  The thesis is no doubt far more intelligible due
            to their eﬀorts. Special thanks also to my mother Gail Legg and Christoph
            Kolodziejski for further proof reading eﬀorts.
              My research has beneﬁted from interaction with many other colleagues,
            both at IDSIA and other research centres, in particular Ju¨rgen Schmidhu-
            ber, Jan Poland, Daniil Ryabko, Faustino Gomez, Matteo Gagliolo, Frederick
            vi
                                                                         PREFACE
          Ducatelle, Alex Graves, Bram Bakker, Viktor Zhumatiy and Laurent Orseau.
          I would also like to thank the institute secretary, Cinzia Daldini, for her amaz-
          ing ability to ﬁnd solutions to all manner of issues. It made coming to work
          at IDSIA and living in Switzerland a breeze. Finally, thanks to etomchek for
          designing the beautiful electric sheep on the front cover, and releasing it under
          the creative commons licence. I always wanted a sheep on the cover of my PhD
          thesis.
            This research was funded by the Swiss National Science Foundation under
          grants 2100-67712.0 and 200020-107616. Many funding agencies are not willing
          to support such blue-sky research. Their backing has been greatly appreciated.
          Lugano, Switzerland, June 2008                                Shane Legg
                                                                                 vii
      PREFACE
      viii
     1. Nature and Measurement of
         Intelligence
        “Innumerable tests are available for measuring intelligence, yet no
        one is quite certain of what intelligence is, or even just what it is
        that the available tests are measuring.” Gregory (1998)
       What is intelligence? It is a concept that we use in our daily lives that
     seems to have a fairly concrete, though perhaps naive, meaning. We say that
     our friend who got an A in his calculus test is very intelligent, or perhaps our
     cat who has learnt to go into hiding at the ﬁrst mention of the word “vet”.
     Although this intuitive notion of intelligence presents us with no diﬃculties, if
     we attempt to dig deeper and deﬁne it in precise terms we ﬁnd the concept to
     be very diﬃcult to nail down. Perhaps the ability to learn quickly is central to
     intelligence? Or perhaps the total sum of one’s knowledge is more important?
     Perhaps communication and the ability to use language play a central role?
     What about “thinking” or the ability to perform abstract reasoning? How
     about the ability to be creative and solve problems? Intelligence involves a
     perplexing mixture of concepts, many of which are equally diﬃcult to deﬁne.
       Psychologists have been grappling with these issues ever since humans ﬁrst
     became fascinated with the nature of the mind. Debates have raged back and
     forth concerning the correct deﬁnition of intelligence and how best to measure
     the intelligence of individuals. These debates have in many instances been very
     heated as what is at stake is not merely a scientiﬁc deﬁnition, but a fundamen-
     tal issue of how we measure and value humans: Is one employee smarter than
     another? Are men on average more intelligent than women? Are white people
     smarter than black people? As a result intelligence tests, and their creators,
     have on occasion been the subject of intense public scrutiny. Simply deter-
     mining whether a test, perhaps quite unintentionally, is partly a reﬂection of
     the race, gender, culture or social class of its creator is a subtle, complex and
     often politically charged issue (Gould, 1981; Herrnstein and Murray, 1996).
     Not surprisingly, many have concluded that it is wise to stay well clear of this
     topic.
       In reality the situation is not as bad as it is sometimes made out to be.
     Although the details of the deﬁnition are debated, in broad terms a fair de-
     gree of consensus has been achieved about the scientiﬁc deﬁnition of human
     intelligence and how to measure it (Gottfredson, 1997a; Sternberg and Berg,
     1986). Indeed it is widely recognised that when standard intelligence tests are
     correctly applied and interpreted, they all measure approximately the same
                                           1
      1. Nature and Measurement of Intelligence
      thing (Gottfredson, 1997a). Furthermore, what they measure is both stable
      over time in individuals and has signiﬁcant predictive power, in particular for
      future academic performance and other mentally demanding pursuits. The is-
      sues that continue to draw debate are questions such as whether the tests test
      only a part or a particular type of intelligence, or whether they are somehow
      biased towards a particular group or set of mental skills. Great eﬀort has gone
      into dealing with these issues, but they are diﬃcult problems with no easy
      solutions.
       Somewhat disconnected from this exists a parallel debate over the nature
      of intelligence in the context of machines. While the debate is less politically
      charged, in some ways the central issues are even more diﬃcult. Machines can
      have physical forms, sensors, actuators, means of communication, information
      processing abilities and exist in environments that are totally unlike those that
      we experience. This makes the concept of “machine intelligence” particularly
      diﬃcult to get a handle on. In some cases, a machine may have properties that
      are similar to human intelligence, and so it might be reasonable to describe
      the machine as also being intelligent. In other situations this view is far too
      limited and anthropocentric. Ideally we would like to be able to measure the
      intelligence of a wide range of systems: humans, dogs, ﬂies, robots or even
      disembodied systems such as chat-bots, expert systems, classiﬁcation systems
      and prediction algorithms (Johnson, 1992; Albus, 1991).
       One response to this problem might be to develop speciﬁc kinds of tests
      for speciﬁc kinds of entities, just as intelligence tests for children diﬀer to
      intelligence tests for adults. While this works well when testing humans of
      diﬀerent ages, it comes undone when we need to measure the intelligence of
      entities which are profoundly diﬀerent to each other in terms of their cognitive
      capacities, speed, senses, environments in which they operate, and so on. To
      measure the intelligence of such diverse systems in a meaningful way we must
      step back from the speciﬁcs of particular systems and establish fundamentally
      what it is that we are really trying to measure.
       The diﬃculty of forming a highly general notion of intelligence is readily
      apparent. Consider, for example, that memory and numerical computation
      tasks were once regarded as deﬁning hallmarks of human intelligence. We now
      know that these tasks are absolutely trivial for a machine and do not test its
      intelligence in any meaningful sense. Indeed, even the mentally demanding
      task of playing chess can now be largely reduced to brute force search (Hsu
      et al., 1995). What else may in time be possible with relatively simple algo-
      rithms running on powerful machines is hard to say. What we can be sure
      of is that, as technology advances, our concept of intelligence will continue to
      evolve with it.
       How then are we to develop a concept of intelligence that is applicable to
      all kinds of systems? Any proposed deﬁnition must encompass the essence
      of human intelligence, as well as other possibilities, in a consistent way. It
      should not be limited to any particular set of senses, environments or goals,
      nor should it be limited to any speciﬁc kind of hardware, such as silicon or
      2
                           1.1. Theories of intelligence
     biological neurons. It should be based on principles which are fundamental
     and thus unlikely to alter over time. Furthermore, the deﬁnition of intelligence
     should ideally be formally expressed, objective, and practically realisable as
     an eﬀective test. Before attempting to construct such a formal deﬁnition in
     Chapter 4, in this chapter we will ﬁrst survey existing deﬁnitions, tests and
     theories of intelligence. We are particularly interested in common themes and
     general perspectives on intelligence that could be applicable to many kinds of
     systems, including machines.
     1.1. Theories of intelligence
     A central question in the study of intelligence concerns whether intelligence
     should be viewed as one ability, or many. On one side of the debate are
     the theories that view intelligence as consisting of many diﬀerent components
     and that identifying these components is important to understanding intelli-
     gence. Diﬀerent theories propose diﬀerent ways to do this. One of the ﬁrst was
     Thurstone’s “multiple-factors” theory which considers seven “primary mental
     abilities”: verbal comprehension, word ﬂuency, number facility, spatial visuali-
     sation, associative memory, perceptual speed and reasoning (Thurstone, 1938).
     Another approach is Sternberg’s “Triarchic Mind” which breaks intelligence
     down into analytical intelligence, creative intelligence, and practical intelli-
     gence (Sternberg, 1985), however this model is now considered outdated, even
     by Sternberg himself.
      Taking the number of components to an extreme is Guilford’s “Structure of
     Intellect” theory. Under this theory there are three fundamental dimensions:
     contents, operations, and products. Together these give rise to 120 diﬀerent
     categories (Guilford, 1967). In later work this increased to 150 categories. This
     theory has been criticised due to the fact that measuring such precise combi-
     nations of cognitive capacities in individuals seems to be infeasible and thus it
     is diﬃcult to experimentally study such a ﬁne-grained model of intelligence.
      Arecently popular approach is Gardner’s “multiple intelligences” where he
     argues that the components of human intelligence are suﬃciently separate
     that they are actually diﬀerent “intelligences”(Gardner, 1993). Based on the
     structure of the human brain he identiﬁes these intelligences to be linguistic,
     musical, logical-mathematical, spatial, bodily kinaesthetic, intra-personal and
     inter-personal intelligence. Although Gardner’s theory of multiple intelligences
     has certainly captured the imagination of the public, it remains to be seen to
     what degree it will have a lasting impact in professional circles.
      At the other end of the spectrum is the work of Spearman and those that
     have followed in his approach. Here intelligence is seen as a very general
     mental ability that underlies and contributes to all other mental abilities. As
     evidence they point to the fact that an individual’s performance levels in rea-
     soning, association, linguistic, spatial thinking, pattern identiﬁcation etc. are
     positively correlated. Spearman called this positive statistical correlation be-
                                       3
      1. Nature and Measurement of Intelligence
      tween diﬀerent mental abilities the “g-factor”, where g stands for “general
      intelligence”(Spearman, 1927). Because standard IQ tests measure a range of
      key cognitive abilities, from a collection of scores on diﬀerent cognitive tasks
      we can estimate an individual’s g-factor. Some who consider the generality
      of intelligence to be primary take the g-factor to be the very deﬁnition of
      intelligence (Gottfredson, 2002).
       Awell known reﬁnement to the g-factor theory due to Cattell is to distin-
      guish between “ﬂuid intelligence”, which is a very general and ﬂexible innate
      ability to deal with problems and complexity, and “crystallized intelligence”,
      which measures the knowledge and abilities that an individual has acquired
      over time (Cattell, 1987). For example, while an adolescent may have a similar
      level of ﬂuid intelligence to that of an adult, their level of crystallized intelli-
      gence is typically lower due to less life experience (Horn, 1970). Although it is
      diﬃcult to determine to what extent these two inﬂuence each other, the dis-
      tinction is an important one because it captures two distinct notions of what
      the word “intelligence” means.
       As the g-factor is simply the statistical correlation between diﬀerent kinds
      of mental abilities, it is not fundamentally inconsistent with the view that
      intelligence can have multiple aspects or dimensions. Thus a synthesis of the
      two perspectives is possible by viewing intelligence as a hierarchy with the g-
      factor at its apex and increasing levels of specialisation for the diﬀerent aspects
      of intelligence forming branches (Carroll, 1993). For example, an individual
      might have a high g-factor, which contributes to all of their cognitive abilities,
      but also have an especially well developed musical sense. This hierarchical
      view of intelligence is now quite popular (Neisser et al., 1996).
      1.2. Deﬁnitions of human intelligence
        “Viewed narrowly, there seem to be almost as many deﬁnitions of
        intelligence as there were experts asked to deﬁne it.” R. J. Stern-
        berg quoted in (Gregory, 1998)
       In this section and the next we will overview a range of deﬁnitions of in-
      telligence that have been given by psychologists. For an even more extensive
      collection of deﬁnitions of intelligence, indeed the largest collection that we
      are aware of, see Appendix C or visit our online collection (Legg and Hutter,
      2007a). Although deﬁnitions diﬀer, there are many reoccurring features; in
      some cases these are explicitly stated, while in others they are more implicit.
      Westart by considering ten deﬁnitions that take a similar perspective:
        “It seems to us that in intelligence there is a fundamental faculty, the
        alteration or the lack of which, is of the utmost importance for practi-
        cal life. This faculty is judgement, otherwise called good sense, practi-
        cal sense, initiative, the faculty of adapting oneself to circumstances.”
        Binet and Simon (1905)
      4
                       1.2. Deﬁnitions of human intelligence
       “The capacity to learn or to proﬁt by experience.” Dearborn quoted
       in (Sternberg, 2000)
       “Ability to adapt oneself adequately to relatively new situations in life.”
       Pinter quoted in (Sternberg, 2000)
       “Apersonpossessesintelligence insofar as he has learned, or can learn, to
       adjust himself to his environment.” Colvin quoted in (Sternberg, 2000)
       “We shall use the term ‘intelligence’ to mean the ability of an organism
       to solve new problems ...” Bingham (1937)
       “Aglobalconceptthatinvolvesanindividual’sabilitytoactpurposefully,
       think rationally, and deal eﬀectively with the environment.” Wechsler
       (1958)
       “Individuals diﬀer from one another in their ability to understand com-
       plex ideas, to adapt eﬀectively to the environment, to learn from expe-
       rience, to engage in various forms of reasoning, to overcome obstacles
       by taking thought.” American Psychological Association (Neisser et al.,
       1996)
       “...I prefer to refer to it as ‘successful intelligence.’ And the reason is
       that the emphasis is on the use of your intelligence to achieve success in
       your life. So I deﬁne it as your skill in achieving whatever it is you want
       to attain in your life within your sociocultural context — meaning that
       people have diﬀerent goals for themselves, and for some it’s to get very
       goodgradesinschoolandtodowellontests,andforothersitmightbeto
       become a very good basketball player or actress or musician.” Sternberg
       (2003)
       “Intelligence is part of the internal environment that shows through at
       the interface between person and external environment as a function of
       cognitive task demands.” R. E. Snow quoted in (Slatter, 2001)
       “...certain set of cognitive capacities that enable an individual to adapt
       and thrive in any given environment they ﬁnd themselves in, and those
       cognitive capacities include things like memory and retrieval, and prob-
       lem solving and so forth. There’s a cluster of cognitive abilities that lead
       to successful adaptation to a wide range of environments.” Simonton
       (2003)
      Perhaps the most elementary common feature of these deﬁnitions is that
     intelligence is seen as a property of an individual who is interacting with an
     external environment, problem or situation. Indeed, at least this much is
     common to practically all proposed deﬁnitions of intelligence.
      Another common feature is that an individual’s intelligence is related to
     their ability to succeed or proﬁt. This implies the existence of some kind of
                                       5
             1. Nature and Measurement of Intelligence
             objective or goal. What the goal is, is not speciﬁed, indeed individuals’ goals
             may be varied. The important thing is that the individual is able to carefully
             choose their actions in a way that leads to them accomplishing their goals.
             The greater this capacity to succeed with respect to various goals, the greater
             the individual’s intelligence.
               The strong emphasis on learning, adaption and experience in these deﬁni-
             tions implies that the environment is not fully known to the individual and
             may contain new situations that could not have been anticipated in advance.
             Thus intelligence is not the ability to deal with a fully known environment,
             but rather the ability to deal with some range of possibilities which cannot be
             wholly anticipated. What is important then is that the individual is able to
             quickly learn and adapt so as to perform as well as possible over a wide range
             of environments, situations, tasks and problems. Collectively we will refer to
             these as “environments”, similar to some of the deﬁnitions above.
               Bringing these key features together gives us what we believe to be the
             essence of intelligence in its most general form:
                   Intelligence measures an agent’s ability to achieve goals in a wide
                   range of environments.
             Wetakethistobeourinformalworkingdeﬁnitionofintelligence for this thesis.
               The remainder of this section considers a range of other deﬁnitions that are
             not as strongly connected to our adopted deﬁnition. Usually it is not that they
             are entirely incompatible with our deﬁnition, but rather they stress diﬀerent
             aspects of intelligence.  The following deﬁnition is an especially interesting
             deﬁnition as it was given as part of a group statement signed by 52 experts in
             the ﬁeld. As such it obviously represents a fairly mainstream perspective:
                   “Intelligence is a very general mental capability that, among other things,
                   involves the ability to reason, plan, solve problems, think abstractly,
                   comprehend complex ideas, learn quickly and learn from experience.”
                   Gottfredson (1997a)
               Reasoning, planning, solving problems, abstract thinking, learning from ex-
             perience and so on, these are all mental abilities that allow us to successfully
             achieve goals. If we were missing any one of these capacities, we would clearly
             be less able to successfully deal with such a wide range of environments. Thus,
             these capacities are implicit in our deﬁnition also. The diﬀerence is that our
             deﬁnition does not attempt to specify what capabilities might be needed, some-
             thing which is clearly very diﬃcult and would depend on the particular tasks
             that the agent must deal with. Our approach is to consider intelligence to be
             the eﬀect of capacities such as those listed above. It is not the result of having
             any speciﬁc set of capacities. Indeed, intelligence could also be the eﬀect of
             many other capacities, some of which humans may not have. In summary,
             our deﬁnition is not in conﬂict with the above deﬁnition, rather it is that our
             deﬁnition is more abstract and general.
             6
                                                1.2. Deﬁnitions of human intelligence
               “...in its lowest terms intelligence is present where the individual an-
               imal, or human being, is aware, however dimly, of the relevance of his
               behaviour to an objective. Many deﬁnitions of what is indeﬁnable have
               been attempted by psychologists, of which the least unsatisfactory are 1.
               the capacity to meet novel situations, or to learn to do so, by new adap-
               tive responses and 2. the ability to perform tests or tasks, involving the
               grasping of relationships, the degree of intelligence being proportional to
               the complexity, or the abstractness, or both, of the relationship.” Drever
               (1952)
            This deﬁnition has many similarities to ours. Firstly, it emphasises the
          agent’s ability to choose its actions so as to achieve an objective, or in our
          terminology, a goal. It then goes on to stress the agent’s ability to deal with
          situations which have not been encountered before. In our terminology, this is
          the ability to deal with a wide range of environments. Finally, this deﬁnition
          highlights the agent’s ability to perform tests or tasks, something which is
          entirely consistent with our performance orientated perspective of intelligence.
               “Intelligence is not a single, unitary ability, but rather a composite of sev-
               eral functions. The term denotes that combination of abilities required
               for survival and advancement within a particular culture.” Anastasi
               (1992)
            This deﬁnition does not specify exactly which capacities are important, only
          that they should enable the individual to survive and advance with the culture.
          As such this is a more abstract “success” orientated deﬁnition of intelligence,
          like ours. Naturally, culture is a part of the agent’s environment, though only
          complex environments with other agents would have true culture.
               “The ability to carry on abstract thinking.”   L. M. Terman quoted
               in (Sternberg, 2000)
            This is not really much of a deﬁnition as it simply shifts the problem of
          deﬁning intelligence to the problem of deﬁning abstract thinking. The same
          is true of many other deﬁnitions that refer to things such as imagination,
          creativity or consciousness. The following deﬁnition has a similar problem:
               “Thecapacityforknowledge, andknowledgepossessed.” Henmon(1921)
            Whatexactly constitutes “knowledge”, as opposed to perhaps data or infor-
          mation? For example, does a library contain a lot of knowledge, and if so, is it
          intelligent? Or perhaps the internet? Modern concepts of the word knowledge
          stress the fact that the information has to be in some sense properly contex-
          tualised so that it has meaning. Deﬁning this more precisely appears to be
                                                                                  7
      1. Nature and Measurement of Intelligence
      diﬃcult however. Because this deﬁnition of intelligence dates from 1921, per-
      haps it reﬂects pre-information age thinking when computers with vast storage
      capacities did not exist.
       Nonetheless, our deﬁnition of intelligence is not entirely inconsistent with
      the above deﬁnition in that an individual may be required to know many
      things, or have a signiﬁcant capacity for knowledge, in order to perform well
      in some environments. However, our deﬁnition is narrower in that knowledge,
      or the capacity for knowledge, is not by itself suﬃcient. We require that the
      knowledgecanbeusedeﬀectively. Indeed, unless information can be eﬀectively
      utilised for various purposes, it seems reasonable to consider it to be merely
      “data”, rather than “knowledge”.
        “The capacity to acquire capacity.” H. Woodrow quoted in (Sternberg,
        2000)
       Thedeﬁnition of Woodrow is typical of those that emphasise not the current
      ability of the individual, but rather the individual’s ability to expand and
      develop new abilities. This is a fundamental point of divergence for many
      views on intelligence. Consider the following question: Is a young child as
      intelligent as an adult? From one perspective, children are very intelligent
      because they can learn and adapt to new situations quickly. On the other
      hand, a child is unable to do many things due to a lack of knowledge and
      experience and thus will make mistakes an adult would know to avoid. These
      need not just be physical acts, they could also be more subtle things like
      errors in reasoning as their mind, while very malleable, has not yet matured.
      In which case, perhaps their intelligence is currently low, but will increase with
      time and experience?
       Fundamentally, this diﬀerence in perspective is a question of time scale:
      Must an agent be able to tackle some task immediately, or perhaps after a
      short period of time during which learning can take place, or perhaps it only
      matters that they can eventually learn to deal with the problem? Being able
      to deal with a diﬃcult problem immediately is a matter of experience, rather
      than intelligence. While being able to deal with it in the very long run might
      not require much intelligence at all, for example, simply trying a vast number
      of possible solutions might eventually produce the desired results. Intelligence
      then seems to be the ability to adapt and learn as quickly as possible given
      the constraints imposed by the problem at hand.
        “Intelligence is a general factor that runs through all types of perfor-
        mance.” A. Jensen
       At ﬁrst this might not look like a deﬁnition of intelligence, but it makes an
      important point: Intelligence is not really the ability to do anything in partic-
      ular, rather it is a very general ability that aﬀects many kinds of performance.
      Conversely, by measuring many diﬀerent kinds of performance we can estimate
      an individual’s intelligence. This is consistent with our deﬁnition’s emphasis
      on the agent’s ability to perform well in many environments.
      8
                      1.3. Deﬁnitions of machine intelligence
       “Intelligence is what is measured by intelligence tests.” Boring (1923)
      Boring’s famous deﬁnition of intelligence takes this idea a step further. If in-
     telligence is not the ability to do anything in particular, but rather an abstract
     ability that indirectly aﬀects performance in many tasks, then perhaps it is
     most concretely described as the ability to do the kind of abstract problems
     that appear in intelligence tests? In which case, Boring’s deﬁnition is not as
     facetious as it ﬁrst appears. This deﬁnition also highlights the fact that the
     concept of intelligence, and how it is measured, are intimately related. In the
     context of this paper we refer to these as deﬁnitions of intelligence, and tests of
     intelligence, respectively, although in some cases the distinction is not sharp.
     1.3. Deﬁnitions of machine intelligence
     The following sample of informal deﬁnitions of machine intelligence capture a
     range of perspectives. There also exist several formal deﬁnitions and tests of
     machine intelligence, however we will deal with those in Chapter 4. We begin
     with ﬁve deﬁnitions that have clear connections to our informal deﬁnition:
       “...the mental ability to sustain successful life.” K. Warwick quoted
       in (Asohan, 2003)
       “...doing well at a broad range of tasks is an empirical deﬁnition of
       ‘intelligence’ ” Masum et al. (2002)
       “Intelligence is the computational part of the ability to achieve goals in
       the world. Varying kinds and degrees of intelligence occur in people,
       many animals and some machines.” McCarthy (2004)
       “Any system ...that generates adaptive behaviour to meet goals in a
       range of environments can be said to be intelligent.” Fogel (1995)
       “...the ability of a system to act appropriately in an uncertain environ-
       ment, where appropriate action is that which increases the probability
       of success, and success is the achievement of behavioral subgoals that
       support the system’s ultimate goal.” Albus (1991)
      The position taken by Albus is especially similar to ours. Although the
     quote above does not explicitly mention the need to be able to perform well in
     a wide range of environments, at a later point in the same paper he mentions
     the need to be able to succeed in a “large variety of circumstances”.
       “Intelligent systems are expected to work, and work well, in many dif-
       ferent environments. Their property of intelligence allows them to max-
       imize the probability of success even if full knowledge of the situation
       is not available. Functioning of intelligent systems cannot be considered
       separately from the environment and the concrete situation including
       the goal.” Gudwin (2000)
                                       9
      1. Nature and Measurement of Intelligence
       While this deﬁnition is consistent with the position we have taken, when
      trying to actually test the intelligence of an agent Gudwin does not believe
      that a “black box” behaviour based approach is suﬃcient, rather his approach
      is to look at the “...architectural details of structures, organizations, processes
      and algorithms used in the construction of the intelligent systems,” (Gudwin,
      2000). Our perspective is simply to not care whether an agent looks intelligent
      on the inside. If it is able to perform well in a wide range of environments,
      that is all that matters.
        “We deﬁne two perspectives on artiﬁcial system intelligence: (1) na-
        tive intelligence, expressed in the speciﬁed complexity inherent in the
        information content of the system, and (2) performance intelligence, ex-
        pressed in the successful (i.e., goal-achieving) performance of the system
        in a complicated environment.” Horst (2002)
       Here we see two distinct notions of intelligence, a performance based one
      and an information content one. This is similar to the distinction between
      ﬂuid intelligence and crystallized intelligence made by the psychologist Cattell
      (see Section 1.1). The performance based notion of intelligence is similar to
      our deﬁnition with the exception that performance is measured in a complex
      environmentratherthanacrossawiderangeofenvironments. Thisperspective
      appears in some other deﬁnitions also,
        “...the ability to solve hard problems.” Minsky (1985)
        “Achieving complex goals in complex environments” Goertzel (2006)
       The emphasis on complex goals and environments is not really so diﬀerent
      to our “wide range of environments” in that any agent which could not achieve
      simple goals in simple environments presumably would not be considered in-
      telligent. One might argue that the ability to achieve truly complex goals in
      complex environments requires the ability to achieve simple ones, in which
      case the two perspectives are equivalent.
       Somedeﬁnitions emphasise not just the ability to perform well, but also the
      need for eﬃciency:
        “[An intelligent agent does what] is appropriate for its circumstances
        and its goal, it is ﬂexible to changing environments and changing goals,
        it learns from experience, and it makes appropriate choices given per-
        ceptual limitations and ﬁnite computation.” Poole et al. (1998)
        “...in any real situation behavior appropriate to the ends of the system
        and adaptive to the demands of the environment can occur, within some
        limits of speed and complexity.” Newell and Simon (1976)
        “Intelligence is the ability to use optimally limited resources – including
        time – to achieve goals.” Kurzweil (2000)
      10
                             1.4. Intelligence testing
       “Intelligence is the ability for an information processing agent to adapt
       to its environment with insuﬃcient knowledge and resources.” Wang
       (1995)
      We consider the addition of resource limitations to the deﬁnition of intel-
     ligence to be either superﬂuous, or wrong. In the ﬁrst case, if limited com-
     putational resources are a fundamental and unavoidable part of reality, which
     certainly seems to be the case, then their addition to the deﬁnition of intel-
     ligence is unnecessary. Perhaps the ﬁrst three deﬁnitions above fall into this
     category.
      On the other hand, if limited resources are not a fundamental restriction,
     for example a new model of computation was discovered that was vastly more
     powerful than the current model, then it would be odd to claim that the
     unbelievably powerful machines that would then result were not intelligent.
     Normallywedonotjudgetheintelligenceofsomethingrelativetotheresources
     it uses. For example, if a rat had human level learning and problem solving
     abilities, we would not think of the rat as being more intelligent than a human
     due to the fact that its brain was much smaller.
      While we do not consider eﬃciency to be a part of the deﬁnition of intelli-
     gence, this is not to say that considering the eﬃciency of agents is unimportant.
     Indeed, a key goal of artiﬁcial intelligence is to ﬁnd algorithms which have the
     greatest eﬃciency of intelligence, that is, which achieve the most intelligence
     per unit of computational resources consumed.
     1.4. Intelligence testing
     Having explored what intelligence is, we now turn to how it is measured.
     Contrary to popular public opinion, most psychologists believe that standard
     psychometric tests of intelligence, such as IQ tests, reliably measure some-
     thing important in humans (Neisser et al., 1996; Gottfredson, 1997b). In fact,
     standard intelligence tests are among the most statistically stable and reli-
     able psychological tests. Furthermore, it is well known that these scores are a
     good predictor of various things, such as academic performance. The question
     then is not whether these tests are useful or measure something meaningful,
     but rather whether what they measure is indeed “intelligence”. Some experts
     believe that they do, while others think that they only succeed in measuring
     certain aspects of, or types of, intelligence.
      There are many properties that a good test of intelligence should have. One
     important property is that the test should be repeatable, in the sense that it
     consistently returns about the same score for a given individual. For example,
     the test subject should not be able to signiﬁcantly improve their performance
     if tested again a short time later. Statistical variability can also be a problem
     in short tests. Longer tests help in this regard, however they are naturally
     more costly to administer.
                                      11
      1. Nature and Measurement of Intelligence
       Another important reliability factor is the bias that might be introduced by
      the individual administering the test. Purely written tests avoid this problem
      as there is minimal interaction between the tested individual and the tester.
      However, this lack of interaction also has disadvantages as it may mean that
      other sources of bias, such as cultural diﬀerences, language problems or even
      something as simple as poor eyesight, might not be properly identiﬁed. Thus,
      even with a written test the individual being tested should ﬁrst be examined
      by an expert in order to ensure that the test is appropriate.
       Cultural bias in particular is a diﬃcult problem, and tests should be de-
      signed to minimise this problem where possible, or at least detect potential
      bias problems when they occur. One way to do this is to test each ability in
      multiple ways, for example both verbally and visually. While language is an
      obvious potential source of cultural bias, more subtle forms of bias are diﬃ-
      cult to detect and remedy. For example, diﬀerent cultures emphasise diﬀerent
      cognitive abilities and thus it is diﬃcult, perhaps impossible, to compare intel-
      ligence scores in a way that is truly objective. Indeed, this choice of emphasis
      is a key issue for any intelligence test, it depends on the perspective taken on
      what intelligence is.
       Anintelligence test should be valid in the sense that it appears to be testing
      what it claims it is testing for. One way to check this is to show that the test
      produces results consistent with other manifestations of intelligence. A test
      should also have predictive power, for example the ability to predict future
      academic performance, or performance in other cognitively demanding tasks.
      This ensures that what is being measured is somehow meaningful, beyond just
      the ability to answer the questions in the test. Standard intelligence tests are
      thoroughly tested for years on the above criteria, and many others, before they
      are ready for wide spread use.
       Finally, when testing large numbers of individuals, for example when testing
      army recruits, the cost of administering the test becomes important. In these
      cases less accurate but more economical test procedures may be used, such
      as purely written tests without any direct interaction between the individuals
      being tested and a psychologist.
       Standard intelligence tests, such as those described in the next section, are
      all examples of “static tests”. By this we mean that they test an individual’s
      knowledge and ability to solve one-oﬀ problems. They do not directly measure
      the ability to learn and adapt over time. If an individual was good at learning
      andadaptingthenwemightexpectthistobereﬂectedintheirtotalknowledge
      and thus be picked up in a static test. However, it could be that an individual
      has a great capacity to learn, but that this is not reﬂected in their knowledge
      due to limited education. In which case, if we consider the capacity to learn
      and adapt to be a deﬁning characteristic of intelligence, rather than the sum
      of knowledge, then to class an individual as unintelligent due to limited access
      to education would be a mistake.
       What is needed is a more direct test of an individual’s ability to learn and
      adapt: a so called “dynamic test”(Sternberg and Grigorenko, 2002) (for re-
      12
                          1.5. Human intelligence tests
     lated work see also Johnson-Laird and Wason, 1977). In a dynamic test the
     individual interacts over a period of time with the tester, who now becomes a
     kind of teacher. The tester’s task is to present the test subject with a series
     of problems. After each attempt at solving a problem, the tester provides
     feedback to the individual who then has to adapt their behaviour accordingly
     in order to solve the next problem.
      Although dynamic tests could in theory be very powerful, they are not yet
     well established due to a number of diﬃculties. One of the drawbacks is that
     they require a much greater degree of interaction between the test subject and
     the tester. This makes dynamic testing more costly to perform and increases
     the danger of tester bias.
     1.5. Human intelligence tests
     The ﬁrst modern style intelligence test was developed by the French psychol-
     ogist Alfred Binet in 1905. Binet believed that intelligence was best studied
     by looking at relatively complex mental tasks, unlike earlier tests developed
     by Francis Galton which focused on reaction times, auditory discrimination
     ability, physical coordination and so on. Binet’s test consisted of 30 short
     tasks related to everyday problems such as: naming parts of the body, com-
     paring lengths and weights, counting coins, remembering digits and deﬁnitions
     of words. For each task category there were a number of problems of increas-
     ing diﬃculty. The child’s results were obtained by normalising their raw score
     against peers of the same age. Initially his test was designed to measure
     the mental performance of children with learning problems (Binet and Simon,
     1905). Later versions were also developed for normal children (Binet, 1911). It
     wasfoundthatBinet’stestresults were a good predictor of children’s academic
     performance.
      Lewis Terman of Stanford University developed a version of Binet’s test in
     English. As the age norms for French children did not correspond well with
     American children, he revised Binet’s test in various ways, in particular he
     increased the upper age limit. This resulted in the now famous Stanford-Binet
     test (Terman and Merrill, 1950). This test formed the basis of a number of
     other intelligence tests, such as the Army Alpha and Army Beta tests which
     were used to classify recruits. Since its development, the Stanford-Binet has
     been periodically revised, with updated versions being widely used today.
      David Wechsler believed that the original Binet tests were too focused on
     verbal skills and thus disadvantaged certain otherwise intelligent individuals,
     for example the deaf or people who did not speak the test language as a
     ﬁrst language. To address this problem, he proposed that tests should con-
     tain a combination of both verbal and nonverbal problems. He also believed
     that in addition to an overall IQ score, a proﬁle should be produced show-
     ing the performance of the individual in the various areas tested. Borrowing
     signiﬁcantly from the Stanford-Binet, the US army Alpha test, and others,
                                      13
      1. Nature and Measurement of Intelligence
      he developed a range of tests targeting speciﬁc age groups from preschool-
      ers up to adults (Wechsler, 1958). Due in part to problems with revisions of
      the Stanford-Binet test in the 1960’s and 1970’s, Wechsler’s tests became the
      standard. They continue to be well respected and widely used.
       Modernversions of the Wechsler and the Stanford-Binet have a similar basic
      structure (Kaufman, 2000). Both test the individual in a number of verbal
      and non-verbal ways. In the case of a Stanford-Binet the test is broken up
      into ﬁve key areas: ﬂuid reasoning, knowledge, quantitative reasoning, visual-
      spatial processing, and working memory. In the case of the Wechsler Adult
      Intelligence Scale (WAIS-III), the verbal tests include areas such as knowledge,
      basic arithmetic, comprehension, vocabulary, and short term memory. Non-
      verbal tests include picture completion, spatial perception, problem solving,
      symbol search and object assembly.
       As part of an eﬀort to make intelligence tests more culture neutral John
      Raven developed the progressive matrices test (Raven, 2000). In this test each
      problem consists of a short sequence of basic shapes. For example, a circle
      in a box, then a circle with a cross in the middle followed by a circle with
      a triangle inside. The test subject then has to select from a second list the
      image that best continues the pattern. Simple problems have simple patterns,
      while diﬃcult problems have more subtle and complex patterns. In each case,
      the simplest pattern that can explain the observed sequence is the one that
      correctly predicts its continuation. Thus, not only is the ability to recognise
      patterns tested, but also the ability to evaluate the complexity of diﬀerent
      explanations and then correctly apply the philosophical principle of Occam’s
      razor (see Section 2.1). This will play a key role for us in later chapters.
       Today several diﬀerent versions of the Raven test exist designed for diﬀer-
      ent age groups and ability levels. As the tests depend strongly on the ability
      to identify abstract patterns, rather than knowledge, they are considered to
      be some of the most “g-loaded” intelligence tests available (see Section 1.1).
      The Raven tests remain in common use today, particularly when it is thought
      that culture or language bias could be an issue. The universality of abstract
      sequence prediction tests makes them potentially useful in the context of ma-
      chine intelligence, indeed we will see that some tests of machine intelligence
      take this approach.
       The intelligence quotient, or IQ, was originally introduced by Stern (1912).
      It was computed by taking the age of a child as estimated by their performance
      in an intelligence test, and then dividing this by their true biological age and
      multiplying by 100. Thus a 10 year old child whose mental performance was
      equal to that of a normal 12 year old, had an IQ of 120. As the concept of
      mental age has now been discredited, and was never applicable to adults any-
      way, modern IQ scores are simply normalised to a Gaussian distribution with
      a mean of 100. The standard deviation used varies: in the United States 15 is
      commonly used, while in Europe 25 is common. For children the normalising
      Gaussian is based on peers of the same age.
      14
                          1.6. Animal intelligence tests
      Whatever normalising distribution is used, by deﬁnition an individual’s IQ
     is always an indication of their cognitive performance relative to some larger
     group. Clearly this would be problematic in the context of machines where
     the performance of some machines could be many orders of magnitude greater
     than others. Furthermore, the distribution of machine performance would be
     continually changing due to advancing technology. Thus, for machine intelli-
     gence, an absolute measure is more meaningful than a traditional IQ type of
     measure.
      For an overview of the history of intelligence testing and the structure of
     modern tests, see (Kaufman, 2000).
     1.6. Animal intelligence tests
     Testing the intelligence of animals is of particular interest to us as it moves
     beyond strictly human focused concepts of intelligence and testing methods.
     Diﬃcult problems in human intelligence testing, such as bias due to language
     diﬀerences or physical handicap, become even more diﬃcult if we try to com-
     pare animals with diﬀerent perceptual and cognitive capacities. Even within
     a single species measurement is diﬃcult as it is not always obvious how to
     conduct the tests, or even what should be tested for. Furthermore, as humans
     devise the tests, there is a persistent danger that the tests may be biased in
     terms of our sensory, motor, and motivational systems (Macphail, 1985). For
     example, it is known that rats can learn some types of relationships much more
     easily through smell rather than other senses (Slotnick and Katz, 1974). Fur-
     thermore, while an IQ test for children might in some sense be validated by its
     ability to predict future academic or other success, it is not always clear how
     to validate an intelligence test for animals: if survival or the total number of
     oﬀspring was a measure of success, then bacteria would be the most intelligent
     life on earth!
      Asis often the case when we try to generalise concepts, abstraction is neces-
     sary. When attempting to measure the intelligence of lower animals it is nec-
     essary to focus on simple things like short and long term memory, the forming
     of associations, the ability to generalise simple patterns and make predictions,
     simple counting and basic communication. It is only with relatively intelligent
     social animals, such as birds and apes, that more sophisticated properties such
     as deception, imitation and the ability to recognise oneself are relevant. For
     simpler animals, the focus is more on the animal’s essential information pro-
     cessing capacity. For example, the work on measuring the capacity of ants to
     remember patterns (Reznikova and Ryabko, 1986).
      One interesting diﬃculty when testing animal intelligence is that we are
     unable to directly explain to the animal what its goal is. Instead, we have to
     guide the animal towards a problem by carefully rewarding selected behaviours
     with something like food. In general, when testing machine intelligence we
     face a similar problem in that we cannot assume that a machine will have a
                                      15
            1. Nature and Measurement of Intelligence
            suﬃcient level of language comprehension to be able to understand commands.
            Asimple solution is to use basic “rewards” to guide behaviour, as we do with
            animals. Although this approach is extremely general, one diﬃculty is that
            solving the task, and simply learning what the task is, become confounded
            and thus the results need to be interpreted carefully (Zentall, 1997).
              For good overviews of animal intelligence research see (Zentall, 2000), (Her-
            man and Pack, 1994) or (Reznikova, 2007).
            1.7. Machine intelligence tests
            This section surveys proposed tests of machine intelligence. Given that the
            measurement of machine intelligence is fundamental to the ﬁeld of artiﬁcial
            intelligence, it is remarkable that few researchers are aware of research in this
            area beyond the Turing test and some of its variants. Indeed, to the best of
            our knowledge the survey presented in this section (derived from Legg and
            Hutter, 2007b) is the only general survey of tests of machine intelligence that
            has been published!
            Turing test and derivatives.  The classic approach to determining whether
            a machine is intelligent is the so called Turing test (Turing, 1950) which has
            been extensively debated over the last 50 years (Saygin et al., 2000). Turing
            realised how diﬃcult it would be to directly deﬁne intelligence and thus at-
            tempted to side step the issue by setting up his now famous imitation game:
            if human judges cannot eﬀectively discriminate between a computer and a hu-
            manthroughteletyped conversation then we must conclude that the computer
            is intelligent.
              Though simple and clever, the test has attracted much criticism. Block
            and Searle argue that passing the test is not suﬃcient to establish intelligence
            (Block, 1981; Searle, 1980; Eisner, 1991). Essentially they both argue that a
            machine could appear to be intelligent without having any “real intelligence”,
            perhaps by using a very large table of answers to questions. While such a
            machine would be impossible in practice, due to the vast size of the table
            required, it is not logically impossible. Thus, an unintelligent machine could,
            at least in theory, consistently pass the Turing test. Some consider this to
            bring the validity of the test into question.
              In response to these challenges, even more demanding versions of the Turing
            test have been proposed such as the total Turing test in which the machine
            must respond to all forms of input that a human could, rather than just
            teletyped text (Harnad, 1989). For example, the machine should have senso-
            rimotor capabilities. Going further, the truly total Turing test demands the
            performance of not just one machine, but of the whole “race” of machines
            over an extended period of time (Schweizer, 1998). Another extension is the
            inverted Turing test in which the machine takes the place of a judge and must
            be able to distinguish between humans and machines (Watt, 1996). Dowe
            16
                                                     1.7. Machine intelligence tests
          argues that the Turing test should be extended by ensuring that the agent
          has a compressed representation of the domain area, thus ruling out look-up
          table counter arguments (Dowe and Hajek, 1998). Of course these attacks on
          the Turing test can be applied to any test of intelligence that considers only a
          system’s external behaviour, that is, most intelligence tests.
            A more common criticism is that passing the Turing test is not necessary
          to establish intelligence. Usually this argument is based on the fact that the
          test requires the machine to have a highly detailed model of human knowledge
          and patterns of thought, making it a test of humanness rather than intelli-
          gence (French, 1990; Ford and Hayes, 1998). Indeed, even small things like
          pretending to be unable to perform complex arithmetic quickly and faking
          human typing errors become important, something which clearly goes against
          the purpose of the test.
            The Turing test has other problems as well. Current AI systems are a long
          way from being able to pass an unrestricted Turing test. From a practical
          point of view this means that the full Turing test is unable to oﬀer much guid-
          ance to our work. Indeed, even though the Turing test is the most famous test
          of machine intelligence, almost no current research in artiﬁcial intelligence is
          speciﬁcally directed towards passing it. Simply restricting the domain of con-
          versation in the Turing test to make the test easier, as is done in the Loebner
          competition (Loebner, 1990), is not suﬃcient. With restricted conversation
          possibilities the most successful Loebner entrants are even more focused on fak-
          ing human fallibility, rather than anything resembling intelligence (Hutchens,
          1996). Finally, the Turing test returns diﬀerent results depending on who the
          human judges are. Its unreliability has in some cases lead to clearly unintelli-
          gent machines being classiﬁed as human, and at least one instance of a human
          actually failing a Turing test. When queried about the latter, one of the judges
          explained that “no human being would have that amount of knowledge about
          Shakespeare”(Shieber, 1994).
          Compression tests.  Mahoney has proposed a particularly simple solution to
          the binary pass or fail problem with the Turing test: replace the Turing test
          with a text compression test (Mahoney, 1999). In essence this is somewhat
          similar to a “Cloze test” where an individual’s comprehension and knowledge
          in a domain is estimated by having them guess missing words from a passage
          of text.
            While simple text compression can be performed with symbol frequencies,
          the resulting compression is relatively poor. By using more complex models
          that capture higher level features such as aspects of grammar, the best com-
          pressors are able to compress text to about 1.5 bits per character for English.
          However humans, which can also make use of general world knowledge, the
          logical structure of the argument etc., are able to reduce this down to about
          1 bit per character. Thus the compression statistic provides an easily com-
                                                                               17
            1. Nature and Measurement of Intelligence
            puted measure of how complete a machine’s models of language, reasoning and
            domain knowledge are, relative to a human.
              To see the connection to the Turing test, consider a compression test based
            on a very large corpus of dialogue. If a compressor could perform extremely
            well on such a test, this is mathematically equivalent to being able to determine
            which sentences are probable at a give point in a dialogue, and which are
            not (for the equivalence of compression and prediction see Bell et al., 1990).
            Thus, as failing a Turing test occurs when a machine (or person!) generates a
            sentence which would be improbable for a human, extremely good performance
            on dialogue compression implies the ability to pass a Turing test.
              A recent development in this area is the Hutter Prize (Hutter, 2006). In
            this test the corpus is a 100 MB extract from Wikipedia. The idea is that
            this should represent a reasonable sample of world knowledge and thus any
            compressor that can perform very well on this test must have a good model of
            not just English, but also world knowledge in general.
              One criticism of compression tests is that it is not clear whether a powerful
            compressor would easily translate into a general purpose artiﬁcial intelligence.
            Also, while a young child has a signiﬁcant amount of elementary knowledge
            about how to interact with the world, this knowledge would be of little use
            when trying to compress an encyclopedia full of abstract “adult knowledge”
            about the world.
            Linguistic complexity.  A more linguistic approach is taken by the HAL
            project at the company Artiﬁcial Intelligence NV (Treister-Goren and
            Hutchens, 2001). They propose to measure a system’s level of conversational
            ability by using techniques developed to measure the linguistic ability of chil-
            dren. These methods examine things such as vocabulary size, length of ut-
            terances, response types, syntactic complexity and so on. This would allow
            systems to be “...assigned an age or a maturity level beside their binary Tur-
            ing test assessment of ‘intelligent’ or ‘not intelligent’ ”(Treister-Goren et al.,
            2000). As they consider communication to be the basis of intelligence, and the
            Turing test to be a valid test of machine intelligence, in their view the best
            way to develop intelligence is to retrace the way in which human linguistic
            development occurs. Although they do not explicitly refer to their linguistic
            measure as a test of intelligence, because it measures progress towards what
            they consider to be a valid intelligence test, it acts as one.
            Multiple cognitive abilities.  A broader developmental approach is being
            taken by IBM’s Joshua Blue project (Alvarado et al., 2002). In this project
            they measure the performance of their system by considering a broad range
            of linguistic, social, association and learning tests. Their goal is to ﬁrst pass
            what they call a toddler Turing test, that is, to develop an AI system that can
            pass as a young child in a similar set up to the Turing test.
            18
                                                        1.7. Machine intelligence tests
            Anothercompanypursuingasimilardevelopmentalapproachbasedonmea-
          suring system performance through a broad range of cognitive tests is the a2i2
          project at Adaptive AI (Voss, 2005). Rather than toddler level intelligence,
          their current goal is to work toward a level of cognitive performance simi-
          lar to that of a small mammal. The idea being that even a small mammal
          has many of the key cognitive abilities required for human level intelligence
          working together in an integrated way.
          Competitive games. The Turing Ratio method of Masum et al. has more
          emphasis on tasks and games rather than cognitive tests. Similar to our own
          deﬁnition, they propose that “...doing well at a broad range of tasks is an em-
          pirical deﬁnition of ‘intelligence’.”(Masum et al., 2002) To quantify this they
          seek to identify tasks that measure important abilities, admit a series of strate-
          gies that are qualitatively diﬀerent, and are reproducible and relevant over an
          extended period of time. They suggest a system of measuring performance
          through pairwise comparisons between AI systems that is similar to that used
          to rate players in the international chess rating system. The key diﬃculty
          however, which the authors acknowledge is an open challenge, is to work out
          what these tasks should be, and to quantify just how broad, important and
          relevant each is. In our view these are some of the most central problems that
          must be solved when attempting to construct an intelligence test. Thus we
          consider this approach to be incomplete in its current state.
          Collection of psychometric tests.  Anapproach called Psychometric AI tries
          to address the problem of what to test for in a pragmatic way. In the view
          of Bringsjord and Schimanski, “Some agent is intelligent if and only if it ex-
          cels at all established, validated tests of [human] intelligence.”(Bringsjord and
          Schimanski, 2003) They later broaden this to also include “tests of artistic and
          literary creativity, mechanical ability, and so on.” With this as their goal, their
          research is focused on building robots that can perform well on standard psy-
          chometric tests designed for humans, such as the Wechsler Adult Intelligence
          Scale and Raven Progressive Matrices (see Section 1.5).
            As eﬀective as these tests are for humans, we believe that they are unlikely
          to be adequate for measuring machine intelligence. For a start they are highly
          anthropocentric. Another problem is that they embody basic assumptions
          aboutthetestsubjectthatarelikelytobeviolatedbycomputers. Forexample,
          consider the fundamental assumption that the test subject is not simply a
          collection of specialised algorithms designed only for answering common IQ
          test questions.  While this is obviously true of a human, or even an ape,
          it may not be true of a computer. The computer could be nothing more
          than a collection of speciﬁc algorithms designed to identify patterns in shapes,
          predict number sequences, write poems on a given subject or solve verbal
          analogy problems — all things that AI researchers have worked on. Such a
          machine might be able to obtain a respectable IQ score (Sanghi and Dowe,
                                                                                    19
             1. Nature and Measurement of Intelligence
             2003), even though outside of these speciﬁc test problems it would be next
             to useless.  If we try to correct for these limitations by expanding beyond
             standard tests, as Bringsjord and Schimanski seem to suggest, this once again
             opens up the diﬃculty of exactly what, and what not, to test for. Thus
             we consider Psychometric AI, at least as it is currently formulated, to only
             partially address this central question.
             C-Test.    Oneperspective among psychologists is that intelligence is “the abil-
             ity to deal with complexity”(Gottfredson, 1997b). Thus, in a test of intelli-
             gence, the most diﬃcult questions are the ones that are the most complex
             because these will, by deﬁnition, require the most intelligence to solve. It fol-
             lows then that if we could formally deﬁne and measure the complexity of test
             problems using complexity theory we could construct a formal test of intel-
             ligence. The possibility of doing this was perhaps ﬁrst suggested by Chaitin
             (1982). While this path requires numerous diﬃculties to be dealt with, we
             believe that it is the most natural and oﬀers many advantages: it is formally
             motivated and precisely deﬁned, and potentially could be used to measure
             the performance of both computers and biological systems on the same scale
             without the problem of bias towards any particular species or culture.
                TheC-Testconsists of a number of sequence prediction and abduction prob-
             lems similar to those that appear in many standard IQ tests (Hern´andez-
             Orallo, 2000b). This test has been successfully applied to humans with inter-
             esting results showing a positive correlation between individual’s IQ test scores
             and C-Test scores (Hern´andez-Orallo and Minaya-Collado, 1998; Hern´andez-
             Orallo, 2000a). Similar to standard IQ tests, the C-Test always ensures that
             each question has an unambiguous answer in the sense that there is always one
             hypothesis that is consistent with the observed pattern that has signiﬁcantly
             lower complexity than the alternatives. Other than making the test easier to
             score, it has the added advantage of reducing the test’s sensitivity to changes
             in the reference machine used to deﬁne the complexity measure.
                The key diﬀerence to sequence problems that appear in standard intelli-
             gence tests is that the questions are based on a formally expressed measure of
             complexity. As Kolmogorov complexity is not computable (see Section 2.5),
             the C-Test instead uses Levin’s related Kt complexity (Levin, 1973). In order
             to retain the invariance property of Kolmogorov complexity, Levin complexity
             requires the additional assumption that the universal Turing machines are able
             to simulate each other in linear time, for example, pointer machines. As far
             as we know, this is the only formal deﬁnition of intelligence that has so far
             produced a usable test of intelligence.
                To illustrate the C-Test, below are some example problems taken
             from (Hern´andez-Orallo and Minaya-Collado, 1998). Beside each question is
             its complexity, naturally more complex patterns are also more diﬃcult:
             20
                                                                      1.7. Machine intelligence tests
                                  Sequence Prediction Test
                 Complexity       Sequence                                           Answer
                       9          a, d, g, j,   , . . .                                  m
                       12         a, a, z, c, y, e, x,   , . . .                         g
                       14         c, a, b, d, b, c, c, e, c, d,    , . . .               d
                                  Sequence Abduction Test
                 Complexity       Sequence                                           Answer
                       8          a,   , a, z, a, y, a, ...                              a
                       10         a, x,    , v, w, t, u, ...                             y
                       13         a, y, w,    , w, u, w, u, s, ...                       y
               Our main criticism of the C-Test is that it does not require the agent to be
             able to deal with problems that require interacting with an environment. For
             example, an agent could have a very high C-Test score due to being a very
             good sequence predictor, and yet be unable to deal with more general kinds
             of problems. This falls short of what is required by our informal deﬁnition of
             intelligence, that is, the ability to achieve goals in a wide range of environments.
             Smith’s Test.      Anothercomplexitybasedformaldeﬁnitionofintelligencethat
             appeared recently in an unpublished report is due to Smith (2006). His ap-
             proach has a number of connections to our work, indeed Smith states that his
             work is largely a “...rediscovery of recent work by Marcus Hutter”. Perhaps
             this is over stating the similarities because while there are some connections,
             there are also many important diﬀerences.
               The basic structure of Smith’s deﬁnition is that an agent faces a series of
             problems that are generated by an algorithm. In each iteration the agent must
             try to produce the correct response to the problem that it has been given. The
             problem generator then responds with a score of how good the agent’s answer
             was. If the agent so desires it can submit another answer to the same problem.
             Atsomepointtheagent requests the problem generator to move onto the next
             problem and the score that the agent received for its last answer to the current
             problem is then added to its cumulative score. Each interaction cycle counts
             as one time step and the agent’s intelligence is then its total cumulative score
             considered as a function of time. In order to keep things feasible, the problems
             must all be in the complexity class P, that is, decision problems which can
             be solved by a deterministic Turing machine using a polynomial amount of
             computation time.
               Wehave three main criticisms of Smith’s deﬁnition. Firstly, while for prac-
             tical reasons it might make sense to restrict problems to be in P, we do not see
             why this practical restriction should be a part of the very deﬁnition of intelli-
             gence. If some breakthrough meant that agents could solve diﬃcult problems
             in not just P but sometimes also in the larger complexity class NP, then surely
             these new agents would be more intelligent? We had similar objections to in-
                                                                                                        21
             1. Nature and Measurement of Intelligence
             formal deﬁnitions of machine intelligence that included eﬃciency requirements
             in Section 1.3.
               Oursecondcriticism is similar to that of the C-Test. Although there is some
             interaction between the agent and the environment, this interaction is rather
             limited. The problem-answer format of the test is too limited to fully test an
             agent’s capabilities.
               The ﬁnal criticism is that while the deﬁnition is somewhat formally deﬁned,
             it still leaves open the important question of what exactly the individual tests
             should be. Smith suggests that researchers should dream up tests and then
             contribute them to some common pool of tests. As such, this intelligence test
             is not fully speciﬁed.
             1.8. Conclusion
             Although this chapter provides only a short treatment of the complex topic
             of intelligence, for a work on artiﬁcial intelligence to devote more than a few
             paragraphs to the topic is rare. We believe that this is a mistake: if artiﬁcial
             intelligence research is ever to produce systems with real intelligence, questions
             of what intelligence actually means and how to measure it in machines need
             to be taken seriously. At present practically nobody is doing this. The reason,
             it appears, is that the deﬁnition and measurement of intelligence are viewed
             as being too diﬃcult. We accept that the topic is diﬃcult, however we do not
             accept that the topic is so diﬃcult as to be hopeless and best avoided. As we
             have seen in our survey of deﬁnitions, there are many commonalities across
             the various proposals. This leads to our informal deﬁnition of intelligence that
             we argue captures the essence of these. Furthermore, although intelligence
             tests for humans are widely treated with suspicion by the public, by various
             metrics these tests have proven to be very eﬀective and reliable when correctly
             applied.  This gives us hope that useful tests of machine intelligence may
             also be possible.  At present only a handful of researchers are working on
             these problems, mostly in obscurity. No doubt these fundamental issues will
             someday return to the fore when the ﬁeld is more advanced.
             22
     2. Universal Artiﬁcial Intelligence
     Having reviewed what intelligence is and how it is measured, we now turn
     our attention to artiﬁcial systems that appear to be intelligent, at least in
     theory. The problem is that although machines and algorithms are becoming
     progressively more powerful, as yet no existing system can be said to have
     true intelligence — they simply lack the power, and in particular the breadth,
     to really be called intelligent. However, among theoretical models which are
     free of practical concerns such as computational resource limitations, intelli-
     gent machines can be deﬁned and analysed. In this chapter we introduce a
     very powerful theoretical model: Hutter’s universal artiﬁcial intelligence agent,
     known as AIXI.
      Afulltreatmentofthistopicrequiresasigniﬁcantamountoftechnicalmath-
     ematics. The goal here is to explain the foundations of the topic and some of
     the key results in the area in a relatively easy to understand fashion. For the
     full details, including precise mathematical deﬁnitions, proofs and connections
     to other ﬁelds, see (Hutter, 2005), or for a more condensed presentation (Hut-
     ter, 2007b). At this point the reader may wish to browse Appendix A that
     describes the mathematical notation and conventions used in this thesis.
     2.1. Inductive inference
     Inductive inference is the process by which one observes the world and then
     infers the causes behind what has been observed. This is a key process by which
     we try to understand the universe and so naturally examples of it abound.
     Indeed much of science can be viewed as a process of inductively inferring
     natural causes. For example, at a microscopic level, one may ﬁre sub-atomic
     particles into a gas chamber, observe the patterns they trace out, and then
     try to infer what the underlying principles are that govern these events. At
     a larger scale one may observe that global temperatures are changing along
     with other atmospheric conditions, and from this information attempt to infer
     what processes may be driving climate change.
      Science is not the only domain where inductive inference is important. A
     businessman may observe stock prices over time and then attempt to infer a
     model of this process in order to predict the market. A parent may return
     home from work to discover a chair propped against the refrigerator with the
     cookie jar on top a little emptier. Whether we are a detective trying to catch
     a thief, a scientist trying to discover a new physical law, or a businessman
                                          23
      2. Universal Artiﬁcial Intelligence
      attempting to understand a recent change in demand, we are all in the process
      of collecting information and trying to infer the underlying causes.
       Formally we can abstract the inductive inference problem as follows: An
      agent has observed some data D := x1,x2,...xt and has a set of hypotheses
      H:=h1,h2,..., some of which may be good models of the unknown process
      µ that is generating D. The task is to decide which hypothesis, or hypotheses
      in H are the most likely to accurately reﬂect µ. For example, x1,x2,... might
      be the market value of a stock over time and H might consist of a set of
      mathematical models of the stock price. Once we have identiﬁed which model
      or models are likely to accurately describe the price behaviour, we may want
      to use this information to predict future stock prices. Typically this is the
      case: Often our goal is not just to understand our observations, but also to
      be able to predict future observations. It is in prediction that good models
      become truly useful.
       Inductive inference has a long history in philosophy. An early thinker on
      the subject was the Greek philosopher Epicurus (342? B.C. – 270 B.C.) who
      noted that there typically exist many hypotheses which are consistent with all
      of the available data. Logically then, we cannot use the data to rule out any
      of these hypotheses; they must all be kept as potential explanations. This is
      known as Epicurus’ principle of multiple explanations and is often stated as,
        Keep all hypotheses that are consistent with the data.
       To illustrate this, consider the cookie jar example again and place yourself
      in the position of the parent returning home from work. Having observed the
      chair by the refrigerator and missing cookies, one seemingly likely hypothesis
      is that your daughter has pushed the chair over to the refrigerator, climbed on
      top of it, and then removed some cookies. Another hypothesis is that a hungry
      but unusually short thief picked the lock on the back door, saw the cookie jar
      and decided to move the chair over to the refrigerator in order to get some
      cookies. Although this seems much less likely, you cannot completely rule out
      this possibility, or even more elaborate explanations, based solely on the scene
      in the kitchen. Philosophically this leaves you in the uncomfortable situation
      of having to consider all sorts of strange explanations as being theoretically
      possible given the information available. The need to keep these hypotheses
      would become clear if you were then to walk into the living room and no-
      tice that your new television and other expensive items were also missing —
      suddenly the unlikely seems more plausible.
       Although we may accept that all hypotheses which are consistent with the
      observed facts should be considered at least possible, it is intuitively clear
      that some hypotheses are much more likely than others. For example, if you
      had previously observed similar techniques being employed by your daughter
      to access the cookie jar, but had never been burgled, it would be natural to
      consider that the small thief in question was of your own ﬂesh and blood,
      rather than a career criminal. However, you are basing this judgement on
      your experience prior to returning home. What if you really had very little
      24
                                                                           2.2. Bayes’ rule
           prior knowledge? How then should you judge the relative likelihood of diﬀerent
           hypotheses?
           2.1.1 Example. Consider the following sequence:
                                              1, 3, 5, 7
           Whatprocess do you think is generating these numbers? What do you predict
           will come next?
             An obvious hypothesis is that these are the positive odd numbers. If this
           is true then the next number is going to be 9. A more complex hypotheses
           is that the sequence is being generated by the equation 2n − 1 + (n − 1)(n −
           2)(n −3)(n−4) for n ∈ N. In this case the next number would be 33. Even
           whenpeople are aware that this equation generates a sequence consistent with
           the digits above, most would not consider it to be very likely at all.        3
             The philosophical principle behind this intuition was ﬁrst clearly stated by
           the English logician and Franciscan friar, William of Ockham (1285 – 1349,
           also spelt Occam). He argued that when inferring a cause one should not
           include in the explanation anything that is not strictly required to explain the
           observations. Or as originally stated, “entia non sunt multiplicanda praeter
           necessitatem”, which translates as “entities should not be multiplied beyond
           necessity”. A more modern and perhaps clearer expression is,
                 Amongall hypotheses consistent with the observations, the simplest
                 is the most likely.
           This philosophical principle is known as Occam’s razor as it allows one to
           cut away unnecessary baggage from explanations. If we consider the number
           prediction problem again, it is clear that the principle of Occam’s razor agrees
           with our intuition: the simple hypothesis seemed to be more likely, a priori,
           than the complex hypothesis. As we saw in the previous chapter, the ability
           to apply Occam’s razor is a standard feature of intelligence tests.
           2.2. Bayes’ rule
           Although fundamental, the principles of Epicurus and Occam by themselves
           are insuﬃcient to provide us with a mechanism for performing inductive in-
           ference. A major step forward in this direction came from the English math-
           ematician and Presbyterian minister, Thomas Bayes (1702 – 1761).
             In inductive inference we seek to ﬁnd the most likely hypothesis, or hypothe-
           ses, given the data. Expressed in terms of probability, we seek to ﬁnd h ∈ H
           such that the probability of h given D, written P(h|D), is high. From the
           deﬁnition of conditional probability, P(h|D) := P(h∩D)/P(D). Rearranging
           this we get P(h|D)P(D) = P(h ∩ D) = P(D|h)P(h), from which it follows
           that,
                                     P(D|h)P(h)           P(D|h)P(h)
                          P(h|D) =      P(D)       = Ph′∈HP(D|h′)P(h′).
                                                                                         25
      2. Universal Artiﬁcial Intelligence
      This equation is known as Bayes’ rule. It allows one to compute the probabil-
      ity of diﬀerent hypotheses h ∈ H given the observed data D, and a distribution
      P(h) over H. The probability of the observed data, P(D), is known as the
      evidence. P(h) is known as the prior distribution as it is the distribution over
      the space of hypotheses before taking into account the observed data. The
      distribution P(h|D) is known as the posterior distribution as it is the distri-
      bution after taking the data into account. Thus in essence, Bayes’ rule takes
      some beliefs that we may have about the world and updates these according
      to some observed data. In the above formulation we have assumed that the
      set of hypotheses H is countable. For uncountable sets the sum is replaced by
      an integral.
       Despite its elegance and simplicity, Bayesian inference is controversial. To
      this day professional statisticians can be roughly divided into Bayesians who
      accept the rule, and classical statisticians who do not. The debate is a subtle
      and complex one and there are many diﬀerent positions within each of the two
      camps. At the core of the debate is the very notion of what probability means,
      and in particular what the prior probability P(h) means.
       How can one talk about the probability of a hypothesis before seeing any
      data? Even if this prior probability is meaningful, how can one know what
      its value is? We need to take this question seriously because in Bayes’ rule
      the choice of prior aﬀects the relative values of P(h|D) for each h, and thus
      inﬂuences the inference results. Indeed, if Bayesians are free to choose the
      prior over H, how can they claim to have objective results?
       Bayesians respond to this in a number of ways. Firstly, they point out that
      the problem is generally small, in the sense that with a reasonable prior and
      quantity of data, the posterior distribution P(h|D) depends almost entirely
      on D rather than the chosen prior P(h). In fact on any sizable data set, not
      only does the choice of prior not especially matter, but Bayesian and classical
      statistical methods typically produce similar results, as one would expect. It
      is only with relatively small data sets or complex models that the choice of
      prior becomes an issue.
       If classical statistical methods could avoid the problem of prior bias when
      dealing with small data sets then this would be a signiﬁcant argument in their
      favour. However Bayesians argue that all systems of inductive inference that
      obey some basic consistency principles deﬁne, either explicitly or implicitly,
      a prior distribution over hypotheses. Thus, methods from classical statistics
      make assumptions that are in eﬀect equivalent to deﬁning a prior. The dif-
      ference is that in Bayesian statistics these assumptions take the form of an
      explicit prior distribution. In other words, it is not that prior bias in Bayesian
      statistics is necessarily any better or worse than in classical statistics, it is
      simply more transparent.
       In practical terms, if priors cannot be avoided, one strategy to reduce the
      potential for prior selection abuse is to use well known priors whenever possi-
      ble. To this end many standard prior distributions have been developed. The
      key desirable property is that a prior should not strongly inﬂuence the poste-
      26
                                     2.3. Binary sequence prediction
       rior distribution and thus unduly aﬀect the inference results. This means that
       the prior should express a high degree of impartiality by treating the various
       hypotheses somewhat equally. For example, when the set H is ﬁnite, an ob-
       vious choice is to assign equal prior probability to each hypothesis, formally,
       P(h) := 1 for all h ∈ H. Things become more problematic in inﬁnite hy-
             |H|
       pothesis spaces as it is then mathematically impossible to assign an equal ﬁnite
       probability to each of the hypotheses in H, and still have P(H) = 1. Some
       Bayesians abandon this condition and use so called improper priors which are
       not true probability distributions. For the classical statistician, such a radical
       departure from the deﬁnition of probability does not really solve the prob-
       lem of the unknown prior, rather it suggests that something is fundamentally
       amiss.
        Instead of mathematical tricks or other workarounds, what Bayesians would
       ideally like is to solve the unknown prior problem once and for all by having
       a universal prior distribution. Only then would the Bayesian approach be
       truly complete. The principles of Epicurus and Occam provide some hints on
       how this might be done. From Epicurus, whenever a hypothesis is consistent
       with the data, that is P(D|h) > 0, we should keep this hypothesis by having
       P(h|D) > 0. From Bayes’ rule this requires that ∀h ∈ H : P(h) > 0. From
       Occam we see that P(h) should decrease with the complexity of h, and thus
       we need a way to measure the complexity of hypotheses. However before
       continuing with this, we ﬁrst consider the inference problem from another
       perspective.
       2.3. Binary sequence prediction
       An alternate characterisation of inductive inference can be made in terms of
       binary sequence prediction. One reason this is useful is that binary sequences
       and strings provide a more natural setting in which to deal with issues of com-
       putability. The problem can be formulated as follows: There is an unknown
       probability distribution µ over the space of binary sequences B∞. From this
       distribution a sequence ω is drawn one bit at a time. At time t ∈ N we have
       observed the initial string ω1:t := ω1ω2 ...ωt, and our task is to predict what
       the next bit in the sequence will be, that is, ωt+1. To do this we select a
       model, or models, from a set of potential models that explain the observed
       sequence so far and that, we hope, will be good at predicting future bits in
       the sequence.
        In terms of inductive inference the observed initial binary string ω1:t is the
       observed data D, and our set of potential models of the data is the set of
       hypotheses H. We would like to ﬁnd a model ν ∈ H, or models, that are as
       close as possible to the unknown true model of the data µ, in the sense that ν
       will allow us to predict future bits in the sequence as accurately as possible.
        Webeginbyclarifying what we mean by a probability distribution. In math-
       ematical statistics a probability distribution is known as a probability measure
                                                        27
             2. Universal Artiﬁcial Intelligence
             as it belongs to the class of functions known as measures. Over the space of
             binary strings these can be deﬁned as follows:
             2.3.1 Deﬁnition.     A probability measure is a function ν : B∗ → [0,1] such
             that,
                                                ν(ǫ) = 1,
                                    ∀x∈B∗ ν(x)=ν(x0)+ν(x1).
               In this thesis we will interpret ν(x) to mean the probability that a binary
             sequencesampledaccordingtothedistributionν beginswiththestringx ∈ B∗.
             Asall strings and sequences begin with the null string ǫ, by deﬁnition, the ﬁrst
             condition above simply says that the ν probability that a sequence belongs to
             the set of all sequences is 1. The second condition says that the ν probability
             that a sequence begins with string x0, plus the ν probability that it begins with
             x1, is equal to the ν probability that it begins with x. This makes sense given
             that all sequences that begin with x must have either a 0 or a 1 as their next
             bit and so we would expect the probabilities of these sets of sequences to add
             up. This style of notation for measures will be convenient for our purposes,
             however it is somewhat unusual. To see how it relates to conventional measure
             theory see Appendix A.
               Sequence prediction forms a large part of this thesis and thus we will often
             be interested in what comes next in a sequence given an initial string. More
             precisely, if a sequence ω has been sampled from the distribution µ, and ω
             begins with the string y ∈ B∗, what is the probability that the next bits from
             ω will be the string x ∈ B∗? For this will we adopt the following notation for
             the conditional probability, µ(yx) := µ(yx)/µ(y). The beneﬁt of this notation
             will become apparent later when we need to deal with complex interaction
             sequences. Not only does it preserve the order in which the sequence occurs,
             it also allows for more compact expressions when we need to condition on only
             certain parts of a sequence.
               As noted earlier in this section, sequence prediction can be viewed as an
             inductive inference problem. Thus, we can use Bayes’ rule to estimate how
             likely some model ν ∈ H is given the observed sequence ω1:t:
                                      P(ω1:t|ν)P(ν)        ν(ω1:t)P(ν)
                           P νω1:t =      P(ω1:t)     = P̺∈H̺(ω1:t)P(̺).
             2.3.2 Example. Consider the problem of inferring whether a coin is a nor-
             mal fair coin based on a sample of coin ﬂips. To simplify things, assume
             that the coin is either heads on both sides, tails on both sides, or a nor-
             mal fair coin. Further assume that t = 4 and we have the observed data
             D=head,head,head,head. In terms of binary sequence prediction, the out-
             comeoftcointossescanbeexpressedasastringω1:t ∈ Bt, with each tail being
             represented as a 0 bit, and each head as a 1 bit. Thus we have ω1:4 = 1111.
               Let H be the set of models consisting of the distributions ν (ω  ) := pr(1−
                                                                           p  1:t
               t−r             1 	              Pt
             p)   , where p ∈ 0, 2,1 and r :=      i=1 ωi is the number of observed heads.
             28
                                                              2.3. Binary sequence prediction
           As there are just three models in H, assume a uniform prior, that is, ∀p ∈
            1 	                    1
             0, 2,1  let P(νp) := 3. Now from Bayes’ rule,
                                                                   
                                                      1  1 4 1− 1 0                         1
                                                      3  2         2
             P(ν1|ω1:4 = 1111) =        h                                          i =      .
                  2                    1   4       0     1 4       1 0     4       0       17
                                       3 0 (1−0) + 2          1−2 +1 (1−1)
           Similarly, P(ν |ω     =1111)=0andP(ν |ω             =1111)= 16. Thustheresults
                          0   1:4                       1  1:4             17
           clearly point towards the coin being double headed, as we would expect having
           just observed four heads in a row.                                                 3
              More complex examples could involve data collected from medical measure-
           ments, music or weather satellite images. Good models would then have to
           describe biological processes, music styles, or the dynamics of weather systems
           respectively. In each case the binary string representing D could be simply the
           string of bits as they would appear in a computer ﬁle. However, ﬁnding a good
           prior over such spaces is not trivial. Furthermore, actually computing Bayes’
           rule and ﬁnding the most likely models, as we did in the example above, can
           become very computationally diﬃcult and may need to be approximated. In
           any case, Bayes’ rule at least tells us how to solve the induction problem in
           theory, so long as we have a prior distribution.
              Rather than just estimating which model or models are the most likely, we
           may be interested in actually predicting the sequence. One possibility is to
           calculate the probability that the next bit is a 1 based on the most likely model.
           Thefull Bayesian approach, however, is to consider each possible model ν ∈ H
           and weight the prediction made by each according to how conﬁdent we are
           about each model, i.e. P(ν|ω1:t). This gives us the mixture model predictor,
                        P(ω1:t1)    = XP(ν|ω1:t)ν(ω1:t1)
                                        ν∈H
                                    = Xν(ω1:t)P(ν) ν(ω1:t1) = P(ω1:t1).
                                        ν∈H    P(ω1:t)     ν(ω1:t)        P(ω1:t)
           As we can see, the Bayes mixture predictor reduces to the deﬁnition of condi-
           tional probability. This has removed the prior over H, and in its place we now
           have the related prior over D, in this setting the space of binary sequences.
           The fact that we can use one prior to deﬁne the other means that the two
           unknown priors are in fact two perspectives on the same fundamental problem
           of specifying our prior knowledge.
                                                                                              29
            2. Universal Artiﬁcial Intelligence
            2.3.3 Example. Continuing Example 2.3.2, we can compute the prior distri-
            bution over sequences from the prior distribution over H,
                 P(ω1:t)   = Xν(ω1:t)P(ν)
                               ν∈H
                               1 " t      t−r   1t       1t−r    t       t−r#
                           = 3 0(1−0)         + 2       1−2       +1(1−1)
                           = 1"12t−r+δtr#.
                               3    2
            where r := Pt    ωi is the number of observed heads, and the Kronecker delta
                         i=1
            symbol δab is deﬁned to be 1 if a = b, and 0 otherwise. Thus, given that
            ω1:4 = 1111, according to the mixture model the probability that the next bit
            is a 1 is,
                                     1 h 12(5)−5+δ5,5i     15 +1
                                     3   2                  2          33
                         P(11111) = 1 h 12(4)−4+δ4,4i =  14 +1 = 34.
                                     3   2                  2
                                                                                     3
            2.4. Solomonoﬀ’s prior and Kolmogorov
                   complexity
            In the 1960’s Ray J. Solomonoﬀ (1926–) investigated the problem of inductive
            inference from the perspective of binary sequence prediction (Solomonoﬀ 1964;
            1978). He was interested in a very general form of the problem, speciﬁcally,
            learning to predict a binary sequence that has been sampled from an arbitrary
            unknown computable distribution. Solomonoﬀ deﬁned his prior distribution
            over sequences as follows: The prior probability that a sequence begins with
            a string x ∈ B∗ is the probability that a universal Turing machine running
            a randomly generated program computes a sequence that begins with x. By
            randomly generated, we mean that the bits of the program have a uniform
            distribution, for example, they could come from ﬂipping a fair coin. Formally,
            2.4.1 Deﬁnition. The Solomonoﬀ prior probability that a sequence begins
            with the string x ∈ B∗ is,
                                                 X −ℓ(p)
                                      M(x):=           2     ,
                                               p:U(p)=x∗
            where U(p) = x∗ means that the universal Turing machine U computes an
            output sequence that begins with x ∈ B∗ when it runs the program p, and ℓ(p)
            is the length of p in bits.
            30
                                  2.4. Solomonoﬀ’s prior and Kolmogorov complexity
                           −ℓ(p)
            Note that the 2     term in this deﬁnition comes from the fact that the
          probability of p under a uniform distribution halves for each additional bit.
            We will assume that U is a preﬁx universal Turing machine. This means
          that no valid program for U is a preﬁx of any other. More precisely, if p,q ∈ B∗
          are valid programs on U, then there does not exist a string x ∈ B∗ such that
          p = qx. Preﬁx universal Turing machines have technical properties that we
          will need and so throughout this thesis we will assume that U is of this type.
          Technically, U is actually a type of preﬁx universal Turing machine known as
          a monotone universal Turing machine (see Section 5.1). For the moment we
          can safely gloss over these details.
          2.4.2 Example. Rather than a classic universal Turing machine running a
          programspeciﬁed by a binary string on an input tape, it is often more intuitive
          to think in terms of a program written in a high level programing language
          that is being executed on a real computer. Indeed, if a computer had inﬁnite
          memoryandneverbrokedownitwouldbetechnicallyequivalenttoauniversal
          Turing machine. Consider a short program in C that prints a binary sequence
          of all 1’s:
               main(){while(1)printf("1");}
          As far as C programs go, this is nearly as simple as they get. This is not
          surprising given that the output is also very simple. If we want a program that
          generates a more complex sequence, such as an inﬁnite sequence of successive
          digits of the mathematical constant π = 3.141592..., such a program would
          be at least ten times as long. It follows then that the probability of randomly
          generating a program that outputs all 1’s is far higher than the probability of
          randomly generating a program that computes π. Thus, Solomonoﬀ’s prior
          assigns much higher probability to the sequences of all 1’s than to the sequence
          for π. More complex sequences would require still larger programs and thus
          have even lower prior probability.                                     3
            Although Solomonoﬀ’s deﬁnition requires nothing more than random bits
          being fed into a universal Turing machine, we can see that the resulting dis-
          tribution over sequences neatly formalises Occam’s razor. Speciﬁcally, sets
          of sequences that have short programs, and are thus in some sense simple,
          are given higher prior probability than sets of sequences that have only long
          programs.
            The idea that the complexity of a sequence is related to the length of the
          shortest program that generates the sequence motivates the following deﬁni-
          tion:
          2.4.3 Deﬁnition. The Kolmogorov complexity of a sequence ω ∈ B∞ is,
                               K(ω) := min{ℓ(p) : U(p) = ω},
                                        p∈B∗
                                                                                31
      2. Universal Artiﬁcial Intelligence
      where U is a preﬁx universal Turing machine. If no such p exists, we deﬁne
      K(ω) = ∞. For a string x ∈ B∗, we deﬁne K(x) to be the length of the
      shortest program that outputs x and then halts.
       Kolmogorov complexity has many powerful theoretical properties and is a
      central ingredient in the theory of universal artiﬁcial intelligence. Its most
      important property is that the complexity it assigns to strings and sequences
      does not depend too much on the choice of the universal Turing machine U.
      This comes from the fact that universal Turing machines are universal in the
      sense that they are able to simulate each other with a constant number of
      additional input bits. Thus, if we change U above to some other universal
      Turing machine U′, the minimal value of ℓ(p) and thus K(x), can only change
      by a bounded number of bits. This bound depends on U and U′, but not on
      x.
       The biggest problem with Kolmogorov complexity is that the value of K
      is not in general computable. It can only be approximated from above. The
      reason for this is that in general we cannot ﬁnd the shortest program to com-
      pute a string x on U due to the halting problem. Intuitively, there might exist
      a very short program p∗ such that U(p∗) = x, however we do not know this
      because p∗ takes such a long time to run. Nevertheless, in theoretical appli-
      cations the simplicity and theoretical power of Kolmogorov complexity often
      outweighs this computability problem. In practical applications Kolmogorov
      complexity is approximated, for example by using a compression algorithm to
      estimate the length of the shortest program (Cilibrasi and Vit´anyi, 2005).
      2.5. Solomonoﬀ-Levin prior
      Besides the prior described in the previous section, Solomonoﬀ also suggested
      to deﬁne a universal prior by taking a mixture of distributions (Solomonoﬀ,
      1964). In the 1970’s this alternate approach was generalised and further de-
      veloped (Zvonkin and Levin, 1970; Levin, 1974). As Leonid Levin (1948–)
      played an important role in this, here we will refer to this as the Solomonoﬀ-
      Levin prior. It is closely related to the universal prior in the previous section:
      they lie within a multiplicative constant of each other and share key technical
      properties.
       Although taking mixtures is perhaps less intuitive, it has the advantage
      of making important theoretical properties of the prior more transparent. It
      also gives an explicit prior over both the hypothesis space and the space of
      sequences. The topic is quite technical, however it is worth spending some
      time on as it lies at the heart of universal artiﬁcial intelligence.
       The hypotheses we have been working with up to now have all been proba-
      bility measures. These can be generalised as follows:
      2.5.1 Deﬁnition. A semi-measure is a function ν : B∗ → [0,1] such that,
                     ν(ǫ) ≤ 1,
      32
                           2.5. Solomonoﬀ-Levin prior
               ∀x∈B∗ ν(x)≥ν(x0)+ν(x1).
      Intuitively one may think of a semi-measure that is not a probability measure
     as being a kind of defective probability measure whose probabilities do not
     quite add up as they should. This defect can be ﬁxed in the sense that a
     semi-measure can be built up to be a probability measure by appropriately
     normalising things.
      Intuitively, a function is enumerable if it can be progressively approximated
     from below. More formally, f : X → R is enumerable if there exists a com-
     putable function g : X × N → Q such that ∀x ∈ X,∀i ∈ N : gi+1(x) ≥ gi(x)
     and ∀x ∈ X : limi→∞gi(x) = f(x). Enumerability is weaker than computabil-
     ity because for any x ∈ X we only ever have the lower bound gi(x) on the
     value of f(x). Thus we can never know for sure how far our bound is from the
     true value of f(x), that is, we do not know how large f(x) − gi(x) might be.
      If a similar condition holds, but with the approximation function converging
     to f from above rather than below, we say that f is coenumerable. One
     example of such a function is the Kolmogorov complexity function K in the
     previous section. If a function is both enumerable and coenumerable, then
     we have both upper and lower bounds and thus can compute the value of f
     to any required accuracy. In this case we simply say that f is a real valued
     computable function. Clearly then, the enumerable functions are a superset of
     the computable functions.
      Our task is to construct a prior distribution over the enumerable semi-
     measures. To do this we need to formalise Occam’s razor, and for that we
     need to deﬁne a way to measure the complexity of enumerable semi-measures.
     Solomonoﬀ measured the complexity of sequences according to the length of
     their programs, here we can do something similar.
      By deﬁnition, all enumerable functions can be approximated from below
     by a computable function. Thus, it is not too hard to prove that the set of
     enumerablefunctionscanbeindexedbyaTuringmachine,andthatthiscanbe
     further restricted to just the set of enumerable semi-measures. More precisely,
     there exists a Turing machine T that for any enumerable semi-measure µ there
     exists an index i ∈ N such that ∀x ∈ B∗ : µ(x) = νi(x) := limk→∞T(i,k,x)
     with T increasing in k. In eﬀect, the index i is a description of µ in that once
     we know i we can approximate the value of µ from below for any x by using
     the Turing machine T. As k increases, these approximations increase towards
     the true value of µ(x). For details on how all this is done see Section 4.5
     of (Li and Vit´anyi, 1997) or (Legg, 1997). The main thing we will need is the
     computable enumeration of enumerable semi-measures itself, which we will
     denote by Me := ν1,ν2,ν3,.... As all probability measures are semi-measures
     by deﬁnition, and all computable functions are enumerable, it follows that
     the set of enumerable semi-measures is a superset of the set of computable
     probability measures. We write Mc to denote an enumeration of just the
     computable measures.
                                      33
             2. Universal Artiﬁcial Intelligence
               Note the two uses of the word “enumerable” above. When we say that a set
             can be enumerated, what we mean is that there exists a way to step though all
             the elements in this set. When we say that an individual function is enumer-
             able, such as a semi-measure, what we mean is that it can be approximated
             from below by a series of computable functions. Some authors avoid this
             dual usage by referring to enumerable functions as lower semi-computable. In
             this terminology, what Me provides is a computable enumeration of the lower
             semi-computable semi-measures. In any case, it is still a mouthful!
               We can now return to the question of how to measure the complexity of
             an enumerable semi-measure. As noted above, for νi ∈ Me the index i is in
             eﬀect a description of νi. At this point, it might seem that the natural thing
             to do is to take the value of an enumerable semi-measure’s index to be its
             complexity. The problem, however, is that some extremely large index values,
                      1000
             such as 2    , contain a lot less information than far smaller index values which
             are not as easily described: for example, an index whose binary representation
             is a string of 100 random bits. The solution is that we must measure not
             the value, but the information content of the index in order to measure the
             complexity of the enumerable semi-measure it describes. We do this by taking
             the Kolmogorov complexity of the index. That is, we deﬁne the complexity
             of an enumerable semi-measure to be the length of the shortest program that
             computes its index. Formally,
             2.5.2 Deﬁnition. The Kolmogorov complexity of µ ∈ Me is,
                                    K(µ) := min{ℓ(p) : U(p) = i},
                                              p∈B∗
             where µ is the ith element in the recursive enumeration of all enumerable
             semi-measures Me, and U is a preﬁx universal Turing machine.
               InessencethisisjustanextensionoftheKolmogorovcomplexityfunctionfor
             strings and sequences (Deﬁnition 2.4.3), to enumerable semi-measures. Indeed,
             all the key theoretical properties of the complexity function remain the same.
             Wecanagain see echos of Solomonoﬀ’s prior for sequences in that enumerable
             semi-measures that can be described by short programs are considered to be
             simple, while ones that require long programs are complex.
               Having deﬁned a suitable complexity measure for enumerable semi-
             measures, we can now construct a prior distribution over Me in a way that
             is similar to what Solomonoﬀ did. As each enumerable semi-measure µ has
             some shortest program p ∈ B∗ that speciﬁes its index, we can set the prior
             probability of µ to be the probability of randomly generating p by ﬂipping a
             coin to get each bit. Formally:
             2.5.3 Deﬁnition. The algorithmic prior probability of µ ∈ Me is,
                                                         −K(µ)
                                           PMe(µ) := 2         .
             34
                                                              2.5. Solomonoﬀ-Levin prior
             As K is coenumerable, from the deﬁnition above we can see that PMe is
           enumerable. Thus, this distribution can only be approximated from below.
             With K as the deﬁnition of hypothesis complexity, PMe clearly respects
           Occam’s razor as each hypothesis µ ∈ Me is assigned a prior probability that
           is a decreasing function of its complexity.   Furthermore, every enumerable
           semi-measure has some shortest program that speciﬁes its index and so ∀µ ∈
           Me:K(µ)>0andthus ∀µ∈Me :PMe(µ)>0. It follows then that for an
           induction system based on Bayes’ rule and the prior PMe, the value of P(ν|D)
           will be non-zero whenever D is consistent with ν, that is, P(D|ν) > 0. Such
           systems will not discard hypotheses that are consistent with the data and thus
           respect Epicurus’ principle of multiple explanations.
             With a prior over our hypothesis space Me, we can now take a mixture to
           deﬁne a prior over the space of sequences, just as we did in Example 2.3.3:
           2.5.4 Deﬁnition.      The Solomonoﬀ-Levin prior probability of a binary se-
           quence beginning with the string x ∈ B∗ is,
                                     ξ(x) := X PMe(ν)ν(x).
                                             ν∈Me
             Clearly this distribution respects Occam’s razor as sets of sequences which
           have high probability under some simple distribution ν, will also have high
           probability under ξ, and vice versa.
             It is easy to see that the presence of just one semi-measure in the above
           mixture is suﬃcient to cause ξ to also be a semi-measure, rather than a prob-
           ability measure. Furthermore, it can be proven that ξ is enumerable but not
           computable. Thus, we have that ξ ∈ Me. The fact that ξ is not a probabil-
           ity measure is not too much of a problem because, as mentioned earlier, it is
           possible to normalise a semi-measure to convert it into a probability measure.
           In situations where we need a universal probability measure the normalised
           version of ξ is useful. Its main drawback is that it is no longer enumerable,
           and thus no longer a member of Me. In most theoretical applications it is
           usual to work with the plain ξ as deﬁned above.
             Afundamental result is that the two priors are strongly related:
           2.5.5 Theorem. The Solomonoﬀ prior M and Solomonoﬀ-Levin prior ξ lie
                                                                       ×
           within a multiplicative constant of each other. That is, M = ξ.
           Due to this relation, in many theoretical applications the diﬀerences between
           the two priors are unimportant. Indeed, it is their shared property of domi-
           nance that is the key to their theoretical power:
           2.5.6 Deﬁnition. For some set of semi-measures M, we say that ν ∈ M is
           dominant if ∀̺ ∈ M there exists a constant c̺ > 0 such that ∀x : c̺ν(x) ≥
                                                  ×
           ̺(x). Or more compactly, ∀̺ ∈ M : ν ≥ ̺.
                                                                                        35
             2. Universal Artiﬁcial Intelligence
               It is easy to see that ξ is dominant over the set of enumerable semi-measures
                                              ∗                                   −K(µ)
             from its construction: for x ∈ B we have ξ(x) ≥ PMe(µ)µ(x) = 2             µ(x).
                                                    ∗   K(µ)                      ×
             It follows then that ∀µ ∈ Me,∀x ∈ B : 2         ξ(x) ≥ µ(x). As M = ξ, we see
             that M is also dominant. We call distributions that are dominant over large
             spaces, such as Me, universal priors. This is due to their extreme generality
             and performance as prior distributions, something that we will explore in the
             next two sections: ﬁrstly in the context of Bayesian theory in general, and
             then in the context of sequence prediction.
             2.6. Universal inference
             As we saw in Section 2.2, Bayes’ rule partially solves the induction problem
             by providing an equation for updating beliefs. This is only a partial solution
             because it leaves open two important issues: how do we choose the class of
             hypotheses H, and what prior distribution PH should we use over this class?
             Various principles and methods have been proposed to solve these problems,
             however they tend to run into trouble, especially for large H. In this section
             we will look at how taking the universal prior PMe over the hypothesis space
             Metheoretically solves these problems.
               Whenapproaching an inductive inference problem from a Bayesian perspec-
             tive, the ﬁrst step is to deﬁne H. The most obvious consideration is that H
             should be large enough to contain the correct hypothesis, or at least a suﬃ-
             ciently close one. Being forced to expand our initial H due to new evidence
             is problematic as both the redistribution of the prior probabilities, and the
             way in which H is extended, can bias the induction process. In order to avoid
             these problems, we should make sure that H is large enough to contain a good
             hypothesis to start with. One solution is to simply choose H to be very large,
             as we did in the previous section where we set H = Me. As this contains all
             computable stochastic hypotheses, a larger hypothesis space should never be
             required.
               Having selected H, the next problem is to deﬁne a good prior distribution
             over this space. Essentially, a prior distribution PH over a space of hypotheses
             Hisanexpression of how likely we think diﬀerent hypotheses are before taking
             the data into account. If this prior knowledge is easily quantiﬁable we can use
             it to construct a prior.   In the case of PMe this simply means taking the
             conditional form of the Kolmogorov complexity function and conditioning on
             this prior information.
               More often, however, we either have insuﬃcient prior information to con-
             struct a prior, or we simply wish the data to ‘speak for itself’. The latter
             case is important when we want to present our ﬁndings to others who may
             not share our prior beliefs. The standard solution is to select a prior that
             is in some sense neutral about the relative likelihood of diﬀerent hypotheses.
             This is known as the indiﬀerence principle. It is what we applied in the coin
             36
                                                                      2.6. Universal inference
           estimation problem in Example 2.3.2 when we set P(ν ) := |H|−1 = 1. That
                                                                       p                3
           is, we simply assumed that a priori all hypotheses were equally likely.
              The indiﬀerence principle works well for small discrete H, however if we
           extend the concept to even small continuous parametrised classes by deﬁning
           a probability density function in terms of the volume of H, problems start to
           arise. Consider again the coin problem, however this time allow the bias of
           the coin to be θ ∈ [0,1]. By the indiﬀerence principle the prior is the uniform
           probability density P(νθ) = 1. Now consider what happens if we look at this
           coin estimation problem in a diﬀerent way, where the parameter of interest
           is actually θ′ := √θ. Obviously, if we take a uniform prior over θ′ this is
           not equivalent to taking a uniform prior over θ. In other words, the way in
           which we view a problem, and thus the way in which we parametrise it, aﬀects
           the prior probabilities assigned by a ‘uniform prior’. Obviously this is not as
           neutral and objective as we would like.
              Consider how the algorithmic prior probability PMe behaves under a simple
           reparametrisation. Let H := {νθ ∈ Mc : θ ∈ Θ} be a set of probability
           measures indexed by a parameter θ ∈ Θ, and deﬁne θ′ := f(θ) where f is
           a computable bijection. It is an elementary fact of Kolmogorov complexity
                                    +                                       −1 ′     +      ′
           theory that K(f(θ)) < K(θ) + K(f), and similarly K(f               (θ )) < K(θ ) +
                                                         +       ′
           K(f), from which it follows that K(θ) = K(θ ). With a straight forward
           extension, the same argument can be applied to the Kolmogorov complexity
                                                             +
           of the indexed measures, resulting in K(νθ) = K(νθ′). From Deﬁnition 2.5.3
           we then see that,
                                            −K(νθ) ×    −K(ν ′)
                            PM (νθ) := 2            = 2      θ   =: PM (νθ′).
                                e                                        e
           Thatis, for any bijective reparametrisation f the algorithmic prior probability
           assigned to parametrised hypotheses is invariant up to a multiplicative con-
           stant. If f is simple this constant is small and thus quickly washes out in the
           posterior distribution, leaving the inference results essentially unaﬀected by
           the change.
              Amore diﬃcult version of the above problem occurs when the transforma-
           tion is non-bijective. For example, deﬁne the new parameter θ′ := (θ − 1)2.
                        1             3                                               ′     2
           Now θ = 4 and θ = 4 both correspond to the same value of θ . Unlike
           in the bijective case, non-bijective transformations also cause problems for
           ﬁnite discrete hypothesis spaces. For example, we might have three hypothe-
           ses, H3 := {heads biased, tails biased, fair}. Alternatively, we could
           regroup to have just two hypotheses, H2 := {biased, fair}. Both H3 and
           H2 cover the full range of possibilities for the coin. However, a uniform prior
           over H assigns a prior probability of 1 to the coin being fair, while a uniform
                   3                                 3
           prior over H assigns a prior probability of just 1 to the same thing.
                         2                                       2
              The standard Bayesian approach is to try to ﬁnd a symmetry group for the
           problem and a prior that is invariant under group transformations. However,
           in some cases there may be no obvious symmetry, and even if there is the
           resulting prior may be improper, meaning that the area under the distribution
                                                                                              37
           2. Universal Artiﬁcial Intelligence
           is no longer 1. Invariance under group transformations is a highly desirable but
           diﬃcult property to attain. Remarkably, under simple group transformations
           PMe canbeproventobeinvariant, againuptoasmallmultiplicative constant.
           For a proof of this, as well as further powerful properties of the universial prior
           distribution, see the paper that this section is based on (Hutter, 2007a).
           2.7. Solomonoﬀ induction
           Givenapriordistributionξ overB∞, itisstraightforward to predict the contin-
           uation of a binary sequence using the same approach as we used in Section 2.3.
           Given prior distribution ξ and the observed string ω1:t ∈ B∞ from a sequence
           ω ∈ B∞ that has been sampled from an unknown computable distribution
           µ∈Mc,ourestimate of the probability that the next bit will be 0 is,
                                     ξ(ω 0)= ξ(ω1:t0).
                                        1:t    ξ(ω  )
                                                  1:t
             Is this predictor based on ξ any good? By deﬁnition, the best possible
           predictor would be based on the unknown true distribution µ that ω has been
           sampled from. That is, the true probability that the next bit is a 0 given an
           observed initial string ω1:t is,
                                    µ(ω 0)= µ(ω1:t0).
                                        1:t    µ(ω )
                                                  1:t
           As this predictor is optimal by construction, it can be used to quantify the
           relative performance of the predictor based on ξ. For example, consider the
           expected squared error in the estimated probability that the tth bit will be a
           0:
                                    X                     2
                              St =       µ(x) ξ(x0)−µ(x0) .
                                   x∈Bt−1
           If ξ is a good predictor, then its predictions should be close to those made by
           the optimal predictor µ, and thus St will be small.
             Solomonoﬀ (1978) was able to prove the following remarkable convergence
           theorem:
           2.7.1 Theorem. For any computable probability measure µ ∈ Mc,
                                     ∞
                                    XSt ≤ ln2K(µ).
                                    t=1        2
             That is, the total of all the prediction errors over the length of the inﬁ-
           nite sequence ω is bounded by a constant. This implies rapid convergence for
           any unknown hypothesis that can be described by a computable distribution
           (for a precise analysis see Hutter, 2007a). This set includes all computable
           38
                                                        2.8. Agent-environment model
          hypotheses over binary strings, which is essentially the set of all well deﬁned
          hypotheses. If it were not for the fact that the universal prior ξ is not com-
          putable, Solomonoﬀ induction would be the ultimate all purpose universal
          predictor.
            Although we will not present Solomonoﬀ’s proof, the following highlights
          the key step required to obtaining the convergence result. For any probability
          measure µ the following relation can be proven,
                                  n
                                 XSt ≤ 1 X µ(x)lnµ(x).
                                  t=1       2 x∈Bn       ξ(x)
          This in fact holds for any semi-measure ξ, thus no special properties of the
          universal distribution have been used up to this point in the proof. Now, by the
                                                                ∗          −K(µ)
          universal dominance property of ξ, we know that ∀x ∈ B : ξ(x) ≥ 2      µ(x).
          Substituting this into the above equation,
             n
            XSt ≤ 1 X µ(x)ln           µ(x)      = ln2K(µ) X µ(x) = ln2K(µ).
                      2              −K(µ)           2                      2
            t=1         x∈Bn        2      µ(x)              x∈Bn
          As this holds for all n ∈ N, the result follows.    It is this application of
          dominance to obtain powerful convergence results that lies at the heart of
          Solomonoﬀ induction, and indeed universal artiﬁcial intelligence in general.
            Although Solomonoﬀ induction is not computable and is thus impractical,
          it nevertheless has many connections to practical principles and methods that
          are used for inductive inference. Clearly, if we deﬁne a computable prior rather
          than ξ, we recover normal Bayesian inference. If we deﬁne our prior to be uni-
          form, for example by assuming that all models have the same complexity, then
          the result is maximum a posteriori (MAP) estimation, which in turn is related
          to maximum likelihood (ML) estimation. Relations can also be established to
          MinimumMessageLength(MML),MinimumDescriptionLength(MDL),and
          Maximum entropy (ME) based prediction (see Chapter 5 of Li and Vit´anyi,
          1997). Thus, although Solomonoﬀ induction does not yield a prediction al-
          gorithm itself, it does provide a theoretical framework that can be used to
          understand various practical inductive inference methods. It is a kind of ideal,
          but unattainable, model of optimal inductive inference.
          2.8. Agent-environment model
          Up to this point we have only considered the inductive inference problem,
          either in terms of inferring hypotheses, or predicting the continuation of a
          sequence. In both cases the agents were passive in the sense that they were
          unable to take actions that aﬀect the future. Obviously this greatly limits
          them. More powerful is the class of active agents which not only observe their
          environment, they are also able to take actions that may aﬀect the environ-
          ment. Such agents are able to explore and achieve goals in their environment.
                                                                                    39
            2. Universal Artiﬁcial Intelligence
                                           observation
                                             reward
                               agent                     environment
                                            action
            Figure 2.1.: The agent and the environment interact by sending action, obser-
                        vation and reward signals to each other.
            Wewill need to consider active agents in order to satisfy our deﬁnition of in-
            telligence, that is, the ability to achieve goals in a wide range of environments.
            Solomonoﬀ induction, although extremely powerful for sequence prediction,
            operates in too limited a setting.
            2.8.1 Example. Consider an agent that plays chess. It is not suﬃcient for
            the agent to merely observe the other player. The agent actually has to decide
            which moves to make, so as to win the game. Of course an important part of
            this will be to carefully observe the other player, infer the strategy they are
            using, and then predict which moves they are likely to make in the future.
            Clearly then, inductive inference still plays an important role in the active
            case. Now, however, the agent has to somehow take this inferred knowledge
            and use it to develop a strategy of moves that will likely lead to winning the
            game. This second part may not be easy. Indeed, even if the agent knew the
            other player’s strategy in detail, it might take considerable eﬀort to ﬁnd a way
            to overcome the other player’s strategy and win the game.                 3
              Theframework in which we describe active agents is what we call the agent-
            environment model. The model consists of two entities called the agent and
            the environment. The agent receives input information from the environment,
            which we will refer to as perceptions, and sends output information back to
            the environment, which we call actions. The environment on the other hand
            receives actions from the agent as input and generates perceptions as output.
            Each perception consists of an observation component and a reward com-
            ponent. Observations are just regular information, however rewards have a
            special signiﬁcance because the goal of the agent is to try to gain as much
            reward as possible from the environment. The basic structure of this agent-
            environment interaction model is illustrated in Figure 2.1.
              The only way that the agent can inﬂuence the environment, and thus the
            rewards it receives, is through its action signals. Thus a good agent is one
            that carefully selects its actions so as to cause the environment to generate as
            muchrewardaspossible. Presumably such an agent will make good use of any
            40
                                2.8. Agent-environment model
      useful information contained in past rewards, actions and observations. For
      example, the agent might ﬁnd that certain actions tend to produce rewards
      while others do not. In more complex environments the relationship between
      the agent’s actions, what it observes and the rewards it receives might be very
      diﬃcult to discover.
       The agent-environment model is the framework used in the area of artiﬁcial
      intelligence known as reinforcement learning. It is equivalent to the controller-
      plant framework used in control theory, where the controller takes the place of
      the agent, and the plant is the environment that must be controlled. With a
      little imagination, a huge variety of problems can be expressed in this frame-
      work: everything from playing a game of chess, to landing an aeroplane, to
      writing an award winning novel. Furthermore, the model says nothing about
      how the agent or the environment work, it only describes their role within the
      framework and thus many diﬀerent environments and agents are possible.
      2.8.2 Example. (Two coins game)Toillustrate the agent-model consider
      the following game. In each cycle two 50¢ coins are tossed. Before the coins
      settle the player must guess at the number of heads that will result: either 0,
      1, or 2. If the guess is correct the player gets to keep both coins and then two
      new coins are produced and the game repeats. If the guess is incorrect the
      player does not receive any coins, and the game is repeated.
       In terms of the agent-environment model, the player is the agent and the
      system that produces all the coins, tosses them and distributes the reward
      when appropriate, is the environment. The agent’s actions are its guesses at
      the number of heads in each iteration of the game: 0, 1 or 2. The observation
      is the state of the coins when they settle, and the reward is either $0 or $1.
       It is easy to see that for unbiased coins the most likely outcome is 1 head
      and thus the optimal strategy for the agent is to always guess 1. However,
      if the coins are signiﬁcantly biased it might be optimal to guess either 0 or 2
      heads depending on the bias.              3
       Having introduced the framework, we now formalise it. The agent sends
      information to the environment by sending symbols from some ﬁnite alphabet
      of symbols, for example, {left,right,up,down}. We call this set the action
      space and denote it by A. Similarly, the environment sends signals to the agent
      with symbols from an alphabet called the perception space, which we denote
      X. The reward space, denoted by R, is always a subset of the rational unit
      interval [0,1]∩Q. Restricting to ration numbers is a technical detail to ensure
      that the information contained in each perception is ﬁnite. Every perception
      consists of two separate parts: an observation and a reward. For example,
      we might have X := {(cold,0.0),(warm,1.0),(hot,0.3)} where the ﬁrst part
      describes what the agent observes (cold, warm or hot) and the second part
      describes the reward (0.0, 1.0 or 0.3).
       To denote symbols being sent we use the lower case variable names a, o and
      r for actions, observations and rewards respectively. We index these in the
                                                41
        2. Universal Artiﬁcial Intelligence
        order in which they occur, thus a1 is the agent’s ﬁrst action, a2 is the sec-
        ond action and so on. The agent and the environment take turns at sending
        symbols, starting with the agent. This produces a history of actions, observa-
        tions and rewards which can be written, a o r a o r a o r .... As we refer
                                   1 1 1 2 2 2 3 3 3
        to interaction histories a lot, we need to be able to represent these compactly.
        Firstly, we introduce the symbol x ∈ X to stand for a perception that con-
        sists of an observation and a reward. That is, ∀k : x := o r . Our second
                                          k   k k
        trick is to squeeze symbols together and then index them as blocks of sym-
        bols. For the complete interaction history up to and including cycle t, we
        can write ax1:t := a1x1a2x2a3...atxt. For the history before cycle t we use
        ax<t := ax1:t−1.
         Before this section all our strings and sequences have been binary, now we
        have strings and sequences from potentially larger alphabets, such as A and
        X. Either we can encode symbols from these alphabets as uniquely identiﬁable
        binary strings, or we can extend our previous deﬁnitions of strings, measures
        etc. to larger alphabets in the obvious way. In some results technical problems
        canarise, for example, it takes some work to extend Theorem 2.7.1 to arbitrary
        alphabets (Hutter, 2001). Here we can safely ignore these technical issues and
        simply extend our previous deﬁnitions to general alphabets.
         Formally, the agent is a function, denoted by π, which takes the current
        history as input and chooses the next action as output. We do not want to
        restrict the agent in any way, in particular we do not require that it is deter-
        ministic. A convenient way of representing the agent then is as a probability
        measure over actions conditioned on the complete interaction history. Thus,
        π(ax1a2) is the probability of action a2 in the second cycle, given that the
        current history is ax1. A deterministic agent is simply one that always assigns
        a probability of 1 to a single action for any given history. As the history that
        the agent can use to select its action expands indeﬁnitely, the agent need not
        be Markovian. Indeed, how the agent produces its distribution over actions
        for any given history is left open. In practical artiﬁcial intelligence the agent
        will of course be a machine and so π will be a computable function. In general
        however, the system generating the probabilities for diﬀerent actions could be
        just about anything: An algorithm that generates probabilities according to
        successive digits of √e, an incomputable function, or even a human pushing
        buttons on a keyboard.
         Wedeﬁne the environment, denoted by µ, in a similar way. Speciﬁcally, for
        all k ∈ N the probability of xk, given the current interaction history ax<kak,
        is given by the conditional probability measure µ(ax<kaxk).
         Technically, neither µ nor π completely deﬁne a measure over the space of
        interaction sequences. They only deﬁne the conditional probability of certain
        symbols given an interaction history: π deﬁnes the conditional probability over
        the actions, and µ of the perceptions. However, taken together they do deﬁne
        a measure over the interaction sequences that we will denote π. Speciﬁcally,
                                               µ
        wecanchain together the conditional probabilities deﬁned by π and µ to work
        42
                                                                   2.9. Optimal informed agents
            out the probability of any interaction. For example,
                               π(ax    ) := π(a )µ(ax )π(ax a )µ(ax ax ).
                               µ     1:2         1      1       1 2        1   2
            When we need to take an expectation over interaction sequences this is the
            measure we will use. However in most other cases we will only need the
            conditional probabilities deﬁned by π or µ.
            2.8.3 Example. To illustrate this formalism, consider again the Two Coins
            Game introduced in Example 2.8.2. Let X := {0,1,2}×{0,1} be the percep-
            tion space representing the number of heads after tossing the two coins and
            the value of the received reward. Likewise let A := {0,1,2} be the action
            space representing the agent’s guess at the number of heads that will occur.
            Assuming two fair coins, and recalling that x := o r , we can represent this
                                                                k      k k
            environment by deﬁning ∀k ∈ N:
                                            1        if a  =0∧o =0∧r =1,
                                            4            k         k         k
                                           
                                           
                                           
                                            3
                                                     if a  =0∧o 6=0∧r =0,
                                            4            k         k         k
                                           
                                           
                                            1
                                           
                                                     if a  =1∧o =1∧r =1,
                                                          k         k         k
                                            2
                          µ(ax    ax ) :=  1         if a  =1∧o 6=1∧r =0,
                               <k    k         2          k         k         k
                                           
                                           
                                            1
                                                     if a  =2∧o =2∧r =1,
                                            4            k         k         k
                                           
                                           
                                            3
                                           
                                                     if a  =2∧o 6=2∧r =0,
                                                          k         k         k
                                            4
                                           
                                           
                                            0       otherwise.
            An agent that performs well in this environment would be,
                                      π(ax<kak) :=  1       for ak = 1,
                                                         0 otherwise.
            That is, always guess that one head will be the result of the two coins being
            tossed. A more complex agent might keep count of how many heads occur in
            each cycle and then adapt its strategy if it seems that the coins are suﬃciently
            biased. For example, a Bayesian agent might use techniques similar to those
            used to predict coin ﬂips in Examples 2.3.2 and 2.3.3.                                 3
            2.9. Optimal informed agents
            In the agent-environment model, the agent’s goal is to receive as much reward
            as possible. Unfortunately, this is not suﬃciently precise as there may be many
            possible reward sequences in a given environment and it is not clear which is
            preferred.
            2.9.1 Example. Consider the following two agents: Agent π1 immediately
            ﬁnds a way to get a reward of 0.5 and does so in every cycle. Thus, after
                                                                                                  43
             2. Universal Artiﬁcial Intelligence
             100 cycles it has received a total reward of 50. Agent π2, however, spends the
             ﬁrst 90 cycles trying to ﬁnd the best possible way to receive reward in each
             cycle. During this time it gets an average reward of 0.1 in each cycle. At
             cycle 90 it works out the optimal behaviour and then receives a reward of 1
             in every cycle thereafter. Thus, after 100 cycles it has received a total reward
             of 90 × 0.1 + 10 = 19. In terms of the total reward received after 100 cycles,
             π1 is superior to π2. However, after 1,000 cycles this has reversed as π1 has a
             total reward of 500, while π2 has a total reward of 919.                     3
               Which of these two agents is the better one? The answer depends on how
             we value reward at diﬀerent points in the future. In some situations we may
             want our agent to perform well quickly, and thus place more value on short
             term rewards. In others, we might only care that it eventually reaches a level
             of performance that is as high as possible, and thus place relatively high value
             on rewards far into the future. Before we can deﬁne an optimal agent, we ﬁrst
             need to formally express our temporal preferences.
               A general approach is to weight, or discount, each reward in a way that
             depends on which cycle it occurs in. Let γ1,γ2,... be the discounts we apply
             to the reward in each successive cycle, where ∀i : γi ≥ 0, and P∞ γi <
                                                                                   i=1
             ∞in order to avoid inﬁnite weighted sums. Now deﬁne the expected future
             discounted reward for agent π interacting with environment µ given interaction
             history ax<t to be,
                                          ∞            !
                     Vπµ(ax ) := E Xγr ax
                      γ     <t             i=t  i i  <t
                                  =     lim X(γ r +···+γ r )π(ax ax               ).
                                       m→∞         t t         m m µ      <t   t:m
                                            axt:m
             As the sum is monotonically increasing in m, and ﬁnitely upper bounded, the
             limit always exists. For t = 1 we drop the interaction history from the notation
             and simply write Vπµ.
                                γ
               Oneof the most common ways to set the discount parameters is to decrease
             them geometrically into the future. That is, set ∀i : γ := αi for some discount
                                                                  i
             rate α ∈ (0,1). By increasing α towards 1 we weight long term rewards more
             heavily, conversely by reducing it we weight them less so. Thus, the parameter
             α controls how short term greedy, or long term farsighted, the agent should
             be.
             2.9.2 Example. Consider again the two agents from Example 2.9.1. As the
             rewards are deterministic for π1 we can drop the expectation,
                                                ∞
                                       Vπ1µ = Xαi0.5=0.5A,
                                         γ
                                                i=1
             44
                                                  2.9. Optimal informed agents
         where A := α is from the standard formulae for geometric series. On the
                    1−α     2
         other hand for agent π ,
                            90        ∞
                    Vπ2µ = Xαi0.1+ Xαi =0.1A(1−α90)+Aα90.
                     γ
                           i=1       i=91
         Equating the two and then solving, we ﬁnd that π2 has higher expected future
                                          p
         discounted reward than π1 when α > 90 4/9 ≈ 0.991.               3
           A major advantage of geometric discounting is that it is mathematically
         convenient to work with. Indeed, it is what we will use in Chapter 6 for our
         reinforcement learning algorithm. In the present context, however, we want
         to keep the development fairly general and thus we will leave the structure
         of γ unspeciﬁed. An even more general approach is to consider the space of
         all bounded enumerable discount sequences. We will take this approach when
         formally deﬁning intelligence in Chapter 4 as it will allow us to completely
         remove γ from the model. Here we will follow the more conventional approach
         to AIXI and simply take γ to be a free parameter.
           Having formalised the agent’s temporal preference in terms of γ, we can now
         deﬁne the optimal agent:
         2.9.3 Deﬁnition.  The optimal agent for an environment µ and discount-
         ing γ is the agent πµ that has maximal expected future discounted reward.
         Formally,
                                 πµ := argmaxVπµ.
                                            π   γ
           Thesuperscript µ emphasises the fact that the agent is optimal with respect
         to the speciﬁc environment µ. This optimality is possible because the agent
         was constructed using µ. In a sense the agent knows what its environment
         is before it has even interacted with it. This is similar to Section 2.7 where
         the optimal sequence predictor was deﬁned using the distribution that was
         generating the sequence to be predicted.
           To understand how the optimal agent πµ behaves in each cycle, we ﬁrst
         express the value function in a recursive form for an arbitrary agent π. From
         the deﬁnition of V ,
         Vπµ(ax ) = lim X(γ r +···+γ r )π(ax ax           )
          γ    <t    m→∞        t t       m m µ    <t  t:m
                          axt:m
                     X"        X                                 #
                   =       lim    (γ r +···+γ r )π(ax     ax    ) π(ax  ax )
                          m→∞       t t        m m µ   1:t  t+1:m µ   <t  t
                      axt     axt+1:m
                   = Xhγr + Vπµ(ax )iπ(ax ax ).                        (2.1)
                           t t   γ    1:t µ   <t  t
                      axt
           In the ﬁrst step we broke cycle t oﬀ both the sum and π. As these do not
                                                           µ
         involve m, we pushed them outside the square brackets and moved the limit
                                                                         45
              2. Universal Artiﬁcial Intelligence
              inside. In the second step we broke oﬀ the ﬁrst discounted reward and dropped
              the sum for this term as it was redundant. The remaining discounted rewards
              are just V with t advanced by one, thus producing the desired recursion in V .
                 This is essentially a discrete time form of the Bellman equation commonly
              used in control theory, ﬁnance, reinforcement learning and other ﬁelds con-
              cerned with optimising dynamic systems (Bellman, 1957; Sutton and Barto,
              1998). Usually it is assumed that the environment is Markovian and thus only
              a limited history needs to be taken into account. Here, however, we include
              the entire interaction history and thus are able to avoid these restrictions on
              the environment. Again, this is to keep the model as general as possible.
                 Consider now how a is chosen by the optimal agent πµ. By deﬁnition, the
                                       t
              optimal action is the one that maximises V. Therefore πµ(ax               a ) = 1 for
                                                                                      <t t
              the expected future discounted reward maximising action, and zero otherwise
              (ties can be broken arbitrarily). Thus, after expanding Equation (2.1) with π
              replaced by πµ, we can replace Pat and πµ(ax<tat) with simply a maximum
              over the possible actions,
                 µ            XXh                  µ         i
              Vπ µ(ax ) =                γ r +Vπ µ(ax ) πµ(ax a )µ(ax ax )
               γ       <t                 t t     γ       1:t        <t t       <t   t
                               at   xt
                            =maxXhγr +Vπµµ(ax )iµ(ax ax )
                                a         t t     γ       1:t       <t   t
                                 t  x
                                      t
                                   X           Xh                           µ          i
                            =max ···max             γ r +···+γ r +Vπ µ(ax             ) µ(ax   ax    )
                                a          a         t t         m m      γ       1:m        <t   t:m
                                 t  x       m x
                                     t           m
              In the last step we have simply unfolded the recursion in V for the ﬁrst m
              cycles. As m → ∞ the term Vπµµ(ax               ) → 0. Thus, if we take the limit
                                                  γ       1:m
              m→∞abovewecandropV withoutaﬀectingtheresult. It follows then that
              the action taken by the optimal policy πµ in the tth cycle is,
              aπµ := argmax lim X maxX··· maxXγ r +···+γ r µ(ax ax                                ).
               t            a   m→∞         a             a          t t        m m         <t   t:m
                              t        x     t+1 x         m x
                                         t        t+1           m
              If there is more than one maximising action in cycle t, we simply select one of
              these in an arbitrary way.
                 Intuitively, in the above equation we can see that the optimal agent takes
              the distribution µ, and in eﬀect does a brute force search through all possible
              futures looking for the action in the current cycle that maximises the expected
              future discounted reward. The agent knows that the environment will always
              respond according to the distribution µ, thus in each cycle it takes the expec-
              tation by summing over all the possible observations x and weighting these by
              their probability according to µ. Furthermore, as the agent always follows an
              optimal strategy, its own future actions are just a series of value maximising
              actions.
              46
                                       2.10. Universal AIXI agent
       2.10. Universal AIXI agent
       Although the agent πµ performs optimally in environment µ, this does not
       meet the requirements for intelligence according to our deﬁnition adopted in
       Chapter1. Whatwerequireisageneralagentthatworkswellinmanydiﬀerent
       environments. Such an agent must learn about its environment by interacting
       with it, and then modify its behaviour accordingly in order to optimise per-
       formance. Only then will the agent have the kind of adaptability to diﬀerent
       environments that we require of an intelligent agent.
        This problem with the optimal agent πµ is similar to the one we encountered
       in Section 2.7. There the optimal sequence predictor was based on the distri-
       bution µ that was actually generating the sequence. Of course, for inductive
       inference µ is unknown and must be inferred by observing the sequence. Thus,
       basing a predictor on µ might be “optimal” in terms of prediction performance,
       but it is in some sense cheating. Moreover, it is certainly not general as the
       predictor is designed for just one environment.
        Solomonoﬀ’s solution was to replace the unknown µ in the optimal pre-
       dictor with a universal prior distribution, such as ξ. This produced an ex-
       tremely powerful universal predictor that rapidly converged to optimal pre-
       dictions whenever the distribution µ was computable (Theorem 2.7.1). In this
       way Solomonoﬀ solved, at least in theory, the problem of predicting sequences
       from unknown distributions. Hutter’s innovation was to do essentially the
       same trick for active agents: he took the optimal active agent πµ, described in
       the previous section, and replaced the unknown µ with a generalised universal
       prior distribution ξ. This produced πξ, also known as AIXI, which will be
       described in this section.
        In order to construct πξ, the ﬁrst thing to do is to generalise ξ in Deﬁni-
       tion 2.5.4 from sequences to active environments. As we saw earlier, an active
       environment µ is an enumerable semi-measure conditioned, in chronological
       order, on a sequence of actions from an agent. The presence of these actions
       causes no problems, indeed the development of a universal distribution over ac-
       tive environments is virtually identical to what we did for sequence prediction
       in Section 2.5.
        It can be shown that the space of all enumerable chronological semi-measures
       can be eﬀectively enumerated. Let E := {µ1,µ2,...} be such an enumeration.
       Deﬁne the Kolmogorov complexity of one of these environments to be the
       length of the shortest program that computes the environment’s index: just
       as we did for distributions over sequences in Deﬁnition 2.5.2. This gives us the
       universal prior probability of a chronological environment ν ∈ E,
                                  −K(ν)
                          PE(ν) := 2 .
       From this prior over environments we can construct a prior over the agent’s
       observations in a given interaction history by taking a mixture over the envi-
                                                       47
             2. Universal Artiﬁcial Intelligence
             ronments,
                                                   X −K(ν)
                                     ξ(ax1:n) :=       2       ν(ax1:n).
                                                   ν∈E
                                                                   −K(ν)
             It is easy to see that ξ is enumerable because 2             is enumerable, as is
             each ν in the sum. Furthermore, this sum of chronological semi-measures is
             itself a chronological semi-measure. Thus, we see that ξ ∈ E. As was the
             case for sequences, the dominance property for ξ can easily be seen by taking
             one element from the sum corresponding to the semi-measure to be dominated.
             Notethatwearereusingthesymbolξ. Whetherwearetalkingaboutξ deﬁned
             over sequences, or over chronological environments, will always be clear from
             the context.
                To construct the AIXI agent πξ, simply take πµ and replace µ with ξ,
             aπξ := argmax lim X maxX··· maxXγ r +···+γ r ξ(ax ax                           ).
               t           a  m→∞        a             a         t t        m m       <t   t:m
                            t        x    t+1 x         m x
                                      t        t+1          m
             This gives us an agent that does not depend on the unknown µ. Replacing
             the true distribution µ in the optimal agent πµ with the universal prior distri-
             bution ξ is essentially the same as what we did in Section 2.7 when deﬁning
             Solomonoﬀ’s universal predictor. Of course now we are working in the more
             general setting of chronological environments rather than just sequences.
                Given that Solomonoﬀ prediction works so well for sequence prediction, we
             might expect the agent πξ deﬁned above to be similarly powerful in chrono-
             logical environments. To some extent this is the case, however analysing the
             performance of universal agents in chronological environments turns out to be
             signiﬁcantly more complex.
                Perhaps the most elementary question concerns whether our generalised ξ
             converges to the true environment µ. It turns out that convergence results
             can be proven, including a result similar to Solomonoﬀ’s convergence result
             generalised to interaction histories. More precisely, it can be proven that the
             total µ-expected squared diﬀerence between µ and ξ is ﬁnite for interaction
             histories sampled from πµ interacting with a computable environment µ. Un-
             fortunately, when the interaction history comes from µ interacting with πξ,
             rather than πµ, we run into trouble. This problem is well illustrated by the
             Heaven and Hell environment from Section 5.3.2 of (Hutter, 2005):
             2.10.1 Example. (Heaven and Hell) Imagine an environment where in
             the ﬁrst cycle the agent is faced with two unmarked doors, one of which must
             be opened. One of these doors leads to “heaven” where the agent receives
             plentiful rewards, and the other leads to “hell” where the agent never gets any
             reward. Once a door is chosen there is no way to go back, the agent is stuck
             in either heaven or hell forever.
                This is no problem for πµ as it knows µ and so it knows which door to take
             to get to heaven. Thus it always achieves maximal future discounted reward.
             The agent πξ, on the other hand, must learn through experience. Of course
             48
                                                                 2.10. Universal AIXI agent
           once it has the necessary experience to make the right choice, it may already
           be too late. All πξ can do is to guess which door to take and hope for the best.
           Obviously the expected performance of πξ will be far below that of πµ.           3
             This example does not expose a design ﬂaw in πξ, in the sense that no
           general agent is able to consistently behave optimally in such environments.
           For example, consider an environment µ′ with the two doors switched. Agent
           πµ would always go to hell in this environment. We could deﬁne an agent
              ′
           πµ which would be optimal in µ′, however it would always go to hell in µ.
           Clearly, no agent could behave optimally in both environments without being
           told what the true environment was in advance. Matching the performance of
           optimal agents in each of their respective environments is thus an impossible
           task for any one agent. As such, we need to think carefully about what it is
           that we want to prove if we are to show that πξ is indeed a very powerful and
           general agent.
             Wehave already seen in Section 2.7 that the above problem does not occur
           in the sequence prediction setting. This is because in sequence prediction an
           agent’s predictions do not aﬀect the future observed sequence and thus mis-
           takes have no consequences beyond the current cycle. This allows Solomonoﬀ’s
           prediction system to be able to learn to perform optimally across the entire
           space of computable sequence prediction problems. Such optimising behaviour
           is possible in certain other classes of environments. What this suggests then is
           that we should focus on classes of environments, such as sequence prediction,
           in which it is at least possible for a general agent to learn to behave optimally.
             We begin by generalising the AIXI model to diﬀerent classes of environ-
           ments. Let E be a non-strict subset of the enumeration E of all enumerable
           chronological semi-measures. Deﬁne the mixture distribution over E,
                                                 X−K(ν)
                                   ζ(ax1:n) :=       2      ν(ax1:n).
                                                 ν∈E
           Nowdeﬁnetheagentπζ basedonζ,justaswedeﬁnedπξ basedonξ. Notethat
           while πξ is a single agent, the agent πζ depends on which class of environments
           E we are considering. If E = E then ζ = ξ and so πζ = πξ. In this sense πζ
           generalises πξ.
             Perhaps the most elementary property that an optimal general agent must
           have is that there should not exist any other agent that is strictly superior.
           More precisely:
           2.10.2 Deﬁnition. An agent π is Pareto optimal if there is no other agent
           ρ such that ∀µ ∈ E,
                                  Vρµ(ax ) ≥ Vπµ(ax )           ∀ax ,
                                    γ     <t       γ      <t        <t
           with strict inequality for at least one µ.
                                                                                           49
           2. Universal Artiﬁcial Intelligence
             NotethatParetooptimality does not rule out the possibility that some other
           agent exists which performs better in some environment in E. It simply means
           that no other agent exists which is at least as good in all environments in E,
           and strictly better in at least one. For the agent πζ the following optimality
           result can be proven (Section 5.5 of Hutter, 2005):
           2.10.3 Theorem. For any E ⊆ E, the agent πζ is Pareto optimal.
             As this holds for any E, it also holds for E = E. Thus, the AIXI agent πξ is
           a Pareto optimal agent over the space of environments E. Note that for any
           space of environments E many Pareto optimal agents may exist.
             Astronger result can be proven showing that πζ is also balanced Pareto op-
           timal (Hutter, 2005). Essentially, this means that any increase in performance
           in some environment due to switching to another agent, is compensated for by
           an equal or greater decrease in performance in some other environment.
             While these Pareto optimality results are very general, they only succeed
           in showing that πζ is superior, or at least equal, to other general agents over
           the same class of environments. The result does not rule out the possibility
           that all general agents, including πζ, typically perform poorly. What we need
           is to show that πζ does indeed learn to perform well in many environments.
           The complication, as we saw in Example 2.10.1 above, is that in some types
           of environments it is impossible for general agents to perform well. Thus we
           somehow need to characterise those types of environments in which it is at
           least possible for a general agent to perform well. Furthermore, even when
           optimal performance is possible for a general agent, we cannot expect such an
           agent to perform optimally immediately. We need a performance measure that
           gives the agent time to learn about the structure of µ through interaction. One
           way to formalise the concept of optimal performance after a period of learning
           is the following:
           2.10.4 Deﬁnition.  An agent π is said to be self-optimising in an environ-
           ment µ if,
                              1 Vπµ(ax ) → 1 Vπµµ(a˙x˙   )
                              Γ γ      <t     Γ γ      <t
                               t               t
           with µ probability 1 as t → ∞. Here ax<t is an interaction history sampled
           from π interacting with µ, and a˙x˙<t is an interaction history sampled from
           πµ interacting with µ. The normalisation factor, deﬁned Γ := P∞ γ , is the
                                                             t     i=t i
           total discount remaining at time t.
             Essentially this says that with high probability the performance of the agent
           π converges to the performance of the optimal agent. The normalisation is
           necessary because the un-normalised expected future discounted reward always
           converges to zero. Thus without it convergence would trivially hold for any
           agent.
             We say that π is self-optimising for the set of environments E if it is self-
           optimising for every environment in E. Furthermore, we say that a set of
           50
                                      2.10. Universal AIXI agent
       environments E admits self-optimising agents if there exists an agent π that
       is self-optimising for E. We also extend the result to non-stationary agents by
       saying that a series of agents π1,π2,... is self-optimising if the above result
       holds with π replaced by π . That is, in the tth cycle agent π is applied.
                       t                     t
        The following powerful self-optimising result can be proven (Hutter, 2005):
       2.10.5 Theorem. If there exists a sequence of self-optimising agents πm for
       a class of environments E, then the agent πζ is also self-optimising for E.
        Intuitively, this result says that the performance of πζ will converge to op-
       timal performance in any class of environments where this is possible for a
       single agent, even if the agent is non-stationary. This is the minimal require-
       ment possible, in the sense that if no self-optimising agent existed for some
       class of environments, then trivially πζ cannot be self-optimising in the class
       either.
        Although this is a strong optimality result, it does have two limitations.
       Firstly, while the result shows that πξ converges to optimal performance when-
       ever this is possible in a class of environments, it does not tell us how fast the
       convergence is. In Solomonoﬀ’s convergence theorem we saw that the con-
       vergence of the universal predictor was extremely rapid. Ideally we would
       like a similar result for active environments. Unfortunately, such a result is
       impossible in general:
       2.10.6 Example. (Needle in a haystack) Imagine an environment with
       N buttons, one of which generates a reward of 1 in every cycle when pressed,
       and all the rest produce no reward. The location of the correct button would
       take roughly log2N bits to encode, thus for most values of N we have K(µ) =
       O(log N). As πξ is not informed prior to the start of the game as to which
          2
       button generates reward, the best it can do is to press the buttons one at
       a time. Thus the expected number of incorrect choices that πζ will make is
               K(µ)
       O(N)=O 2   .                                   3
        Compare this to Theorem 2.7.1 for sequence prediction which bounds the
       total squared diﬀerence in prediction error by O(K(µ)). Here in the active
       case the best possible bound for any general agent is exponentially worse at
          K(µ)                ξ
       O 2   . This is not a design ﬂaw in π as the above limit applies to any gen-
       eral agent. Clearly then, bounds showing rapid convergence are not possible
       in general. We can only hope to prove convergence speed bounds for speciﬁc
       classes of environments. Unfortunately, results in this direction currently exist
       only in very simple settings.
        This is a signiﬁcant weakness in the theory of universal agents. Until further
       results are proven it is hard to say just how fast, or slow, convergence to optimal
       behaviour is in diﬀerent classes of environments. As it appears that results in
       this direction will be diﬃcult, a more elementary result is to establish which
       classes of environments at least admit self-optimising agents, and under what
       conditions. Once we have established this, by Theorem 2.10.5 it would then
                                                      51
      2. Universal Artiﬁcial Intelligence
      follow that πζ is also self-optimising in these environments. If we can show
      this for many classes of environments, it then follows that πζ is able to perform
      well in a wide range of environments, at least in the limit.
       Although the required analysis is not particularly diﬃcult for many of the
      basic classes of environments, sorting out all the deﬁnitions, the relation-
      ships between them and the conditions under which they admit self-optimising
      agents is lengthy and requires some care. This is the subject of the next chap-
      ter.
      52
     3. Taxonomy of Environments
     In the previous chapter we introduced the AIXI agent πξ, and its generalisation
     to arbitrary spaces of environments, πζ. Of particular importance was Theo-
     rem 2.10.5 which roughly said: For any class of environments for which there
     exists a self-optimising agent, the agent πζ deﬁned over this class is also self-
     optimising. Thus, in order to understand the performance of πζ across a wide
     range of environments, we need to understand which classes of environments
     admit self-optimising agents, and which do not. In this chapter we present a
     partial answer to this question by showing that many well known classes of
     environments admit self-optimising agents under reasonable conditions.
      Webegin by formalising some common classes of environments. To do this
     we examine the environments’ measures and in particular the way in which
     they condition on the interaction history. In this way we characterise and
     relate many well known classes of environments, such as Bernoulli schemes,
     Markov chains, and Markov decision processes (MDPs). Some interesting new
     classes of environments naturally arise from the analysis. We then take some
     important classes of problems studied in artiﬁcial intelligence, such as sequence
     prediction and classiﬁcation, and express these too in terms of the structure of
     their measures. This formalisation in terms of chronological measures reveals
     that many classes of environments are special cases of other classes, that is, a
     hierarchy of classes of environments exists. Studying this more closely we see
     that many of these classes are in fact reducible to the class of ergodic MDPs.
     Putting all these relationships together produces a taxonomy of classes of
     environments.
      It is known that certain machine learning algorithms, such as Q-learning,
     are self-optimising in the class of ergodic MDPs. As many of the classes in
     our taxonomy are reducible to ergodic MDPs, it follows that these classes
     also admit self-optimising agents. Thus, from Theorem 2.10.5, we see that πζ
     converges to optimal behaviour in these classes of environments. In this way,
     weclarify our earlier claim that universal agents can learn to behave optimally
     in a wide range of environments, as required by our deﬁnition of intelligence.
     3.1. Passive environments
     The ﬁrst class of environments we will consider is the class of passive environ-
     ments. Loosely speaking, such environments are not aﬀected by the agent’s
     actions. We will be more interested in the active environments to be described
     in later sections.
                                      53
            3. Taxonomy of Environments
            3.1.1 Deﬁnition.    ABernoulli scheme is an environment (A,X,µ) such
            that ∀ax1:k,               µ(ax   ax ) = µ(x ).
                                           <k   k       k
            Astherandomvariables x1,x2,... are independent and identically distributed
            (i.i.d.), one may think of a Bernoulli scheme as being an i.i.d. process.
              The above deﬁnition involves some slight abuse of notation. Essentially,
            whatweareshowingisthatanequivalentmeasurewiththesamename(onthe
            right hand side) can be deﬁned over a reduced parameter space by dropping the
            parameters that have no eﬀect on the value of the original measure (on the left
            hand side). In other words, the equation above indicates that the distribution
            µ over xk can be deﬁned in a way that it is completely independent of the
            history ax<kak.
              Note also that we have written (A,X,µ) in order to specify the action and
            perception spaces associated with the measure. This will be necessary in this
            chapter as we will often need to consider relationships between environments
            that diﬀer in their action and perception spaces.
            3.1.2 Example.      Many simple stochastic processes can be described as
            Bernoulli schemes. Imagine a game where a 6 sided die is thrown repetitively.
            The agent receives a reward of 1 whenever a 6 is thrown, and 0 otherwise.
            There are no actions that the agent can take. Formally, A := {ǫ}, O :=
            {1,2,3,4,5,6} and R := B. The measure is deﬁned ∀ax1:k,
                        1 for x =o r ∈{(1,0),(2,0),(3,0),(4,0),(5,0),(6,1)},
               µ(x ) :=   6       k    k k
                  k       0   otherwise.
                                                                                    3
              Other than perhaps a constant environment, Bernoulli schemes are about
            the simplest environments possible. Despite their simplicity, they are impor-
            tant in statistics where sets of i.i.d. random variables play an important role.
            For example, sampling from a population should ideally produce individuals
            that are both independent of each other, and come from the same underlying
            distribution.
              A natural generalisation of Bernoulli schemes is to allow the next percep-
            tion to depend on the previous observation. This gives us a richer class of
            environments where the distribution over perceptions can change with time:
            3.1.3 Deﬁnition.    AMarkov chain is an environment (A,X,µ) that is a
            Bernoulli scheme ∀ax , and ∀ax   with k > 1,
                                1         1:k
                                     µ(ax<kax ) = µ(ok−1x ).
                                              k            k
              For a Markov chain the last observation completely deﬁnes the system’s
            state and so these outputs are usually referred to as states. In more general
            54
                                                      3.1. Passive environments
         classes of environments this is not the case, so for consistency we will use our
         usual terminology of observations and perceptions. Note that we treat the
         ﬁrst cycle as a special case as there is no previous perception to condition on.
         By requiring the system to be a Bernoulli scheme in the ﬁrst cycle we ensure
         that the ﬁrst action has no eﬀect.
         3.1.4 Example. Imagineagamewherewehavearingshapedplayingboard
         that has been divided into 20 diﬀerent cells. There is a pebble that starts in
         cell 1 and moves around the board as follows: On each turn a standard six
         sided die is thrown to decide how many positions the pebble will be moved
         clockwise around the board. In cells 5 and 15 the agent receives a reward of
         1. Otherwise the reward is 0.
           We can model this system as a Markov chain.  Let A := {ǫ}, O :=
         {0,1,2,...,19}, R := B. For k = 1 the pebble is in cell 1 and thus ∀ax ,
                                                                      1
                                    1 for o =1 ∧ r =0,
                           µ(x ) :=         1       1
                              1      0  otherwise.
         Now deﬁne ∀ax1:k with k > 1,
                      1 for o ∈(o       +1)mod20, ..., (o   +6)mod20	
                      6       k      k−1                  k−1
         µ(o   x ) :=       ∧r =δ      +δ    ,
            k−1 k              k   5,ok   15,ok
                      0 otherwise.
         In this game there is a 1 chance of obtaining reward if the pebble is currently
                             6
         in one of the six cells before either cell 5 or cell 15.          3
           From the deﬁnitions of Bernoulli schemes and Markov chains it is clear that
         in both of these classes an agent is “passive”, in the sense that its actions have
         no eﬀect on the environment’s behaviour. The diﬀerence between the two
         classes is the size of the history that is relevant to determining the next per-
         ception. Increasing the length of this history to the full history of observations
         gives us the most general class of completely passive environments:
         3.1.5 Deﬁnition.   A Totally Passive Environment is an environment
         (A,X,µ) such that ∀ax1:k,
                                µ(ax<kaxk) = µ(o<kxk).
           By construction, this class of environments is a superset of the classes of
         environments deﬁned thus far. We could deﬁne a more limited version of this
         class where the next perception only depends on the last n observations, rather
         than the full history. However, such an environment is mathematically equiv-
         alent to a standard Markov chain where for each history of length n we create
         a unique observation in an enlarged observation space. In this new space the
         environment can then be represented by a ﬁrst order Markov chain. This re-
         duction technique will be used in a more general setting to prove Lemma 3.2.6.
                                                                          55
           3. Taxonomy of Environments
             While totally passive environments are useful in modelling some systems,
           in terms of artiﬁcial intelligence they are relatively uninteresting because the
           agent cannot do anything. We can relax this constraint just a little by only
           requiring that the agent cannot aﬀect future observations:
           3.1.6 Deﬁnition.  A Passive Environment is an environment (A,X,µ)
           such that ∀ax<kaok,
                                  µ(ax<kaok) = µ(o<kok).
             Note that there are no restrictions on the rewards; the environment is free
           to reward or punish the agent in any way. Totally passive environments are
           clearly a special case of passive environments. For more on AIXI in passive
           environments see Section 5.3.2 of (Hutter, 2005).
             Another important special case is the class of problems where the agent is
           rewarded for correctly predicting a sequence that it cannot inﬂuence:
           3.1.7 Deﬁnition. A Sequence Prediction Problem is a passive environ-
           ment (A,X,µ) such that ∀ax ,
                                    1:k
                                  µ(ax<kaork) = µ(aork).
             That is, the reward in each cycle depends entirely on the action and the
           observation that immediately follows it. As sequence prediction environments
           are passive the observations do not depend on the agent’s actions, however
           there is no limit on how long the relevant observation history can be. The
           above deﬁnition makes precise what we meant in previous chapters where
           sequence prediction problems were referred to as being “passive”. For more on
           how AIXI deals with sequence prediction problems see Section 6.2 of (Hutter,
           2005).
           3.1.8 Example. Let A = O := {0,1,...,9}. Deﬁne ∀ax1:k,
                     µ(o<kok) :=  1 if ok the kth digit in 3.141592...,
                                  0 otherwise,
           and
                                     1 if r =1− 1|o −a |,
                          µ(aor ) :=        k      9  k   k
                               k      0  otherwise.
           Thus, in order to maximise reward, the agent must generate successive digits
           of the mathematical constant π. A correct digit gets a reward of 1, while an
           incorrect digit gets a lesser reward proportional to the diﬀerence between the
           correct digit and the guess.                                    3
           56
                                                           3.2. Active environments
          3.2. Active environments
          The simplest active environment is one where the next perception xk depends
          on only the last action ak:
          3.2.1 Deﬁnition. A Bandit is an environment (A,X,µ) such that ∀ax1:k,
                                    µ(ax<kaxk) = µ(axk).
            This class of environments is named after the bandit machines found in casi-
          nos around the world, although the relation to real bandit machines is tenuous.
          Bandit environments are weaker than Markov chains as future perceptions do
          not depend on past observations. However, they do have the ability to re-
          act to the last action, and so in this respect they are more powerful. Even
          though bandit problems are conceptually simple, solving them optimally is
          surprisingly involved (Berry and Fristedt, 1985; Gittins, 1989).
          3.2.2 Example. Imagineamachinethathasndiﬀerentlevers, orarms, that
          the agent can pull. Each arm has a diﬀerent but ﬁxed probability of generating
          a reward. For this example, let the reward be either 0 or 1. The agent’s task
          is to ﬁgure out which arm to pull so that it maximises its expected reward.
          One approach might be to spend time pulling diﬀerent arms and collecting
          statistics in order to estimate which produces the most reward.
            We can formally deﬁne this bandit as follows: Let O := {ǫ}, R := B and
          let A := {1,2,3,...,n} represent the n diﬀerent arms that the agent can pull.
          Let β1,β2,...,βn be the respective probabilities of obtaining a reward of 1
          after pulling the corresponding arm. Now deﬁne ∀ax1:k,
                                         β         for r = 1,
                              µ(ax ) :=     ak          k
                                  k        1−β      for r = 0.
                                               ak       k
                                                                                 3
            Anatural extension to the class of bandits is to allow the next perception
          to depend on both the last observation and the last action. This produces a
          much more powerful class of environments that has been intensively studied
          and has many theoretical and practical applications:
          3.2.3 Deﬁnition.      A (stationary) Markov Decision Process (MDP)
          is an environment (A,X,µ) that is a Bernoulli scheme ∀ax1, and ∀ax1:k with
          k > 1,
                                  µ(ax<kaxk) = µ(ok−1axk).
            Usually MDPs are deﬁned in such a way that the agent does not act before
          the ﬁrst perception. However, in our deﬁnition the ﬁrst cycle is a Bernoulli
          scheme and so the ﬁrst action has no eﬀect anyway. Aside from this detail,
          our deﬁnition is equivalent to the standard deﬁnition (Bellman, 1957). It is
                                                                                 57
            3. Taxonomy of Environments
            immediately clear that this class generalises Bernoulli schemes, bandits and
            Markov chains.
               Whatwehavedeﬁnedabove is a stationary MDP. This is because the prob-
            ability of a perception given the current action and the last perception does
            not change, that is, it is independent of k. In some deﬁnitions the measure µ
            is allowed to vary over time. These non-stationary MDPs can be modelled as
            POMDPs, which will be deﬁned shortly.
            3.2.4 Example. Consider again the simple Markov chain in Example 3.1.4.
            This can be extended to an MDP by allowing the agent to decide whether to
            move clockwise or anticlockwise around the board. Let O := {0,1,...,19}
            and R := B as before, but now A := {,	} as the agent can select which
            direction to move in. For k = 1 the pebble is in cell 1 and thus ∀ax1,
                                           1 for o =1 ∧ r =0,
                                µ(ax ) :=            1        1
                                    1        0 otherwise.
            Now deﬁne ∀ax1:k with k > 1,
                             1 for o ∈{(o        +1)mod20,...,(o        +6)mod20}
                             6        k      k−1                    k−1
                            
                            
                                   ∧r =δ       +δ       ∧ a =,
                             1        k    5,ok    15,ok    k
             µ(ok−1axk) :=     6   for ok ∈ {(ok−1 −1) mod 20,...,(ok−1 −6) mod 20}
                            
                                   ∧r =δ       +δ       ∧ a =	,
                                      k    5,ok    15,ok    k
                            
                             0 otherwise.                                             3
               Anatural way to generalise the class of MDPs is to allow the next perception
            to depend on the last n observations and actions:
            3.2.5 Deﬁnition.     A(stationary) nth order Markov Decision Process
            is an environment (A,X,µ) that is a Bernoulli scheme ∀ax1, and ∀ax1:k with
            k > 1,
                               µ(ax<kax ) = µ(ok−maok−m+1:k−1ax ),
                                        k                          k
            where m := min{n,k}.
                                                                                       th
               Immediately from the deﬁnition we can see that a standard MDP is an n
            order MDP where n = 1. The added complication of the variable m is to allow
            for the situation where the current history length k is less than n.
                                     th
               It might appear that n  order MDPs are more general than standard MDPs,
                                           th
            however it turns out that any n   order MDP can be converted into an equiva-
            lent MDP. This is done by extending the observation space and appropriately
            modifying the measure. The proof follows the same pattern as the reduction
                th
            of n   order Markov chains to standard Markov chains, except that now we
            have to deal with the complication of having actions in the history.
                              th
            3.2.6 Lemma. n       order MDPs can be reduced to MDPs.
            58
                                                              3.2. Active environments
                                                     th
          Proof.     Let (A,X = O×R,µ) be an n         order MDP. To prove the result
          we will deﬁne an equivalent ﬁrst order MDP (A,Z = Q×R,µ˜). We begin by
          deﬁning the new observation space,
                                           n
                                    Q:= [O×(A×O)i−1.
                                          i=1
                                             th
          Everyinteractionhistorythatthen      orderMDPconditionsonisthusuniquely
          represented in Q. Although it complicates things, we need to include histories
          of length less than n to accommodate the ﬁrst n−1 cycles of the system.
             Next we deﬁne a measure µ˜ over the perception space Z, that is equivalent
          to the measure µ over the perception space X. We begin by dealing with the
          ﬁrst interaction cycle: ∀az1 ∈ A × Z deﬁne,
                                µ˜(az1) :=  µ(az1)  for z1 ∈ X,
                                             0       otherwise.
          This makes the processes equivalent in the ﬁrst cycle.
             Next, for interaction cycles k > 1, deﬁne ∀qk−1azk ∈ Q ×A×Z,
                         µ(o      ao          ax )   if q   =o      ao
                              k−m k−m+1:k−1 k           k−1     k−m k−m+1:k−1
                        
                                                       ∧z =q r
          µ˜(q   az ) :=                                    k    k k
              k−1 k                                     ∧q =o          ao           ao ,
                                                           k    k−m+1 k−m+2:k−1 k
                        
                         0                           otherwise.
             That is, if the transition qk−1azk is possible when represented in the original
          environment, then this transition is given the same probability by µ˜ in the
          new environment. Any transition qk−1azk which is impossible in the original
          environment, for example because the two histories represented by qk−1 and
          qk are inconsistent, is given a transition probability of zero by µ˜. Thus, the
          two environments have equivalent structure and dynamics.                   2
             As the proof illustrates, writing down the equation for a higher order MDP
          andworkingwithitiscumbersome. Thus,theaboveresultisusefulasitmeans
          that we only have to deal with ﬁrst order MDPs in our analysis. Nevertheless,
                                                                                      th
          conceptually it is often more natural to think of certain problems as being n
          order MDPs.
             Rather than increasing the history that the measure conditions on, another
          extension is to assume that the agent cannot properly observe the MDP’s
          outputs:
          3.2.7 Deﬁnition.       A Partially Observable Markov Decision Pro-
          cess (POMDP) is an environment (A,X = O×R,µ) deﬁned as follows: Let
               ˜    ˜                                                 ˜
          (A,X =O×R,µ˜)beanMDPcalledthecoreMDP.Letφ:X×X →[0,1]bea
          conditional probability measure of the form φ(x˜x) which expresses the proba-
          bility of perceiving x when the core MDP outputs x˜. Deﬁne ∀ax1:k ∈ (A×X)k,
           µ(ax   ) :=   X µ˜(ax˜ )φ(x˜ x )µ˜(o˜ ax˜ )φ(x˜ x ) ··· µ˜(o˜ ax˜ )φ(x˜ x ).
                1:k                1     1 1     1  2     2 2         k−1   k     k k
                            ˜k
                       x˜1:k∈X
                                                                                     59
            3. Taxonomy of Environments
               The nature of POMDPs is perhaps best illustrated by example.
                                                        ˜
            3.2.8 Example.      Let the core MDP (A,X,µ˜) be the MDP deﬁned in Ex-
            ample 3.2.4. Now imagine that the agent cannot reliably observe which cell
                                                   ˜
            the pebble is in. To do this, let X := X and deﬁne the observation function
            ∀x˜,x ∈ X,                     ( 61
                                              100       if x˜ = x,
                                  φ(x˜x) :=    1        otherwise.
                                              100
            Thus, with probability 0.61 the agent observes the true output of the core
            MDP. The rest of the time it observes some other randomly chosen output.
            The probabilities add up as |X| = |O| × |R| = 20 × 2 = 40. Thus, x can take
            39 values other than x˜.                                                   3
                                                                                   ˜
               The fact that POMDPs generalise MDPs can be seen by letting X = X and
            φ(x˜x) := δx˜,x, in which case it follows that µ = µ˜. That is, the POMDP reduces
                                                    th
            to being its core MDP. Furthermore, as n   order MDPs can be reduced to ﬁrst
            order MDPs, it follows that POMDPs also generalise higher order MDPs.
               To see that POMDPs can deﬁne non-stationary MDPs, consider a POMDP
                                           ˜
            where X is a strict subset of X. Now use this extra internal information in
            the core MDP to keep a parameter that varies over time and that aﬀects the
            core MDP’s behaviour. To the external agent who cannot observe this extra
            information, it appears that the environment is non-stationary.
               Both theoretically and practically this class of environments is diﬃcult to
            work with. However, it does encompass a huge variety of possibilities, includ-
            ing all of the environments considered in this chapter, and many real world
            problems.
            3.3. Some common problem classes
            Many of the problems considered in artiﬁcial intelligence can be expressed as
            classes of environments using our measure notation. In this section we will
            formalise some of them.
            3.3.1 Deﬁnition. AFunctionMaximisationProblemisanenvironment
            (A,X,µ)suchthat O = R and for some objective function f : A → R we have
            ∀ax1:k,
                                    1 if o =f(a ) ∧ r =max{o ,...,o },
                    µ(ax   ax ) :=           k      k      k         1      k
                         <k   k       0 otherwise.
               Essentially, the agent’s actions are interpreted as input to some function,
            and in each cycle the result of the function is returned as an observation. We
            donotsimplyreturnthecurrentvalue of the function as the reward as this dis-
            courages the agent from exploring once a good value has been found. Rather
            60
                                                    3.3. Some common problem classes
          we return the reward associated with the best value found so far. Obviously,
          maximisation problems with diﬀerent ranges, or minimisation problems, can
          be expressed by applying a simple transformation to the original objective
          function. For more on how AIXI deals with various types of function optimi-
          sation problems see Section 6.4 of (Hutter, 2005).
          3.3.2 Example.      Let f(a) := 1 − (a − 1)2. To maximise reward the agent
                                                    4              1
          must generate the action that maximises f, that is, a = 4.                  3
             Artiﬁcial intelligence often considers environments that consist of some kind
          of a game that is repetitively played by the agent. Games such as chess, various
          card games, tic-tac-toe and others belong to this class, so long as at the end
          of each match a new match is started. This can be formalised as follows:
          3.3.3 Deﬁnition.        A Repeated Strategic Game is an environment
          (A,X,µ) where ∃l ∈ N such that ∀ax1:k,
                                  µ(ax<kaxk) = µ(aolm:k−1axk)
          where m := ⌊k/l⌋ is the number of the episode when in cycle k, and l is the
          episode length.
             Clearly, Bernoulli schemes and bandits are repeated strategic games. If we
          want to allow games to ﬁnish before the episode ﬁnishes we can pad the re-
          mainingcycles, and perhaps also reward the system for padded cycles following
          a victory in order to encourage rapid wins. For more on how AIXI deals with
          strategic games see Section 6.3 of (Hutter, 2005).
             Another common type of problem considered in artiﬁcial intelligence is clas-
          siﬁcation. A classiﬁcation problem consists of a domain space W and a set of
          classes Z and some function f : W → Z. The agent must try to learn this
          mapping based on examples. When given cases where the class is missing it
          has to correctly guess the class in order to obtain reward. More formally:
          3.3.4 Deﬁnition.      AClassiﬁcation Problemisanenvironment(A,X,µ)
          set up as follows.  Let W and Z be two sets called the attribute space
          and the class space respectively. Z includes a special symbol “?” used to
          indicate whether the agent needs to guess the class. Let O ⊂ W × Z and let
          f : W → Z \{“?”} where ∀ax       such that x     =wzr      and x =wzr ,
                                        1:k            k−1       k−1       k      k
                                      µ(ax<kawk) = µ(wk),
          and for α ∈ (0,1),
                                          α        if z =f(w ),
                                                      k       k
                               µ(wzk) =     1−α if zk =“?”,
                                          0        otherwise,
                                                                                     61
             3. Taxonomy of Environments
             and
                                      1 z       =“?”∧a =f(w          ) ∧ r = 1,
                     µ(z    a r ) =  1 zk−1 6= “?”∧rk = 0, k−1           k
                         k−1 k k            k−1           k
                                      0 otherwise.
               The ﬁrst condition says that the distribution of the points in the attribute
             space is independent of the system’s history. In other words, this part of xk is
             a Bernoulli scheme. The second condition says that in each cycle the system
             musteither provide a training instance or ask for the agent to classify based on
             the attribute vector. The parameter α controls how often the agent is asked
             to guess the class. The third condition says that reward is given when the
             system asks for a classiﬁcation and the agent guesses it correctly. It is easy to
             see that classiﬁcation problems are passive MDPs.
             3.3.5 Example. LeteachelementofW beavectorofmedicalmeasurements
             for a patient, and Z some list of diseases.    When provided with a list of
             patients’ statistics and their diseases, the agent’s job is to learn a function
             that determines which disease is present given a patient’s medical data.    3
               For more on how AIXI deals with supervised learning problems see Sec-
             tion 6.5 of (Hutter, 2005).
             3.4. Ergodic MDPs
             Intuitively, a Markov chain is ergodic if the current observation cannot impose
             any long term constraints on future observations. Given that the reward in a
             Markov chain only depends on the last observation, being ergodic also implies
             that the current observation does not place any long term constraints on fu-
             ture rewards either. Although the ergodic property is typically studied in the
             context of Markov chains, in this section we will extend the notion to MDPs.
             To illustrate the idea, we begin by considering some Markov chains that are
             not ergodic.
             3.4.1 Example.         Imagine a Markov chain with O := {A,B,C}. The
             chain starts with observation A and then transitions to either B or C. In all
             subsequent cycles the observation remains the same. Thus, once observation
             B has been generated, observation C will never occur. This is a long term
             constraint on future observations, and thus the Markov chain is not ergodic. 3
               It is as if the agent has gone through a one-way door into a part of the
             environment that it can never return from. This is similar to the Heaven
             and Hell environment in Example 2.10.1, except that in the above example
             the agent was unable to choose where to go as the environment was passive.
             Consider now a slightly more subtle example of non-ergodic behaviour.
             62
                                                                 3.4. Ergodic MDPs
          3.4.2 Example. Imagine a Markov chain with O := {A,B}. The environ-
          ment starts with observation A and then in the ﬁrst cycle transitions to B.
          In the following cycle it transitions back to state A, and then in the next it
          returns to B. In this way the system alternates between the two observations.
            This environment might seem ergodic as both possible observations continue
          to occur forever and so the agent clearly has not become conﬁned to just one
          part of the environment. However, if the current observation is A, it must
          be the case that two time steps into the future the observation will again be
          A. In fact, for any even number of time steps into the future the observation
          will always be A. As this is a long term constraint on future observations the
          environment is not ergodic.                                            3
            The above example can be modiﬁed so that it is ergodic: after outputting
          observation A make it so that there is a 0.1 probability of generating this
          again, and a 0.9 probability of outputting B.  Thus, no matter what the
          current observation is, after three time steps the environment could output
          either observation.
            We now formalise the proceeding concepts. We say that two observations
          communicate if it is possible to go from one observation to the other and
          return after some ﬁnite number of steps. A communicating class is a set of
          observations that all communicate with each other, and do not communicate
          with any observations outside this set. If all observations of the Markov chain
          belong to the same communicating class, we say that the Markov chain is
          irreducible. An observation has period k if any return to the observation must
          occur in some multiple of k time steps and k is the largest number with this
          property. For example, if it is only possible to return to an observation in an
          even number of steps, then this observation has period 2. If an observation
          has period 1 then it is aperiodic, and if all observations are aperiodic we say
          that the Markov chain is aperiodic. We can now formally deﬁne what it means
          for a Markov chain to be ergodic:
          3.4.3 Deﬁnition.    AMarkov chain environment (A,X,µ) is ergodic if and
          only if it is irreducible and aperiodic.
            Consider again the two examples above. The technical reason the Markov
          chain in Example 3.4.1 was not ergodic was because the observations A and
          Bwerenotcommunicating and thus the Markov chain was not irreducible. In
          Example 3.4.2 the problem was that both observations had period 2 and thus
          the Markov chain was not aperiodic.
            To extend the concept of being ergodic to cover MDPs consider again the
          relationship between Markov chains and MDPs. LetµbeanMDPenvironment,
          and π an agent that is conditioned on only the last observation. That is,
          ∀ax   a with k > 1 we have,
             <k k
                                   π(ax<kak) = π(ok−1ak).
                                                                                 63
           3. Taxonomy of Environments
           Now consider the measure π that describes how the above environment and
                                    µ
           agent interact. In each cycle we have,
                          π(ax<kax )  :=   Xπ(ok−1a )µ(ok−1ax )
                          µ       k                   k         k
                                          ak∈A
                                      = π(o     x ).
                                          µ k−1 k
           Thus, if π has the form above and is ﬁxed, the distribution over the next
           perception depends on only the last observation. That is, π deﬁnes a Markov
                                                                µ
           chain. In other words, an MDP can be thought of as a Markov chain with the
           addition of actions that allow an agent to inﬂuence future observations. We
           can remove this by taking an appropriate agent and building it into the MDP.
           The resulting system no long has free actions to be chosen, and reverts back
           to being a Markov chain.
             Using this relationship, we can now deﬁne ergodic MDPs in the natural way:
           3.4.4 Deﬁnition.   An MDP environment (A,X,µ) is ergodic if and only if
           there exists an agent (A,X,π) such that π deﬁnes an ergodic Markov chain.
                                                 µ
             Ashigher order MDPs are reducible to MDPs, we will say that a higher order
           MDPis ergodic if it is reducible to an ergodic MDP.
             Consider again the Heaven and Hell environment deﬁned in Example 2.10.1.
           After the ﬁrst interaction cycle the agent is always in either heaven or hell.
           Furthermore, no matter what the agent does, it cannot switch between being
           in heaven or being in hell, it is stuck in its current location for all eternity.
           Thus, no matter what agent we select we cannot create a Markov chain such
           that the observations of being in heaven communicate with the observations
           of being in hell, and so this MDP environment is not ergodic.
             The importance of ergodic MDPs for us will be their relationship to learn-
           ing. In an MDP environment only the current observation and action have
           any impact on future perceptions. When the MDP is ergodic, the current ob-
           servation also has no long term impact on future perceptions that cannot be
           overcome by taking the right actions. This means that in an ergodic MDP
           environment no matter what mistakes an agent might make, there is always a
           way to recover from these. Obviously this is a signiﬁcant property for learning
           agents, and indeed the following important result can be proven:
           3.4.5 Theorem. Ergodic MDPs admit self-optimising agents.
             For references and other details see Appendix B. As ergodic MDPs admit
           self-optimising agents it follows by Theorem 2.10.5 that the universal agent πζ
           deﬁned over the class of ergodic MDPs is also self-optimising. What remains
           to be shown is that many important classes of environments are in fact special
           cases of ergodic MDPs. This is the topic of the next section.
           64
                                    3.5. Environments that admit self-optimising agents
          3.5. Environments that admit self-optimising
                  agents
          In this section we will prove that some of the environments we have deﬁned
          are in fact ergodic MDPs. Thus, by the results in the last section, πζ is
          self-optimising in these environments. Before we begin we ﬁrst need to intro-
          duce one extra property: we need to assume that environments are accessible,
          meaningthattherearenoobservations that have zero probability of ever being
          observed.
          3.5.1 Deﬁnition.      A chronological environment (A,X,µ) is accessible if
          ∀ok,∃ax<kak such that µ(ax<kakok) > 0.
             It is reasonable to assume this because if it is impossible to ﬁnd any ﬁnite
          interaction history that gives some observation a non-zero probability then we
          can simply remove it from the observation space. This produces an equivalent
          environment that is accessible. In particular, this does not interfere with the
          property that ergodic MDPs admit self-optimising agents. These unused extra
          observations play no role.
          3.5.2 Lemma. Bernoulli schemes are ergodic MDPs.
          Proof.     Consider a Bernoulli scheme (A,X,µ). From the deﬁnition of a
          Bernoulli scheme we immediately see that they are a special case of the MDP
          deﬁnition. As the environment is accessible, ∀ok, ∃ax<kak : µ(ax<kakok) > 0.
          Applying the deﬁnition of a Bernoulli scheme, this reduces to ∀ok : µ(ok) > 0.
          Thus, as the next observation does not depend on observations prior to ok−1,
          nor does it depend on the actions or rewards, the agent and environment
          together deﬁne a Markov chain. As all observations are possible at every point
          in time it follows that all observations belong to the same communicating class
          and are aperiodic. That is, the Markov chain is ergodic.                    2
             Notethattheaboveresultholdsindependentoftheagentastheenvironment
          is passive. The same is true for classiﬁcation problems:
          3.5.3 Lemma. Classiﬁcation problems are ergodic MDPs.
          Proof. From Deﬁnition 3.3.4 we see that the distribution over the attribute
          space W is not dependent on the interaction history, and the distribution over
          the class space Z depends only on the attribute in the current cycle and so
          it too is independent of anything in prior cycles. More formally, ∀ax    a o :
                                                                                <k k k
          µ(ax<kakok) = µ(wk)µ(wkzk) where ok := wkzk. Thus the distribution over
          observations is completely independent of prior cycles.
             Again from the deﬁnition of classiﬁcation problems we see that the distribu-
          tion over rewards depends on only the observation in the previous interaction
          cycle and the action in the current cycle. It follows then that in each cycle the
                                                                                      65
                  3. Taxonomy of Environments
                  perception (consisting of an attribute, class and reward) depends on only the
                  action in the current cycle and the observation in the previous cycle. Thus,
                  classiﬁcation problems are MDPs.
                     As the environment is accessible, ∀ok, ∃ax<kak : µ(ax<kakok) > 0. Because
                  the distribution over observations is independent of previous interaction cy-
                  cles this immediately reduces to ∀ok : µ(ok) > 0 and so the environment is
                  ergodic.                                                                                                      2
                     As classiﬁcation problems are passive, in the above proof it was again pos-
                  sible to construct an ergodic Markov chain without specifying the agent. In
                  active environments, such as bandits, we must deﬁne an appropriate agent:
                  3.5.4 Lemma. Bandits are ergodic MDPs.
                  Proof.         Consider a bandit (A,X,µ). By deﬁnition it is trivially an MDP.
                  As the environment is accessible ∀ok,∃ax<kak : µ(ax<kaok) > 0. Applying the
                  deﬁnition of a Bandit this reduces to,
                                                           ∀ok,∃ak : µ(aok) > 0.                                             (3.1)
                     Next we need to show that there exists an agent under which the agent
                  interacting with the environment deﬁnes an ergodic Markov chain. If we deﬁne
                  an agent ∀a : π(a ) := 1 it follows that ∀ax                            ao ,
                                   k         k        |A|                             <k k
                             π(ax<kao ) := X π(a )µ(ao ) = 1 X µ(ao ) =: π(o ).
                             µ            k                      k        k        |A|                 k         µ k
                                                    ak∈A                                 ak∈A
                  From Equation 3.1 it then follows that for each ok at least one of the terms
                  in the above sum is non-zero. Thus, ∀o : π(o ) > 0 and so π is an ergodic
                                                                             k     µ k                        µ
                  Markov chain and therefore µ is an ergodic MDP.                                                               2
                     Unfortunately, repeated strategic games are not ergodic MDPs. The problem
                  is that there may be observations which can only occur at certain points in each
                  episode, for example at the start or the end. Clearly then one cannot deﬁne
                  an agent such that these observations have period 1, making it impossible to
                  construct an ergodic Markov chain. Nevertheless, through a change of action
                  and perception spaces a repeated strategic game can be converted into an
                  equivalent system which is a bandit. Bandits, as we saw above, are ergodic
                  MDPs. For our purposes such a conversion is suﬃcient as it allows these
                  environments to admit self-optimising agents.
                  3.5.5 Lemma. Repeated strategic games are reducible to ergodic MDPs.
                  Proof.           Let (A,X,µ) be a repeated strategic game with episode length
                                                                       ˜          l
                  l.  Now deﬁne a new action space A := A . In this new action space every
                  combination of actions that an agent can take in a single episode of the game
                  is represented by a single action. Similarly, deﬁne a new perception space that
                  66
                                                            3.6. Conclusion
                              ˜     l
         represents each episode, X := X , and now deﬁne a chronological measure µ˜
         over the new spaces such that ∀a˜x˜1:k,
                     µ˜(a˜x˜<ka˜x˜k) = µ˜(a˜x˜k) := µ(ax(k−1)l+1:(k−1)l+l).
                                          ˜ ˜
         By construction the environment (A,X,µ˜) is a bandit and thus by
         Lemma 3.5.4 it is an ergodic MDP.                             2
           Theaboveresults show that Bernoulli schemes, classiﬁcation problems, ban-
         dits and repeated strategic games are either ergodic MDPs or can be reduced to
         one. As such, they all admit self-optimising policies and thus an appropriately
         deﬁned universal agent πζ is self-optimising in these classes of environments.
         As the rewards received in totally passive environments are independent of
         the agent’s behaviour, these trivially admit self-optimising agents, indeed all
         agents are equally “optimal”. What about function optimisation and sequence
         prediction problems?
           Unfortunately, the above approach does not work for function optimisation
         problems as we have deﬁned them. The problem is that they are not MDPs as
         the reward signal depends on more than just the current action and the last
         observation. They can be modelled as a POMDP by including the last reward
         in the core MDP and making this unobservable. In any case, this is not a
         problem because any agent that enumerates the action space will eventually
         hit upon the optimal action and thus is self-optimising. What this highlights
         is that being self-optimising only tells us something about performance in the
         limit, it says nothing about how quickly an agent will learn to perform well.
           Sequence prediction problems are more problematic. In general, no agent
         can be self-optimising over the class of all sequence prediction problems. To
         see this, simply consider that for any prediction agent there exists a sequence
         wherethenextobservationis always the observation which the agent predicted
         wouldbetheleastlikely (for a formal statement of this see Lemma 5.2.4). This
         is true even for incomputable agents. If we restrict the sequences to have com-
         putable distributions, but still allow the agent to be incomputable, then we
         have seen that Solomonoﬀ’s predictor has a bounded total expected predic-
         tion error. As the prediction error converges to zero, the reward converges to
         optimal and so Solomonoﬀ’s predictor is self-optimising. Given that the uni-
         versal agent was built upon the same foundations as Solomonoﬀ’s predictor,
         wemightthenexpectthesameresulttohold. Atpresentnobodyhasbeenable
         to prove this, though it is conjectured to be true. Currently the best bound
         is exponentially worse and holds only for deterministic computable sequence
         prediction (Section 6.2.2 of Hutter, 2005). For our purposes an exponentially
         worse bound is still ﬁnite and thus it follows that a universal agent deﬁned
         over the space of computable sequences is self-optimising.
                                                                        67
      3. Taxonomy of Environments
      3.6. Conclusion
      In this chapter we have deﬁned a range of classes of environments and shown
      that many of these are either special cases of other more general classes, or
      are at least reducible to more elementary classes through a change of action
      and perception spaces. This hierarchy of classes deﬁnes a kind of taxonomy
      of environments. To the best of our knowledge this analysis has not been
      done before. Figure 3.1 summarises these relationships. The most general
      and powerful classes of environments are at the top and the most limited and
      speciﬁc classes at the bottom. As we can see, all of the more concrete classes at
      the bottom of the hierarchy admit self-optimising agents. By Theorem 2.10.5
      it then follows that a universal agent deﬁned over one of these classes is also
      self-optimising. This supports our earlier claim that universal agents are able
      to perform well in a wide range of environments, as required by our informal
      deﬁnition of intelligence.
      68
                                3.6. Conclusion
     Figure 3.1.: Taxonomy of environments. Downward arrows indicate that the
          class below is a special case of the class above. Dotted horizontal
          lines indicate that two classes of environments are reducible to
          each other. The greyed area contains the classes of environments
          that admit self-optimising agents, that is, the environments in
          which a universal agent will learn to behave optimally.
                                      69
           4. Universal Intelligence Measure
                 “...we need a deﬁnition of intelligence that is applicable to ma-
                 chines as well as humans or even dogs. Further, it would be helpful
                 to have a relative measure of intelligence, that would enable us to
                 judge one program more or less intelligent than another, rather
                 than identify some absolute criterion. Then it will be possible to
                 assess whether progress is being made ...” Johnson (1992)
             In Chapter 1 we explored the concept of intelligence and proposed an in-
           formal deﬁnition of intelligence. In Chapter 2 we introduced universal agents,
           and in Chapter 3 we detailed some of the classes of environments in which
           their behaviour converges to optimal. This shows that universal agents are
           highly intelligent with respect to the deﬁnition of intelligence that we have
           adopted. One could argue that the universal agent deﬁned over the space of
           all enumerable chronological environments, that is AIXI, is in some sense an
           optimal machine intelligence.
             In this chapter we turn this idea on its head: Instead of using the theory of
           universal artiﬁcial intelligence to deﬁne powerful agents, we use it instead to
           formally deﬁne intelligence itself. One approach is to take AIXI and to math-
           ematically deﬁne a performance measure under which AIXI is the maximal
           agent by construction. This is the approach taken by the Intelligence Order
           Relation (see Section 5.1.4 in Hutter, 2005). Although this produces a very
           general relation for comparing the relative performance of agents, in order to
           justify calling this a formal deﬁnition of “intelligence” one must carefully ex-
           amine the way in which intelligence is deﬁned, and then show how this relates
           to the equation.
             In this chapter we bridge this gap by proceeding in the opposite direction.
           Webegin with our informal deﬁnition of intelligence from Chapter 1 that was
           based on a range of standard deﬁnitions given by psychologists and artiﬁcial
           intelligence researchers.  We then formalise this deﬁnition, borrowing ideas
           from reinforcement learning, Kolmogorov complexity, Solomonoﬀ induction
           and universal artiﬁcial intelligence theory as necessary. The result is an equa-
           tion for intelligence that is strongly related to existing deﬁnitions, and with
           respect to which highly intelligent agents can be proven to have powerful op-
           timality properties. We then look at some of this deﬁnition’s properties and
           compare it to other tests and deﬁnitions of machine intelligence.
                                                                                          71
                 4. Universal Intelligence Measure
                 4.1. A formal deﬁnition of machine intelligence
                 Consider again our informal deﬁnition of intelligence:
                         Intelligence measures an agent’s ability to achieve goals in a wide
                         range of environments.
                    Thisdeﬁnition contains three essential components: An agent, environments
                 and goals. Clearly, the agent and the environment must be able to interact
                 with each other, speciﬁcally, the agent needs to be able to send signals to
                 the environment and also receive signals being sent from the environment.
                 Similarly, the environment must be able to send and receive signals. In our
                 terminology we will adopt the agent’s perspective on these communications
                 and refer to the signals sent from the agent to the environment as actions, and
                 the signals sent from the environment to the agent as perceptions.
                    Our deﬁnition of an agent’s intelligence also requires there to be some kind
                 of goal for the agent to try to achieve. Perhaps an agent could be intelligent,
                 in an abstract sense, without having any objective to apply its intelligence
                 to. Or perhaps the agent has no desire to exercise its intelligence in a way
                 that aﬀects its environment. In either case, the agent’s intelligence would be
                 unobservable and, more importantly, of no practical consequence. Intelligence
                 then, at least the concrete kind that interests us, comes into eﬀect when the
                 agent has an objective or goal that it actively pursues by interacting with its
                 environment.
                    The existence of a goal raises the problem of how the agent knows what the
                 goal is. One possibility would be for the goal to be known in advance and
                 for this knowledge to be built into the agent. The problem with this is that
                 it limits each agent to just one goal. We need to allow agents that are more
                 ﬂexible, speciﬁcally, we need to be able to inform the agent of what the goal
                 is.  For humans this is easily done using language. In general however, the
                 possession of a suﬃciently high level of language is too strong an assumption
                 to make about the agent. Indeed, even for something as intelligent as a dog
                 or a cat, direct explanation is not very eﬀective.
                    Fortunately there is another possibility which is, in some sense, a blend of
                 the above two. We deﬁne an additional communication channel with the sim-
                 plest possible semantics: a signal that indicates how good the agent’s current
                 situation is.      We will call this signal the reward. The agent simply has to
                 maximise the amount of reward it receives, which is a function of the goal. In
                 a complex setting the agent might be rewarded for winning a game or solving
                 a puzzle. If the agent is to succeed in its environment, that is, receive a lot of
                 reward, it must learn about the structure of the environment and in particular
                 what it needs to do in order to get reward.
                    This system of an agent interacting with an environment and trying to
                 achieve some goal is the reinforcement learning agent-environment framework
                 from Section 2.8. That this framework ﬁts well with our informal deﬁnition
                 of intelligence is not surprising given how simple and general it is. Indeed,
                 72
                             4.1. A formal deﬁnition of machine intelligence
        it is not only used in artiﬁcial intelligence, in control theory it is known as
        the plant-controller framework. For example, the plant could be a nuclear
        power plant, and the controller a system designed to keep the reactor within
        safe operating guidelines. Even the way in which you might train your dog
        to perform tricks by rewarding certain behaviours ﬁts into this very general
        framework.
         AsinSection2.8, wewillincludetherewardsignalasapartoftheperception
        generated by the environment. The perceptions also contain a non-reward
        part, which we will refer to as observations. The goal is implicitly deﬁned by
        the environment as this is what controls when rewards are generated. Thus,
        in the framework as we have deﬁned it, to test an agent in any given way it is
        suﬃcient to fully deﬁne the environment.
         Unfortunately, maximising reward is not suﬃcient to deﬁne how the agent
        should behave over time. We have to deﬁne some kind of a temporal preference
        that describes how much the agent should value near term rewards verses
        rewards further into the future. As we saw in Section 2.9, a general approach
        is to weight, or discount, each reward in a way that depends on which cycle
        it occurs in. Let γ1,γ2,... be the discounts we apply to the reward in each
        successive cycle, where ∀i : γi ≥ 0, and P∞ γi < ∞ in order to avoid inﬁnite
                                     i=1
        weighted sums. Now deﬁne the expected future discounted reward for agent π
        interacting with environment µ to be,
                                    ∞    !
                           Vπµ := E Xγr .
                            γ          i i
                                    i=t
        It is this value function that incorporates our temporal preferences that the
        agent must optimise. Although this is very general, the discounting parame-
        ters γ1,γ2,... are nevertheless free parameters. In order to make our formal
        measure unique we want to remove these parameters, and of course we must
        do so in a way that is still completely general.
         If we look at the value function above, we see that discounting plays two
        roles. Firstly, it normalises rewards received so that their sum is always ﬁnite.
        Secondly, it weights the rewards at diﬀerent points in the future which in eﬀect
        deﬁnes a temporal preference. A direct way to solve both of these problems,
        without needing an external parameter, is to simply require the total reward
        returned by the environment to be bounded. Without loss of generality, we
        set the bound to be 1. We denote this set of reward-summable environments
        by E. For any µ ∈ E, it follows that the expected value of the sum of rewards
        is also ﬁnite and thus discounting is no longer required,
                                   ∞ !
                           Vπ := E Xr    ≤1.
                            µ         i
                                   i=1
         One way of viewing this is that the rewards returned by the environment
        now have the temporal preference already factored in. Indeed, because every
                                                             73
      4. Universal Intelligence Measure
      reward summable environment is included, in eﬀect every possible temporal
      preference is represented in the space of environments. The cost is that this is
      anadditional condition that we place on the space of environments. Previously
      we required that each reward signal was in a subset of [0,1]∩Q, now we have
      the additional constraint that the reward sum is always bounded.
       Nextweneedtoquantifywhatwemeanby“goalsinawide rangeofenviron-
      ments.” As we have argued previously, intelligence is not simply the ability
      to perform well at a narrowly deﬁned task; it is much broader. An intelli-
      gent agent is able to adapt and learn to deal with many diﬀerent situations,
      kinds of problems and types of environments. In our informal deﬁnition this
      was described as the agent’s general ability to perform well in a “wide range
      of environments.” This ﬂexibility is a deﬁning characteristic and one of the
      most important diﬀerences between humans and many current AI systems:
      while Gary Kasparov would still be a formidable player if we were to change
      the rules of chess, IBM’s Deep Blue chess super computer would be rendered
      useless without signiﬁcant human intervention.
       As we want our deﬁnition to be as broad and encompassing as possible, the
      space of environments used should be as large as possible. As the environment
      is a probability measure with a certain structure, an obvious possibility would
      be to consider the space of all probability measures of this form. Unfortu-
      nately, this extremely broad class of environments causes serious problems.
      As the space of all probability measures is uncountably inﬁnite, some environ-
      mentscannotbedescribedinaﬁnitewayandsoareincomputable. Thiswould
      make it impossible, by deﬁnition, to test an agent in such an environment us-
      ing a computer. Further, most environments would be inﬁnitely complex and
      have little structure for the agent to learn from.
       The solution is to require the environmental probability measures to be
      computable. Not only is this condition necessary if we are to have an eﬀective
      measure of intelligence, it is also not as restrictive as it might ﬁrst appear.
      There are still an inﬁnite number of environments with no upper bound on
      their maximal complexity. Also, although the measures that describe the en-
      vironments are computable, this does not mean that the environments are de-
      terministic. For example, although a typical sequence of 1’s and 0’s generated
      by ﬂipping a coin is not computable, the probability measure that describes
      this distribution is computable and thus it is included in our space of pos-
      sible environments. Indeed, there is currently no evidence that the physical
      universe cannot be simulated by a Turing machine in the above sense (for fur-
      ther discussion of this point see Section 4.4). This appears to be the largest
      reasonable space of environments.
       Wehavenowformalisedalltheelementsofourinformaldeﬁnition. Thenext
      problem is how to bring these together in order to deﬁne an overall measure
      of performance; we need to ﬁnd a way to combine an agent’s performance
      in many diﬀerent environments into a single overall measure. As there are an
      inﬁnite number of environments, we cannot simply take a uniform distribution
      74
                                       4.1. A formal deﬁnition of machine intelligence
          over them. Mathematically, we must weight some environments higher than
          others. But how?
            Consider the agent’s perspective on this situation: there exists a probabil-
          ity measure that describes the true environment, however this measure is not
          known to the agent. The only information the agent has are some past ob-
          servations of the environment. From these, the agent can construct a list of
          probability measures that are consistent with the observations. We call these
          potential explanations of the true environment hypotheses. As the number of
          observations increases, the set of hypotheses shrinks and hopefully the remain-
          ing hypotheses become increasingly accurate at modelling the environment.
            The problem is that in any given situation there will likely be a large num-
          ber of hypotheses that are consistent with the current set of observations.
          The agent must keep these in accordance with Epicurus’ principle of multiple
          explanations, as we saw in Section 2.1. Because they are all consistent with
          the current observations, if the agent is going to estimate which hypotheses
          are the most likely to be correct it must resort to something other than this
          observational information. This is a frequently occurring problem in inductive
          inference for which the most common approach is to invoke the principle of
          Occam’s razor, which we also met in Section 2.1:
                Given multiple hypotheses that are consistent with the data, the
                simplest should be preferred.
          This is generally considered the rational and intelligent thing to do (Wallace,
          2005). Indeed, as noted in Section 1.5, standard IQ tests implicitly test an
          individual’s ability to use Occam’s razor. In some cases we may even consider
          the correct use of Occam’s razor to be a more important demonstration of
          intelligence than achieving a successful outcome. Consider, for example, the
          following game:
          4.1.1 Example. (Dumb luck game) A questioner lays twenty $10 notes
          out on a table before you and then points to the ﬁrst one and asks “Yes or
          No?”. If you answer “Yes” he hands you the money. If you answer “No”
          he takes it from the table and puts it in his pocket. He then points to the
          next $10 note on the table and asks the same question. Although you, as
          an intelligent agent, might experiment with answering both “Yes” and “No”
                                th
          a few times, by the 13   round you would have decided that the best choice
          seems to be “Yes” each time. However what you do not know is that if you
                                 th
          answer “Yes” in the 13    round then the questioner will pull out a gun and
          shoot you! Thus, although answering “Yes” in the 13th round is the most
          intelligent choice, given what you know, it is not the most successful one. An
          exceptionally dim individual may have failed to notice the obvious relationship
          between answers and getting the money, and thus might answer “No” in the
            th
          13   round, thereby saving his life due to what could truly be called “dumb
          luck”.                                                                   3
                                                                                   75
          4. Universal Intelligence Measure
            Whatisimportantthen,isnotthatanintelligentagentsucceedsinanygiven
          situation, but rather that it takes actions that we would expect to be the most
          likely ones to lead to success. Given adequate experience this might be clear,
          however experience is often not suﬃcient and one must fall back on good prior
          assumptions about the world, such as Occam’s razor. It is important then that
          we test the agents in such a way that they are, at least on average, rewarded
          for correctly applying Occam’s razor, even if in some cases this leads to failure.
            Note that this does not necessarily mean always following the simplest hy-
          pothesis that is consistent with the observations. It is just that simpler hy-
          potheses are considered to be more likely to be correct. Thus, if there is a
          simple hypothesis suggesting one thing, and a large number of slightly more
          complex hypotheses suggesting something else, the latter may be considered
          the most likely.
            There is another subtlety that needs to be pointed out. Often intelligence is
          thought of as the ability to deal with complexity. Or in the words of one psy-
          chologist, “...[intelligence] is the ability to deal with cognitive complexity —
          in particular, with complex information processing.”(Gottfredson, 1997b) It is
          tempting then to equate the diﬃcultly of an environment with its complex-
          ity. Unfortunately, things are not so straightforward. Consider the following
          environment:
          4.1.2 Example.   Imagine a very complex environment with a rich set of
          relationships between the agent’s actions and observations. The measure that
          describes this will have a high complexity. However, also imagine that the re-
          ward signal is always maximal no matter what the agent does. Thus, although
          this is a very complex environment in which the agent is unlikely to be able to
          predict what it will observe next, it is also an easy environment in the sense
          that all agents are optimal, even very simple ones that do nothing at all. The
          environment contains a lot of structure that is irrelevant to the goal that the
          agent is trying to achieve.                                   3
            From this perspective, a problem is thought of as being diﬃcult if the sim-
          plest good solution to the problem is complex. Easy problems on the other
          hand are those that have simple solutions. This is a very natural way to think
          about the diﬃculty of problems, or in our terminology, environments.
            Fortunately, this distinction does not aﬀect our use of Occam’s razor. This
          is because Occam’s razor assigns to each hypothesis a prior probability of it
          being the correct model according to its complexity. It says nothing about how
          relevant or useful that hypothesis might be to the agent’s goals. For example,
          according to Occam’s razor a simple environment that always gives the agent
          maximal reward would be more likely than a complex environment that also
          always gives the agent maximal reward, even though the two environments are
          equally easy to succeed in. Of course from an agent’s perspective, an incorrect
          hypothesis that fails to model much of the environment may be a good one if
          the parts of the environment that the hypothesis fails to model are not relevant
          to receiving reward. Nevertheless, if we want to reward agents on average for
          76
                                        4.1. A formal deﬁnition of machine intelligence
          correctly using Occam’s razor, we must weight the environments according to
          their complexity, not their diﬃculty.
             Although we have chosen to follow a fairly strict interpretation of Occam’s
          razor, the idea of weighting according to the complexity of the simplest good
          solution may have some merit. For example, if we weight the diﬀerent “ex-
          perts” in a prediction with expert advice algorithm according to their complex-
          ity, we are in eﬀect applying this alternate principle. In practice, this can work
          well.
             Our remaining problem now is to measure the complexity of environments.
          This is a problem that we have already solved for sequences in Section 2.5, and
          then generalised to active environments in Section 2.10: for any environment
          µ∈Ewedeﬁneitscomplexity to be K(µ), that is, its Kolmogorov complexity
          which is essentially just the length of the shortest program that describes µ.
             Bringing all these pieces together, we can now deﬁne our formal measure of
          intelligence:
          4.1.3 Deﬁnition.      The universal intelligence of an agent π is its expected
                                                                   −K(µ)
          performance with respect to the universal distribution 2       over the space
          of all computable reward-summable environments E, that is,
                                           X −K(µ) π           π
                                 Υ(π) :=       2      Vµ = Vξ .
                                           µ∈E
          The ﬁnal equality above follows from the linearity of V and the deﬁnition of ξ
          as a weighted mixture of environments. It shows that the universal intelligence
          of an agent is simply its expected performance with respect to the universal
          distribution.
             Consider how this equation corresponds to our informal deﬁnition. We need
          to measure an agent’s ability to achieve goals in a wide range of environments.
          Clearly present in the equation is the agent π, the environment µ and, implicit
          in the environment, a goal. The agent’s “ability to achieve” is represented by
          the value function V π. By a “wide range of environments” we have taken the
                              µ
          space of all computable reward-summable environments, where these environ-
          ments have been characterised as computable chronological measures in the
                                                       −K(µ)
          set E. Occam’s razor is given by the term 2         which weights the agent’s
          performance in each environment in a way that decreases according to its com-
          plexity. The deﬁnition is very general in terms of which sensors or actuators
          the agent might have, as all information exchanged between the agent and the
          environment takes place over very general communication channels. Finally,
          the formal deﬁnition places no limits on the internal workings of the agent.
          Thus, we can apply the deﬁnition to any system that is able to receive and
          generate information with a view to achieving goals.
             The main drawback is that the Kolmogorov complexity function K is not
          computable and can only be approximated. This is acceptable as our aim has
          simply been to deﬁne the concept of intelligence in the most general, powerful
          and elegant way. In future research we will explore ways to approximate this
                                                                                     77
         4. Universal Intelligence Measure
         ideal with a practical test. Naturally, the process of estimation will introduce
         weaknesses and ﬂaws that the current deﬁnition does not have. For example,
         while the deﬁnition considers the general performance of an agent over all
         computable environments with bounded reward sum, in practice a test could
         only ever estimate this by testing the agent on a ﬁnite sample of environments.
          This situation is similar to the deﬁnition of randomness for sequences: In-
         formally, an inﬁnite sequence is said to be Martin-L¨of random when it has no
         signiﬁcant regularity (Martin-L¨of, 1966). This lack of regularity is equivalent
         to saying that the sequence cannot be compressed in any signiﬁcant way, and
         thus we can characterise randomness using Kolmogorov complexity. Naturally,
         we cannot test a sequence for every possible regularity, which is equivalent to
         saying that we cannot compute its Kolmogorov complexity. We can however
         test sequences for randomness by checking them for a large number of sta-
         tistical regularities; indeed, this is what is done in practice. Of course, just
         because a sequence passes all our tests does not mean that it must be random.
         There could always be some deeper structure to the sequence that our tests
         were not able to detect. All we can say is that the sequence seems random
         with respect to our ability to detect patterns.
          Some might argue that the deﬁnition of something should not just capture
         the concept, it should also be practical. For example, the deﬁnition of intel-
         ligence should be such that intelligence can be easily measured. The above
         example, however, illustrates why this approach is sometimes ﬂawed: if we
         were to deﬁne randomness with respect to a particular set of tests, then one
         could speciﬁcally construct a sequence that followed a regular pattern in such
         a way that it passed all of our randomness tests. This would completely under-
         mine our deﬁnition of randomness. A better approach is to deﬁne the concept
         in the strongest and cleanest way possible, and then to accept that our ability
         to test for this ideal has limitations. In other words, our task is to ﬁnd better
         and more eﬀective tests, not to redeﬁne what it is that we are testing for.
         This is the attitude we have taken here, though in this thesis our focus is on
         the ﬁrst part, that is, establishing a strong theoretical deﬁnition of machine
         intelligence.
         4.2. Universal intelligence of various agents
         In order to gain some intuition for our deﬁnition of intelligence, in this sec-
         tion we will consider a range of diﬀerent agents and their relative degrees of
         universal intelligence.
         A random agent. The agent with the lowest intelligence, at least among
         those that are not actively trying to perform badly, would be one that makes
         uniformly random actions. We will call this πrand. Although this is clearly a
         weak agent, we cannot simply conclude that the value of Vπrand will always be
                                                 µ
         low as some environments will generate high reward no matter what the agent
         78
                                          4.2. Universal intelligence of various agents
          does. Nevertheless, in general such an agent will not be very successful as it
          will fail to exploit any regularities in the environment, even trivial ones. It
          follows then that the values of V πrand will typically be low compared to other
                                        µ
          agents, and thus Υ(πrand) will be low. Conversely, if Υ(πrand) is very low, then
          the equation for Υ implies that for simple environments, and many complex
          environments, the value of V πrand must also be relatively low. This kind of poor
                                    µ
          performance in general is what we would expect of an unintelligent agent.
          A very specialised agent.  From the equation for Υ, we see that an agent
          could have very low universal intelligence but still perform extremely well at
          a few very speciﬁc and complex tasks. Consider, for example, IBM’s Deep
          Blue chess supercomputer, which we will represent by πdblue. When µchess
                                      πdblue                      −K(µchess)
          describes the game of chess, V chess is very high. However 2     is small,
                                      µ
          and for µ 6= µchess the value function will be low as πdblue only plays chess.
          Therefore, the value of Υ(πdblue) will be very low. Intuitively, this is because
          Deep Blue is too inﬂexible and narrow to have general intelligence. Current
          artiﬁcial intelligence systems fall into this category: powerful in some domain,
          but not general and adaptable enough to be truly intelligent.
            Interestingly, universal intelligence becomes somewhat counter intuitive
          when we use it to compare very specialised agents. Consider an agent πsimple
          which is only able to learn to predict sequences of the form 0000... and
          1111.... Obviously this agent will fail in most environments and thus will
          have a low universal intelligence, as we would expect. However, environments
          of this form will have short programs and thus are much more likely than
          environments which describe, for example, chess. As πdblue can only play
          chess, it cannot learn to predict these simple sequences, it follows then that
          Υ(πdblue) < Υ(πsimple). Intuitively we would expect the reverse to be true.
            What this shows is that the universal intelligence measure strongly empha-
          sises the ability to solve simple problems. If any system cannot do this, even if
          it can do something relatively complex like play chess, then it is considered to
          have very little intelligence. Of course extreme cases such the one above only
          occur with artiﬁcial constructions such as chess playing machines. Any human
          able to play chess would easily be able to learn to predict trivial patterns such
          as 0000....
            Withtheaboveinmind, it is interesting to consider the progress of artiﬁcial
          intelligence as a ﬁeld from the perspective of universal intelligence. In the early
          daysofartiﬁcial intelligence there was a lot of emphasis on developing machines
          that were able to do simple reasoning and pattern matching etc. Extending
          the power of these general systems was diﬃcult and over time the ﬁeld become
          increasingly concerned with very narrow systems that were able to solve quite
          speciﬁc problems. This has lead some people to complain that while we now
          have impressive systems for some speciﬁc things, we have not progressed much
          towards true intelligence, meaning artiﬁcial general intelligence. Indeed, from
                                                                                79
            4. Universal Intelligence Measure
            the perspective of universal intelligence, by focusing on increasingly specialised
            systems we have in fact have gone backwards.
            A general but simple agent.      Imagine an agent that performs very basic
            learning by building up a table of observation and action pairs and keeping
            statistics on the rewards that follow. Each time an observation that it has
            seen before occurs, the agent takes the action with highest estimated expected
            reward in the next cycle with 0.9 probability, or a random action with 0.1
            probability. We will call this agent πbasic. It is clear that many environments,
            both complex and very simple, will have at least some structure that such an
            agent would take advantage of. Thus, for almost all µ we will have Vπbasic >
                                                                                 µ
            Vπrand and so Υ(πbasic) > Υ(πrand). Intuitively, this is what we would expect
              µ
            as πbasic, while very simplistic, is surely more intelligent than πrand.
               Similarly, as πdblue will fail to take advantage of even trivial regularities in
            some of the most basic environments, Υ(πbasic) > Υ(πdblue). This is reason-
            able as our aim is to measure a machine’s level of general intelligence. Thus
            an agent that can take advantage of basic regularities in a wide range of envi-
            ronments should rate more highly than a specialised machine that fails outside
            of a very limited domain.
            Asimple agent with more history.     The ﬁrst order structure of πbasic, while
            very general, will miss many simple exploitable regularities.   Consider the
            following environment µalt. Let A = {up,down} and O = {ε}. In cycle k
                                                      −k
            the environment generates a reward of 2      each time the agent’s action is
            diﬀerent to its previous action. Otherwise the reward is 0. We can deﬁne this
            environment formally,
                                                                      −k
                                            1 if a 6= a      ∧r =2 ,
                           alt                       k     k−1   k
                          µ (ax ax ):=        1   if a =a     ∧r =0,
                                 <k   k              k     k−1   k
                                            0 otherwise.
            Clearly the optimal strategy for an agent is simply to alternate between the
            actions up and down. Even though this is very simple, this strategy requires
            the agent to correlate its current action with its previous action, something
                  basic                                                        k
            that π     cannot do. Note that we set the reward in cycle k to be 2 in order
            to satisfy our bounded reward sum condition.
               A natural extension of πbasic is to use a longer history of actions, ob-
            servations and rewards in its internal table.  Let π2back be the agent that
            builds a table of statistics for the expected reward conditioned on the last
            two actions, rewards and observations. It is immediately clear that π2back
            will exploit the structure of the µalt environment. Furthermore, by deﬁnition
            π2back is a generalisation of πbasic and thus it will adapt to any regularity that
            πbasic can adapt to. It follows then that in general Vπ2back > V πbasic and so
                                                                   µ        µ
            Υ(π2back) > Υ(πbasic), as we would expect. In the same way we can extend the
            history that the agent utilises back further and produce even more powerful
            80
                                                    4.2. Universal intelligence of various agents
                                                                               top
                                                 a = rest or climb
                                                        r = 2-k
                            a = rest                                        a = climb
                                    -k-4          bottom                     r = 0.0
                              r = 2
            Figure 4.1.: A simple game in which the agent climbs a playground slide and
                          slides back down again. A shortsighted agent will always just rest
                          at the bottom of the slide.
            agents that are able to adapt to more lengthy temporal structures and which
            will have still higher universal intelligence.
            A simple forward looking agent.            In some environments simply trying to
            maximise the next reward is not suﬃcient, the agent must also take into ac-
            count the rewards that are likely to follow further into the future, that is,
            the agent must plan ahead. Consider the following environment µslide. Let
            A={rest,climb} and O = {ε}. Imagine there is a slide such as you would
            see in a playground. The agent can rest at the bottom of the slide, for which
                                       −k−4
            it receives a reward of 2        . The alternative is to climb the slide, which gives
            a reward of 0. Once at the top of the slide the agent always slides back down
                                                                            −k
            no matter what action is taken; this gives a reward of 2           . This deterministic
            environment is illustrated in Figure 4.1.
               Because climbing receives a reward of 0, while resting receives a reward of
             −k−4
            2      , a very shortsighted agent that only tries to maximise the reward in
            the next cycle will choose to stay at the bottom of the slide. Both πbasic and
            π2back have this problem, even though they also take random actions with 0.1
            probability and so will occasionally climb the slide by chance. Clearly this is
            not optimal in terms of total reward over time.
               Wecanextend the π2back agent again by deﬁning a new agent π2forward that
            with 0.9 probability chooses its next action to maximise not just the next
            reward, but rˆ +rˆ       , where rˆ and rˆ       are the agent’s estimates of the next
                            k    k+1           k        k+1
            two rewards. As the estimate of rˆ           will potentially depend not only on a ,
                                                    k+1                                              k
            but also on ak+1, the agent assumes that ak+1 is chosen to simply maximise
            the estimated reward rˆ        .
                                       k+1
               The π2forward agent can see that by missing out on the resting reward of
             −k−4                                                             −k
            2       for one cycle and climbing, a greater reward of 2             will be had when
            sliding back down the slide in the following cycle. Note that the value of the
                                                                                                    81
            4. Universal Intelligence Measure
            time index k will have increased by the time the agent gets to slide down,
            however this is not enough to change the optimal course of action.
              By deﬁnition π2forward generalises π2back in a way that more closely reﬂects
            the value function V and thus in general Vπ2forward > V π2back. It then follows
                                                      µ          µ
            that Υ(π2forward) > Υ(π2back) as we would intuitively expect for this more
            powerful agent.
              In a similar way agents of increasing complexity and adaptability can be
            deﬁned which will have still greater intelligence. However, with more complex
            agents it is usually diﬃcult to see whether one agent has more universal intel-
            ligence than another. Nevertheless, the simple examples above illustrate how
            the more ﬂexible and powerful an agent is, the higher V typically is and thus
            the higher its universal intelligence.
            A very intelligent agent.  A very intelligent agent would perform well in
            simple environments, and reasonably well compared to most other agents in
            more complex environments. From the equation for universal intelligence this
            would clearly produce a high value for Υ. Conversely, if Υ was high then
            the equation for Υ implies that the agent must perform well in most simple
            environments and reasonably well in many complex ones also. Thus, the agent
            is able to achieve goals in a wide range of environments, as required by our
            informal deﬁnition of intelligence.
            A super intelligent agent.  Consider what would be required to maximise
            the value of Υ. By deﬁnition, a “perfect” agent would always pick the action
            which had greatest expected future reward. To do this, for every environment
            µ ∈ E the agent must take into account how likely it is that it is facing µ,
            given the interaction history so far and the prior probability of µ, that is,
             −K(µ)
            2     .  It would then consider all possible future interactions that might
            occur, how likely they are, and from this select the action in the current cycle
            that maximises the expected future reward.
              Essentially, this is the AIXI agent described in Section 2.10. The only dif-
            ference is that AIXI was deﬁned using discount parameters, while universal
            intelligence avoided these by requiring the total reward from environments to
            be bounded. If we remove discounting for AIXI and deﬁne it to work over
            reward bounded environments, then the universal intelligence measure is in
            some sense the dual of the universal agent AIXI. It follows that agents with
            very high universal intelligence have powerful performance characteristics.
              With our modiﬁed AIXI being the most intelligent agent by construction,
            we can deﬁne the upper bound on universal intelligence to be,
                                    ¯                     ξ
                                    Υ := maxΥ(π) = Υ π .
                                           π
            This upper bounds the intelligence of all future machines, no matter how
            powerful their hardware and algorithms might be.
            82
                                                 4.3. Properties of universal intelligence
          Ahuman. Forsimpleenvironments, a human should be able to identify their
          structure and exploit this to maximise reward. However, for more complex
          environments it is hard to know how well a human would perform. Much of the
          human brain is set up to process certain kinds of structured information from
          the human sense organs, and thus is quite specialised, at least compared to the
          extremely general setting considered here. Perhaps the amount of universal
          machine intelligence that a human has is not that high compared to some
          machine learning algorithms? It is diﬃcult to know without experimental
          results.
          4.3. Properties of universal intelligence
          What we have presented is a deﬁnition of machine intelligence. It is not a
          practical test of machine intelligence, indeed the value of Υ is not computable
          due to the use of Kolmogorov complexity. Although some of the criteria by
          which we judge practical tests of intelligence are not relevant to a pure def-
          inition of intelligence, many of the desirable properties are similar. Thus, to
          understand the strengths and weaknesses of our deﬁnition, consider again the
          desirable properties for a test of intelligence from Section 1.4.
          Valid.   The most important property of any proposed formal deﬁnition of
          intelligence is that it does indeed describe something that can reasonably be
          called intelligence. Essentially, this is the core argument of this chapter so far:
          wehavetaken a mainstream informal deﬁnition and step by step formalised it.
          Thus, so long as our informal deﬁnition is acceptable, and our formalisation
          argument holds, the result can reasonably be described as a formal deﬁnition
          of intelligence.
             As we saw in the previous section, universal intelligence orders the power
          and adaptability of simple agents in a natural way. Furthermore, a high value
          of Υ implies that the agent performs well on most simple and moderately
          complex environments. Such an agent would be an impressively powerful and
          ﬂexible piece of technology, with many potential uses. Clearly then, univer-
          sal intelligence is inherently meaningful, independent of whether or not one
          considers it to be a measure of intelligence.
          Informative.   Υ(π) assigns to agent π a real value that is independent of the
          performance of other possible agents. Thus we can make direct comparisons
          between many diﬀerent agents on a single scale. This property is useful if we
          want to use this measure to study new algorithms or modiﬁcations to existing
          algorithms. In comparison, some other tests return only comparative results,
          i.e. that one algorithm is better than another, or even just a binary pass or
          fail.
                                                                                      83
            4. Universal Intelligence Measure
            Wide range. As we saw in the previous section, universal intelligence is able
            to order the intelligence of even the most basic agents such as πrand, πbasic,
            π2back and π2forward. At the other extreme we have the theoretical super in-
            telligent agent AIXI which has maximal Υ value. Thus, universal intelligence
            spans trivial learning algorithms right up to incomputable super intelligent
            agents. This seems to be the widest range possible for a measure of machine
            intelligence.
            General.  As the agent’s performance on all well deﬁned environments is fac-
            tored into its Υ value, a broader performance metric is diﬃcult to imagine.
            Indeed, a well deﬁned measure of intelligence that is broader than universal
            intelligence would seem to contradict the Church-Turing thesis as it would
            imply that we could eﬀectively measure an agent’s performance for some well
            deﬁned problem that was outside of the space of computable measures.
            Dynamic. Universal intelligence includes environments in which the agent
            has to learn and adapt its behaviour over time in order to maximise reward.
            Assuch, it is a so called “dynamic intelligence test” that allows rich interaction
            between the agent being tested and its environment (see Section 1.4). In
            comparison, most other intelligence tests are “static”, in the sense that they
            only require the agent to solve isolated one-oﬀ problems. Such tests cannot
            directly measure an agent’s ability to learn and adapt over time.
            Unbiased.  In a standard intelligence test, an individual’s performance is
            judged on speciﬁc kinds of problems, and then these scores are combined to
            produce an overall result. Thus the outcome of the test depends on which
            types of problems it uses, and how each score is weighted to produce the end
            result. Unfortunately, how we do this is a product of many things, includ-
            ing our culture, values and the theoretical perspective on intelligence that
            we have taken. For example, while one intelligence test might contain many
            logical puzzle problems, another might be more linguistic in emphasis, while
            another stresses visual reasoning. Modern intelligence tests like the Stanford-
            Binet try to minimise this problem by covering the most important areas of
            human reasoning both verbally and non-verbally. This helps but it is still
            very anthropocentric as we are only testing those abilities that we think are
            important for human intelligence.
              For an intelligence measure for machines we have to base the test on some-
            thing more general and principled: universal Turing computation. As all pro-
            posed models of computation have thus far been equivalent in their expressive
            power, the concept of computation appears to be a fundamental theoretical
            property rather than the product of any speciﬁc culture. Thus, by weighting
            diﬀerent environments depending on their Kolmogorov complexity, and con-
            sidering the space of all computable environments, we have avoided having to
            deﬁne intelligence with respect to any particular culture, species etc.
            84
                                                   4.3. Properties of universal intelligence
             Unfortunately, we have not entirely removed the problem. The environmen-
                             −K(µ)
           tal distribution 2       that we have used is invariant, up to a multiplicative
           constant, to changes in the reference machine U. Although this aﬀords us some
           protection, the relative intelligence of agents can change if we change our ref-
           erence machine. One approach to this problem is to limit the complexity of
           the reference machine, for example by limiting its state-symbol complexity.
           Weexpectthat for highly intelligent machines that can deal with a wide range
           of environments of varying complexity, the eﬀect of changing from one simple
           reference machine to another will be minor. For simple agents, such as those
           considered in Section 4.2, the ordering of their machine intelligence was also
           not particularly sensitive to natural choices of reference machine. Recently at-
           tempts have been made to make algorithmic probability completely unique by
           identifying which universal Turing machines are, in some sense, the most sim-
           ple (Mu¨ller, 2006). Unfortunately however, an elegant solution to this problem
           has not yet been found.
             Analternate solution, suggested by Peter Dayan (personal communication),
           would be to allow the agent to maintain state between diﬀerent test environ-
           ments. This would mitigate any bias introduced as intelligent agents would
           then be able to adapt to the test’s reference machine over multiple test trials.
           Fundamental. Universal intelligence is based on Turing computation, infor-
           mation and complexity. These are fundamental universal concepts that are
           unlikely to change in the future with changes in technology. It also means
           that universal intelligence is in no way anthropocentric.
           Formal and objective.     Asuniversal intelligence is expressed as a mathemati-
           cal equation, there is little space for ambiguity in the deﬁnition. In particular,
           it in no way depends on any subjective criteria, unlike some other intelligence
           tests and deﬁnitions.
           Fully deﬁned.    For a ﬁxed reference machine, the universal intelligence mea-
           sure is fully deﬁned. In comparison, some tests of machine intelligence have
           aspects which are currently unspeciﬁed and in need of further research.
           Impractical.   In its current form the deﬁnition cannot be directly turned into
           atest of intelligence as the Kolmogorov complexity function is not computable.
           Thus, in its pure form, we can only use it to analyse the nature of intelligence
           and to theoretically examine the intelligence of mathematically deﬁned learn-
           ing algorithms.
             In order to use universal intelligence more generally we will need to con-
           struct a workable test that approximates an agent’s Υ value. The equation
           for Υ suggests how we might approach this problem. Essentially, an agent’s
           universal intelligence is a weighted sum of its performance over the space of all
                                                                                          85
            4. Universal Intelligence Measure
            environments. Thus, we could randomly generate programs that describe envi-
            ronmental probability measures and then test the agent’s performance against
            each of these environments. After sampling suﬃciently many environments,
            the agent’s approximate universal intelligence could be estimated by weighting
            its score in each environment according to the complexity of the environment
            as given by the length of its program. Another possibility might be to try to
            approximate the sum by enumerating environmental programs from short to
            long, as the short ones will make by far the greatest contribution to the sum.
            However, in this case we will need to be able to reset the state of the agent so
            that it cannot cheat by learning our environmental enumeration method. In
            any case, various practical challenges will need to be addressed before univer-
            sal intelligence can be used to construct an eﬀective intelligence test. As this
            would be a signiﬁcant project in its own right, here we focus on the theoretical
            issues surrounding universal intelligence.
            Deﬁnition rather than a test.   As it is not practical in its current form, uni-
            versal intelligence is more of a formal deﬁnition of intelligence than a test of
            intelligence. Some proposals we have reviewed aim to be just tests of intel-
            ligence, others aim to be deﬁnitions, and in some cases they are intended to
            be both. Often the exact classiﬁcation of a proposal as a test or deﬁnition, or
            both, is somewhat subjective.
              Having covered the key properties of the universal intelligence measure, we
            now compare these properties with the properties of the other proposed tests
            and deﬁnitions of machine intelligence surveyed in Section 1.7. Although we
            have attempted to be as fair as possible, some of our judgements will of course
            be debatable. Nevertheless, we hope that it provides a rough overview of the
            relative strengths and weaknesses of the proposals. The summary comparison
            appears in Table 4.1.
            4.4. Response to common criticisms
            Attempting to mathematically deﬁne intelligence is very ambitious and so,
            not surprisingly, the reactions we get can be interesting. Having presented the
            essence of this work as posters at several conferences, and also as a 30 minute
            talk, we now have some idea of what the typical responses are. Most people
            start out sceptical but end up generally enthusiastic, even if they still have a
            few reservations. This positive feedback has helped motivate us to continue
            this direction of research. In this section, however, we will attempt to cover
            some of the more common criticisms.
            It’s obviously false, there’s nothing in your deﬁnition, just a few equations.
            Perhapsthemostcommoncriticismisalsothemostvacuousone: It’sobviously
            86
                                                          4.4. Response to common criticisms
                                                                                   l     j.
                                                        e     e                   a     b   ed          ef.
                                                        v    g                    t    O   n
                                                      ti    n         c    d    en   &    ﬁ    l      D
                                                     a    Ra    l    i    e   m    l     e   ca     s.
                                                    m         ra   m     s   a     a    D    i     v
             Intelligence Test                id   r    e    e     a    ia  d    m    ly   ct     t
                                             l    fo   id   en   yn    b   n    r     l    a     s
                                            a    n                    n   u    o     u    r     e
                                           V    I    W G D U F F F P                           T
             Turing Test                  •     ·    ·    ·    •    ·    ·    ·    •    •     T
             Total Turing Test            •     ·    ·    ·    •    ·    ·    ·    •    ·     T
             Inverted Turing Test         •    •     ·    ·    •    ·    ·    ·    •    •     T
             Toddler Turing Test          •     ·    ·    ·    •    ·    ·    ·    ·    •     T
             Linguistic Complexity        •      • · · · · • · •                              T
             Text Compression Test        •        • · • •       T
             Turing Ratio                 •          ? ? ? ? · ? T/D
             Psychometric AI                  •   ? • · • • • T/D
             Smith’s Test                 •        • · ?     · • T/D
             C-Test                       •        • ·           T/D
             Universal Intelligence                         ·                                 D
            Table 4.1.: In the table   means “yes”, • means “debatable”, · means “no”,
                        and ? means unknown. When something is rated as unknown it is
                        because the test in question is not suﬃciently speciﬁed.
            wrong! Thesepeopleseemtobelievethatdeﬁningintelligencewithanequation
            is clearly impossible, and thus there must be very large and obvious ﬂaws in
            our work. Not surprisingly, these people are also the least likely to want to
            spend 10 minutes having the material explained to them. Unfortunately, none
            of these people have been able to communicate why the work is so obviously
            ﬂawed in any concrete way — despite in one instance chasing the poor fellow
            out of the conference centre and down the street begging for an explanation.
            If anyone would like to properly explain their position to us in the future, we
            promise not to chase you down the street!
            It’s obviously correct, indeed everybody already knows this.           Curiously, the
            second most common criticism is the exact opposite: The work is obviously
            right, and indeed it is already well known. Digging deeper, the heart of this
            criticism comes from the perception that we have not done much more than just
            describe reinforcement learning. If you already accept that the reinforcement
            learning framework is the most general and ﬂexible way to describe artiﬁcial
            intelligence, and not everybody does, then by mixing in Occam’s razor and a
            dash of complexity theory the equation for universal intelligence follows in a
            fairly straightforward way. While this is true, the way in which these things
            have been brought together is new. Furthermore, simply coming up with an
            equation is not enough, one must argue that what the equation describes is in
            fact intelligence in a sense that is reasonable for machines.
              We have addressed this question in three main ways: Firstly, in Chap-
            ter 1 we explored many expert deﬁnitions of intelligence. Based on these,
            we adopted our own informal deﬁnition of intelligence in Section 1.2. In the
                                                                                                87
            4. Universal Intelligence Measure
            present chapter this informal deﬁnition was piece by piece formalised leading
            to the equation for Υ. This chain of argument ties our equation for intelli-
            gence to existing informal deﬁnitions and ideas on the nature of intelligence.
            Secondly, in Sections 4.2 and 4.3 we showed that the equation has properties
            that are consistent with a deﬁnition of intelligence. Finally, in Section 4.2 it
            was shown that universal intelligence is strongly connected to the theory of
            universally optimal learning agents, in particular AIXI. From this it follows
            that machines with very high universal intelligence have a wide range of pow-
            erful optimality properties. Clearly then, what we have done goes far beyond
            merely restating reinforcement learning theory.
            Assuming that the environment is computable is too strong.  It is certainly
            possible that the physical universe is not computable, in the sense that the
            probability distribution over future events cannot, even in theory, be simulated
            to an arbitrary precision by a computable process. Some people take this posi-
            tion on various philosophical grounds, such as the need for freewill. However,
            in standard physics there is no law of the universe that is not computable in the
            above sense. Nor is there any evidence showing that such a physical law must
            exist. This includes quantum theory and chaotic systems, both of which can
            beextremelydiﬃculttocomputeforsomephysicalsystems, butarenotfunda-
            mentally incomputable theories. In the case of quantum computers, they can
            compute with lower time complexity than classical Turing machines, however
            they are unable to compute anything that a classical Turing machine cannot,
            when given enough time. Thus, as there is no hard evidence of incomputable
            processes in the universe, our assumption that the agent’s environment has a
            computable distribution is certainly not unreasonable.
              If a physical process was ever discovered that was not Turing computable,
            then this would likely result in a new extended model of computation. Just as
            we have based universal intelligence on the Turing model of computation, it
            might be possible to construct a new deﬁnition of universal intelligence based
            on this new model in a natural way.
              Finally, even if the universe is not computable, and we do not update our
            formal deﬁnition of intelligence to take this into account, the fact that every-
            thing in physics so far is computable means that a computable approximation
            to our universe would still be extremely accurate over a huge range of situa-
            tions. In which case, an agent that could deal with a wide range of computable
            environments would most likely still function well within such a universe.
            Assuming that environments return bounded sum rewards is unrealistic.  If
            an environment µ is an artiﬁcial game, like chess, then it seems fairly natural
            for µ to meet any requirements in its deﬁnition, such as having a bounded
            reward sum. However, if we think of the environment µ as being the universe
            in which the agent lives, then it seems unreasonable to expect that it should
            be required to respect such a bound.
            88
                       4.4. Response to common criticisms
      Strictly speaking, reward is an interpretation of the state of the environ-
     ment. In this case the environment is the universe, and clearly the universe
     does not have any notion of reward for particular agents. In humans this in-
     terpretation is internal, for example, the pain that is experienced when you
     touch something hot. In this case, should it be a part of the agent rather than
     the environment? If we gave the agent complete control over rewards then our
     framework would become meaningless: the perfect agent could simply give
     itself constant maximum reward. Perhaps the analogous situation for humans
     would be taking the “perfect” drug.
      Amoreaccurate framework would consist of an agent, an environment and
     a separate goal system that interpreted the state of the environment and re-
     warded the agent appropriately. In such a set up the bounded rewards restric-
     tion would be a part of the goal system and thus the above problem would not
     occur. However, for our current purposes, it is suﬃcient just to fold this goal
     mechanism into the environment and add an easily implemented constraint to
     how the environment may generate rewards. One simple way to bound an en-
     vironment’s total rewards would be to use geometric discounting as discussed
     in Section 2.9.
     How do you respond to Block’s “Blockhead” argument? The approach
     we have taken is unabashedly functional. Theoretically, we desired to have a
     formal, simple and very general deﬁnition. This is easier to do if we abstract
     over the internal workings of the agent and deﬁne intelligence only in terms
     of external communications. Practically, what matters is how well something
     works. By deﬁnition, if an agent has a high value of Υ, then it must work well
     over a wide range of environments.
      Block attacks this perspective by describing a machine that appears to be
     intelligent as it is able to pass the Turing test, but is in fact no more than
     just a big look-up table of questions and answers (Block, 1981, for a related
     argument see Gunderson, 1971). Although such a look-up table would be
     unfeasibly large, the fact that a ﬁnite machine could in theory consistently pass
     the Turing test, seemingly without any real intelligence, intuitively seems odd.
     Our formal measure of machine intelligence could be challenged in the same
     way, as could any test of intelligence that relies only on an agent’s external
     behaviour.
      Our response to this is very simple: if an agent has a very high value of
     Υ then it is, by deﬁnition, able to successfully operate in a wide range of
     environments. We simply do not care whether the agent is eﬃcient, due to
     some very clever algorithm, or absurdly ineﬃcient, for example by using an
     unfeasibly gigantic look-up table of precomputed answers. The important
     point for us is that the machine has an amazing ability to solve a huge range
     of problems in a wide variety of environments.
                                      89
            4. Universal Intelligence Measure
            How do you respond to Searle’s “Chinese room” argument?        Searle’s Chi-
            nese room argument attacks our functional position in a similar way by argu-
            ing that a system may appear to be intelligent without really understanding
            anything (Searle, 1980). From our perspective, whether or not an agent un-
            derstands what it is doing is only important to the extent that it aﬀects the
            measurable performance of the agent. If the performance is identical, as Searle
            seems to suggest, then whether or not the room with Searle inside understands
            the meaning of what is going on is of no practical concern; indeed, it is not
            even clear to us how to deﬁne understanding if its presence has no measurable
            eﬀects. So long as the system as a whole has the powerful properties required
            for universal machine intelligence, then we have the kind of extremely general
            and powerful machine that we desire. On the other hand, if understanding
            does have a measurable impact on an agent’s performance in some situations,
            then it is of interest to us. In which case, because Υ measures performance in
            all well deﬁned situations, it follows that Υ is in part a measure of how much
            understanding an agent has.
            But you don’t deal with consciousness (or creativity, imagination, freewill,
            emotion, love, soul, etc.)  We apply the same argument to consciousness,
            emotions, freewill, creativity, the soul and other such things. Our goal is to
            build powerful and ﬂexible machines and thus these somewhat vague properties
            are only relevant to our goal to the extent to which they have some measurable
            eﬀect on performance in some well deﬁned environment. If no such measurable
            eﬀect exists, then they are not relevant to our objective. Of course this is not
            the same as saying that these things do not exist. The question is whether
            they are relevant or not. We would consider understanding, imagination and
            creativity, appropriately deﬁned, to have a signiﬁcant impact on an agent’s
            ability to adapt to challenging environments. Perhaps the same is also true
            of emotions, freewill and other qualities. If one accepts that these properties
            aﬀect an agent’s performance, then universal intelligence is in part a test for
            these properties.
            Intelligence is fundamentally an anthropocentric concept.   As artiﬁcial in-
            telligence researchers our goal is not to create an “artiﬁcial human”. We are
            interested in making machines that are able to process information in pow-
            erful ways in order to achieve many kinds of goals and solve many kinds of
            problems. As such, a limited anthropocentric concept of intelligence is not
            interesting to us. Or at least, if such a deﬁnition were to be adopted, it would
            simply mean that we are interested in something more general and powerful
            than this human focused concept of “intelligence”.
              Perhaps this is similar to the development of heavier than air ﬂight. One
            of the most outspoken sceptics of this was the American astronomer Simon
            Newcomb. Interestingly, even after planes were regularly “ﬂying” he refused
            to accept defeat. He accepted that planes existed and moved around at great
            90
                                                     4.4. Response to common criticisms
           speed up in the air, however he did not accept that what they were doing was
           “ﬂying”. To his mind, what birds did was quite diﬀerent. Of course, the rest
           of the population simply generalised and adapted their concept of ﬂying to
           reﬂect the new technology.
             Webelieve that with more progress in artiﬁcial intelligence the same thing
           will eventually happen to the everyday concept of intelligence. At present,
           most people think of intelligence in human terms simply because this is the
           only kind of powerful intelligence they have ever encountered. Indeed, from the
           perspective of evolutionary psychology, it appears that we may have evolved
           to expect other intelligent agents to think and act like ourselves.
           Universal intelligence does not agree with some everyday intuitions about
           the nature of intelligence.  Everyday intuitions are not a good guide. People
           informally use the word “intelligence” to mean a variety of diﬀerent things, and
           even a single person will use the word in multiple ways that are not consistent
           with each other. Thus, although our deﬁnition should clearly be related to the
           everyday concept, it is not necessarily desirable, or even possible, for a precise
           and self-consistent deﬁnition to always agree with everyday usage.
             Consider a word that has both an everyday meaning and a precise technical
           meaning. When someone says “It’s a beautiful spring day, I am full of energy
           and could run up a mountain”, what they mean by the word ‘energy’ is related
           to the concept of energy in physics, i.e. they need energy in the technical
           sense to get up the mountain. However, the deﬁnition of energy from physics
           does not entirely capture what they mean. This does not imply that there is
           something wrong with the concept of energy in physics. We expect the same
           in artiﬁcial intelligence: people will continue to use the word “intelligence” in
           an informal way, however in order to do research we will need to adopt a more
           precise deﬁnition that may be slightly diﬀerent.
           An agent may be intelligent even if it doesn’t achieve anything in any
           environments.    Consider, for example, that you are quietly sitting in a dark
           room thinking about a problem you are trying to solve. Even though you are
           not achieving anything in your environment, some would argue that you are
           still intelligent due to your internal process of thought. Or imagine perhaps the
           famousphysicist Stephen Hawking disconnected from his motorised wheelchair
           andtalking computer. Although his ability to achieve goals in the environment
           would be limited, he is still no doubt highly intelligent. A related problem is
           that an agent may simply lack the motivation to exhibit intelligent behaviour,
           such as a child who wants to get an IQ test out of the way as soon as possibly
           in order to go outside and play. In both of these cases the agent’s intelligence
           seems to be divorced from the intelligence observable in its behaviour.
             Theabovehighlights the fact that when informally considering the measure-
           ment of intelligence we need to be careful about exactly how the measurement
           is done. Obviously, it only makes sense to measure an individual’s intelligence
                                                                                       91
            4. Universal Intelligence Measure
            if certain essential requirements are met: a purely written test for a blind
            person is senseless, as are the results of a test where the test subject has no
            interest in performing well. Informally, we just assume that the test has been
            conducted in an appropriate way. When we say that an agent is intelligent,
            what we actually mean is that there exists some reasonable setup such that
            the agent exhibits a high level of intelligence: a blind person may require an
            oral test, and a disinterested child some kind of a bribe. Clearly then, if we
            want to be precise about the measured intelligence of an agent we must specify
            these details about exactly how the test was conducted.
               Whenwesaythatanagenthaslowintelligence, what we mean is that there
            does not exist any reasonable test setup such that the agent exhibits intelligent
            behaviour. Some take the position that an entity could be intelligent even if it
            has no measurable intelligence under any test setup. However, such an agent’s
            “intelligence” would be rather meaningless as it would be a property that has
            no measurable eﬀects or consequences. Even the philosophical “brain in a vat”
            could in theory be interfaced to in order to measure its universal intelligence.
            Such an agent is not intelligent because it cannot choose its goals.   In the
            setup we have deﬁned the agent cannot decide what its primary goal is. It
            simply tries to maximise the reward signal deﬁned by the environment. In the
            context of machines this is probably a good idea: we want to be the ones deﬁn-
            ing what the machine’s primary objective is. However, this does not address
            the question as to whether such a machine should really be called intelligent,
            or whether it is just a very powerful and general optimiser. Intelligent humans,
            after all, can choose their own goals in life. But is this really true?
               Obviously we can decide that we want to become a successful scientist, a
            teacher, or maybe a rock star. So we certainly have some choice in our goals.
            But are these things our primary motivations? If we want to be a successful
            scientist or a rock star, perhaps this is due to a deeper biological drive to attain
            high status because this increases our chances of reproductive success. Perhaps
            the desire to become a teacher stems from a biological drive to care for children,
            again because having this drive tends to increase our probability of passing on
            our genes and memes to future generations. Our interest in mating with an
            attractive member of the opposite sex, avoiding intense physical pain or the
            pleasure of eating energy rich foods, all of these things have clear biological
            motivations. Even a suicide bomber who kills himself and thus destroys his
            future reproductive potential may be driven by an out-of-control biological
            motivation that tries to increase his societal status in an eﬀort to improve his
            chances of reproductive success.
               These ideas appear in areas such as evolutionary psychology and, it must
            be said, attract a fair amount of controversy. Here we will not attempt to
            defend them. We simply note that they oﬀer one explanation as to how we
            are able to choose our goals in life, while at the same time having relatively
            ﬁxed fundamental motivations.
            92
                                4.5. Conclusion
     Universal intelligence is impossible due to the No-Free-Lunch theorem.
     Some, such as Edmonds (2006), argue that universal deﬁnitions of intelligence
     are impossible due to Wolpert’s so called “No Free Lunch” theorem (Wolpert
     and Macready, 1997). However this theorem, or any of the standard variants
     on it, cannot be applied to universal intelligence for the simple reason that we
     have not taken a uniform distribution over the space of environments. Instead
     we have used a highly non-uniform distribution based on Occam’s razor.
      It is conceivable that there might exist some more general kind of “No Free
     Lunch” theorem for agents that limits their maximal intelligence according to
     our deﬁnition. Clearly any such result would have to apply only to computable
     agents given that the incomputable AIXI agent faces no such limit. If such a
     result were true, it would suggest that our deﬁnition of intelligence is perhaps
     too broad in its scope. Currently we know of no such result (c.f. Chapter 5).
     4.5. Conclusion
     Given the obvious signiﬁcance of formal deﬁnitions of intelligence for research,
     and calls for more direct measures of machine intelligence to replace the prob-
     lematic Turing test and other imitation based tests (Johnson, 1992), little work
     has been done in this area. In this chapter we have attempted to tackle this
     problem by taking an informal deﬁnition of intelligence modelled on expert
     deﬁnitions of human intelligence, and then formalising it. We believe that the
     resulting mathematical deﬁnition captures the concept of machine intelligence
     in a very powerful and elegant way. Furthermore, we have seen that agents
     which are extremely intelligent with respect to this deﬁnition, such as AIXI,
     can be proven to have powerful optimality properties.
      The central challenge for future work on universal intelligence is to convert
     this theoretical deﬁnition of machine intelligence into a workable test. The
     basic structure of such a test is already apparent from the equation for Υ: the
     test would work by evaluating the performance of an agent on a large sample
     of simulated environments, and then combine the agent’s performance in each
     environment into an overall intelligence value. This would be done by weight-
     ing the agent’s performance in each environment according the environment’s
     complexity.
      Atheoretical challenge that will need to be dealt with is to ﬁnd a suitable
     replacement for the incomputable Kolmogorov complexity function. One so-
     lution could be to use Kt complexity (Levin, 1973), another might be to use
     the Speed prior (Schmidhuber, 2002). Both of these consider the complexity
     of an algorithm to be determined by both its minimal description length and
     running time, thus forcing the complexity measures to be computable. Taking
     computation time into account also makes reasonable intuitive sense because
     we would not usually consider a very short algorithm that takes an enormous
     amount of time to run to be a particularly simple one. The fact that such an
     approach can be made to work is evidenced by the C-Test (see Section 1.7).
                                      93
     5. Limits of Computational Agents
     One of the reasons for studying mathematical models such as AIXI is to gain
     insights that may be useful for designing practical artiﬁcial intelligence agents.
     Thequestion then arises as to how useful all this incomputable theory really is.
     In this chapter we will explore this question by looking at the theoretical limi-
     tations faced by computable sequence predictors that attempt to approximate
     the power and generality of Solomonoﬀ induction. As sequence prediction lies
     at the heart of reinforcement learning, any limitations found here will carry
     over to general computable agents.
      AswesawinChapter2,sequenceprediction and compression are intimately
     related. Indeed, they are two diﬀerent ways of looking at the same problem.
     Intuitively, if you can accurately predict what is coming next you do not need
     to use much information to encode what the data actually is, and vice versa.
     For sequence predictors based on the Minimum Description Length (MDL)
     principle (Rissanen, 1996) or the Minimum Message Length (MML) principle
     (Wallace and Boulton, 1968), this connection is especially evident as they
     predict by attempting to ﬁnd the shortest possible description, or model, of the
     data. Not surprisingly then, they can be viewed as computable approximations
     of Solomonoﬀ induction (Section 5.5 of Li and Vit´anyi, 1997). Furthermore,
     in order to produce a working sequence predictor these methods can easily
     be combined with general purpose data compressors, such as the Lempel-Ziv
     algorithm(Federetal., 1992) or Context Tree Weighting (Willems et al., 1995).
     Unfortunately, while useful in practice, these real world compressors have their
     limitations: they are able to ﬁnd some kinds of computable regularities in
     sequences, but not others. As such, predictors based on them fall short of the
     power and generality of Solomonoﬀ induction. Furthermore, even with ideal
     compression MDL and MML based predictors can take exponentially longer
     than Solomonoﬀ induction to learn as they use only the shortest description of
     the data, rather than the set of all possible descriptions (Poland and Hutter,
     2004).
      Can we do better than this? Do universal and computable predictors for
     computable sequences exist? Unfortunately, it is easy to see that they do not:
     simply consider a sequence where the next bit is always the opposite of what
     the predictor predicts. This is essentially the same as what Dawid noted when
     he found that for any statistical forecasting system there exist sequences for
     which the predictor is not calibrated, and thus cannot be learnt (Dawid, 1985).
     However, hedoesnotdealwiththecomplexityofthesequencesthemselves, nor
     does he make a precise statement in terms of a speciﬁc measure of complexity.
     Theimpossibility of forecasting has since been developed in considerably more
                                      95
      5. Limits of Computational Agents
      depth by V’yugin (1998), in particular he proves that there is an eﬃcient
      randomised procedure producing sequences that cannot be predicted, with
      high probability, by computable forecasting systems.
       In this chapter we study the prediction of computable sequences from the
      perspective of Kolmogorov complexity. The central question we look at is the
      prediction of sequences which have bounded Kolmogorov complexity. This
      leads us to a new notion of complexity: Rather than the length of the short-
      est program able to generate a given sequence, in other words standard Kol-
      mogorov complexity, we take the length of the shortest program able to learn
      to predict the sequence. This new complexity measure has the same fun-
      damental invariance property as Kolmogorov complexity, and certain strong
      relationships between the two measures are proven. Nevertheless, in some
      cases the two can diverge signiﬁcantly. For example, although a long random
      string that indeﬁnitely repeats has a very high Kolmogorov complexity, this
      sequence also has a relatively simple structure that even a simple predictor
      can learn to predict.
       Wethen prove that some sequences can only be predicted by very complex
      predictors. This implies that very general prediction algorithms, in particular
      those that can learn to predict all sequences up to a given Kolmogorov com-
      plexity, must themselves be complex. This puts an end to our hope of there
      being an extremely general and yet relatively simple prediction algorithm. We
      then use this fact to prove that although very powerful prediction algorithms
      exist, they cannot be mathematically discovered due to G¨odel incompleteness.
      This signiﬁcantly constrains our ability to design and analyse powerful pre-
      diction algorithms, and indeed powerful artiﬁcial intelligence algorithms in
      general.
      5.1. Preliminaries
      For the basic notation for strings and sequences see Appendix A. In this
      chapter we will sometimes need to encode a natural number as a string. Using
      simple encoding techniques it can be shown that there exists a computable
      injective function f : N → B∗ where no string in the range of f is a preﬁx of
      any other, and ∀n ∈ N : ℓ(f(n)) ≤ log2n+2log2log2n+1 = O(logn).
       Of particular interest to us will be the class of sequences which can be
      generated by an algorithm executed on the following type of machine:
      5.1.1 Deﬁnition. A monotone universal Turing machine U is deﬁned
      as a universal Turing machine with one unidirectional input tape, one unidi-
      rectional output tape, and some bidirectional work tapes. Input tapes are read
      only, output tapes are write only, unidirectional tapes are those where the head
      can only move from left to right. All tapes are binary (no blank symbol) and
      the work tapes are initially ﬁlled with zeros. We say that U outputs/computes
      a sequence ω on input p, and write U(p) = ω, if U reads all of p but no more
      as it continues to write ω to the output tape.
      96
                                                            5.1. Preliminaries
           We ﬁx U and deﬁne U(p,x) by simply using a standard coding technique
         to encode a program p along with a string x ∈ B∗ as a single input string for
         U. For simplicity of notation we will often write p(x) to mean the function
         computed by the program p when executed on U along with the input string
         x, that is, p(x) is short hand for U(p,x).
         5.1.2 Deﬁnition. A sequence ω ∈ B∞ is a computable binary sequence
         if there exists a program q ∈ B∗ that writes ω to a one-way output tape when
         run on a monotone universal Turing machine U, that is, ∃q ∈ B∗ : U(q) = ω.
         Wedenote the set of all computable sequences by C.
           A similar deﬁnition for strings is not necessary as all strings have ﬁnite
         length and are therefore trivially computable: all an algorithm has to do is to
         copy the desired string to the output tape and then halt.
         5.1.3 Deﬁnition. A computable binary predictor is a program p ∈ B∗
         that on a universal Turing machine U computes a total function B∗ → B.
           Having x1:n as input, the objective of a predictor is for its output, called
         its prediction, to match the next symbol in the sequence. Formally we express
         this by writing p(x1:n) = xn+1.
           Note that this is diﬀerent to earlier chapters where a predictor generated
         a distribution over the possible symbols that might occur next, rather than
         outputting the predicted symbol. What we consider here is a special case of
         probabilistic prediction as it is equivalent to a probabilistic predictor that in
         each cycle always assigns probability 1 to some symbol.
           As the algorithmic prediction of incomputable sequences, such as the halt-
         ing sequence, is impossible by deﬁnition, we only consider the problem of
         predicting computable sequences. To simplify things we will assume that the
         predictor has an unlimited supply of computation time and storage. We will
         also make the assumption that the predictor has unlimited data to learn from,
         that is, we are only concerned with whether or not a predictor can learn to
         predict in the following sense:
         5.1.4 Deﬁnition.    We say that a predictor p can learn to predict a
         sequence ω := x1x2... ∈ B∞ if there exists m ∈ N such that ∀n ≥ m :
         p(x1:n) = xn+1.
           The existence of m in the above deﬁnition need not be constructive, that is,
         we might not know when the predictor will stop making prediction errors for
         a given sequence, just that this will occur eventually. This is essentially “next
         value” prediction as characterised by Barzdin (1972), which follows the notion
         of identiﬁability in the limit for languages from Gold (1967).
         5.1.5 Deﬁnition.   Let P(ω) be the set of all predictors able to learn to
         predict ω. Similarly for sets of sequences S ⊂ B∞, deﬁne P(S) := Tω∈S P(ω).
                                                                         97
             5. Limits of Computational Agents
               Astandard measure of complexity for sequences is the length of the shortest
             program which generates the sequence:
             5.1.6 Deﬁnition. For any sequence ω ∈ B∞ the Kolmogorov complexity
             of the sequence is,
                                    K(ω) := min{ℓ(q) : U(q) = ω},
                                               q∈B∗
             where U is a monotone universal Turing machine. If no such q exists, we deﬁne
             K(ω):=∞.
               In essentially the same way we can deﬁne the Kolmogorov complexity of a
             string x ∈ Bn, written K(x), by requiring that U(q) halts after generating x
             on the output tape.
               It can be shown that Kolmogorov complexity depends on our choice of uni-
             versal Turing machine U, but only up to an additive constant that is inde-
             pendent of ω. This is due to the fact that a universal Turing machine can
             simulate any other universal Turing machine with a ﬁxed length program. For
             more explanation see Section 2.4, or for an extensive treatment of Kolmogorov
             complexity and some of its applications see (Li and Vit´anyi, 1997) or (Calude,
             2002).
             5.2. Prediction of computable sequences
             Themostelementaryresultisthateverycomputablesequencecanbepredicted
             byatleast one predictor, and that this predictor need not be signiﬁcantly more
             complex than the sequence to be predicted.
                                                          +
             5.2.1 Lemma. ∀ω ∈C,∃p∈P(ω):K(p)<K(ω).
             Proof.      As the sequence ω is computable, there must exist at least one
             algorithm that generates ω. Let q be the shortest such algorithm and construct
             an algorithm p that “predicts” ω as follows: Firstly the algorithm p reads x1:n
             to ﬁnd the value of n, then it runs q to generate x1:n+1 and returns xn+1 as its
             prediction. Clearly p perfectly predicts ω and ℓ(p) < ℓ(q) + c, for some small
             constant c that is independent of ω and q.                                     2
               Not only can any computable sequence be predicted, there also exist very
             simple predictors able to predict arbitrarily complex sequences:
             5.2.2 Lemma. There exists a predictor p such that ∀n ∈ N,∃ω ∈ C : p ∈
             P(ω) and K(ω) > n.
             Proof.    Take a string x such that K(x) = ℓ(x) ≥ 2n, and from this deﬁne a
             sequence ω := x0000.... Clearly K(ω) > n and yet a simple predictor p that
             always predicts 0 can learn to predict ω.                                      2
             98
                                                 5.2. Prediction of computable sequences
             The predictor used in the above proof is very simple and can only “learn”
           sequences that end with all 0’s, albeit where the initial string can have ar-
           bitrarily high Kolmogorov complexity. It may seem that this is due to se-
           quences that are initially complex but where the “tail complexity”, deﬁned
           liminfi→∞K(ωi:∞), is zero. This is not the case:
           5.2.3 Lemma. There exists a predictor p such that ∀n ∈ N,∃ω ∈ C : p ∈
           P(ω) and liminfi→∞K(ωi:∞) > n.
           Proof.      A predictor p for eventually periodic sequences can be deﬁned
           as follows:  On input ω1:k the predictor goes through the ordered pairs
           (1,1),(1,2),(2,1),(1,3),(2,2),(3,1),(1,4),... checking for each pair (a,b)
           whether the string ω1:k consists of an initial string of length a followed by
           a repeating string of length b. On the ﬁrst match that is found p predicts that
           the repeating string continues, and then p halts. If a + b > k before a match
           is found, then p outputs a ﬁxed symbol and halts. Clearly K(p) is a small
           constant and p will learn to predict any sequence that is eventually periodic.
                                 2              ∗              m             n
             For any (m,n) ∈ N , let ω := x(y ) where x ∈ B , and y ∈ B is a random
           string, that is, K(y) = n. As ω is eventually periodic p ∈ P(ω) and also we see
           that liminfi→∞K(ωi:∞) = min{K(ωm+1:∞),K(ωm+2:∞),...,K(ωm+n:∞)}.
             For any k ∈ {1,...,n} let q∗ be the shortest program that can gener-
                                             k
           ate ωm+k:∞.     We can deﬁne a halting program q′ that outputs y where
                                                                 k
           this program consists of q∗, n and k.      Thus, ℓ(q′) = ℓ(q∗) + O(logn) =
                                       k                        k         k
           K(ωk:∞)+O(logn). Asn = K(y) ≤ ℓ(q′),weseethatK(ωk:∞) > n−O(logn).
                                                   k
           As n and k are arbitrary the result follows.                                  2
             Using a more sophisticated version of this proof it can be shown that there
           exist predictors that can learn to predict arbitrary regular or primitive re-
           cursive sequences. Thus we might wonder whether there exists a computable
           predictor able to learn to predict all computable sequences. Unfortunately, no
           universal predictor exists, indeed for every predictor there exists a sequence
           which it cannot predict at all:
           5.2.4 Lemma. For any predictor p there constructively exists a sequence
                                                                            +
           ω := x1x2... ∈ C such that ∀n ∈ N : p(x1:n) 6= xn+1 and K(ω) < K(p).
           Proof.    For any computable predictor p there constructively exists a com-
           putable sequence ω = x1x2x3... computed by an algorithm q deﬁned as fol-
           lows: Set x1 = 1−p(λ), then x2 = 1−p(x1), then x3 = 1−p(x1:2) and so on.
           Clearly ω ∈ C and ∀n ∈ N : p(x1:n) = 1−xn+1.
             Let p∗ be the shortest program that computes the same function as p and
           deﬁne a sequence generation algorithm q∗ based on p∗ using the procedure
           above. By construction, ℓ(q∗) = ℓ(p∗)+c for some constant c that is indepen-
           dent of p∗. Because q∗ generates ω, it follows that K(ω) ≤ ℓ(q∗). By deﬁnition
                      ∗                +
           K(p) = ℓ(p ) and so K(ω) <K(p).                                               2
                                                                                         99
              5. Limits of Computational Agents
                 Allowing the predictor to be probabilistic, as we did in previous chapters,
              doesnotfundamentallyavoidtheproblemofLemma5.2.4. Ineachstep,rather
              thangenerating the opposite to what will be predicted by p, instead q attempts
              to generate the symbol that p is least likely to predict given x1:n. To do this
              q must simulate p in order to estimate the probability that p(x1:n) = 1. With
              suﬃcient simulation eﬀort, q can estimate this probability to any desired ac-
              curacy for any x1:n. This produces a computable sequence ω such that ∀n ∈ N
              the probability that p(x       ) = x       is not signiﬁcantly greater than 1, that
                                          1:n      n+1                                        2
              is, the performance of p is no better than a predictor that makes completely
              random predictions. As probabilistic prediction complicates things without
              avoiding this problem, in this chapter we will consider only deterministic pre-
              dictors. This will also allow us to see the root of this problem as clearly as
              possible.
                 With the preliminaries covered, we now move on to the central problem:
              predicting sequences of limited Kolmogorov complexity.
              5.3. Prediction of simple computable sequences
              As the computable prediction of any computable sequence is impossible, a
              weaker goal is to be able to predict all “simple” computable sequences.
              5.3.1 Deﬁnition.        For n ∈ N, let Cn := {ω ∈ C : K(ω) ≤ n}. Further, let
              Pn := P(Cn) be the set of predictors able to learn to predict all sequences in
              Cn.
                 Firstly, we establish that prediction algorithms exist that can learn to predict
              all sequences up to a given complexity, and that these predictors need not be
              signiﬁcantly more complex than the sequences they can predict:
                                                             +
              5.3.2 Lemma. ∀n∈N,∃p∈Pn :K(p)<n+O(logn).
              Proof.      Let h ∈ N be the number of programs of length n or less which
              generate inﬁnite sequences. Build the value of h into a prediction algorithm p
              constructed as follows:
                 Inthekth predictioncycleruninparallelallprogramsoflengthnorlessuntil
              hof these programs have each produced k+1 symbols of output. Next predict
                                       th
              according to the k +1       symbol of the generated string whose ﬁrst k symbols
              is consistent with the observed string. If more than one generated string is
              consistent with the observed sequence, pick the one which was generated by
              the program that occurs ﬁrst in a lexicographical ordering of the programs. If
              no generated output is consistent, give up and output a ﬁxed symbol.
                 Forsuﬃcientlylargek, onlythehprogramswhichproduceinﬁnitesequences
              will produce output strings of length k + 1. As this set of sequences is ﬁnite,
              they can be uniquely identiﬁed by ﬁnite initial strings. Thus, for suﬃciently
              100
                                           5.3. Prediction of simple computable sequences
           large k, the predictor p will correctly predict any computable sequence ω for
           which K(ω) ≤ n, that is, p ∈ Pn.
                            n+1                                                 n+1
             As there are 2      −1possible strings of length n or less, h < 2       and thus
           we can encode h with log2h+2log2log2h = n+1+2log2(n+1) bits. Thus,
           K(p) < n+1+2log2(n+1)+c for some constant c that is independent of
           n.                                                                              2
             Can we do better than this? Lemmas 5.2.2 and 5.2.3 show us that there
           exist predictors able to predict at least some sequences vastly more complex
           than themselves. This suggests that there might exist simple predictors able
           to predict arbitrary sequences up to a high complexity. Formally, could there
           exist p ∈ Pn where n ≫ K(p)? Unfortunately, these simple but powerful
           predictors are not possible:
                                                          +
           5.3.3 Theorem. ∀n ∈ N : p ∈ Pn ⇒ K(p) >n.
           Proof.      For any n ∈ N let′p ∈ Pn, that′is, ∀ω ∈ Cn :′p ∈ P(ω). By
           Lemma5.2.4 we know that ∃ω ∈ C : p ∈/ P(ω ) . As p ∈/ P(ω ) it must be the
           case that ω′ ∈/ Cn, that is, K(ω′) ≥ n. From Lemma 5.2.4 we also know that
                 +      ′
           K(p) >K(ω ) and so the result follows.                                          2
             Intuitively the reason for this is as follows: Lemma 5.2.4 guarantees that
           every simple predictor fails for at least one simple sequence. Thus, if we want
           a predictor that can learn to predict all sequences up to a moderate level of
           complexity, then clearly the predictor cannot be simple. Likewise, if we want a
           predictor that can predict all sequences up to a high level of complexity, then
           the predictor itself must be very complex. Even though we have made the
           generous assumption of unlimited computational resources and data to learn
           from, only very complex algorithms can be truly powerful predictors.
             These results easily generalise to notions of complexity that take computa-
           tion time into consideration. As sequences are inﬁnite, the appropriate mea-
           sure of time is the time needed to generate or predict the next symbol in the
           sequence. Under any reasonable measure of time complexity, the operation of
           inverting a single output from a binary valued function can be performed with
           little cost. If C is any complexity measure with this property, it is trivial to
           see that the proof of Lemma 5.2.4 still holds for C. From this, an analogue of
           Theorem 5.3.3 for C easily follows.
             With similar arguments these results also generalise, in a straightforward
           way, to complexity measures that take space or other computational resources
           into account. Thus, the fact that extremely powerful predictors must be very
           complex, holds under any measure of complexity for which inverting a single
           bit is inexpensive.
                                                                                          101
             5. Limits of Computational Agents
             5.4. Complexity of prediction
             Another way of viewing these results is in terms of an alternate notion of
             sequence complexity deﬁned as the size of the smallest predictor able to learn
             to predict the sequence. This allows us to express the results of the previous
             sections more concisely. Formally, for any sequence ω deﬁne the complexity
             measure,
                                     ˙
                                    K(ω) := min{ℓ(p) : p ∈ P(ω)},
                                              p∈B∗
                   ˙                                    ˙
             and K(ω) := ∞ if P(ω) = ∅. Thus, if K(ω) is high then the sequence ω is
             complex in the sense that only complex prediction algorithms are able to learn
             to predict it. It can easily be seen that this notion of complexity has the same
             invariance to the choice of reference universal Turing machine as the standard
             Kolmogorov complexity measure.
               It may be tempting to conjecture that this deﬁnition simply describes what
                                                                             ˙
             might be called the “tail complexity” of a sequence, that is, K(ω) is equal to
             liminfi→∞K(ωi:∞). This is not the case. In the proof of Lemma 5.2.3 we saw
             that there exists a single predictor capable of learning to predict any sequence
                                                                               ˙
             that consists of a repeating string, and thus for these sequences K is bounded.
             It was further shown that there exist sequences of this form with arbitrarily
                                                                        ˙
             high tail complexity. Clearly then tail complexity and K cannot be equal in
             general.
                       ˙
               Using K we can now rewrite a number of our previous results much more
             succinctly. From Lemma 5.2.1 it immediately follows that,
                                                   ˙     +
                                        ∀ω : 0 ≤ K(ω) < K(ω).
                                                                                    ˙
             From Lemma 5.2.2 we know that ∃c ∈ N,∀n ∈ N,∃ω ∈ C such that K(ω) < c
                                       ˙
             and K(ω) > n, that is, K can attain the lower bound above within a small
             constant, no matter how large the value of K is. The sequences for which the
                               ˙
             upper bound on K is tight are interesting as they are the ones which demand
             complex predictors. We prove the existence of these sequences and look at
             some of their properties in the next section.
                                          ˙
               The complexity measure K can also be generalised to sets of sequences, for
                    ∞          ˙
             S ⊂ B     deﬁne K(S) := minp{ℓ(p) : p ∈ P(S)}. This allows us to rewrite
             Lemma 5.3.2 and Theorem 5.3.3 as simply,
                                              + ˙        +
                                 ∀n∈N : n < K(Cn) < n+O(logn).
             This is just a restatement of the fact that the simplest predictor capable of
             predicting all sequences up to a Kolmogorov complexity of n, has itself a
             Kolmogorov complexity of roughly n.
                                                          ˙
               Perhaps the most surprising thing about K complexity is that this very nat-
             ural deﬁnition of the complexity of a sequence, as viewed from the perspective
             of prediction, does not appear to have been studied before.
             102
                                                            5.5. Hard to predict sequences
           5.5. Hard to predict sequences
           We have already seen that some individual sequences, such as the repeating
           string used in the proof of Lemma 5.2.3, can have arbitrarily high Kolmogorov
           complexity but nevertheless can be predicted by trivial algorithms. Thus,
           although these sequences contain a lot of information in the Kolmogorov sense,
           in a deeper sense their structure is very simple and easily learnt.
             What interests us in this section is the other extreme: individual sequences
           that can only be predicted by complex predictors. As we are concerned with
           prediction in the limit, this extra complexity in the predictor must be some
           kind of special information which cannot be learnt through observing the se-
           quence. Our ﬁrst task is to show that these hard to predict sequences exist.
                                                    + ˙       +        +
           5.5.1 Theorem. ∀n ∈ N,∃ω ∈ C : n < K(ω) < K(ω) < n+O(logn).
           Proof. For any n ∈ N, let Q ⊂ B<n be the set of programs shorter than n
                                           n
           that are predictors, and let x1:k ∈ Bk be the observed initial string from the
           sequence ω that is to be predicted. Now construct a meta-predictor pˆ:
             By dovetailing the computations, run in parallel every program of length
           less than n on every string in B≤k. Each time a program is found to halt on
           all of these input strings, add the program to a set of “candidate prediction
                                ˜k
           algorithms”, called Qn. As each element of Qn is a valid predictor, and thus
           halts for all input strings in B∗ by deﬁnition, for every n and k it eventually
                                  ˜k
           will be the case that |Qn| = |Qn|. At this point the simulation to approximate
           Qn terminates. It is clear that for suﬃciently large values of k all of the valid
           predictors, and only the valid predictors, will halt with a single symbol of
                                                                          ˜k
           output on all tested input strings. That is, ∃r ∈ N,∀k > r : Qn = Qn.
             The second part of the pˆ algorithm uses these candidate prediction algo-
                                                  ˜k          k       Pk−1
           rithms to make a prediction. For p ∈ Qn deﬁne d (p) :=       i=1 |p(x1:i)−xi+1|.
           Informally, dk(p) is the number of prediction errors made by p so far. Com-
                                 ˜k                 ∗    ˜k
           pute this for all p ∈ Qn and then let pk ∈ Qn be the program with minimal
           dk(p). If there is more than one such program, break the tie by letting p∗ be
                                                                                        k
           the lexicographically ﬁrst of these. Finally, pˆ computes the value of p∗(x1:k)
                                                                                     k
           and then returns this as its prediction and halts.
             By Lemma 5.2.4, there exists ω′ ∈ C such that pˆ makes a prediction error
           for every k when trying to predict ω′. Thus, in each cycle at least one of
           the ﬁnitely many predictors with minimal dk makes a prediction error and
           so ∀p ∈ Qn : dk(p) → ∞ as k → ∞. Therefore, ∄p ∈ Qn : p ∈ P(ω′),
           that is, no program of length less than n can learn to predict ω′ and so n ≤
            ˙   ′                                                ˙   ′  +      ′
           K(ω). Further, from Lemma 5.2.1 we know that K(ω ) < K(ω ), and from
                                     ′ +
           Lemma 5.2.4 again, K(ω ) <K(pˆ).
             Examining the algorithm for pˆ, we see that it contains some ﬁxed length
                                                                      n
           program code and an encoding of |Qn|, where |Qn| < 2 − 1. Thus, using a
                                                           +
           standard encoding method for integers, K(pˆ) < n+O(logn). Chaining these,
             + ˙    ′  +      ′ +       +
           n<K(ω)<K(ω)<K(pˆ)<n+O(logn),which proves the theorem.                         2
                                                                                        103
              5. Limits of Computational Agents
                                                                                              ˙
                 This establishes the existence of sequences with arbitrarily high K com-
              plexity which also have a similar level of Kolmogorov complexity. Next we
                                                                ˙
              establish a fundamental property of high K complexity sequences: they are
              extremely diﬃcult to compute.
                 For an algorithm q that generates ω ∈ C, deﬁne tq(n) to be the number of
                                                                   th
              computation steps performed by q before the n           symbol of ω is written to the
              output tape. For example, if q is a simple algorithm that outputs the sequence
              010101..., then clearly tq(n) = O(n) and so ω can be computed quickly. The
              following theorem proves that if a sequence can be computed in a reasonable
                                                                           ˙
              amount of time, then the sequence must have a low K complexity:
                                                                                                    n
              5.5.2 Lemma. ∀ω ∈ C, if ∃q : U(q) = ω and ∃r ∈ N,∀n > r : tq(n) < 2 ,
                     ˙     +
              then K(ω) = 0.
              Proof. Construct a prediction algorithm p˜ as follows:
                                                                                       n+1
                 On input x1:n, run all programs of length n or less, each for 2            steps. In
              a set Wn collect together all generated strings which are at least n+1 symbols
              long and where the ﬁrst n symbols match the observed string x1:n. Now order
              the strings in W     according to a lexicographical ordering of their generating
                                 n
              programs. If Wn = ∅, then just return a prediction of 1 and halt. If |Wn| > 1
                                      th
              then return the n+1        symbol from the ﬁrst sequence in the above ordering.
                                                                                            n
                 Assume that ∃q : U(q) = ω such that ∃r ∈ N,∀n > r : tq(n) < 2 . If q is
              not unique, take q to be the lexicographically ﬁrst of these. Clearly ∀n > r
              the initial string from ω generated by q will be in the set Wn. As there
              is no lexicographically lower program which can generate ω within the time
                                     n
              constraint tq(n) < 2 for all n > r, for suﬃciently large n the predictor p˜must
              converge on using q for each prediction and thus p˜ ∈ P(ω). As ℓ(p˜) is clearly
                                                                                      ˙              +
              a ﬁxed constant that is independent of ω, it follows then that K(ω) < ℓ(p˜) =
              0.                                                                                   2
                                           n
                 Wecould replace the 2 bound in the above result with any monotonically
                                                                   2n
              growing computable function, for example, 2            .  In any case, this does not
                                                                                        ˙
              change the fundamental result that sequences which have a high K complexity
              are practically impossible to compute. However, from our theoretical perspec-
              tive, these sequences present no problem as they can be predicted, albeit with
              immense diﬃculty.
              5.6. The limits of mathematical analysis
              One way to interpret the results of the previous sections is in terms of con-
              structive theories of prediction. Essentially, a constructive theory of prediction
              expressed in some suﬃciently rich formal system F, is in eﬀect a description
              of a prediction algorithm with respect to a universal Turing machine which
              implements the required parts of F. Thus, from Theorems 5.3.3 and 5.5.1,
              it follows that if we want to have a predictor that can learn to predict all
              104
                                                 5.6. The limits of mathematical analysis
           sequences up to a high level of Kolmogorov complexity, or even just predict
                                                  ˙
           individual sequences which have high K complexity, the constructive theory of
           prediction that we base our predictor on must be very complex. Elegant and
           highly general constructive theories of prediction simply do not exist, even if
           we assume unlimited computational resources. This is in marked contrast to
           Solomonoﬀ’s highly elegant but non-constructive theory of prediction.
             Naturally, highly complex theories of prediction will be very diﬃcult to
           mathematically analyse, if not practically impossible. Thus, at some point
           the development of very general prediction algorithms must become mainly
           an experimental endeavour due to the diﬃculty of working with the required
           theory.  Interestingly, an even stronger result can be proven showing that
           beyond some point the mathematical analysis is in fact impossible, even in
           theory:
           5.6.1 Theorem. In any consistent formal axiomatic system F that is suﬃ-
           ciently rich to express statements of the form “p ∈ Pn”, there exists m ∈ N
           such that for all n > m and for all predictors p ∈ Pn the true statement
           “p ∈ Pn” cannot be proven in F.
             In other words, even though we have proven that very powerful sequence
           prediction algorithms exist, beyond a certain complexity it is impossible to
           ﬁnd any of these algorithms using mathematics. The proof has a similar
           structure to Chaitin’s information theoretic proof (Chaitin, 1982) of G¨odel’s
           incompleteness theorem for formal axiomatic systems (G¨odel, 1931).
           Proof.     For each n ∈ N let Tn be the set of statements expressed in the
           formal system F of the form “p ∈ Pn”, where p is ﬁlled in with the complete
           description of some algorithm in each case. As the set of programs is denumer-
           able, Tn is also denumerable and each element of Tn has ﬁnite length. From
           Lemma 5.3.2 and Theorem 5.3.3 it follows that each Tn contains inﬁnitely
           many statements of the form “p ∈ Pn” which are true.
             Fix n and create a search algorithm s that enumerates all proofs in the
           formal system F searching for a proof of a statement in the set Tn. As the set
           Tn is recursive, s can always recognise a proof of a statement in Tn. If s ﬁnds
           any such proof, it outputs the corresponding program p and then halts.
             By way of contradiction, assume that s halts, that is, a proof of a theorem
           in Tn is found and p such that p ∈ Pn is generated as output. The size of
           the algorithm s is a constant (a description of the formal system F and some
           proof enumeration code) as well as an O(logn) term needed to describe n. It
                                    +
           follows then that K(p) < O(logn). However, from Theorem 5.3.3 we know
                      +
           that K(p) > n. Thus, for suﬃciently large n, we have a contradiction and
           so our assumption of the existence of a proof must be false. That is, for
           suﬃciently large n and for all p ∈ Pn, the true statement “p ∈ Pn” cannot be
           proven within the formal system F.                                           2
                                                                                       105
      5. Limits of Computational Agents
       The exact value of m depends on our choice of formal system F and which
      reference machine U we measure complexity with respect to. However, for
      reasonable choices of F and U the value of m would be in the order of 1000.
      That is, the bound m is certainly not so large as to be vacuous.
      5.7. Conclusion
      We have shown that there does not exist an elegant constructive theory of
      prediction for computable sequences, even if we assume unbounded compu-
      tational resources, unbounded data and learning time, and place moderate
      bounds on the Kolmogorov complexity of the sequences to be predicted. Very
      powerful computable predictors are therefore necessarily complex. We have
      further shown that the source of this problem is the existence of computable
      sequences which are extremely expensive to compute. While we have proven
      that very powerful prediction algorithms which can learn to predict these se-
      quences exist, we have also proven that, unfortunately, mathematical analysis
      cannot be used to discover these algorithms due to G¨odel incompleteness.
       These results can be extended to more general settings, speciﬁcally to those
      problemswhichareequivalentto, ordependon,sequenceprediction. Consider,
      for example, a reinforcement learning agent interacting with an environment,
      as described in Chapters 2 and 3. In each interaction cycle the agent must
      choose its actions so as to maximise the future rewards that it receives from
      the environment. Of course the agent cannot know for certain if some action
      will lead to rewards in the future. Whether explicitly or implicitly, it must
      somehow predict these. Thus, at the heart of reinforcement learning lies a
      prediction problem, and so the results for computable predictors presented in
      this paper also apply to computable reinforcement learners. More speciﬁcally,
      from Theorem 5.3.3 it follows that very powerful computable reinforcement
      learners are necessarily complex, and from Theorem 5.6.1 it follows that it is
      impossible to discover any of these extremely powerful reinforcement learning
      algorithms mathematically. These relationships are illustrated in Figure 5.1.
       It is reasonable to ask whether the assumptions we have made in our model
      need to be changed. If we increase the power of the predictors further, for
      example by providing them with some kind of an oracle, this would make the
      predictors even more unrealistic than they currently are. This goes against
      our goal of ﬁnding an elegant, powerful and general prediction theory that is
      more realistic in its assumptions than Solomonoﬀ’s incomputable model. On
      the other hand, if we weaken our assumptions about the predictors’ resources
      to make them more realistic, we are in eﬀect taking a subset of our current
      class of predictors. As such, all the same limitations and problems will still
      apply, as well as some new ones.
       It seems then that the way forward is to further restrict the problem space.
      One possibility would be to bound the amount of computation time needed to
      generate the next symbol in the sequence. However, if we do this without re-
      106
                                5.7. Conclusion
     stricting the predictors’ resources then the simple predictor from Lemma 5.5.2
     easily learns to predict any such sequence and thus the problem of prediction
     in the limit has become trivial. Another possibility might be to bound the
     memoryofthemachineusedtogenerate the sequence, however this makes the
     generator a ﬁnite state machine and thus bounds its computation time, again
     making the problem trivial.
      Perhaps the only reasonable solution would be to add additional restrictions
     to both the algorithms which generate the sequences to be predicted, and to
     the predictors. We may also want to consider not just learnability in the limit,
     but also how quickly the predictor is able to learn. Of course we are then
     facing a much more diﬃcult analysis problem.
                                      107
      5. Limits of Computational Agents
      Figure 5.1.: Theorem 5.3.3 rules out simple but powerful artiﬁcial intelligence
           algorithms, as indicated by the greyed out region in the upper
           left. Theorem 5.6.1 upper bounds how powerful an algorithm can
           be before it can no longer be proven to be a powerful algorithm.
           This is indicated by the horizontal line separating the region of
           provable algorithms from the region of G¨odel incompleteness.
      108
     6. Temporal Diﬀerence Updating
         without a Learning Rate
     In Chapters 2 and 3 we saw how universal agents are able to learn to behave
     optimally across a wide range of environments. Unfortunately, these agents
     are incomputable as they are based on incomputable universal prior distribu-
     tions. Thus, in order to use the theory of universal artiﬁcial intelligence to
     design and build practical algorithms, we must ﬁrst ﬁnd a way to scale the
     theory down. In Chapter 5 we investigated some of the constraints faced when
     attempting this. What we uncovered was a number of fundamental negative
     results. In short, computable predictors capable of predicting all sequences up
     to a moderate Kolmogorov complexity are both highly complex and mathe-
     matically impossible to ﬁnd due to G¨odel incompleteness. The only way out
     of this bind, it seems, is to move to a more sophisticated measure of complex-
     ity that takes not only information content into account, but also time and
     space. Unfortunately, the theory of resource bounded complexity is notori-
     ously diﬃcult to work with and has many unsolved fundamental questions.
     Furthermore, even if the universal prior distribution could be replaced by a
     suitable computable prior, perhaps something like the Speed prior (Schmid-
     huber, 2002), there still remains the fact that AIXI’s search through possible
     futures requires computation time that is exponential in the depth of the look
     ahead (see the equation for AIXI in Section 2.10).
       Despite these diﬃculties, several attempts at scaling AIXI down have been
     made. The most theoretically founded of these is AIXItl (Chapter 7 of Hutter,
     2005). In this model, proof search is used to limit the size and computation
     time of the algorithm. Unfortunately, although technically computable, the
     resulting agent still requires impossibly vast computational resources. Another
     more drastic scaling down of AIXI did produce a usable algorithm (Poland and
     Hutter, 2006). Here the problem domain was limited to games that could be
     described by 2×2 matrices, and the look ahead was bounded to 8 interaction
     cycles. The resulting algorithm was able to learn simple game theoretic inter-
     action strategies. While this proves that some kind of scaling down of AIXI is
     possible, the problem space of 2 × 2 matrix games falls well short of what is
     needed for a useful artiﬁcial intelligence algorithm.
       In this chapter we present what started out as another attempt to scale
     AIXI down. As we have seen in previous chapters, at the core of the rein-
     forcement learning problem lies the problem of estimating the expected future
     discounted reward. We begin by expressing this estimation problem as a loss
     function, more speciﬁcally, as the squared diﬀerence between the empirical
                                          109
            TDUPDATINGWITHOUTALEARNINGRATE
            future discounted reward and our estimate. We then derive an equation for
            this estimator by minimising the loss function. Although the resulting learning
            equations no longer bear much resemblance to AIXI, they do strongly resemble
            the standard equation for temporal diﬀerence learning with eligibility traces,
            also known as the TD(λ) algorithm. Interestingly, while the standard algo-
            rithm has a free learning rate parameter, in our new equation there is none. In
            its place there is an equation that automatically sets the learning rate in a way
            that is speciﬁc to each state transition. We have experimentally tested this new
            learning rule against TD(λ) and found that it oﬀers superior performance in
            various settings. We have also extended the algorithm to reinforcement learn-
            ing and again found encouraging results. This chapter covers the derivation
            of this algorithm and our experimental results.
              Note that while the notation used in this chapter is fairly standard for the
            temporal diﬀerence learning literature, it is a little diﬀerent to what we used
            to deﬁne AIXI. For example, we now talk of states rather than observations,
            and index the value function in a new way.
            6.1. Temporal diﬀerence learning
            In the ﬁeld of reinforcement learning, perhaps the most popular way to esti-
            mate the future discounted reward of states is the method of temporal diﬀer-
            ence learning. It is unclear who exactly introduced this ﬁrst, however the ﬁrst
            explicit version of temporal diﬀerence as a learning rule appears to be Witten
            (1977). The idea is as follows: The expected future discounted reward of a state
            s is,
                           V :=Er +γr          +γ2r     +···|s =s	,
                             s        k     k+1      k+2       k
            where the rewards r ,r   , . . . are geometrically discounted into the future by
                               k  k+1
            γ < 1. From this deﬁnition it follows that,
                                  V =Er +γV           |s =s	.                    (6.1)
                                    s       k      sk+1 k
              Our task, at time t, is to compute an estimate Vt of V  for each state s.
                                                              s      s
            The only information we have to base this estimate on is the current history
            of state transitions, s1,s2,...,st, and the current history of observed rewards,
            r ,r ,...,r . Equation (6.1) suggests that at time t+1 the value of r +γV
             1  2      t                                                     t     st+1
            provides us with information on what Vt should be: if it is higher than Vt
                                                   s                                st
            then perhaps this estimate should be increased, and vice versa. This intuition
            gives us the following estimation heuristic for state st,
                               Vt+1 := Vt +αr +γVt        −Vt,
                                 st      st      t     st+1    st
            whereαisaparameterthatcontrolstherateoflearning. Thistypeoftemporal
            diﬀerence learning is known as TD(0).
              One shortcoming of this method is that at each time step the value of only
            the last state st is updated. States before the last state are also aﬀected by
            110
                                             6.1. Temporal diﬀerence learning
         Algorithm 1 TD(λ)
           Initialise V (s) arbitrarily and E(s) = 0 for all s
           Initialise s
           repeat
            Make state transition and observe r, s′
            ∆←r+γV(s′)−V(s)
            E(s) ← E(s)+1
            for all s do
              V(s) ←V(s)+αE(s)∆
              E(s) ← γλE(s)
            end for
            s ←s′
           until end of run
         changes in the last state’s value and thus these could be updated too. This is
         what happens with so called temporal diﬀerence learning with eligibility traces,
         where a history, or trace, is kept of which states have been recently visited.
         Under this method, when we update the value of a state we also go back
         through the trace updating the earlier states as well. Formally, for any state
         s its eligibility trace is computed by,
                            t    γλEt−1      if s 6= st,
                           E :=       s
                            s     γλEt−1 +1   if s = st,
                                      s
         where λ is used to control the rate at which the eligibility trace is discounted.
         The temporal diﬀerence update is then, for all states s,
                         Vt+1 := Vt +αEtr+γVt    −Vt.              (6.2)
                          s      s     s       st+1  st
         This more powerful version of temporal diﬀerent learning is known as TD(λ)
         (Sutton, 1988). The complete algorithm appears in Algorithm 1.
           Although it has been successfully applied to many simple problems, plain
         TD(λ) has a number of drawbacks. One of these is that the learning rate
         parameter α has to be experimentally tuned by hand. Indeed, even this is
         not always enough, for optimal performance some monotonically decreasing
         function has to be experimentally found that decreases the learning rate over
         time at the right rate. If the learning rate decreases too quickly, the system
         may become stuck at a level of sub-optimal performance, if it decreases too
         slowly then convergence will be unnecessarily late. The main contribution of
         this chapter is to solve this problem by deriving a temporal diﬀerence rule
         from statistical principles that automatically sets its learning rate.
           Perhaps the closest work to ours is the LSTD(λ) algorithm (Bradtke and
         Barto, 1996; Boyan, 1999; Lagoudakis and Parr, 2003). LSTD(λ) is concerned
         with ﬁnding a least-squares linear function approximation to the true value
                                                                      111
          TDUPDATINGWITHOUTALEARNINGRATE
          function. The unknown expected rewards and transition probabilities are re-
          placed by empirical averages up to current time t. In contrast, we consider
          ﬁnite state spaces and no function approximation. We derive a least-squares
          estimate of the empirical values including future rewards by bootstrapping.
          The computation time for our update is linear in the number of states, like
          TD(λ), while LSTD is quadratic (even in the case of state-indicator features
          and no function approximation). Indeed our algorithm exactly coincides with
          TD/Q/Sarsa(λ) but with a novel learning rate derived from statistical princi-
          ples. LSTDhasnotyetbeendevelopedforgeneralλandγ. Ouralgorithmand
          LSTDbothget rid of the learning rate and the necessity to initialise V . Since
          LSTDhasprimarily been developed for linear function approximation and has
          a much more expensive update rule, we focused our experimental comparison
          to the algorithms for which we determined the learning rate (ﬁnite state space,
          linear time TD/Q/Sarsa algorithms). It remains to be seen how our approach
          generalises to (linear) function approximation.
          6.2. Derivation
          The empirical future discounted reward of a state sk is the sum of actual
          rewards following from state sk in time steps k,k + 1,..., where the rewards
          are discounted as they go into the future. Formally, the empirical value of
          state sk at time k for k = 1,...,t is,
                                         ∞
                                   v := Xγu−kr ,                       (6.3)
                                    k            u
                                        u=k
          where the future rewards r are geometrically discounted by γ < 1. In practice
                                u
          the exact value of vk is always unknown to us as it depends not only on rewards
          that have been already observed, but also on unknown future rewards. Note
          that if sm = sn for m 6= n, that is, we have visited the same state twice at
          diﬀerent times m and n, this does not imply that vn = vm as the observed
          rewards following the state visit may be diﬀerent each time.
            OurgoalisthatforeachstatestheestimateVt shouldbeascloseaspossible
                                                  s
          to the true expected future discounted reward Vs. Thus, for each state s we
          would like Vs to be close to vk for all k such that s = sk. Furthermore, in
          non-stationary environments we would like to discount old evidence by some
          parameter λ ∈ (0,1]. Formally, we want to minimise the loss function,
                                      t
                                    1 X t−k        t 2
                               L:= 2    λ    vk −Vsk  .                (6.4)
                                     k=1
          For stationary environments we may simply set λ = 1 a priori.
          112
                                                                                    6.2. Derivation
               As we wish to minimise this loss, we take the partial derivative with respect
            to the value estimate of each state and set to zero,
                            t                                t                t
                ∂L        X t−k              t           t X t−k            X t−k
               ∂Vt =−         λ      vk −Vsk δsks = Vs          λ    δsks −      λ    δsksvk = 0,
                  s       k=1                              k=1               k=1
            where we could change Vt into Vt due to the presence of the Kronecker δ                  ,
                                         sk        s                                              sks
            deﬁned δxy := 1 if x = y, and 0 otherwise. By deﬁning a discounted state visit
            counter Nt := Pt        λt−kδs s we get
                        s       k=1         k
                                                       t
                                           VtNt =Xλt−kδs svk.                                   (6.5)
                                             s  s                k
                                                     k=1
            Since v    depends on future rewards r , Equation (6.5) can not be used in
                    k                                    k
            its current form. Next we note that vk has a self-consistency property with
            respect to the rewards. Speciﬁcally, the tail of the future discounted reward
            sum for each state depends on the empirical value at time t in the following
            way,
                                                t−1
                                          v =Xγu−kr +γt−kv .
                                           k               u          t
                                                u=k
            Substituting this into Equation (6.5) and exchanging the order of the double
            sum,
                                     t−1 u                           t
                       VtNt = XXλt−kδ γu−kr +Xλt−kδ γt−kv
                         s   s                      sks        u              sks       t
                                     u=1k=1                        k=1
                                     t−1         u                      t
                                = Xλt−uX(λγ)u−kδ r +X(λγ)t−kδ v
                                                              sks u                  sks t
                                     u=1       k=1                     k=1
                                = Rt+Etv,
                                       s     s t
            where Et := Pt         (λγ)t−kδs s is the eligibility trace of state s, and Rt :=
                      s        k=1            k                                                  s
            Pt−1 λt−uEur is the discounted reward with eligibility.
               u=1         s u
               Et and Rt depend only on quantities known at time t. The only unknown
                 s        s
            quantity is vt, which we have to replace with our current estimate of this value
            at time t, which is V t . In other words, we bootstrap our estimates. This gives
                                   st
            us,
                                             VtNt =Rt +EtVt.                                    (6.6)
                                              s   s      s     s st
            For state s = st, this simpliﬁes to
                                                          Rt
                                               Vt =         st    .
                                                st    Nt −Et
                                                        st      st
                                                                                                  113
              TDUPDATINGWITHOUTALEARNINGRATE
              Substituting this back into Equation (6.6) we obtain,
                                                                   Rt
                                           VtNt =Rt +Et              st    .                       (6.7)
                                             s  s      s     sNt −Et
                                                                 st     st
              This gives us an explicit expression for our V estimates. However, from an
              algorithmic perspective an incremental update rule is more convenient. To
              derive this we make use of the relations,
                                    Nt+1 =      λNt+δ          ,        N0 =0,
                                      s             s     st+1s            s
                                     Et+1 = λγEt+δ              ,       E0 =0,
                                       s             s    st+1s           s
                                     Rt+1 =     λRt +λEtr ,             R0 =0,
                                       s            s       s t           s
              Inserting these into Equation (6.7) with t replaced by t + 1,
                                                                   Rt+1
                            Vt+1Nt+1 = Rt+1+Et+1                     st+1
                              s     s             s       s   Nt+1 −Et+1
                                                                 st+1     st+1
                                                                        Rt    +Et r
                                           = λRt+λEtr +Et+1 st+1                   st+1 t.
                                                   s       s t     s    Nt    −γEt
                                                                          st+1      st+1
              By solving Equation (6.6) for Rt and substituting back in,
                                                   s
                                                                    Nt Vt −Et Vt+Et r
                t+1   t+1        t t        t  t       t       t+1   st+1 st+1      st+1 st     st+1 t
              V     N      =λ V N −E V            +λE r +E
                s     s           s   s     s st        s t     s            Nt     −γEt
                                                                               st+1       st+1
                          =  λNt+δ           Vt−δ         Vt−λEtVt +λEtr
                                   s    st+1s   s     st+1s s       s st        s t
                                                    Nt Vt −Et Vt+Et r
                                           +Et+1 st+1 st+1           st+1 st      st+1 t.
                                                s            Nt     −γEt
                                                                st+1       st+1
              Dividing through by Nt+1(= λNt+δ                 ),
                                         s          s     st+1s
                                       −δ       Vt−λEtVt +λEtr
                       Vt+1 = Vt +         st+1s s       s st        s t
                         s        s              λNt+δ
                                                    s    st+1s
                                       (λγEt +δ         )(Nt     Vt    −Et Vt+Et r)
                                    +        s     st+1s    st+1 st+1      st+1 st      st+1 t .
                                                 (Nt     −γEt )(λNt+δs s)
                                                     st+1       st+1      s     t+1
                 Making the ﬁrst denominator the same as the second, then expanding the
              numerator,
                             λEtr Nt       −λEtVtNt          −δ       VtNt      −λγEt Etr
              Vt+1 = Vt+         s t  st+1       s st   st+1     st+1s s   st+1         st+1  s t
                s        s                   (Nt     −γEt )(λNt+δ               )
                                                st+1       st+1      s     st+1s
                      λγEt     EtVt +γEt         Vtδ       +λγEtNt Vt −λγEtEt Vt
                   +       st+1  s st       st+1 s st+1s          s   st+1 st+1         s  st+1 st
                                          (Nt     −γEt )(λNt+δs s)
                                              st+1       st+1      s     t+1
              114
                                                       6.3. Estimating a small Markov process
                    λγEtEt       r +δ        Nt Vt −δ              Et    Vt +δ        Et    r
                  +      s   st+1 t    st+1s   st+1 st+1     st+1s   st+1 st     st+1s  st+1 t.
                                        (Nt     −γEt )(λNt+δs s)
                                           st+1       st+1      s     t+1
            After cancelling equal terms (keeping in mind that in every term with a Kro-
            necker δxy factor we may assume that x = y as the term is always zero other-
            wise), and factoring out Et the right hand side becomes,
                                          s
                     t       t         t   t         t              t    t              t           
                   E λr N        −λV N        +γV δ         +λγN       V     −δ       V +δ        r
            Vt+ s         t  st+1     st  st+1      s st+1s        st+1 st+1    st+1s st     st+1s t
              s                           (Nt     −γEt )(λNt+δ               )
                                             st+1       st+1      s     st+1s
            Finally, by factoring out λNt         +δ        we obtain our update rule,
                                             st+1    st+1s
                               t+1       t       t                       t       t 
                             V      = V + E β (s,s           ) r +γV        −V ,                (6.8)
                               s         s      s   t    t+1     t      st+1     st
            where the learning rate is given by,
                                                            1          Nt
                                   β (s,s     ) :=                       st+1 .                 (6.9)
                                    t     t+1       Nt     −γEt          Nt
                                                       st+1      st+1      s
               Examining Equation (6.8), we ﬁnd the usual update equation for temporal
            diﬀerence learning with eligibility traces (see Equation (6.2)), however the
            learning rate α has now been replaced by βt(s,st+1). This learning rate was
            derived by minimising the squared loss between the estimated and true state
            value. In the derivation we have exploited the fact that the latter must be
            self-consistent and then bootstrapped to get Equation (6.6). This gives us an
            equation for the learning rate for each state transition at time t, as opposed to
            the standard temporal diﬀerence learning where the learning rate α is either
            a ﬁxed free parameter for all transitions, or is decreased over time by some
            monotonically decreasing function. In either case, the learning rate is not
            automaticandmustbeexperimentallytunedforgoodperformance. Theabove
            derivation appears to theoretically solve this problem.
               The ﬁrst term in βt seems to provide some type of normalisation to the
            learning rate, though the intuition behind this is not clear to us. The meaning
            of second term however can be understood as follows: Nt measures how often
                                                                             s
            we have visited state s in the recent past. Therefore, if Nt ≪ Nt                    then
                                                                                   s       st+1
            state s has a value estimate based on relatively few samples, while state st+1
            has a value estimate based on relatively many samples. In such a situation,
            the second term in β boosts the learning rate so that Vt+1 moves more ag-
                                    t                                         s
            gressively towards the presumably more accurate r +γVt                . In the opposite
                                                                       t      st+1
            situation when st+1 is a less visited state, we see that the reverse occurs and
            the learning rate is reduced in order to maintain the existing value of Vs.
            6.3. Estimating a small Markov process
            For our ﬁrst test we consider a small Markov process with 21 states. In each
            step the state number is either incremented or decremented by one with equal
                                                                                                  115
                   TDUPDATINGWITHOUTALEARNINGRATE
                       0.30                                                           0.30
                                                          HL(1.0)                                                         HL(1.0)
                                                          TD(0.7) a = 0.07                                                TD(0.7) a = 0.07
                       0.25                               TD(0.7) a = 0.13             0.25                               TD(0.7) a = 0.13
                       0.20                                                           0.20
                      RMSE0.15                                                        RMSE0.15
                       0.10                                                           0.10
                       0.05                                                            0.05
                       0.00                                                           0.00
                          0.0       0.2       0.4      0.6       0.8       1.0            0.0      0.2       0.4       0.6       0.8       1.0
                                                  Time                  x1e+4                                    Time                   x1e+4
                   Figure 6.1.: 21 state Markov process,                           Figure 6.2.: 21 state Markov process,
                                      average performance over                                        average performance over
                                      10 runs.                                                        100 runs.
                   probability, unless the system is in state 0 or 20 in which case it always tran-
                   sitions to state 10 in the following step. When the state transitions from 0 to
                   10 a reward of 1.0 is generated, and for a transition from 20 to 10 a reward of
                   -1.0 is generated. All other transitions have a reward of 0. We set the discount
                   value γ = 0.9 and then computed the true discounted value of each state by
                   running a brute force Monte Carlo simulation.
                       For our ﬁrst test we ran our algorithm 10 times on the above Markov chain
                   and computed the root mean squared error in the value estimate across the
                   states at each time step averaged across each run. The optimal value of λ for
                   our algorithm, which we will call HL(λ), was 1.0. This was to be expected
                   given that the environment is stationary and thus discounting old experience
                   is not helpful. Setting this parameter correctly was important, for example if
                   we reduced the value of λ to 0.98 performance became poor.
                       For TD(λ) the optimal value of λ was about 0.7. This algorithm was much
                   less sensitive to the setting of λ. The other important parameter for TD(λ)
                   was the learning rate α. We tested a variety of values, the eﬀect of which is
                   illustrated by the two values α = 0.07 and α = 0.13 on Figure 6.1.
                       Whenαwashigh, TD(λ) learnt more quickly, as we would expect, but then
                   became unstable as the learning rate was too high for ﬁne tuning the value
                   estimates. With the lower learning rate of 0.07, TD(λ) learnt more slowly, but
                   eventually achieved a more accurate value estimate before becoming stuck. In
                   fact rather than just becoming stuck, what we see is that the error reaches
                   a minimum at around t = 4,000 and then actually becomes worse for the
                   remainder of the run. This is a well known undesirable characteristic of TD(λ)
                   (see for example Section 6.2 of Sutton and Barto, 1998). With a ﬁxed learning
                   rate we also see that the variability in the error estimate does not improve
                   towards the end of the run.
                   116
                                                               6.4. A larger Markov process
              In comparison, HL(λ) had very fast learning initially, combined with a more
           accurate ﬁnal estimate of the discounted state values, and the mean squared
           error in the second half of the experiment was very stable. This is signiﬁcant
           given that TD(λ) required two parameters to be tuned for good performance,
           while HL(λ) had just λ which could be set a priori to 1.0.
              Figure 6.2 shows the same experiment averaged over 100 runs. This obscures
           the better stability of HL(λ), but more clearly illustrates its faster learning and
           better convergence.
           6.4. A larger Markov process
           In order to understand how well HL(λ) scales as the number of states increases,
           we ran the previous experiment again but with 51 states. As the movement
           through the states is almost entirely a random walk with reward on just two
           transitions, estimating the value function on this Markov chain is signiﬁcantly
           more diﬃcult than before, even though the total number of states has not
           grown all that much. In the new experiment the return state was still in
           the middle of the chain, i.e. state 25. As most of the state space was a long
           way from the rewards, we increased the γ value to 0.99 so that states in the
           middle of the chain would not have values too close to 0. The true discounted
           value of each state was again computed by running a brute force Monte Carlo
           simulation.
              We ran our algorithm 10 times on the above Markov chain and computed
           the root mean squared error in the value estimate across the states at each
           time step averaged across each run. The optimal value of λ for HL(λ) was 1.0,
           which was to be expected given that the environment is stationary and thus
           discounting old experience is not helpful.
              For TD(λ) we tried various diﬀerent learning rates and values of λ. We
           could ﬁnd no settings where TD(λ) was competitive with HL(λ). If the learn-
           ing rate α was set too high the system would learn as fast as HL(λ) brieﬂy
           before becoming stuck. With a lower learning rate the ﬁnal performance was
           improved, however the initial performance was now much worse than HL(λ).
           The results of these tests appear in Figure 6.3.
              Similar tests were performed with larger and smaller Markov chains, and
           with diﬀerent values of γ. HL(λ) was consistently superior to TD(λ) across
           these tests. One wonders whether this may be due to the fact that the im-
           plicit learning rate that HL(λ) uses is not ﬁxed. To test this we explored the
           performance of a number of diﬀerent learning rate functions on the 51 state
           Markov chain described above. We found that functions of the form κ always
                                                                                      t
           performed poorly, however good performance was possible by setting κ cor-
                                               κ       κ
           rectly for functions of the form √ and √ . As the results were much closer,
                                                t      3 t
           we averaged over 300 runs. These results appear in Figure 6.4.
              With a variable learning rate TD(λ) is performing much better, however we
           were still unable to ﬁnd an equation that reduced the learning rate in such
                                                                                           117
                   TDUPDATINGWITHOUTALEARNINGRATE
                       0.40                                                           0.40
                                                            HL(1.0)                                                 HL(1.0)
                       0.35                                 TD(0.9) a = 0.1            0.35                         TD(0.9) a = 8.0/sqrt(t)
                                                            TD(0.9) a = 0.2                                         TD(0.9) a = 2.0/cbrt(t)
                       0.30                                                           0.30
                       0.25                                                            0.25
                      RMSE0.20                                                        RMSE0.20
                       0.15                                                            0.15
                       0.10                                                           0.10
                       0.05                                                            0.05
                          0.0         0.5          1.0         1.5         2.0            0.0         0.5         1.0          1.5         2.0
                                                  Time                  x1e+4                                    Time                   x1e+4
                   Figure 6.3.: 51 state Markov process                            Figure 6.4.: 51 state Markov process
                                      averaged over 10 runs.                                          averaged over 300 runs.
                                      The parameter a is the
                                      learning rate α.
                   a way that TD(λ) would outperform HL(λ). This is evidence that HL(λ) is
                   adapting the learning rate optimally without the need for manual equation
                   tuning.
                   6.5. Random Markov process
                   To test on a Markov process with a more complex transition structure, we
                   created a random 50 state Markov process. We did this by creating a 50 by 50
                   transition matrix where each element was set to 0 with probability 0.9, and a
                   uniformly random number in the interval [0,1] otherwise. We then scaled each
                   row to sum to 1. Then to transition between states we interpret the ith row
                   as a probability distribution over which state follows state i. To compute the
                   reward associated with each transition we created a random matrix as above,
                   but without normalising. We set γ = 0.9 and then ran a brute force Monte
                   Carlo simulation to compute the true discounted value of each state.
                       The λ parameter for HL(λ) was simply set to 1.0 as the environment is
                   stationary. For TD we experimented with a range of parameter settings and
                   learning rate decrease functions. We found that a ﬁxed learning rate of α = 0.2,
                                                            1.5
                   and a decreasing rate of √ performed reasonable well, but never as well as
                                                            3 t
                   HL(λ). The results were generated by averaging over 10 runs, and are shown
                   in Figure 6.5.
                       Although the structure of this Markov process is quite diﬀerent to that used
                   in the previous experiment, the results are again similar: HL(λ) preforms as
                   well or better than TD(λ) from the beginning to the end of the run. Further-
                   more, stability in the error towards the end of the run is better with HL(λ)
                   and no manual learning tuning was required for these performance gains.
                   118
                                                                                 6.6. Non-stationary Markov process
                     0.7                                                           0.30
                                                 HL(1.0)                                                               HL(0.9995)
                     0.6                         TD(0.9) a = 0.2                                                       TD(0.8) a = 0.05
                                                 TD(0.9) a = 1.5/cbrt(t)           0.25                                TD(0.9) a = 0.05
                     0.5
                                                                                   0.20
                     0.4
                    RMSE                                                           RMSE0.15
                     0.3
                                                                                   0.10
                     0.2
                     0.1                                                           0.05
                     0.0                                                           0.00
                       0        1000     2000      3000      4000      5000           0.0          0.5         1.0         1.5          2.0
                                              Time                                                            Time                   x1e+4
                Figure 6.5.: Random 50 state Markov                             Figure 6.6.: 21           state     non-stationary
                                   process. The parameter a                                        Markov process.
                                   is the learning rate α.
                6.6. Non-stationary Markov process
                The λ parameter in HL(λ), introduced in Equation (6.4), reduces the impor-
                tance of old observations when computing the state value estimates. When the
                environment is stationary this is not useful and so we can set λ = 1.0, however
                in a non-stationary environment we need to reduce this value so that the state
                values adapt properly to changes in the environment. The more rapidly the
                environment is changing, the lower we need to make λ in order to more rapidly
                forget old observations.
                    To test HL(λ) in such a setting we reverted back to the 21 state Markov
                chain from Section 6.3 in order to speed up convergence. We used this Markov
                chain for the ﬁrst 5,000 time steps. At that point, we changed the reward
                when transitioning from the last state to the middle state from -1.0 to be 0.5.
                At time 10,000 we then switched back to the original Markov chain, and so on
                alternating between the models of the environment every 5,000 steps. At each
                switch, we also changed the target state values that the algorithm was trying
                to estimate to match the current conﬁguration of the environment. For this
                experiment we set γ = 0.9.
                    As expected, the optimal value of λ for HL(λ) fell from 1 down to about
                0.9995. This is about what we would expect given that each phase is 5,000
                steps long. For TD(λ) the optimal value of λ was around 0.8 and the optimum
                learning rate was around 0.05. As we would expect, for both algorithms when
                we pushed λ above its optimal value this caused poor performance in the
                periods following each switch in the environment (these bad parameter settings
                are not shown in the results). On the other hand, setting λ too low produced
                initially fast adaption to each environment switch, but poor performance after
                that until the next environment change. To get accurate statistics we averaged
                over 200 runs. The results of these tests appear in Figure 6.6.
                                                                                                                                    119
             TDUPDATINGWITHOUTALEARNINGRATE
             Algorithm 2 HLS(λ)
                Initialise Q(s,a) = 0, N(s,a) = 1 and E(s,a) = 0 for all s, a
                Initialise s and a
                repeat
                  Take action a, observed r, s′
                  Choose a′ by using ǫ-greedy selection on Q(s′,·)
                  ∆←r+γQ(s′,a′)−Q(s,a)
                  E(s,a) ← E(s,a)+1
                  N(s,a) ← N(s,a)+1
                  for all s,a do
                               ′  ′              1         N(s′,a′)
                    β((s,a),(s ,a )) ← N(s′,a′)−γE(s′,a′)   N(s,a)
                  end for
                  for all s,a do
                    Q(s,a) ← Q(s,a)+β (s,a),(s′,a′)E(s,a)∆
                    E(s,a) ← γλE(s,a)
                    N(s,a) ← λN(s,a)
                  end for
                  s ←s′;a ←a′
                until end of run
                For some reason HL(0.9995) learns faster than TD(0.8) in the ﬁrst half
             of the ﬁrst cycle, but only equally fast at the start of each following cycle.
             Furthermore, its performance in the second half of the ﬁrst cycle is poor. We
             are not sure why this is happening. We could improve the initial speed at
             which HL(λ) learnt in the last three cycles by reducing λ, however that comes
             at a performance cost in terms of the lowest mean squared error attained at
             the end of each cycle. In any case, in this non-stationary situation HL(λ) again
             performed well in general.
             6.7. Windy Gridworld
             Reinforcement learning algorithms such as Watkins’ version of Q(λ) (Watkins,
             1989) and Sarsa(λ) (Rummery and Niranjan, 1994; Rummery, 1995) are based
             ontemporaldiﬀerence updates. This suggests that new reinforcement learning
             algorithms based on HL(λ) should be possible.
                For our ﬁrst experiment we took the standard Sarsa(λ) algorithm and mod-
             iﬁed it in the obvious way to use an HL temporal diﬀerence update. In the
             presentation of this algorithm we have changed notation slightly to make things
             more consistent with that typical in reinforcement learning. Speciﬁcally, we
             have dropped the t super script as this is implicit in the algorithm speciﬁca-
             tion, and have deﬁned Q(s,a) := V(s,a), E(s,a) := E(s,a) and N(s,a) := N(s,a).
             Our new reinforcement learning algorithm, which we call HLS(λ) is given in
             Algorithm 2. Essentially the only changes to the standard Sarsa(λ) algorithm
             120
                                                                6.7. Windy Gridworld
          Figure 6.7.: [Windy Gridworld] S marks the start state and G the goal state,
                      at which the agent jumps back to S with a reward of 1. Small
                      arrows indicate an upward wind of one row per time step. The
                      large arrows indicate a wind of two rows per time step.
          have been to add code to compute the visit counter N(s,a), add a loop to
          compute the β values, and replace α with β in the temporal diﬀerence update.
            To test HLS(λ) against standard Sarsa(λ) we used the Windy Gridworld
          environment described on page 146 of (Sutton and Barto, 1998). This world is
          a grid of 7 by 10 squares that the agent can move through by going either up,
          down, left or right. If the agent attempts to move oﬀ the grid it simply stays
                                                th             st
          where it is. The agent starts in the 4   row of the 1   column and receives
                                                        th             th
          a reward of 1 when it ﬁnds its way to the 4     row of the 8    column. To
          make things more diﬃcult, there is a “wind” blowing the agent up 1 row in
          columns 4, 5, 6, and 9, and a strong wind of 2 rows in columns 7 and 8. This
          is illustrated in Figure 6.7. Unlike in the original version, we have set up this
          problem to be a continuing discounted task with an automatic transition from
          the goal state back to the start state. This is because we have not yet derived
          an episodic version of our learning rule.
            Weset γ = 0.99 and in each run computed the empirical future discounted
          reward at each point in time. As this value oscillated we also ran a moving
          average through these values with a window length of 50. Each run lasted for
          50,000 time steps as this allowed us to see at what level each learning algorithm
          topped out. These results appear on the left of Figure 6.8 and were averaged
          over 500 runs to get accurate statistics.
            Despite putting considerable eﬀort into tuning the parameters of Sarsa(λ),
          we were unable to achieve a ﬁnal future discounted reward above 5.0. The
                                                                                  121
                TDUPDATINGWITHOUTALEARNINGRATE
                     6                                                    6
                     5                                                    5
                     4                                                    4
                     3                                                    3
                     2                                                    2
                     Future Discounted Reward1                            Future Discounted Reward1
                                      HLS(0.995) e = 0.003                                   HLQ(0.99) e = 0.01
                                      Sarsa(0.5) a = 0.4 e = 0.005                           Q(0.75) a = 0.99 e = 0.01
                     0                                                    0
                      0     10000    20000   30000   40000   50000         0     10000   20000   30000    40000   50000
                                         Time                                                 Time
                Figure 6.8.:      Windy Gridworld performance tests.                   The left graph shows
                                HLS(λ) vs. Sarsa(λ), while the right graph shows HLQ(λ) vs.
                                Q(λ). In both tests the algorithm based on HL learning performed
                                best. e represents the exploration parameter ǫ, and a represents
                                the learning rate α. For both tests the performance was averaged
                                over 500 runs.
                settings shown on the graph represent the best ﬁnal value we could achieve.
                In comparison HLS(λ) easily beat this result at the end of the run, while being
                slightly slower than Sarsa(λ) at the start. By setting λ = 0.99 we were able
                to achieve the same performance as Sarsa(λ) at the start of the run, however
                the performance at the end of the run was then only slightly better than
                Sarsa(λ). This combination of superior performance and fewer parameters
                to tune suggest that the beneﬁts of HL(λ) carry over into the reinforcement
                learning setting. In terms of computational cost, HL(λ) was about 1.7 times
                slower than Sarsa(λ) per time step due to the cost of computing the β values.
                   Anotherpopularreinforcement learning algorithm is Watkins’ Q(λ). Similar
                to Sarsa(λ) above, we simply inserted the HL(λ) temporal diﬀerence update
                into the usual Q(λ) algorithm in the obvious way. We call this new algorithm
                HLQ(λ) and it is given in Algorithm 3. The test environment was exactly the
                same as we used with Sarsa(λ) above.
                   The results this time were more competitive and appear on the right hand
                side of Figure 6.8. Nevertheless, despite spending a considerable amount of
                time ﬁne tuning the parameters of Q(λ), we were unable to beat HLQ(λ). As
                the performance advantage was relatively modest, the main beneﬁt of HLQ(λ)
                was that it achieved this level of performance without having to manually tune
                a learning rate.
                122
                                                                                    6.8. Conclusion
            Algorithm 3 HLQ(λ)
               Initialise Q(s,a) = 0, N(s,a) = 1 and E(s,a) = 0 for all s, a
               Initialise s and a
               repeat
                 Take action a, observed r, s′
                 Choose a′ by using ǫ-greedy selection on Q(s′,·)
                 a∗ ←argmaxbQ(s′,b)
                 ∆←r+γQ(s′,a∗)−Q(s,a)
                 E(s,a) ← E(s,a)+1
                 N(s,a) ← N(s,a)+1
                 for all s,a do
                                ′   ′              1           N(s′,a′)
                    β((s,a),(s ,a )) ←         ′ ′       ′  ′
                 end for                   N(s ,a )−γE(s ,a )   N(s,a)
                 for all s,a do
                                                        ′  ′ 
                    Q(s,a) ← Q(s,a)+β (s,a),(s ,a ) E(s,a)∆
                    N(s,a) ← λN(s,a)
                    if a′ = a∗ then
                       E(s,a) ← γλE(s,a)
                    else
                       E(s,a) ← 0
                    end if
                 end for
                 s ←s′;a ←a′
               until end of run
            6.8. Conclusion
            Wehave derived a new equation for setting the learning rate in temporal dif-
            ference learning with eligibility traces. The equation replaces the free learning
            rate parameter α, which is normally experimentally tuned by hand. In every
            setting tested, be it stationary Markov chains, non-stationary Markov chains
            or reinforcement learning, our new method produced superior results.
               To further our theoretical understanding, the next step would be to try
            to prove that the method converges to correct estimates. This can be done
            for TD(λ) under certain assumptions on how the learning rate decreases over
            time (Dayan, 1992; Peng, 1993). Hopefully, something similar can be proven
            for our new method. In terms of experimental results, it would be interesting
            to try diﬀerent types of reinforcement learning problems and to more clearly
            identify where the ability to set the learning rate diﬀerently for diﬀerent state
            transition pairs helps performance.
               Many extensions to the algorithm should also be possible. One would be
            to generalise the learning rule to episodic tasks, another would be to merge
            our update rule with Peng’s version of Q(λ) (Peng and Williams, 1996), as we
                                                                                                   123
       TDUPDATINGWITHOUTALEARNINGRATE
       have done with Sarsa(λ) and Watkins’ version of Q(λ). Finally, it would be
       useful to extend the algorithm to work with function approximation methods
       so that it could deal with larger state spaces.
       124
     7. Discussion
     Thetitle of this thesis is deliberately provocative. It asks the reader to consider
     not just intelligent machines, but the possibility of machines that are super
     intelligent. Many ﬁnd this idea diﬃcult to take seriously. Among researchers
     the topic is almost taboo: it belongs in science ﬁction. The most intelligent
     computer in the world, they assure the public, is perhaps as smart as an ant,
     and that’s on a good day. True machine intelligence, if it is ever developed,
     lies in the distant future.
      This was not always the case. In the 1960’s pioneering artiﬁcial intelligence
     researchers had initial successes in a range of areas. Emboldened, they pre-
     dicted that more powerful systems were not far oﬀ, and that truly intelligent
     machines might follow. As the researcher Herbert Simon wrote in 1965, “ma-
     chines will be capable, within twenty years, of doing any work a man can
     do.” (quoted in Crevier, 1993) The ﬁeld was alight with ambition and, not
     surprisingly, attracted plenty of attention.
      Over the decade that followed a series of high proﬁle failures brought this
     dream crashing back to earth. Although progress was being made, it was
     far slower than many had expected. Naturally the public, and more impor-
     tantly the funding agencies, wanted to know where the intelligent machines
     were. The cuts that ensued marked the beginning of the so called ‘AI win-
     ter’, although usage of this label varies considerably (Crevier, 1993; Russell
     and Norvig, 1995). During the early 80’s there was a brief reprieve as expert
     systems became popular, however by the late 80’s these too had failed to live
     up to expectations. Over time some areas distanced themselves from the label
     ‘artiﬁcial intelligence’, emphasising that their work was more practical and
     limited in scope. Any talk of building machines with human level intelligence
     was frowned upon.
      Since the early 90’s steady progress has slowly begun to reinvigorate the
     ﬁeld, particularly since the late 90’s. More powerful algorithms coupled with
     dramatically improved hardware has produced countless advances in robotics,
     speech recognition, natural language processing, image processing, clustering,
     classiﬁcation, prediction, various types of optimisation, and many other areas.
     As a result the reputation of artiﬁcial intelligence has started to recover and
     funding has improved, leading some to believe that the ‘AI spring’ may have
     ﬁnally arrived (Havenstein, 2005).
      With the mood becoming more positive, the grand dream of artiﬁcial in-
     telligence is starting to make a come back. A number of books predicting
     the arrival of advanced artiﬁcial intelligence have been published and major
     conferences have held workshops on ‘Human level AI’. Small conferences on
                                       125
            7. Discussion
            ‘Artiﬁcial General Intelligence’ and on the safety issues surrounding powerful
            machine intelligence have also appeared. Perhaps over the next few years these
            ideas will become more mainstream, however for now they are at the fringe.
            Most researchers remain very sceptical about the idea of truly intelligent ma-
            chines within their lifetime.
              Onegoal of this thesis is to promote the idea that intelligent machines, even
            super intelligent machines, is a topic that is both important and one that can
            be scientiﬁcally studied, even if just theoretically for now. In Chapters 2 and 3
            we described Hutter’s model of an intelligent machine and examined some of
            its remarkable properties. In Chapter 4 we saw how this model can be used to
            construct a general measure of machine intelligence that formalises many of
            the standard perspectives on the nature of intelligence outlined in Chapter 1.
            In Chapter 5 we then examined some of the theoretical limitations faced by
            powerful artiﬁcial intelligence algorithms. Thus, although highly intelligent
            machines do not exist yet, theoretical tools are starting to emerge to allow us
            to study their properties. While this thesis makes some contributions to this
            eﬀort, many fundamental questions remain open (see for example the open
            problems listed in Hutter, 2005).
              None of this theoretical work would be of much importance if intelligent
            machines were impossible in practice, or exceedingly unlikely in any reasonable
            time frame. In this last chapter we will argue that this is not the case. Our
            goal is not to conclusively argue that this will happen, or exactly when it
            will happen, but simply to argue that the possibility cannot be completely
            discounted.  This is important because if a super intelligent machine ever
            did exist the implications for humanity would be immense. Thus, if there is
            even a small probability that intelligent machines could be developed in the
            foreseeable future, it is important that we start to think seriously about the
            nature of these machines and what the implications might be.
            7.1. Are super intelligent machines possible?
            Many people outside of the ﬁeld are deeply sceptical about the idea that ma-
            chines, mere physical objects of our construction, could ever have anything
            resembling real intelligence: machines can only ever be strictly logical; they
            cannot do anything they were not programed to do; and they certainly could
            not be superior to their own creator — that would be a paradox! However,
            as anybody working in the ﬁeld knows, these common beliefs are baseless
            myths. Artiﬁcial intelligence algorithms regularly ﬁnd solutions to problems
            using heuristics and forms of reasoning that are not strictly logical. They dis-
            cover powerful new designs for problems that the system’s programmers had
            never thought of (Koza et al., 2003). They also learn to play games such as
            chess (Hsu et al., 1995) and backgammon (Tesauro, 1995) at levels superior
            to that of any human, let alone the researchers who designed and created the
            126
                   7.1. Are super intelligent machines possible?
     system. Indeed, in the case of checkers, computers are now literally unbeatable
     as they can play a provably perfect game (Schaeﬀer et al., 2007).
      The persistence of these beliefs seems to be due to a number of things. One
     is that algorithms from artiﬁcial intelligence are not consumer products: they
     are hidden in the magic of sophisticated technology. For example, when hand
     writing the address on a card most people do not know that it will likely be
     read by a computer rather than a human at the sorting oﬃce. People do
     not think about the learning algorithms that are monitoring their credit card
     transactions looking for fraud, ﬁltering spam to their email address, automat-
     ically trading their retirement savings on international markets, monitoring
     their behaviour on the internet in order to decide which ads should appear on
     webpages they view, or even just the vision processing algorithms that graded
     the apples at the supermarket. The steady progress that artiﬁcial intelligence
     algorithms are making is out of sight, and thus generally out of mind.
      Another common objection is that we humans have something mysterious
     and special that makes us tick, something that machines, by deﬁnition, do not
     have. Perhaps some type of non-physical consciousness or feelings, qualia, or
     ‘quantum ﬁeld’ etc. Of course it is impossible to rule out mysterious possibili-
     ties until an intelligent machine has been constructed without needing anything
     particularly mysterious. Nonetheless, we should view such objections for what
     they are: a form of vitalism. Throughout history, whenever science could not
     explain some unusual phenomenon, many people readily assumed that God or
     magic was at work. Even distinguished scientists have fallen into this, only
     to be embarrassed once more sceptical and curious scientists worked out what
     wasactually going on. Things ranging from the motion of whole galaxies to the
     behaviour of sub-atomic particles are now known to follow extremely precise
     physical laws. To conjecture that our brains are somehow special and diﬀerent
     in some strange way is to speculate based on nothing but our own feelings of
     specialness.
      If the human brain is merely a ‘meat machine’, as some have put it, it is
     certainly not the most powerful intelligence possible. To start with, there
     is the issue of scale: a typical adult human brain weights about 1.4 kg and
     consumesjust25wattsofpower(Kandeletal.,2000). Thisisidealforamobile
     intelligence, however an artiﬁcial intelligence need not be mobile and thus could
     be orders of magnitude larger and more energy intensive. At present a large
     supercomputer can ﬁll a room twice the size of a basketball court and consume
     10 megawatts of power. With a few billion dollars much larger machines could
     be built. Google, for example, is currently constructing a data centre next to
     a power station in Oregon that will cover two football ﬁelds and have cooling
     towers four stories high (Markoﬀ and Hansell, 2005). Biology never had the
     option of building brains on such an enormous scale.
      Another point is that brains use fairly large and slow components. Consider
     one of the simpler of these, axons: essentially the wiring of the nervous system.
     These are typically around 1 micrometre wide, carry spike signals at up to 75
     metres per second at a frequency of at most a few hundred hertz (Kandel et al.,
                                      127
      7. Discussion
      2000). Compare these characteristics with those of a wire that carries signals
      on a microchip. Currently these are 45 nanometres wide, propagate signals at
      300 million metres per second and can easily operate at 4 billion hertz. Some
      might debate whether an electrochemical spike travelling down an axon is so
      directly comparable to an electrical pulse travelling down a wire, however it
      is well established that at least the primary role of an axon is simply to carry
      this information. Given that present day technology produces wires which are
      20 times thinner, propagate signals 4 million times faster and operate at 20
      million times the frequency, it is hard to believe that the performance of axons
      could not be improved by at least a few orders of magnitude.
       Of course, the above assumes that the brain’s design is what we should
      replicate. Perhaps the brain’s algorithm is close to optimal for some things,
      but it certainly is not optimal for all problems. Even the most outstanding sa-
      vants cannot store information anywhere near as quickly, accurately and in the
      quantities that are possible for a computer. Also savants’ impressive ability to
      perform fast mental calculations is insigniﬁcant next to even a basic calculator.
      Brains are poorly designed for such feats. A machine, however, would have no
      such limitations: it could employ a range of specialised algorithms for diﬀerent
      types of problems. Concepts like education become obsolete when knowledge
      and understanding can simply be copied from one intelligent machine to an-
      other. It is easy to think up many more advantages.
       Most likely improvements over brains are possible in algorithms, hardware
      and scale. This is not to take away from the amazing system that the brain
      is, something that we are still unable to match in many ways. All we wish to
      point out is that if the brain is essentially just a machine, which appears to be
      the case, then it certainly is not the most intelligent machine that could exist.
      This idea is reasonable once you think about it: machines can easily carry
      more, ﬂy higher, move faster and see further than even the most able animals
      in each of these categories. Why would human intelligence be any diﬀerent? Of
      course, just because systems with greater than human intelligence are possible
      in principle, this does not mean that we will be able to build one. Designing
      and constructing such an advanced machine could be beyond our capabilities.
      7.2. How could intelligent machines be developed?
      There are many ways in which machine intelligence might be developed. Un-
      fortunately, it is diﬃcult to estimate how likely any of these approaches are to
      succeed. In this section we speculate on a few of them.
      Theoretical approaches
      The approach most closely related to this thesis would be to take the AIXI
      model and ﬁnd a way to usefully scale it down. A number of attempts to
      do this have been made: the HL(λ) algorithm presented in Chapter 6, the
      128
                   7.2. How could intelligent machines be developed?
     AIXItl algorithm (Chapter 7 of Hutter, 2005), the AIXI based algorithm for
     repeated matrix games (Poland and Hutter, 2006) and Fitness Uniform Opti-
     misation (Hutter and Legg, 2006) all originate in eﬀorts to scale down AIXI.
     Although not deriving from AIXI, the Shortest and Fastest algorithm in (Hut-
     ter, 2002a), the Speed prior (Schmidhuber, 2002), the Optimal Ordered Prob-
     lem Solver (Schmidhuber, 2004), and the G¨odel Machine (Schmidhuber, 2005)
     come from a related background in algorithmic probability and Kolmogorov
     complexity theory. While each of these has their strengths, as yet none come
     close to bringing the power of theoretical models such as AIXI into reality.
       The key question is how to make a general and powerful artiﬁcial intelli-
     gence like AIXI work eﬃciently. The results of Chapter 5 oﬀer some hints
     on the direction that such a project might take. For certain, the prediction
     of general computable sequences is out of the question (Lemma 5.2.4), as is
     the prediction of all computable sequences whose Kolmogorov complexity is
     below some moderate bound. Otherwise serious problems arise with the neces-
     sary complexity of the prediction algorithms (Theorem 5.3.3), and even worse
     G¨odel incompleteness (Theorem 5.6.1). Nevertheless, we know that certain
     types of complex sequences can be predicted by relatively simple algorithms
     (Lemma 5.2.3), and that many theoretical problems go away when even weak
     bounds are placed on the computation time of the sequences to be predicted
     (Lemma 5.5.2). Thus, what we need to aim for is the ability to eﬃciently pre-
     dict computable sequences that have certain computational resource bounds.
     How best to characterise such sequences is an open problem, let alone how to
     eﬃciently learn to predict them. Perhaps any breakthrough is more likely to
     come from the opposite direction: somebody discovers a theoretically elegant
     andverypowerful algorithm that is able to eﬃciently predict many kinds of se-
     quences. The structure of this algorithm and how easily it can model diﬀerent
     sequences will then implicitly deﬁne a natural measure of resource bounded
     sequence complexity.
       When a breakthrough in this area might occur is impossible to predict,
     and the same is true of other theoretical approaches. Perhaps with the right
     theoretical insight Bayesian networks, prediction with expert advice, artiﬁcial
     neural networks, reasoning engines, or any one of a dozen other techniques
     will suddenly advance in a dramatic way. Although some progress in all of
     these areas is a near certainty, true breakthroughs by their very nature are
     rare and highly unpredictable. Furthermore, even if a huge breakthrough did
     occur, whether existing computer hardware would be suﬃcient to support the
     creation of highly intelligent machines would depend on the nature of this
     new algorithm. In summary, although we cannot rule out the possibility of a
     large breakthrough leading to intelligent machines, there is little we can do to
     estimate how likely this is.
                                          129
      7. Discussion
      Brain simulation
      RatherthantakingabstracttheorieslikeAIXIandtryingtoscalethemdownto
      makethempractical, another approach is to proceed in the opposite direction:
      start by studying the details of how real biological brains work and then try to
      abstract from this a design for an artiﬁcial intelligence. Although this idea is
      simple in principle, studying and understanding the brain is very challenging.
       Oneofthemostbasicproblemsisthatitis currently impossible to observe a
      brain in action with suﬃcient spatial and temporal resolution to really see how
      it works. Methods to study the brain include: PET scans, fMRI scans, arrays
      of probes, marking neurons with chemicals which cause them to ﬂuoresce when
      they ﬁre, placing extracted neural tissue on microchips that can sense when
      neurons ﬁre, and the use of staining and microscopy to study the anatomical
      structure of the brain. The problem is that none of these methods allows a
      researcher to take a sizable area of a brain and simultaneously observe exactly
      which neurons are ﬁring, precisely when they ﬁre, what type of neurons they
      are, which other neurons they are connected to, the types of these connections,
      how these connections change over short intervals of time, and so on. Instead,
      researchers have at their disposal a range of methods, each of which provides
      only a limited view of what is going on. Indeed, what is known about the brain
      is largely dictated by the strengths and weaknesses of the diﬀerent methods of
      study.
       Another major problem is the sheer complexity of the system. A human
      brain consists of hundreds of billions of neurons, and hundreds of trillions
      of synapses. There are over a hundred diﬀerent types of neurons, diﬀerent
      synapses employ diﬀerent combinations of neurotransmitters, connection pat-
      terns vary from one part of the brain to another, and so on (see any standard
      text book, for example Kandel et al., 2000). When looking at slices of brain
      tissue where a small percentage of neurons and their dendritic trees have been
      stained, the brain’s wiring starts to look about as comprehensible as an ocean
      of tangled spaghetti. Even worse, all these elements interact with each other
      in highly dynamic ways. Via some kind of a miracle, from this monstrous
      cacophony emerges the human mind. With so much complexity, even if the
      perfect brain scanning technology existed it might still be very diﬃcult to
      understand how the brain actually works.
       Given the scale of these diﬃculties, is building a brain simulation feasible?
      Perhaps the ﬁrst point to note is that building a working simulation of some-
      thing does not require understanding everything about how the system works.
      What is required is that the basic units which comprise the system can be
      faithfully reproduced and connected together. If this is done properly, the re-
      sulting dynamics will be the same as the dynamics in the real system — even
      if we do not fully understand what these higher level dynamics are, or why
      they are important to the system’s overall functioning. Indeed, simulations
      are often constructed for the very purpose of better understanding a system’s
      emergent dynamics once its low level dynamics are understood. Thus, rather
      130
                 7.2. How could intelligent machines be developed?
     than needing to understand all the mysteries of how the brain works, in order
     to build a functional simulation it is suﬃcient to understand the nature and or-
     ganisation of the brain’s elementary units: neurons, dendrites, axons, synapses
     etc. There is already a large body of knowledge about how these basic units
     work and how they are wired together in various parts of the brain. Literally
     thousands of researchers around the world are reﬁning and developing this
     knowledge.
      Another important point, at least for people interested in developing arti-
     ﬁcial intelligence, is that much of the brain’s complexity is not relevant. A
     signiﬁcant part of the human brain is a jumble of diﬀerent subsystems that
     take care of basic instinctive things like breathing, heart beat, blood pressure,
     reproduction, hunger, thirst, rhythms such as sleeping and body temperature,
     ﬁghtorﬂightresponseandsoon. Evenoneofthelargestpartsofthebrain, the
     cerebellum which is involved in movement and precision timing, is not needed
     for an artiﬁcial intelligence: individuals without one are still intellectually
     and emotionally able. The key, it seems, lies in understanding the neocortex,
     and its interaction with two smaller structures, namely, the thalamus and the
     hippocampus. It is known that the neocortex is the part of the brain that
     is primarily responsible for processing vision, sound, touch, proprioception,
     understanding and generating language, planning, spatial reasoning, coordi-
     nating and executing movement, and logical thought (Fuster, 2003). Clearly
     then, the key to artiﬁcial intelligence via brain simulation lies in understanding
     the neocortex and related structures.
      As diﬀerent regions of the neocortex perform diﬀerent functions, one might
     expect that they would have signiﬁcantly diﬀerent anatomical structures.
     Amazingly, this is not the case. Essentially the whole neocortex has the same
     six layer structure, or up to 12 layers depending on how you count them. Each
     layer is characterised by the types of neurons present, where axons from neu-
     rons in the layer project to, and where axons come from that form synapses
     on the dendritic trees of these neurons. Besides some thickening and thinning
     of the layers in diﬀerent regions, and the fact that primary visual cortex actu-
     ally has an extra layer, this six layer structure is consistent across the whole
     neocortex (Fuster, 2003; Abeles, 1991). What this suggests is that the same
     information processing mechanism is being applied across the neocortex, and
     that the variations in function across diﬀerent regions are actually adaptations
     to the information passing through each region (Creutzfeldt, 1977; Mountcas-
     tle, 1978).
      Anumberofresults back up this hypothesis. One is that with increased use
     the region of cortex responsible for performing some action tends to expand,
     in the sense that neighbouring cortex is recruited. In extreme cases, such as
     the congenitally blind, the unused areas of visual cortex start to perform other
     roles, such as helping touch processing for reading braille. In a more dramatic
     example, the brain of a ferret was physically altered at birth so that visual
     neocortex received auditory input and vice versa. Each region of neocortex
                                      131
      7. Discussion
      thenlearnttoprocesstheinputitwasreceiving(Melchneretal., 2000). Similar
      experiments with rats have produced consistent results.
       If the hypothesis of a single underlying learning and adaptation dynamic
      across the neocortex is correct, then the key to understanding much of the in-
      tellectual capacity of the brain lies in understanding how these six layers work,
      and the way in which they interact with the thalamus and hippocampus. In
      recent years this approach to artiﬁcial intelligence has been popularised by Jeﬀ
      Hawkins (2004), though most of these ideas have been known in neuroscience
      for some time. The main vehicle for Hawkins’ artiﬁcial intelligence work is
      his company Numenta, with related neuroscience research being carried out at
      the Redwood Center for Theoretical Neuroscience, also founded by Hawkins.
      They are certainly not alone in trying to understand the cortex, indeed it is
      one of the largest areas of neuroscience research with whole journals dedicated
      to the topic.
       One group of researchers whose work may be useful to brain modelling is
      currently cutting a cortical column into 30 nanometre slices and then scan-
      ning these using both electron and light based microscopes. As this produces
      enormous quantities of data, machine learning algorithms are being developed
      to automatically identify the structures in these images. Over the next few
      years this process should produce an extremely detailed three dimensional
      anatomical model of the cortex (Singer, 2007).
       Agroup with more of a simulation emphasis is the BlueBrain project based
      at EPFL.Usinginformationonthestructureandbehaviourofcorticalcolumns
      collected from a wide range of sources, they have built a computer model of
      a column that they run on an IBM BlueGene supercomputer. To calibrate
      their model they use segments of rat cortex which they stimulate in diﬀerent
      waysandthencomparetheresultingdynamicswithwhattheirmodelpredicts.
      They now claim to have succeeded in accurately modelling the dynamics of
      a cortical column, and are working on ways to expand this model to be able
      to simulate groups of columns working together (Graham-Rowe, 2007). Their
      goal is to eventually be able to simulate an entire human neocortex.
       Another group working at the IBM Almaden Research Lab recently an-
      nounced that they had simulated a mouse scale neocortex, also on an IBM
      BlueGene supercomputer. Their model consisted of 8 million neurons and
      50 billion synapses and ran at one seventh real time speed. They claim that
      this simulation produced dynamical properties consistent with what is ob-
      served in a real mouse brain, including EEG like waves (Ananthanarayanan
      and Modha, 2007). Their aim is also to scale up to a human sized neocortex as
      more powerful supercomputers become available in the coming years. Unlike
      the BlueBrain project, their core goals are less neuroscience orientated: as
      their model starts to do more interesting things, their aim is to extract from
      this useful algorithms for artiﬁcial intelligence.
       Obviously these simulations are pushing the limits of what is known about
      the cortex, and also what is possible with current computer technology. Nev-
      ertheless, the fact that these simulations are being attempted at all illustrates
      132
                                        7.2. How could intelligent machines be developed?
           how the gap between the power of supercomputers and brains is perhaps not
           as large as some people think. At present the world’s fastest machine is the
           IBM Roadrunner supercomputer at the US department of defence which has
                                                                  15
           an oﬃcial LINPACK benchmark performance of 10            ﬂoating point operations
                                                           16
           per second (FLOPS). Machines capable of 10         FLOPSarebeingdesignedand
           should appear in a few years. No doubt these will be superseded by even more
           powerful machines.
              To put these numbers in perspective, a human cortex has on the order of
              10                 14
           10   neurons and 10      synapses (Koch, 1999). Given that neurons can ﬁre on
           the order of 100 Hz, this gives a crude estimate of the computational capacity
                            16
           of the brain: 10    operations per second (Moravec, 1998; Kurzweil, 2000). Of
           course, until a simulation succeeds in producing intelligence, nobody knows
           for sure how much computer power will be needed. Researchers working in
           molecular neuroscience tend to think it is much more, while some working in
                                                                                  16
           theoretical neuroscience think it could be less. If the estimate of 10   FLOPSis
           in fact 100 times too low, then we will have to wait 10 years before a suﬃciently
           powerful computer exists.
           Evolution
           Another approach that is becoming more attractive with increasing computer
           power is artiﬁcial evolution. After all, natural evolution produced the human
           brain so we know that the approach does work — at least as a planet wide
           phenomenon over billions of years! No computer in the foreseeable future
           could hope to simulate evolution on such a scale. Fortunately, evolving an
           artiﬁcial intelligence via an evolutionary algorithm is a much smaller problem.
           To begin with, it is not necessary to start from scratch the way nature did.
           Of the approximately 4 billion years since simple cellular life ﬁrst came into
           existence, about 3 billion years of this time was required just to get to the level
           of multicellular life. Only then could more complex organisms start to evolve.
           Wecan short circuit this by building a virtual body for the agent. Of course,
           evolving an intelligence for a complex body might be too diﬃcult, thus we may
           want to begin with trying to evolve a simple intelligence for a simple body in
           a simple environment, and then scale up. In any case, all the evolutionary
           algorithm has to do is to work on the design of the agent’s intelligence: much
           of the remaining complexity can be taken care of by us.
              Another important point is that natural evolution does not seek to max-
           imise intelligence: its eﬀect is to optimise agents’ ability to spread their genes.
           Intelligence is a secondary feature that is more useful in some ecological niches
           than others. In artiﬁcial evolution this does not need to be the case. So long
           as we can deﬁne and measure intelligence in a suﬃciently general way, we
           can use this to evaluate the ﬁtness of individuals. With evolution explicitly
           directed towards maximising intelligence, progress towards more intelligent
           agents should be far more rapid.
                                                                                           133
            7. Discussion
              The universal intelligence measure described in Chapter 4 lays the founda-
            tion for such a measure. Essentially, what the universal intelligence measure
            says is that we should test agents initially over extremely trivial pattern recog-
            nition and interaction problems as these are the most important. As agents
            learn to solve these, we should slowly include more complex problems, always
            ensuring that agents are still able to solve the simple problems that came
            before. This ensures that the agents’ abilities remain very general as they
            develop. As the universal intelligence measure is still a theoretical deﬁnition,
            some further work would be needed to ﬁgure out how best to convert it into a
            practical test for evolving agents.
              Theimportanceofhavingagoodﬁtnessfunctioncannotbestressedenough:
            withaﬁtnessfunctionthatisreliable, smoothandhasagradientthatgenerally
            points in the right direction, even high dimensional optimisation problems
            can become relatively easy. With an unreliable or deceptive ﬁtness function
            seemingly simple problems can become practically unsolvable. It is currently
            unknown how well universal intelligence would work as a ﬁtness function for
            evolving agents with general intelligence.
              Another major issue concerns the model that we use for the agents’ intel-
            ligence: should the agents be neural networks, programs in some language,
            or some other kind of object? In theory any Turing equivalent representation
            should do, however in practice the choice of representation is important as
            it has the eﬀect of biasing the space of possibilities towards certain types of
            agents. Often choosing the right representation is critical to making artiﬁcial
            evolution work. For an intelligent agent there are many possible systems of
            representation, each with their respective proponents. Unfortunately, nobody
            really knows which of these is the best. About the only thing we know for sure
            is that nature was able to evolve intelligence working with networks of spik-
            ing neurons. On the other hand, with a computer system perhaps something
            closer to a traditional programing language would be more eﬃcient.
              One important problem faced in large scale artiﬁcial evolution is diversity
            control. Essentially, if you apply too much selection pressure the population
            tends to collapse around a small group of individuals that are all related to
            the ﬁttest individual. At this point evolution becomes stuck due to a lack
            of genetic diversity. If you reduce the selection pressure this helps, however
            now the speed at which the population ﬁtness rises is reduced. Even with no
            selection pressure at all population diversity tends to collapse over time due to
            the phenomenon of genetic drift. For problems where the evolving individuals
            are fairly simple this can easily be dealt with by creating a distance metric
            to evaluate how similar individuals are to each other. This can then be used
            to ensure that similar individuals tend not to mate with each other, or have
            reduced ﬁtness (see for example Goldberg and Richardson, 1987; Jong, 1975).
            In more complex problems, however, it can become very diﬃcult to judge
            how similar two individuals really are.  For example, two neural networks
            may compute the same function, but have completely diﬀerent weights and
            134
                 7.3. Is building intelligent machines a good idea?
     topologies. Indeed, due to Rice’s theorem it is impossible in general to decide
     whether two algorithms compute the same function.
      One solution is to use diversity control methods that do not rely on com-
     paring the genotypes of the individuals, but rather by comparing properties
     of their phenotypes. A simple example of this is Fitness Uniform Optimisa-
     tion where the evolutionary algorithm tries to increase the diversity in the
     population by increasing the diversity of ﬁtness (Hutter and Legg, 2006). In
     the case of the Fitness Uniform Selection Scheme (FUSS) this is achieved
     through selection pressure (Hutter, 2002b; Legg et al., 2004), while for the
     Fitness Uniform Deletion Scheme it is achieved by deletion (Legg and Hutter,
     2005a). Although these methods have not yet been applied to the evolution
     of programs or neural networks, they have proven to be eﬀective on a number
     of deceptive optimisation problems.
      Another possibility might be to mix biologically and theoretically derived
     designs with evolution. For example, as a starting point take a model of the
     neocortex based on biological studies, such as those in the previous section,
     andthenapplyartiﬁcial evolution to modify and tune the model. Although we
     might not get the initial design right due to limitations in our understanding of
     the cortex, it is reasonable to suppose that in the space of all neural networks
     this initial design is relatively close to the correct design, or a related design
     that also works. Starting with individuals that are reasonably close to the
     target makes the optimisation problem that evolution has to solve orders of
     magnitude easier.
      Even if each of the above suggestions succeeded in reducing the diﬃculty of
     evolving intelligence by several orders of magnitude, perhaps the biggest prob-
     lem is still computer power. Advancing technology over the coming decades
     will help, but this could still be too little. Perhaps the closest we could come
     to nature’s planet scale evolution would be to construct a world wide net-
     work of machines donating computation time. After all, more than 2 million
     years of computer time have so far been donated to the Search for Extrater-
     restrial Intelligence (SETI), why not something similar to search for artiﬁcial
     intelligence?
     7.3. Is building intelligent machines a good idea?
     It is impossible to know whether any of the approaches discussed in the pre-
     vious section, or other approaches, will succeed in producing truly intelligent
     machines. But this is not the point we want to make: the point is that it is
     not obvious that they will all fail. This is important, because the impact of
     this event would be huge. Following the ﬁrst credible demonstration of true
     general intelligence in a machine, for sure a much larger and more powerful
     machine will be constructed shortly thereafter. This leads to what I. J. Good
     referred to as an intelligence explosion:
                                      135
            7. Discussion
                  “Let an ultraintelligent machine be deﬁned as a machine that can
                  far surpass all the intellectual activities of any man however clever.
                  Since the design of machines is one of these intellectual activi-
                  ties, an ultraintelligent machine could design even better machines;
                  there would then unquestionably be an ‘intelligence explosion,’ and
                  the intelligence of man would be left far behind. Thus the ﬁrst
                  ultraintelligent machine is the last invention that man need ever
                  make.” (Good, 1965)
               Thedeﬁningcharacteristic of our species is intelligence. It is not by superior
            size, strength or speed that we dominate life on earth, but by our intelligence.
            If our intelligence were to be signiﬁcantly surpassed, it is diﬃcult to imagine
            what the consequences of this might be. It would certainly be a source of
            enormous power, and with enormous power comes enormous responsibility.
               Machine intelligence could bring unprecedented wealth and opportunity if
            used constructively and safely. Alternatively, it could bring about some kind
            of a nightmare scenario. The latter possibility is certainly well known, being
            a staple of science ﬁction.   Positive ﬁctional depictions are rare, probably
            because casting the machines as villains is a convenient plot device. Outside
            of works of ﬁction, however, the implications of powerful machine intelligence
            are rarely encountered. Indeed, the whole subject of truly intelligent machines
            is generally avoided by academics, as noted at the start of this chapter.
               If one accepts that the impact of truly intelligent machines is likely to be
            profound, and that there is at least a small probability of this happening in
            the foreseeable future, it is only prudent to try to prepare for this in advance.
            If we wait until it seems very likely that intelligent machines will soon appear,
            it will be too late to thoroughly discuss and contemplate the issues involved.
            Historically technology has advanced in leaps and bounds, while social and
            ethical considerations have developed more slowly, often only as a reaction to
            the problems created by a technology after it arrived. Even what now seem to
            be obvious moral principles, such as gender and racial equality, were debated
            for centuries and are still not accepted in many parts of the world. Given that
            the implications of powerful machine intelligence are likely to be complex, we
            cannot expect to ﬁnd good answers quickly. We need to be seriously working
            on these things now.
               A small but growing number of forward thinking individuals and organ-
            isations are thinking about these issues. Perhaps the premier organisation
            dedicated to the safe and beneﬁcial development of powerful artiﬁcial intelli-
            gence is the Singularity Institute for Artiﬁcial Intelligence (SIAI). In 2007 SIAI
            organised a conference that attracted well known speakers including Rodney
            Brooks (director of the computer science and AI laboratory at MIT), Bar-
            ney Pell (AI researcher and CEO of Powerset), Wendell Wallach (bioethics
            lecturer at Yale), Sam Adams (IBM distinguished engineer), Paul Saﬀo (lec-
            turer at Sanford), Peter Norvig (director of research at Google), Peter Thiel
            (founder of Clarium Capital and co-founder of Paypal) and Ray Kurzweil (fu-
            136
                   7.3. Is building intelligent machines a good idea?
     turologist, inventor and entrepreneur). Although this event was one of the
     ﬁrst of its kind, the calibre of these speakers makes it clear that issues sur-
     rounding the development of advanced artiﬁcial intelligence are starting to be
     taken seriously. Other notable people who have spoken and written about the
     potential dangers of advanced artiﬁcial intelligence in recent years include Bill
     Joy (co-founder of Sun Microsystems), Nick Bostrom (director of the Future of
     Humanity Institute at Oxford), and Sir Martin Rees (professor at Cambridge
     and president of the Royal Society). Hopefully this trend will continue.
       At the SIAI itself the principle research fellow is Eliezer Yudkowsky. He
     has written a number of documents that deal mostly with safety and ethical
     issues surrounding the development of powerful artiﬁcial intelligence, as well as
     ideas on how he thinks so called ‘Friendly AI’ should be developed. Although
     these can be accessed through the SIAI website, none of his writings have yet
     appeared in mainstream peer reviewed journals. As AIXI is currently the only
     comprehensive mathematical theory of machine super intelligence, SIAI follows
     this work with interest and lists (Hutter, 2007b) among their core readings.
     PhD candidate Nick Hay, who is associated with SIAI, is examining whether
     AIXI theory can be used to study the safety of intelligent machines. Hopefully
     the gentle introduction to AIXI in Chapter 2 will encourage more people to
     explore some of these research directions.
       In 1887 Lord Acton famously wrote, “Power tends to corrupt, and absolute
     power corrupts absolutely.” It is not that power itself is inherently good or
     evil, rather it grants the ability to be so: power ampliﬁes intention. Although
     Acton’s quote has a ring of truth to it, perhaps it is excessively pessimistic
     about human nature. In any case, if there is ever to be something approaching
     absolute power, a super intelligent machine would come close. By deﬁnition,
     it would be capable of achieving a vast range of goals in a wide range of
     environments. If we carefully prepare for this possibility in advance, not only
     might we avert disaster, we might bring about an age of prosperity unlike
     anything seen before.
                                          137
         A. Notation and Conventions
         When deﬁning a symbol or equation we use := to stress that the item on the
         left is newly deﬁned. When we just need to assert that two things are equal
         we use the plain = symbol. Not equal is 6=, and approximately equal ≈. By
         x≫ywemeanthatxismuchgreater than y, and similarly for ≪.
           The cardinality of a set S is written |S|. The empty set is written ∅, subset
         with the possbility of equality ⊆, and proper subset ⊂. The natural numbers
         are denoted N := {1,2,...}, naturals with 0 included N := {0,1,2,3,...},
                                                         0
         the integers Z := {... − 2,−1,0,1,2,...}, the rational numbers Q := {n :
                                                                       m
         n,m ∈ Z}, and the real numbers R. We use standard notation for intervals
         on the real line. Speciﬁcally, we deﬁne [x,y] := {z ∈ R : x ≤ z ≤ y}, and
         (x,y) := {z ∈ R : x < z < y}. Intervals such as (0,1] have the obvious
         meaning.
           loga x is the logarithm of x base a. lnx := logex where e = 2.71828....
         When the speciﬁc base used makes no diﬀerence we simply write logx. The
         factorial of n, written n!, is deﬁned 0! = 1 and n! := n(n − 1)(n − 2)···1 for
                                       
                                      n       n!             0
         n ∈ N. The binomial is written r := (n−r)!r!. Although 0 is technically
         indeterminate, in derivations we follow the standard convention and take its
         value to be 1. The Kronecker delta symbol δab is deﬁned to be 1 if a = b, and
         0 otherwise.
           Analphabet is a ﬁnite set of elements which are called symbols. For example,
         {a,b,c,...,z} is an alphabet, as is {up,down,left,right}. Mostly we use the
         binary alphabet B := {0,1}, in which context the symbols are known as bits. A
         binary string is a ﬁnite ordered n-tuple of bits. This is denoted x := x1x2 ...xn
         where ∀i ∈ {1,...,n} : x ∈ B, or more succinctly, x ∈ Bn. The 0-tuple is
                              i
         denoted ǫ and is called the null string. The expression B≤n represents the set
         of binary strings of length n or less, and B∗ := Sn∈N Bn is the set of all binary
         strings. A substring of a string x is itself a string deﬁned xj:k := xjxj+1 ...xk
         where 1 ≤ j ≤ k ≤ n. Concatenation is indicated by juxtaposition, for
         example, if x = x1x2 ∈ B2 and y = y1y2y3 ∈ B3, then xy = x1x2y1y2y3. In
         some cases we will also concatenate constants, for example if x ∈ B∗ then x101
         is the string x with 101 added on the end. By ℓ(x) we mean the length of the
         string x, for example, if x ∈ Bn then ℓ(xx) = 2ℓ(x) = 2n, and for k < n we
         have ℓ(xj:k) = k −j +1.
           Unlike strings which always have ﬁnite length, a binary sequence ω is an
         inﬁnite list of bits, for example ω := x1x2x3 ... ∈ B∞. For a sequence ω ∈ B∞
         wemightbeinterestedinthepredictionofthe(t+1)th bit, denoted ωt+1, given
         that we have so far observed only the ﬁnite initial string ω1:t ∈ Bt. Obviously
                                                                       139
             A. Notation and Conventions
             a string cannot be concatenated onto the end of a sequence as a sequence has
             no end, however a sequence can be concateded onto the end of a string.
               Our notation and usage of measure theory in this thesis is non-standard
             as it makes working with strings and sequences easier. Although the usage
             is explained in the text, for the mathematically inclined the relationship to
             standard measure theory is as follows: For a given sample space Ω, a probability
             measure ν is a type of [0,1] valued function deﬁned over a set of subsets of Ω,
             known as a σ-algebra. For prediction we need to measure the probability of
             sets of sequences that begin with a given string, thus we need a σ-algebra that
             contains these sets, the so called cylinder sets deﬁned, Γ := {xω : ω ∈ B∞}
                                                                       x
             for x ∈ B∗. Such a σ-algebra can easily be constructed by considering the
             smallest σ-algebra that contains the cylinder sets. In this thesis we do not
             need to be able to measure arbitrary sets within this σ-algebra and thus we
             adopt a simpliﬁed notation for measures by deﬁning ν(x) to be shorthand for
             ν(Γ ). In other words, ν(x) is the probability that a binary sequence sampled
                x
             according to the distribution ν begins with the string x ∈ B∗. This shorthand
             does not get us into trouble as it can be proven that measures deﬁned on the
             set B∗ correspond uniquely to measures deﬁned on the full σ-algebra (Calude,
             2002). As we are often interested in the probability that a string x ∈ B∗ follows
             a string y ∈ B∗ in a sequence sampled according to some distribution µ, we
             further deﬁne the shorthand notation µ(yx) := µ(yx)/µ(y).
               Probability distributions, measures and semi-measures are usually denoted
             by a lowercase greek letter, for example, µ,ν,̺. For an unnamed probabilty
             distribution over a random variable X, we write P(X). Eµ(X) is the expected
             value of X with respect to µ. When µ is the true distribution we can omit
             this from the notation for expectations.
               Throughout the thesis we refer to an agent, usually denoted by the condi-
             tional measure π, that is interacting with some kind of an environment, usually
             denoted by the conditional measure µ. This interaction occurs by having ac-
             tion symbols from the alphabet A being sent by the agent to the environment,
             and perception symbols from the alphabet X being sent back in the other di-
             rection. Each perception consists of an observation from the alphabet O and
             a reward from the alphabet R. When referring to the full measure deﬁned by
             π interacting with µ we use the symbol π.
                                                      µ
               To denote symbols being sent we use the lower case variable names a, o,
             r and x for actions, observations, rewards and perceptions respectively. We
             index these in the order in which they occur, thus a1 is the agent’s ﬁrst action,
             a2 is the second action and so on. The agent and the environment take turns at
             sending symbols, starting with the agent. This produces a history of actions,
             observations and rewards which can be written, a o r a o r a o r .... As
                                                                  1 1 1 2 2 2 3 3 3
             we refer to interaction histories a lot, we need to be able to represent these
             compactly. One trick that we use is to squeeze symbols together and then
             index them as blocks of symbols. Thus for the complete interaction history
             up to and including cycle t, we can write ax1:t := a1x1a2x2a3...atxt. For the
             history before cycle t we use ax   := ax     . Note that x := o r .
                                             <t      1:t−1              t     t t
             140
             Some of our results will have the property of holding within an additive
          constant that is independent of the variables in the expression. We indicate
          this by placing a small plus above the equality or inequality symbol. For
                       +                                                    +    +
          example, f < g means that ∃c ∈ R,∀x : f(x) < g(x) + c. If f < g < f, we
                   +
          write f = g. When using standard “big O” notation this is superﬂuous as
          expressions are already understood to hold within an independent constant,
          however we will sometimes still use it for consistency of notation. Similarly,
                       ×                                                     ×    ×
          we deﬁne f ≤ g to mean that ∃c ∈ R,∀x : f(x) ≤ c·g(x), and if f ≤ g ≤ f we
                   ×
          write f = g.
                                                                                     141
                B. Ergodic MDPs admit
                            self-optimising agents
                Anumberofresults similar to Theorem 3.4.5 exist, such as the proof that with
                probability 1 the Q-learning algorithm converges to optimal in an ergodic MDP
                environment (Watkins and Dayan, 1992). Unfortunately, in order to establish
                the necessary chain of results we need something a little diﬀerent: we require
                the convergence to hold for any history and with an eﬀective horizon that
                goes to inﬁnity. For a precise statement of the theorem, necessary technical
                conditions and proof, see Theorem 5.38 in (Hutter, 2005). Although Hutter’s
                proof is straight forward, it assumes a certain continuity condition on value
                functions based on estimated transition probabilities for ergodic MDPs. This
                is left as a (large!) exercise for the reader (Problem 5.12 in Hutter, 2005). In
                this appendix we follow the plan of attack suggested in Problem 5.12 to prove
                the missing continuity result, namely, Theorem B.4.3 that appears at the end
                of this section.
                B.1. Basic deﬁnitions
                For deﬁnitions of agents, environments, ergodic and many other things used,
                see Chapters 2 and 3. Rather than just talking about agents, as we do in the
                rest of this thesis, here we will use the slightly more reﬁned notion of a policy.
                Essentially, a policy is some rule or mode of operation that an agent has. For
                example, whennavigatingamazeanagent’spolicymightbetoalwaysturnleft
                at each intersection. The distinction is useful when we wish to consider agents
                which have multiple diﬀerent modes of operation. That is, an agent which
                follows some policy for a while and then, perhaps due to certain conditions
                such as a lack of success, switches to a diﬀerent policy.
                    The proofs in this section will make extensive use of results from linear
                algebra. The mathematical notation used is fairly standard: we will represent
                                                                                                                n×m
                real valued matrices with capital letters, for example A ∈ R                                           .  By A we
                                                                                                                                   ij
                meanthe single scalar element of A on the ith row and jth column. By A                                                 we
                                  th                                                                                              ∗j
                mean the j            column of A and similarly for Ai∗. We represent vectors with a
                                                                                   n
                bold lowercase variable, for example a ∈ R . Similar to the case for matrices,
                by a we mean the ith element of a. In some situations a matrix or vector may
                       i
                already have other indexes, in this case we place square brackets around it and
                then index so as to avoid confusion. For example, [b ] is the ith element of
                                                                                                     π i
                                                                                                                                     143
              B. Ergodic MDPs admit self-optimising agents
              the vector bπ. We represent the classical adjoint of a matrix A by adj(A) and
              the determinant by det(A).
                In order to express the Markov chains as matrices, we need to be able
              to index the actions and perceptions with natural numbers. This could be
              achieved by a simple numbering scheme; here we will just assume that this
              has already been done. That is, without loss of generality we assume that
              X := {1,...,n } and A := {1,...,n } for n ,n ∈ N. The diﬃculty with this
                             1                       2        1  2
              is that each perception x is still associated with an observation o and a reward
              r. To recover the reward associated with x we write r(x) ∈ R, and similarly
              for o(x) ∈ O. Both R and O are ﬁnite, as always, but otherwise unspeciﬁed.
                Let π be a stationary policy such that ∀ax<kak : π(ax<kak) = π(o(x)k−1ak).
              That is, under the policy π the distribution of actions depends on only the last
              observation in a way that is independent of k. It follows that the equation for
              the kth perception x given history ax        is,
                                    k                   <k
                          π(ax<kak)µ(ax<kaxk) = π(o(xk−1)ak)µ(o(xk−1)axk).
              Thus, for a given µ and π the next perception xk depends on only the previous
              observation o(x)k−1 in a way that is independent of k, that is, π and µ together
              form a stationary Markov chain. Given that everything is stationary, we can
              drop the index k and write x, a and x′ for the perception, action and the
              following perception.
                FromthedeﬁnitionofanMDPitisclearthatwecanrepresenta(stationary)
                                                                      n ×n ×n                    ′
              MDPasathreedimensional Cartesian tensor D ∈ R 1              2   1 deﬁned ∀x,a,x ,
                                            Dxax′ := µ(o(x)ax′).
              Note that the only part of x that plays a part in deﬁning the structure of D is
              the associated observation o(x), as required by our deﬁnition of an MDP. The
              reward associated with x, that is r(x), has no role.
                Wecan now express the interaction of the policy and the environment as a
                                                n ×n
              square stochastic matrix T ∈ R 1       1 deﬁned,
                          Txx′ := Xπ(o(x)a)µ(o(x)ax′) = Xπ(o(x)a)Dxax′.                      (B.1)
                                   a∈A                           a∈A
                It should be noted that this characterisation of the interaction between µ
              and π as a stochastic matrix T is only possible if µ is a stationary MDP and
              π is a stationary policy. Fortunately this is all we will need for optimality,
              though we will brieﬂy have to consider non-stationary policies in Section B.3
              in order to prove this. It is worth keeping in mind that when we see a matrix T
              this represents the complete system of an agent and environment interacting,
              rather than just an environment. That is, it represents the interaction measure
              π.
              µ
                                             n ×n
                Deﬁne a matrix Rxa ∈ R 1          2 to be the expected reward when choosing
                                                                                        n
              action a after perception x. Further deﬁne the column vector rπ ∈ R 1 where,
                                    [rπ]x := E(Rxa|x) = Xπ(xa)Rxa.
                                                             a∈A
              144
                                                       B.1. Basic deﬁnitions
           The advantage of expressing everything in matrix notation is that the full
         range of linear algebra techniques is now easy to work with. For example, the
         probability of transiting between any two perceptions can be easily computed
         by taking powers of T: if we have a perception i then the probability that
         exactly m cycles later the perception will be j, is given by [Tm] .
                                                              ij
         B.1.1 Deﬁnition.   For an environment µ and a policy π the expected
         average value in cycles k to m given history ax<k is deﬁned to be,
                                                      m
                      Vπµ(ax<k) := 1 X π(ax<kax     )Xr(xi).
                       km          m     µ       k:m
                                     axk:m           i=k
         Additionally we deﬁne the expected long run average value to be,
                            Vπµ(ax  ) := lim Vπµ(ax  ),
                             k∞   <k    m→∞ km     <k
         when this limit exists.
         When k = 1 there is no history, that is, ax<k = ǫ, the null string. In this
         case we simplify the notation slightly by deﬁning Vπµ := Vπµ(ǫ). Similarly
             πµ                                      1m     1m
         for V1∞.
           In matrix notation we can express the expected long run average value for
         each initial perception x1 ∈ X = {1,...,n1} as a vector of value functions,
                             Vπµ(1)                    !
                                1∞                 m−1
                      πµ         .              1 X k
                     V =          .      = lim         T   r ,       (B.2)
                      1∞         .        m→∞m            π
                              Vπµ(n )               k=0
                                1∞ 1
         if the limit exists.
         B.1.2 Deﬁnition. For an environment µ the optimal policy, denoted πµ,
         is deﬁned as:
                                 πµ := argmaxVπµ,
                                           π   1∞
         where the maximum is taken over all policies, including non-stationary ones.
           In some sense the optimal policy is the ideal policy. However, the optimal
         policy is usually only optimal with respect to the speciﬁc environment for which
         it was deﬁned. If we do not know the speciﬁc details of the environment that
         the policy will face in advance, the best we can do is to have a policy which will
         adapt to the environment based on experience. In such a situation the policy
         is unlikely to be optimal as it will probably make some non-optimal actions as
         it learns about the environment it faces. In this situation the following concept
         is useful:
                                                                       145
             B. Ergodic MDPs admit self-optimising agents
             B.1.3 Deﬁnition. We say that a policy π is self-optimising in an environ-
             mentµifits expected average value converges to the optimal expected average
             value as m → ∞, that is,         πµ         πµµ
                                            V1m −→ V1m .
               Intuitively this means that the expected performance of the policy in the
             long run is as good as an optimal policy which was designed with complete
             knowledge of the environment in advance. Classes of environments which
             admit self-optimising policies are important because they are environments in
             which it is possible for general purpose policies to adapt their behaviour until
             eventually their actions become optimal.
             B.2. Analysis of stationary Markov chains
             In this section we will establish some of the properties of Markov chains that
             we will require. Our ﬁrst lemma shows that the key term (I − αT)−1 can be
             expanded using a Taylor series. The proof of this lemma and the following
             lemmaandtheoremarebased on the proof of Proposition 1.1 from Section 4.1
             of (Bertsekas, 1995).
                                                             n×n
             B.2.1 Lemma. For a stochastic matrix T ∈ R           and scalar α ∈ (0,1) there
                                        ∗     n×n             n×n
             exist stochastic matrices T  ∈R       and H ∈ R       such that
                              (I −αT)−1 = (1−α)−1T∗+H +O(|1−α|)
             where limα→1O(|1−α|) = 0.
             Proof. Deﬁne the n×n matrix,
                                      M(α):=(1−α)(I−αT)−1.
             Applying the matrix inversion formula we see that
                                      M(α)=(1−α)adj(I−αT),
                                                      det(I −αT)
                                                          th
             where the determinant det(I − αT) is an n       order polynomial in α and the
                                                                     th
             classical adjoint adj(I −αT) is an n×n matrix of n−1      order polynomials in
             α. Therefore M(α) can be expressed as an n×n matrix where each element is
             either zero or a fraction of two polynomials in α that have no common factors.
               We know that the denominator polynomials of M(α) cannot have 1 as a
             root as this would imply that the corresponding element of M(α) → ∞ as
             α→1. This cannot happen because,
                                   (1 −α)−1M(α)rπ = (I −αT)−1rπ
             146
                                              B.2. Analysis of stationary Markov chains
                                            −1               −1           
           where ∀i ∈ {1,...,n} :  (I − αT)     rπ i ≤ (1 − α)    maxk[rπ]k. Clearly
                                                                                        
           then the absolute value of the elements of M(α)rπ are bounded by maxk [rπ]k
           for α < 1. Therefore we can express the ijth element of M(α) as,
                                 Mij(α) = γ(α−φ1)···(α−φp)
                                             (α−ψ1)···(α−ψq)
           where γ,φi,ψj ∈ R for all i ∈ {1,...p} and j ∈ {1,...q}.
             Using this expression we can take a Taylor expansion of M(α) about 1 as
                                               ∗     n×n
           follows. Firstly, deﬁne the matrix T  ∈R      as,
                                          T∗ := lim M(α)
                                                α→1
                                 n×n
           and the matrix H ∈ R       as
                                                          
                                               ∂          
                                     H := −       M (α)]     .                     (B.3)
                                       ij           ij
                                               ∂α         α=1
           That is, H is a matrix having as it ijth element the ﬁrst derivative of −M (α)
                                                                                    ij
           with respect to α evaluated at α = 1.
             From the equation for a ﬁrst order Taylor expansion,
                               M(α)=T∗+(1−α)H+O((1−α)2)
           where O((1−α)2) is an α-dependent matrix such that
                                            O((1−α)2)
                                       lim               =0.
                                       α→1     1−α
           Dividing through by (1−α) we get
                         (1 −α)−1M(α)=(1−α)−1T∗+H+O(|1−α|)
           where limα→1O(|1−α|) = 0. The result then follows as (I − αT)−1 = (1 −
           α)−1M(α) by deﬁnition.                                                      2
             We will soon show that T∗ as deﬁned above plays a signiﬁcant role in the
           analysis. Before looking at this more closely, we will ﬁrstly prove some useful
           identities.
           B.2.2 Lemma. It follows from the deﬁnitions of T∗ and M(α) that,
                                     T∗ =T∗T =TT∗ =T∗T∗
           and for k ∈ N                      ∗ k     k    ∗
                                       (T −T ) =T −T .
                                                                                      147
           B. Ergodic MDPs admit self-optimising agents
           Proof.  By subtracting the identity αI = α(I − αT)(I − αT)−1 from the
           identity I = (I − αT)(I −αT)−1 we see that,
                           (1 −α)I = (I −αT)(1−α)(I −αT)−1
           and thus,
                    αT(1−α)(I −αT)−1 =(1−α)(I −αT)−1+(α−1)I.
           Taking α → 1 gives,
             lim αT · lim(1−α)(I −αT)−1 = lim(1−α)(I −αT)−1 + lim(α−1)I
             α→1     α→1                  α→1                   α→1
           which, using the deﬁnition of M(α), becomes,
                                T · lim M(α) = lim M(α).
                                   α→1        α→1
           Finally using the deﬁnition of T∗ this reduces to just,
                                       TT∗ =T∗.
           Using essentially the same argument it can also be shown that T∗T = T∗. It
           then immediately follows that ∀k ∈ N : TkT∗ = T∗Tk = T∗.
             From the relation TT∗ = T∗ it follows that T∗ −αTT∗ = T∗ −αT∗ and so
           (I −αT)T∗ =(1−α)T∗ and thus,
                                T∗ =(1−α)(I −αT)−1T∗.
           Taking α → ∞ gives,
                          lim T∗ = lim(1−α)(I −αT)−1 · lim T∗,
                          α→1     α→1                  α→1
           which by the deﬁnition of T∗ is just,
                                      T∗ =T∗T∗.
           This establishes the ﬁrst result.
             Thesecondresult will be proven by induction. Trivially (T−T∗)1 = T1−T∗
           which establishes the case k = 1. Now assume that the induction hypothesis
           holds for the kth case and consider the (k + 1)th case:
                       (T −T∗)k+1 = (T −T∗)k(T −T∗)
                                   = (Tk−T∗)(T −T∗)
                                   = Tk+1−TkT∗−T∗Tk+T∗T∗
                                   = Tk+1−T∗.
           The second line follows from the induction assumption and the ﬁnal line from
           the results above.                                             2
             Wewillusethesesimplerelationsfrequentlyintheproofsthatfollowwithout
           further comment. Now we can prove a key result about the structure of T∗:
           it is the limiting average distribution for the matrix T.
           148
                                         B.2. Analysis of stationary Markov chains
                                                      n×n
         B.2.3 Theorem. For a stochastic matrix T ∈ R     and ∀m∈N,
                                     m−1
                             T∗ = 1 XTk+ 1(Tm−I)H.
                                   mk=0       m
                     n×n
         where H ∈ R     is the matrix that satisﬁes Lemma B.2.1.
         Proof.    As T is a stochastic matrix, from Lemma B.2.1 we see that there
                            n×n       ∗    n×n
         exist matrices H ∈ R   and T ∈R       such that,
                        H=(I−αT)−1−(1−α)−1T∗−O(|1−α|)                     (B.4)
         where α ∈ (0,1) and limα→1O(|1−α|) = 0.
           However from the geometric series equations it follows that,
                                         ∞             ∞       ∞
           (I −αT)−1 −(1−α)−1T∗ = XαkTk−T∗Xαk=Xαk(Tk−T∗)
                                         k=0          k=0     k=0
                                                  ∞
                                     = I−T∗+X(α(T−T∗))k
                                                 k=1
                                     = I−T∗+ α(T−T∗)
                                                 I −α(T −T∗)
                                     = (I−α(T −T∗))−1−T∗.
         Substituting this result into Equation (B.4) and taking α → 1,
                     H = lim(I−α(T −T∗))−1−T∗−O(|1−α|)
                            α→1
                         = (I−T+T∗)−1−T∗.
         Multiplying by (I −T +T∗) and then T∗ we see that,
                      (I −T +T∗)H = I−(I−T +T∗)T∗
                     H−TH−T∗H = I−T∗+TT∗−T∗T∗=I−T∗
                  T∗H−T∗H−T∗H = T∗−T∗
                                T∗H = 0.
         It now also follows that H −TH = I −T∗ and so T∗ +H = I +TH.
           Multiplying by Tk on the left for k ∈ N now gives,
                                               0
                                T∗+TkH=Tk+Tk+1H.
         Summing over k = 0,1,...,m−1 and cancelling equal terms and dividing
         through by m produces,
                               m−1           m−1      m−1
                        mT∗+XTkH = XTk+XTk+1H
                               k=0           k=0      k=0
                                             m−1
                              mT∗+H = XTk+TmH
                                             k=0
                                                                            149
            B. Ergodic MDPs admit self-optimising agents
            from which the result follows as m 6= 0.                             2
              This establishes bounds on the convergence of 1 Pm−1Tk to T∗ that we
                                                          m k=0
            will need. As H is bounded, simply taking m → ∞ yields the following:
                                                          n×n
            B.2.4 Corollary. For a stochastic matrix T ∈ R   ,
                                                   m−1
                                     T∗ = lim 1 XTk.
                                           m→∞mk=0
              By applying this result to Equation (B.2) we can now express the expected
            long run average value very simply in terms of T∗,
                                               m−1   !
                             Vπµ :=     lim  1 XTk rπ=T∗rπ.                    (B.5)
                               1∞      m→∞m
                                               k=0
            Thus by the existence of T∗ we can infer that the expected long run average
            value also exists in this case.
            B.2.5 Corollary. Let µ be a stationary MDP environment and π a stationary
            policy. ∀m ∈ N,                         1
                                    |V πµ − V πµ| = O     .
                                      1∞    1m        m
                              n×n
            Proof. Let T ∈ R      represent the Markov chain formed by the interaction
            of µ and π. From Theorem B.2.3 we see that ∀m ∈ N,
                                        m−1
                                T∗− 1 XTk= 1(Tm−I)H.
                                      mk=0        m
            Multiplying by rπ on the right gives
                                    m−1 !
                           T∗rπ −   1 XTk rπ= 1 (Tm−I)Hrπ,
                                    mk=0            m
            and thus the result follows as the elements of both Tm and H are bounded. 2
              Of course this result is not surprising as we would expect the expected
            average value to converge to its limit in a reasonable way when both the
            environment and policy are stationary.
              Finally let us note some technical results on the relationship between T and
            T∗.
                                                                 n×n
            B.2.6 Lemma. For an ergodic stochastic matrix T ∈ R      the row vectors
            of T∗ are all the same and deﬁne a stationary distribution under T.
            150
                                              B.2. Analysis of stationary Markov chains
             This is a standard result in the theory of ergodic Markov chains. See for ex-
           ample Chapter V of (Doob, 1953) or any book on discrete stochastic processes
           for a proof.
             The following result shows that the limiting matrix T∗ is in some sense
           continuous with respect to small changes in T. This will be important because
           it means that if we have an estimate of T that converges in the limit then our
           estimate of T∗ will also converge.
                                                                          n×n
           B.2.7 Theorem. For an ergodic stochastic matrix T ∈ R               the matrix
            ∗      n×n                                                   ˆ      n×n
           T ∈ R        is continuous in T in the following sense: If T ∈ R          is a
                                               ˆ 
           stochastic matrix where maxij Tij −Tij is small, then ∃cT > 0 which depends
                                   ∗    ˆ∗                  ˆ 
           on T, such that maxij Tij −Tij ≤ cT maxij Tij −Tij.
                                                        n×n                      ∗
           Proof. For an ergodic square matrix T ∈ R         the row vectors of T  are all
                                                                               ∗     1×n
           the same and correspond to the stationary distribution row vector t ∈ R       .
           That is,
                                                  t∗ 
                                            ∗     . 
                                           T =       .                              (B.6)
                                                  . 
                                                    t∗
                  ∗       ∗                                       1×n             ∗     ∗
           where t T = t and for all distribution vectors t ∈ R       we have tT = t .
                                               ∗
           ThusT hasaneigenvalue of 1 with t being the corresponding left eigenvector.
                                                        n×n
             From linear algebra we know that ∀T ∈ R        ,
                                 adj(I −T)(I −T) = det(I −T)I.
           However as T has an eigenvalue of 1, det(I −T) = 0 and thus,
                                    adj(I −T)T = adj(I −T),
           or equivalently, ∀i ∈ {1,...,n},
                                 [adj(I − T)]i∗T = [adj(I − T)]i∗.
                      ∗
           Because {t } is a basis for the eigenspace corresponding to the eigenvalue 1,
           [adj(I − T)]i∗ must be in this eigenspace. That is, ∀i ∈ {1,...,n}, ∃ci ∈ R:
                      [adj(I − T)]i∗ =  cof1i(I − T),...,cofni(I − T) = cit∗
           where the cofactor is deﬁned cof (I −T) := (−1)j+idet(minor (I −T)).
                                            ji                             ji
             As T has an eigenvalue of 1 with geometric multiplicity 1 it follows that
           I − T has an eigenvalue of 0 also with geometric multiplicity 1. Thus the
           nullity of I − T is 1 and so rank(I − T) = n − 1. While we deﬁne the rank
           of a matrix to be the dimension of its column or row space, it also can be
           deﬁned as the size of the largest non-zero minor and the two deﬁntions can be
           proven to be equivalent. As the adjoint is composed of order n − 1 minors it
                                                                                      151
            B. Ergodic MDPs admit self-optimising agents
            immediately follows that adj(I − T) 6= 0 and thus ∃k, which depends on T,
            such that ck > 0.
              Asminorjk(I−T)isan(n−1)×(n−1)sub-matrixof(I−T)thedeterminant
            of this is an order n−1 polynomial in the elements of T. Thus, by the continuity
            of polynomials, ∃c′ > 0 such that for a suﬃciently small ε > 0 change in any
            element of T we will get at most a c′ε change in each cofjk(I − T). However
            we know that t∗ = 1 [adj(I − T)] , and so an ε change in the elements of
                                ck            k∗
            T results in at most a c′ ε change in the elements of t∗ and thus T∗. Deﬁne
                                   ck
            c  := c′ to indicate that this constant depends on T and we are done.    2
             T    ck
            B.3. An optimal stationary policy
            We now turn our attention to optimal policies. While our analysis so far
            has only dealt with stationary policies, in general optimal policies need not
            be stationary. As non-stationary policies are more diﬃcult to analyse our
            preference is to deal with only stationary policies if possible. In this section
            we prove that for the class of ergodic ﬁnite stationary MDP environments an
            optimal policy can indeed be chosen so that it is stationary. This will simplify
            our analysis in later sections. However, in order to show this result we will
            need to brieﬂy consider policies which are potentially non-stationary. The
            proofs in this section follow those of Section 4.2 in (Bertsekas, 1995).
              Let us assume that the policy π is deterministic but not necessarily station-
            ary, that is, π := {π ,π ,...}. Thus in the kth cycle we apply π . Deﬁne
                                 1  2                                         k
            pi(x) := argmaxy∈Aπi(xa) to be the action chosen by policy π in cycle i.
            Clearly this is unique for deterministic π.
              In order to make some of the equations that follow more manageable we
            need to deﬁne the following two mappings. For any function f : X → R
            and deterministic policy π := {π1,π2,...} we deﬁne the mapping Bπk for any
            k ∈ N to be ∀x ∈ X,
                           (Bπkf)(x) := Rxpk(x) + X µ(xpk(x)x′)f(x′).
                                                  x′∈X
            Of interest will be the policy that simply selects the action which maximises
            this expression in each cycle for any given x. For this we deﬁne for any function
            f : X → R and ∀x ∈ X,
                            (Bf)(x) := max"Rxa + X µ(xax′)f(x′)#.
                                        a∈A         x′∈X
            Clearly this policy is stationary as the maximising a depends only on x and is
            independent of which cycle the system is in. By (B2f)(x) we mean (B(Bf))(x)
            and similarly higher powers such as (Bif)(x) and (Bi f)(x). The equation
                                                                 πk
            Bf =f is the well known Bellman equation (Bellman, 1957).
            152
                                                                                     B.3. An optimal stationary policy
                    An elementary property of the mappings Bπk and B is their monotonicity
                in the following sense.
                B.3.1 Lemma. For any f,f′ : X → R such that ∀x ∈ X : f(x) ≤ f′(x) and
                for any possibly non-stationary deterministic policy π := {π1,π2,...}, we have
                ∀x∈X, ∀i,k ∈N,                                 i                    i    ′
                                                          (Bπkf)(x) ≤ (Bπkf )(x)
                and
                                                            (Bif)(x) ≤ (Bif′)(x).
                Proof.           Clearly the cases B1 and B1 are true from their deﬁnitions. A
                                                                              πk
                simple induction argument establishes the general result.                                                              2
                                                                                     t        n×1
                    Deﬁne the column vector e := (1,...,1) ∈ R                                      .   Using these mappings
                we can now prove that the optimal policy can be chosen stationary if certain
                conditions hold.
                B.3.2 Theorem. Let µ be a ﬁnite stationary MDP environment. If λ ∈ R is
                                              n×1
                a scalar and h ∈ R                   a column vector such that ∀x ∈ X,
                                            λ+[h]x =max"Rxa+ X µ(xax′)[h]x′#                                                      (B.7)
                                                              a∈A                 x′∈X
                or equivalently,
                                                                  λe+h=Bh,
                then
                                                            λ=V∗µ :=maxVπµ.
                                                                     1∞           π      1∞
                Furthermore, if a stationary policy πµ attains the maximum in Equation (B.7)
                                                                                            πµµ
                for each x then this policy is optimal, that is, V1∞ = λ.
                                                                               n×1
                Proof.           We have λ ∈ R and h ∈ R                              such that for any (possibly non-
                stationary) policy π = {π1,π2,...} and cycle m ∈ N and ∀xm ∈ X,
                            λ+[h]xm ≥Rxmpm(xm)+ X µ(xmpm(xm)xm+1)[h]xm+1.
                                                                     xm+1∈X
                Furthermore, if πm attains the maximum in Equation (B.7) for each xm ∈ X
                                                               th
                then equality holds in the m                        cycle and pm(xm) is optimal for this single
                cycle. The main idea of this proof is to extend this result so that we get a
                policy which is optimal across all cycles.
                    Using the mapping Bπm we can express the above equation more compactly
                as,
                                                                λe+h≥Bπmh.
                                                                                                                                     153
            B. Ergodic MDPs admit self-optimising agents
            Applying now Bπm−1 to both sides and using the monotonicity property from
            Lemma B.3.1 we see that,
                                  λe+Bπm−1h≥Bπm−1Bπmh.
            However we also know that λe+h ≥ Bπm−1h and so it follows that,
                                     2λe+h≥Bπm−1Bπmh.
            Repeating this m times we get
                                  mλe+h≥Bπ1Bπ2···Bπmh,
            where equality continues to hold in the case where πk attains the maximum
            in Equation (B.7) in each cycle k ∈ {1,...,m}. When this is the case we see
            that π is optimal for the cycles 1 to m.
              From the deﬁnition of Bπk we see that,
                                            (           m                )
                                                       X           
                    [Bπ1Bπ2 ···Bπmh]x1 = E [h]xm+1 +      Rxkpk(xk)x1,π,µ
                                                       k=1         
            is the total expected reward over m cycles from the initial perception x1 to
            the ﬁnal perception xm+1 under policy π and environment µ. Thus ∀x1 ∈ X,
                                       (           m                 )
                                                  X            
                        mλ+[h]x1 ≥E [h]xm+1 +         Rxkpk(xk)x1,π,µ
                                                  k=1          
            where equality holds if πk attains the maximum in Equation (B.7) in each
            cycle.
              Dividing by m, gives ∀x1 ∈ X,
                                                      (m                   )
                 1         1                 	    1    X           
             λ+ [h]x1 ≥      E [h]xm+1|x1,π,µ +     E       Rxkpk(xk)x1,π,µ  . (B.8)
                 m         m                      m     k=1         
            Taking m → ∞ this reduces to ∀x1 ∈ X,
                                           (m                  )
                                       1     X           
                             λ≥ lim      E      Rxkpk(xk)x1,π,µ  ,
            or equivalently,     m→∞m        k=1         
                                            λ≥Vπµ,
                                                 1∞
            where equality holds if πk attains the maximum in Equation (B.7) for each
            cycle. When this is the case, π is optimal and thus Vπµ = max Vπµ = λ.
                                                                1∞        π 1∞
            Furthermore, we see that this optimal policy is stationary because in Equation
            (B.7) the action a only depends on the current perception x and is independent
            of the cycle number. We call this optimal stationary policy πµ.       2
            154
                                                          B.4. Convergence of expected average value
                 The above result only guarantees the existence of an optimal stationary
              policy πµ for a stationary MDP environment µ in the case where there is
              a solution to the Bellman equation λe + h = Bh. Fortunately for ergodic
              MDPs it can be shown that such a solution always exists (our deﬁnition of
              ergodicity implies condition (2) of Proposition 2.6 in (Bertsekas, 1995) where
              the existence of a solution is proven). It now follows that:
              B.3.3 Theorem. For any ergodic ﬁnite stationary MDP environment µ there
              exists an optimal stationary policy πµ.
                 This is a useful result because the interaction between a stationary MDP
              environment and a stationary policy is much simpler to analyse than the non-
              stationary case. We will refer back to this result a number of times when we
              need to assert the existence of an optimal stationary policy.
                 One thing that we have not shown is that the optimal policy with respect
              to a given MDP can be computed. Given that our MDP is ﬁnite and therefore
              the number of possible stationary deterministic policies is also ﬁnite we might
              expect that this problem should be solvable. Indeed, it can be shown that the
              Policy Iteration algorithm is able to compute an optimal stationary policy in
              this situation (see Section 4.3 of Bertsekas, 1995).
              B.4. Convergence of expected average value
              Our goal is to ﬁnd a good policy for an unknown stationary MDP µ. Because
              we do not know the structure of the MDP, that is µ, we create an estimate
              µˆ and then ﬁnd the optimal policy with respect to this estimate, which we
              will call πµˆ. Our hope is that if our estimate µˆ is suﬃciently close to µ, then
              πµˆ will perform well compared to the true optimal policy πµ. Speciﬁcally we
                              πµˆµ       πµµ
              would like V1∞ −V1∞  = 0.
                 In the analysis that follows we will need to be careful about whether we are
              talking about the true environment µ, our estimate of this µˆ, or various combi-
              nations of environments interacting with various policies. Sometimes policies
              will be optimal with respect to the environment that they are interacting with,
              sometimes they will only be optimal with respect to an estimate of the envi-
              ronment that they are actually interacting with, and in some cases the policy
              may be arbitrary. Needless to say that care is required to avoid mixing things
              up.
                                                           X×A×X
                 As deﬁned previously, let D ∈ R                      represent the chronological system
                        ˆ       X×A×X
              µ and D ∈ R                  the chronological system µˆ. From Equation (B.1) we
                                                   X×X
              know that the matrix T ∈ R                  representing the Markov chain formed by a
              policy π interacting with µ is deﬁned by,
                                                 Txx′ := Xπ(xa)Dxax′.
                                                           a∈A
                                                                                                               155
                           B. Ergodic MDPs admit self-optimising agents
                                                                               ˆ                              ˆ                                                     ˆ
                           Wecansimilarly deﬁne T from π and D. It now follows that if D is close to D,
                                                                                          ′              ′       ˆ         ′
                           in the sense that ε := maxxax |Dxax −Dxax | is small, then for any stationary
                                                                                                                   ˆ
                           policy π the associated matrices T and T are close:
                                                                                                               X                                                         
                                                                                                                                                                           
                                                                             ˆ                                                                              ˆ           
                                                  maxTxx′ −Txx′ = max                                                    π(xa) Dxax′ −Dxax′ 
                                                    xx′                                                 xx′ a∈A                                                            
                                                                                                                 X                       
                                                                                                                                         
                                                                                             ≤ max                         π(xa)ε = ε.
                                                                                                         x a∈A                           
                           This is important as it means that we can take bounds on the accuracy of our
                           estimate of the true MDP and imply from this bounds on the accuracy of the
                                                ˆ
                           estimate T for any stationary policy.
                                For any given stationary policy this bound also carries over to the associated
                           expected long run average value functions in a straightforward way:
                           B.4.1 Lemma. For stationary ﬁnite MDPs such that ε := maxxax′ |Dxax′ −
                            ˆ         ′
                           Dxax | it follows that for any stationary policy π,
                                                                                                                  
                                                                                         Vπµ −Vπµˆ = O(ε).
                                                                         ˆ                1∞                1∞                                                        ˆ
                           Proof.               Let T and T be the Markov chains deﬁned by D and D interacting
                           with a stationary policy π. By the argument above we see that maxxx′ |Txx′ −
                            ˆ ′
                           Txx | ≤ ε. Thus by Theorem B.2.7 we know that there exists cT such that
                                         ′      ∗           ˆ∗
                           maxxx |Txx′ −Txx′| ≤ cTε, where cT depends on T. By Equation (B.5) we see
                           that,
                                                                                                                                  
                                                                           πµ             πµˆ                ∗        ˆ∗          
                                                                      V1∞ −V1∞ = (T −T )rπ = O(ε).                                                                                   (B.9)
                                                                                                                                                                                               2
                                From this lemma we can show that the optimal policies with respect to µ
                           and µˆ are bounded:
                           B.4.2 Theorem. ForastationaryﬁniteMDPsuchthatε := maxxax′ |Dxax′−
                            ˆ         ′
                           Dxax | it follows that,                                           µ                 µˆ   
                                                                                      Vπ µ −Vπ µˆ = O(ε),
                                            µ                µˆ                        1∞                   1∞ 
                           where π and π are optimal policies that are also stationary.
                           Proof.                 For any two functions f,f′ : D → R such that ∀x ∈ D : |f(x) −
                           f′(x)| ≤ δ it follows that |maxx∈D f(x) − maxx′∈D f′(x′)| ≤ δ. From Lemma
                           B.4.1 it then follows that,
                                                                                                                         ′   
                                                                              maxVπµ−maxVπµˆ=O(ε)
                                                           ′                       π        1∞             π′         1∞ 
                           where π and π belong to the set of stationary policies. However by Theorem
                           B.3.3 we know that the optimal policies for µ and µˆ can be chosen stationary
                           and thus the result follows.                                                                                                                                        2
                           156
                                                                                                B.4. Convergence of expected average value
                           We now have all the necessary results to show that if µˆ is a good estimate
                      of µ then our policy πµˆ that is based on µˆ will perform near optimally with
                      respect to the true environment in the limit.
                      B.4.3 Theorem. Let µ and µˆ be two ergodic stationary ﬁnite MDP environ-
                                                                                                                                       ′              ′        ˆ         ′
                      ments that are close in the sense that ε := maxxax |Dxax − Dxax | is small.
                      It can be shown that for m ∈ N,
                                                                              µˆ               µ                 1
                                                                      Vπ µ −Vπ µ = O                                           +O(ε)
                                       µ                               1m                   1m                       m                                        µˆ
                      where π is an optimal policy for the true distribution µ, and π                                                                               is an optimal
                      policy with respect to the estimate of the true distribution µˆ.
                      Proof.
                           From Theorem B.3.3 we see that πµˆ can be chosen stationary. From the
                      triangle inequality and the results of Corollary B.2.5 (with π   πµˆ), Lemma
                      B.4.1 (with π   πµˆ) and Theorem B.4.2 we see that,
                                   µˆ                µ                          µˆ                µˆ                µˆ               µˆ                µˆ                µ 
                            Vπ µ −Vπ µ = Vπ µ−Vπ µ+Vπ µ−Vπ µˆ+Vπ µˆ−Vπ µ
                             1m                   1∞                      1m                  1∞                1∞                1∞                1∞               1∞ 
                                                                                  µˆ                µˆ                 µˆ                µˆ                 µˆ                µ 
                                                                  ≤ Vπ µ−Vπ µ+Vπ µ−Vπ µˆ+Vπ µˆ−Vπ µ
                                                                            1m                  1∞               1∞                  1∞               1∞                  1∞ 
                                                                  = O1+O(ε).
                                                                                    m
                      Now from Corollary B.2.5 (with π   πµ) and the triangle inequality again,
                                  µˆ                µ                          µˆ                µ                µ                 µ 
                           Vπ µ −Vπ µ = Vπ µ−Vπ µ+Vπ µ−Vπ µ
                            1m                  1m                       1m                  1∞                1∞                1m                       
                                                                                 µˆ                µ                  µ                µ                      1
                                                                 ≤ Vπ µ−Vπ µ+Vπ µ−Vπ µ=O                                                                               +O(ε).
                                                                           1m                  1∞               1∞                  1m                       m                         2
                                                                                                                                                                                         157
     C. Deﬁnitions of Intelligence
        “Viewed narrowly, there seem to be almost as many deﬁnitions of
        intelligence as there were experts asked to deﬁne it.” R. J. Stern-
        berg quoted in (Gregory, 1998)
      Despite a long history of research and debate, there is still no standard
     deﬁnition of intelligence. This has lead some to believe that intelligence may
     be approximately described, but cannot be fully deﬁned. We believe that
     this degree of pessimism is too strong. Although there is no single standard
     deﬁnition, if one surveys the many deﬁnitions that have been proposed, strong
     similarities between many of the deﬁnitions quickly become obvious.
      Here we take the opportunity to present the many informal deﬁnitions that
     we have collected over the years. Naturally, compiling a complete list would
     be impossible as many deﬁnitions of intelligence are buried deep inside articles
     and books. Nevertheless, the 70 odd deﬁnitions presented below are, to the
     best of our knowledge, the largest and most well referenced collection there is.
     C.1. Collective deﬁnitions
     In this section we present deﬁnitions that have been proposed by groups or
     organisations. In many cases deﬁnitions of intelligence given in encyclopedias
     have been either contributed by an individual psychologist or quote an earlier
     deﬁnition given by a psychologist. In these cases we have chosen to attribute
     the quote to the psychologist, and have placed it in the next section. In this
     section we only list those deﬁnitions that either cannot be attributed to speciﬁc
     individuals, or represent a collective deﬁnition agreed upon by many individ-
     uals. As many dictionaries source their deﬁnitions from other dictionaries, we
     have endeavoured to always list the original source.
      1. “The ability to use memory, knowledge, experience, understanding, rea-
        soning, imagination and judgement in order to solve problems and adapt
        to new situations.” AllWords Dictionary, 2006
      2. “The capacity to acquire and apply knowledge.” The American Heritage
        Dictionary, fourth edition, 2000
      3. “Individuals diﬀer from one another in their ability to understand com-
        plex ideas, to adapt eﬀectively to the environment, to learn from expe-
        rience, to engage in various forms of reasoning, to overcome obstacles
                                       159
      C. Deﬁnitions of Intelligence
        by taking thought.” American Psychological Association (Neisser et al.,
        1996)
       4. “The ability to learn, understand and make judgments or have opinions
        that are based on reason” Cambridge AdvanceLearner’sDictionary, 2006
       5. “Intelligence is a very general mental capability that, among other things,
        involves the ability to reason, plan, solve problems, think abstractly,
        comprehend complex ideas, learn quickly and learn from experience.”
        Common statement with 52 expert signatories (Gottfredson, 1997a)
       6. “Theability to learn facts and skills and apply them, especially when this
        ability is highly developed.” Encarta World English Dictionary, 2006
       7. “...ability to adapt eﬀectively to the environment, either by making a
        change in oneself or by changing the environment or ﬁnding a new one
        . . . intelligence is not a single mental process, but rather a combination
        of many mental processes directed toward eﬀective adaptation to the
        environment.” Encyclopedia Britannica, 2006
       8. “the general mental ability involved in calculating, reasoning, perceiving
        relationships and analogies, learning quickly, storing and retrieving infor-
        mation, using language ﬂuently, classifying, generalizing, and adjusting
        to new situations.” Columbia Encyclopedia, sixth edition, 2006
       9. “Capacity for learning, reasoning, understanding, and similar forms of
        mental activity; aptitude in grasping truths, relationships, facts, mean-
        ings, etc.” Random House Unabridged Dictionary, 2006
       10. “The ability to learn, understand, and think about things.” Longman
        Dictionary or Contemporary English, 2006
       11. “: the ability to learn or understand or to deal with new or trying situa-
        tions : ... the skilled use of reason (2) : the ability to apply knowledge
        to manipulate one’s environment or to think abstractly as measured by
        objective criteria (as tests)” Merriam-Webster Online Dictionary, 2006
       12. “The ability to acquire and apply knowledge and skills.” Compact Ox-
        ford English Dictionary, 2006
       13. “...the ability to adapt to the environment.” World Book Encyclopedia,
        2006
       14. “Intelligence is a property of mind that encompasses many related mental
        abilities, such as the capacities to reason, plan, solve problems, think
        abstractly, comprehend ideas and language, and learn.” Wikipedia, 4
        October, 2006
      160
                           C.2. Psychologist deﬁnitions
      15. “Capacity of mind, especially to understand principles, truths, facts or
       meanings, acquire knowledge, and apply it to practise; the ability to
       learn and comprehend.” Wiktionary, 4 October, 2006
      16. “The ability to learn and understand or to deal with problems.” Word
       Central Student Dictionary, 2006
      17. “The ability to comprehend; to understand and proﬁt from experience.”
       Wordnet 2.1, 2006
      18. “The capacity to learn, reason, and understand.” Wordsmyth Dictio-
       nary, 2006
     C.2. Psychologist deﬁnitions
     This section contains deﬁnitions from psychologists. In some cases we have
     not yet managed to locate the exact reference and would appreciate any help
     in doing so.
      1. “Intelligence is not a single, unitary ability, but rather a composite of sev-
       eral functions. The term denotes that combination of abilities required
       for survival and advancement within a particular culture.” Anastasi
       (1992)
      2. “...that facet of mind underlying our capacity to think, to solve novel
       problems, to reason and to have knowledge of the world.” Anderson
       (2006)
      3. “It seems to us that in intelligence there is a fundamental faculty, the
       alteration or the lack of which, is of the utmost importance for practi-
       cal life. This faculty is judgement, otherwise called good sense, practi-
       cal sense, initiative, the faculty of adapting ones self to circumstances.”
       Binet and Simon (1905)
      4. “We shall use the term ‘intelligence’ to mean the ability of an organism
       to solve new problems ...” Bingham (1937)
      5. “Intelligence is what is measured by intelligence tests.” Boring (1923)
      6. “...a quality that is intellectual and not emotional or moral: in measur-
       ing it we try to rule out the eﬀects of the child’s zeal, interest, industry,
       and the like. Secondly, it denotes a general capacity, a capacity that
       enters into everything the child says or does or thinks; any want of ‘in-
       telligence’ will therefore be revealed to some degree in almost all that he
       attempts;” Burt (1957)
                                      161
      C. Deﬁnitions of Intelligence
       7. “A person possesses intelligence insofar as he has learned, or can learn,
        to adjust himself to his environment.” S. S. Colvin quoted in (Sternberg,
        2000)
       8. “...the ability to plan and structure one’s behavior with an end in view.”
        J. P. Das
       9. “The capacity to learn or to proﬁt by experience.” W. F. Dearborn
        quoted in (Sternberg, 2000)
       10. “...in its lowest terms intelligence is present where the individual an-
        imal, or human being, is aware, however dimly, of the relevance of his
        behaviour to an objective. Many deﬁnitions of what is indeﬁnable have
        been attempted by psychologists, of which the least unsatisfactory are 1.
        the capacity to meet novel situations, or to learn to do so, by new adap-
        tive responses and 2. the ability to perform tests or tasks, involving the
        grasping of relationships, the degree of intelligence being proportional to
        the complexity, or the abstractness, or both, of the relationship.” Drever
        (1952)
       11. “Intelligence A: the biological substrate of mental ability, the brain’s
        neuroanatomy and physiology; Intelligence B: the manifestation of in-
        telligence A, and everything that inﬂuences its expression in real life
        behavior; Intelligence C: the level of performance on psychometric tests
        of cognitive ability.” H. J. Eysenck.
       12. “Sensory capacity, capacity for perceptual recognition, quickness, range
        or ﬂexibility or association, facility and imagination, span of attention,
        quickness or alertness in response.” F. N. Freeman quoted in (Sternberg,
        2000)
       13. “...adjustment or adaptation of the individual to his total environment,
        or limited aspects thereof ...the capacity to reorganize one’s behavior
        patterns so as to act more eﬀectively and more appropriately in novel
        situations ...the ability to learn ...the extent to which a person is edu-
        cable ...the ability to carry on abstract thinking ...the eﬀective use of
        concepts and symbols in dealing with a problem to be solved ...” W.
        Freeman
       14. “An intelligence is the ability to solve problems, or to create products,
        that are valued within one or more cultural settings.” Gardner (1993)
       15. “...performing an operation on a speciﬁc type of content to produce a
        particular product.” J. P. Guilford
       16. “Sensation, perception, association, memory, imagination, discrimina-
        tion, judgement and reasoning.” N. E. Haggerty quoted in (Sternberg,
        2000)
      162
                           C.2. Psychologist deﬁnitions
      17. “Thecapacityforknowledge, andknowledgepossessed.” Henmon(1921)
      18. “...cognitive ability.” Herrnstein and Murray (1996)
      19. “...the resultant of the process of acquiring, storing in memory, retriev-
       ing, combining, comparing, and using in new contexts information and
       conceptual skills.” Humphreys
      20. “Intelligence is the ability to learn, exercise judgment, and be imagina-
       tive.” J. Huarte
      21. “Intelligence is a general factor that runs through all types of perfor-
       mance.” A. Jensen
      22. “Intelligence is assimilation to the extent that it incorporates all the given
       data of experience within its framework ...There can be no doubt either,
       that mental life is also accommodation to the environment. Assimilation
       can never be pure because by incorporating new elements into its earlier
       schematatheintelligence constantly modiﬁes the latter in order to adjust
       them to new elements.” Piaget (1963)
      23. “Ability to adapt oneself adequately to relatively new situations in life.”
       R. Pinter quoted in (Sternberg, 2000)
      24. “A biological mechanism by which the eﬀects of a complexity of stimuli
       are brought together and given a somewhat uniﬁed eﬀect in behavior.”
       J. Peterson quoted in (Sternberg, 2000)
      25. “...certain set of cognitive capacities that enable an individual to adapt
       and thrive in any given environment they ﬁnd themselves in, and those
       cognitive capacities include things like memory and retrieval, and prob-
       lem solving and so forth. There’s a cluster of cognitive abilities that lead
       to successful adaptation to a wide range of environments.” Simonton
       (2003)
      26. “Intelligence is part of the internal environment that shows through at
       the interface between person and external environment as a function of
       cognitive task demands.” R. E. Snow quoted in (Slatter, 2001)
      27. “...I prefer to refer to it as ‘successful intelligence.’ And the reason is
       that the emphasis is on the use of your intelligence to achieve success in
       your life. So I deﬁne it as your skill in achieving whatever it is you want
       to attain in your life within your sociocultural context — meaning that
       people have diﬀerent goals for themselves, and for some it’s to get very
       goodgradesinschoolandtodowellontests,andforothersitmightbeto
       become a very good basketball player or actress or musician.” Sternberg
       (2003)
                                      163
            C. Deﬁnitions of Intelligence
             28. “...the ability to undertake activities that are characterized by (1) diﬃ-
                 culty, (2) complexity, (3) abstractness, (4) economy, (5) adaptedness to
                 goal, (6) social value, and (7) the emergence of originals, and to maintain
                 such activities under conditions that demand a concentration of energy
                 and a resistance to emotional forces.” Stoddard
             29. “The ability to carry on abstract thinking.”   L. M. Terman quoted
                 in (Sternberg, 2000)
             30. “Intelligence, considered as a mental trait, is the capacity to make im-
                 pulses focal at their early, unﬁnished stage of formation. Intelligence is
                 therefore the capacity for abstraction, which is an inhibitory process.”
                 Thurstone (1924)
             31. “The capacity to inhibit an instinctive adjustment, the capacity to rede-
                 ﬁnetheinhibited instinctive adjustment in the light of imaginally experi-
                 enced trial and error, and the capacity to realise the modiﬁed instinctive
                 adjustment in overt behavior to the advantage of the individual as a
                 social animal.” L. L. Thurstone quoted in (Sternberg, 2000)
             32. “Aglobalconceptthatinvolvesanindividual’sabilitytoactpurposefully,
                 think rationally, and deal eﬀectively with the environment.” Wechsler
                 (1958)
             33. “The capacity to acquire capacity.” H. Woodrow quoted in (Sternberg,
                 2000)
             34. “...the term intelligence designates a complexly interrelated assemblage
                 of functions, no one of which is completely or accurately known in man
                 . . . ” Yerkes and Yerkes (1929)
             35. “...that faculty of mind by which order is perceived in a situation previ-
                 ously considered disordered.” R. W. Young quoted in (Kurzweil, 2000)
            C.3. AI researcher deﬁnitions
            This section lists deﬁnitions from researchers in artiﬁcial intelligence.
               1. “...the ability of a system to act appropriately in an uncertain environ-
                 ment, where appropriate action is that which increases the probability
                 of success, and success is the achievement of behavioral subgoals that
                 support the system’s ultimate goal.” Albus (1991)
               2. “Any system ...that generates adaptive behviour to meet goals in a
                 range of environments can be said to be intelligent.” Fogel (1995)
               3. “Achieving complex goals in complex environments.” Goertzel (2006)
            164
                          C.3. AI researcher deﬁnitions
      4. “Intelligent systems are expected to work, and work well, in many dif-
       ferent environments. Their property of intelligence allows them to max-
       imize the probability of success even if full knowledge of the situation
       is not available. Functioning of intelligent systems cannot be considered
       separately from the environment and the concrete situation including
       the goal.” Gudwin (2000)
      5. “[Performance intelligence is] the successful (i.e., goal-achieving) perfor-
       mance of the system in a complicated environment.” Horst (2002)
      6. “Intelligence is the ability to use optimally limited resources – including
       time – to achieve goals.” Kurzweil (2000)
      7. “Intelligence is the power to rapidly ﬁnd an adequate solution in what
       appears a priori (to observers) to be an immense search space.” Lenat
       and Feigenbaum (1991)
      8. “Intelligence measures an agent’s ability to achieve goals in a wide range
       of environments.” Legg and Hutter (2006)
      9. “...doing well at a broad range of tasks is an empirical deﬁnition of
       ‘intelligence’ ” Masum et al. (2002)
      10. “Intelligence is the computational part of the ability to achieve goals in
       the world. Varying kinds and degrees of intelligence occur in people,
       many animals and some machines.” McCarthy (2004)
      11. “...the ability to solve hard problems.” Minsky (1985)
      12. “Intelligence is the ability to process information properly in a complex
       environment. Thecriteria of properness are not predeﬁned and hence not
       available beforehand. They are acquired as a result of the information
       processing.” Nakashima (1999)
      13. “...in any real situation behavior appropriate to the ends of the system
       and adaptive to the demands of the environment can occur, within some
       limits of speed and complexity.” Newell and Simon (1976)
      14. “[An intelligent agent does what] is appropriate for its circumstances
       and its goal, it is ﬂexible to changing environments and changing goals,
       it learns from experience, and it makes appropriate choices given per-
       ceptual limitations and ﬁnite computation.” Poole et al. (1998)
      15. “Intelligence means getting better over time.” Schank (1991)
      16. “...the essential, domain-independent skills necessary for acquiring a
       wide range of domain-speciﬁc knowledge – the ability to learn anything.
       Achievingthiswith‘artiﬁcial general intelligence’ (AGI) requires a highly
                                      165
      C. Deﬁnitions of Intelligence
        adaptive, general-purpose system that can autonomously acquire an ex-
        tremely wide range of speciﬁc knowledge and skills and can improve its
        own cognitive ability through self-directed learning.” (Voss, 2005)
       17. “Intelligence is the ability for an information processing system to adapt
        to its environment with insuﬃcient knowledge and resources.” Wang
        (1995)
       18. “...the mental ability to sustain successful life.” K. Warwick quoted
        in (Asohan, 2003)
      166
     Bibliography
     Abeles, M. (1991). Corticonics: Neural circuits of the cerebral cortex. Cam-
      bridge University Press.
     Albus, J. S. (1991). Outline for a theory of intelligence. IEEE Trans. Systems,
      Man and Cybernetics, 21, 473–509.
     Alvarado, N., Adams, S., Burbeck, S., and Latta, C. (2002). Beyond the
      Turing test: Performance metrics for evaluating a computer simulation of
      the human mind. Performance Metrics for Intelligent Systems Workshop.
      Gaithersburg, MD, USA: North-Holland.
     Ananthanarayanan, R., and Modha, D. S. (2007). Anatomy of a cortical sim-
      ulator. ACM/IEEE SC2007 Conference on High Performance Networking
      and Computing. Reno, NV, USA.
     Anastasi, A. (1992). What counselors should know about the use and inter-
      pretation of psychological tests. Journal of Counseling and Development,
      70, 610–615.
     Anderson, M. (2006). Intelligence. MS Encarta online encyclopedia.
     Asohan, A. (2003). Leading humanity forward. The Star.
     Barzdin, J. M. (1972). Prognostication of automata and functions. Information
      Processing, 71, 81–84.
     Bell, T. C., Cleary, J. G., and Witten, I. H. (1990). Text compression. Prentice
      Hall.
     Bellman, R. (1957). Dynamic programming. New Jersey: Princeton University
      Press.
     Berry, D. A., and Fristedt, B. (1985). Bandit problems: Sequential allocation
      of experiments. London: Chapman and Hall.
     Bertsekas, D. P. (1995). Dynamic programming and optimal control, vol. (ii).
      Belmont, Massachusetts: Athena Scientiﬁc.
     Binet, A. (1911). Les idees modernes sur les enfants. Paris: Flammarion.
     Binet, A., and Simon, T. (1905). Methodes nouvelles por le diagnostic du
      niveai intellectuel des anormaux. L’Ann´ee Psychologique, 11, 191–244.
                                       167
      BIBLIOGRAPHY
      Bingham, W. V. (1937). Aptitudes and aptitude testing. New York: Harper &
       Brothers.
      Block, N. (1981). Psychologism and behaviorism. Philosophical Review, 90,
       5–43.
      Boring, E. G. (1923). Intelligence as the tests test it. New Republic, 35, 35–37.
      Boyan, J. A. (1999). Least-squares temporal diﬀerence learning. Proc. 16th
       International Conf. on Machine Learning (pp. 49–56). Morgan Kaufmann,
       San Francisco, CA.
      Bradtke, S. J., and Barto, A. G. (1996). Linear least-squares algorithms for
       temporal diﬀerence learning. Machine Learning, 22, 33–57.
      Bringsjord, S., and Schimanski, B. (2003). What is artiﬁcial intelligence?
       Psychometric AI as an answer. Eighteenth International Joint Conference
       on Artiﬁcial Intelligence, 18, 887–893.
      Burt, C. L. (1957). The causes and treatments of backwardness. University of
       London press.
      Calude, C. S. (2002). Information and randomness. Berlin: Springer. 2nd
       edition.
      Calude, C. S., Ju¨rgensen, H., and Legg, S. (2000). Solving ﬁnitely refutable
       mathematical problems. In C. S. Calude and G. Puaun (Eds.), Finite ver-
       sus inﬁnite: Contributions to an eternal dilemma, 39–52. Springer-Verlag,
       London.
      Carroll, J. B. (1993). Human cognitive abilities: A survey of factor-analytic
       studies. New York: Cambridge University Press.
      Cattell, R. B. (1987). Intelligence: Its structure, growth, and action. New
       York: Elsevier.
      Chaitin, G. J. (1982). G¨odel’s theorem and information. International Journal
       of Theoretical Physics, 22, 941–954.
      Cilibrasi, R., and Vit´anyi, P. M. B. (2005). Clustering by compression. IEEE
       Trans. Information Theory, 51, 1523–1545.
      Cleary, J. G., Legg, S., and Witten, I. H. (1996). An MDL estimate of the
       signiﬁcance of rules. Information, Statistics and Induction in Science (pp.
       43–45).
      Creutzfeldt, O. D. (1977). Generality of the functional structure of the neo-
       cortex. Naturwissenschaften, 64, 507–517.
      168
                                                                    BIBLIOGRAPHY
          Crevier, D. (1993). AI: The tumultuous search for artiﬁcial intelligence. New
            York: BasicBooks.
          Dawid, A. P. (1985). Comment on The impossibility of inductive inference.
            Journal of the American Statistical Association, 80, 340–341.
          Dawkins, R. (2006). The god delusion. Bantam Books.
          Dayan, P. (1992). The convergence of TD(λ) for general λ. Machine learning,
            8, 341–362.
          Doob, J. L. (1953). Stochastic processes. New York: John Wiley & Sons.
          Dowe, D. L., and Hajek, A. R. (1998). A non-behavioural, computational
            extension to the Turing test. International Conference on Computational
            Intelligence & Multimedia Applications (ICCIMA ’98) (pp. 101–106). Gipp-
            sland, Australia.
          Drever, J. (1952).  A dictionary of psychology.   Harmondsworth: Penguin
            Books.
          Edmonds, B. (2006). The social embedding of intelligence – Towards produc-
            ing a machine that could pass the turing test. In The turing test sourcebook:
            Philosophical and methodological issues in the quest for the thinking com-
            puter. Kluwer.
          Eisner, J. (1991). Cognitive science and the search for intelligence. Invited
            paper presented to the Socratic Society, University of Cape Town.
          Feder, M., Merhav, N., and Gutman, M. (1992). Universal prediction of indi-
            vidual sequences. IEEE Trans. on Information Theory, 38, 1258–1270.
          Fi´evet, C. (2005).  Mesurer l’intelligence d’une machine.    Le Monde de
            l’intelligence (pp. 42–45). Paris: Mondeo publishing.
          Fogel, D. B. (1995). Review of computational intelligence: Imitating life. Proc.
            of the IEEE, 83.
          Ford, K. M., and Hayes, P. J. (1998). On computational wings: Rethinking
            the goals of artiﬁcial intelligence. Scientiﬁc American, Special Edition.
          French, R. M. (1990). Subcognition and the limits of the Turing test. Mind,
            99, 53–65.
          Fuster, J. M. (2003). Cortex and mind: Unifying cognition. Oxford University
            Press.
          Gardner, H. (1993). Frames of mind: Theory of multiple intelligences. Fontana
            Press.
                                                                                  169
      BIBLIOGRAPHY
      Gittins, J. C. (1989). Multi-armed bandit allocation indices. John Wiley &
       Sons.
              ¨
      G¨odel, K. (1931). Uber formal unentscheidbare S¨atze der Principia Mathemat-
       ica und verwandter Systeme I. Monatshefte fu¨r Matematik und Physik, 38,
       173–198. [English translation by E. Mendelsohn: “On undecidable proposi-
       tions of formal mathematical systems”. In M. Davis, editor, The undecidable,
       pages 39–71, New York, 1965. Raven Press, Hewlitt].
      Goertzel, B. (2006). The hidden pattern. Brown Walker Press.
      Gold, E. M. (1967). Language identiﬁcation in the limit. Information and
       Control, 10, 447–474.
      Goldberg, D. E., and Richardson, J. (1987). Genetic algorithms with sharing
       for multi-modal function optimization. Proc. 2nd International Conference
       on Genetic Algorithms and their Applications (pp. 41–49). Cambridge, MA:
       Lawrence Erlbaum Associates.
      Good, I. J. (1965). Speculations concerning the ﬁrst ultraintelligent machine.
       Advances in Computers, 6, 31–88.
      Gottfredson, L. S. (1997a). Mainstream science on intelligence: An editorial
       with 52 signatories, history, and bibliography. Intelligence, 24, 13–23.
      Gottfredson, L. S. (1997b). Why g matters: The complexity of everyday life.
       Intelligence, 24, 79–132.
      Gottfredson, L. S. (2002). g: Highly general and highly practical. The general
       factor of intelligence: How general is it? (pp. 331–380). Erlbaum.
      Gould, S. J. (1981). The mismeasure of man. W. W. Norton & Company.
      Graham-Rowe, D. (2005). Spotting the bots with brains. New Scientist mag-
       azine, 2512, 27.
      Graham-Rowe, D. (2007). A working brain model. Technology Review. 28
       November.
      Gregory, R. L. (1998). The Oxford companion to the mind. Oxford, UK:
       Oxford University Press.
      Gudwin, R. R. (2000). Evaluating intelligence: A computational semiotics
       perspective. IEEE International conference on systems, man and cybernetics
       (pp. 2080–2085). Nashville, Tenessee, USA.
      Guilford, J. P. (1967). The nature of human intelligence. New York: McGraw-
       Hill.
      170
                                                                                   BIBLIOGRAPHY
            Gunderson, K. (1971). Mentality and machines. Garden City, New York, USA:
               Doubleday and company.
            Harnad, S. (1989). Minds, machines and Searle. Journal of Theoretical and
               Experimental Artiﬁcial Intelligence, 1, 5–25.
            Havenstein, H. (2005). Spring comes to AI winter. Computer World. 14
               February.
            Hawkins, J., and Blakeslee, S. (2004). On intelligence. New York: Owl books.
            Henmon,V.A.C.(1921). Themeasurementofintelligence. School and Society,
               13, 151–158.
            Herman, L. M., and Pack, A. A. (1994). Animal intelligence: Historical per-
               spectives and contemporary approaches. In R. Sternberg (Ed.), Encyclopedia
               of human intelligence, 86–96. New York: Macmillan.
            Hern´andez-Orallo, J. (2000a).         Beyond the Turing test. Journal of Logic,
               Language and Information, 9, 447–466.
            Hern´andez-Orallo, J. (2000b). On the computational measurement of intelli-
               gence factors. Performance Metrics for Intelligent Systems Workshop (pp.
               1–8). Gaithersburg, MD, USA.
            Hern´andez-Orallo, J., and Minaya-Collado, N. (1998). A formal deﬁnition
               of intelligence based on an intensional variant of Kolmogorov complexity.
               Proceedings of the International Symposium of Engineering of Intelligent
               Systems (EIS’98) (pp. 146–163). ICSC Press.
            Herrnstein, R. J., and Murray, C. (1996). The bell curve: Intelligence and
               class structure in American life. Free Press.
            Horn, J. (1970). Organization of data on life-span development of human
               abilities.  Life-span developmental psychology: Research and theory. New
               York: Academic Press.
            Horst, J. (2002). A native intelligence metric for artiﬁcial systems. Perfor-
               mance Metrics for Intelligent Systems Workshop. Gaithersburg, MD, USA.
            Hsu, F. H., Campbell, M. S., and Hoane, A. J. (1995). Deep Blue system
               overview. Proceedings of the 1995 International Conference on Supercom-
               puting (pp. 240–244).
            Hutchens, J. L. (1996).             How to pass the Turing test by cheating.
               www.cs.umbc.edu/471/current/papers/hutchens.pdf.
            Hutter, M. (2001). Convergence and error bounds for universal prediction of
               nonbinary sequences. Proc. 12th Eurpean Conference on Machine Learning
               (ECML-2001), 239–250.
                                                                                                     171
            BIBLIOGRAPHY
            Hutter, M. (2002a). The fastest and shortest algorithm for all well-deﬁned
               problems. International Journal of Foundations of Computer Science, 13,
               431–443.
            Hutter, M. (2002b). Fitness uniform selection to preserve genetic diversity.
               Proc. 2002 Congress on Evolutionary Computation (CEC-2002) (pp. 783–
               788). Washington D.C, USA: IEEE.
            Hutter, M. (2005).       Universal artiﬁcial intelligence:   Sequential deci-
               sions based on algorithmic probability.     Berlin:  Springer. 300 pages,
               http://www.hutter1.net/ai/uaibook.htm.
            Hutter,    M. (2006).        The Human knowledge compression prize.
               http://prize.hutter1.net.
            Hutter, M. (2007a). On universal prediction and Bayesian conﬁrmation. The-
               oretical Computer Science, 384, 33–48.
            Hutter, M. (2007b).     Universal algorithmic intelligence:  A mathematical
               top→down approach. In Artiﬁcial general intelligence, 227–290. Berlin:
               Springer.
            Hutter, M., and Legg, S. (2006). Fitness uniform optimization. IEEE Trans-
               actions on Evolutionary Computation, 10, 568–589.
            Hutter, M., and Legg, S. (2007). Temporal diﬀerence updating without a
               learning rate. Neural Information Processing Systems (NIPS ’07).
            Hutter, M., Legg, S., and Vit´anyi, P. M. B. (2007). Algorithmic probability.
               Scholarpedia.
            Johnson, W. L. (1992). Needed: A new test of intelligence.         SIGARTN:
               SIGARTNewsletter (ACMSpecial Interest Group on Artiﬁcial Intelligence),
               3.
            Johnson-Laird, P. N., and Wason, P. C. (1977). A theoretical analysis of insight
               into a reasoning task. In P. N. Johnson-Laird and P. C. Wason (Eds.),
               Thinking: Readings in cognitive science, 143–157. Cambridge University
               Press.
            Jong, K. (1975). An analysis of the behavior of a class of genetic adaptive
               systems. Dissertation Abstracts International, 36.
            Kandel, E. R., Schwartz, J. H., and Jessell, T. M. (2000). Principles of neural
               science. New York: McGraw-Hill. 4nd edition.
            Kaufman, A. S. (2000). Tests of intelligence. In R. J. Sternberg (Ed.), Hand-
               book of intelligence. Cambridge University Press.
            172
                                                                     BIBLIOGRAPHY
          Koch, C. (1999). Biophysics of computation: Information processing in single
             neurons. New York: Oxford University Press.
          Koza, J. R., Keane, M. A., Streeter, M. J., Mydlowec, W., Yu, J., and Lanza,
             G. (2003). Genetic programming IV: Routine human-competitive machine
             intelligence. Kluwer Academic.
          Kurzweil, R. (2000). The age of spiritual machines: When computers exceed
             human intelligence. Penguin.
          Lagoudakis, M.G., andParr, R.(2003). Least-squarespolicyiteration. Journal
             of Machine Learning Research, 4, 1107–1149.
          Legg, S. (1996). Minimum information estimation of linear regression models.
             Information, Statistics and Induction in Science (pp. 103–111).
          Legg, S. (1997). Solomonoﬀ induction (Technical Report CDMTCS-030). Cen-
             tre of Discrete Mathematics and Theoretical Computer Science, University
             of Auckland.
          Legg, S. (2006a). Incompleteness and artiﬁcial intelligence. Collegium Logicum
             of the Kurt G¨odel Society. Vienna.
          Legg, S. (2006b). Is there an elegant universal theory of prediction?   Proc.
             Algorithmic Learning Theory (ALT 06). Barcelona, Spain.
          Legg, S., and Hutter, M. (2004). A taxonomy for abstract environments (Tech-
             nical Report IDSIA-20-04). IDSIA.
          Legg, S., and Hutter, M. (2005a). Fitness uniform deletion for robust optimiza-
             tion. Proc. Genetic and Evolutionary Computation Conference (GECCO’05)
             (pp. 1271–1278). Washington, OR: ACM SigEvo.
          Legg, S., and Hutter, M. (2005b). A universal measure of intelligence for arti-
             ﬁcial agents. Proc. 21st International Joint Conf. on Artiﬁcial Intelligence
             (IJCAI-2005) (pp. 1509–1510). Edinburgh.
          Legg, S., and Hutter, M. (2006). A formal measure of machine intelligence. An-
             nual Machine Learning Conference of Belgium and The Netherlands (Bene-
             learn’06) (pp. 73–80). Ghent.
          Legg, S., and Hutter, M. (2007a). A collection of deﬁnitions of intelligence.
             Advances in Artiﬁcial General Intelligence: Concepts, Architectures and
             Algorithms (pp. 17–24).   Amsterdam, NL: IOS Press.       Online version:
             www.vetta.org/shane/intelligence.html.
          Legg, S., and Hutter, M. (2007b). Tests of machine intelligence. 50 Years of
             Artiﬁcial Intelligence (pp. 232–242). Monte Verit`a, Switzerland.
                                                                                   173
            BIBLIOGRAPHY
            Legg, S., and Hutter, M. (2007c). Universal intelligence: A deﬁnition of ma-
              chine intelligence. Minds and Machines, 17, 391–444.
            Legg, S., Hutter, M., and Kumar, A. (2004). Tournament versus ﬁtness uni-
              formselection. Proc. 2004 Congress on Evolutionary Computation (CEC’04)
              (pp. 2144–2151). Portland, OR: IEEE.
            Legg, S., Poland, J., and Zeugmann, T. (2008). On the limits of learning with
              computational models. Knowledge Media Science. To appear.
            Lenat, D., and Feigenbaum, E. (1991). On the thresholds of knowledge. Arti-
              ﬁcial Intelligence, 47, 185–250.
            Levin, L. A. (1973). Universal sequential search problems. Problems of Infor-
              mation Transmission, 9, 265–266.
            Levin, L. A. (1974).  Laws of information conservation (non-growth) and
              aspects of the foundation of probability theory. Problems of Information
              Transmission, 10, 206–210.
            Li, M., and Vit´anyi, P. M. B. (1997). An introduction to Kolmogorov complex-
              ity and its applications. Springer. 2nd edition.
            Loebner, H. G. (1990).     The Loebner prize — The ﬁrst Turing test.
              http://www.loebner.net/Prizef/loebner-prize.html.
            Macphail, E. M. (1985). Vertebrate intelligence: The null hypothesis. In
              L. Weiskrantz (Ed.), Animal intelligence, 37–50. Oxford: Clarendon.
            Mahoney, M. V. (1999). Text compression as a test for artiﬁcial intelligence.
              AAAI/IAAI (p. 970).
            Markoﬀ, J., and Hansell, S. (2005). Hiding in plain sight, google seeks more
              power. New York Times. 14 June.
            Martin-L¨of, P. (1966). The deﬁnition of random sequences. Information and
              Control, 9, 602–619.
            Masum, H., Christensen, S., and Oppacher, F. (2002). The Turing ratio:
              Metrics for open-ended tasks. GECCO 2002: Proceedings of the Genetic and
              Evolutionary Computation Conference (pp. 973–980). New York: Morgan
              Kaufmann Publishers.
            McCarthy,     J.    (2004).         What     is   artiﬁcial   intelligence?
              www-formal.stanford.edu/jmc/whatisai/whatisai.html.
            Melchner, L., Pallas, S. L., and Sur, M. (2000). Visual behaviour mediated by
              retinal projections directed to the auditory pathway. Nature, 404, 871–876.
            Minsky, M. (1985). The society of mind. New York: Simon and Schuster.
            174
                               BIBLIOGRAPHY
     Moravec, H. (1998). When will computer hardware match the human brain?
      Journal of Transhumanism, 1.
     Mountcastle, V. B. (1978). An organizing principle for cerebral function: The
      unit model and the distributed system. In The mindful brain. MIT Press.
     Mu¨ller, M. (2006). Stationary algorithmic probability (Technical Report). TU
      Berlin, Berlin. http://arXiv.org/abs/cs/0608095.
     Nakashima, H. (1999). AI as complex information processing. Minds and
      machines, 9, 57–80.
     Neisser, U., Boodoo, G., Bouchard, Jr., T. J., Boykin, A. W., Brody, N., Ceci,
      S. J., Halpern, D. F., Loehlin, J. C., Perloﬀ, R., Sternberg, R. J., and Urbina,
      S. (1996). Intelligence: Knowns and unknowns. American Psychologist, 51,
      77–101.
     Newell, A., and Simon, H. A. (1976). Computer science as empirical enquiry:
      Symbols and search. Communications of the ACM 19, 3, 113–126.
     Peng, J. (1993). Eﬃcient dynamic programming-based learning for control.
      Doctoral dissertation, Northeastern University, Boston, MA.
     Peng, J., and Williams, R. J. (1996). Increamental multi-step Q-learning.
      Machine Learning, 22, 283–290.
     Piaget, J. (1963). The psychology of intelligence. New York: Routledge.
     Poland, J., and Hutter, M. (2004). Convergence of discrete MDL for sequential
      prediction. Proc. 17th Annual Conf. on Learning Theory (COLT’04) (pp.
      300–314). Banﬀ: Springer, Berlin.
     Poland, J., and Hutter, M. (2006). Universal learning of repeated matrix
      games. Proc. 15th Annual Machine Learning Conf. of Belgium and The
      Netherlands (Benelearn’06) (pp. 7–14). Ghent.
     Poole, D., Mackworth, A., and Goebel, R. (1998). Computational intelligence:
      Alogical approach. New York, NY, USA: Oxford University Press.
     Raven, J. (2000). The Raven’s progressive matrices: Change and stability over
      culture and time. Cognitive Psychology, 41, 1–48.
     Reznikova, Z. I. (2007). Animal intelligence. Cambridge University Press.
     Reznikova, Z. I., and Ryabko, B. (1986). Analysis of the language of ants by
      information-theoretic methods. Problems Inform. Transmission, 22, 245–
      249.
     Rissanen, J. J. (1996). Fisher Information and Stochastic Complexity. IEEE
      Trans. on Information Theory, 42, 40–47.
                                      175
             BIBLIOGRAPHY
             Rummery,G.A.(1995). Problem solving with reinforcement learning. Doctoral
                dissertation, Cambridge University.
             Rummery, G. A., and Niranjan, M. (1994). On-line Q-learning using connec-
                tionist systems. Technial Report CUED/F-INFENG/TR 166). Engineering
                Department, Cambridge University.
             Russell, S. J., and Norvig, P. (1995). Artiﬁcial intelligence. A modern ap-
                proach. Englewood Cliﬀs: Prentice-Hall.
             Sanghi, P., and Dowe, D. L. (2003). A computer program capable of passing
                I.Q. tests. Proc. 4th ICCS International Conference on Cognitive Science
                (ICCS’03) (pp. 570–575). Sydney, NSW, Australia.
             Saygin, A., Cicekli, I., and Akman, V. (2000). Turing test: 50 years later.
                Minds and Machines, 10.
             Schaeﬀer, J., Burch, N., Bj¨ornsson, Y., Kishimoto, A., Mu¨ller, M., Lake, R.,
                Lu, P., and Sutphen, S. (2007). Checkers is solved. Science. 19 July.
             Schank, R. (1991). Where’s the AI? AI magazine, 12, 38–49.
             Schmidhuber, J. (2002). The Speed Prior: a new simplicity measure yield-
                ing near-optimal computable predictions. Proc. 15th Annual Conference
                on Computational Learning Theory (COLT 2002) (pp. 216–228). Sydney,
                Australia: Springer.
             Schmidhuber, J. (2004). Optimal ordered problem solver. Machine Learning,
                54, 211–254.
             Schmidhuber, J. (2005). G¨odel machines: Self-referential universal problem
                solvers making provably optimal self-improvements. In Artiﬁcial general
                intelligence. Springer, in press.
             Schweizer, P. (1998). The truly total Turing test. Minds and Machines, 8,
                263–272.
             Searle, J. (1980). Minds, brains, and programs. Behavioral & Brain Sciences,
                3, 417–458.
             Shieber, S. (1994). Lessons from a restricted Turing test. CACM: Communi-
                cations of the ACM, 37.
             Simonton, D. K. (2003). An interview with Dr. Simonton. In J. A. Plucker
                (Ed.), Human intelligence:      Historical inﬂuences, current controversies,
                                                                ∼
                teaching resources. http://www.indiana.edu/ intell.
             Singer, E. (2007). A wiring diagram of the brain. Technology Review. 19
                November.
             176
                                                                       BIBLIOGRAPHY
           Slatter, J. (2001). Assessment of children: Cognitive applications. San Diego:
             Jermone M. Satler Publisher Inc. 4th edition.
           Slotnick, B. M., and Katz, H. M. (1974). Olfactory learning-set formation in
             rats. Science, 185, 796–798.
           Smith, T. C., Witten, I. H., Cleary, J. G., and Legg, S. (1994). Objective
             evaluation of inferred context-free grammars. Australian and New Zealand
             Conference on Intelligent Information Systems (pp. 393–396).
           Smith, W. D. (2006). Mathematical deﬁnition of “intelligence” (and conse-
             quences). http://math.temple.edu/∼wds/homepage/works.html.
           Solomonoﬀ, R. J. (1964). A formal theory of inductive inference: Part 1 and
             2. Inform. Control, 7, 1–22, 224–254.
           Solomonoﬀ, R. J. (1978). Complexity-based induction systems: comparisons
             and convergence theorems. IEEE Trans. Information Theory, IT-24, 422–
             432.
           Spearman, C. E. (1927). The abilities of man, their nature and measurement.
             New York: Macmillan.
           Stern, W. L. (1912).     Psychologischen Methoden der Intelligenz-Pru¨fung.
             Leipzig: Barth.
           Sternberg, R. J. (1985). Beyond IQ: A triarchic theory of human intelligence.
             New York: Cambridge University Press.
           Sternberg, R. J. (Ed.). (2000). Handbook of intelligence. Cambridge University
             Press.
           Sternberg, R. J. (2003). An interview with Dr. Sternberg. In J. A. Plucker
             (Ed.), Human intelligence:    Historical inﬂuences, current controversies,
                                                          ∼
             teaching resources. http://www.indiana.edu/ intell.
           Sternberg, R. J., and Berg, C. A. (1986). Quantitative integration: Deﬁnitions
             of intelligence: A comparison of the 1921 and 1986 symposia. What is
             intelligence? Contemporary wiewpoints on its nature and deﬁnition (pp.
             155–162). Norwood, NJ: Ablex.
           Sternberg, R. J., and Grigorenko, E. L. (Eds.). (2002). Dynamic testing: The
             nature and measurement of learning potential. Cambridge University Press.
           Sutton, R., and Barto, A. (1998). Reinforcement learning: An introduction.
             Cambridge, MA, MIT Press.
           Sutton, R. S. (1988). Learning to predict by the methods of temporal diﬀer-
             ences. Machine Learning, 3, 9–44.
                                                                                      177
      BIBLIOGRAPHY
      Terman, L. M., and Merrill, M. A. (1950). The Stanford-Binet intelligence
       scale. Boston: Houghton Miﬄin.
      Tesauro, G. (1995). Temporal diﬀerence learning and TD-Gammon. Commu-
       nications of the ACM, 39, 58–68.
      Thurstone, L. L. (1924). The nature of intelligence. London: Routledge.
      Thurstone, L. L. (1938). Primary mental abilities. Chicago: University of
       Chicago Press.
      Treister-Goren, A., Dunietz, J., and Hutchens, J. L. (2000). The developmen-
       tal approach to evaluating artiﬁcial intelligence – a proposal. Performance
       Metrics for Intelligence Systems.
      Treister-Goren, A., and Hutchens, J. L. (2001). Creating AI: A unique inter-
       play between the development of learning algorithms and their education.
       Proceeding of the First International Workshop on Epigenetic Robotics.
      Turing, A. M. (1950). Computing machinery and intelligence. Mind.
      Voss, P. (2005). Essentials of general intelligence: The direct path to AGI.
       Artiﬁcial General Intelligence. Springer-Verlag.
      V’yugin, V. V. (1998). Non-stochastic inﬁnite and ﬁnite sequences. Theoretical
       computer science, 207, 363–382.
      Wallace, C. S. (2005). Statistical and inductive inference by minimum message
       length. Berlin: Springer.
      Wallace, C. S., and Boulton, D. M. (1968). An information measure for clas-
       siﬁcation. Computer Jrnl., 11, 185–194.
      Wang, P. (1995). On the working deﬁnition of intelligence (Technical Re-
       port 94). Center for Research on Concepts and Cognition, Indiana Univer-
       sity.
      Watkins, C. (1989). Learning from delayed rewards. Doctoral dissertation,
       King’s College, Oxford.
      Watkins, C. J. C. H., and Dayan, P. (1992). Q-learning. Machine Learning,
       8, 279–292.
      Watt, S. (1996). Naive psychology and the inverted Turing test. Psycoloquy,
       7.
      Wechsler, D. (1958). The measurement and appraisal of adult intelligence.
       Baltimore: Williams & Wilkinds. 4 edition.
      178
                               BIBLIOGRAPHY
     Willems, F., Shtarkov, Y., and Tjalkens, T. (1995). The context-tree weighting
      method: Basic properties. IEEE Transactions on Information Theory, 41,
      653–664.
     Witten, I. H. (1977). An adaptive optimal controller for discrete-time Markov
      environments. Information and Control, 34, 286–295.
     Wolpert, D. H., and Macready, W. G. (1997). No free lunch theorems for
      optimization. IEEE Trans. on Evolutionary Computation, 1, 67–82.
     Yerkes, R. M., and Yerkes, A. W. (1929). The great apes: A study of anthropoid
      life. New Haven: Yale University Press.
     Zentall, T. R. (1997). Animal memory: The role of instructions. Learning and
      Motivation, 28, 248–267.
     Zentall, T. R. (2000). Animal intelligence. In R. J. Sternberg (Ed.), Handbook
      of intelligence. Cambridge University Press.
     Zvonkin, A. K., and Levin, L. A. (1970). The complexity of ﬁnite objects and
      the development of the concepts of information and randomness by means
      of the theory of algorithms. Russian Mathematical Surveys, 25, 83–124.
                                      179
           Index
           abstract thinking, 6                       Bingham, W. V., 5
           action space, 41                           Block, N., 16
           actions, 40, 72                            Blockhead argument, 89
           Acton, Lord, 137                           BlueBrain project, 132
           Adams, S., 136                             BlueGene supercomputer, 132
           Adaptive Intelligence Inc., 19             Boring, E. G., 9
           agent                                      brain, 128
               active, 39                             brain simulation, 130
               AIXI, 48, 82, 93, 109, 137             Bringsjord, S., 19
               choose own goals, 92                   Brooks, R., 136
               formal, 42
               human, 83                              C-Test, 20
               informal, 40                           Carroll, J. B., 4
               optimal, 45                            Cattell, R. B., 4, 10
               passive, 39                            Chaitin, G., 20, 105
               random, 78                             checkers, 127
               self-optimising, 50, 64                chess, 2, 40, 126
               simple, 80                             Chinese room argument, 89
               specialised, 79                        coenumerable function, 33
               super intelligent, 82                  Colvin, 5
           agent-environment model, 39                communicating class, 63
           AI winter, 125                             competitive games, 19
           Albus, J. S., 9                            complexity
           aliens, 135                                     Kolmogorov, 20, 31, 34, 98
           alphabet, 41                                    Levin, 20, 93
           Anastasi, A., 7                                 of environments, 76
           aperiodic, 63                                   of prediction, 102
           Artiﬁcial   general     intelligence            time, 21, 104
                    (AGI), 126                        computable binary sequence, 97
           axons, 127                                 computable function, 33
                                                      consciousness, 90, 127
           backgammon, 126                            control theory, 41, 73
           Bayes’ rule, 25                            cortex, 131
           Bayes, T., 25                              creativity, 90
           Bellman equation, 46
           Binet, A., 4, 13                           Dearborn, 5
                                                                                      181
             Index
             diversity control, 134                       freewill, 90
             Dowe, D., 17
             Drever, J., 7                                g-factor, 4, 14
             dumb luck, 75                                G¨odel incompleteness, 105, 129
                                                          G¨odel machine, 129
             eligibility trace, 111                       Galton, F., 13
             emotion, 90                                  Gardner, H., 3
             empirical   future  discounted re-           genetic algorithm, 133
                       ward, 112                          geometric temporal discounting, 45
             enumerable function, 33                      Goertzel, B., 10
             environment, 6, 40                           Good, I. J., 135
                  accessible, 65                          Google, 127
                  active, 57                              Gottfredson, L. S., 1, 6, 20
                  Bandit, 57                              Gudwin, R. R., 9
                  bandit, 66                              Guilford, P. J., 3
                  Bernoulli scheme, 54, 65
                  Classiﬁcation problem, 65               HALproject, 18
                  classiﬁcation problem, 61               Hawkins, J., 132
                  computable, 74                          Hay, N., 137
                  ergodic, 64                             Henmon, 7
                  formal, 42                              Hernandez-Orallo, J., 20
                  function optimisation, 60, 67           HL(λ) algorithm, 115
                  higher order MDP, 58                    Horst, J., 10
                  Markov chain, 54, 62                    human, 83
                  Markov      decision    process         Hutter prize, 18
                       (MDP), 57                          Hutter, M., 18, 21
                  Partially   Observable    MDP
                       (POMDP), 59                        imagination, 90
                  passive, 56                             indiﬀerence principle, 36
                  repeated strategic game, 61,            inductive inference, 23
                       66                                 intelligence
                  reward summable, 73, 88                      animal, 15
                  sequence prediction, 56, 67                  crystallized, 4
                  totally passive, 55                          cultural bias, 12
                  wide range of, 74                            deﬁnitions, 4, 9
             Epicurus, principle of multiple ex-               dynamic tests, 13
                       planations, 24                          eﬃciency, 11
             evidence, 26                                      everyday meaning, 91
             evolution, 133                                    explosion, 135
             expected future discounted reward,                ﬂuid, 4
                       44, 73, 110                             informal thesis deﬁnition, 6
                                                               IQ, 11, 14, 20
             ﬁtness function, 134                              machine, 16
             ﬁtness uniform optimisation, 135                  order relation, 71
             Fogel, D. B., 9                                   static tests, 12
             182
                                                                                     Index
               tests, 11                               observations, 40, 73
               theories, 3                             Occam’s razor, 14, 25, 35, 75
               universal, 77, 133                      Ockham, W., 25
           interaction history, 42, 44                 Optimal ordered problem solver
                                                                 (OOPS), 129
           Jensen, A., 8
           Joshua blue project, 18                     Pareto optimal, 49
           Joy, B., 137                                Pell, B., 136
                                                       perception space, 41
           knowledge, 7                                perceptions, 40, 72
           Kronecker delta, 30, 115                    Pinter, 5
           Kurzweil, R., 10, 133, 137                  planning, 6
           learning, 6                                 polynomial time, P, 21
           Levin, L., 32                               Poole, D., 10
           linguistic complexity, 18                   posterior distribution, 26
           Loebner prize, 17                           prediction with expert advice, 77
           love, 90                                    predictor
           lower semi-computable, 34                        computable, 95, 97
           LSTD(λ) algorithm, 111                           Solomonoﬀ, 38
                                                       prior distribution, 26
           Mahoney, M., 17                                  algorithmic, 34
           Masum, H., 9, 19                                 dominant, 35
           maximum a posteriori (MAP), 39                   improper, 27
           maximum likelihood (ML), 39                      Solomonoﬀ, 30
           McCarthy, J., 9                                  Solomonoﬀ-Levin, 32, 35
           measure                                          Speed, 93, 109, 129
               computable, Mc, 33                           universal, 36
               enumerable, Me, 33                           universal over environments,
               probability, 28                                   47
           meat machine, 127                           prosperity, 137
           minimum       description    length         psychometric tests, 19
                    (MDL), 39, 95
           minimum message length (MML),               Q(λ), 122
                    39, 95                             qualia, 127
           Minsky, M., 10
           mixture distribution, 29, 49                R. J. Sternberg, 4
           Moravec, H., 133                            randomness, Martin-L¨of, 78
                                                       Raven, J., 14
           Neisser, U., 5                              reasoning, 6
           Newell, A., 10                              Rees, M, 137
           No free lunch theorem, 93                   reference machine, 32
           non-deterministic polynomial time,          reinforcement learning, 41
                    NP, 22                             reward, 40, 72
           Norvig, P., 136                             reward space, 41
           Numenta, 132                                Roadrunner supercomputer, 133
                                                                                        183
             Index
             Saﬀo, P., 136                               Wechsler adult intelligence scale,
             Sarsa(λ), 121                                         14
             Search for Extraterrestrial Intelli-        Wechsler, D., 5, 13
                      gence (SETI), 135                  Wikipedia, 18
             Searle, J., 16                              Woodrow, H., 8
             self-optimising, 50                         working memory, 14
             semi-measure, 32
             sequence prediction, 27, 56                 Yudkowsky, E., 137
             sequence prediction test, 21
             Simon, H., 125
             Simonton, D. K., 5
             simple computable sequences, 100
             Singularity Institute for Artiﬁcial
                      Intelligence (SIAI), 136
             Smith’s test, 21
             Smith, W. D., 21
             Snow, 5
             Solomonoﬀ induction, 38
             Solomonoﬀ, R. J., 30
             soul, 90
             Standford-Binet, 13
             Sternberg, R. J., 5
             supercomputer, 132
             symbols, 41
             temporal diﬀerence learning, 110
             temporal preferences, 44
             Terman, L. M., 7, 13
             text compression, 17
             Thiel, P., 136
             Thurstone, L. L., 3
             Turing machine
                  monotone universal, 96
                  preﬁx universal, 30
             Turing test, 16
                  fail, 17
             universal inference, 36
             vitalism, 127
             Voss, P., 19
             Wallach, W, 136
             Wang, P., 11
             Warwick, K., 9
             184
