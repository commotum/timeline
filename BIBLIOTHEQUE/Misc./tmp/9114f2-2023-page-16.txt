B PRMs800K

We collected 1,085,590 step-level labels over 101,599 solution samples. We
present the whole unfiltered dataset as PRM800K. During training we discard
labels used for quality control, as well as any step-level labels for which the
labeler was unable to complete the task. The filtered dataset contains about
800,000 step-level labels over 75,000 solutions. The full PRM800K dataset is
available at https://github.com/openai/prm800k.

The data collection was split into two separate phases. In phase 1, we col-
lected labels for multiple alternative completions at each step of a solution. This
seeded our dataset but was cumbersomeâ€”for many steps the alternatives were
repetitive, and we found labelers spent a lot of time supervising long uninter-
esting solutions. As a result, the step-level labels we collected in this phase are
more repetitive than those collected later. In total, phase 1 represents about
5% of PRM800K, or about 40,000 step-level labels.

The majority of our labels were collected as part of phase 2, during which we
scaled up and streamlined the data collection process. Phase 2 data collection
is split into 10 generations. For each generation, we sample N solutions per
problem from the generator. We rank these solutions with our current best

PRM and surface the highest scoring wrong-answer solutions to our labelers.
We retrain this PRM between each generation using all the latest data. This
active learning strategy changes the balance of our data considerably. Though
we sometimes surfaced correct solutions (either by manually injecting correct
solutions or because of errors in our automatic grading), the vast majority of the

labels we collected in this phase are for incorrect solutions. Table 3 breaks down
the balance of correct/incorrect steps and solutions between the different phases
of data collection. Though we mostly collected labels on incorrect solutions, we
still collected many labels for correct individual steps. In fact, our small-scale
ablations in Section 4.2 suggest that this active learning strategy, which favors
labelling high-scoring wrong-answer solutions, improves performance despite the
resulting imbalance in the dataset.

phase 1 phase 2. combined
% end in correct solution 85.1 13.2 14.2
% correct steps 58.6 74.1 73.1

Table 3: Distribution of positive/negative steps/solutions.

Some of our phase 2 questions are intended for quality control. For a quality
control question, researchers mark which steps are reasonable to label as in-
correct. Then we assess that labelers are able to consistently mark those steps
as incorrect. Prior to starting on phase 2, we required all labelers to label 30
quality control questions. This served as a screening test, and we only admitted
labelers that agreed with our gold labels at least 75% of the time.

We then designated 10-20 problems per generation as additional quality
control questions, and we randomly served them to labelers as they worked

17
