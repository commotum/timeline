                        preserved on the stack until some later time, signal encouraging the controller to push with higher
                        certainty is unlikely to be propagated back if the RNN controller suffers from vanishing gradient
                        issues. Likewise, the gradient for the decision to pop is 0 (as each pop empties the stack). We
                        conclude that under-using the memory in such a way makes its proper manipulation hard to learn by
                        the controller.
                        Conversely, over-using the stack (even incorrectly) means that gradient obtained with regard to the
                        (mis)use is properly communicated, as the pop gradient will not be zero (Equation 17) for all t.
                        Additionally, the (non-vanishing) gradient propagated through the stack state (Equation 12) will
                        allow the decision to push at some timestep to be rewarded or penalised based on reads at some
                        muchlater time. These remarks also apply to the continuous queue and double-ended queue.
                        Sinceinoursettingthedecisiontopushandpopisproducedbytakingabiasedlineartransformofan
                        RNNhiddenstatefollowedbyacomponent-wisesigmoidoperation,wehypothesised,basedonthe
                        above analysis, that initialising the bias for popping to a negative number would solve the variance
                        issue described above. We tested this on short sequences of the copy task, and found that a small
                        bias of −1 produced the desired algorithmic behaviour of the stack-enhanced controller across all
                        seeds tested. Setting this initialisation policy for the controller across all experiments allowed us to
                        reproduce the results produced in the paper without need for repeated initialisation. We recommend
                        that other controller implementations provide similar trainable biases for the decision to pop, and
                        initialise them following this policy (and likewise for controllers controlling other continuous data
                        structures presented in this paper).
                        C InversionTransductionGrammarsusedinExperiments
                        We present here, in Table 1, the inverse transduction grammars described in Section 4.2. Sets of
                        terminal-generating rules are indicated by the form ‘X → ...’, where i ∈ [1,k] and p(X ) ≈
                                                                        i                               i
                        (100/k)−1 for k terminal generating non-terminal symbols (classes of terminals), so that the gener-
                        ated vocabulary is balanced across classes and of a size similar to other experiments.
                                                                         p      ITGRules
                                                                         1      A→B1|B1
                           p      ITGRules                               1/4    B→B1orB2|B1oderB2
                                                                         1/4    B→S1andS2|S1undS2
                           1      A→S1VT2O3|S1O3VT2                      1/2    B→B1V1|B1V1
                           1/5    S→S1S2|S1S2                            3/4    V→W1B2|W1B2
                           1/5    S→S1rpiS2VT3|S1rpoS2VT3                1/4    V→W1|W1
                           3/5    S→ST1|ST1                              1/6    S→theM1|derM1
                           1/5    O→O1O2|O1O2                            1/6    S→theF1|dieF1
                           1/5    O→S1rpiS2VT3|S1rpoS2VT3                1/6    S→theN1|dasN1
                           3/5    O→OT1|OT1                              1/6    S→aM1|einM1
                           1/33   ST →si |so                             1/6    S→aF1|eineF1
                                    i    i   i
                           1/33   OTi →oii | ooi                         1/6    S→aN1|einN1
                           1/33   VT →vi |vo                             1/25   W →we |wg
                                    i    i    i                                  i     i    i
                                                                         1/25   M →me |mg
                                    (a) SVO-SOVGrammar                           i     i    i
                                                                         1/25   F →fe |fg
                                                                                 i    i   i
                                                                         1/25   N →ne |ng
                                                                                 i    i    i
                                                                       (b) English-German Conjugation Grammar
                                        Table 1: Inversion Transduction Grammars used in ITG Tasks
                        D ModelSizes
                        Weshow,inTable2,thenumberofparameterspermodel,forallmodelsusedintheexperimentsof
                        the paper.
                                                                 13
