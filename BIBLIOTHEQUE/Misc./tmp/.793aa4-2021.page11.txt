                    A Experimentalsetup
                    In this section we present more details of our experimental setup.
                    Pre-training        Wepre-train the models using a masked LM task (Devlin et al., 2018) and do not use
                    the Next Sentence Prediction (NSP) loss as suggested in RoBERTa (Liu et al., 2019). Each input is
                    constructed with full sentences from documents, and packed up to the maximum sequence length. We use
                    the same architecture as BERTBASE (Devlin et al., 2018) (L = 12, H = 768, A = 12) for our experiments.
                    Fine-tuning        Somedownstream tasks have different groups of full sentences provided at inputs. For
                    those tasks (e.g. MNLI, CoLA, XNLI,SQuAQ),weﬁne-tunemodelswithsupplementalsegmentencoding
                    discussed in Section §3. We leave models for other tasks unchanged as their pre-training correspondences.
                    Hyper-parameters            Hyper-parameters we use are presented in Table 7.
                                                                        English                             Multilingual
                                                           Pretrain                  Finetune    Pretrain                   Finetune
                                   MaxSteps                  500K              5 or 10 epochs       125K                    3 epochs
                                   Learning Rate            0.0018    {1e-5, 2e-5, 3e-5, 4e-5}    0.0018     {1e-5, 2e-5, 3e-5, 4e-5}
                                   WarmupProportion          0.025                         0.1      0.025                         0.1
                                   Sequence Length             128                        128         512                        512
                                   Batch Size                 4096                          32      4096                          32
                                   Checkpoint Interval         20k                        3.5k        20k                       3.5k
                                                             Table 7: Hyperparameters for all models
                    Translate       For our Translate experiments we follow the setup of Vaswani et al. (2017) and use their
                    Tensor2Tensor framework (Vaswani et al., 2018). We train using WMT18 ((Europarl v7, Common Crawl
                    corpus and News Commentary v13) en-de, de-en, en-cs and cs-en datasets. We report BLUE scores
                    provided by SacreBLEU (Post, 2018) on newstest 2018 dataset. We train a 6 layer Transformer model.
                    Anychangestoposition encoding are applied to all the attention layers both in the encoder and decoder.
                    WeuseAdamoptimizerandtrainfor250ksteps. Fordecodingweusebeamsearchwithbeamsize10
                    and length penalty 0.6.
                    B Proofs
                    Proof of Theorem 1. The ﬁrst claim follows easily by observing that rank of product of an two matrices is
                    upper bounded by the minimum of the individual ranks.
                                                                                   >            >
                                     rank(Aa) = rank((X+P)W W (X+P) )
                                                                             Q     K
                                                   ≤min(rank(X+P),rank(W ),rank(X+P),rank(W ))
                                                                                           Q                                  K
                                                   ≤dh.
                                                                     >             >                                     d×dh
                                         rank((X+P)W W (X+P) )≤d ,whereW ,W ∈R
                                                               Q     K                     h              Q      K
                       Thelast inequality follows from rank(W ) ≤ d as W                        ∈Rd×dh.
                                                                           Q        h        Q
                       Toprove the second claim we follow a construction approach. Let us ﬁrst take W                        =W tobesame
                                                                                                                          Q         K
                    matrices with ﬁrst d rows being identity matrix and the remaining d − d rows being all zeros. Then
                                             h                                                                h
                                                                           I                0            
                                                                    >            d ,d         d ,d−d
                                                          W W =                   h h          h      h      .
                                                              Q     K         0            0
                                                                               d−d ,d        d−d ,d−d
                                                                                    h h          h     h
                                                                          d ×d                                                        d ,d
                    Here Id ,d denotes the identity matrix in R h               h and 0d ,d denotes the all zeros matrix in R h .
                              h h                                                         h
                       WeletXbesuchthattheﬁrstdrowsformanidentitymatrixandrestarezeros-X> = [I                                           , 0       ].
                    HenceXW W>X>becomesasimilardiagonalmatrixwith                                                                     d,d   n−d,d
                                   Q     K
                                                                              I                0             
                                                                  > >               d ,d          d ,n−d
                                                      XW W X =                       h h           h      h      .
                                                             Q    K              0             0
                                                                                   n−d ,d       n−d ,n−d
                                                                                       h h           h     h
                                                                                 2984
