                                           MaximizingthePosition EmbeddingforVisionTransformers
                                                                               with Global Average Pooling
                                                                                            1,2                            1                           2*
                                                                     WonjunLee ,BumsubHam ,SuhyunKim
                                                                                   1Yonsei University, Republic of Korea
                                                                  2Korea Institute of Science and Technology, Republic of Korea
                                                              {velpegor, bumsub.ham}@yonsei.ac.kr, dr.suhyun.kim@gmail.com
                                                        Abstract
                        In vision transformers, position embedding (PE) plays a cru-
                        cial role in capturing the order of tokens. However, in vi-
                        sion transformer structures, there is a limitation in the expres-
                        siveness of PE due to the structure where position embed-
                        ding is simply added to the token embedding. A layer-wise
                        method that delivers PE to each layer and applies indepen-
                        dent Layer Normalizations for token embedding and PE has
                        been adopted to overcome this limitation. In this paper, we
                        identify the conflicting result that occurs in a layer-wise struc-
                        ture when using the global average pooling (GAP) method
                        instead of the class token. To overcome this problem, we pro-
                        pose MPVG, which maximizes the effectiveness of PE in a
                        layer-wise structure with GAP. Specifically, we identify that                                                                 Then, 
                        PEcounterbalances token embedding values at each layer in                                                   Original          Layer-wise + GAP ?           Layer-wise
                        a layer-wise structure. Furthermore, we recognize that the                                           (Class token -> GAP)                              (Class token -> GAP)
                        counterbalancing role of PE is insufficient in the layer-wise                                              +0.26%                                            -0.16%
                        structure, and we address this by maximizing the effective-                                                                                                      +0.73%
                        nessofPEthroughMPVG.Throughexperiments,wedemon-                                                      Original -> Layer-wise                             Proposed Method
                        strate that PE performs a counterbalancingroleandthatmain-                                                (Class token)                                    Layer-wise
                                                                                                                                   +0.80%                                         (GAP + MPVG)
                        taining this counterbalancing directionality significantly im-
                        pacts vision transformers. As a result, the experimental re-                                Figure 1: The conflicting result between the GAP method
                        sults show that MPVG outperforms existing methods across
                        vision transformers on various tasks.                                                       and the Layer-wise method. In DeiT-Ti, using the GAP
                                                                                                                    methodandtheLayer-wisemethodseparatelyresultsinper-
                                                   Introduction                                                     formance improvements, but combining these two methods
                    Recently, vision transformers have become essential archi-                                      leads to a decrease in performance. As a result, MPVG re-
                    tecture in the field of computer vision due to their superior                                   solves this phenomenon between the GAP and Layer-wise
                    performance, surpassing CNNs in various tasks such as im-                                       structure, maximizing the effect of PE.
                    age classification, object detection, and semantic segmenta-
            arXiv:2502.02919v1  [cs.CV]  5 Feb 2025tion. This superiority has led to extensive research into nu-
                    merous elements of vision transformer architecture, starting                                    methodhasbeenwidelyadoptedinvisiontransformers(Liu
                    with ViT (Dosovitskiy et al. 2020).                                                             et al. 2021; Chu et al. 2021a; Chang et al. 2023; Zhai et al.
                        Amongtheresearch on vision transformers, image repre-                                       2022).
                    sentation methods for class prediction have been studied. In                                       Another research topic in vision transformers is position
                    ViT, the class token is used to perform image representation,                                   embedding (PE). PE plays a crucial role in providing po-
                    andtheoutputofthistokenisthenusedtomakeclasspredic-                                             sitional information of tokens in the vision transformer, as
                    tions via Multi-Layer Perceptron (MLP) (Dosovitskiy et al.                                      the self-attention mechanism has an inherent deficiency in
                    2020). However, in several vision transformers, global aver-                                    capturing the ordering of input tokens (Wu et al. 2021; Xu
                    age pooling (GAP) has been preferred over the class token                                       et al. 2024). In the original vision transformer, the expres-
                    methodduetoitstranslation-invariantcharacteristicsandsu-                                        siveness of the PE is limited due to its structure, where PE
                    perior performance (Chu et al. 2021b). As a result, the GAP                                     is simply added to the token embedding before being input
                         *Corresponding author.                                                                     into the first layer. To address this problem, each layer has
                    Copyright Â© 2025, Association for the Advancement of Artificial                                 independent Layer Normalizations (LNs) for the token em-
                    Intelligence (www.aaai.org). All rights reserved.                                               bedding and PE, with PE being gradually delivered across
