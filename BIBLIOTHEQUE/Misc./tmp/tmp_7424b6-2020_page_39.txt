                   6.2.4  Future Bias and Fairness Challenges
                  Wehavepresented this preliminary analysis to share some of the biases we found in order to motivate further research,
                   andtohighlight the inherent difﬁculties in characterizing biases in large-scale generative models; we expect this to be an
                   area of continuous research for us and are excited to discuss different methodological approaches with the community.
                  Weviewtheworkinthissectionassubjective signposting - we chose gender, race, and religion as a starting point, but
                   werecognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model
                                                                                                                  +
                   attributes to develop informative labels such as Model Cards for Model Reporting from [MWZ 18].
                   Ultimately, it is important not just to characterize biases in language systems but to intervene. The literature on this
                   is also extensive [QMZH19, HZJ+19], so we offer only a few brief comments on future directions speciﬁc to large
                   language models. In order to pave the way for effective bias prevention in general purpose models, there is a need for
                   building a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for
                   these models. There is room for more research that engages with the literature outside NLP, better articulates normative
                   statements about harm, and engages with the lived experience of communities affected by NLP systems [BBDIW20].
                   Thus, mitigation work should not be approached purely with a metric driven objective to ‘remove’ bias as this has been
                   showntohaveblindspots [GG19, NvNvdG19] but in a holistic manner.
                   6.3  EnergyUsage
                   Practical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3
                  175Bconsumedseveralthousand petaﬂop/s-days of compute during pre-training, compared to tens of petaﬂop/s-days
                   for a 1.5B parameter GPT-2 model (Figure 2.2). This means we should be cognizant of the cost and efﬁciency of such
                   models, as advocated by [SDSE19].
                   Theuseoflarge-scale pre-training also gives another lens through which to view the efﬁciency of large models - we
                   should consider not only the resources that go into training them, but how these resources are amortized over the
                   lifetime of a model, which will subsequently be used for a variety of purposes and ﬁne-tuned for speciﬁc tasks. Though
                   models like GPT-3 consume signiﬁcant resources during training, they can be surprisingly efﬁcient once trained: even
                   with the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or
                   only a few cents in energy costs. Additionally, techniques like model distillation [LHCG19a] can further bring down
                   the cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efﬁcient
                   versions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efﬁciency
                   of such models over time, similar to trends observed in image recognition and neural machine translation [HB20].
                   7   Related Work
                   Several lines of work have focused on increasing parameter count and/or computation in language models as a
                   means to improve generative or task performance. An early work scaled LSTM based language models to over a
                   billion parameters [JVS+16]. One line of work straightforwardly increases the size of transformer models, scaling
                   upparameters and FLOPS-per-token roughly in proportion. Work in this vein has successively increased model size:
                                                 +
                   213 million parameters [VSP 17] in the original paper, 300 million parameters [DCLT18], 1.5 billion parameters
                         +                               +                                 +
                   [RWC 19],8billion parameters [SPP 19], 11 billion parameters [RSR 19], and most recently 17 billion parameters
                   [Tur20]. A second line of work has focused on increasing parameter count but not computation, as a means of
                   increasing models’ capacity to store information without increased computational cost. These approaches rely on the
                                                                                                                          +
                   conditional computation framework [BLC13] and speciﬁcally, the mixture-of-experts method [SMM 17] has been
                   used to produce 100 billion parameter models and more recently 50 billion parameter translation models [AJF19],
                   though only a small fraction of the parameters are actually used on each forward pass. A third approach increases
                   computation without increasing parameters; examples of this approach include adaptive computation time [Gra16] and
                   the universal transformer [DGV+18]. Our work focuses on the ﬁrst approach (scaling compute and parameters together,
                   by straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ
                   this strategy.
                                                                                                                                    +
                   Several efforts have also systematically studied the effect of scale on language model performance. [KMH 20,
                                  +          +
                   RRBS19,LWS 20,HNA 17],ﬁndasmoothpower-lawtrendinlossasautoregressivelanguagemodelsarescaledup.
                   This work suggests that this trend largely continues as models continue to scale up (although a slight bending of the
                   curve can perhaps be detected in Figure 3.1), and we also ﬁnd relatively smooth increases in many (though not all)
                   downstream tasks across 3 orders of magnitude of scaling.
                   Another line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language
                   models that are as small as possible. This approach includes ALBERT [LCG+19] as well as general [HVD15] and
                                                                            39
