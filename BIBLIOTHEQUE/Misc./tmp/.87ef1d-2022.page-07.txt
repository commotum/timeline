                         Published as a conference paper at ICLR 2022
                                                   Context   Pretrain  Fine-tune  Perplexity
                                                   512       8192      None       2.37
                                                   512       65K       None       2.31
                                                   512       8192      65K        2.32
                                                   512       8192      131K       2.30
                                                   512       8192      262K       2.26
                                                   2048      8192      None       2.33
                                                   2048      65K       None       2.26
                                                   2048      65K       131K       2.23
                                                   2048      65K       262K       2.21
                               Table 5: Finetuning for 20K steps to make use of a larger memory on the arXiv data set.
                         C4(4K+)dataset, adding memory of size 8192 improves the perplexity of the vanilla Transformer
                         (with context size 512) from 17.20 to 14.42, and improves Transformer-XL from 15.38 to 14.04.
                         Increasing the size of the memory increases the beneﬁt of the memory.   The best perplexities
                         for all datasets and architectures were obtained with a memory size of 65K.
                         Note that Transformer-XL with context size 2048 already has a theoretical receptive ﬁeld that is quite
                         large. Each token in a higher layer can attend up to 2048 tokens away in the layer below, so the total
                         receptive ﬁeld is 2048 · 12 (layers) ∼ 25K. Nevertheless, we still saw a substantial gain when adding
                         an external memory of size 8192 to this model. kNN attention into memory would appear to be a
                         moreeffective way to retrieve information from the distant past than the Transformer-XL cache.
                         Ontheotherhand,wealsosawimprovementsbyaddingXLcachetothelarge-memory(65K)models.
                         In a vanilla (non-XL) Transformer, the ﬁrst few tokens in a sequence have very little context, and
                         thus have higher perplexity. The XL cache provides additional local short-range context at the start of
                         a sequence, which complements the long-range context provided by external memory.
                         Interestingly, in a vanilla Transformer, using even a small external memory of size 1536 provides
                         a gain in perplexity which is almost as good as using a local context of size 2048 but no memory
                         (e.g. Table 4). This is surprising, because the external memory is not differentiable, and is added only
                         to one layer of the Transformer, whereas increasing the context size is differentiable and affects all
                         layers. We conclude that the lower layers of a Transformer don’t necessarily need long-range context,
                         and having a differentiable memory is not as important as one might suspect.
                         4.4   SCALING TO LARGER MODELS
                         WescaleduptheTransformermodeltosizesof1and8billionparameters. Forthe1billionparameter
                         model, we use 8 layers, 32 heads with head dimension 128, d_model 2048, and d_ff 16384. For the
                         8 billion parameter model, we use 64 heads, 16 layers, d_model 4096, and d_ff 32768. We used a
                         context size of 2048, memory size of 8192, and no XL cache. We ran the comparisons to the vanilla
                         Transformer on the arXiv math dataset. Scaling plots are shown in Figure 1.
                         External memory provides a consistent improvement to the model as it is scaled up. Remarkably, we
                         found that the smaller Memorizing Transformer with just 8k tokens in memory can match the
                         perplexity of a larger vanilla Transformer which has 5X more trainable parameters.
                         4.5   FINETUNING ON LARGER MEMORIES
                         Finetuningonalargermemory. Insomecases,trainingwasunstablewhenusinglargememories,
                         possibly due to distributional shift early in the training (See Section 3.2). Thus, for memories of
                         131Kormoretokens,weﬁrstpretrain the model with a memory size of 8192 or 65K for 500K steps,
                         and then ﬁnetune it with the larger memory for an additional 20K steps. The results of ﬁnetuning on
                         the arXiv Math data set are shown in Table 5. Increasing the size of external memory provided
                         consistent gains up to a size of 262K. Note that 262K tokens is longer than almost all of the
                         documents in arXiv, and thus we would not expect to see any gain past this point (see Appendix A).
                                                                      7
