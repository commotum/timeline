                 AFastLearningAlgorithmforDeepBeliefNets                                           1551
                 Fori = 0thisisgivenbydeﬁnitionofthedistributioninequationA.13.For
                 i > 0, we have:
                            (i)     (i) (i−1)  (i−1)          fx(i),y(i−1)        (i−1)
                        P(x )=         P x |y        P y        =          fyy(i−1)   fy y
                                   (i−1)                            (i−1)
                                  y                                y
                                =fxx(i)                                                        (A.18)
                 andsimilarly for P(y(i)). Now we see that the following conditional distri-
                 butions also hold true:
                             (i)  (i)     (i)  (i)   (i)       (i)  (i)
                           P x |y      =P x ,y         P y      =gx x |y                         (A.19)
                           (i)  (i+1)     (i)  (i+1)    (i+1)       (i) (i+1)
                        P y |x         =P y ,x            P x       =gy y |x          .          (A.20)
                 Soourjointdistribution over the stack of variables also leads to the appro-
                 priateconditionaldistributionsfortheunrolledgraphinthe“downwards”
                 direction. Inference in this inﬁnite graph is equivalent to inference in the
                 joint distribution over the sequence of variables, that is, given x(0),wecan
                 obtain a sample from the posterior simply by sampling y(0)|x(0), x(1)|y(0),
                 y(1)|x(1),....This directly shows that our inference procedure is exact for
                 the unrolled graph.
                 AppendixB: PseudocodeforUp-DownAlgorithm
                 WenowpresentMATLAB-stylepseudocodeforanimplementationofthe
                 up-down algorithm described in section 5 and used for back-ﬁtting. (This
                 methodisacontrastive version of the wake-sleep algorithm; Hinton et al.,
                 1995.)
                     The code outlined below assumes a network of the type shown in
                 Figure 1 with visible inputs, label nodes, and three layers of hidden units.
                 Beforeapplyingtheup-downalgorithm,wewouldﬁrstperformlayer-wise
                 greedytraining as described in sections 3 and 4.
                 \% UP-DOWN ALGORITHM
                 \%
                 \% the data and all biases are row vectors.
                 \% the generative model is: lab <--> top <--> pen --> hid --> vis
                 \% the number of units in layer foo is numfoo
                 \% weight matrices have names fromlayer tolayer
                 \% "rec" is for recognition biases and "gen" is for generative
                 \% biases.
                 \% for simplicity, the same learning rate, r, is used everywhere.
