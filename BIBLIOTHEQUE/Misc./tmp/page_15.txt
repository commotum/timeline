correlates with its effective dimensionality. To quantify this phenomenon, we can examine the
Participation Ratio (PR), which serves as a standard measure of the effective dimensionality of a
high-dimensional representation’. The PR is calculated using the formula

(oA)?
2 ,
iri

where {,} are the eigenvalues of the covariance matrix of neural trajectories. Intuitively, a higher
PR value signifies that variance is distributed more evenly across many dimensions, corresponding
to a higher-dimensional representation. Conversely, a lower PR value indicates that variance is
concentrated in only a few principal components, reflecting a more compact, lower-dimensional
structure.

PR =

The dimensionality hierarchy can be observed, for example, in the mouse cortex, where the PR of
population activity increases monotonically from low-level sensory areas to high-level associative
areas, supporting this link between dimensionality and functional complexity” (Figure 8 (a,b)).

We evaluated whether HRM reproduces this neuroscientific principle by calculating the PR for
both recurrent modules after training on the Sudoku-Extreme Full dataset. The PR computation
used the covariance matrix derived from neural states gathered across multiple Sudoku-solving
trajectories. The results show a striking parallel to the biological findings. The low-level module’s
state (z,,) occupies a relatively small subspace with a participation ratio of 30.22, whereas the high-
level module’s state (zz) operates in a substantially larger subspace with a participation ratio of
89.95, as shown in Figure 8(c). Furthermore, Figure 8(d) shows that increasing the number of
unique tasks (trajectories) from 10 to 100 causes zz dimensionality to scale up accordingly, while
zr dimensionality remains stable. These results suggest an emergent separation of representational
capacity between the modules that parallels their functional roles.

To confirm that this hierarchical organization is an emergent property of training, and not an artifact
of the network’s architecture, we performed a control analysis using an identical but untrained
network with random weights.

We initialized an identical HRM architecture with random weights and, without any training, mea-
sured the PR of its modules as the network processed the same task-specific inputs given to the
trained model.

The results, shown in Figure 8(e,f), reveal a stark contrast: the high-level and low-level modules of
the untrained network exhibit no hierarchical separation, with their PR values remaining low and
nearly indistinguishable from each other. This control analysis validates that the dimensionality
hierarchy is an emergent property that arises as the model learns to perform complex reasoning.

The high-to-low PR ratio in HRM (2/2, * 2.98) closely matches that measured in the mouse
cortex (= 2.25). In contrast, conventional deep networks often exhibit neural collapse, where
last-layer features converge to a low-dimensional subspace ***!*?, HRM therefore departs from the
collapse pattern and instead fosters a high-dimensional representation in its higher module. This
is significant because such representations are considered crucial for cognitive flexibility and are a
hallmark of higher-order brain regions like the prefrontal cortex (PFC), which is central to complex
reasoning.

This structural parallel suggests the model has discovered a fundamental organizational principle.
By learning to partition its representations into a high-capacity, high-dimensional subspace (zz)

15
