                  Sample efﬁciency       Weexplore how many train-          Type         #N        IB Top-5 Top-20 Top-100
                  ing examples are needed to achieve good passage           Random       7          7   47.0    64.3     77.8
                  retrieval performance. Figure 1 illustrates the top-k     BM25         7          7   50.0    63.3     74.8
                  retrieval accuracy with respect to different num-         Gold         7          7   42.6    63.1     78.3
                  bers of training examples, measured on the devel-         Gold         7         3 51.1       69.1     80.8
                  opment set of Natural Questions. As is shown, a           Gold         31        3 52.1       70.8     82.1
                                                                            Gold         127       3 55.8       73.0     83.1
                  dense passage retriever trained using only 1,000 ex-      G.+BM25(1) 31+32       3 65.0       77.3     84.4
                  amples already outperforms BM25. This suggests            G.+BM25(2) 31+64       3 64.5       76.4     84.0
                  that with a general pretrained language model, it is      G.+BM25(1) 127+128     3 65.8       78.0     84.9
                  possible to train a high-quality dense retriever with
                  a small number of question–passage pairs. Adding         Table 3: Comparison of different training schemes,
                  more training examples (from 1k to 59k) further          measured as top-k retrieval accuracy on Natural Ques-
                  improves the retrieval accuracy consistently.            tions (development set).    #N: number of negative
                                                                                                                         (1)
                                                                           examples, IB: in-batch training.    G.+BM25       and
                                                                                     (2)
                  In-batch negative training        We test different      G.+BM25       denote in-batch training with 1 or 2 ad-
                  training schemes on the development set of Natural       ditional BM25 negatives, which serve as negative pas-
                  Questions and summarize the results in Table 3.          sages for all questions in the batch.
                  Thetopblock is the standard 1-of-N training set-
                  ting, where each question in the batch is paired         Our experiments on Natural Questions show that
                  with a positive passage and its own set of n neg-        switching to distantly-supervised passages (using
                  ative passages (Eq. (2)). We ﬁnd that the choice         the highest-ranked BM25 passage that contains the
                  of negatives — random, BM25 or gold passages             answer), has only a small impact: 1 point lower
                  (positive passages from other questions) — does          top-k accuracy for retrieval. Appendix A contains
                  not impact the top-k accuracy much in this setting       moredetails.
                  whenk ≥20.                                               Similarity and loss     Besides dot product, cosine
                     Themiddlebockisthein-batchnegativetraining            andEuclideanL2distancearealsocommonlyused
                  (Section 3.2) setting. We ﬁnd that using a similar       asdecomposablesimilarityfunctions. Wetestthese
                  conﬁguration (7 gold negative passages), in-batch        alternatives and ﬁnd that L2 performs compara-
                  negative training improves the results substantially.    ble to dot product, and both of them are superior
                  Thekeydifference between the two is whether the          to cosine. Similarly, in addition to negative log-
                  gold negative passages come from the same batch          likelihood, a popular option for ranking is triplet
                  or from the whole training set. Effectively, in-batch    loss, which compares a positive passage and a nega-
                  negative training is an easy and memory-efﬁcient         tive one directly with respect to a question (Burges
                  waytoreusethenegative examples already in the            et al., 2005). Our experiments show that using
                  batch rather than creating new ones. It produces         triplet loss does not affect the results much. More
                  morepairs and thus increases the number of train-        details can be found in Appendix B.
                  ing examples, which might contribute to the good
                  model performance. As a result, accuracy consis-         Cross-dataset generalization        One interesting
                  tently improves as the batch size grows.                 question regarding DPR’s discriminative training
                     Finally, we explore in-batch negative training        is how much performance degradation it may suf-
                  with additional “hard” negative passages that have       fer from a non-iid setting. In other words, can
                  high BM25 scores given the question, but do not          it still generalize well when directly applied to
                  containtheanswerstring(thebottomblock). These            a different dataset without additional ﬁne-tuning?
                  additional passages are used as negative passages        To test the cross-dataset generalization, we train
                  for all questions in the same batch. We ﬁnd that         DPRonNaturalQuestionsonlyandtest it directly
                  adding a single BM25 negative passage improves           on the smaller WebQuestions and CuratedTREC
                  the result substantially while adding two does not       datasets. We ﬁnd that DPR generalizes well, with
                  help further.                                            3-5 points loss from the best performing ﬁne-tuned
                                                                           model in top-20 retrieval accuracy (69.9/86.3 vs.
                  Impact of gold passages       Weuse passages that        75.0/89.1 for WebQuestions and TREC, respec-
                  match the gold contexts in the original datasets         tively), while still greatly outperforming the BM25
                  (whenavailable) as positive examples (Section 4.2).      baseline (55.0/70.9).
