                        Published as a conference paper at ICLR 2025
                        In this task, an animal is presented with a display of moving dots, some of which move randomly while
                        somemoveinacoherentdirection. The animal must identify the direction of the coherent motion. This task
                        has been shown to elicit consistent results across different animals, including pigeons (Bischof et al., 1999),
                        rats (Reinagel, 2013), monkeys (Newsome et al., 1989), and humans (Heekeren et al., 2004). To create a
                        meaningful evaluation within the realm of predictive models and convolutional neural networks (CNNs), we
                        adapted this task into a two-image input scenario. The CNN would receive an original image and its shifted
                        counterpart — the shift can be to the left, right, up, or down.
                        For varying levels of difficulty, we used dot consolidation, a widely-utilized approach, and elected to present
                        states of an Ising model at different temperatures. The difficulty of these levels ranges from a fully random
                        state (hardest) to a highly consolidated, low-temperature state (easiest). Figure 2 provides four sample
                        scenarios with varying degrees of difficulty. Notably, the number of layers in the FPI is clearly correlated with
                        the problem’s complexity, and errors are randomly distributed across the difficulty levels. This demonstrates
                        howtheMINDmodelsuccessfullyadjustscomputationbasedoninputcomplexity. The MINDmodelwas
                        able to reach an accuracy of 0.85 ± 0.007, while a CNN with the same number of layers and channels as the
                        prediction network could only achieve an accuracy of 0.56 ± 0.0004 on this 4-class task with variance across
                        a 9-fold cross-validation.
                        4.2   LAYER UTILIZATION ANALYSIS
                        Toevaluate the introspection mechanism of the
                        MIND model, we analyzed how the model al-
                        locates computational resources based on input
                        complexity. WecategorizedtheImageNetdataset
                        into three complexity levels—easy, medium,
                        and hard—using the confidence scores from a
                        pre-trained ResNet-50 model (He et al., 2016).
                        Specifically, inputs with softmax probabilities
                        above 0.8 were labeled as easy, those between
                        0.4 and 0.8 as medium, and those below 0.4 as       Figure 3: Analysis of layer utilization for samples from
                        hard.                                               the ImageNet dataset, categorized into three complexity
                        Thefrequency distribution of the number of lay-     levels: easy, medium, and hard. The figure illustrates how
                        ers used across different input complexity levels   the MINDmodelallocates computational resources based
                        as illustrated in Figure 3 demonstrates that the    oninput complexity, with more layers being utilized for
                        MINDmodeladaptsitscomputationaleffort ac- harder examples.
                        cording to input complexity. Inputs classified as
                        easy predominantly trigger minimal computation, often utilizing only the first layer in a convergent loop.
                        In contrast, more complex inputs engage additional layers, indicating the model’s capacity to dynamically
                        allocate resources when faced with challenging tasks. This adaptive behavior validates the effectiveness of
                        the introspection mechanism in assessing input complexity and adjusting computation accordingly. Due to
                        limited space, relevant experiments are also shown in Appendix E.1.
                        TheMINDmodelachievessuperiorperformance on both datasets while using significantly fewer parameters
                        compared to the baseline models. Notably, on ImageNet, the MIND model attains a Top-1 accuracy of
                        88%,outperforming EfficientNet-B7 by 3.7 percentage points despite having approximately 12 times fewer
                        parameters. Additionally, the MIND model demonstrates enhanced robustness, which we attribute to its
                        adaptive computation and ability to allocate resources effectively based on input complexity.
                        4.3   EXPERIMENTS ON VISION TASKS
                        To evaluate the effectiveness of the MIND model in vision tasks, we implemented a 3-layer Convolution
                        Neural Network (CNN) (LeCun et al., 2015) as the prediction network within the MIND framework. We
                        comparedourmodelagainsttraditional baselines and state-of-the-art architectures, specifically ResNet-50
                        (He et al., 2016) and EfficientNet-B7 (Tan & Le, 2021), which are renowned for their high performance
                                                                          7
