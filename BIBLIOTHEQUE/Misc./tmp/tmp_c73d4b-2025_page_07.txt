                           2.2  Nested Optimization Problems
                           In the previous section, we provided examples to demonstrate how one can decompose a machine
                           learning model into a set of nested or multi-level optimization problems. Next, we first aim to present
                           a formal formulation for nested learning problems and then define Neural Learning Module–an
                           integrated computational system that learns from data.
                           Asweobservedintheprevioussection, while we decomposed the model into a set of optimization
                           process, it is still unclear if we can define a hierarchy (or order) over these problems, and uniquely
                           represent the model in this format. Inspired by the hierarchy of brain waves that indicates the
                           information processing frequency rate of each part (discussed in Section 1), we use the update rate of
                           each optimization problem to order the components in multiple levels. To this end, we let the one
                           update step over one data point to be the unit of time, and define the update frequency rate of each
                           component as:
                           Definition 2 (Update Frequency). For any component of A, which can be a parametric component
                          (e.g., learnable weights or momentum term in gradient descent in momentum) or a non-parametric
                           component (e.g., attention block), we define its frequency, denoted as fA, as its number of updates
                           per unit of time.
                           Given the above update frequency, we can order the components of a machine learning algorithm
                           based on operator (· ≻ ·). We let A to be faster than B and denote A ≻ B if: (1) fA > fB, or
                           (2) f  =f butthecomputation of the B’s state at time t requires the computation of A’s state
                               A      B
                                                                                          f
                           at time t. In this definition, when A ⊁ B and B ⊁ A, we let A = B, which indicates that A and
                           Bhasthe same frequency update, but their computation is independent of each other (Later, we
                           provide an example of this cases in AdamW optimizer). Based on the above operator, we sort the
                           components into an ordered set of “levels”, where (1) components in the same level have the same
                           frequency update, and (2) the higher the level is, the lower its frequency. Notably, based on the above
                           definition, each component has its own optimization problem and so context. While we optimize
                           the component’s inner objective with gradient-based optimizers, the above statement is equivalent to
                           having exclusive gradient flow for each component in the model. In general case, however, one can
                           use non-parametric solution (as we later discuss about attention).
                           Neural Learning Module. Given the above definition of nested learning problems, we define neural
                           learning module as a new way of representation of machine learning models that shows the model
                           as an interconnected system of components, each of which with its own gradient flow. Note that,
                           orthogonal to deep learning, nested learning allows us to define neural learning models with more
                           levels, resulting in more expressive architecture.
                              Nested learning allows computational models that are composed of multiple (multi-layer)
                               levels to learn from and process data with different levels of abstraction and time-scales.
                           Next, we study optimizers and well-known deep learning architectures from the nested learning
                           perspective, and provide examples that how NL can help to enhance those components.
                           2.3  Optimizers as Learning Modules
                           In this section, we start by understanding how well-known optimizers and their variants are special
                           instances of nested learning. Recall the gradient descent method with momentum,
                                                         W =W+m
                                                           i+1      i     i+1
                                                         m =α m −η∇L(W;x),                                             (17)
                                                           i+1     i+1   i    t       i  i
                           where matrix (or vector) m is the momentum at state i and α and η are adaptive learning and
                                                      i                                  i      i
                           momentumrates, respectively. Assuming αi+1 = 1, the momentum term can be viewed as the result
                           of optimizing the following objective with gradient descent:
                                                             min ⟨m∇L(W ;x )⊤,I⟩.                                      (18)
                                                               m             i  i
                           This interpretation shows that momentum can indeed be viewed as a meta memory module that
                           learns how to memorize gradients of the objective into its parameters. Building on this intuition, in
                                                                          7
