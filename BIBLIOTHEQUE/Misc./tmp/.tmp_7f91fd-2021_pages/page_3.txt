                                                    Large-scale few-shot program induction and synthesis 
               We generate PROGRES by applying a C++ program inter-              Few-shot program induction and synthesis  Program 
               preter to programs solving the 5,500 problems from Code-          synthesis (Shaw et al., 1975; Solar-Lezama et al., 2006; 
               Forces.  This allows us to create a much larger dataset of        Gulwani et al., 2017) aims at generating code that satisfes 
               more than 200,000 few-shot program-induction tasks of             a set of input-output examples.  Typically, these methods 
               varying diffculty and style (sec. 3.4).  A careful analysis       are data effcient, often generalizing from few examples. 
               of baselines (sec. 4.3) shows that there is both an initial       However, the combinatorial space of programs is hard to 
               promise and a long road ahead in the quest for building           search, often restricting the capacity of the language or the 
               effective solutions to this problem.                              size of programs.  Machine learning, and deep learning 
               In summary, our contributions are the following:                  in particular, have been increasingly used to improve the 
                                                                                 search over programs (Parisotto et al., 2016; Kalyan et al., 
                 1.  We propose a generic method of building large, real         2018; Brockschmidt et al., 2018; Ellis et al., 2019; Nye et al., 
                    program induction and synthesis benchmarks.                  2020). In this work, we implement one of these methods, 
                                                                                 RobustFill (Devlin et al., 2017), as a baseline. We believe 
                 2.  We provide a new dataset of more than 200, 000 pro-         that the substantial increase in data will facilitate further 
                    gram induction tasks, with multiple challenges for the       progress in neural searchers. Finally, there have been demos 
                    program synthesis and few-shot learning communities.         using GPT-3 (Brown et al., 2020) to predict short programs 
                                                                                 from English descriptions. Inspired by this nascent line of 
                 3.  We analyse the effect of adding different types of data     research, we modify the LSTM in the original RobustFill by 
                    from PROGRES to a transformer-based algorithm.               a pretrained transformer (Lewis et al., 2019). 
               2. Related work                                                   Program induction datasets  There have been multiple 
               Learning to few-shot learn  meta-learning (Schmidhuber,           few-shot program induction datasets, such as those used in 
               1987; Bengio et al., 1995; Thrun & Pratt, 1998) aims at learn-    FlashFill (Gulwani, 2011; Gulwani et al., 2015) and Dream-
               ing priors from many tasks to generalize to a new task from       Coder (Ellis et al., 2020), as well as the Abstract Reasoning 
               small amounts of data. Most of these methods assume that          Challenge(ARC) (Chollet, 2019), a list functions bench-
               the input form is constant( (Iwata & Kumagai, 2020) being a       mark (Rule, 2020), or the SyGus competition (Alur et al., 
               recent exception) and few-shot learning datasets are mainly       2017). Although these benchmarks contain many interesting 
               image classifcation (Lake et al., 2015; Vinyals et al., 2016;     problems, they have been manually created by humans in-
               Ren et al., 2018; Antoniou et al., 2020; Chen et al., 2019;       stead of being automatically generated from real programs. 
               Triantafllou et al., 2019) or low-dimensional continuous          This creates a signifcant bias on the datasets (often being 
               regression (Finn et al., 2017; Bauza et al., 2019). Moreover,     captured by a relatively simple Domain Specifc Language) 
               deep learning-based meta-learning algorithms do not typi-         and restricts the amount of tasks to a few hundred tasks. In 
               cally generalize broadly outside the data distribution, espe-     contrast, our benchmark, PROGRES, contains more than 
               cially non-optimization-based approaches (Finn, 2018). To         200,000 tasks, two orders of magnitude more.  This will 
               improve this, more compositional methods to meta-learning         allow neural-based methods, often data-ineffcient, to learn 
               are increasingly being proposed (Alet et al., 2018; Bengio        to generalize or search in these domains. Larger program 
               et al., 2019; Ke et al., 2019; Mendez & Eaton, 2020; Ruis         datasets have been shown to be useful to learn to search (Ba-
               et al., 2020). PROGRES provides a relevant benchmark for          log et al., 2016; Shin et al., 2019).  However, in contrast 
               these compositional few-shot learning methods.                    to PROGRES, these programs were randomly generated 
                                                                                 from restricted DSLs, and therefore do not capture the struc-
               Neural program induction  Neural networks are univer-             ture of real programs. 
               sal approximators that have delivered great results in a wide 
               variety of felds.  Motivated by these successes, multiple         Datasets  leveraging  competitive  programming  code 
               works have applied neural or neuro-symbolic methods to            Data from programming competitions, and codeforces. 
               latent program induction, where programs are represented          com in particular, has been used before to build several 
               only implicitly, without any reference to a specifc DSL.          benchmarks.  Zavershynskyi et al. (2018) is probably closest 
               Many of these approaches propose neural architectures             to our benchmark, combining a mixture of crowd-sourced 
               inspired by computational modules (Graves et al., 2014;           descriptions of subprograms with input-output examples 
               Kurach et al., 2015; Reed & De Freitas, 2015; Joulin &            for the entire programs (not subprograms).  Kulal et al. 
               Mikolov, 2015; Graves et al., 2016; Dong et al., 2019; Li         (2019) improved and standardized the pseudo-code annota-
               et al., 2020), training weights end-to-end. However, most         tion with line-by-line annotations and learned to translate 
               of these works aim at learning a single task performed by         from single-line pseudo-code to instruction. While useful, 
               a program. In contrast, PROGRES measures the ability to           language annotations are hard to scale because they have 
               learn new tasks from few examples.                                to be crowd-sourced and require expertise. Moreover, they 
