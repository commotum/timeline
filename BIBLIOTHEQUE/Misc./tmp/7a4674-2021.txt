                              ContemporarySymbolicRegressionMethodsand
                                                  their Relative Performance
                                                            ‚àó                                              ‚Ä†
                                           William La Cava                           Patryk Orzechowski
                                  Institute for Biomedical Informatics         Institute for Biomedical Informatics
                                       University of Pennsylvania                  University of Pennsylvania
                                          lacava@upenn.edu                    patryk.orzechowski@gmail.com
                                               BogdanBurlacu                           Fabr√≠cio Olivetti de Fran√ßa
                                                                                                                 ‚Ä°
                                  Josef Ressel Center for Symbolic Regression          Federal University of ABC
                                  University of Applied Sciences Upper Austria             Santo Andre, Brazil
                                         bogdan.burlacu@fh-ooe.at                      folivetti@ufabc.edu.br
                                               MarcoVirgolin                                  YingJin
                                       Mechanics and Maritime Sciences                Department of Statistics
                                      Chalmers University of Technology                 Stanford University
                                       marco.virgolin@chalmers.se                    ying531@stanford.edu
                                           Michael Kommenda                                Jason H. Moore
                                Josef Ressel Center for Symbolic Regression       Institute for Biomedical Informatics
                               University of Applied Sciences Upper Austria           University of Pennsylvania
                                     michael.kommenda@fh-ooe.at                          jhmoore@upenn.edu
                                                                     Abstract
                                   Manypromising approaches to symbolic regression have been presented in recent years,
                                   yet progress in the Ô¨Åeld continues to suffer from a lack of uniform, robust, and transparent
                                   benchmarking standards. In this paper, we address this shortcoming by introducing an
                                   open-source, reproducible benchmarking platform for symbolic regression. We assess 14
                                   symbolic regression methods and 7 machine learning methods on a set of 252 diverse
        arXiv:2107.14351v1  [cs.NE]  29 Jul 2021regression problems. Our assessment includes both real-world datasets with no known
                                   model form as well as ground-truth benchmark problems, including physics equations
                                   and systems of ordinary differential equations. For the real-world datasets, we benchmark
                                   the ability of each method to learn models with low error and low complexity relative to
                                   state-of-the-art machine learning methods. For the synthetic problems, we assess each
                                   method‚Äôs ability to Ô¨Ånd exact solutions in the presence of varying levels of noise. Under
                                   these controlled experiments, we conclude that the best performing methods for real-world
                                   regression combine genetic algorithms with parameter estimation and/or semantic search
                                   drivers. When tasked with recovering exact equations in the presence of noise, we Ô¨Ånd that
                                   deep learning and genetic algorithm-based approaches perform similarly. We provide a
                                   detailed guide to reproducing this experiment and contributing new methods, and encourage
                                   other researchers to collaborate with us on a common and living symbolic regression
                                   benchmark.
                             ‚àócorresponding author
                             ‚Ä†Department of Automatics and Robotics, AGH University of Science and Technology, Krakow, Poland
                             ‚Ä°Center for Mathematics, Computation and Cognition | Heuristics, Analysis and Learning Laboratory
                          Preprint. Under review.
                        1   Introduction
                        Symbolic regression (SR) is an approach to machine learning (ML) in which both the parameters
                        and structure of an analytical model are optimized. SR can be useful when one wishes to describe a
                        process via a mathematical expression, especially a simple expression; thus, it is often applied in the
                        hopes of producing a model of a process that, by virtue of its simplicity, may be easy to interpret.
                        Interpretable ML is becoming increasingly important as model deployments in high stakes societal
                        applications such as Ô¨Ånance and medicine grow [1, 2]. Moreover, the mathematical expressions
                        produced by SR are well-suited to be analyzed and controlled for their out-of-distribution behavior
                        (e.g., in terms of asymptotic behavior, periodicity, etc.). These attractive properties of SR have led to
                        its application in a number of areas, such as physics [3], biology [4], clinical informatics [5], climate
                        modeling [6], Ô¨Ånance [7], and many Ô¨Åelds of engineering [8‚Äì10].
                        SR literature has, in general, fallen short of evaluating and ranking new methods in a way that
                        facilitates their widespread adoption. Our view is that this shortcoming largely stems from a lack of
                        standardized, transparent and reproducible benchmarks, especially those that test a large and diverse
                        array of problems [11]. Although community surveys [11, 12] have led to suggestions for improving
                        benchmarking standards, and even black-listed certain problems, contemporary literature continues
                        to be published that violates those standards. Absent these standards, it is difÔ¨Åcult to assess which
                        methods or family of methods should be considered ‚Äústate-of-the-art‚Äù (SotA).
                        AchievingaÔ¨ÇeetingsenseofSotAiscertainlynotthesingularpursuitofmethodsresearch,yetwithout
                        common,robust benchmarking studies, promising avenues of investigation cannot be well-informed
                        by empirical evidence. We hope the benchmarking platform introduced in this paper improves
                        the cross-pollination between research communities interested in SR, which include evolutionary
                        computation, physics, engineering, statistics, and more traditional machine learning disciplines.
                        In this paper, we describe a large benchmarking effort that includes a dataset repository curated for
                        SR, as well as a benchmarking library designed to allow researchers to easily contribute methods. To
                        achieve this, we incorporated 130 datasets with ground truth forms into the Penn Machine Learning
                        Benchmark(PMLB)[13],includingmetadatadescribing the underlying equations, their units, and
                                                                                                                4
                        various summary statistics. Furthermore, we created a SR benchmark repository called SRBench
                        and sought contributions from researchers in this area. Here we describe this process and the results,
                        which consist of comparisons of 14 contemporary SR methods on hundreds of regression problems.
                        Toourknowledge, this is by far the largest and most comprehensive SR benchmark effort to date,
                        which allows us to make claims concerning current SotA methods for SR with better certainty.
                        Importantly, and in contrast to many previous efforts, the datasets, methods, benchmarking code,
                        and results are completely open-source, reproducible, and revision-controlled, which should allow
                        SRBenchtoexist as a living benchmark for future studies.
                        2   BackgroundandMotivation
                                                                 ÀÜ    ÀÜ    d
                        ThegoalofSRistolearnamappingyÀÜ(x) = œÜ(x,Œ∏) : R ‚Üí Rusingadatasetofpairedexamples
                                      N                      d
                        D={(x,y)} ,withfeaturesx ‚àà R andtargety. SR assumes the existence of an analytical
                                 i  i i=1
                                                  ‚àó     ‚àó
                        model of the form y(x) = œÜ (x,Œ∏ ) +  that would generate the observations in D, and seeks to
                        estimate this model by searching the space of expressions, œÜ, and parameters, Œ∏, in the presence of
                        white noise, .
                           4https://github.com/EpistasisLab/srbench
                                                                    2
                        Koza [14] introduced SR as an application of genetic programming (GP), a Ô¨Åeld that investigates
                        the use of genetic algorithms (GAs) to evolve executable data structures, i.e. programs. In the
                        case of so-called ‚ÄúKoza-style‚Äù GP, the programs to be optimized are syntax trees consisting of
                        functions/operations over input features and constants. Like in other GAs, GP is a process that
                        evolves a population of candidate solutions (e.g., syntax trees) by iteratively producing offspring from
                        parent solutions (e.g., by swapping parents‚Äô subtrees) and eliminating unÔ¨Åt solutions (e.g., programs
                        with sub-par behavior). Most SR research to date has emerged from within this sub-Ô¨Åeld and its
                                             5
                        associated conferences.
                        Despite the availability of post-hoc methods for explaining black-box model predictions [15], there
                        have been recent calls to focus on learning interpretable/transparent models explicitly [2]. Perhaps
                        due to this renewed interest in model interpretability, entirely different methods for tackling SR
                        have been proposed [16‚Äì22]. These include methods based in Bayesian optimization [16], recurrent
                        neural networks (RNNs) [17], and physics-inspired divide-and-conquer strategies [18, 23]. Some
                        of these papers refer to Eureqa, a commercial, GP-based SR method used to re-discover known
                        physics equations [3], as the ‚Äúgold standard‚Äù for SR [17] and/or the best method for SR ‚Äúby far‚Äù [18].
                        However, Schmidt and Lipson [24] make no claim to being the SotA method for SR, nor is this
                        hypothesis tested in the body of work on which Eureqa is based [25].
                        Although commercial platforms like Eureqa and Wolfram [26] are successful tools for applying
                        SR, they are not designed to support controlled benchmark experiments, and therefore experiments
                        utilizing them have serious caveats. Due to the design of the front-end API for both tools, it is not
                        possible to benchmark either method against others while holding important parameters of such an
                        experiment constant, including the computational effort, number of model evaluations, CPU/memory
                        limits, and Ô¨Ånal solution assessment. More generally, researchers cannot uniquely determine which
                        features of the software and/or experiment lead to observed differences in performance, given that
                        these commercial tools are closed-source. In this light, it is not clear what insights are to be gained
                        when comparing to Eureqa and Wolfram beyond a simple head-to-head comparison. Therefore,
                        rather than benchmark against Eureqa in this paper, we implement its underlying algorithms in an
                        open-source package, which allows our experiment to remain transparent, reproducible, accessible,
                        and controlled. We discuss the algorithms underlying Eureqa in detail in Sec. A.3.
                        Aclose reading of SR literature since 2009 implies that a number of proposed methods would
                        outperform Eureqa in controlled tests, and are therefore suitable choices for benchmarking (e.g. [27,
                        28]). Unfortunately, the widespread adoption of these promising SR approaches is hamstrung by a
                        lack of consensus on good benchmark problems, testing frameworks, and experimental designs. Our
                        effort to establish a common benchmark is motivated by our view that common, robust, standardized
                        benchmarksforSRcouldspeedprogressintheÔ¨Åeldbyprovidingaclearbaselinefromwhichtoassert
                        the quality of new approaches. Consider the NN community‚Äôs focus on common benchmarks (e.g.
                        ImageNet [29]), frameworks (e.g. TensorFlow, PyTorch) and experiment designs. By contrast, it is
                        commontoobserveresultsinSRliterature that are based on a small number of low dimensional, easy
                        and unrealistic problems, comparing only to very basic GP systems such as those described in [14]
                        nearly thirty years ago. Despite detailed descriptions of these issues [11], community surveys and
                        proposals to ‚Äúblack-list" toy problems [12], toy datasets and comparisons to out-dated SR methods
                        continue to appear in contemporary literature.
                        Theaspects of performance assessment for SR differ from typical regression benchmarking due to the
                        interest in obtaining concise, symbolic expressions. In general, the trade-off between accuracy and
                           5Anon-exhaustive list: GECCO, EuroGP, FOGA, PPSN, and IEEE CEC.
                                                                    3
                        simplicity must be considered when evaluating the merits of different models. Furthermore, model
                        simplicity, typically measured as sparsity or model size, is but a proxy for model interpretability; a
                        simple model may still be un-interpretable, or simply wrong [30‚Äì32]. With these concerns in mind,
                        datasets with ground truth solutions are useful, in that they allow researchers to assess whether or
                        not the symbolic model regressed by a given method corresponds to a known analytical solution.
                        Nevertheless, benchmarks utilizing synthetic datasets with ground-truth solutions are not sufÔ¨Åcient
                        for assessing real-world performance, and so we consider it essential to also evaluate the performance
                        of SR on real-world or otherwise black-box regression problems, relative to SotA ML methods.
                        There have been a few recent efforts to benchmark SR algorithms [33], including a precursor to this
                        workbenchmarkingfourSRmethodson94regressionproblems[34]. Inbothcases,SRmethodswere
                        assessed solely on their ability to make accurate predictions. In contrast, Udrescu and Tegmark [18]
                        proposed 120 new synthetic, physics-based datasets for SR, but compared only to Eureqa and only in
                        terms of solution rates. A major contribution of our work is its signiÔ¨Åcantly more comprehensive
                        scope than previous studies. We include 14 SR methods on 252 datasets in comparison to 7 ML
                        methods. Our metrics of comparison are also more comprehensive, and include 1) accuracy, 2)
                        simplicity, and 3) exact or approximate symbolic matches to the ground truth process. Furthermore,
                        wehavemadethebenchmarkopenlyavailable,reproducible, and open for contributions supported by
                        continuous integration [35].
                        3   SRBench
                        Wecreated SRBench to be a reproducible, open-source benchmarking project by pulling together a
                        large set of diverse benchmark datasets, contemporary SR methods, and ML methods around a shared
                        model evaluation and analysis environment. SRBench overcomes several of the issues in current
                        benchmarking literature as described in Sec. 2. For example, it makes it easy for methodologists
                        to benchmark new algorithms over hundreds of problems, in comparison to strong, contemporary
                        reference methods. These improvements allow us to reason with more certainty than in previous
                        workaboutthe SotA methods for SR.
                        In order to establish common datasets, we extended PMLB, a repository of standardized regression
                        and classiÔ¨Åcation problems [13, 36], by adding 130 SR datasets with known model forms. PMLB
                        provides utilities for fetching and handling data, recording and visualizing dataset metadata, and
                        contributing new datasets. The SR methods we benchmarked are all contemporary implementations
                        (2011-2020)fromseveralmethodfamilies,asshowninTbl.1. Werequiredcontributorstoimplement
                        a minimal, Scikit-learn compatible [37], Python API for their method. In addition, contributors
                        were required to provide the Ô¨Ånal Ô¨Åtted model as a string that was compatible with the symbolic
                        mathematics library sympy. Note that although we require a Python wrapper, SR implementations
                        in many different languages are supported, as long as the Python API is available and the language
                                                               6
                        environment can be managed via Anaconda .
                        Toensure reproducibility, we deÔ¨Åned a common environment (via Anaconda) with Ô¨Åxed versions of
                        packages and their dependencies. In contrast to most SR studies, the full installation code, experiment
                        code, results and analysis are available via the repository for use in future studies. To make SRBench
                        as extensible as possible, we automated the process of incorporating new methods and results into
                        the analysis pipeline. The repository accepts rolling contributions of new methods that meet the
                        minimal API requirements. To achieve this, we created a continuous integration (CI) [35] framework
                        that assures contributions are compatible with the benchmark code as they arrive. CI also supports
                           6https://www.anaconda.com/
                                                                   4
                                          Table 1: Short descriptions of the SR methods benchmarked in our experiment, including references
                                          and links to implementations.
                                            Method                  Year      Description                                         MethodFamily                               Implementation
                                            AFP[38]                 2011      Age-Ô¨Åtness Pareto Optimization                      GP                                      C++/Python (link)
                                            AFP_FE[24]              2011      AFPwithco-evolved Ô¨Åtness estimates;                 GP                                      C++/Python (link)
                                                                              Eureqa-esque
                                            AIFeynman[23]           2020      Physics-inspired method                             Divide and conquer                   Fortran/Python (link)
                                            BSR[16]                 2020      Bayesian Symbolic Regression                        MarkovChainMonteCarlo                         Python (link)
                                            DSR[17]                 2020      DeepSymbolicRegression                              Recurrent neural networks         Python (PyTorch) (link)
                                            EPLEX[39]               2016      -lexicase selection                                GP                                      C++/Python (link)
                                            FEAT[40]                2019      Feature Engineering Automation Tool                 GP                                      C++/Python (link)
                                            FFX[41]                 2011      Fast function extraction                            Randomsearch                            C++/Python (link)
                                            GP-GOMEA[42]            2020      GPversion of the Gene-pool Optimal                  GP                                      C++/Python (link)
                                                                              Mixing Evolutionary Algorithm
                                            gplearn                 2015      Koza-style symbolic regression in Python            GP                                      C++/Python (link)
                                            ITEA[43]                2020      Interaction-Transformation EA                       GP                                   Haskell/Python (link)
                                            MRGP[44]                2014      Multiple Regression Genetic Programming             GP                                               Java (link)
                                            Operon[45]              2019      SRwithNon-linear least squares                      GP                                      C++/Python (link)
                                            SBP-GP[46]              2019      Semantic Back-propagation Genetic                   GP                                      C++/Python (link)
                                                                              Programming
                                          Table 2:        Settings used in the benchmark experiments. ‚ÄúTotal comparisons" refers to the total
                                          evaluatons of an algorithm on a dataset for a given noise level and random seed.
                                                              Setting                       Black-box Problems                       Ground-truth Problems
                                                              No. of datasets               122                                      130
                                                              No. of algorithms             21 (14 SR, 7 ML)                         14
                                                              No. of trials per dataset     10                                       10
                                                              Train/test Split              .75/.25                                  .75/.25
                                                              Hyperparameter Tuning         5-fold Halving Grid Search CV            Tuned set from Black-box problems
                                                              Termination criteria          500k evaluations/train or 48 hours       1Mevaluations or 8 hours
                                                              Levels of target noise        None                                     0, 0.001, 0.01, 0.1
                                                              Total comparisons             26840                                    54600
                                                              ComputingBudget               1.29Mcorehours                           436.8K core hours
                                          continuous updates to results reporting and visualization whenever new experiments are available,
                                          allowing us to maintain a standing leader-board of contemporary SR methods. Ideally these features
                                         will quicken the adoption of SotA approaches throughout the SR research community. Further details
                                          onhowtouseandcontributetoSRBenchareprovidedinSec.A.1.
                                          4     ExperimentDesign
                                         Weevaluated SR methods on two separate tasks. First, we assessed their ability to make accurate
                                          predictions on ‚Äúblack-box‚Äù regression problems (in which the underlying data generating function
                                          remains unknown) while minimizing the complexity of the discovered models. Second, we tested
                                          the ability of each method to Ô¨Ånd exact solutions to synthetic datasets with known, ground-truth
                                          functions, originating from physics and various Ô¨Åelds of engineering.
                                          Thebasic experiment settings are summarized in Tbl. 2. Each algorithm was trained on each dataset
                                         (and level of noise, for ground-truth problems) in 10 repeated trials with a different random state
                                          that controlled both the train/test split and the seed of the algorithm. Datasets were split 75/25%
                                          in training and testing. For black-box regression problems, each algorithm was tuned using 5-fold
                                          cross validation with halving grid search. The SR algorithms were limited to 6 hyperparameter
                                          combinations; the ML methods were allowed more, as shown in Tbls. 4-6. The best hyperparameter
                                          settings were used to tune a Ô¨Ånal estimator and evaluate it according to the metrics described above.
                                          Details for running the experiment are given in Sec. A.1.
                                                                                                                    5
                           4.1  Symbolic Regression Methods
                           Here we characterize the SR methods summarized in Tbl. 1 by describing how they Ô¨Åt into broader
                           research trends within the SR Ô¨Åeld. The most traditional implementation of GP-based SR we test is
                           gplearn, which initializes a random population of programs/models, and then iterates through the
                           steps of tournament selection, mutation and crossover.
                           Pareto optimization methods [8, 47‚Äì49] are popular evolutionary strategies that exploit Pareto
                           dominance relations to drive the population of models towards a set of efÔ¨Åcient trade-offs between
                           competing objectives. Half of the SR methods we test use Pareto optimization in some form during
                           training. Age-Fitness Pareto optimization (AFP), proposed by Eureqa‚Äôs authors Schmidt and Lipson
                           [38], uses a model‚Äôs age as an objective in order to reduce premature convergence as well as bloat [50].
                           AFP_FEcombinesAFPwithEureqa‚ÄôsmethodforÔ¨Åtnessestimation[51]. Thus we expect AFP_FE
                           and AFPtoperformsimilarly to Eureqa as described in literature.
                           Anotherpromisinglineofresearchhasbeentoleverageprogramsemantics(inthiscase,theequation‚Äôs
                           intermediate and Ô¨Ånal outputs over training samples) more heavily during optimization, rather than
                           compressing that information into aggregate Ô¨Åtness values [52]. -lexicase selection (EPLEX) [27] is
                           a parent selection method that utilizes semantics to conduct selection by Ô¨Åltering models through ran-
                           domized subsets of cases, which rewards models that perform well on difÔ¨Åcult regions of the training
                           data. EPLEX is also used as the parent selection method in FEAT [40]. Semantic backpropagation
                           (SBP) is another semantic technique to compute, for a given target value and a tree node position,
                           that value which makes the output of the model match the target (i.e., the label) [53‚Äì55]. Here, we
                           evaluate the (SBP-GP) algorithm by Virgolin et al. [46] which improves SBP-based recombination
                           bydynamically adapting intermediate outputs using afÔ¨Åne transformations.
                           Backpropagation-based gradient descent was proposed for GP-SR by Topchy and Punch [56], but
                           tends to appear less often than stochastic hill climbing (e.g. [3, 57]). More recent studies [45, 58]
                           have made a strong case for the use of gradient-based constant optimization as an improvement over
                           stochastic and evolutionary approaches. The aforementioned studies are embodied by Operon, a
                           GPmethod that incorporates non-linear least squares constant optimization using the Levenberg-
                           Marquadt algorithm [59].
                           In addition to the question of how to best optimize constants, a line of research has proposed different
                           waysofdeÔ¨Åningprogramand/or model encodings. The methods FEAT, MRGP, ITEA, and FFX each
                           impose additional structural assumptions on the models being evolved. In FEAT, each model is a
                           linear combination of a set of evolved features, the parameters of which are encoded as edges and
                           optimized via gradient descent. In MRGP [44], the entire program trace (i.e., each subfunction of
                           the model) is decomposed into features and used to train a Lasso model. In ITEA, each model is
                           an afÔ¨Åne combination of interaction-transformation expressions, which compose a unary function
                           (the transformation) and a polynomial function (the interaction) [43, 60]. Finally, FFX [41] simply
                           initializes a population of equations, selects the Pareto optimal set, and returns a single linear model
                           bytreating the population of equations as features.
                           GP-GOMEAisaGPalgorithmwhererecombinationisadaptedovertime[42,61]. Everygeneration,
                           GP-GOMEAbuilds a statistical model of interdependencies within the encoding of the evolving
                           programs, and then uses this information to recombine interdependent blocks of components, as to
                           preserve their concerted action.
                           Jin et al. [16] recently proposed Bayesian Symbolic Regression (BSR), in which a prior is placed
                           on tree structures and the posterior distributions are sampled using a Markov Chain Monte Carlo
                                                                          6
                           (MCMC)method. As in GP-based SR, arithmetic expressions are expressed with symbolic trees,
                           although BSRexplicitly deÔ¨Ånes the Ô¨Ånal model form as a linear combination of several symbolic trees.
                           Modelparsimony is encouraged by specifying a prior that presumes additive, linear combinations of
                           small components.
                           DeepSymbolicRegression (DSR) [17] uses reinforcement learning to train a generative RNN model
                           of symbolic expressions. Expressions sampled from the model distribution are assessed to create a
                           reward signal. DSR introduces a variant of the Monte Carlo policy gradient algorithm [62] dubbed a
                          ‚Äúrisk-seeking policy gradient" in an effort to bias the generative model towards exact expressions.
                           AIFeynmanisadivide-and-conquer approach that recursively applies a set of solvers and problem
                           decomposition heuristics to build a symbolic model [18]. If the problem is not directly solve-able
                           by polynomial Ô¨Åtting or brute-force search, AIFeynman trains a NN on the data and uses it to
                           estimate functional modularities (e.g., symmetry and/or separability), which are used to partition the
                           data into simpler problems and recurse. An updated version of the algorithm, which we test here,
                           integrates Pareto optimization with an information-theoretic complexity metric to improve robustness
                           to noise [23].
                           4.2  Datasets
                           All of the benchmark datasets are summarized by number of instances and number of features in
                           Fig. 5. The problems range from 47 to 1 million instances, and two to 124 features. We used
                          122black-box regression problems available in PMLB v.1.0. These problems are pulled from, and
                           overlap with, various open-source repositories, including OpenML [63] and the UCI repository [64].
                           PMLBstandardizes these datasets to a common format and provides fetching functions to load them
                           into Python (and R). The black-box regression datasets consist of 46 ‚Äúreal-world" problems (i.e.,
                           observational data collected from physical processes) and 76 synthetic problems (i.e., data generated
                           computationallyfromstaticfunctionsorsimulations). Theblack-boxproblemscoverdiversedomains,
                           including health informatics (11), business (10), technology (10), environmental science (11) and
                           government (12); in addition, they are derived from varied data sources, including human subjects
                           (14), environmental observations (11), government studies (12), and economic markets (7). The
                           datasets can be browsed by their properties at epistasislab.github.io/pmlb. Each dataset includes
                           metadata describing source information as well as a detailed proÔ¨Åle page summarizing the data
                           distributions and interactions (here is an example).
                           WeextendedPMLBwith130datasetswithknown,ground-truthmodelforms. Thesedatasets were
                           used to assess the ability of SR methods to recover known process physics. The 130 datasets came
                           from two sources: the Feynman Symbolic Regression Database, and the ODE-Strogatz repository.
                           Both sets of data come from Ô¨Årst principles models of physical systems. The Feynman problems
                           originate in the Feynman Lectures on Physics [65], and the datasets were recently created and
                           proposed as SR benchmarks [18]. Whereas the Feynman datasets represent static systems, the
                           Strogatz problems are non-linear and chaotic dynamical processes [66]. Each dataset is one state of a
                           2-state system of Ô¨Årst-order, ordinary differential equations (ODEs). They were used to benchmark
                           SRmethodsinpreviouswork[25,67],andaredescribed in more detail in Sec. A.4 and Tbl. 3.
                                                                          7
                           4.3  Metrics
                           Accuracy WeassessedaccuracyusingthecoefÔ¨Åcient of determination, deÔ¨Åned as
                                                                        P
                                                                           N(y ‚àíyÀÜ)2
                                                             R2 =1‚àí        i   i    i  .
                                                                        P
                                                                           N(y ‚àíy¬Ø)2
                                                                           i   i    i
                           Complexity    Anumberofdifferent complexity measures have been proposed for SR, including
                           those based on syntactic complexity (i.e. related to the complexity of the symbolic model); those
                           based on semantic complexity (i.e. related to the behavior of the model over the data) [23, 68];
                           those using both deÔ¨Ånitions [69]; and those estimating complexity via meta-learning [70]. The
                           pros and cons of these methods and their relation to notions of interpretability is a point of discus-
                           sion [71]. For the sake of simplicity, we opted to deÔ¨Åne complexity as the number of mathematical
                           operators, features and constants in the model, where the mathematical operators are in the set
                           {+,‚àí,‚àó,/,sin,cos,arcsin,arccos,exp,log,pow,max,min}. In addition to calculating the com-
                           plexity of the raw model forms returned by each method, we calculated the complexity of the models
                           after simplifying via sympy.
                           Solution Criteria  For the ground-truth regression problems, we used the following solution deÔ¨Åni-
                           tion.
                                                                         ÀÜ    ÀÜ
                           DeÔ¨Ånition 4.1 (Symbolic Solution). A model œÜ(x,Œ∏) is a Symbolic Solution to a problem with
                                                      ‚àó     ‚àó         ÀÜ
                           ground-truth model y = œÜ (x,Œ∏ ) + , if œÜ does not reduce to a constant, and if either of the
                                                            ‚àó   ÀÜ             ‚àó ÀÜ
                           following conditions are true: 1) œÜ ‚àí œÜ = a; or 2) œÜ /œÜ = b,b 6= 0, for some constants a and b.
                           This deÔ¨Ånition is designed to capture models that differ from the true model by a constant or
                           scalar. Prior to assessing symbolic solutions, each model underwent sympy simpliÔ¨Åcation, as did
                           the conditions above. Relative to accuracy metrics, the Symbolic Solution metric is a more faithful
                           evaluation of the ability of an SR method to discover the data generating process. However, because
                           models can be represented in myriad ways, and sympy‚Äôs simpliÔ¨Åcation procedure is non-optimal, we
                           cannot guarantee that all symbolic solutions are captured with perfect Ô¨Ådelity by this metric.
                           5   Results
                           Themediantest set performance on all problems and methods for the black-box benchmark problems
                           is summarized in Fig. 1. Across the problems, we Ô¨Ånd that the models generated by Operon are
                           signiÔ¨Åcantly more accurate than any other method‚Äôs models in terms of test set R2 (p ‚â§6.5e-05).
                           SBP-GPandFEATranksecondandthirdandattainsimilaraccuracies,althoughthemodelsproduced
                           byFEATaresigniÔ¨Åcantly smaller (p =9.2e-22).
                           WenotethatfourofthetopÔ¨Åvemethods(Operon,SBP-GP,FEAT,EPLEX)andsixofthetopten
                           methods (GP-GOMEA,ITEA)areGP-basedSRmethods. Theothertopmethodsareensembletree-
                           based methods, including two popular gradient-boosting algorithms, XGBoost and LightGBM [72,
                           73]); Random Forest [74]; and AdaBoost [75]. Among these methods, Operon, FEAT and SBP-GP
                           signiÔ¨Åcantly outperform and LightGBM (p ‚â§1.3e-07) and Operon and SBP-GP outperform XGBoost
                           (p ‚â§1.3e-04). We also note ITEA‚Äôs overall accuracy is not signiÔ¨Åcantly different from RandomForest
                           or AdaBoost. Of note, the models produced by the top Ô¨Åve SR methods (aside from SBP-GP)
                           are 1-3 orders of magnitude smaller than models produced by the ensemble tree-based approaches
                           (p ‚â§1.3e-21).
                                                                          8
                                                              2
                                                            R  Test
                                                                                            Model Size                    Training Time (s)
                                         *Operon
                                         *SBP-GP
                                           *FEAT
                                          *EPLEX
                                             XGB
                                           LGBM
                                     *GP-GOMEA
                                        AdaBoost
                                   RandomForest
                                            *ITEA
                                         *AFP_FE
                                            *AFP
                                            *FFX
                                     KernelRidge
                                            *DSR
                                          *MRGP
                                         *gplearn
                                             MLP
                                           Linear
                                            *BSR
                                     *AIFeynman
                                                                                                       4                                  4
                                                                                           2                             0       2
                                                                                                     10                                10
                                               ‚àí0.25 0.00 0.25 0.50 0.75 1.00
                                                                                         10                           10       10
                                  Figure 1: Results on the black-box regression problems. Points indicate the mean of the median test
                                  set performance on all problems, and bars show the 95% conÔ¨Ådence interval. Methods marked with
                                  an asterisk are SR methods.
                                                        RandomForest
                                      20
                                                    XGB                       MRGP*
                                                             AdaBoost
                                                                                                                         Feynman                Strogatz
                                                      LGBM
                                                                                                     AIFeynman
                                                                        MLP
                                                                                                         AFP_FE
                                      15
                                                                FFX*
                                                                                                            DSR
                                                                                    AIFeynman*
                                                                                                             AFP
                                          SBP-GP*
                                                      KernelRidge
                                                                                                         gplearn
                                                FEAT*
                                                                ITEA*
                                                                                                            ITEA
                                      10
                                                                                                          EPLEX
                                                    EPLEX*
                                           Operon*
                                                                                                      GP-GOMEA
                                     Model Size Rank
                                                                                                         Operon
                                                                            AFP_FE*
                                                                     AFP*
                                                                                                         SBP-GP
                                       5
                                                                                                                                               Target Noise
                                                   GP-GOMEA*
                                                                                                             BSR
                                                                          gplearn*
                                                                                                                                                      0.0
                                                                                                           MRGP
                                                                                                                                                      0.001
                                                                                       BSR*
                                                                                                                                                      0.01
                                                                                                             FFX
                                                                           DSR*       Linear
                                                                                                                                                      0.1
                                                                                                            FEAT
                                       0
                                                                                                                  0      25     50        0     25     50
                                         0            5           10          15           20
                                                                2
                                                                                                                   Solution Rate (%)      Solution Rate (%)
                                                              R  Test Rank
                                  Figure 2:      Pareto plot comparing the rankings of Figure 3:                   Solution rates for the ground-truth
                                  SRmethodsintermsofmodelsizeandR2 scoreon regressionproblems. Color/shapeindicateslevel
                                  the black-box problems. Points denote median rank- of noise added to the target variable.
                                  ings and the bars denote 95% conÔ¨Ådence intervals.
                                  ConnectinglinesandcolordenoteParetodominance
                                  rankings.
                                                                                               9
                   Amongthenon-GP-basedSRalgorithms,FFXandDSRperformsimilarlytoeachother(p =0.76)
                   and signiÔ¨Åcantly better than BSR and AIFeynman (p ‚â§6.1e-05). FFX trains more quickly than
                   DSR,although DSRproduces some of the smallest solutions, akin to penalized regression. We note
                   that AIFeynman performs poorly on these problems, suggesting that not many of them exhibit the
                   qualities of physical systems (rotational/translational invariance, symmetry, etc.) that AIFeynman
                   wasdesigned to exploit. Additional statistical comparisons are given in Figs. 9-11.
                   In Fig. 2, we illustrate the performance of the methods on the black-box problems when accuracy and
                   simplicity are considered simultaneously. The optimal Pareto front for these two objectives (solid
                   line) is composed of three methods: Operon, GP-GOMEA, and DSR, which taken together give the
                   set of best trade-offs between accuracy and simplicity across the black-box regression problems.
                   Performance on the ground-truth regression problems is summarized in Fig. 3, with methods sorted
                   bytheir median solution rate and grouped by data source (Feynman or Strogatz). On average, when
                   the target is free of noise, we observe that AIFeynman identiÔ¨Åes exact solutions 53% of the time,
                   nearly twice as often as the next closest method (GP-GOMEA, 27%). However, at noise levels above
                   0.01, four other methods recover exact solutions more often: DSR, gplearn, AFP_FE, and AFP. Taken
                   together, the black-box and ground-truth regression results suggest AIFeynman may be brittle in
                   application to real-world and/or noisy data, yet its performance with little to no noise is signiÔ¨Åcant
                   for the Feynman problems. On the Strogatz datasets, AIFeynman‚Äôs performance is not signiÔ¨Åcantly
                   different than other methods, and indeed there are few signiÔ¨Åcant differences in performance between
                   the top 10 methods at any noise level. We note that the best method on real-world data, Operon,
                   struggles to recover solutions to these problems, despite Ô¨Ånding many candidate solutions with near
                   prefect test set scores. See Sec. A.6-A.7 for additional analysis.
                   6  Discussion and Conclusions
                   This paper introduces a SR benchmarking framework that allows objective comparisons of contempo-
                   rary SR methods on a wide range of diverse regression problems. We have found that, on real-world
                   and black-box regression tasks, contemporary GP-based SR methods (e.g. Operon) outperform
                   new SR methods based in other Ô¨Åelds of optimization, and can also perform as well as or better
                   than gradient boosted trees while producing simpler models. On synthetic ground-truth physics and
                   dynamical systems problems, we have veriÔ¨Åed that AIFeynman Ô¨Ånds exact solutions signiÔ¨Åcantly
                   better than other methods when noise is minimal; otherwise, both deep learning-based methods (DSR)
                   and GP-based SR methods (e.g. AFP_FE) perform best.
                   WeseeclearwaystoimproveSRBenchbyimprovingthedatasetcuration,experimentdesign and
                   analysis. For one, we have not benchmarked the methods in a setting that allows them to exploit
                   parallelism, which may change relative run-times. There are also many promising SR methods not
                   included in this study that we hope to add in future revisions. In addition, whereas our benchmark
                   includes real-world data as well as simulated data with ground-truth models, it does not include
                   real-world data from phenomena with known, Ô¨Årst principles models (e.g., observations of a mass-
                   spring-damper system). Data such as these could help us better evaluate the ability of SR methods to
                   discover relations under real-world conditions. We intend to include these data in future versions,
                   given the evidence that SR models can sometimes discover unexpected analytical models that
                   outperform the expert models in a Ô¨Åeld (e.g., in studies of yeast metabolism [76] and Ô¨Çuid tank
                   systems [67]). As a Ô¨Ånal note, our current study highlights orthogonal approaches to SR that show
                   promise, and in future work we hope to explore whether combinations of proposed methods (e.g.,
                   non-linear parameter optimization plus semantic search drivers) would have synergistic effects.
                                                   10
             Acknowledgments
             William La Cava was supported by NIH/NLM grant K99-LM012926. He would like to thank Curt
             Calafut and the Penn Medicine Academic Computing Services (PMACS), as well as the PLGrid
             Infrastructure, for supporting the computational experiments. He also thanks members of the Epistasis
             Labfor their patience, and Joseph D. Romano for coming through in a pinch.
             Ying Jin would like to thank Doctor Jian Guo for hosting an internship for the project and Professor
             Jian Kang for helpful and inspiring guidance in Bayesian statistics.
             TheauthorswouldalsoliketothankJamesMcDermottforhisgenerouscontributionstotherepository,
             and Randal Olson and Weixuan Fu for their initial push to integrate regression benchmarking into
             PMLB.Authorsdeclarenocompetinginterests.
             References
             [1] Anna Jobin, Marcello Ienca, and Effy Vayena. The global landscape of AI ethics guidelines. Nature
               MachineIntelligence, 1(9):389‚Äì399, September 2019. ISSN 2522-5839. doi: 10.1038/s42256-019-0088-2.
             [2] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use
               interpretable models instead. Nature Machine Intelligence, 1(5):206‚Äì215, 2019.
             [3] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science, 324
               (5923):81‚Äì85, 2009.
             [4] Michael Douglas Schmidt and Hod Lipson. Automated modeling of stochastic reactions with large
               measurement time-gaps. In Proceedings of the 13th Annual Conference on Genetic and Evolutionary
               Computation, pages 307‚Äì314. ACM, 2011.
             [5] William La Cava, Paul C. Lee, Imran Ajmal, Xiruo Ding, Priyanka Solanki, Jordana B. Cohen, Jason H.
               Moore, and Daniel S. Herman. Application of concise machine learning to construct accurate and
               interpretable EHR computable phenotypes. medRxiv, page 2020.12.12.20248005, February 2021. doi:
               10.1101/2020.12.12.20248005.
             [6] Karolina Stanislawska, Krzysztof Krawiec, and Zbigniew W. Kundzewicz. Modeling global temperature
               changes with genetic programming. Computers & Mathematics with Applications, 64(12):3717‚Äì3728,
               December2012. ISSN0898-1221. doi: 10.1016/j.camwa.2012.02.049.
             [7] Shu-Heng Chen. Genetic Algorithms and Genetic Programming in Computational Finance. Springer
               Science & Business Media, 2012.
             [8] Guido F. Smits and Mark Kotanchek. Pareto-front exploitation in symbolic regression. In Genetic
               ProgrammingTheoryandPracticeII, pages 283‚Äì299. Springer, 2005.
             [9] William La Cava, Kourosh Danai, Lee Spector, Paul Fleming, Alan Wright, and Matthew Lackner.
               AutomaticidentiÔ¨Åcationofwindturbinemodelsusingevolutionarymultiobjectiveoptimization. Renewable
               Energy, 87, Part 2:892‚Äì902, March 2016. ISSN 0960-1481. doi: 10.1016/j.renene.2015.09.068.
             [10] Mauro Castelli, Sara Silva, and Leonardo Vanneschi. A C++ framework for geometric semantic genetic
               programming. GeneticProgrammingandEvolvableMachines,16(1):73‚Äì81,March2015. ISSN1389-2576,
               1573-7632. doi: 10.1007/s10710-014-9218-0.
             [11] James McDermott, David R. White, Sean Luke, Luca Manzoni, Mauro Castelli, Leonardo Vanneschi,
               Wojciech Jaskowski, Krzysztof Krawiec, Robin Harper, and Kenneth De Jong. Genetic programming
               needs better benchmarks. In Proceedings of the Fourteenth International Conference on Genetic and
               Evolutionary Computation Conference, pages 791‚Äì798. ACM, 2012.
                                  11
                          [12] DavidR.White,JamesMcDermott,MauroCastelli,LucaManzoni,BrianW.Goldman,GabrielKronberger,
                                          ¬¥
                               Wojciech Jaskowski, Una-May O‚ÄôReilly, and Sean Luke. Better GP benchmarks: Community survey
                               results and proposals. Genetic Programming and Evolvable Machines, 14(1):3‚Äì29, December 2012. ISSN
                               1389-2576, 1573-7632. doi: 10.1007/s10710-012-9177-2.
                          [13] Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore. PMLB:
                               ALargeBenchmarkSuiteforMachineLearningEvaluationandComparison. BioData Mining, 2017.
                          [14] John R. Koza. Genetic Programming: On the Programming of Computers by Means of Natural Selection.
                               MITPress, Cambridge, MA, USA, 1992. ISBN 0-262-11170-5.
                          [15] Scott M Lundberg and Su-In Lee. A UniÔ¨Åed Approach to Interpreting Model Predictions. page 10.
                          [16] Ying Jin, Weilin Fu, Jian Kang, Jiadong Guo, and Jian Guo.   Bayesian Symbolic Regression.
                               arXiv:1910.08892 [stat], January 2020.
                          [17] Brenden K. Petersen, Mikel Landajuela Larma, Terrell N. Mundhenk, Claudio Prata Santiago, Soo Kyung
                               Kim,andJoanneTaeryKim. Deepsymbolicregression: Recovering mathematical expressions from data
                               via risk-seeking policy gradients. In International Conference on Learning Representations, September
                               2020.
                          [18] Silviu-Marian Udrescu and Max Tegmark. AI Feynman: A Physics-Inspired Method for Symbolic
                               Regression. arXiv:1905.11481 [hep-th, physics:physics], April 2020.
                          [19] MaysumPanju. Automated Knowledge Discovery Using Neural Networks. 2021.
                          [20] Matthias Werner, Andrej Junginger, Philipp Hennig, and Georg Martius. Informed Equation Learning.
                               arXiv preprint arXiv:2105.06331, 2021.
                          [21] Subham Sahoo, Christoph Lampert, and Georg Martius. Learning equations for extrapolation and control.
                               In International Conference on Machine Learning, pages 4442‚Äì4450. PMLR, 2018.
                          [22] Matt J. Kusner, Brooks Paige, and Jos√© Miguel Hern√°ndez-Lobato. Grammar variational autoencoder. In
                               International Conference on Machine Learning, pages 1945‚Äì1954. PMLR, 2017.
                          [23] Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, and Max Tegmark. AI
                               Feynman2.0: Pareto-optimal symbolic regression exploiting graph modularity. arXiv:2006.10782 [physics,
                               stat], December 2020.
                          [24] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science, 324
                               (5923):81‚Äì85, 2009.
                          [25] Michael Douglas Schmidt. Machine Science: Automated Modeling of Deterministic and Stochastic
                               Dynamical Systems. PhD thesis, Cornell University, Ithaca, NY, USA, 2011.
                          [26] Giorgia Fortuna. Automatic Formula Discovery in the Wolfram Language ‚Äì from Wolfram Library Archive.
                               https://library.wolfram.com/infocenter/Conferences/9329/, 2015.
                          [27] William La Cava, Lee Spector, and Kourosh Danai. Epsilon-Lexicase Selection for Regression. In
                               Proceedings of the Genetic and Evolutionary Computation Conference 2016, GECCO ‚Äô16, pages 741‚Äì748,
                               NewYork,NY,USA,2016.ACM. ISBN978-1-4503-4206-3. doi: 10.1145/2908812.2908898.
                          [28] Pawe\l Liskowski and Krzysztof Krawiec. Discovery of Search Objectives in Continuous Domains. In
                               Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ‚Äô17, pages 969‚Äì976, New
                               York, NY, USA, 2017. ACM. ISBN 978-1-4503-4920-8. doi: 10.1145/3071178.3071344.
                          [29] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
                               image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248‚Äì255.
                               Ieee, 2009.
                                                                        12
             [30] Zachary C Lipton. The Mythos of Model Interpretability: In machine learning, the concept of interpretabil-
               ity is both important and slippery. Queue, 16(3):31‚Äì57, 2018.
             [31] Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Wortman Vaughan,
               and Hanna Wallach. Manipulating and measuring model interpretability. In Proceedings of the 2021 CHI
               Conference on Human Factors in Computing Systems, pages 1‚Äì52, 2021.
             [32] Marco Virgolin, Andrea De Lorenzo, Francesca Randone, Eric Medvet, and Mattias Wahde. Model
               learning with personalized interpretability estimation (ML-PIE). arXiv:2104.06060 [cs], 2021.
             [33] Jan ≈Ωegklitz and Petr Po≈°√≠k. Benchmarking state-of-the-art symbolic regression algorithms. Genetic
               ProgrammingandEvolvableMachines, pages 1‚Äì29, 2020.
             [34] Patryk Orzechowski, William La Cava, and Jason H. Moore. Where are we now? A large benchmark study
               of recent symbolic regression methods. In Proceedings of the 2018 Genetic and Evolutionary Computation
               Conference, GECCO ‚Äô18, April 2018. doi: 10.1145/3205455.3205539.
             [35] Martin Fowler. Continuous Integration. https://martinfowler.com/articles/continuousIntegration.html,
               2006.
             [36] Joseph D. Romano, Trang T. Le, William La Cava, John T. Gregg, Daniel J. Goldberg, Natasha L. Ray,
               Praneel Chakraborty, Daniel Himmelstein, Weixuan Fu, and Jason H. Moore. PMLB v1.0: An open source
               dataset collection for benchmarking machine learning methods. arXiv:2012.00058 [cs], April 2021.
             [37] Fabian Pedregosa, Ga√´l Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
               Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in
               Python. Journal of Machine Learning Research, 12(Oct):2825‚Äì2830, 2011.
             [38] Michael Schmidt and Hod Lipson. Age-Ô¨Åtness pareto optimization. In Genetic Programming Theory and
               Practice VIII, pages 129‚Äì146. Springer, 2011.
             [39] William La Cava, Thomas Helmuth, Lee Spector, and Jason H. Moore. A probabilistic and multi-objective
               analysis of lexicase selection and epsilon-lexicase selection. Evolutionary Computation, 27(3):377‚Äì402,
               September 2019. ISSN 1063-6560. doi: 10.1162/evco_a_00224.
             [40] William La Cava, Tilak Raj Singh, James Taggart, Srinivas Suri, and Jason H. Moore. Learning concise
               representations for regression by evolving networks of trees. In International Conference on Learning
               Representations, ICLR, 2019.
             [41] Trent McConaghy. FFX: Fast, scalable, deterministic symbolic regression technology. In Genetic
               ProgrammingTheoryandPracticeIX, pages 235‚Äì260. Springer, 2011.
             [42] Marco Virgolin, Tanja Alderliesten, Cees Witteveen, and Peter A N Bosman. Improving model-based
               genetic programming for symbolic regression of small expressions. Evolutionary Computation, page tba,
               2020.
             [43] F. O. de Franca and G. S. I. Aldeia. Interaction-Transformation Evolutionary Algorithm for Symbolic
               Regression. Evolutionary Computation, pages 1‚Äì25, December 2020. ISSN 1063-6560. doi: 10.1162/
               evco_a_00285.
             [44] Ignacio Arnaldo, Krzysztof Krawiec, and Una-May O‚ÄôReilly. Multiple regression genetic programming.
               In Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation, pages 879‚Äì886.
               ACM,2014.
             [45] Michael Kommenda, Bogdan Burlacu, Gabriel Kronberger, and Michael Affenzeller. Parameter identiÔ¨Åca-
               tion for symbolic regression using nonlinear least squares. Genetic Programming and Evolvable Machines,
               December2019. ISSN1573-7632. doi: 10.1007/s10710-019-09371-3.
                                  13
             [46] Marco Virgolin, Tanja Alderliesten, and Peter AN Bosman. Linear scaling with and within semantic
               backpropagation-based genetic programming for symbolic regression. In Proceedings of the Genetic and
               Evolutionary Computation Conference, pages 1084‚Äì1092, 2019.
             [47] Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, and T Meyarivan. A Fast Elitist Non-dominated Sorting
               Genetic Algorithm for Multi-objective Optimization: NSGA-II. In Marc Schoenauer, Kalyanmoy Deb,
               G√ºnther Rudolph, Xin Yao, Evelyne Lutton, Juan Julian Merelo, and Hans-Paul Schwefel, editors, Parallel
               Problem Solving from Nature PPSN VI, volume 1917, pages 849‚Äì858. Springer Berlin Heidelberg, Berlin,
               Heidelberg, 2000. ISBN 978-3-540-41056-0.
             [48] Eckart Zitzler, Marco Laumanns, and Lothar Thiele. SPEA2: Improving the Strength Pareto Evolutionary
               Algorithm. Eidgen√∂ssische Technische Hochschule Z√ºrich (ETH), Institut f√ºr Technische Informatik und
               Kommunikationsnetze (TIK), 2001.
             [49] S. Bleuler, M. Brack, L. Thiele, and E. Zitzler. Multiobjective genetic programming: Reducing bloat
               using SPEA2. In Proceedings of the 2001 Congress on Evolutionary Computation, 2001, volume 1, pages
               536‚Äì543 vol. 1, 2001. doi: 10.1109/CEC.2001.934438.
             [50] Gregory S. Hornby. ALPS: The age-layered population structure for reducing the problem of premature
               convergence. In Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation,
               GECCO‚Äô06,pages815‚Äì822, New York, NY, USA, 2006. ACM. ISBN 1-59593-186-4. doi: 10.1145/
               1143997.1144142.
             [51] M.D. Schmidt and H. Lipson. Coevolution of Fitness Predictors. IEEE Transactions on Evolutionary
               Computation, 12(6):736‚Äì749, December 2008. ISSN 1941-0026, 1089-778X. doi: 10.1109/TEVC.2008.
               919006.
             [52] Raja MuhammadAtif Azad. Krzysztof Krawiec: Behavioral program synthesis with genetic programming.
               Genetic ProgrammingandEvolvableMachines,18(1):111‚Äì113,March2017. ISSN1389-2576,1573-7632.
               doi: 10.1007/s10710-016-9283-7.
             [53] Bartosz Wieloch and Krzysztof Krawiec. Running programs backwards: Instruction inversion for effective
               search in semantic spaces. In Proceedings of the 15th Annual Conference on Genetic and Evolutionary
               Computation, pages 1013‚Äì1020, 2013.
             [54] Krzysztof Krawiec and Tomasz Pawlak. Approximating geometric crossover by semantic backpropagation.
               In Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation, pages 941‚Äì948,
               2013.
             [55] Tomasz P Pawlak, Bartosz Wieloch, and Krzysztof Krawiec. Semantic backpropagation for designing
               searchoperatorsingeneticprogramming. IEEETransactionsonEvolutionaryComputation,19(3):326‚Äì340,
               2014.
             [56] Alexander Topchy and William F. Punch. Faster genetic programming based on local gradient search of
               numeric leaf values. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-
               2001), pages 155‚Äì162, 2001.
             [57] J.C. Bongard and H. Lipson. Nonlinear System IdentiÔ¨Åcation Using Coevolution of Models and Tests.
               IEEETransactions on Evolutionary Computation, 9(4):361‚Äì384, August 2005. ISSN 1089-778X. doi:
               10.1109/TEVC.2005.850293.
             [58] Michael Kommenda, Gabriel Kronberger, Stephan Winkler, Michael Affenzeller, and Stefan Wagner.
               Effects of constant optimization by nonlinear least squares minimization in symbolic regression. In
               Christian Blum, Enrique Alba, Thomas Bartz-Beielstein, Daniele Loiacono, Francisco Luna, Joern Mehnen,
               Gabriela Ochoa, Mike Preuss, Emilia Tantar, and Leonardo Vanneschi, editors, GECCO ‚Äô13 Companion:
               Proceeding of the Fifteenth Annual Conference Companion on Genetic and Evolutionary Computation
               Conference Companion, pages 1121‚Äì1128, Amsterdam, The Netherlands, 6. ACM. doi: doi:10.1145/
               2464576.2482691.
                                  14
             [59] Bogdan Burlacu, Gabriel Kronberger, and Michael Kommenda. Operon C++ an efÔ¨Åcient genetic pro-
               gramming framework for symbolic regression. In Proceedings of the 2020 Genetic and Evolutionary
               Computation Conference Companion, pages 1562‚Äì1570, 2020.
             [60] Fabr√≠cio Olivetti de Fran√ßa. A greedy search tree heuristic for symbolic regression. Information Sciences,
               442:18‚Äì32, 2018.
             [61] MarcoVirgolin,TanjaAlderliesten, CeesWitteveen, andPeterANBosman. Scalablegeneticprogramming
               bygene-pool optimal mixing and input-space entropy-based building-block learning. In Proceedings of
               the Genetic and Evolutionary Computation Conference, pages 1041‚Äì1048, 2017.
             [62] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
               learning. Machine learning, 8(3-4):229‚Äì256, 1992.
             [63] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. OpenML: Networked Science in
               MachineLearning. SIGKDDExplorations, 15(2):49‚Äì60, 2013. doi: 10.1145/2641190.2641198.
             [64] M. Lichman. UCI Machine Learning Repository. University of California, Irvine, School of Information
               and Computer Sciences, 2013.
             [65] Richard P. Feynman, Robert B. Leighton, and Matthew Sands. The Feynman Lectures on Physics, Vol. I:
               The New Millennium Edition: Mainly Mechanics, Radiation, and Heat. Basic Books, September 2015.
               ISBN978-0-465-04085-8.
             [66] Steven H Strogatz. Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry,
               and Engineering. Westview press, 2014.
             [67] William La Cava, Kourosh Danai, and Lee Spector. Inference of compact nonlinear dynamic models by
               epigenetic local search. Engineering Applications of ArtiÔ¨Åcial Intelligence, 55:292‚Äì306, October 2016.
               ISSN0952-1976. doi: 10.1016/j.engappai.2016.07.004.
             [68] E.J. Vladislavleva, G.F. Smits, and D. den Hertog. Order of Nonlinearity as a Complexity Measure for
               Models Generated by Symbolic Regression via Pareto Genetic Programming. IEEE Transactions on
               Evolutionary Computation, 13(2):333‚Äì349, 2009. ISSN 1089-778X. doi: 10.1109/TEVC.2008.926486.
             [69] Michael Kommenda, Gabriel Kronberger, Michael Affenzeller, Stephan M. Winkler, and Bogdan Burlacu.
               Evolving Simple Symbolic Regression Models by Multi-objective Genetic Programming. In Genetic
               Programming Theory and Practice, volume XIV of Genetic and Evolutionary Computation. Springer, Ann
               Arbor, MI, 2015.
             [70] Marco Virgolin, Andrea De Lorenzo, Eric Medvet, and Francesca Randone. Learning a formula of
               interpretability to learn interpretable formulas. In International Conference on Parallel Problem Solving
               from Nature, pages 79‚Äì93. Springer, 2020.
             [71] W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. DeÔ¨Ånitions, methods,
               and applications in interpretable machine learning. Proceedings of the National Academy of Sciences, 116
               (44):22071‚Äì22080, 10 2019-10-29. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1900654116.
             [72] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd
               AcmSigkddInternational Conference on Knowledge Discovery and Data Mining, pages 785‚Äì794. ACM,
               2016.
             [73] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.
               Lightgbm: A highly efÔ¨Åcient gradient boosting decision tree. Advances in neural information processing
               systems, 30:3146‚Äì3154, 2017.
             [74] Leo Breiman. Random forests. Machine learning, 45(1):5‚Äì32, 2001.
                                  15
             [75] Robert E. Schapire. The boosting approach to machine learning: An overview. In Nonlinear Estimation
               and ClassiÔ¨Åcation, pages 149‚Äì171. Springer, 2003.
             [76] Michael D Schmidt, Ravishankar R Vallabhajosyula, Jerry W Jenkins, Jonathan E Hood, Abhishek S Soni,
               John P Wikswo, and Hod Lipson. Automated reÔ¨Ånement and inference of analytical models for metabolic
               networks. Physical Biology, 8(5):055011, October 2011. ISSN 1478-3975. doi: 10.1088/1478-3975/8/5/
               055011.
             [77] Michael Schmidt and Hod Lipson. Comparison of Tree and Graph Encodings As Function of Problem
               Complexity. In Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation,
               GECCO‚Äô07, pages 1674‚Äì1679, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-697-4. doi:
               10.1145/1276958.1277288.
             [78] Grant Dick, Caitlin A. Owen, and Peter A. Whigham. Feature standardisation and coefÔ¨Åcient optimisation
               for effective symbolic regression. In Proceedings of the 2020 Genetic and Evolutionary Computation
               Conference, GECCO ‚Äô20, pages 306‚Äì314, Canc√∫n, Mexico, June 2020. Association for Computing
               Machinery. ISBN 978-1-4503-7128-5. doi: 10.1145/3377930.3390237.
             [79] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing Fairness Gerrymandering:
               Auditing and Learning for Subgroup Fairness. arXiv:1711.05144 [cs], December 2018.
             [80] William La Cava and Jason H. Moore. Genetic programming approaches to learning fair classiÔ¨Åers. In
               Proceedings of the 2020 Genetic and Evolutionary Computation Conference, GECCO ‚Äô20, 2020. doi:
               10.1145/3377930.3390157.
             [81] Jerome H Friedman. Greedy function approximation: A gradient boosting machine. Annals of statistics,
               pages 1189‚Äì1232, 2001.
             [82] JanezDem≈°ar. Statistical Comparisons of ClassiÔ¨Åers over Multiple Data Sets. Journal of Machine Learning
               Research, 7(Jan):1‚Äì30, 2006. ISSN ISSN 1533-7928.
             Checklist
               1. For all authors...
                 (a) Do the main claims made in the abstract and introduction accurately reÔ¨Çect the paper‚Äôs
                   contributions and scope? [Yes] The results can be veriÔ¨Åed by visiting our repository.
                   SpeciÔ¨Åc claims are supported by statistical tests.
                 (b) Did you describe the limitations of your work? [Yes] See discussion and conclusions.
                 (c) Did you discuss any potential negative societal impacts of your work? [Yes] See
                   Sec. A.4.
                 (d) Have you read the ethics review guidelines and ensured that your paper conforms to
                   them? [Yes] In addition to releasing the benchmark in a transparent way, we discuss
                   ethics in Sec. A.4.
               2. If you ran experiments (e.g. for benchmarks)...
                 (a) Did you include the code, data, and instructions needed to reproduce the main ex-
                   perimental results (either in the supplemental material or as a URL)? [Yes] See
                   https://github.com/EpistasisLab/srbench.
                 (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
                   were chosen)? [Yes] See Table 2.
                 (c) Did you report error bars (e.g., with respect to the random seed after running experi-
                   ments multiple times)? [Yes] See Figs. 1-3 for example.
                                  16
               (d) Did you include the total amount of compute and the type of resources used (e.g., type
                 of GPUs, internal cluster, or cloud provider)? [Yes] see Appendix
              3. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
               (a) If your work uses existing assets, did you cite the creators? [Yes]
               (b) Did you mention the license of the assets? [Yes]
               (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
               (d) Did you discuss whether and how consent was obtained from people whose data you‚Äôre
                 using/curating? [Yes] Datasets are released under an MIT license.
               (e) Didyoudiscusswhetherthedatayouareusing/curatingcontainspersonallyidentiÔ¨Åable
                 information or offensive content? [Yes] These datasets do not contain personally
                 identiÔ¨Åable information.
                               17
                        A Appendix
                        Please refer to https://github.com/EpistasisLab/srbench/ for the most up-to-date guide to
                        SRBench.
                        A.1  RunningtheBenchmark
                        TheREADMEinourGithubrepositoryincludesthesetofcommandstoreproducethebenchmark
                        experiment, which are summarized here. Experiments are launched from the experiments/ folder
                        via the script analyze.py. The script can be conÔ¨Ågured to run the experiment in parallel locally,
                        onanLSFjobscheduler,oronaSLURMjobscheduler. Toseethefullsetofoptions,runpython
                        analyze.py -h.
                        After installing and conÔ¨Åguring the conda environment, the complete black-box experiment can be
                        started via the command:
                                python analyze.py /path/to/pmlb/datasets -n_trials 10 -results
                               ../results -time_limit 48:00
                        Similarly, the ground-truth regression experiment for Strogatz datasets and a target noise of 0.0 are
                        run by the command:
                                python analyze.py -results ../results_sym_data -target_noise
                               0.0 "/path/to/pmlb/datasets/strogatz*" -sym_data -n_trials 10
                               -time_limit 9:00 -tuned
                        A.2  Contributing a Method
                        Aliving version of the method contribution instructions are described in the Contribution Guide. To
                        illustrate the simplicity of contributing a method, Figure 4 shows the script submitted for Bayesian
                        Symbolic Regression [16]. In addition to the code snippet, authors may either add their code package
                        to the conda/pip environment, or provide an install script. When a pull request is issued by a
                        contributor, new methods and installs are automatically tested on a minimal version of the benchmark.
                        Once the tests pass and the method is approved by the benchmark maintainers, the contribution
                        becomes part of the resource and can be tested via the commands above.
                        A.3  Additional Background and Motivation
                                                                                                           7
                        Eureqa EureqaisacommercialGP-basedSRsoftwarethatwasacquiredbyDataRobotin2017 .
                        Duetoits closed-source nature and incorporation into the DataRobot platform, it is impossible to
                        benchmark its performance while controlling for important experimental variables such as number
                        of evaluations, space and time limits, population size, and so forth. However, the novel algorithmic
                        aspects of Eureqa are rooted in a number of ablation studies [38, 51, 77] that we summarize here.
                        First is its use of directed acyclic graphs for representing equations in lieu of trees, which resulted in
                        morespace-efÔ¨Åcientmodelencodingrelativetotrees, withoutasigniÔ¨Åcantdifferenceinaccuracy[77].
                        The most signiÔ¨Åcant improvement over traditional tournament-based selection is Eureqa‚Äôs use of
                        age-Ô¨Åtness Pareto optimization (AFP), a method in which random restarts are incorporated each
                        generation as new offspring, and are protected from competing with older, more Ô¨Åt equations by
                        including age as an objective to be minimized [38]. Eureqa also includes the co-evolution of Ô¨Åtness
                          7https://www.datarobot.com/nutonian/
                                                                 18
                                         1   # method: Bayesian Symbolic Regression
                                         2   # contributor: Ying Jin
                                         3   # source: https://github.com/ying531/MCMC-SymReg
                                         4   from bsr.bsr_class import BSR
                                         5
                                         6   hyper_params = []
                                         7   for val, itrNum in zip([100,500,1000],[5000,1000,500]):
                                         8         for treeNum in [3,6]:
                                         9                hyper_params.append(
                                        10                                   {'treeNum': [treeNum],
                                        11                                    'itrNum': [itrNum],
                                        12                                    'val': [val],
                                        13                                   })
                                        14   # default estimator
                                        15   est = BSR(val=100, itrNum=5000, treeNum=3, alpha1=0.4, alpha2=0.4,
                                        16                   beta=-1, disp=False, max_time=2*60*60)
                                        17
                                        18   def complexity(est):
                                        19   """returns final model complexity"""
                                        20         return est.complexity()
                                        21
                                        22   def model(est):
                                        23   """returns final model as string"""
                                        24         return est.model()
                                     Figure 4: An example code contribution, deÔ¨Åning the estimator, its hyperparameters, and functions
                                     to return the complexity and symbolic model.
                                     predictors, in which Ô¨Åtness assignment is sped up by optimizing a second population of training
                                     sample indices that best distinguish between equations in the population [51]. Unfortunately we
                                     cannot guarantee that Eureqa currently uses any of these reported algorithms for SR, due to its closed-
                                     source nature. We chose instead to benchmark known algorithms (AFP, AFP_FE) with open-source
                                     implementations, hoping that the resulting study‚Äôs conclusions may better inform future methods
                                     development. We note that AFP has been outperformed by a number of other optimization methods
                                     in controlled studies since its release (e.g., [27, 28]).
                                     Constant optimization in Genetic Programming                           Oneoftheclearest improvements over Koza-
                                     style GP has been the adoption of local search methods to handle constant optimization distinctly
                                     from evolutionary learning. Regarding the optimization of constants in GP, several reasons can
                                     explain why backpropagation and gradient descent can be considered to be relatively under-used
                                     in GP (compared to, e.g., evolutionary neural architecture search). For example, early works often
                                     ignored the use of feature standardization (e.g., by z-scoring), the lack of which can harm gradient
                                     propagation [78]. Next to this, GP relies on crafting compositions out of a multitude of operations,
                                     someofwhicharepronetocausevanishingorexploding gradients. Last but not least, to the best of
                                     our knowledge, the Ô¨Åeld lacks a comprehensive study that provides guidelines for the appropriate
                                     hyperparameters for constant optimization (learning rate schedule, iterations, batch size, etc.), and
                                     howtoeffectively balance parameter learning with the evolutionary process.
                                     A.4     Additional Dataset Information
                                     All datasets, including metadata, are available from PMLB. Each dataset is stored using Git Large
                                     File Storage and PMLB is planned for long-term maintenance. PMLB is available under an MIT
                                                                                                     19
                                                   3
                                                 10
                                                             black-box
                                                             feynman
                                                             strogatz
                                                   2
                                                 10
                                                No. of Features
                                                   1
                                                 10
                                                                                         4            5
                                                               2            3                                      6
                                                                                       10           10
                                                             10           10                                    10
                                                                              No. of Samples
                                                        Figure 5: Distribution of dataset sizes in PMLB.
                             license, and is described in detail in Romano et al. [36]. The authors bear all responsibility in case of
                             violation of rights.
                             Dataset Properties      Thedistribution of dataset sizes by samples and features are shown in Fig. 5.
                             Datasets vary in size from tens to millions of samples, and up to thousands of features. The datasets
                             can be navigated and inspected in the repository documentation.
                             Ethical Considerations and Intended Uses           PMLBis intended to be used as a framework for
                             benchmarking ML and SR algorithms and as a resource for investigating the structure of datasets.
                             This paper does not contribute new datasets, but rather collates and standardizes datasets that were
                             already publicly available. In that regard, we do not foresee SRBench as creating additional ethical
                             issues around their use. Nevertheless, it is worth noting that PMLB contains well-known, real-world
                             datasets from UCI and OpenML for which ethical considerations are important, such as the USCrime
                             dataset. Whereas we would view the risk of harm arising speciÔ¨Åcally from this dataset to be low (the
                             data is from 1960), it is exemplary of a task for which algorithmic decision making could exacerbate
                             existing biases in the criminal justice system. As such it is used as a benchmark in a number of
                             papers in the ML fairness literature (e.g. [79, 80]). None of the datasets herein contain personally
                             identiÔ¨Åable information.
                             Feynmandatasets TheFeynmanbenchmarksweresourcedfromtheFeynmanSymbolicRegres-
                             sion Database. We standardized the Feynman and Bonus equations to PMLB format and included
                             metadata detailing the model form and the units for each variable. We used the version of the
                             equations that were not simpliÔ¨Åed by dimensional analysis. Udrescu and Tegmark [18] describe each
                                                        5                                       6
                             dataset as containing 10 rows, but each actually contains 10 . Given this discrepancy and after
                             noting that sub-sampling did not signiÔ¨Åcantly change the correlation structure of any of the problems,
                             eachdataset was down-sampledfrom1millionsamplesto100,000tolowerthecomputationalburden.
                             Wealso observed that Eqn. II.11.17 was missing from the database. Finally, we excluded three
                             datasets from our analysis that contained arcsin and arccos functions, as these were not implemented
                             in the majority of SR algorithms we tested.
                                                                                 20
                                      Strogatz datasets            The Strogatz datasets were sourced from the ODE-Strogatz repository [67].
                                      Each dataset is one state of a 2-state system of Ô¨Årst-order, ordinary differential equations (ODEs).
                                      The goal of each problem is to predict rate of change of the state given the current two states on
                                      which it depends. Each represents natural processes that exhibit chaos and non-linear dynamics. The
                                      problems were originally adapted from [66] by Schmidt [25]. In order to simulate their behavior,
                                      initial conditions were chosen within stable basins of attraction. Each system was simulated using
                                      Simulink, and the simulation code is available in the repository above. The equations for each of
                                      these datasets are shown in Table 3.
                                                                                Table 3: The Strogatz ODE problems.
                                                                Name                                     Target
                                                                Bacterial Respiration                    xÀô = 20 ‚àí x ‚àí       x¬∑y 2
                                                                                                                          1+0.5¬∑x
                                                                                                         yÀô = 10 ‚àí      x¬∑y 2
                                                                                                                     1+0.5¬∑x
                                                                                                         Àô
                                                                Bar Magnets                              Œ∏ = 0.5 ¬∑ sin(Œ∏ ‚àí œÜ) ‚àí sin(Œ∏)
                                                                                                         Àô
                                                                                                         œÜ=0.5¬∑sin(œÜ‚àíŒ∏)‚àísin(œÜ)
                                                                Glider                                   vÀô = ‚àí0.05 ¬∑ v2 ‚àí sin(Œ∏)
                                                                                                         Àô
                                                                                                         Œ∏ = v ‚àícos(Œ∏)/v
                                                                                                                                    2
                                                                Lotka-Volterra interspecies dynamics     xÀô = 3 ¬∑ x ‚àí 2 ¬∑ x ¬∑ y ‚àí x
                                                                                                         yÀô = 2 ¬∑ y ‚àí x ¬∑ y ‚àí y2
                                                                                                                              y 
                                                                Predator Prey                            xÀô = x ¬∑4 ‚àí x ‚àí 1+x         
                                                                                                         yÀô = y ¬∑    x ‚àí0.075¬∑y
                                                                                                                    1+x
                                                                                                         Àô
                                                                Shear Flow                               Œ∏ = cot(œÜ)¬∑cos(Œ∏)
                                                                                                         Àô         2                 2    
                                                                                                         œÜ= cos (œÜ)+0.1¬∑sin (œÜ) ¬∑sin(Œ∏)
                                                                                                                         1      3      
                                                                van der Pol oscillator                   xÀô = 10 ¬∑ y ‚àí 3 ¬∑ (x ‚àí x)
                                                                                                         yÀô = ‚àí 1 ¬∑ x
                                                                                                                 10
                                      AddingNoise Whitegaussiannoisewasaddedtothetargetasafractionofthesignal root mean
                                      square value. In other words, for target noise level Œ≥,
                                                                                                               r1X 2!
                                                                            y         =y+, ‚àºN 0,Œ≥                                y
                                                                              noise                                      N          i
                                      A.5     Additional Experiment Details
                                      Experiments were run in a heterogeneous cluster computing environment composed of hosts with
                                      24-28 core Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz processors and 250 GB of RAM. Jobs
                                      consisted of the training of each method on a single dataset for a Ô¨Åxed random seed. Each job received
                                      one CPUcoreandupto16GBofRAM,andwastime-limitedasshowninTable2. Fortheground-
                                      truth problems, the Ô¨Ånal models from each method were given an additional hour of computing time
                                      with 8GB of RAMtobesimpliÔ¨Åedwithsympyandassessedbythesolutioncriteria (see Def. 4.1).
                                      For the black-box problems, if a job was killed due to the time limit, we re-ran the experiment without
                                      hyperparameter tuning, thereby only requiring a single training iteration to complete within 48 hours.
                                      To ease the computational burden for large datasets, training data exceeding 10,000 samples was
                                      randomly subset to 10,000 rows; test set predictions were still evaluated over the entire test fold.
                                      Thehyperparameter settings for each method are shown in Tables 4-6. Each SR method was tuned
                                      from a set of six hyperparameter combinations. The most common parameter setting chosen during
                                      the black-box regression experiments was then used as the ‚Äútuned" version of each algorithm for
                                                                                                         21
                                           Table 4: ML methods and the hyperparameter spaces used in tuning.
                              Method             Hyperparameters
                              AdaBoost           {‚Äôlearning_rate‚Äô: (0.01, 0.1, 1.0, 10.0), ‚Äôn_estimators‚Äô: (10, 100, 1000)}
                              KernelRidge        {‚Äôkernel‚Äô: (‚Äôlinear‚Äô, ‚Äôpoly‚Äô, ‚Äôrbf‚Äô, ‚Äôsigmoid‚Äô), ‚Äôalpha‚Äô: (0.0001, 0.01, 0.1, 1), ‚Äôgamma‚Äô: (0.01,
                                                 0.1, 1, 10)}
                              LassoLars          {‚Äôalpha‚Äô: (0.0001, 0.001, 0.01, 0.1, 1)}
                              LGBM               {‚Äôn_estimators‚Äô: (10, 50, 100, 250, 500, 1000), ‚Äôlearning_rate‚Äô: (0.0001, 0.01, 0.05, 0.1, 0.2),
                                                ‚Äôsubsample‚Äô: (0.5, 0.75, 1), ‚Äôboosting_type‚Äô: (‚Äôgbdt‚Äô, ‚Äôdart‚Äô, ‚Äôgoss‚Äô)}
                              LinearRegression   {‚ÄôÔ¨Åt_intercept‚Äô: (True,)}
                              MLP                {‚Äôactivation‚Äô: (‚Äôlogistic‚Äô, ‚Äôtanh‚Äô, ‚Äôrelu‚Äô), ‚Äôsolver‚Äô: (‚Äôlbfgs‚Äô, ‚Äôadam‚Äô, ‚Äôsgd‚Äô), ‚Äôlearning_rate‚Äô:
                                                 (‚Äôconstant‚Äô, ‚Äôinvscaling‚Äô, ‚Äôadaptive‚Äô)}
                              RandomForest       {‚Äôn_estimators‚Äô: (10, 100, 1000), ‚Äômin_weight_fraction_leaf‚Äô: (0.0, 0.25, 0.5), ‚Äômax_features‚Äô:
                                                 (‚Äôsqrt‚Äô, ‚Äôlog2‚Äô, None)}
                              SGD                {‚Äôalpha‚Äô: (1e-06, 0.0001, 0.01, 1), ‚Äôpenalty‚Äô: (‚Äôl2‚Äô, ‚Äôl1‚Äô, ‚Äôelasticnet‚Äô)}
                              XGB                {‚Äôn_estimators‚Äô: (10, 50, 100, 250, 500, 1000), ‚Äôlearning_rate‚Äô: (0.0001, 0.01, 0.05, 0.1, 0.2),
                                                ‚Äôgamma‚Äô: (0, 0.1, 0.2, 0.3, 0.4), ‚Äôsubsample‚Äô: (0.5, 0.75, 1)}
                            the ground-truth problems, with updates to 1) include any mathematical operators needed for those
                            problems and 2) double the evaluation budget.
                            A.6   Additional Results
                            A.6.1   Subgroupanalysis of black-box regression results
                            Manyoftheblack-box problems for regression in PMLB were originally sourced from OpenML.
                            Afewauthorshavenotedthatseveral of these datasets are sourced from Friedman [81]‚Äôs synthetic
                            benchmarks. These datasets are generated by non-linear functions that vary in degree of noise,
                            variable interactions, variable importance, and degree of non-linearity. Due to their number, they may
                            have an out-sized effect on results reporting in PMLB. In Fig. 6, we separate out results on this set
                            of problems relative to the rest of PMLB. We do Ô¨Ånd that, relative to the rest of PMLB, the results
                            onthe Friedman datasets distinguish top-ranked methods more strongly than among the rest of the
                            benchmark, on which performance between top-performing methods is more similar. In general,
                            although we do see methods rankings change somewhat when looking at speciÔ¨Åc data groupings, we
                            do not observe large differences. An exception is Kernel ridge regression, which performs poorly on
                            the Friedman datasets but very well on the rest of PMLB. We recommend that future revisions to
                            PMLBexpandthedatasetcollection to minimize the effect of any one source of data, and include
                            subgroup analysis to identify which types of problems are best solved by speciÔ¨Åc methods.
                            Toget a better sense of the performance variability across methods and datasets, method rankings on
                            each dataset are bi-clustered and visualized in Fig. 7. Methods that perform most similarly across
                            the benchmark are placed adjacent to each other, and likewise datasets that induce similar method
                            rankings are grouped. We note some expected groupings Ô¨Årst: AFP and AFP_FE, which differ
                            only in Ô¨Åtness estimation, and FEAT and EPLEX, which use the same selection method, perform
                            similarly. We also observe clustering among the Friedman datasets (names beginning with ‚Äúfri_"),
                            and again note stark differences between methods that perform well on these problems, e.g. Operon,
                            SBP-GP, and FEAT, and those that do not, e.g. MLP. This view of the results also reveals a cluster of
                            SRmethods(AFP,AFP_FE,DSR,gplearn)that perform well on a subset of real-world problems
                            (analcatdata_neavote_523 - vineyard_192) for which linear models also perform well. Interestingly,
                            for that problem subset, Operon‚Äôs performance is mediocre relative to its strong performance on other
                            datasets. We also note with surprise that DSR and gplearn exhibit performance similarity on par with
                                                                             22
                          Table 5:   Part 1: SR methods and the hyperparameter spaces used in tuning on the black-box
                          regression problems.
                                     Method     Hyperparameters
                                     AFP        {‚Äôpopsize‚Äô: 100, ‚Äôg‚Äô: 2500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 100, ‚Äôg‚Äô: 2500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô,
                                               ‚Äôsin‚Äô, ‚Äôcos‚Äô]}
                                                {‚Äôpopsize‚Äô: 500, ‚Äôg‚Äô: 500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 500, ‚Äôg‚Äô: 500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô, ‚Äôsin‚Äô,
                                               ‚Äôcos‚Äô]}
                                                {‚Äôpopsize‚Äô: 1000, ‚Äôg‚Äô: 250, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 1000, ‚Äôg‚Äô: 250, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô,
                                               ‚Äôsin‚Äô, ‚Äôcos‚Äô]}
                                     AIFeynman  {‚ÄôBF_try_time‚Äô: 60, ‚ÄôNN_epochs‚Äô: 4000, ‚ÄôBF_ops_Ô¨Åle_type‚Äô="10ops.txt"}
                                                {‚ÄôBF_try_time‚Äô: 60, ‚ÄôNN_epochs‚Äô: 4000, ‚ÄôBF_ops_Ô¨Åle_type‚Äô="14ops.txt"}
                                                {‚ÄôBF_try_time‚Äô: 60, ‚ÄôNN_epochs‚Äô: 4000, ‚ÄôBF_ops_Ô¨Åle_type‚Äô="19ops.txt"}
                                                {‚ÄôBF_try_time‚Äô: 600, ‚ÄôNN_epochs‚Äô: 400, ‚ÄôBF_ops_Ô¨Åle_type‚Äô="10ops.txt"}
                                                {‚ÄôBF_try_time‚Äô: 600, ‚ÄôNN_epochs‚Äô: 400, ‚ÄôBF_ops_Ô¨Åle_type‚Äô="14ops.txt"}
                                                {‚ÄôBF_try_time‚Äô: 600, ‚ÄôNN_epochs‚Äô: 400, ‚ÄôBF_ops_Ô¨Åle_type‚Äô="19ops.txt"}
                                     BSR        {‚ÄôtreeNum‚Äô: 6, ‚ÄôitrNum‚Äô: 500, ‚Äôval‚Äô: 1000}
                                                {‚ÄôtreeNum‚Äô: 6, ‚ÄôitrNum‚Äô: 1000, ‚Äôval‚Äô: 500}
                                                {‚ÄôtreeNum‚Äô: 3, ‚ÄôitrNum‚Äô: 500, ‚Äôval‚Äô: 1000}
                                                {‚ÄôtreeNum‚Äô: 6, ‚ÄôitrNum‚Äô: 5000, ‚Äôval‚Äô: 100}
                                                {‚ÄôtreeNum‚Äô: 3, ‚ÄôitrNum‚Äô: 5000, ‚Äôval‚Äô: 100}
                                                {‚ÄôtreeNum‚Äô: 3, ‚ÄôitrNum‚Äô: 1000, ‚Äôval‚Äô: 500}
                                     DSR        {‚Äôbatch_size‚Äô: array([ 10, 100, 1000, 10000, 100000])}
                                     EPLEX      {‚Äôpopsize‚Äô: 1000, ‚Äôg‚Äô: 250, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 500, ‚Äôg‚Äô: 500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 1000, ‚Äôg‚Äô: 250, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôsin‚Äô, ‚Äôcos‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô,
                                               ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 100, ‚Äôg‚Äô: 2500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 100, ‚Äôg‚Äô: 2500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôsin‚Äô, ‚Äôcos‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô,
                                               ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 500, ‚Äôg‚Äô: 500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôsin‚Äô, ‚Äôcos‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô,
                                               ‚Äôsqrt‚Äô]}
                                     FEAT       {‚Äôpop_size‚Äô: 100, ‚Äôgens‚Äô: 2500, ‚Äôlr‚Äô: 0.1}
                                                {‚Äôpop_size‚Äô: 100, ‚Äôgens‚Äô: 2500, ‚Äôlr‚Äô: 0.3}
                                                {‚Äôpop_size‚Äô: 500, ‚Äôgens‚Äô: 500, ‚Äôlr‚Äô: 0.1}
                                                {‚Äôpop_size‚Äô: 500, ‚Äôgens‚Äô: 500, ‚Äôlr‚Äô: 0.3}
                                                {‚Äôpop_size‚Äô: 1000, ‚Äôgens‚Äô: 250, ‚Äôlr‚Äô: 0.1}
                                                {‚Äôpop_size‚Äô: 1000, ‚Äôgens‚Äô: 250, ‚Äôlr‚Äô: 0.3}
                                     FE_AFP     {‚Äôpopsize‚Äô: 1000, ‚Äôg‚Äô: 250, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôsin‚Äô, ‚Äôcos‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô,
                                               ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 500, ‚Äôg‚Äô: 500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 1000, ‚Äôg‚Äô: 250, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 100, ‚Äôg‚Äô: 2500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 100, ‚Äôg‚Äô: 2500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôsin‚Äô, ‚Äôcos‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô,
                                               ‚Äô3‚Äô, ‚Äôsqrt‚Äô]}
                                                {‚Äôpopsize‚Äô: 500, ‚Äôg‚Äô: 500, ‚Äôop_list‚Äô: [‚Äôn‚Äô, ‚Äôv‚Äô, ‚Äô+‚Äô, ‚Äô-‚Äô, ‚Äô*‚Äô, ‚Äô/‚Äô, ‚Äôsin‚Äô, ‚Äôcos‚Äô, ‚Äôexp‚Äô, ‚Äôlog‚Äô, ‚Äô2‚Äô, ‚Äô3‚Äô,
                                               ‚Äôsqrt‚Äô]}
                          AFP/AFP_FE,andarethenextmostsimilar-performing methods (note the dendrogram connecting
                          these columns).
                          A.6.2   Extendedanalysis of ground-truth regression results
                          AsnotedinSec. 6, despite Operon‚Äôs good performance on black-box regression, it Ô¨Ånds few models
                          with symbolic equivalence. An alternative (and weaker) notion of solution is based on test set
                          accuracy, which we show in Fig. 8; by this metric, the relative method performance corresponds
                          moreclosely to that seen for black-box regression. We also note that methods that impose structural
                          assumptions on the model (BSR, FEAT, ITEA, FFX) are worse at Ô¨Ånding symbolic solutions, most of
                          which do not match those assumptions (e.g. most processes in Table 3).
                                                                        23
                                             Table 6:           Part 2: SR methods and the hyperparameter spaces used in tuning on the black-box
                                             regression problems.
                                                              Method                  Hyperparameters
                                                              GPGOMEA                 {‚Äôinitmaxtreeheight‚Äô: (4,), ‚Äôfunctions‚Äô: (‚Äô+_-_*_p/_plog_sqrt_sin_cos‚Äô,), ‚Äôpopsize‚Äô: (1000,),
                                                                                     ‚Äôlinearscaling‚Äô: (True,)}
                                                                                      {‚Äôinitmaxtreeheight‚Äô: (6,), ‚Äôfunctions‚Äô: (‚Äô+_-_*_p/_plog_sqrt_sin_cos‚Äô,), ‚Äôpopsize‚Äô: (1000,),
                                                                                     ‚Äôlinearscaling‚Äô: (True,)}
                                                                                      {‚Äôinitmaxtreeheight‚Äô: (4,), ‚Äôfunctions‚Äô: (‚Äô+_-_*_p/‚Äô,), ‚Äôpopsize‚Äô: (1000,), ‚Äôlinearscaling‚Äô:
                                                                                     (True,)}
                                                                                      {‚Äôinitmaxtreeheight‚Äô: (6,), ‚Äôfunctions‚Äô: (‚Äô+_-_*_p/‚Äô,), ‚Äôpopsize‚Äô: (1000,), ‚Äôlinearscaling‚Äô:
                                                                                     (True,)}
                                                                                      {‚Äôinitmaxtreeheight‚Äô: (4,), ‚Äôfunctions‚Äô: (‚Äô+_-_*_p/_plog_sqrt_sin_cos‚Äô,), ‚Äôpopsize‚Äô: (1000,),
                                                                                     ‚Äôlinearscaling‚Äô: (False,)}
                                                                                      {‚Äôinitmaxtreeheight‚Äô: (6,), ‚Äôfunctions‚Äô: (‚Äô+_-_*_p/_plog_sqrt_sin_cos‚Äô,), ‚Äôpopsize‚Äô: (1000,),
                                                                                     ‚Äôlinearscaling‚Äô: (False,)}
                                                              ITEA                    {‚Äôexponents‚Äô: ((-5, 5),), ‚Äôtermlimit‚Äô: ((2, 15),), ‚Äôtransfunctions‚Äô: (‚Äô[Id, Tanh, Sin, Cos, Log,
                                                                                      Exp, SqrtAbs]‚Äô,)}
                                                                                      {‚Äôexponents‚Äô: ((-5, 5),), ‚Äôtermlimit‚Äô: ((2, 5),), ‚Äôtransfunctions‚Äô: (‚Äô[Id, Tanh, Sin, Cos, Log,
                                                                                      Exp, SqrtAbs]‚Äô,)}
                                                                                      {‚Äôexponents‚Äô: ((-5, 5),), ‚Äôtermlimit‚Äô: ((2, 15),), ‚Äôtransfunctions‚Äô: (‚Äô[Id, Sin]‚Äô,)}
                                                                                      {‚Äôexponents‚Äô: ((0, 5),), ‚Äôtermlimit‚Äô: ((2, 15),), ‚Äôtransfunctions‚Äô: (‚Äô[Id, Sin]‚Äô,)}
                                                                                      {‚Äôexponents‚Äô: ((0, 5),), ‚Äôtermlimit‚Äô: ((2, 5),), ‚Äôtransfunctions‚Äô: (‚Äô[Id, Sin]‚Äô,)}
                                                                                      {‚Äôexponents‚Äô: ((0, 5),), ‚Äôtermlimit‚Äô: ((2, 15),), ‚Äôtransfunctions‚Äô: (‚Äô[Id, Tanh, Sin, Cos, Log,
                                                                                      Exp, SqrtAbs]‚Äô,)}
                                                              MRGP                    {‚Äôpopsize‚Äô: 1000, ‚Äôg‚Äô: 250, ‚Äôrt_cross‚Äô: 0.8, ‚Äôrt_mut‚Äô: 0.2}
                                                                                      {‚Äôpopsize‚Äô: 100, ‚Äôg‚Äô: 2500, ‚Äôrt_cross‚Äô: 0.2, ‚Äôrt_mut‚Äô: 0.8}
                                                                                      {‚Äôpopsize‚Äô: 100, ‚Äôg‚Äô: 2500, ‚Äôrt_cross‚Äô: 0.8, ‚Äôrt_mut‚Äô: 0.2}
                                                                                      {‚Äôpopsize‚Äô: 500, ‚Äôg‚Äô: 500, ‚Äôrt_cross‚Äô: 0.2, ‚Äôrt_mut‚Äô: 0.8}
                                                                                      {‚Äôpopsize‚Äô: 500, ‚Äôg‚Äô: 500, ‚Äôrt_cross‚Äô: 0.8, ‚Äôrt_mut‚Äô: 0.2}
                                                                                      {‚Äôpopsize‚Äô: 1000, ‚Äôg‚Äô: 250, ‚Äôrt_cross‚Äô: 0.2, ‚Äôrt_mut‚Äô: 0.8}
                                                              Operon                  {‚Äôpopulation_size‚Äô: (500,), ‚Äôpool_size‚Äô: (500,), ‚Äômax_length‚Äô: (50,), ‚Äôallowed_symbols‚Äô:
                                                                                     (‚Äôadd,mul,aq,constant,variable‚Äô,), ‚Äôlocal_iterations‚Äô: (5,), ‚Äôoffspring_generator‚Äô: (‚Äôbasic‚Äô,),
                                                                                     ‚Äôtournament_size‚Äô: (5,), ‚Äôreinserter‚Äô: (‚Äôkeep-best‚Äô,), ‚Äômax_evaluations‚Äô: (500000,)}
                                                                                      {‚Äôpopulation_size‚Äô: (500,), ‚Äôpool_size‚Äô: (500,), ‚Äômax_length‚Äô: (25,), ‚Äôallowed_symbols‚Äô:
                                                                                     (‚Äôadd,mul,aq,exp,log,sin,tanh,constant,variable‚Äô,),             ‚Äôlocal_iterations‚Äô:          (5,),     ‚Äôoff-
                                                                                      spring_generator‚Äô:       (‚Äôbasic‚Äô,),   ‚Äôtournament_size‚Äô:         (5,),  ‚Äôreinserter‚Äô:     (‚Äôkeep-best‚Äô,),
                                                                                     ‚Äômax_evaluations‚Äô: (500000,)}
                                                                                      {‚Äôpopulation_size‚Äô: (500,), ‚Äôpool_size‚Äô: (500,), ‚Äômax_length‚Äô: (25,), ‚Äôallowed_symbols‚Äô:
                                                                                     (‚Äôadd,mul,aq,constant,variable‚Äô,), ‚Äôlocal_iterations‚Äô: (5,), ‚Äôoffspring_generator‚Äô: (‚Äôbasic‚Äô,),
                                                                                     ‚Äôtournament_size‚Äô: (5,), ‚Äôreinserter‚Äô: (‚Äôkeep-best‚Äô,), ‚Äômax_evaluations‚Äô: (500000,)}
                                                                                      {‚Äôpopulation_size‚Äô: (100,), ‚Äôpool_size‚Äô: (100,), ‚Äômax_length‚Äô: (50,), ‚Äôallowed_symbols‚Äô:
                                                                                     (‚Äôadd,mul,aq,constant,variable‚Äô,), ‚Äôlocal_iterations‚Äô: (5,), ‚Äôoffspring_generator‚Äô: (‚Äôbasic‚Äô,),
                                                                                     ‚Äôtournament_size‚Äô: (3,), ‚Äôreinserter‚Äô: (‚Äôkeep-best‚Äô,), ‚Äômax_evaluations‚Äô: (500000,)}
                                                                                      {‚Äôpopulation_size‚Äô: (100,), ‚Äôpool_size‚Äô: (100,), ‚Äômax_length‚Äô: (25,), ‚Äôallowed_symbols‚Äô:
                                                                                     (‚Äôadd,mul,aq,exp,log,sin,tanh,constant,variable‚Äô,),             ‚Äôlocal_iterations‚Äô:          (5,),     ‚Äôoff-
                                                                                      spring_generator‚Äô:       (‚Äôbasic‚Äô,),   ‚Äôtournament_size‚Äô:         (3,),  ‚Äôreinserter‚Äô:     (‚Äôkeep-best‚Äô,),
                                                                                     ‚Äômax_evaluations‚Äô: (500000,)}
                                                                                      {‚Äôpopulation_size‚Äô: (100,), ‚Äôpool_size‚Äô: (100,), ‚Äômax_length‚Äô: (25,), ‚Äôallowed_symbols‚Äô:
                                                                                     (‚Äôadd,mul,aq,constant,variable‚Äô,), ‚Äôlocal_iterations‚Äô: (5,), ‚Äôoffspring_generator‚Äô: (‚Äôbasic‚Äô,),
                                                                                     ‚Äôtournament_size‚Äô: (3,), ‚Äôreinserter‚Äô: (‚Äôkeep-best‚Äô,), ‚Äômax_evaluations‚Äô: (500000,)}
                                                              gplearn                 {‚Äôpopulation_size‚Äô: 100, ‚Äôgenerations‚Äô: 5000, ‚Äôfunction_set‚Äô: (‚Äôadd‚Äô, ‚Äôsub‚Äô, ‚Äômul‚Äô, ‚Äôdiv‚Äô, ‚Äôlog‚Äô,
                                                                                     ‚Äôsqrt‚Äô)}
                                                                                      {‚Äôpopulation_size‚Äô: 1000, ‚Äôgenerations‚Äô: 500, ‚Äôfunction_set‚Äô: (‚Äôadd‚Äô, ‚Äôsub‚Äô, ‚Äômul‚Äô, ‚Äôdiv‚Äô, ‚Äôlog‚Äô,
                                                                                     ‚Äôsqrt‚Äô)}
                                                                                      {‚Äôpopulation_size‚Äô: 1000, ‚Äôgenerations‚Äô: 500, ‚Äôfunction_set‚Äô: (‚Äôadd‚Äô, ‚Äôsub‚Äô, ‚Äômul‚Äô, ‚Äôdiv‚Äô, ‚Äôlog‚Äô,
                                                                                     ‚Äôsqrt‚Äô, ‚Äôsin‚Äô, ‚Äôcos‚Äô)}
                                                                                      {‚Äôpopulation_size‚Äô: 500, ‚Äôgenerations‚Äô: 1000, ‚Äôfunction_set‚Äô: (‚Äôadd‚Äô, ‚Äôsub‚Äô, ‚Äômul‚Äô, ‚Äôdiv‚Äô, ‚Äôlog‚Äô,
                                                                                     ‚Äôsqrt‚Äô)}
                                                                                      {‚Äôpopulation_size‚Äô: 500, ‚Äôgenerations‚Äô: 1000, ‚Äôfunction_set‚Äô: (‚Äôadd‚Äô, ‚Äôsub‚Äô, ‚Äômul‚Äô, ‚Äôdiv‚Äô, ‚Äôlog‚Äô,
                                                                                     ‚Äôsqrt‚Äô, ‚Äôsin‚Äô, ‚Äôcos‚Äô)}
                                                                                      {‚Äôpopulation_size‚Äô: 100, ‚Äôgenerations‚Äô: 5000, ‚Äôfunction_set‚Äô: (‚Äôadd‚Äô, ‚Äôsub‚Äô, ‚Äômul‚Äô, ‚Äôdiv‚Äô, ‚Äôlog‚Äô,
                                                                                     ‚Äôsqrt‚Äô, ‚Äôsin‚Äô, ‚Äôcos‚Äô)}
                                                              sembackpropgp           {‚Äôpopsize‚Äô:    (1000,), ‚Äôfunctions‚Äô:       (‚Äô+_-_*_aq_plog_sin_cos‚Äô,), ‚Äôlinearscaling‚Äô:           (False,),
                                                                                     ‚Äôsbrdo‚Äô:    (0.9,),  ‚Äôsubmut‚Äô:     (0.1,),  ‚Äôtournament‚Äô:      (4,),  ‚Äômaxsize‚Äô:     (250,), ‚Äôsblibtype‚Äô:
                                                                                     (‚Äôp_6_9999‚Äô,)}
                                                                                      {‚Äôpopsize‚Äô:    (1000,), ‚Äôfunctions‚Äô:       (‚Äô+_-_*_aq_plog_sin_cos‚Äô,), ‚Äôlinearscaling‚Äô:            (True,),
                                                                                     ‚Äôsbrdo‚Äô: (0.9,), ‚Äôsubmut‚Äô: (0.1,), ‚Äôtournament‚Äô: (4,), ‚Äômaxsize‚Äô: (1000,)}
                                                                                      {‚Äôpopsize‚Äô:    (1000,), ‚Äôfunctions‚Äô:       (‚Äô+_-_*_aq_plog_sin_cos‚Äô,), ‚Äôlinearscaling‚Äô:            (True,),
                                                                                     ‚Äôsbrdo‚Äô: (0.9,), ‚Äôsubmut‚Äô: (0.1,), ‚Äôtournament‚Äô: (8,), ‚Äômaxsize‚Äô: (1000,)}
                                                                                      {‚Äôpopsize‚Äô:    (1000,), ‚Äôfunctions‚Äô:       (‚Äô+_-_*_aq_plog_sin_cos‚Äô,), ‚Äôlinearscaling‚Äô:            (True,),
                                                                                     ‚Äôsbrdo‚Äô: (0.9,), ‚Äôsubmut‚Äô: (0.1,), ‚Äôtournament‚Äô: (4,), ‚Äômaxsize‚Äô: (5000,)}
                                                                                      {‚Äôpopsize‚Äô:    (1000,), ‚Äôfunctions‚Äô:       (‚Äô+_-_*_aq_plog_sin_cos‚Äô,), ‚Äôlinearscaling‚Äô:            (True,),
                                                                                     ‚Äôsbrdo‚Äô: (0.9,), ‚Äôsubmut‚Äô: (0.1,), ‚Äôtournament‚Äô: (8,), ‚Äômaxsize‚Äô: (5000,)}
                                                                                      {‚Äôpopsize‚Äô: (10000,), ‚Äôfunctions‚Äô: (‚Äô+_-_*_aq_plog_sin_cos‚Äô,), ‚Äôlinearscaling‚Äô: (False,),
                                                                                     ‚Äôsbrdo‚Äô:    (0.9,),  ‚Äôsubmut‚Äô:     (0.1,),  ‚Äôtournament‚Äô:      (8,),  ‚Äômaxsize‚Äô:     (250,), ‚Äôsblibtype‚Äô:
                                                                                     (‚Äôp_6_9999‚Äô,)}
                                                                                                                              24
                                    *Operon
                                                Data Subset
                                    *SBP-GP
                                                  No Friedman
                                      *FEAT
                                       XGB
                                                  Only Friedman
                                     *EPLEX
                                                  All
                                KernelRidge
                                *GP-GOMEA
                                      *ITEA
                                      LGBM
                                       *FFX
                                       MLP
                              RandomForest
                                   AdaBoost
                                       *AFP
                                    *AFP_FE
                                     *MRGP
                                      *DSR
                                   *gplearn
                                     Linear
                                      *BSR
                                *AIFeynman
                                               0.0             0.2              0.4             0.6              0.8             1.0
                                                                                    2
                                                                                   R  Test Norm
                             Figure 6: Comparison of normalized R2 test scores on all black-box datasets, just the Friedman
                             datatasets, and just the non-Friedman datasets.
                             A.7    Statistical Tests
                             Figures 9-11 give summary signiÔ¨Åcance levels of pairwise tests of signiÔ¨Åcance between estimators on
                             the black-box and ground-truth problems. All pair-wise statistical tests are Wilcoxon signed-rank
                             tests. A Bonferroni correction was applied, yielding the Œ± levels given in each. This methodology
                             for assessing statistical signiÔ¨Åcance is based on the recommendations of Dem≈°ar [82] for comparing
                             multiple estimators over many datasets. These Ô¨Ågures are intended to complement Figures 1-3 in
                             which effect sizes are shown.
                                                                                 25
                                    20.0
                                    17.5
                                    15.0
                                    12.5
                                    10.0
                                    7.5
                                    5.0
                                    2.5
                                                                                                                      analcatdata_neavote_523
                                                                                                                      chscase_geyser1_712
                                                                                                                      cloud_210
                                                                                                                      sleuth_case1202_706
                                                                                                                      chatfield_4_695
                                                                                                                      elusage_228
                                                                                                                      sleuth_ex1605_687
                                                                                                                      pollution_542
                                                                                                                      FacultySalaries_1096
                                                                                                                      bodyfat_560
                                                                                                                      ESL_1027
                                                                                                                      ERA_1030
                                                                                                                      cpu_561
                                                                                                                      titanic
                                                                                                                      poker_1595
                                                                                                                      autoPrice_207
                                                                                                                      houses_537
                                                                                                                      pol_201
                                                                                                                      BNG_echoMonths_1199
                                                                                                                      house_8L_218
                                                                                                                      mv_344
                                                                                                                      analcatdata_apnea1_557
                                                                                                                      puma8NH_225
                                                                                                                      fri_c2_1000_5_599
                                                                                                                      2dplanes_215
                                                                                                                      BNG_lowbwt_1193
                                                                                                                      BNG_pwLinear_1203
                                                                                                                      cpu_small_227
                                                                                                                      cpu_act_197
                                                                                                                      tecator_505
                                                                                                                      fri_c0_500_5_649
                                                                                                                      fri_c1_250_5_601
                                                                                                                                       dataset
                                                                                                                      fri_c0_250_10_635
                                                                                                                      fri_c0_500_10_654
                                                                                                                      fri_c2_500_5_597
                                                                                                                      fri_c0_1000_10_595
                                                                                                                      fri_c3_250_10_602
                                                                                                                      fri_c4_1000_100_588
                                                                                                                      fri_c4_500_50_616
                                                                                                                      fri_c1_250_10_647
                                                                                                                      fri_c2_500_50_626
                                                                                                                      fri_c3_250_25_658
                                                                                                                      fri_c2_250_25_605
                                                                                                                      fri_c0_1000_50_590
                                                                                                                      fri_c1_500_25_582
                                                                                                                      fri_c3_1000_25_586
                                                                                                                      fri_c2_1000_25_589
                                                                                                                      fri_c1_1000_50_583
                                                                                                                      fri_c4_1000_25_592
                                                                                                                      fri_c2_1000_10_606
                                                                                                                      fri_c4_1000_10_623
                                                                                                                      fri_c3_500_10_646
                                                                                                                      fri_c4_500_25_584
                                                                                                                      fri_c3_500_50_645
                                                                                                                      pm10_522
                                                                                                                      fri_c0_250_25_653
                                                                                                                      fri_c0_500_50_650
                                                                                                                      fri_c0_100_10_621
                                                                                                                      fri_c3_100_5_611
                                                                                                                      fri_c1_100_5_656
                                                                                                                      fri_c1_100_10_591
                                                                         FFX
                                                           AFP
                                                     BSR
                                                                                      MLP
                                                                  DSR
                                                                                                                    XGB
                                                                               ITEA
                                                                                                FEAT
                                                                                                          LGBM
                                                                            MRGP
                                                        Linear
                                                                                                   EPLEX
                                                               AFP_FE
                                                                                         Operon
                                                                                            SBP-GP
                                                                     gplearn
                                                                                                             AdaBoost
                                                                                                      GP-GOMEA
                                                  AIFeynman
                                                                                   KernelRidge
                                                                                                                RandomForest
                                                                               algorithm
                             Figure 7: Rankings of methods by R2 test score on the black-box problems (lower is better). Results
                             are bi-clustered by SR method (columns) and dataset (rows). Darker cells indicate that a method
                             performs well on that dataset relative to its competitors. Note only a subset of the datasets are labelled
                             due to space constraints.
                                                                                 26
                                                                                                                     Feynman                                    Strogatz
                                                                                               MRGP
                                                                                            Operon
                                                                                            SBP-GP
                                                                                      GP-GOMEA
                                                                                     AIFeynman
                                                                                             AFP_FE
                                                                                              EPLEX
                                                                                                   AFP
                                                                                                 FEAT
                                                                                            gplearn
                                                                                                                                                              Target Noise
                                                                                                  ITEA
                                                                                                                                                                          0.0
                                                                                                  DSR
                                                                                                                                                                          0.001
                                                                                                                                                                          0.01
                                                                                                  BSR
                                                                                                                                                                          0.1
                                                                                                   FFX
                                                                                                           0.0             0.5             1.0       0.0             0.5             1.0
                                                                                                            Accuracy Solution                         Accuracy Solution
                                                Figure 8: Subset comparison of "Accuracy Solutions", i.e. models with R2>0.999 on the Feynman
                                                and Strogatz problems, differentiated by noise level.
                                                               Wilcoxon signed-rank test, R2 Test,  = 2.4e-04                                         Wilcoxon signed-rank test, Model Size,  = 2.4e-04
                                                        AFP_FE                                                                                   AFP_FE
                                                    AIFeynman                                                                                AIFeynman
                                                      AdaBoost                                                          p<1e-3                 AdaBoost                                                          p<1e-3
                                                            BSR                                                                                     BSR
                                                           DSR                                                                                      DSR
                                                         EPLEX                                                          p<1e-2                    EPLEX                                                          p<1e-2
                                                           FEAT                                                                                    FEAT
                                                            FFX                                                                                     FFX
                                                     GP-GOMEA                                                                                GP-GOMEA
                                                           ITEA                                                         p<1e-1                      ITEA                                                         p<1e-1
                                                    KernelRidge                                                                             KernelRidge
                                                          LGBM                                                                                    LGBM
                                                         Linear                                                                                   Linear
                                                            MLP                                                         p<                          MLP                                                          p<
                                                          MRGP                                                                                    MRGP
                                                        Operon                                                                                   Operon
                                                 RandomForest                                                           no significance   RandomForest                                                           no significance
                                                        SBP-GP                                                                                   SBP-GP
                                                           XGB                                                                                      XGB
                                                        gplearn                                                                                  gplearn
                                                                   AFPAFP_FEBSRDSREPLEXFEATFFXITEALGBMLinearMLPMRGPOperonSBP-GPXGB                         AFPAFP_FE BSRDSREPLEXFEATFFXITEALGBMLinearMLPMRGPOperonSBP-GPXGB
                                                                       AIFeynmanAdaBoost GP-GOMEAKernelRidge                                                    AIFeynmanAdaBoostGP-GOMEAKernelRidge
                                                                                                            RandomForest                                                                             RandomForest
                                                Figure 9: Pairwise statistical comparisons on the black-box regression problems. Wilcoxon signed-
                                                rank tests are used with a Bonferonni correction on Œ± for multiple comparisons. (Left) R2 test scores,
                                                (Right) model size.
                                                                                                                                      27
                                                     Wilcoxon signed-rank test, R2 Test,  = 5.5e-04                               Wilcoxon signed-rank test, R2 Test,  = 5.5e-04
                                              AFP_FE                                                                       AFP_FE
                                           AIFeynman                                                     p<1e-3         AIFeynman                                                     p<1e-3
                                                 BSR                                                                          BSR
                                                 DSR                                                     p<1e-2               DSR                                                     p<1e-2
                                               EPLEX                                                                        EPLEX
                                                FEAT                                                                         FEAT
                                                 FFX                                                     p<1e-1                FFX                                                    p<1e-1
                                           GP-GOMEA                                                                     GP-GOMEA
                                                 ITEA                                                    p<                   ITEA                                                    p<
                                                MRGP                                                                         MRGP
                                              Operon                                                                       Operon
                                              SBP-GP                                                     no significance   SBP-GP                                                     no significance
                                              gplearn                                                                      gplearn
                                                        AFPAFP_FE BSR DSREPLEXFEATFFX  ITEAMRGPOperonSBP-GP                          AFPAFP_FE BSR DSREPLEXFEATFFX  ITEAMRGPOperonSBP-GP
                                                               AIFeynman            GP-GOMEA                                                AIFeynman            GP-GOMEA
                                          Figure 10: Pairwise statistical comparisons of R2 test scores on the ground-truth regression problems.
                                          WereportWilcoxonsigned-rank tests with a Bonferonni correction on Œ± for multiple comparisons.
                                          (Left) target noise of 0, (Right) target noise of 0.01.
                                              Wilcoxon signed-rank test, Symbolic Solution Rate,  = 5.5e-04                Wilcoxon signed-rank test, Symbolic Solution Rate,  = 5.5e-04
                                              AFP_FE                                                                       AFP_FE
                                           AIFeynman                                                     p<1e-3         AIFeynman                                                     p<1e-3
                                                 BSR                                                                          BSR
                                                 DSR                                                     p<1e-2               DSR                                                     p<1e-2
                                               EPLEX                                                                        EPLEX
                                                FEAT                                                                         FEAT
                                                 FFX                                                     p<1e-1                FFX                                                    p<1e-1
                                           GP-GOMEA                                                                     GP-GOMEA
                                                 ITEA                                                    p<                   ITEA                                                    p<
                                                MRGP                                                                         MRGP
                                              Operon                                                                       Operon
                                              SBP-GP                                                     no significance   SBP-GP                                                     no significance
                                              gplearn                                                                      gplearn
                                                        AFPAFP_FE BSR DSREPLEXFEATFFX  ITEAMRGPOperonSBP-GP                          AFPAFP_FE BSR DSREPLEXFEATFFX  ITEAMRGPOperonSBP-GP
                                                               AIFeynman            GP-GOMEA                                                AIFeynman            GP-GOMEA
                                          Figure 11: Pairwise statistical comparisons of solution rates on the ground-truth regression problems.
                                          WereportWilcoxonsigned-rank tests with a Bonferonni correction on Œ± for multiple comparisons.
                                          (Left) target noise of 0, (Right) target noise of 0.01.
                                                                                                                     28
