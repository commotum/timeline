                                              ANEURALPROBABILISTIC LANGUAGEMODEL
                     models described here shifts the difﬁculty elsewhere: many more computations are required, but
                     computation and memory requirements scale linearly, not exponentially with the number of condi-
                     tioning variables.
                     ACKNOWLEDGMENTS
                     Theauthors would like to thank Léon Bottou, Yann Le Cun and Geoffrey Hinton for useful discus-
                     sions. This research was made possible by funding from the NSERC granting agency, as well as the
                     MITACSandIRISnetworks.
                     References
                     D. Baker and A. McCallum. Distributional clustering of words for text classiﬁcation. In SIGIR’98,
                       1998.
                     J.R. Bellegarda. A latent semantic analysis framework for large–span language modeling. In Pro-
                       ceedings of Eurospeech 97, pages 1451–1454, Rhodes, Greece, 1997.
                     S. Bengio and Y. Bengio. Taking on the curse of dimensionality in joint distributions using neural
                       networks. IEEE Transactions on Neural Networks, special issue on Data Mining and Knowledge
                       Discovery, 11(3):550–557, 2000a.
                     Y. Bengio. New distributed probabilistic language models. Technical Report 1215, Dept. IRO,
                       Université de Montréal, 2002.
                     Y. Bengio and S. Bengio. Modeling high-dimensional discrete data with multi-layer neural net-
                       works. In S. A. Solla, T. K. Leen, and K-R. Müller, editors, Advances in Neural Information
                       Processing Systems, volume 12, pages 400–406. MIT Press, 2000b.
                     Y. Bengio and J-S. Senécal. Quick training of probabilistic neural nets by importance sampling. In
                       AISTATS, 2003.
                     A. Berger, S. Della Pietra, and V. Della Pietra. A maximum entropy approach to natural language
                       processing. Computational Linguistics, 22:39–71, 1996.
                     A. Brown and G.E. Hinton. Products of hidden markov models. Technical Report GCNU TR
                       2000-004, Gatsby Unit, University College London, 2000.
                     P.F. Brown, V.J. Della Pietra, P.V. DeSouza, J.C. Lai, and R.L. Mercer. Class-based n-gram models
                       of natural language. Computational Linguistics, 18:467–479, 1992.
                     S.F. Chen and J.T. Goodman. An empirical study of smoothing techniques for language modeling.
                       Computer, Speech and Language, 13(4):359–393, 1999.
                     S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. Indexing by latent
                       semantic analysis.  Journal of the American Society for Information Science, 41(6):391–407,
                       1990.
                     J. Dongarra, D. Walker, and The Message Passing Interface Forum. MPI: A message passing in-
                       terface standard. Technical Report http://www-unix.mcs.anl.gov/mpi, University of Tenessee,
                       1995.
                                                                   1153
