                                              TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                 C. TTTTransformationsforARC
                 Wepresent the transformations used in TTT and the training details.
                                                                 x0    x1      x2
                                                                                                                                          Test-Time 
                                                                                                                                      Training Data
                                                                                     Identity
                                                                y0     y1      y2
                                                                                                             TTT Label
                               Test Task
                          x0     x1    x2      x3
                                                                 x2    x1      x0
                                                                                     Horizonta
v
                                                                                       Flip
                                              ?
                                                                                                               ?
                                                                                                                                                 ?
                          y0     y1    y2
                                                                 y2    y1      y0
                                                                                                             TTT Label
                                                                 x2    x0      x1
                                                                                      Vertica
v
                                                                                       Flip
                                                                                                               ?
                                                                 y2    y0      y1
                                                                                                             TTT Label
                                                                     Step-1:
                         Step-2:

                                                               Leave-one-out Tasks           Rule Based Augmentations
                 Figure 14. TTT dataset generation for a test task (Section 3.1): We start by creating leave-one-out tasks from the given training
                 examples of the task. These tasks are then augmented through rule-based transformations to obtain the full TTT dataset. Finally, we train
                 task-specific LoRA adapters on top of the base FT model.
                 C.1. Transformations
                 Weprovidetheaugmentations used in TTT in Appendix C.1, please refer to our code base for their implementations. After
                 applying these augmentations, we additionally shuffle colors and shuffle training examples. Note that these transformations
                 are applied to all input and output grids. The procedure for generating the dataset for TTT is shown in Figure 14.
                 C.2. Training Setup & Hyperparameters
                 Weusethetorchtune(torchtune Maintainers & Contributors, 2024) library to train LoRA adapters on Llama-3 family of
                 models. WeapplyLoRAtrainingtoqueryandvalueprojectionweightsoftheself-attentionlayer, to the MLP weights and to
                 the output projection layer (was only available for Llama-3 8B in torchtune). We present hyperparameters of this training
                 in Table 6. We also found that using quantized LoRA adapters (Dettmers et al., 2023) instead of standard (full-precision)
                 LoRAleadstoonlyasmalldropinperformance(29 â†’ 26taskssolvedwiththe1B-parameter model), making it a viable
                 option in memory-constrained settings.
                 Weresort to the vLLM (Kwon et al., 2023) library for prediction as it provides fast kernels and batched inference for our
                 models and LoRA inference. We just use greed decoding as we did not see improvements with temperature sampling in our
                 early experiments. We use 90, 180 degree rotations, horizontal, vertical, and diagonal (transpose) flips as our invertible
                 transformations.
                 With that, the whole TTT and inference process takes approximately 12 hours for 100 randomly sampled validation tasks
                 whenusinganNVIDIAA100GPU.
                                                                                      16
