                                                                                         ScatterFormer       3
                               can be converted into a “recurrent” form:
                                                        o =q S ;    S =S      +kTv                         (1)
                                                         t    t t    t     t−1    t t
                               wherethesequencescanbedividedintonon-overlappingchunks,{q ,k ,v }             .
                                                                                                j  j  j j=1:t
                               The output can be calculated by scanning over the sequence, based on an up-
                               datedhiddenstateSt.Thisenablestheuseofchunk-levelcomputationtoaddress
                               the inability to parallelize at the sequence level due to varying lengths.
                                   Inspired by this, we introduce the Scattered Linear Attention(SLA)mod-
                               ule, which accommodates linear attention in a window-based voxel transformer.
                               As shown in Figure 1(b), the SLA module treats the voxels of the entire scene
                               into a single sequence and processes them directly without padding voxels. Us-
                               ing the recurrent form of linear attention, we develop an I/O-aware algorithm to
                               further optimize matrix multiplication on voxel sequences. Specifically, we divide
                               the voxel sequence into multiple chunks and loaded them into the shared mem-
                               ory (SRAM) of the GPU. The computation of the hidden state matrix in each
                               window is then achieved through a series of chunk-wise matrix multiplications
                               and cumulative sums. This optimized implementation significantly reduces I/O
                               overhead and memory usage, resulting in extremely fast and memory-efÏcient
                               attention computation.
                                   In addition, current window-based transformer models use window shifting to
                               propagate the information across the windows. After performing an instruction-
                               level analysis of existing implementations, we observe that the voxel permuta-
                               tion operations caused by window shifting consume significant computational
                               overhead. To address this issue, we propose a cross-window interaction (CWI)
                               module, which is composed of depth-wise convolutions with small 2D kernels and
                               lengthy 1D kernels that allow the voxel features in each window to fully interact
                               with the features in other windows. As a result, the proposed CWI module can
                               improve both the locality and connectivity of the voxel features while requiring
                               minimal computational effort.
                                   Building upon the SLA and CWI modules, we propose the ScatterFormer,
                               an innovative voxel transformer for large-scale point cloud understanding. Our
                               experiments show that ScatterFormer achieves linear complexity without com-
                               promising accuracy. It achieves superior results compared to the state-of-the-art
                               model DSVT [47]. The latency of ScatterFormer is significantly lower than that
                               of transformer-based detectors [8,14,47], which is comparable to that of sparse
                               convolution-based detectors [52].
                                   In conclusion, we delve into attention on voxels grouped by windows, high-
                               lighting the challenges in memory allocation and computation for variable-length
                               sequences. Then, we introduce an SLA module, which can directly process the
                               voxels grouped by windows by facilitating the linear attention formula. A chunk-
                               wise matrix multiplication algorithm is proposed to further accelerate the atten-
                               tion computation in SLA. Finally, we present ScatterFormer, which has linear
                               complexity and can efÏciently process large-scale LiDAR scenes, achieving better
                               accuracy with lower latency compared to existing transformer-based detectors.
