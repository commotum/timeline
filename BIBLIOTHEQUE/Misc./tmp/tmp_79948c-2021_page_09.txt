                         Table 5: Performance Comparison on large-scale JFT dataset. TPUv3-core-days denotes the pre-
                          training time, Top-1 Accuracy denotes the ﬁnetuned accuracy on ImageNet. Note that the last 3 rows
                          use a larger dataset JFT-3B [26] for pre-training, while others use JFT-300M [15]. See Appendix A.2
                          for the size details of CoAtNet-5/6/7. †: Down-sampling in the MBConv block is achieved by stride-2
                          Depthwise Convolution. ⇧: ViT-G/14 computation consumption is read from Fig. 1 of the paper [26].
                                Models         Eval Size   #Params     #FLOPs     TPUv3-core-days     Top-1 Accuracy
                           ResNet + ViT-L/16      3842       330M          -              -                87.12
                                ViT-L/16          5122       307M       364B           0.68K               87.76
                               ViT-H/14           5182       632M       1021B           2.5K               88.55
                              NFNet-F4+           5122       527M       367B           1.86K                89.2
                                        †            2
                              CoAtNet-3           384        168M       114B           0.58K               88.52
                                        †            2
                              CoAtNet-3           512        168M       214B           0.58K               88.81
                               CoAtNet-4          5122       275M       361B           0.95K               89.11
                               CoAtNet-5          5122       688M       812B           1.82K               89.77
                                                     2                                      ⇧
                               ViT-G/14           518        1.84B      5160B          >30K                90.45
                               CoAtNet-6          5122       1.47B      1521B           6.6K               90.45
                               CoAtNet-7          5122       2.44B      2586B          20.1K               90.88
                          match the performance of ViT-G/14 of 90.45%, and with 1.5x less computation, CoAtNet-7 achieves
                          89.77%ontop-1accuracy 90.88%, achieving the new state-of-the-art performance.
                          4.3  Ablation Studies
                          In this section, we will ablate our design choices for CoAtNet.
                          Firstly, we study the importance of the relative attention from combining convolution and attention
                          into a single computation unit. Speciﬁcally, we compare two models, one with the relative attention
                          and the other without, under both the ImageNet-1K alone and ImageNet-21K transfer setting. As we
                          can see from Table 6, when only the ImageNet-1K is used, relative attention clearly outperforms the
                          standard attention, indicating a better generalization. In addition, under the ImageNet-21K transfer
                          setting, the relative attention variant achieves a substantially better transfer accuracy, despite their
                          very close pre-training performances. This suggests the main advantage of relative attention in visual
                          processing is not in higher capacity but in better generalization.
                                                      Table 6: Ablation on relative attention.
                                  Setting                  Metric              WithRel-Attn    WithoutRel-Attn
                               ImageNet-1K            Accuracy (2242)               84.1              83.8
                                                      Accuracy (3842)               85.7              85.3
                              ImageNet-21K       Pre-train Precision@1 (2242)       53.0              52.8
                              )ImageNet-1K        Finetune Accuracy (3842)          87.9              87.4
                                                     Table 7: Ablation on architecture layout.
                                       Setting          Models              Layout          Top-1 Accuracy
                                                     V0: CoAtNet-2    [2, 2, 6, 14, 2]            84.1
                                    ImageNet-1K      V1: S2 (S3       [2, 2, 2, 18, 2]            83.4
                                                     V2: S2 )S3       [2, 2, 8, 12, 2]            84.0
                                   ImageNet-21K      V0: CoAtNet-3    [2, 2, 6, 14, 2]        53.0 !87.6
                                  )ImageNet-1K       V1: S2 (S3       [2, 2, 2, 18, 2]        53.0 !87.4
                          Secondly, as S2 with MBConv blocks and S3 with relative Transformer blocks occupy most of the
                          computation of the CoAtNet, a question to ask is how to split the computation between S2 (MBConv)
                          and S3 (Transformer) to achieve a good performance. In practice, it boils down to deciding the
                          numberofblocks to have in each stage, which we will refer to as “layout” design. For this purpose,
                         wecompareafewdifferent layouts that we experimented with in Table 7.
                                                                       9
