                  Model     PEMethod       Last LN     Top-1 Accuracy (%)               Model                  Structure                  Top-1
                                            pos11             73.30                               Layer 0     Hierarchical    (x+PE)     Acc(%)
                 DeiT-Ti      MPVG           pos8             73.38                                   ✗            ✓             ✓         73.31
                                             pos5             73.39                    MPVG          ✓             ✗             ✓         73.48
                                            pos               73.51
                                                 0                                                   ✓             ✓             ✗         73.28
               Table 5: Comparison of the value of PE added to the Last                              ✓             ✓             ✓         73.51
               LNinMPVG.pos0 referstotheinitial position embedding,                  Table 6: Structural Differences in MPVG. ”Layer 0” de-
               and pos11 represents the position embedding after applying            notes whether layer 0 is included when delivering PE to lay-
               LN in the last layer. pos    (=LN′ (pos        )) indicates the                                                         ′
                                         N        N      N−1                         ers. “Hierarchical” denotes whether posl is LN (posl−1) or
                                                                                                                                       l
               PEinput for the (N +1)th layer.                                       LN′(pos ). ”(x+PE)” denotes whether PE is added to the
                                                                                         l    0
                                                                                     token embedding(x) before entering layer 0 or not.
               a non-Layer-wise structure while using GAP. We perform
               these experiments with DeiT-Ti on ImageNet-1K.
                 InFig5,wecompare(a),whereonlyGAPisapplied,with                         Specifically, we conduct comparative experiments on
               (b), where PE is delivered to the Last LN in a non-Layer-             three structural differences: (1) Our methods exclude layer
               wise structure with GAP. Fig 5-(a) shows a 72.40% per-                0 when delivering PE. Through our experiments, we find
               formance, while Fig 5-(b) shows a decreased performance               that delivering PE to layer 0, which was previously included,
               of 72.14%. This indicates that adding PE to the Last LN               is not only unnecessary but also improves the performance
               is only effective in Layer-wise structure where PE is deliv-          of vision transformers when excluded. (2) We add PE to
               ered to each layer. In other words, these experimental results        the token embedding before it enters layer 0. Unlike LaPE
               provethatinLayer-wisestructure,thetokenembeddingcon-                  where token embedding x and PE are separated before en-
               tains values that are counterbalanced by PE. Specifically, in         tering the first layer, our methods add PE to x before enter-
               Fig 5-(b) structure, the token embedding that passes through          ing layer 0. This structure does not limit the expressiveness
               the layers does not contain the directional values, which is          of PE because independent LN is applied to both the token
               counterbalanced by PE. Thus, adding PE to the Last LN not             embedding and PE in each layer, and PE is delivered in a
               only has no effect but actually leads to a decrease in per-           Layer-wisestructure. Moreover, the (x+PE) structure boosts
               formance. As a result, as shown in Fig 2-(b), in Layer-wise           performance by approximately 0.23%. (3) We observe that
               structure, the token embedding progresses while retaining             the performance is similar between hierarchical and non-
               values that are meant to be counterbalanced by PE, but af-            hierarchical structures. However, in non-hierarchical struc-
               ter passing through the final layer, this directional value is        tures, performance often declines in small or large vision
               not adequately compensated by PE. This proves that PE is              transformers due to overfitting (Yu et al. 2023). Through Ta-
               necessary to perform this additional counterbalancing role            ble6,wedemonstratethatourmethodsrepresenttheoptimal
               in the Last LN.                                                       structure in the Layer-wise structure.
               Ablation Study                                                                                Conclusion
               The impact of PE values delivered to the Last LN           We
               conduct experiments to investigate the impact of varying the          Wereveal that position embedding can play additional roles
               PEvaluespassedtotheLastLNinMPVG.InTable5,posN
               represents the value of LN′ (pos        ). Since MPVG does            in vision transformers using the GAP method. Specifically,
                                            N      N−1                               in a Layer-wise structure, PE has a counterbalancing effect
               not deliver PE to layer 0, N ranges from 1 to L−1, where L            on the values of token embedding, and maintaining this di-
               is the number of layers. Experiments show that MPVG con-              rectional balance by PE is beneficial for vision transform-
               sistently outperforms PVG, which achieves a performance               ers. Based on these observations, we propose a simple yet
               of 73.17%, regardless of the PE values passed to the Last             effective method, MPVG. MPVG utilizes the characteris-
               LN. This suggests that delivering PE in the Last LN has               tics of PE observed in the Layer-wise structure to maximize
               a significant positive impact on the performance of vision            the PE. Throughextensiveexperiments,wedemonstratethat
               transformers. Furthermore, it demonstrates that maintaining           MPVGisgenerallyeffectiveonvisiontransformers, outper-
               the role of PE in the Last LN is generally effective. MPVG            forming previous methods. However, MPVG has a poten-
               adopts pos0, which shows the best performance by compar-              tial limitation in that it is incompatible with the class token
               ing various PE values delivered to the Last LN.                       method. Through these limitations, we will further explore
               Structural differences in MPVG           We also experiment           the broader applicability of MPVG and the effects of PE’s
               by varying the architecture structure in MPVG. Table 6                counterbalancing as part of our future work. In this paper,
               presents the ablations for the differences in architecture            wedemonstrate that MPVG effectively addresses the issues
               within MPVG. The experiments are conducted on DeiT-Ti                 arising in GAP and Layer-wise structures, providing a sig-
               using the ImageNet-1K. Through this experiment, we adopt              nificantly more meaningful approach. Through this, we look
               an improved Layer-wise structure that differs from the con-           forwardtoMPVGofferingabroaderperspectiveonposition
               ventional Layer-wise structure.                                       embedding.
