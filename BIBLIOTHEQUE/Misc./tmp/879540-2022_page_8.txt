                        GT          Trailer        Pedestrian         Bicycle       Traffic Cone        Motorcycle         Car       Construction Vehicle
                  Ì†µÌ≤ï                    Ì†µÌ≤ï
                                                                Ì†µÌ≤ï                    Ì†µÌ≤ï                    Ì†µÌ≤ï                     Ì†µÌ≤ï
                   Â†µÌøè                    Ì†µÌøê
                r                                               Â†µÌøè                    Ì†µÌøê                     Â†µÌøè                    Ì†µÌøê
                illa
                P
                t
                n
                i
                o
                P
                *
                  Ì†µÌ≤ï                    Ì†µÌ≤ï
                                                               Ì†µÌ≤ï                    Ì†µÌ≤ï                     Ì†µÌ≤ï                    Ì†µÌ≤ï
                   Â†µÌøè                    Ì†µÌøê
                ng                                              Â†µÌøè                    Ì†µÌøê                     Â†µÌøè                    Ì†µÌøê
                i
                nt
                e
                m
                ug
                A
                nt
                i
                o
                P
                  Ì†µÌ≤ï                    Ì†µÌ≤ï
                                                               Ì†µÌ≤ï                    Ì†µÌ≤ï                     Ì†µÌ≤ï                    Ì†µÌ≤ï
                  Â†µÌøè                    Ì†µÌøê
                s                                               Â†µÌøè                    Ì†µÌøê                     Â†µÌøè                    Ì†µÌøê
                ur
                O
                                     (a)                                           (b)                                          (c)
              Figure 5. Qualitative results. We compare with LiDAR-only PointPillars [12] and cross-modal PointAugmenting [32]. (a) illustrates the
              superiority of temporal fusion, where our method can alleviate false positive detection on human-like objects in t2 to preserve temporal
              consistency with t1. In (b), cross-sensor information helps reduce detection errors, and our method consistently detects the traffic cone in
              adjacent frames. The night-view images in (c) introduces ambiguous features that result in false negative car detection in PointAugmenting,
              while our method successfully utilizes the mutual information across sensors and time to recall the car object. Best viewed in color.
                       Method           mAP      Image     Fusion      Total          resulting in an end-to-end runtime of 315 ms on par with
                  LIFT(448√ó800)         51.78    151ms     164ms      315ms           the recent state-of-the-art detectors [25, 36]. We also ob-
                  LIFT(w/oSparse)       51.30    151ms     201ms      352ms           serve a large runtime jump (i.e. 878 ms) using a larger
                 LIFT(896√ó1600)         51.83    714ms     164ms      878ms           896√ó1600imageresolution,andasignificantperformance
                  LIFT(224√ó400)         44.20    46ms      167ms      213ms           drop (i.e. 44.2 mAP) with a smaller 224 √ó 400 resolution.
              Table 7. Run-time comparison on the nuScenes dataset. We report         Thus, we choose the final design based on the tradeoffs be-
              the runtime for image backbone (Image), encoder and attention           tween speed and accuracy.
              fusion (Fusion) and end-to-end inference (Total).                       5. Conclusion
              sequential cross-modal input. By comparing (c) vs (d) and                  Wehavepresented LIFT, a LiDAR Image Fusion Trans-
              (e) vs (f), our augmentation consistently achieves +4.36%               former that simultaneously aligns the spatiotemporal cross-
              mAPand+4.74%mAPgainsonsequentialpointcloudand                           sensor 4D information for 3D object detection in real-world
              sequential cross-sensor data respectively, showing that our             autonomous-driving scenarios.         Particularly, we encode
              schemeiscapabletopreservethecross-modalandtemporal                      both the LiDAR frames and camera images as sparsely-
              data consistency.                                                       located BEV grid features and propose a sensor-time 4D
              Effects of architecture designs. We report the ablation re-             attention module to effectively and efficiently capture the
              sults of the proposed architecture components in Table 6.               mutual correlations. Furthermore, we devise a general yet
              Note that all experiments are conducted with the proposed               simple data augmentation technique to enhance the train-
              sensor-time data augmentation scheme. From (g) to (k),                  ing dynamics while persevering the data consistency. With
              we observe progressive performance gains with the pro-                  the proposedend-to-endsingle-stage3Dobjectdetector,we
              posed point-wise attention (PA), 4D positional encoding                 improved strong baselines by large margins and achieved
              (PE), pyramid context (PC) and sparse window partition                  state-of-the-art performance on the challenging nuScenes
              (Sparse). Comparing (g) and (k), the proposed network                   and Waymobenchmarkdatasets.
              components further improve mAP by 2.02%.                                Acknowledgements. This work was supported in part by
                                                                                      NSFC (61906119, U19B2035), Shanghai Municipal Sci-
              Run-time efficiency. We report the runtime efficiency in                ence and Technology Major Project (2021SHZDZX0102),
              Table 7. As the Transformer design inevitably introduces                the National Key Research and Development Program of
              extra computational load, our sparse window design can ef-              China (2020AAA0108104), Alibaba Innovative Research
              fectively reduce the Fusion time from 201 ms to 164 ms,                 (AIR) Program and Alibaba Research Intern Program.
                                                                                  17179
