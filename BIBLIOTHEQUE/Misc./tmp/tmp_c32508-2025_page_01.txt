                                                                                           www.nature.com/scientificdata
                      opeN a Comprehensive Behavioral 
    Data DeSCRIptoR Dataset for the abstraction and 
                                    Reasoning Corpus
                                                  1                 2                   1,2                  1
                                    Solim LeGris    ✉, Wai Keen Vong , Brenden M. Lake     & todd M. Gureckis
                                    the abstraction and Reasoning Corpus (aRC) is a visual program synthesis benchmark designed to 
                                    test out-of-distribution generalization in machines. Comparing aI algorithms to human performance 
                                    is essential to measure progress on these problems. In this paper, we present H-aRC (Human-aRC): a 
                                    novel large-scale dataset containing solution attempts from over 1700 humans on ARC problems. The 
                                    dataset spans the full set of 400 training and 400 evaluation tasks from the original ARC benchmark, 
                                    and it is the largest human evaluation to date. By publishing the dataset, we contribute human 
                                    responses to each problem, step-by-step behavioral action traces from the aRC user-interface, and 
                                    natural-language solution descriptions of the inferred program/rule. We believe this dataset will be 
                                    of value to researchers, both in cognitive science and AI, since it of㘶ers the potential to facilitate the 
                                    discovery of underlying mechanisms supporting abstraction and reasoning in people. the insights to be 
                                    gained from these data not only have value for cognitive science, but could in turn inform the design of 
                                    more e㘠陦cient, human-like AI algorithms.
                                    Background & Summary
                                    T㔴e question of how to measure intelligence in humans and machines remains a critical stepping stone towards 
                                    developing more sophisticated AI. In that spirit, the Abstraction and Reasoning Corpus (ARC) benchmark was 
                                    proposed by François Chollet1 to evaluate analogical generalization, measuring how machines handle a broad 
                                    category of novel tasks given just a few examples. Each task requires inferring an underlying transformation rule 
                                    or program from a series of training input-output pairs which consist of abstract visual grids (see Fig. 1), and 
                                    to use this rule to correctly generate an output grid given a novel test input. Although visually simple, the tasks 
                                    are conceptually rich and challenging, requiring the identif㘶cation of compositional rules involving objects and 
                                    relations, geometry, counting, visual instructions, and logical operations.
                                                                                                                             -
                                      In the last several years, large language models (LLMs) have achieved impressive performance on a wide vari
                                    ety of benchmarks, demonstrating competency in natural language understanding, coding, and mathematics2,3. 
                                    With larger and more powerful LLMs, many benchmarks have had a limited shelf life, with performance rapidly 
                                                                          4
                                    increasing to human or even superhuman levels . In contrast, ARC has proven to be a persistent and formidable 
                                    challenge for state-of-the-art AI systems, with little progress observed in the f㘶rst few years af㘶er its creation. In 
                                    a f㘶rst ARC competition held on Kaggle in 2019, the majority of approaches were based on program synthesis 
                                                                                                                             -
                                    techniques, with the winner of the competition achieving a 21% score on the private test set (kaggle.com/com
                                    petitions/abstraction-and-reasoning-challenge). Af㘶er several years of stagnation, an ARC Prize Foundation was 
                                    founded in 2024 to encourage research and development for achieving an open-source, low-resource solution 
                                    to the ARC benchmark, and spurring progress towards human-level intelligence (arcprize.org). A number of 
                                                                                                                             -
                                    advancements resulted from their f㘶rst public competition, with some open-source LLM-based models achiev
                                    ing notable gains in performance, jumping from 33% to 55.5% on the private evaluation set5. Additionally, 
                                    closed-source models from OpenAI and Anthropic have achieved major leaps in performance, reaching up to 
                                                                                                                             -
                                    75.7% on the ARC prize semi-private evaluation set (arcprize.org/leaderboard). While these results are impres
                                    sive, today’s top-performing models rely on massive amounts of text data during pretraining to achieve this level 
                                    of performance, and possibly cognitively implausible data augmentation techniques to support task-specif㘶c 
                                           6,7. Unlike people, these models are both data- and resource-intensive, suggesting that they operate 
                                    inference
                                    under fundamentally dif㘶erent mechanisms to solve ARC problems. Additionally, the ARC Prize Foundation 
                                    1                                    2
                                                                                                            ✉
                                    Department of Psychology, nYU, new York, USA.  center for Data Science, nYU, new York, USA.  e-mail: solim.legris@ 
                                    nyu.edu
          Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                     1
