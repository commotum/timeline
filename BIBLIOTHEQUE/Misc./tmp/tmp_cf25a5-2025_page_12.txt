                                                   Test-Time Learning for Large Language Models
              Niu, S., Wu, J., Zhang, Y., Chen, Y., Zheng, S., Zhao, P.,    Reimers, N. and Gurevych, I. Sentence-bert: Sentence em-
                 and Tan, M. Efficient test-time model adaptation with-        beddings using siamese bert-networks. In Conference
                 out forgetting. In International conference on machine        onEmpirical Methods in Natural Language Processing,
                 learning, pp. 16888–16905. PMLR, 2022a.                       2019.    URL https://api.semanticscholar.
              Niu,S.,Wu,J.,Zhang,Y.,Xu,G.,Li,H.,Zhao,P.,Huang,J.,              org/CorpusID:201646309.
                 Wang,Y., and Tan, M. Boost test-time performance with      Ren, Y., Cao, Y., Guo, P., Fang, F., Ma, W., and Lin, Z.
                 closed-loop inference. arXiv preprint arXiv:2203.10853,       Retrieve-and-sample: Document-level event argument
                 2022b.                                                        extraction via hybrid retrieval augmentation. In Rogers,
              Niu, S., Wu, J., Zhang, Y., Wen, Z., Chen, Y., Zhao, P., and     A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceed-
                 Tan, M. Towards stable test-time adaptation in dynamic        ings of the 61st Annual Meeting of the Association for
                 wild world. In The Eleventh International Conference on       Computational Linguistics (Volume 1: Long Papers),
                 Learning Representations, 2023.                               pp. 293–306, Toronto, Canada, July 2023. Association
                                                                               for Computational Linguistics. doi: 10.18653/v1/2023.
              Niu, S., Miao, C., Chen, G., Wu, P., and Zhao, P. Test-time      acl-long.17. URL https://aclanthology.org/
                 model adaptation with only forward passes. In Forty-          2023.acl-long.17/.
                 first International Conference on Machine Learning,        Schneider, S., Rusak, E., Eck, L., Bringmann, O., Bren-
                 2024. URL https://openreview.net/forum?                       del, W., and Bethge, M. Improving robustness against
                 id=qz1Vx1v9iK.                                                commoncorruptions by covariate shift adaptation. Ad-
              Oh, Y., Lee, J., Choi, J., Jung, D., Hwang, U., and Yoon, S.     vances in neural information processing systems, 33:
                 Efficient diffusion-driven corruption editor for test-time    11539–11551, 2020.
                 adaptation. In European Conference on Computer Vision,     Shao, Z., Yu, Z., Wang, M., and Yu, J. Prompting large
                 pp. 184–201. Springer, 2025.                                  language models with answer heuristics for knowledge-
              Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu:         based visual question answering. In Proceedings of the
                 a method for automatic evaluation of machine transla-         IEEE/CVFConference on computer vision and pattern
                 tion. In Proceedings of the 40th annual meeting of the        recognition, pp. 14974–14983, 2023.
                Association for Computational Linguistics, pp. 311–318,     Shu, M., Nie, W., Huang, D.-A., Yu, Z., Goldstein, T.,
                 2002.                                                         Anandkumar, A., and Xiao, C. Test-time prompt tuning
              Peng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y.,        for zero-shot generalization in vision-language models.
                 Huang, Q., Liden, L., Yu, Z., Chen, W., et al. Check your     Advances in Neural Information Processing Systems, 35:
                 facts and try again: Improving large language models          14274–14289, 2022.
                 with external knowledge and automated feedback. arXiv      Song,J., Lee, J., Kweon, I. S., and Choi, S. Ecotta: Memory-
                 preprint arXiv:2302.12813, 2023.                              efficient continual test-time adaptation via self-distilled
              Qian, H., Zhang, P., Liu, Z., Mao, K., and Dou, Z. Mem-          regularization. In Proceedings of the IEEE/CVF Confer-
                 orag: Moving towards next-gen rag via memory-inspired         ence on Computer Vision and Pattern Recognition, pp.
                 knowledge discovery. arXiv preprint arXiv:2409.05591,         11920–11929, 2023.
                 2024.                                                      Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt,
                                                                               M.Test-timetrainingwithself-supervisionforgeneraliza-
              Radford, A. Improving language understanding by genera-          tion under distribution shifts. In International conference
                 tive pre-training. 2018.                                      onmachinelearning, pp. 9229–9248. PMLR, 2020.
              Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,    Tay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Wei, J.,
                 Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring        Wang,X.,Chung,H.W.,Bahri,D.,Schuster, T., Zheng,
                 the limits of transfer learning with a unified text-to-text   S., Zhou, D., Houlsby, N., and Metzler, D. UL2: Uni-
                 transformer. Journal of machine learning research, 21         fying language learning paradigms. In The Eleventh
                 (140):1–67, 2020.                                             International Conference on Learning Representations,
              Ram,O.,Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua,         2023. URL https://openreview.net/forum?
                 A., Leyton-Brown, K., and Shoham, Y.         In-context       id=6ruVLB727MC.
                 retrieval-augmented language models. Transactions of       Thirunavukarasu,A.J.,Ting,D.S.J.,Elangovan,K.,Gutier-
                 the Association for Computational Linguistics, 11:1316–       rez, L., Tan, T. F., and Ting, D. S. W. Large language
                1331, 2023. doi: 10.1162/tacl a 00605. URL https:              models in medicine. Nature medicine, 29(8):1930–1940,
                //aclanthology.org/2023.tacl-1.75/.                            2023.
                                                                         12
