                     Preprint.
                     Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. Dynamic
                       cheatsheet: Test-time learning with adaptive memory, 2025. URL https://arxiv.org/
                       abs/2504.07952.
                     Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, and
                       Yasin Abbasi Yadkori. Hierarchical reasoning model, 2025. URL https://arxiv.org/
                       abs/2506.21734.
                     Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and
                       AnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels, 2023a.
                       URLhttps://arxiv.org/abs/2305.16291.
                     Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D. Goodman.
                       Hypothesis search: Inductive reasoning with language models, 2024. URL https://arxiv.
                       org/abs/2309.05660.
                     Xintao Wang, Qian Yang, Yongting Qiu, Jiaqing Liang, Qi He, Zhouhong Gu, Yanghua Xiao, and
                       W.Wang. Knowledgpt: Enhancing large language models with retrieval and storage access on
                       knowledge bases. ArXiv, abs/2308.11761, 2023b. URL https://api.semanticscholar.
                       org/CorpusID:261076315.
                     Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers,
                       2022. URLhttps://arxiv.org/abs/2203.08913.
                     Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E. Gonzalez,
                       and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models, 2024.
                       URLhttps://arxiv.org/abs/2406.04271.
                     ZhengyanZhang,XuHan,ZhiyuanLiu,XinJiang,MaosongSun,andQunLiu. Ernie: Enhanced
                       language representation with informative entities, 2019. URL https://arxiv.org/abs/
                       1905.07129.
                     A IMPLEMENTATION DETAILS
                     A.1  PS MEMORYFORMAT
                     Each PS format memory entry contains:
                           • Title: a succinct label for the underlying idea.
                           • Description: elaboration on behavior and role.
                           • Kind: whether this concept encodes a type/structure/routine.
                           • Parameters: a list of fields that parametrize this concept.
                           • OutputTyping: specifies what the output for this routine is, to suggest how various routines
                            can plug into other routines.
                           • Relevance Cues: much like the situation field in the OE concepts, we consider the
                            context in which this concept is relevant.
                           • Implementation Notes: suggestions on how to implement the concept in actual code.
                     A.2  EXPERIMENT PARAMETERS
                     Tobuild on frontier models, we experiment primarily with OpenAI’s o4-mini (max tokens=32000,
                     reasoning_effort=medium). For auxiliary tasks such as concept abstraction and non-reasoning
                     selection, we use OpenAI’s GPT-4.1 (temperature=0.3, max tokens=1000) to reduce token usage. We
                     looked into evaluating the open-source DeepSeek R1, which also has a transparent thinking process,
                     but the output 8000 token limit consistently yielded unfinished solutions.
                                                          12
