                                       TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
              Limitations                                                     .1992.4.6.888. URL https://doi.org/10.1162/neco
              Optimization bias    In development of ARC, we used a set       .1992.4.6.888.
                                                                                                                                    ´
              of 80 tasks for validation/ablation experiments. Standard     Brown,B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re,
              hyper-parameters (learning rate, epochs) were optimized         C., and Mirhoseini, A. Large language monkeys: Scaling
              using this set, which might have introduced some bias.          inference compute with repeated sampling, 2024. URL
              Dataleakage     WhilethebaseLlama-3performspoorlyon             https://arxiv.org/abs/2407.21787.
              the public validation set of ARC, the public availability of  Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
              the dataset introduces the possibility that these models may    J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
              have seen these examples during pre-training. Similarly,        Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,
              while the base model achieves reasonable performance on         Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,
              BBH,its public availability raises similar concerns.            J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
                                                                              Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
              Acknowledgments                                                 S., Radford, A., Sutskever, I., and Amodei, D. Language
                                                                              models are few-shot learners. In Advances in Neural
              We sincerely thank the BARC team (Li et al., 2025) for          Information Processing Systems 33, 2020. URL https:
              their support and collaboration in ensembling our method        //proceedings.neurips.cc/paper/2020/hash/145
              with theirs, resulting in an official joint submission to the   7c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
              ARCpublic set. We thank Aniruddha Nrusimha for help-          Butt, N., Manczak, B., Wiggers, A., Rainone, C., Zhang,
              ful discussions on parameter efficient training. This work      D. W., Defferrard, M., and Cohen, T. CodeIt: Self-
              was supported by MIT–IBM Watson AI Lab, and by the              improving language models with prioritized hindsight
              National Science Foundation under grants IIS-2212310, IIS-      replay. In Proceedings of the 41st International Confer-
              2238240, and CCF-2217064. This work also benefited from         ence on Machine Learning. PMLR, 2024. URL https:
              manyconversations during the Simons Institute Program on        //dl.acm.org/doi/10.5555/3692070.3692267.
              Language Models and Transformers.
                                                                            Chollet, F. On the measure of intelligence, 2019. URL
              References                                                      https://arxiv.org/abs/1911.01547.
              Acquaviva, S., Pu, Y., Kryven, M., Sechopoulos, T., Wong,     Chollet, F., Knoop, M., Kamradt, G., and Landers, B. ARC
                 C., Ecanow, G. E., Nye, M. I., Tessler, M. H., and Tenen-    Prize 2024: Technical report, 2025. URL https://arxi
                 baum, J. Communicating natural programs to humans            v.org/abs/2412.04604.
                 and machines. In Advances in Neural Information Pro-       Damani, M., Shenfeld, I., Peng, A., Bobu, A., and Andreas,
                 cessing Systems 35, 2022. URL http://papers.nips.            J. Learning how hard to think: Input-adaptive allocation
                 cc/paper files/paper/2022/hash/182aed0379591                 of LM computation. In The Thirteenth International
                 ebd1d655b2bdc152075-Abstract-Datasets and Ben                Conference on Learning Representations, 2025. URL
                 chmarks.html.                                                https://openreview.net/forum?id=6qUUgw9bAZ.
                   ¨
              Akyurek, E., Schuurmans, D., Andreas, J., Ma, T., and         Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,
                 Zhou, D. What learning algorithm is in-context learning?     L. QLoRA:Efficient finetuning of quantized LLMs. In
                 Investigations with linear models. In The Eleventh Inter-    Advances in Neural Information Processing Systems 36,
                 national Conference on Learning Representations, 2023.       2023. URLhttp://papers.nips.cc/paper files/p
                 URLhttps://openreview.net/pdf?id=0g0X4H8yN4                  aper/2023/hash/1feb87871436031bdc0f2beaa62a0
                 I.                                                           49b-Abstract-Conference.html.
              Behrouz, A., Zhong, P., and Mirrokni, V. Titans: Learning     Gandelsman, Y., Sun, Y., Chen, X., and Efros, A. A. Test-
                 to memorize at test time, 2025. URL https://arxiv.           time training with masked autoencoders. In Advances in
                 org/abs/2501.00663.                                          Neural Information Processing Systems 35, 2022. URL
                                                                              http://papers.nips.cc/paper files/paper/2022/
              Bober-Irizar, M. and Banerjee, S. Neural networks for           hash/bcdec1c2d60f94a93b6e36f937aa0530-Abstr
                 abstraction and reasoning. Scientific Reports, 2024. ISSN    act-Conference.html.
                 2045-2322. doi: 10.1038/s41598-024-73582-7. URL
                 https://doi.org/10.1038/s41598-024-73582-7.                Greenblatt, R. Getting 50% (SoTA) on ARC-AGI with GPT-
                                                                              4o, 2024. URL https://redwoodresearch.substa
              Bottou, L. and Vapnik, V. Local learning algorithms. Neural     ck.com/p/getting-50-sota-on-arc-agi-with-gpt.
                Computation, 1992. ISSN 0899-7667. doi: 10.1162/neco          Accessed 09-11-2024.
                                                                         9
