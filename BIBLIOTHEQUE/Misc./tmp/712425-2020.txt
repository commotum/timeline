                         HowCanSelf-AttentionNetworksRecognizeDyck-nLanguages?
                                                 Javid Ebrahimi, Dhruv Gelda, Wei Zhang
                                                         Visa Research, Palo Alto, USA
                                               {jebrahim,dhgelda,wzhan}@visa.com
                                        Abstract                                          2                          4
                                                                              T                           T
                                                                              (                            (
                       We focus on the recognition of Dyck-n (Dn)             [                           <
                       languages with self-attention (SA) networks,           ]                           >
                       which has been deemed to be a difﬁcult task            )                            [
                                                                              [                           {
                       for these networks. We compare the perfor-             (                           }
                       mance of two variants of SA, one with a start-         (                            ]
                                                                              )                           <
                                        +                         −           )                           >
                       ing symbol (SA ) and one without (SA ).                ]                            )
                                                +
                       Our results show that SA    is able to general-          T ( [ ] ) [ ( ( ) ) ]        T ( < > [ { } ] < > )
                       ize to longer sequences and deeper dependen-
                                                       −
                       cies. For D , we ﬁnd that SA       completely
                                   2                                         Figure 1: Softmax attention scores of the second layer
                       breaks down on long sequences whereas the                                    +
                                                                             of a sufﬁx-masked SA , for a D and a D sequence.
                                      +                                                                       2         4
                       accuracy of SA   is 58.82%. We ﬁnd attention          Therowsandcolumnsdenotequeriesandkeys,respec-
                       maps learned by SA+ to be amenable to in-             tively. The layer produces virtually hard attentions, in
                       terpretation and compatible with a stack-based        which each symbol attends only to one preceding sym-
                       language recognizer. Surprisingly, the perfor-        bol or itself. The attended symbol is either the starting
                       mance of SA networks is at par with LSTMs,            symbol (T) or the last unmatched opening bracket.
                       which provides evidence on the ability of SA
                       to learn hierarchies without recursion.
                  1    Introduction                                          a starting symbol to the vocabulary, a two-layer
                                                                             multi-headed SA network (i.e., the encoder of a
                                                                             Transformer) is able to learn D       languages, and
                  There is a growing interest in using formal lan-                                              n
                  guages to study fundamental properties of neural           generalize to longer sequences, although not per-
                  architectures, which has led to the extraction of in-      fectly. As shown in Figure 1, the network is able to
                  terpretable models (Weiss et al., 2018; Merrill et al.,    identify the corresponding closing bracket for an
                  2020). Recent work (Hao et al., 2018; Suzgun               opening bracket, in what resembles a stack-based
                  et al., 2019; Skachkova et al., 2018) has explored         automaton. For example, the symbol “]” in the
                  the generalized Dyck-n (Dn) languages, a subset            string “([])”, will ﬁrst pop “[” from the stack, then
                  of context-free languages. Dn consists of “well-           it attends to “(”, the last unmatched symbol, which
                  balanced” strings of parentheses with n different          will determine the next valid closing bracket. The
                  types of bracket pairs, and it is the canonical formal     starting symbol (T) enables the model to learn the
                  language to study nested structures (Chomsky and           occurrence of the end of a clause or the end of the
                       ¨                                                     sequence, which can be regarded as a mechanism
                  Schutzenberger, 1959). Weiss et al. (2018) show
                  that LSTMs (Hochreiter and Schmidhuber, 1997)              to represent an empty stack.
                  are a variant of the k-counter machine and can rec-           Ourworkistheﬁrsttoperform an empirical ex-
                  ognize D1 languages. The dynamic counting mech-            ploration of SA on formal languages. We present
                  anisms, however, are not sufﬁcient for Dn>1 as it          detailed comparison between an SA which incorpo-
                  requires emulating a pushdown automata. Hahn               rates a starting symbol (SA+), andonethatdoesnot
                                                                                 −
                  (2020) shows that for a sufﬁciently large length,          (SA ), and demonstrate signiﬁcant differences in
                  Transformers (Vaswani et al., 2017) will fail to           their generalization across the length of sequences
                  transduce the D language.                                  and the depth of dependencies.
                                    2
                     Weempirically show that with the addition of               Recent work has suggested that the ability of
                                                                        4301
                                 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4301–4306
                                                                c
                                        November16-20,2020. 2020AssociationforComputational Linguistics
                  self-attention mechanisms to model hierarchical           recognition of D languages as a transduction task:
                                                                                             n
                  structures is limited. Shen et al. (2019) show that       Givenavalidstring,weaskthemodeltopredictthe
                  the performance of Transformers on tasks such             next possible symbols auto-regressively. To illus-
                  as logical inference (Bowman et al., 2015) and            trate, consider an input string “[ ( ) ] ( [” in the D2
                  ListOps (Nangia and Bowman, 2018) is either poor          language, we seek to predict the set of next valid
                  or worse than LSTMs. Tran et al. (2018) have              brackets in the string– (, [, or ]. We consider an
                  also reported similar results on SA, concluding that      input to be accurately recognized only if the model
                  recurrence is necessary to model hierarchical struc-      correctly predicts the set of all possible brackets at
                                                                      +
                  tures. In comparison, our results show that SA            each position in the input sequence. Throughout
                  outperforms LSTMonD languagesexceptforD                   the paper, we refer to a clause as a substring, in
                                            n                          2
                  on longer sequences. Papadimitriou and Jurafsky           which the number of closing and opening brackets
                  (2020) posit that the ability of neural models to         of each type of bracket are equal.
                  learn hierarchical structures can be attributed to a         We train two multi-headed self-attention net-
                  “looking back” capability, rather than directly en-       works(i.e., only the encoder part of a Transformer),
                  coding hierarchies. Our analysis sheds light on the       one of which incorporates an additional starting
                  ability of SA to learn hierarchical structures by ele-    symbolinthevocabulary(SA+),andtheotherdoes
                  gantly attending to the correct preceding symbol.                 −
                                                                            not (SA ). For each model, the number of layers is
                  2    Related Work                                         2, the number of attention heads h = 4 and model
                                                                            dimension d = 256. We use learnable embeddings
                                                       n n   n n m m        to convert each input symbol to a 256-dimensional
                  Formal languages such as a b ,a b c d                     vector. We also add residual connections around
                                         n n n    n+m n m
                  (context-free) and a b c ,a          b c    (context-     each layer followed by layer normalization, similar
                  sensitive) have been extensively studied and recog-       to the standard Transformer (Vaswani et al., 2017).
                  nized using RNNs (Elman, 1990; Das et al., 1992;          Wetrain two unidirectional LSTMs, one with the
                                     ¨
                  Steijvers and Grunwald, 1996). But the perfor-            starting symbol (LSTM+) and the other without
                  manceofsamerecurrent architectures on Dn lan-             it (LSTM−). The LSTMs use 320-dimensional
                  guages is poor and suffers from the lack of gen-          hidden states and a 320-dimensional vector for
                  eralization. Sennhauser and Berwick (2018) and            learned input embeddings. Our SA and LSTM
                  Bernardy (2018) study the capability of RNNs to                                                         1
                  predict the next possible closing parenthesis at each     variants all have around 1.6M parameters . We
                  position in the D string and found that the gener-        use Adam(KingmaandBa,2015)foroptimization.
                                    n                                               +         −
                  alization at higher recursion depths is poor. Hao         For SA andSA ,wevarythelearningrateη as
                  et al. (2018) reported that stack-augmented LSTMs                                −0.5              −1.5
                  achieve better generalization on Dn languages                η = const · min(itr     , itr · warmup     ),  (1)
                  but the network computation does not emulate a            whereitrrefers to the iteration number and warmup
                  stack. Morerecently,Suzgunetal.(2019)proposed             is set to 10k. We tuned the hyper-parameter const,
                  memory-augmented recurrent neural networks and            using the values [0.01, 0.1, 1.0, 10], and used 0.1.
                  deﬁned a sequence classiﬁcation task for the recog-       For LSTMs,weuseaninitiallearning rate of 0.001
                  nition of D languages. Yu et al. (2019) explored
                              n                                             but with no learning rate scheduling.
                  the use of attention-based seq2seq framework for             Were-generate the synthetic dataset for our ex-
                  D languagesandfoundthatthegeneralization to
                    2                                                       periments through the probabilistic context-free
                  sequences with higher depths is still lacking. Be-        grammar(PCFG)alreadydescribed in the existing
                  sides empirical investigations, formal languages          literature (Suzgun et al., 2019). For instance, the
                  have been studied theoretically for understanding         PCFGforDyck-2languagecanbedeﬁnedas: (1)
                  the complexity of neural networks (Siegelmann             S −→ [S], (2) S −→ {S}, (3) S −→ SS, and (4)
                                       ´
                  and Sontag, 1992; Perez et al., 2019), mostly under       S −→ ε, each with probability p = 0.25. For each
                  assumptions that cannot be met in an experiment–          D language, we train on 32k sequences of length
                  inﬁnite precision or unbounded computation time.            n
                                                                            2-50, validate on 3.2k sequences of length 52-74,
                  3    Experiments                                          andevaluateon10ksequencesdividedequallyover
                                                                            the length intervals 76-100 and 102-126.
                  We follow prior works (Gers and Schmidhuber,                 1Wefounddropouttobedetrimental to the performance,
                  2001; Suzgun et al., 2019), and formulate the             and hence we removed it from all models.
                                                                       4302
                                                            D                     D                     D                     D
                                          Model               1                     2                     3                     4
                                                    76-100    102-126     76-100    102-126     76-100    102-126     76-100    102-126
                                              −
                                           SA        100.0      98.88      14.52      0.006      32.62      5.50       42.94      9.080
                                              +
                                           SA        100.0      100.0      93.34      58.82      93.18      66.88      93.78      72.38
                                         LSTM−       100.0      99.64      88.30      73.20      85.16      65.06      78.92      60.24
                                         LSTM+       100.0      100.0      87.00      70.90      82.44      63.56      76.66      55.90
                             Table 1: Performance of SA and LSTM variants on Dyck-n languages for different sequence lengths.
                                                                                           4forD ,6forD ,8forD ),yˆ ∈ {0,1}andy are
                                                                                                    2           3            4    i                   i
                         25                              40                                the target and prediction for label i, respectively.
                                                         35
                         20                              30
                        h15                             h25
                        t                               t                                  3.1     Evaluation
                        p                               p
                        e                               e20
                        D10                             D
                                                         15                                                                               +           −
                                                                                           Table 1 compares the accuracy of SA               andSA on
                          5                              10                                D , D , D , and D languages. For both models,
                                                         5                                    1    2     3          4
                          0                                                                the performance on D is almost perfect (> 98%)
                           0   10   20  30   40  50          80  90  100  110 120                                       1
                                     Length                         Length                 and does not show any degradation with increase
                                                                                                                                              −
                      Figure 2: Joint distribution of D language based on                  in sequence length. The accuracy of SA                on D2 is
                                                              2
                      the length and depth of sequences in training (blue) and             14.52%forsequences with length 76-100 and com-
                      evaluation (red). The top and right axes also show the               pletely fails beyond it. In comparison, the perfor-
                      marginal distribution for length and depth respectively.             manceofSA+onD issigniﬁcantlybetter,93.34%
                                                                                                                     2
                                                                                           and 58.82% for sequences of length 76-100 and
                                                                                                                                                          −
                         Figure 2 shows the distribution of length and                     102-126, respectively. The performance of SA
                                                                                           improves on D and D , compared to D , with
                      depth of D sequences in training and evaluation.                                          3          4                       2
                                    2
                      For higher Dyck languages (Dn>2), the training                       an accuracy of 32.62% and 42.94%, respectively
                      and evaluation datasets have similar depth and                       for sequences of length 76-100. The performance
                                                                                           of SA+ is nearly constant (∼93%) on D                        for
                      length distributions because the PCFG give equal                                                                            n≥2
                      probability to different pairs of parentheses and                    sequences of length 76-10 but there is signiﬁcant
                                                                                           improvement from D (58.82%) to D (66.88%)
                      the total probability for rules of the form S −→ (S),                                             2                    3
                                                                                           and D (72.38%) for sequences of length 102-126.
                      S −→ [S], ... is 0.5. We perform experiments on D ,                          4
                                                                                     1
                      D ,D ,andD languages. Notethatthenumberof                                Unlike SA, the performance of LSTM degrades
                        2     3         4
                      pairs of parentheses cannot be increased arbitrarily                 after the addition of the starting symbol, with the
                      withoutrequiringmodiﬁcationstotheexperimental                        biggest drop (4.3%) on D for sequence length of
                                                                                                                             4
                      setup: We varied the length of sequences during                      102-106. The starting symbol has enabled SA to
                      training from 2 to 50, which could contain at most                   attend to the correct preceding token, but it has
                      25 different pairs.                                                  been ineffective for LSTM. For D sequences of
                                                                                                                                         2
                         In our sequence prediction task, the input vo-                    length 102-126, LSTM− achieves an accuracy of
                                     i                                                                                                              +
                      cabulary (V ) for a D language consists of 2n+1                      73.20%, an improvement of ∼14% over SA . On
                                     n            n
                                                                                                                             +                           −
                      symbols: n pairs of brackets (or parentheses), and                   all other comparisons, SA            outperforms LSTM .
                      an additional starting symbol T whereas the out-                         We observe another interesting distinction be-
                      put vocabulary (V o) does not include the starting                   tween the two architectures.              The accuracy of
                                             n
                      symbol T. Since there might exist multiple possi-                    LSTMdeteriorates as the number of pairs of brack-
                      bilities for the next bracket in a sequence, we adopt                                                                  +            −
                                                                                           ets increases, while the accuracy of SA              and SA
                      a multi-label classiﬁcation approach wherein the                     improves. To understand this phenomenon, we
                      outputs are encoded as a k-hot vector and the net-                   looked at the training, validation, and test sets of
                      work is optimized using the binary cross-entropy                     each language, and found that while validation
                      loss function given by                                               and test sets of each D language almost always
                                                                                                                          n
                               |V o|                                                       (> 99%) includes sequences of n different brack-
                                 n
                              X
                                    n                                         o            ets, the training set could include sequences of
                       L=             yˆ log(y )+(1−yˆ) log(1−y ) , (2)
                                        i        i           i              i              1 ≤ m < ntypesofbrackets. This implies that SA
                               i=1                                                         beneﬁts from data augmentation with sequences
                      where |V o| is the output vocabulary size (2 for D1,                 from other languages, and LSTM does not. Put dif-
                                 n
                                                                                      4303
                                                           a) Error distribution                      b) SA− depth-wise comparison                      c) SA+ depth-wise comparison
                                                                                                  1                                                 1
                                              120         SA+                                                                                                                    4
                                                          SA−                                   0.8                                               0.8
                                             d100
                                             a
                                             e 80
                                             h                                                  0.6                                               0.6
                                                                                               cy                                                cy
                                             o                                                                                                                              
                                             t                                                 ra                                                ra                            2
                                               60                                                               4
                                             ce                                                ccu0.4                                            ccu0.4
                                             n                                                 a                                                 a
                                             a 40
                                             st                                                           
                                             i                                                              2
                                             d 20                                               0.2                                               0.2
                                                0                                                 0                                                 0
                                                      25       50      75     100     125           10         15         20        25                10         15        20         25
                                                              failure position                                      depth                                             depth
                                                                                                                                 +              −
                            Figure 3: In a, we plot the distribution of the errors made by SA                                       and SA , based on the position of the mispre-
                            dicted symbol, and its distance to its head. In b and c, we plot the performance of the models as depth increases.
                            ferently, these results suggest LSTMhasastrongin-                                          are mostly concentrated at f                          >80. Figure 3b-
                                                                                                                                                                         p
                            ductive bias, perhaps in counting (Kharitonov and                                          c shows how the performance of SA+ and SA−
                            Chaabouni, 2020), which might result in degrada-                                           change with depth (d ) for D and D languages.
                                                                                                                                                            p             2            4
                                                                                                                             −
                            tion of its performance in higher Dyck languages.                                          SA isverysensitive to depth as the accuracy de-
                                                                                                                       creases rapidly for D from ∼38% at d = 10 to a
                                                                                                                                                          2                              p
                              Algorithm1:Compatibilityofanattention                                                    complete failure beyond d = 20. In comparison,
                                                                                                                                                                    p
                              mapwithastack-based recognizer.                                                          the drop in accuracy for SA+ is less severe, ∼ 94%
                                 import numpy as np                                                                    at d = 10 to ∼ 72% at d = 20.
                                                                                                                              p                                   p
                                 def get_match(seq, opening='(['):
                                   stack, match = [], len(seq)*[-1]                                                    4      Compatibility With a Stack-Based
                                   for idx, s in enumerate(seq):                                                              Recognizer
                                     if s in opening:
                                       stack.insert(0, idx)
                                     elif s != 'T':                                                                    Theability of (memory-less) SA networks to recog-
                                       stack.pop(0)                                                                    nize D             languages is intriguing. In this section,
                                       if len(stack) > 0:                                                                          n>1
                                         match[idx] = stack[0]                                                         wecontrast second-layer attention maps produced
                                   return match                                                                                    +                −
                                                                                                                       by SA and SA , and provide insights into the
                                 def is_compatible(seq, atten_map):                                                    underlying mechanism which leads to the success
                                   match = get_match(seq)                                                                        +
                                   for idx, m in enumerate(match):                                                     of SA .
                                     p = np.argmax(atten_map[idx])                                                         Wedeﬁne compatibility as a quantitative mea-
                                     if m != p and m != -1:                                                            sure for the alignment of the state of a stack-based
                                       return False                                                                    language recognizer (M) with the attention maps.
                                   returnTrue
                                                                                                                       Mhasaccess to the top of a hypothetical stack,
                                                                                                                       and can push and pop depending on the opening
                            3.2       Error Analysis                                                                   and closing brackets, respectively. Based on this
                            Wedeﬁne failure position (f ) as the position of                                           analogy, all opening brackets should attend to them-
                                                                              p
                            the ﬁrst symbol in the sequence where the model                                            selves, and all closing brackets should ﬁrst do a pop,
                            failed to correctly predict the next set of possible                                       and then attend to the last unmatched bracket. For
                            parentheses, For each symbol in a Dn sequence: (i)                                         example, the symbol “]” in the string “([])”, will
                            depth (d ) is the number of unmatched parenthesis                                          ﬁrst pop “[” from the stack, then it attends to “(”,
                                          p
                            uptoandincluding that symbol, and (ii) distance                                            the last unmatched symbol, which will determine
                            to head (d ) is the number of symbols between the                                          the next valid closing bracket. If for every closing
                                             h
                            mis-classiﬁed closing bracket and its opening coun-                                        symbol in the sequence, the highest attention score
                            terpart. Figure 3a plots the error distribution of                                         of at least one of the heads points to the correct
                            SA+andSA−intermsoffailureposition (f ) and                                                 bracket, then we consider the SA compatible. Fur-
                                                                                                      p
                                                                                                                                                                                                  +
                            distance to head (d ). There is a clear separation                                         thermore, for a fair comparison between SA                                     and
                                                             h
                                                                                                                             −
                            between the two models in terms of what “types”                                            SA , we do not push the starting symbol to the
                            of errors are made. SA− breaks quite early on in                                           stack and only consider closing brackets which are
                            the sequence, with majority of the errors occurring                                        not at the end of a clause.
                                                                                                               +                                                                         +               −
                            at f = 25-75 whereas whereas the errors of SA                                                  Figure5plotsthecompatibilityofSA andSA
                                   p
                                                                                                                4304
                                                a) Attention-Map (Head-1)        b) Attention-Map (Head-2)        c) Attention-Map (Head-3)        d) Attention-Map (Head-4)
                                            T                                T                               T                                T
                                            (                                (                                (                                (                                    1.0
                                            [                                [                                [                                [
                                            (                                (                                (                                (
                                            [                                [                                [                                [
                                            ]                                ]                                ]                                ]                                    0.8
                                            )                                )                                )                                )
                                            ]                                ]                                ]                                ]
                                            )                                )                                )                                )                                    0.6
                                                T ( [ ( [ ] ) ] )               T ( [ ( [ ] ) ] )                T ( [ ( [ ] ) ] )                T ( [ ( [ ] ) ] )
                                             (                               (                                (                                (                                    0.4
                                             [                               [                                [                                [
                                             (                               (                                (                                (
                                             [                               [                                [                                [                                    0.2
                                             ]                               ]                                ]                                ]
                                             )                               )                                )                                )
                                             ]                               ]                                ]                                ]                                    0.0
                                             )                               )                                )                                )
                                                (   [  (   [  ]   )  ]   )       (  [   (  [   ]   )  ]   )      (   [   (  [   ]  )   ]  )       (   [  (   [  ]   )   ]  )
                                                               +
                           Figure 4: Comparing SA (top) and SA (bottom), based on their attention maps on a D sequence. The third head
                                                                                                                                                          2
                           of SA+ has produced weights that are compatible with the operations of a stack-based recognizer.
                                                                             4 (SA+)                            of the whole sequence.
                                           1
                                         0.8                                                                     5      Conclusion and Future Work
                                        y         4 (SA−)
                                        t
                                        i
                                        l
                                        i
                                        b0.6
                                        i                                                                        We provide empirical evidence on the ability of
                                        t                                            +
                                        a                                    2 (SA )
                                        p                                                                        self-attention (SA) networks to learn generalized
                                        m0.4
                                        o
                                        c                                                                        D languages. Wecomparetheperformanceoftwo
                                                                                                                    n
                                                  2 (SA−)                                                                                   +               −
                                         0.2                                                                     SAnetworks, SA andSA ,whichdifferonlyin
                                                                                                                 the inclusion of a starting symbol in their vocabu-
                                           0                                                                     lary. We demonstrate that a simple addition of the
                                                 20         40        60         80        100                                                           +
                                                                  length                                         starting symbol helps SA                   generalize to sequences
                                                                                                  +              that are longer and have higher depths. The com-
                           Figure 5: Compatibility versus length for SA                               and        petitive performance of SA (no-recurrence) against
                                −
                           SA onD andD languages.
                                          2           4                                                          LSTMsmightseemsurprising,consideringthatthe
                                                                                                                 recognition of D languages is an inherently hier-
                                                                                                                                            n
                           versus sequence length. We ﬁnd that SA− on D                                          archical task. From our experiments, we conclude
                                                                                                         2
                           has almost zero compatibility, even for sequence                                      that recognizing Dycklanguagesisnottiedtorecur-
                           lengths seen during training (40-50), on which it                                     sion, but rather learning the right representations
                           achieves close-to-perfect accuracy. In comparison,                                    to look up the head token. Further, we ﬁnd that
                           SA+hasperfectcompatibilityforsequencelengths                                                                                                 +
                                                                                                                 the representations learned by SA                          are highly in-
                           seenduringtraining,andmaintainsahighdegreeof                                          terpretable and the network performs computations
                           compatibility for longer ones. Further, perhaps not                                   similar to a stack automaton. Our results suggest
                           surprisingly, the Pearson correlation between the                                     formal languages could be an interesting avenue
                           distribution of accuracy and compatibility across                                     to explore the interplay between performance and
                                                                                    +
                           lengths 50-100 is & 90% for all SA                          models.                   interpretability for SA. Comparisons between SA
                               Figure 4 shows the attention maps of all four                                     and LSTMrevealinteresting contrast between the
                                                 +                 −
                           heads of SA               and SA            for the D            sequence             two architectures which calls for further investi-
                                                                                        2
                                                                                                         +
                          “([([])])”. We observe that the third head of SA                                       gation. Recent work (Katharopoulos et al., 2020)
                           matches our expectation of a stack-based recog-                                       shows how to express the Transformer as an RNN
                           nizer. An important feature of the third head is that                                 through linearization of the attention mechanism,
                           the last symbol attends to the starting symbol T.                                     which could lay grounds for more theoretical anal-
                           Thestarting symbol has enabled the model to learn                                     ysis of these neural architectures (e.g., inductive
                           the occurrence of the end of a clause and the end                                     biases and complexity.)
                                                                                                          4305
                  References                                                Isabel Papadimitriou and Dan Jurafsky. 2020. Pretrain-
                  Jean-Philippe Bernardy. 2018.    Can recurrent neural        ing on non-linguistic structure as a tool for analyz-
                     networks learn nested recursion?   LiLT (Linguistic       ing learning bias in language models. arXiv preprint
                     Issues in Language Technology), 16(1).                    arXiv:2004.14601.
                                                                                    ´                      ´                    ´
                  Samuel R Bowman, Gabor Angeli, Christopher Potts,         Jorge Perez, Javier Marinkovic, and Pablo Barcelo.
                     and Christopher D Manning. 2015. A large anno-            2019. On the turing completeness of modern neu-
                     tated corpus for learning natural language inference.     ral network architectures. In ICLR.
                     In EMNLP.                                              Luzi Sennhauser and Robert Berwick. 2018. Evaluat-
                                                      ¨                        ing the ability of LSTMs to learn context-free gram-
                  Noam Chomsky and Marcel P Schutzenberger. 1959.              mars. In Proceedings of the 2018 EMNLP Work-
                     The algebraic theory of context-free languages. In        shopBlackboxNLP:AnalyzingandInterpretingNeu-
                     Studies in Logic and the Foundations of Mathemat-         ral Networks for NLP.
                     ics, volume 26, pages 118–161. Elsevier.
                  Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. 1992.       Yikang Shen, Shawn Tan, Arian Hosseini, Zhouhan
                     Learning context-free grammars: Capabilities and          Lin, Alessandro Sordoni, and Aaron C Courville.
                     limitations of a recurrent neural network with an ex-     2019. Ordered memory. In NeurIPS.
                     ternal stack memory. In Proceedings of The Four-       Hava T Siegelmann and Eduardo D Sontag. 1992. On
                     teenth Annual Conference of Cognitive Science So-         the computational power of neural nets. In Proceed-
                     ciety. Indiana University, page 14.                       ings of the ﬁfth annual workshop on Computational
                  Jeffrey L Elman. 1990. Finding structure in time. Cog-       learning theory, pages 440–449.
                     nitive science, 14(2):179–211.                         Natalia Skachkova, Thomas Alexander Trost, and Di-
                  Felix A Gers and E Schmidhuber. 2001. Lstm recur-            etrich Klakow. 2018. Closing brackets with recur-
                     rent networks learn simple context-free and context-      rent neural networks. In Proceedings of the 2018
                     sensitive languages. IEEE Transactions on Neural         EMNLPWorkshopBlackboxNLP:AnalyzingandIn-
                     Networks, 12(6):1333–1340.                                terpreting Neural Networks for NLP, pages 232–
                  Michael Hahn. 2020. Theoretical limitations of self-         239.
                     attention in neural sequence models. Transactions                                  ¨
                                                                            MarkSteijvers and Peter Grunwald. 1996. A recurrent
                     of the Association for Computational Linguistics,         network that performs a context-sensitive prediction
                     8:156–171.                                                task. In Proceedings of the 18th annual conference
                  Yiding Hao, William Merrill, Dana Angluin, Robert            of the cognitive science society, pages 335–339.
                     Frank, Noah Amsel, Andrew Benz, and Simon              MiracSuzgun,SebastianGehrmann,YonatanBelinkov,
                     Mendelsohn. 2018. Context-free transductions with         andStuartMShieber.2019. Memory-augmentedre-
                     neural stacks. In Proceedings of the 2018 EMNLP           current neural networks can learn generalized dyck
                     WorkshopBlackboxNLP:AnalyzingandInterpreting              languages. arXiv preprint arXiv:1911.03329.
                     Neural Networks for NLP.
                                            ¨                               KeMTran,AriannaBisazza,andChristofMonz.2018.
                  Sepp Hochreiter and Jurgen Schmidhuber. 1997.               The importance of being recurrent for modeling hi-
                     Long short-term memory.       Neural computation,         erarchical structure. In EMNLP.
                     9(8):1735–1780.
                  Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-         Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
                     pas, and Franc¸ois Fleuret. 2020. Transformers are        Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
                     rnns: Fast autoregressive transformers with linear at-    Kaiser, and Illia Polosukhin. 2017. Attention is all
                     tention. In ICML.                                         youneed. In NeurIPS.
                  Eugene Kharitonov and Rahma Chaabouni. 2020.              Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On
                     What they do when in doubt: a study of induc-             the practical computational power of ﬁnite precision
                     tive biases in seq2seq learners.    arXiv preprint        rnns for language recognition. In ACL.
                     arXiv:2006.14953.                                      Xiang Yu, Ngoc Thang Vu, and Jonas Kuhn. 2019.
                  Diederik P Kingma and Jimmy Ba. 2015. Adam: A                Learning the dyck language with attention-based
                     method for stochastic optimization. In ICLR.              seq2seq models. In Proceedings of the 2019 ACL
                                                                              WorkshopBlackboxNLP:AnalyzingandInterpreting
                  William Merrill, Gail Weiss, Yoav Goldberg, Roy             Neural Networks for NLP, pages 138–146.
                     Schwartz, Noah A Smith, and Eran Yahav. 2020. A
                     formal hierarchy of rnn architectures. In ACL.
                  Nikita Nangia and Samuel Bowman. 2018. Listops:
                     A diagnostic dataset for latent tree learning.   In
                     Proceedings of the 2018 NAACL: Student Research
                     Workshop.
                                                                       4306
