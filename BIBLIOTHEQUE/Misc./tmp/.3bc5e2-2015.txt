                           DeepSpeech2: End-to-EndSpeechRecognitionin
                                                  English and Mandarin
                                                                                       ‚àó
                                                  BaiduResearch‚ÄìSiliconValleyAILab
                           Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro,
                            Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel,
                              Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley,
                              Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman,
                           Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang,
                                              BoXiao,DaniYogatama,JunZhan,ZhenyaoZhu
                                                                Abstract
                                We show that an end-to-end deep learning approach can be used to recognize
                                either English or Mandarin Chinese speech‚Äîtwo vastly different languages. Be-
                                cause it replaces entire pipelines of hand-engineered components with neural net-
                                works, end-to-end learning allows us to handle a diverse variety of speech includ-
                                ing noisy environments, accents and different languages. Key to our approach is
                                our application of HPC techniques, resulting in a 7x speedup over our previous
                                system [26]. Because of this efÔ¨Åciency, experiments that previously took weeks
                                now run in days. This enables us to iterate more quickly to identify superior ar-
                                chitectures and algorithms. As a result, in several cases, our system is competitive
                                with the transcription of human workers when benchmarked on standard datasets.
                                Finally, using a technique called Batch Dispatch with GPUs in the data center, we
                                show that our system can be inexpensively deployed in an online setting, deliver-
                                ing low latency when serving users at scale.
                        1   Introduction
                        Decades worth of hand-engineered domain knowledge has gone into current state-of-the-art auto-
                        maticspeechrecognition(ASR)pipelines. Asimplebutpowerfulalternativesolutionistotrainsuch
                        ASRmodelsend-to-end,usingdeeplearningtoreplacemostmoduleswithasinglemodel[26]. We
                        present the second generation of our speech system that exempliÔ¨Åes the major advantages of end-
                        to-end learning. The Deep Speech 2 ASR pipeline approaches or exceeds the accuracy of Amazon
        arXiv:1512.02595v1  [cs.CL]  8 Dec 2015Mechanical Turk human workers on several benchmarks, works in multiple languages with little
                        modiÔ¨Åcation, and is deployable in a production setting. It thus represents a signiÔ¨Åcant step towards
                        a single ASR system that addresses the entire range of speech recognition contexts handled by hu-
                        mans. Since our system is built on end-to-end deep learning, we can employ a spectrum of deep
                        learning techniques: capturing large training sets, training larger models with high performance
                        computing, and methodically exploring the space of neural network architectures. We show that
                        through these techniques we are able to reduce error rates of our previous end-to-end system [26] in
                        English by up to 43%, and can also recognize Mandarin speech with high accuracy.
                        Oneofthechallenges of speech recognition is the wide range of variability in speech and acoustics.
                        Asaresult, modernASRpipelinesaremadeupofnumerouscomponentsincludingcomplexfeature
                        extraction, acoustic models, language and pronunciation models, speaker adaptation, etc. Build-
                        ing and tuning these individual components makes developing a new speech recognizer very hard,
                        especially for a new language. Indeed, many parts do not generalize well across environments or
                        languages and it is often necessary to support multiple application-speciÔ¨Åc systems in order to pro-
                        vide acceptable accuracy. This state of affairs is different from human speech recognition: people
                           ‚àóAuthorship order is alphabetical.
                                                                    1
                         havetheinnateabilitytolearnanylanguageduringchildhood,usinggeneralskillstolearnlanguage.
                         After learning to read and write, most humans can transcribe speech with robustness to variation in
                         environment, speaker accent and noise, without additional training for the transcription task. To
                         meet the expectations of speech recognition users, we believe that a single engine must learn to be
                         similarly competent; able to handle most applications with only minor modiÔ¨Åcations and able to
                         learn new languages from scratch without dramatic changes. Our end-to-end system puts this goal
                         within reach, allowing us to approach or exceed the performance of human workers on several tests
                         in two very different languages: Mandarin and English.
                         Since Deep Speech 2 (DS2) is an end-to-end deep learning system, we can achieve performance
                         gains by focusing on three crucial components: the model architecture, large labeled training
                         datasets, and computational scale. This approach has also yielded great advances in other appli-
                         cation areas such as computer vision and natural language. This paper details our contribution to
                         these three areas for speech recognition, including an extensive investigation of model architectures
                         and the effect of data and model size on recognition performance. In particular, we describe numer-
                         ousexperimentswithneuralnetworkstrainedwiththeConnectionistTemporalClassiÔ¨Åcation(CTC)
                         loss function [22] to predict speech transcriptions from audio. We consider networks composed of
                         manylayers of recurrent connections, convolutional Ô¨Ålters, and nonlinearities, as well as the impact
                         of a speciÔ¨Åc instance of Batch Normalization [63] (BatchNorm) applied to RNNs. We not only
                         Ô¨Ånd networks that produce much better predictions than those in previous work [26], but also Ô¨Ånd
                         instances of recurrent models that can be deployed in a production setting with no signiÔ¨Åcant loss in
                         accuracy.
                         Beyond the search for better model architecture, deep learning systems beneÔ¨Åt greatly from large
                         quantities of training data. We detail our data capturing pipeline that has enabled us to create larger
                         datasets than what is typically used to train speech recognition systems. Our English speech system
                         is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours. We use
                         data synthesis to further augment the data during training.
                         Training on large quantities of data usually requires the use of larger models. Indeed, our models
                         have many more parameters than those used in our previous system. Training a single model at
                                                            1
                         these scales requires tens of exaFLOPs that would require 3-6 weeks to execute on a single GPU.
                         This makes model exploration a very time consuming exercise, so we have built a highly optimized
                         training systemthatuses8or16GPUstotrainonemodel. Incontrasttopreviouslarge-scaletraining
                         approachesthatuseparameterserversandasynchronousupdates[18,10],weusesynchronousSGD,
                         which is easier to debug while testing new ideas, and also converges faster for the same degree of
                         data parallelism. To make the entire system efÔ¨Åcient, we describe optimizations for a single GPU
                         as well as improvements to scalability for multiple GPUs. We employ optimization techniques
                         typically found in High Performance Computingtoimprovescalability. Theseoptimizationsinclude
                         a fast implementation of the CTC loss function on the GPU, and a custom memory allocator. We
                         also use carefully integrated compute nodes and a custom implementation of all-reduce to accelerate
                         inter-GPU communication. Overall the system sustains approximately 50 teraFLOP/second when
                         training on 16 GPUs. This amounts to 3 teraFLOP/second per GPU which is about 50% of peak
                         theoretical performance. This scalability and efÔ¨Åciency cuts training times down to 3 to 5 days,
                         allowing us to iterate more quickly on our models and datasets.
                         We benchmark our system on several publicly available test sets and compare the results to our
                         previous end-to-end system [26]. Our goal is to eventually reach human-level performance not only
                         on speciÔ¨Åc benchmarks, where it is possible to improve through dataset-speciÔ¨Åc tuning, but on a
                         range of benchmarks that reÔ¨Çects a diverse set of scenarios. To that end, we have also measured
                         the performance of human workers on each benchmark for comparison. We Ô¨Ånd that our system
                         outperforms humans in some commonly-studied benchmarks and has signiÔ¨Åcantly closed the gap in
                         much harder cases. In addition to public benchmarks, we show the performance of our Mandarin
                         system on internal datasets that reÔ¨Çect real-world product scenarios.
                         Deep learning systems can be challenging to deploy at scale. Large neural networks are compu-
                         tationally expensive to evaluate for each user utterance, and some network architectures are more
                         easily deployed than others. Through model exploration, we Ô¨Ånd high-accuracy, deployable net-
                         work architectures, which we detail here. We also employ a batching scheme suitable for GPU
                            1             18
                            1 exaFLOP=10    FLoating-point OPerations.
                                                                    2
                   hardware called Batch Dispatch that leads to an efÔ¨Åcient, real-time implementation of our Mandarin
                   engine on production servers. Our implementation achieves a 98th percentile compute latency of 67
                   milliseconds, while the server is loaded with 10 simultaneous audio streams.
                   The remainder of the paper is as follows. We begin with a review of related work in deep learning,
                   end-to-end speech recognition, and scalability in Section 2. Section 3 describes the architectural and
                   algorithmic improvements to the model and Section 4 explains how to efÔ¨Åciently compute them. We
                   discuss the training data and steps taken to further augment the training set in Section 5. An analysis
                   of results for the DS2 system in English and Mandarin is presented in Section 6. We end with a
                   description of the steps needed to deploy DS2 to real users in Section 7.
                   2  Related Work
                   This work is inspired by previous work in both deep learning and speech recognition. Feed-forward
                   neural network acoustic models were explored more than 20 years ago [7, 50, 19]. Recurrent neu-
                   ral networks and networks with convolution were also used in speech recognition around the same
                   time [51, 67]. More recently DNNs have become a Ô¨Åxture in the ASR pipeline with almost all
                   state of the art speech work containing some form of deep neural network [42, 29, 17, 16, 43, 58].
                   Convolutional networks have also been found beneÔ¨Åcial for acoustic models [1, 53]. Recurrent
                   neural networks, typically LSTMs, are just beginning to be deployed in state-of-the art recogniz-
                   ers [24, 25, 55] and work well together with convolutional layers for the feature extraction [52].
                   Models with both bidirectional [24] and unidirectional recurrence have been explored as well.
                   End-to-end speech recognition is an active area of research, showing compelling results when used
                   to re-score the outputs of a DNN-HMM[23]andstandalone[26]. Twomethodsarecurrentlyusedto
                   map variable length audio sequences directly to variable length transcriptions. The RNN encoder-
                   decoder paradigm uses an encoder RNN to map the input to a Ô¨Åxed length vector and a decoder
                   network to expand the Ô¨Åxed length vector into a sequence of output predictions [11, 62]. Adding an
                   attentional mechanism to the decoder greatly improves performance of the system, particularly with
                   long inputs or outputs [2]. In speech, the RNN encoder-decoder with attention performs well both
                   in predicting phonemes [12] or graphemes [3, 8].
                   The other commonly used technique for mapping variable length audio input to variable length
                   outputistheCTClossfunction[22]coupledwithanRNNtomodeltemporalinformation. TheCTC-
                   RNNmodelperformswellinend-to-endspeechrecognitionwithgraphemeoutputs[23,27,26,40].
                   The CTC-RNN model has also been shown to work well in predicting phonemes [41, 54], though
                   a lexicon is still needed in this case. Furthermore it has been necessary to pre-train the CTC-RNN
                   network with a DNN cross-entropy network that is fed frame-wise alignments from a GMM-HMM
                   system [54]. In contrast, we train the CTC-RNN networks from scratch without the need of frame-
                   wise alignments for pre-training.
                   Exploiting scale in deep learning has been central to the success of the Ô¨Åeld thus far [36, 38]. Train-
                   ing on a single GPU resulted in substantial performance gains [49], which were subsequently scaled
                   linearly to two [36] or more GPUs [15]. We take advantage of work in increasing individual GPU
                   efÔ¨Åciency for low-level deep learning primitives [9]. We build on the past work in using model-
                   parallelism [15], data-parallelism [18] or a combination of the two [64, 26] to create a fast and
                   highly scalable system for training deep RNNs in speech recognition.
                   Data has also been central to the success of end-to-end speech recognition, with over 7000 hours
                   of labeled speech used in Deep Speech 1 (DS1) [26]. Data augmentation has been highly effective
                   in improving the performance of deep learning in computer vision [39, 56, 14]. This has also been
                   showntoimprove speech systems [21, 26]. Techniques used for data augmentation in speech range
                   from simple noise addition [26] to complex perturbations such as simulating changes to the vocal
                   tract length and rate of speech of the speaker [31, 35].
                   Existing speech systems can also be used to bootstrap new data collection. In one approach, the
                   authors use one speech engine to align and Ô¨Ålter a thousand hours of read speech [46]. In another
                   approach, a heavy-weight ofÔ¨Çine speech recognizer is used to generate transcriptions for tens of
                   thousands of hours of speech [33]. This is then passed through a Ô¨Ålter and used to re-train the recog-
                   nizer, resulting in signiÔ¨Åcant performance gains. We draw inspiration from these past approaches in
                                                    3
                                        bootstrapping larger datasets and data augmentation to increase the effective amount of labeled data
                                        for our system.
                                        3      ModelArchitecture
                                        Asimple multi-layer model with a single recurrent layer cannot exploit thousands of hours of la-
                                        belled speech. In order to learn from datasets this large, we increase the model capacity via depth.
                                        Weexplorearchitectures with up to 11 layers including many bidirectional recurrent layers and con-
                                        volutional layers. These models have nearly 8 times the amount of computation per data example as
                                        the modelsinDeepSpeech1makingfastoptimizationandcomputationcritical. Inordertooptimize
                                        these models successfully, we use Batch Normalization for RNNs and a novel optimization curricu-
                                        lum we call SortaGrad. We also exploit long strides between RNN inputs to reduce computation
                                        per example by a factor of 3. This is helpful for both training and evaluation, though requires some
                                        modiÔ¨Åcations in order to work well with CTC. Finally, though many of our research results make
                                        use of bidirectional recurrent layers, we Ô¨Ånd that excellent models exist using only unidirectional
                                        recurrent layers‚Äîa feature that makes such models much easier to deploy. Taken together these
                                        features allow us to tractably optimize deep RNNs and improve performance by more than 40% in
                                        both English and Mandarin error rates over the smaller baseline models.
                                        3.1     Preliminaries
                                        Figure 1 shows the architecture of the DS2 system which at its core is similar to the previous DS1
                                        system [26]: a recurrent neural network (RNN) trained to ingest speech spectrograms and generate
                                        text transcriptions.
                                        Let a single utterance x(i) and label y(i) be sampled from a training set X                                                                   =
                                             (1)     (1)       (2)     (2)                                        (i)                                             (i)
                                        {(x      , y     ), (x     , y     ), . . .}.  Each utterance, x             , is a time-series of length T                    where every
                                                                                                     (i)                     (i)
                                        time-slice is a vector of audio features, x                     , t = 0,...,T             ‚àí1. We use a spectrogram of power
                                                                                                     t
                                        normalized audio clips as the features to the system, so x(i) denotes the power of the p‚Äôth frequency
                                                                                                                           t,p
                                        bin in the audio frame at time t. The goal of the RNN is to convert an input sequence x(i) into a
                                        Ô¨Ånal transcription y(i). For notational convenience, we drop the superscripts and use x to denote a
                                        chosen utterance and y the corresponding label.
                                        Theoutputsofthenetworkarethegraphemesofeachlanguage. Ateachoutputtime-stept,theRNN
                                        makesapredictionovercharacters,p(`t|x),where`t iseitheracharacterinthealphabetortheblank
                                        symbol. In English we have `t ‚àà {a, b, c, ...,z,space,apostrophe,blank}, where we have added
                                        the apostrophe as well as a space symbol to denote word boundaries. For the Mandarin system the
                                        network outputs simpliÔ¨Åed Chinese characters. We describe this in more detail in Section 3.9.
                                        The RNN model is composed of several layers of hidden units. The architectures we experiment
                                        withconsist of one or more convolutional layers, followed by one or more recurrent layers, followed
                                        byoneormorefullyconnectedlayers.
                                                                                                                 l                                        0
                                        The hidden representation at layer l is given by h with the convention that h represents the input
                                        x. The bottom of the network is one or more convolutions over the time dimension of the input. For
                                        a context window of size c, the i-th activation at time-step t of the convolutional layer is given by
                                                                                               hl    =f(wl‚ó¶hl‚àí1                 )                                                    (1)
                                                                                                 t,i           i      t‚àíc:t+c
                                        where ‚ó¶ denotes the element-wise product between the i-th Ô¨Ålter and the context window of the
                                        previous layers activations, and f denotes a unary nonlinear function. We use the clipped rectiÔ¨Åed-
                                        linear (ReLU) function œÉ(x) = min{max{x,0},20} as our nonlinearity. In some layers, usually
                                        the Ô¨Årst, we sub-sample by striding the convolution by s frames. The goal is to shorten the number
                                        of time-steps for the recurrent layers above.
                                        Following the convolutional layers are one or more bidirectional recurrent layers [57]. The forward
                                                   ‚Üí‚àí                                    ‚Üê‚àí
                                        in time hl and backward in time hl recurrent layer activations are computed as
                                                                                                 ‚Üí‚àí l           l‚àí1 ‚Üí‚àí l
                                                                                                  ht = g(ht          , h t‚àí1)                                                        (2)
                                                                                                 ‚Üê‚àí             l‚àí1 ‚Üê‚àí
                                                                                                  hl = g(h           , h l     )
                                                                                                     t          t         t+1
                                                                                                                4
                                                                                                                       CTC
                                                                                                                                                         Fully 
                                                                                                                                                    Connected
                                                                                                                                                     Recurrent
                                                                                                                                                           or
                                                                                                                                                         GRU
                                                                                   Batch                                                           (Bidirectional)
                                                                              Normalization
                                                                                                                                                      1D or 2D
                                                                                                                                                      Invariant
                                                                                                                                                    Convolution
                                                                                                                 Spectrogram
                                           Figure 1: Architecture of the DS2 system used to train on both English and Mandarin speech. We explore
                                           variants of this architecture by varying the number of convolutional layers from 1 to 3 and the number of
                                           recurrent or GRU layers from 1 to 7.
                                                                                                                                                                                        ‚Üí‚àí        ‚Üê‚àí
                                           The two sets of activations are summed to form the output activations for the layer hl = h l + h l.
                                           Thefunction g(¬∑) can be the standard recurrent operation
                                                                                              ‚Üí‚àí l               l  l‚àí1       ‚Üí‚àí l‚Üí‚àí l             l
                                                                                               ht = f(W ht                +U ht‚àí1+b)                                                                (3)
                                                          l                                                        ‚Üí‚àí l                                                          l
                                           whereW istheinput-hiddenweightmatrix, U istherecurrentweightmatrixandb isabiasterm.
                                           In this case the input-hidden weights are shared for both directions of the recurrence. The function
                                           g(¬∑) can also represent more complex recurrence operations such as the Long Short-Term Memory
                                           (LSTM)units[30] and the gated recurrent units (GRU) [11].
                                           After the bidirectional recurrent layers we apply one or more fully connected layers with
                                                                                                           l              l  l‚àí1         l
                                                                                                        h =f(W h                   +b)                                                              (4)
                                                                                                           t                 t
                                           Theoutput layer L is a softmax computing a probability distribution over characters given by
                                                                                                                                    L      L‚àí1
                                                                                                                         exp(w ¬∑h                 )
                                                                                             p(` = k|x) = P                         k      t                                                        (5)
                                                                                                  t                                    L      L‚àí1
                                                                                                                            exp(w ¬∑h                 )
                                                                                                                          j            j      t
                                           The model is trained using the CTC loss function [22]. Given an input-output pair (x,y) and the
                                           current parameters of the network Œ∏, we compute the loss function L(x,y;Œ∏) and its derivative with
                                           respect to the parameters of the network ‚àáŒ∏L(x,y;Œ∏). This derivative is then used to update the
                                           network parameters through the backpropagation through time algorithm.
                                           In the following subsections we describe the architectural and algorithmic improvements made rel-
                                           ative to DS1 [26]. Unless otherwise stated these improvements are language agnostic. We report
                                           results on an English speaker held out development set which is an internal dataset containing 2048
                                           utterances of primarily read speech. All models are trained on datasets described in Section 5.
                                           Wereport Word Error Rate (WER) for the English system and Character Error Rate (CER) for the
                                           Mandarin system. In both cases we integrate a language model in a beam search decoding step as
                                           described in Section 3.8.
                                                                                                                         5
                               Architecture     Hidden Units              Train                        Dev
                                                                  Baseline    BatchNorm       Baseline    BatchNorm
                               1 RNN,5total         2400           10.55            11.99      13.55            14.40
                               3 RNN,5total         1880            9.55             8.29      11.61            10.56
                               5 RNN,7total         1510            8.59             7.61      10.77              9.78
                               7 RNN,9total         1280            8.76             7.68      10.83              9.52
                           Table 1: Comparison of WER on a training and development set for various depths of RNN, with and without
                           BatchNorm. Thenumberofparametersiskeptconstantasthedepthincreases, thus the number of hidden units
                           per layer decreases. All networks have 38 million parameters. The architecture ‚ÄúM RNN, N total‚Äù implies 1
                           layer of 1D convolution at the input, M consecutive bidirectional RNN layers, and the rest as fully-connected
                           layers with N total layers in the network.
                           3.2  Batch Normalization for Deep RNNs
                           ToefÔ¨Åciently scale our model as we scale the training set, we increase the depth of the networks by
                           addingmorehiddenlayers,ratherthanmakingeachlayerlarger. Previousworkhasexamineddoing
                           so by increasing the number of consecutive bidirectional recurrent layers [24]. We explore Batch
                           Normalization (BatchNorm) as a technique to accelerate training for such networks [63] since they
                           often suffer from optimization issues.
                           Recent research has shown that BatchNorm improves the speed of convergence of recurrent nets,
                           without showing any improvement in generalization performance [37]. In contrast, we demonstrate
                           that when applied to very deep networks of simple RNNs on large data sets, batch normalization
                           substantially improves Ô¨Ånal generalization error while greatly accelerating training.
                           In a typical feed-forward layer containing an afÔ¨Åne transformation followed by a non-linearity f(¬∑),
                           weinsert a BatchNorm transformation by applying f(B(Wh)) instead of f(Wh+b), where
                                                           B(x) = Œ≥     x‚àíE[x]       +Œ≤.                                (6)
                                                                     (Var[x] +)1/2
                           The terms E and Var are the empirical mean and variance over a minibatch. The bias b of the
                           layer is dropped since its effect is cancelled by mean removal. The learnable parameters Œ≥ and Œ≤
                           allow the layer to scale and shift each hidden unit as desired. The constant  is small and positive,
                           and is included only for numerical stability. In our convolutional layers the mean and variance
                           are estimated over all the temporal output units for a given convolutional Ô¨Ålter on a minibatch.
                           The BatchNorm transformation reduces internal covariate shift by insulating a given layer from
                           potentially uninteresting changes in the mean and variance of the layer‚Äôs input.
                           WeconsidertwomethodsofextendingBatchNormtobidirectionalRNNs[37]. Anaturalextension
                           is to insert a BatchNorm transformation immediately before every non-linearity. Equation 3 then
                           becomes                        ‚Üí‚àí                    ‚Üí‚àí ‚Üí‚àí
                                                            l           l l‚àí1     l   l
                                                          h =f(B(W h          +U h       )).                            (7)
                                                            t             t           t‚àí1
                           Inthiscasethemeanandvariancestatisticsareaccumulatedoverasingletime-stepoftheminibatch.
                           The sequential dependence between time-steps prevents averaging over all time-steps. We Ô¨Ånd that
                           this technique does not lead to improvements in optimization. We also tried accumulating an average
                           over successive time-steps, so later time-steps are normalized over all present and previous time-
                           steps. This also proved ineffective and greatly complicated backpropagation.
                           WeÔ¨Åndthat sequence-wise normalization [37] overcomes these issues. The recurrent computation
                           is given by                    ‚Üí‚àí                     ‚Üí‚àí ‚Üí‚àí
                                                            l           l l‚àí1      l  l
                                                          h =f(B(W h         ) + U h      ).                            (8)
                                                            t             t           t‚àí1
                           For each hidden unit, we compute the mean and variance statistics over all items in the minibatch
                           over the length of the sequence. Figure 2 shows that deep networks converge faster with sequence-
                           wise normalization. Table 1 shows that the performance improvement from sequence-wise normal-
                           ization increases with the depth of the network, with a 12% performance difference for the deepest
                           network. When comparing depth, in order to control for model size we hold constant the total
                                                                          6
                                                                           60                                                              5-1 BN
                                                                                                                                           5-1 No BN
                                                                           50                                                              9-7 BN
                                                                                                                                           9-7 No BN
                                                                          t
                                                                          os40
                                                                          C
                                                                           30
                                                                           20
                                                                                     50          100         150         200         250         300
                                                                                                           Iteration (‚á•103)
                                        Figure 2: Training curves of two models trained with and without BatchNorm. We start the plot after the Ô¨Årst
                                        epoch of training as the curve is more difÔ¨Åcult to interpret due to the SortaGrad curriculum method mentioned
                                        in Section 3.3
                                                                                                  Train                                       Dev
                                                                                     Baseline           BatchNorm               Baseline           BatchNorm
                                                              NotSorted                10.71                       8.04           11.96                       9.78
                                                              Sorted                     8.76                      7.68           10.83                       9.52
                                        Table 2: Comparison of WER on a training and development set with and without SortaGrad, and with and
                                        without batch normalization.
                                        number of parameters and still see strong performance gains. We would expect to see even larger
                                        improvements from depth if we held constant the number of activations per layer and added lay-
                                        ers. We also Ô¨Ånd that BatchNorm harms generalization error for the shallowest network just as it
                                        converges slower for shallower networks.
                                        The BatchNorm approach works well in training, but is difÔ¨Åcult to implement for a deployed ASR
                                        system, since it is often necessary to evaluate a single utterance in deployment rather than a batch.
                                        We Ô¨Ånd that normalizing each neuron to its mean and variance over just the sequence degrades
                                        performance. Instead, we store a running average of the mean and variance for the neuron collected
                                        during training, and use these for evaluation in deployment [63]. Using this technique, we can
                                        evaluate a single utterance at a time with better results than evaluating with a large batch.
                                        3.3      SortaGrad
                                        Training on examples of varying length pose some algorithmic challenges. One possible solution is
                                        truncating backpropagation through time [68], so that all examples have the same sequence length
                                        during training [52]. However, this can inhibit the ability to learn longer term dependencies. Other
                                        works have found that presenting examples in order of difÔ¨Åculty can accelerate online learning [6,
                                        70]. A common theme in many sequence learning problems including machine translation and
                                        speech recognition is that longer examples tend to be more challenging [11].
                                        TheCTCcostfunctionthat we use implicitly depends on the length of the utterance,
                                                                                                                             T
                                                                               L(x,y;Œ∏) = ‚àílog                   X Yp (`|x;Œ∏).                                                        (9)
                                                                                                                                   ctc   t
                                                                                                            `‚ààAlign(x,y) t
                                        where Align(x,y) is the set of all possible alignments of the characters of the transcription y to
                                        frames of input x under the CTC operator. In equation 9, the inner term is a product over time-steps
                                        of the sequence, which shrinks with the length of the sequence since pctc(`t|x;Œ∏) < 1. This moti-
                                        vates a curriculum learning strategy we title SortaGrad. SortaGrad uses the length of the utterance
                                        as a heuristic for difÔ¨Åculty, since long utterances have higher cost than short utterances.
                                                                                                                 7
                                                                        Architecture                    Simple RNN            GRU
                                                                        5 layers, 1 Recurrent                     14.40      10.53
                                                                        5 layers, 3 Recurrent                     10.56        8.00
                                                                        7 layers, 5 Recurrent                      9.78        7.79
                                                                        9 layers, 7 Recurrent                      9.52        8.19
                                    Table 3: Comparison of development set WER for networks with either simple RNN or GRU, for various
                                    depths. All models have batch normalization, one layer of 1D-invariant convolution, and approximately 38
                                    million parameters.
                                    In the Ô¨Årst training epoch, we iterate through the training set in increasing order of the length of
                                    the longest utterance in the minibatch. After the Ô¨Årst epoch, training reverts back to a random order
                                    over minibatches. Table 2 shows a comparison of training cost with and without SortaGrad on the
                                    9 layer model with 7 recurrent layers. This effect is particularly pronounced for networks without
                                    BatchNorm, since they are numerically less stable. In some sense the two techniques substitute for
                                    one another, though we still Ô¨Ånd gains when applying SortaGrad and BatchNorm together. Even
                                    with BatchNorm we Ô¨Ånd that this curriculum improves numerical stability and sensitivity to small
                                    changes in training. Numerical instability can arise from different transcendental function imple-
                                    mentations in the CPU and the GPU, especially when computing the CTC cost. This curriculum
                                    gives comparable results for both implementations.
                                    WesuspectthatthesebeneÔ¨Åtsoccurprimarily because long utterances tend to have larger gradients,
                                    yet we use a Ô¨Åxed learning rate independent of utterance length. Furthermore, longer utterances are
                                    morelikely to cause the internal state of the RNNs to explode at an early stage in training.
                                    3.4     ComparisonofsimpleRNNsandGRUs
                                    ThemodelswehaveshownsofararesimpleRNNsthathavebidirectionalrecurrent layers with the
                                    recurrence for both the forward in time and backward in time directions modeled by Equation 3.
                                    Current research in speech and language processing has shown that having a more complex re-
                                    currence can allow the network to remember state over more time-steps while making them more
                                    computationallyexpensivetotrain[52,8,62,2]. Twocommonlyusedrecurrentarchitecturesarethe
                                    Long Short-Term Memory (LSTM) units [30] and the Gated Recurrent Units (GRU) [11], though
                                    manyothervariations exist. A recent comprehensive study of thousands of variations of LSTM and
                                    GRUarchitectures showed that a GRU is comparable to an LSTM with a properly initialized forget
                                    gate bias, and their best variants are competitive with each other [32]. We decided to examine GRUs
                                    because experiments on smaller data sets showed the GRU and LSTM reach similar accuracy for
                                    the same number of parameters, but the GRUs were faster to train and less likely to diverge.
                                    TheGRUsweusearecomputedby
                                                                               z =œÉ(W x +U h                     +b )
                                                                                 t           z t        z t‚àí1        z
                                                                               r =œÉ(W x +U h                     +b )
                                                                                 t           r t        r t‚àí1        r                                             (10)
                                                                               Àú
                                                                               h =f(W x +r ‚ó¶U h                        +b )
                                                                                 t           h t       t      h t‚àí1         h
                                                                                                               Àú
                                                                               h =(1‚àíz )h                +zh
                                                                                 t             t   t‚àí1        t  t
                                    where œÉ(¬∑) is the sigmoid function, z and r represent the update and reset gates respectively, and
                                    we drop the layer superscripts for simplicity. We differ slightly from the standard GRU in that we
                                    multiply the hidden state h              byU priortoscalingbytheresetgate. Thisallowsforalloperations
                                                                        t‚àí1         h
                                    on ht‚àí1 to be computed in a single matrix multiplication. The output nonlinearity f(¬∑) is typically
                                    the hyperbolic tangent function tanh. However, we Ô¨Ånd similar performance for tanh and clipped-
                                    ReLUnonlinearities and choose to use the clipped-ReLU for simplicity and uniformity with the rest
                                    of the network.
                                    Both GRU and simple RNN architectures beneÔ¨Åt from batch normalization and show strong re-
                                    sults with deep networks. However, Table 3 shows that for a Ô¨Åxed number of parameters, the GRU
                                    architectures achieve better WER for all network depths. This is clear evidence of the long term
                                    dependencies inherent in the speech recognition task present both within individual words and be-
                                                                                                      8
                            Architecture   Channels        Filter dimension        Stride          Regular Dev    Noisy Dev
                            1-layer 1D     1280            11                      2                   9.52         19.36
                            2-layer 1D     640, 640        5, 5                    1, 2                9.67         19.21
                            3-layer 1D     512, 512, 512   5, 5, 5                 1, 1, 2             9.20         20.22
                            1-layer 2D     32              41x11                   2x2                 8.94         16.22
                            2-layer 2D     32, 32          41x11, 21x11            2x2, 2x1            9.06         15.71
                            3-layer 2D     32, 32, 96      41x11, 21x11, 21x11     2x2, 2x1, 2x1       8.61         14.74
                           Table 4: Comparison of WER for various arrangements of convolutional layers. In all cases, the convolutions
                           are followed by 7 recurrent layers and 1 fully connected layer. For 2D-invariant convolutions the Ô¨Årst dimen-
                           sion is frequency and the second dimension is time. All models have BatchNorm, SortaGrad, and 35 million
                           parameters.
                           tweenwords. AswediscussinSection3.8,evensimpleRNNsareabletoimplicitlylearnalanguage
                           model due to the large amount of training data. Interestingly, the GRU networks with 5 or more re-
                           current layers do not signiÔ¨Åcantly improve performance. We attribute this to the thinning from 1728
                           hidden units per layer for 1 recurrent layer to 768 hidden units per layer for 7 recurrent layers, to
                           keep the total number of parameters constant.
                           TheGRUnetworksoutperformthesimpleRNNsinTable3. However,inlaterresults(Section6)we
                           Ô¨Ånd that as we scale up the model size, for a Ô¨Åxed computational budget the simple RNN networks
                           perform slightly better. Given this, most of the remaining experiments use the simple RNN layers
                           rather than the GRUs.
                           3.5  FrequencyConvolutions
                           Temporal convolution is commonly used in speech recognition to efÔ¨Åciently model temporal trans-
                           lation invariance for variable length utterances. This type of convolution was Ô¨Årst proposed for
                           neural networks in speech more than 25 years ago [67]. Many neural network speech models have a
                           Ô¨Årst layer that processes input frames with some context window [16, 66]. This can be viewed as a
                           temporal convolution with a stride of one.
                           Additionally, sub-sampling is essential to make recurrent neural networks computationally tractable
                           with high sample-rate audio. The DS1 system accomplished this through the use of a spectrogram
                           as input and temporal convolution in the Ô¨Årst layer with a stride parameter to reduce the number of
                           time-steps [26].
                           Convolutions in frequency and time domains, when applied to the spectral input features prior to
                           any other processing, can slightly improve ASR performance [1, 53, 60]. Convolution in frequency
                           attempts to model spectral variance due to speaker variability more concisely than what is pos-
                           sible with large fully connected networks. Since spectral ordering of features is removed by fully-
                           connectedandrecurrentlayers, frequencyconvolutionsworkbetterastheÔ¨Årstlayersofthenetwork.
                           Weexperimentwithaddingbetweenoneandthreelayersofconvolution. Thesearebothinthetime-
                           and-frequency domain (2D invariance) and in the time-only domain (1D invariance). In all cases we
                           use a same convolution, preserving the number of input features in both frequency and time. In
                           somecases, we specify a stride across either dimension which reduces the size of the output. We do
                           not explicitly control for the number of parameters, since convolutional layers add a small fraction
                           of parameters to our networks. All networks shown in Table 4 have about 35 million parameters.
                           We report results on two datasets‚Äîa development set of 2048 utterances (‚ÄúRegular Dev‚Äù) and a
                           much noisier dataset of 2048 utterances (‚ÄúNoisy Dev‚Äù) randomly sampled from the CHiME 2015
                           development datasets [4]. We Ô¨Ånd that multiple layers of 1D-invariant convolutions provides a very
                           small beneÔ¨Åt. The 2D-invariant convolutions improve results substantially on noisy data, while
                           providing a small beneÔ¨Åt on clean data. The change from one layer of 1D-invariant convolution to
                           three layers of 2D-invariant convolution improves WER by 23.9% on the noisy development set.
                                                                          9
                                                              DevnoLM                        DevLM
                                             Stride    Unigrams        Bigrams       Unigrams       Bigrams
                                                2        14.93          14.56           9.52           9.66
                                                3        15.01          15.60           9.65          10.06
                                                4        18.86          14.84          11.92           9.93
                            Table 5: Comparison of WER with different amounts of striding for unigram and bigram outputs on a model
                            with 1 layer of 1D-invariant convolution, 7 recurrent layers, and 1 fully connected layer. All models have
                            BatchNorm, SortaGrad, and 35 million parameters. The models are compared on a development set with and
                            without the use of a 5-gram language model.
                            3.6   Striding
                            In the convolutional layers, we apply a longer stride and wider context to speed up training as fewer
                            time-stepsarerequiredtomodelagivenutterance. Downsamplingtheinputsound(throughFFTand
                            convolutional striding) reduces the number of time-steps and computation required in the following
                            layers, but at the expense of reduced performance.
                            In our Mandarin models, we employ striding in the straightforward way. However, in English,
                            striding can reduce accuracy simply because the output of our network requires at least one time-
                            step per output character, and the number of characters in English speech per time-step is high
                                                                      2
                            enough to cause problems when striding . To overcome this, we can enrich the English alphabet
                            with symbols representing alternate labellings like whole words, syllables or non-overlapping n-
                            grams. In practice, we use non-overlapping bi-graphemes or bigrams, since these are simple to
                            construct, unlike syllables, and there are few of them compared to alternatives such as whole words.
                            Wetransform unigram labels into bigram labels through a simple isomorphism.
                            Non-overlappingbigramsshortenthelengthoftheoutputtranscriptionandthusallowforadecrease
                            in the length of the unrolled RNN. The sentence the cat sat with non-overlapping bigrams is seg-
                            mentedas[th,e,space,ca,t,space,sa,t]. Noticethatforwordswithoddnumberofcharacters,the
                            last character becomes an unigram and space is treated as an unigram as well. This isomorphism
                            ensures that the same words are always composed of the same bigram and unigram tokens. The
                            output set of bigrams consists of all bigrams that occur in the training set.
                            In Table 5 we show results for both the bigram and unigram systems for various levels of striding,
                            with or without a language model. We observe that bigrams allow for larger strides without any
                            sacriÔ¨Åce in in the word error rate. This allows us to reduce the number of time-steps of the unrolled
                            RNNbeneÔ¨Åtingbothcomputationandmemoryusage.
                            3.7   RowConvolutionandUnidirectionalModels
                            Bidirectional RNN models are challenging to deploy in an online, low-latency setting, because they
                            are built to operate on an entire sample, and so it is not possible to perform the transcription process
                            as the utterance streams from the user. We have found an unidirectional architecture that performs
                            as well as our bidirectional models. This allows us to use unidirectional, forward-only RNN layers
                            in our deployment system.
                            Toaccomplish this, we employ a special layer that we call row convolution, shown in Figure 3. The
                            intuition behind this layer is that we only need a small portion of future information to make an
                            accurate prediction at the current time-step. Suppose at time-step t, we use a future context of œÑ
                            steps. We now have a feature matrix h        =[h ,h     , ..., h  ] of size d √ó (œÑ + 1). We deÔ¨Åne a
                                                                   t:t+œÑ     t   t+1      t+œÑ
                            parameter matrix W of the same size as h        . The activations r for the new layer at time-step t
                                                                       t:t+œÑ                   t
                            are
                               2Chinese characters are more similar to English syllables than English characters. This is reÔ¨Çected in our
                            training data, where there are on average 14.1 characters/s in English, while only 3.3 characters/s in Mandarin.
                            Conversely, the Shannon entropy per character as calculated from occurrence in the training set, is less in
                            English due to the smaller character set‚Äî4.9 bits/char compared to 12.6 bits/char in Mandarin. This implies
                            that spoken Mandarin has a lower temporal entropy density, ‚àº41 bits/s compared to ‚àº58 bits/s, and can thus
                            moreeasily be temporally compressed without losing character information.
                                                                             10
                                                              Row conv layer          rt         rt+1        rt+2         rt+3
                                                              Recurrent layer         ht         ht+1        ht+2         ht+3
                                                          Figure 3: Row convolution architecture with future context size of 2
                                                                                 œÑ+1
                                                                        r    =XW h                    , for 1 ‚â§ i ‚â§ d.                                     (11)
                                                                          t,i            i,j t+j‚àí1,i
                                                                                 j=1
                                   Since the convolution-like operation in Eq. 11 is row oriented for both W and ht:t+œÑ, we call this
                                   layer row convolution.
                                   Weplace the row convolution layer above all recurrent layers. This has two advantages. First, this
                                   allowsustostreamallcomputationbelowtherowconvolutionlayeronaÔ¨Ånergranularitygivenlittle
                                   future context is needed. Second, this results in better CER compared to the best bidirectional model
                                   for Mandarin. We conjecture that the recurrent layers have learned good feature representations,
                                   so the row convolution layer simply gathers the appropriate information to feed to the classiÔ¨Åer.
                                   Results for a unidirectional Mandarin speech system with row convolution and a comparison to a
                                   bidirectional model are given in Section 7 on deployment.
                                   3.8    LanguageModel
                                   Wetrain our RNN Models over millions of unique utterances, which enables the network to learn a
                                   powerful implicit language model. Our best models are quite adept at spelling, without any external
                                   languageconstraints. Further, inourdevelopmentdatasetsweÔ¨Åndmanycaseswhereourmodelscan
                                   implicitly disambiguate homophones‚Äîfor example, ‚Äúhe expects the Japanese agent to sell it for two
                                   hundred seventy Ô¨Åve thousand dollars‚Äù. Nevertheless, the labeled training data is small compared
                                   to the size of unlabeled text corpora that are available. Thus we Ô¨Ånd that WER improves when we
                                   supplement our system with a language model trained from external text.
                                   We use an n-gram language model since they scale well to large amounts of unlabeled text [26].
                                   For English, our language model is a Kneser-Ney smoothed 5-gram model with pruning that is
                                   trained using the KenLM toolkit [28] on cleaned text from the Common Crawl Repository3. The
                                   vocabularyisthemostfrequentlyused400,000wordsfrom250millionlinesoftext,whichproduces
                                   a language model with about 850 million n-grams. For Mandarin, the language model is a Kneser-
                                   Ney smoothed character level 5-gram model with pruning that is trained on an internal text corpus
                                   of 8 billion lines of text. This produces a language model with about 2 billion n-grams.
                                   Duringinference we search for the transcription y that maximizes Q(y) shown in Equation 12. This
                                   is a linear combination of log probabilities from the CTC trained network and language model, along
                                   with a word insertion term [26].
                                                            Q(y) = log(p (y|x))+Œ±log(p (y))+Œ≤ word_count(y)                                                (12)
                                                                              ctc                     lm
                                   The weight Œ± controls the relative contributions of the language model and the CTC network. The
                                   weight Œ≤ encourages more words in the transcription. These parameters are tuned on a development
                                   set. We use a beam search to Ô¨Ånd the optimal transcription [27].
                                       3http://commoncrawl.org
                                                                                                11
                                        Language  Architecture   DevnoLM      DevLM
                                        English   5-layer, 1 RNN   27.79       14.39
                                        English   9-layer, 7 RNN   14.93        9.52
                                        Mandarin  5-layer, 1 RNN    9.80        7.13
                                        Mandarin  9-layer, 7 RNN    7.55        5.81
                       Table 6: Comparison of WER for English and CER for Mandarin with and without a language model. These
                       are simple RNN models with only one layer of 1D invariant convolution.
                       Table 6 shows that an external language model helps both English and Mandarin speech systems.
                       Therelative improvement given by the language model drops from 48% to 36% in English and 27%
                       to 23% in Mandarin, as we go from a model with 5 layers and 1 recurrent layer to a model with 9
                       layers and 7 recurrent layers. We hypothesize that the network builds a stronger implicit language
                       model with more recurrent layers.
                       TherelativeperformanceimprovementfromalanguagemodelishigherinEnglishthaninMandarin.
                       Weattribute this to the fact that a Chinese character represents a larger block of information than
                       an English character. For example, if we output directly to syllables or words in English, the model
                       would make fewer spelling mistakes and the language model would likely help less.
                       3.9 Adaptation to Mandarin
                       The techniques that we have described so far can be used to build an end-to-end Mandarin speech
                       recognition system that outputs Chinese characters directly. This precludes the need to construct a
                       pronunciation model, which is often a fairly involved component for porting speech systems to other
                       languages [59]. Direct output to characters also precludes the need to explicitly model language
                       speciÔ¨Åc pronunciation features. For example we do not need to model Mandarin tones explicitly, as
                       somespeechsystems must do [59, 45].
                       TheonlyarchitecturalchangeswemaketoournetworksareduetothecharacteristicsoftheChinese
                       character set. Firstly, the output layer of the network outputs about 6000 characters, which includes
                       the Roman alphabet, since hybrid Chinese-English transcripts are common. We incur an out of
                       vocabulary error at evaluation time if a character is not contained in this set. This is not a major
                       concern, as our test set has only 0.74% out of vocab characters.
                       Weuse a character level language model in Mandarin as words are not usually segmented in text.
                       ThewordinsertiontermofEquation12becomesacharacterinsertionterm. Inaddition,weÔ¨Åndthat
                       the performance of the beam search during decoding levels off at a smaller beam size. This allows
                       us to use a beam size of 200 with a negligible degradation in CER. In Section 6.2, we show that
                       our Mandarin speech models show roughly the same improvements to architectural changes as our
                       English speech models.
                       4  SystemOptimizations
                       Our networks have tens of millions of parameters, and the training algorithm takes tens of single-
                       precision exaFLOPs to converge. Since our ability to evaluate hypotheses about our data and mod-
                       els depends on the ability to train models quickly, we built a highly optimized training system.
                                                                                     ++
                       This system has two main components‚Äîa deep learning library written in C , along with a high-
                                                                          ++
                       performance linear algebra library written in both CUDA and C . Our optimized software, running
                       on dense compute nodes with 8 Titan X GPUs per node, allows us to sustain 24 single-precision
                       teraFLOP/second when training a single model on one node. This is 45% of the theoretical peak
                       computational throughput of each node. We also can scale to multiple nodes, as outlined in the next
                       subsection.
                       4.1 Scalability and Data-Parallelism
                       We use the standard technique of data-parallelism to train on multiple GPUs using synchronous
                       SGD. Our most common conÔ¨Åguration uses a minibatch of 512 on 8 GPUs. Our training pipeline
                                                              12
               binds one process to each GPU. These processes then exchange gradient matrices during the back-
               propagation by using all-reduce, which exchanges a matrix between multiple processes and sums
               the result so that at the end, each process has a copy of the sum of all matrices from all processes.
               We Ô¨Ånd synchronous SGD useful because it is reproducible and deterministic. We have found
               that the appearance of non-determinism in our system often signals a serious bug, and so having
               reproducibility as a goal has greatly facilitates debugging. In contrast, asynchronous methods such
               as asynchronous SGD with parameter servers as found in Dean et al. [18] typically do not provide
               reproducibility and are therefore more difÔ¨Åcult to debug. Synchronous SGD is simple to understand
               and implement. It scales well as we add multiple nodes to the training process.
                           219
                           218                    5-3 (2560)
                                                  9-7 (1760)
                           )217
                           s
                           d216
                           on
                           ec215
                           s
                           (
                           e214
                           m
                           i
                           T213
                           212
                           211
                            20  21  22  23  24  25 26  27
                                         GPUs
               Figure 4: Scaling comparison of two networks‚Äîa 5 layer model with 3 recurrent layers containing 2560
               hidden units in each layer and a 9 layer model with 7 recurrent layers containing 1760 hidden units in each
               layer. The times shown are to train 1 epoch. The 5 layer model trains faster because it uses larger matrices and
               is more computationally efÔ¨Åcient.
               Figure 4 shows that time taken to train one epoch halves as we double the number of GPUs that
               we train on, thus achieving near-linear weak scaling. We keep the minibatch per GPU constant at
               64 during this experiment, effectively doubling the minibatch as we double the number of GPUs.
               Although we have the ability to scale to large minibatches, we typically use either 8 or 16 GPUs
               during training with a minibatch of 512 or 1024, in order to converge to the best result.
               Since all-reduce is critical to the scalability of our training, we wrote our own implementation of
               the ring algorithm [48, 65] for higher performance and better stability. Our implementation avoids
               extraneous copies between CPU and GPU, and is fundamental to our scalability. We conÔ¨Ågure
               OpenMPI with the smcuda transport that can send and receive buffers residing in the memory of
               two different GPUs by using GPUDirect. When two GPUs are in the same PCI root complex,
               this avoids any unnecessary copies to CPU memory. This also takes advantage of tree-structured
               interconnects by running multiple segments of the ring concurrently between neighboring devices.
               WebuiltourimplementationusingMPIsendandreceive,alongwithCUDAkernelsfortheelement-
               wise operations.
               Table7comparestheperformanceofourall-reduceimplementationwiththatprovidedbyOpenMPI
               version 1.8.5. We report the time spent in all-reduce for a full training run that ran for one epoch
               on our English dataset using a 5 layer, 3 recurrent layer architecture with 2560 hidden units for all
               layers. In this table, we use a minibatch of 64 per GPU, expanding the algorithmic minibatch as we
               scale to more GPUs. We see that our implementation is considerably faster than OpenMPI‚Äôs when
               the communication is within a node (8 GPUs or less). As we increase the number of GPUs and
               increase the amount of inter-node communication, the gap shrinks, although our implementation is
               still 2-4X faster.
               All of our training runs use either 8 or 16 GPUs, and in this regime, our all-reduce implementation
               results in 2.5√ó faster training for the full training run, compared to using OpenMPI directly. Opti-
               mizing all-reduce has thus resulted in important productivity beneÔ¨Åts for our experiments, and has
               madeoursimplesynchronous SGDapproachscalable.
                                        13
                                                   GPU       OpenMPI            Our         Performance
                                                             all-reduce      all-reduce         Gain
                                                      4       55359.1          2587.4          21.4
                                                      8       48881.6          2470.9          19.8
                                                     16       21562.6          1393.7          15.5
                                                     32        8191.8          1339.6           6.1
                                                     64        1395.2           611.0           2.3
                                                    128        1602.1           422.6           3.8
                            Table 7: Comparison of two different all-reduce implementations. All times are in seconds. Performance gain
                            is the ratio of OpenMPI all-reduce time to our all-reduce time.
                                      Language     Architecture       CPUCTCTime GPUCTCTime Speedup
                                      English      5-layer, 3 RNN         5888.12             203.56          28.9
                                      Mandarin     5-layer, 3 RNN         1688.01             135.05          12.5
                            Table 8: Comparison of time spent in seconds in computing the CTC loss function and gradient in one epoch
                            for two different implementations. Speedup is the ratio of CPU CTC time to GPU CTC time.
                            4.2  GPUimplementationofCTClossfunction
                            Calculating the CTC loss function is more complicated than performing forward and back prop-
                            agation on our RNN architectures. Originally, we transferred activations from the GPUs to the
                            CPU,wherewecalculatedthelossfunctionusinganOpenMPparallelizedimplementationofCTC.
                            However, this implementation limited our scalability rather signiÔ¨Åcantly, for two reasons. Firstly,
                            it became computationally more signiÔ¨Åcant as we improved efÔ¨Åciency and scalability of the RNN
                            itself. Secondly, transferring large activation matrices between CPU and GPU required us to spend
                            interconnect bandwidth for CTC, rather than on transferring gradient matrices to allow us to scale
                            using data parallelism to more processors.
                            To overcome this, we wrote a GPU implementation of the CTC loss function. Our parallel imple-
                            mentation relies on a slight refactoring to simplify the dependences in the CTC calculation, as well
                            as the use of optimized parallel sort implementations from ModernGPU [5]. We give more details
                            of this parallelization in the Appendix.
                            Table 8 compares the performance of two CTC implementations. The GPU implementation saves
                            us 95 minutes per epoch in English, and 25 minutes in Mandarin. This reduces overall training time
                            by10-20%,whichisalsoanimportant productivity beneÔ¨Åt for our experiments.
                            4.3  Memoryallocation
                            Our system makes frequent use of dynamic memory allocations to GPU and CPU memory, mainly
                            to store activation data for variable length utterances, and for intermediate results. Individual al-
                            locations can be very large; over 1 GB for the longest utterances. For these very large allocations
                            we found that CUDA‚Äôs memory allocator and even std::malloc introduced signiÔ¨Åcant overhead
                            into our application‚Äîover a 2x slowdown from using std::malloc in some cases. This is because
                            both cudaMalloc and std::malloc forward very large allocations to the operating system or GPU
                            driver to update the system page tables. This is a good optimization for systems running multiple
                            applications, all sharing memory resources, but editing page tables is pure overhead for our system
                            where nodes are dedicated entirely to running a single model. To get around this limitation, we
                            wrote our own memory allocator for both CPU and GPU allocations. Our implementation follows
                            the approach of the last level shared allocator in jemalloc: all allocations are carved out of contigu-
                            ous memory blocks using the buddy algorithm [34]. To avoid fragmentation, we preallocate all of
                            GPU memory at the start of training and subdivide individual allocations from this block. Simi-
                            larly, we set the CPU memory block size that we forward to mmap to be substantially larger than
                            std::malloc, at 12GB.
                                                                            14
                                                                                     Dataset                Speech Type              Hours
                                                                                     WSJ                    read                           80
                                                                                     Switchboard            conversational               300
                                                                                     Fisher                 conversational             2000
                                                                                     LibriSpeech            read                         960
                                                                                     Baidu                  read                       5000
                                                                                     Baidu                  mixed                      3600
                                                                                     Total                                           11940
                                        Table 9: Summary of the datasets used to train DS2 in English. The Wall Street Journal (WSJ), Switchboard
                                        and Fisher [13] corpora are all published by the Linguistic Data Consortium. The LibriSpeech dataset [46] is
                                        available free on-line. The other datasets are internal Baidu corpora.
                                        Mostofthememoryrequiredfortrainingdeeprecurrentnetworksisusedtostoreactivationsthrough
                                        each layer for use by back propagation, not to store the parameters of the network. For example,
                                        storing the weights for a 70M parameter network with 9 layers requires approximately 280 MB of
                                        memory, but storing the activations for a batch of 64, seven-second utterances requires 1.5 GB of
                                        memory. TitanX GPUs include 12GB of GDDR5 RAM, and sometimes very deep networks can
                                        exceedtheGPUmemorycapacitywhenprocessinglongutterances. Thiscanhappenunpredictably,
                                        especially when the distribution of utterance lengths includes outliers, and it is desirable to avoid a
                                        catastrophic failure when this occurs. When a requested memory allocation exceeds available GPU
                                        memory, we allocate page-locked GPU-memory-mapped CPU memory using cudaMallocHost in-
                                        stead. This memory can be accessed directly by the GPU by forwarding individual memory trans-
                                        actions over PCIe at reduced bandwidth, and it allows a model to continue to make progress even
                                        after encountering an outlier.
                                        The combination of fast memory allocation with a fallback mechanism that allows us to slightly
                                        overÔ¨Çow available GPU memory in exceptional cases makes the system signiÔ¨Åcantly simpler, more
                                        robust, and more efÔ¨Åcient.
                                        5      Training Data
                                        Large-scale deep learning systems require an abundance of labeled training data. We have collected
                                        an extensive training dataset for both English and Mandarin speech models, in addition to augment-
                                        ing our training with publicly available datasets. In English we use 11,940 hours of labeled speech
                                        data containing 8 million utterances summarized in Table 9. For the Mandarin system we use 9,400
                                        hours of labeled audio containing 11 million utterances. The Mandarin speech data consists of in-
                                        ternal Baidu corpora, representing a mix of read speech and spontaneous speech, in both standard
                                        Mandarin and accented Mandarin.
                                        5.1      Dataset Construction
                                        Someoftheinternal English (3,600 hours) and Mandarin (1,400 hours) datasets were created from
                                        rawdatacapturedaslongaudioclipswithnoisytranscriptions. Thelengthoftheseclipsrangedfrom
                                        several minutes to more than hour, making it impractical to unroll them in time in the RNN during
                                        training. To solve this problem, we developed an alignment, segmentation and Ô¨Åltering pipeline that
                                        can generate a training set with shorter utterances and few erroneous transcriptions.
                                        The Ô¨Årst step in the pipeline is to use an existing bidirectional RNN model trained with CTC to
                                        align the transcription to the frames of audio. For a given audio-transcript pair, (x,y), we Ô¨Ånd the
                                        alignment that maximizes
                                                                                                                   T
                                                                                          `‚àó = argmax Yp (` |x;Œ∏).                                                                  (13)
                                                                                                                         ctc   t
                                                                                                  `‚ààAlign(x,y) t
                                        ThisisessentiallyaViterbialignmentfoundusingaRNNmodeltrainedwithCTC.SinceEquation9
                                        integrates over the alignment, the CTC loss function is never explicitly asked to produce an accurate
                                        alignment. In principle, CTC could choose to emit all the characters of the transcription after some
                                                                                                                15
                           Ô¨Åxed delay and this can happen with unidirectional RNNs [54]. However, we found that CTC
                           produces an accurate alignment when trained with a bidirectional RNN.
                           Following the alignment is a segmentation step that splices the audio and the corresponding aligned
                           transcription whenever it encounters a long series of consecutive blank labels occurs, since this
                           usually denotes a stretch of silence. By tuning the number of consecutive blanks, we can tune the
                           length of the utterances generated. For the English speech data, we also require a space token to be
                           within the stretch of blanks in order to segment only on word boundaries. We tune the segmentation
                           to generate utterances that are on average 7 seconds long.
                           The Ô¨Ånal step in the pipeline removes erroneous examples that arise from a failed alignment. We
                           crowd source the ground truth transcriptions for several thousand examples. The word level edit
                           distance between the ground truth and the aligned transcription is used to produce a good or bad
                           label. The threshold for the word level edit distance is chosen such that the resulting WER of the
                           good portion of the development set is less than 5%. We then train a linear classiÔ¨Åer to accurately
                           predict bad examples given the input features generated from the speech recognizer. We Ô¨Ånd the
                           following features useful: the raw CTC cost, the CTC cost normalized by the sequence length,
                           the CTC cost normalized by the transcript length, the ratio of the sequence length to the transcript
                           length, the number of words in the transcription and the number of characters in the transcription.
                           For the English dataset, we Ô¨Ånd that the Ô¨Åltering pipeline reduces the WER from 17% to 5% while
                           retaining more than 50% of the examples.
                           5.2  DataAugmentation
                           We augment our training data by adding noise to increase the effective size of our training data
                           and to improve our robustness to noisy speech [26]. Although the training data contains some
                           intrinsic noise, we can increase the quantity and variety of noise through augmentation. Too much
                           noise augmentation tends to make optimization difÔ¨Åcult and can lead to worse results, and too little
                           noise augmentation makes the system less robust to low signal-to-noise speech. We Ô¨Ånd that a good
                           balanceistoaddnoiseto40%oftheutterancesthatarechosenatrandom. Thenoisesourceconsists
                           of several thousand hours of randomly selected audio clips combined to produce hundreds of hours
                           of noise.
                           5.3  Scaling Data
                           OurEnglish and Mandarin corpora are substantially larger than those commonly reported in speech
                           recognition literature. In Table 10, we show the effect of increasing the amount of labeled training
                           data on WER. This is done by randomly sampling the full dataset before training. For each dataset,
                           the model was trained for up to 20 epochs though usually early-stopped based on the error on a held
                           out development set. We note that the WER decreases with a power law for both the regular and
                           noisy development sets. The WER decreases by ‚àº40% relative for each factor of 10 increase in
                           training set size. We also observe a consistent gap in WER (‚àº60% relative) between the regular and
                           noisy datasets, implying that more data beneÔ¨Åts both cases equally.
                           This implies that a speech system will continue to improve with more labeled training data. We
                           hypothesize that equally as important as increasing raw number of hours is increasing the number
                           of speech contexts that are captured in the dataset. A context can be any property that makes speech
                           unique including different speakers, background noise, environment, and microphone hardware.
                           While we do not have the labels needed to validate this claim, we suspect that measuring WER as
                           a function of speakers in the dataset would lead to much larger relative gains than simple random
                           sampling.
                           6   Results
                           To better assess the real-world applicability of our speech system, we evaluate on a wide range of
                           test sets. We use several publicly available benchmarks and several test sets collected internally.
                           Together these test sets represent a wide range of challenging speech environments including low
                           signal-to-noise ratios (noisy and far-Ô¨Åeld), accented, read, spontaneous and conversational speech.
                                                                         16
                                                Fraction of Data   Hours    Regular Dev     Noisy Dev
                                                      1%              120      29.23          50.97
                                                    10%             1200       13.80          22.99
                                                    20%             2400       11.65          20.41
                                                    50%             6000        9.51          15.90
                                                   100%            12000        8.46          13.59
                           Table 10: Comparison of English WER for Regular and Noisy development sets on increasing training dataset
                           size. The architecture is a 9-layer model with 2 layers of 2D-invariant convolution and 7 recurrent layers with
                           68Mparameters.
                           All models are trained for 20 epochs on either the full English dataset, described in Table 9, or
                           the full Mandarin dataset described in Section 5. We use stochastic gradient descent with Nesterov
                           momentum [61] along with a minibatch of 512 utterances. If the norm of the gradient exceeds
                           a threshold of 400, it is rescaled to 400 [47]. The model which performs the best on a held-out
                           development set during training is chosen for evaluation. The learning rate is chosen from [1 √ó
                           10‚àí4,6 √ó 10‚àí4] to yield fastest convergence and annealed by a constant factor of 1.2 after each
                           epoch. We use a momentum of 0.99 for all models.
                           Thelanguagemodelsusedarethosedescribed in Section 3.8. The decoding parameters from Equa-
                           tion 12 are tuned on a held-out development set. We use a beam size of 500 for the English decoder
                           and a beam size of 200 for the Mandarin decoder.
                           6.1  English
                           Thebest DS2 model has 11 layers with 3 layers of 2D convolution, 7 bidirectional recurrent layers,
                           a fully-connected output layer along with Batch Normalization. The Ô¨Årst layer outputs to bigrams
                           with a temporal stride of 3. By comparison the DS1 model has 5 layers with a single bidirectional
                           recurrent layer and it outputs to unigrams with a temporal stride of 2 in the Ô¨Årst layer. We report
                           results on several test sets for both the DS2 and DS1 model. We do not tune or adapt either model
                           to any of the speech conditions in the test sets. Language model decoding parameters are set once
                           onaheld-out development set.
                           To put the performance of our system in context, we benchmark most of our results against human
                           workers, since speech recognition is an audio perception and language understanding problem that
                           humansexcelat. WeobtainameasureofhumanlevelperformancebypayingworkersfromAmazon
                           Mechanical Turk to hand-transcribe all of our test sets. Two workers transcribe the same audio clip,
                           that is typically about 5 seconds long, and we use the better of the two transcriptions for the Ô¨Ånal
                           WERcalculation. Theyarefreetolisten to the audio clip as many times as they like. These workers
                           are mostly based in the United States, and on average spend about 27 seconds per transcription.
                           The hand-transcribed results are compared to the existing ground truth to produce a WER. While
                           the existing ground truth transcriptions do have some label error, this is rarely more than 1%. This
                           implies that disagreement between the ground truth transcripts and the human level transcripts is a
                           goodheuristic for human level performance.
                           6.1.1  ModelSize
                           Our English speech training set is substantially larger than the size of commonly used speech
                           datasets. Furthermore, the data is augmented with noise synthesis. To get the best generalization
                           error, we expect that the model size must increase to fully exploit the patterns in the data. In Sec-
                           tion 3.2 we explored the effect of model depth while Ô¨Åxing the number of parameters. In contrast,
                           here we show the effect of varying model size on the performance of the speech system. We only
                           vary the size of each layer, while keeping the depth and other architectural parameters constant. We
                           evaluate the models on the same Regular and Noisy development sets that we use in Section 3.5.
                           ThemodelsinTable11differfromthoseinTable3inthatweincreasethethestrideto3andoutput
                           to bigrams. Because we increase the model size to as many as 100 million parameters, we Ô¨Ånd that
                           an increase in stride is necessary for fast computation and memory constraints. However, in this
                           regime we note that the performance advantage of the GRU networks appears to diminish over the
                                                                          17
                                                 Modelsize    Modeltype      Regular Dev    Noisy Dev
                                                  18√ó106         GRU            10.59          21.38
                                                  38√ó106         GRU             9.06          17.07
                                                  70√ó106         GRU             8.54          15.98
                                                  70√ó106         RNN             8.44          15.09
                                                 100√ó106         GRU             7.78          14.17
                                                 100√ó106         RNN             7.73          13.06
                           Table11: ComparingtheeffectofmodelsizeontheWERoftheEnglishspeechsystemonboththeregularand
                           noisydevelopmentsets. Wevarythenumberofhiddenunitsinallbuttheconvolutionallayers. TheGRUmodel
                           has 3 layers of bidirectional GRUs with 1 layer of 2D-invariant convolution. The RNN model has 7 layers of
                           bidirectional simple recurrence with 3 layers of 2D-invariant convolution. Both models output bigrams with a
                           temporal stride of 3. All models contain approximately 35 million parameters and are trained with BatchNorm
                           and SortaGrad.
                                                              Test set      DS1      DS2
                                                              Baidu Test    24.01   13.59
                           Table 12: Comparison of DS1 and DS2 WER on an internal test set of 3,300 examples. The test set contains a
                           wide variety of speech including accents, low signal-to-noise speech, spontaneous and conversational speech.
                           simple RNN. In fact, for the 100 million parameter networks the simple RNN performs better than
                           the GRUnetworkandisfaster to train despite the 2 extra layers of convolution.
                           Table 11 shows that the performance of the system improves consistently up to 100 million parame-
                           ters. All further English DS2 results are reported with this same 100 million parameter RNN model
                           since it achieves the lowest generalization errors.
                           Table 12 shows that the 100 million parameter RNN model (DS2) gives a 43.4% relative improve-
                           ment over the 5-layer model with 1 recurrent layer (DS1) on an internal Baidu dataset of 3,300
                           utterances that contains a wide variety of speech including challenging accents, low signal-to-noise
                           ratios from far-Ô¨Åeld or background noise, spontaneous and conversational speech.
                           6.1.2   ReadSpeech
                           Readspeechwithhighsignal-to-noiseratio is arguably the easiest large vocabulary for a continuous
                           speech recognition task. We benchmark our system on two test sets from the Wall Street Journal
                           (WSJ) corpus of read news articles. These are available in the LDC catalog as LDC94S13B and
                           LDC93S6B.WealsotakeadvantageoftherecentlydevelopedLibriSpeechcorpusconstructedusing
                           audio books from the LibriVox project [46].
                           Table13showsthattheDS2systemoutperformshumansin3outofthe4testsetsandiscompetitive
                           on the fourth. Given this result, we suspect that there is little room for a generic speech system to
                           further improve on clean read speech without further domain adaptation.
                                                                      ReadSpeech
                                                   Test set                   DS1     DS2    Human
                                                   WSJeval‚Äô92                 4.94    3.60      5.03
                                                   WSJeval‚Äô93                 6.94    4.98      8.08
                                                   LibriSpeech test-clean     7.89    5.33      5.83
                                                   LibriSpeech test-other   21.74    13.25     12.69
                               Table 13: Comparison of WER for two speech systems and human level performance on read speech.
                                                                           18
                                                           Accented Speech
                                         Test set                      DS1    DS2    Human
                                         VoxForge American-Canadian   15.01   7.55     4.85
                                         VoxForge Commonwealth        28.46  13.56     8.15
                                         VoxForge European            31.20  17.55    12.76
                                         VoxForge Indian              45.35  22.44    22.15
                                 Table 14: Comparing WER of the DS1 system to the DS2 system on accented speech.
                                                            Noisy Speech
                                              Test set            DS1    DS2   Human
                                              CHiMEevalclean      6.30   3.34     3.46
                                              CHiMEevalreal      67.94  21.79    11.84
                                              CHiMEevalsim       80.27  45.05    31.33
                        Table 15: Comparison of DS1 and DS2 system on noisy speech. ‚ÄúCHiME eval clean‚Äù is a noise-free baseline.
                        The ‚ÄúCHiME eval real‚Äù dataset is collected in real noisy environments and the ‚ÄúCHiME eval sim‚Äù dataset has
                        similar noise synthetically added to clean speech. Note that we use only one of the six channels to test each
                        utterance.
                        6.1.3 Accented Speech
                        Our source for accented speech is the publicly available VoxForge (http://www.voxforge.org)
                        dataset, which has clean speech read from speakers with many different accents. We group these
                        accents into four categories. The American-Canadian and Indian groups are self-explanatory. The
                        Commonwealth accent denotes speakers with British, Irish, South African, Australian and New
                        Zealand accents. The European group contains speakers with accents from countries in Europe that
                        do not have English as a Ô¨Årst language. We construct a test set from the VoxForge data with 1024
                        examples from each accent group for a total of 4096 examples.
                        Performance on these test sets is to some extent a measure of the breadth and quality of our training
                        data. Table 14 shows that our performance improved on all the accents when we include more
                        accented training data and use an architecture that can effectively train on that data. However human
                        level performance is still notably better than that of DS2 for all but the Indian accent.
                        6.1.4 Noisy Speech
                        We test our performance on noisy speech using the publicly available test sets from the recently
                        completed third CHiME challenge [4]. This dataset has 1320 utterances from the WSJ test set
                        read in various noisy environments, including a bus, a cafe, a street and a pedestrian area. The
                        CHiMEsetalsoincludes 1320 utterances with simulated noise from the same environments as well
                        as the control set containing the same utterances delivered by the same speakers in a noise-free
                        environment. Differences between results on the control set and the noisy sets provide a measure of
                        the network‚Äôs ability to handle a variety of real and synthetic noise conditions. The CHiME audio
                        has 6 channels and using all of them can provide substantial performance improvements [69]. We
                        use a single channel for all our results, since multi-channel audio is not pervasive on most devices.
                        Table 15 shows that DS2 substantially improves upon DS1, however DS2 is worse than human level
                        performance on noisy data. The relative gap between DS2 and human level performance is larger
                        when the data comes from a real noisy environment instead of synthetically adding noise to clean
                        speech.
                        6.2 Mandarin
                        In Table 16 we compareseveralarchitectures trained on the Mandarin Chinese speech, on a develop-
                        mentsetof2000utterancesaswellasatestsetof1882examplesofnoisyspeech. Thisdevelopment
                        set was also used to tune the decoding parameters We see that the deepest model with 2D-invariant
                                                                 19
                        convolution and BatchNorm outperforms the shallow RNN by 48% relative, thus continuing the
                        trend that we saw with the English system‚Äîmultiple layers of bidirectional recurrence improves
                        performance substantially.
                                           Architecture                           Dev     Test
                                           5-layer, 1 RNN                         7.13   15.41
                                           5-layer, 3 RNN                         6.49   11.85
                                           5-layer, 3 RNN + BatchNorm             6.22    9.39
                                           9-layer, 7 RNN + BatchNorm + 2D conv   5.81    7.93
                        Table16: ComparisonoftheimprovementsinDeepSpeechwitharchitecturalimprovements. Thedevelopment
                        and test sets are Baidu internal corpora. All the models in the table have about 80 million parameters each
                        WeÔ¨ÅndthatourbestMandarinChinesespeechsystemtranscribes short voice-query like utterances
                        better than a typical Mandarin Chinese speaker. To benchmark against humans we ran a test with
                        100randomlyselectedutterancesandhadagroupof5humanslabelallofthemtogether. Thegroup
                        of humans had an error rate of 4.0% as compared to the speech systems performance of 3.7%. We
                        also comparedasinglehumantranscribertothespeechsystemon250randomlyselectedutterances.
                        In this case the speech system performs much better: 9.7% for the human compared to 5.7% for the
                        speech model.
                        7   Deployment
                        Real-world applications usually require a speech system to transcribe in real time or with relatively
                        low latency. The system used in Section 6.1 is not well-designed for this task, for several reasons.
                        First, since the RNN has several bidirectional layers, transcribing the Ô¨Årst part of an utterance re-
                        quires the entire utterance to be presented to the RNN. Second, since we use a wide beam when
                        decoding with a language model, beam search can be expensive, particularly in Mandarin where the
                        numberofpossible next characters is very large (around 6000). Third, as described in Section 3, we
                        normalize power across an entire utterance, which again requires the entire utterance to be available
                        in advance.
                        Wesolvethepowernormalizationproblembyusingsomestatisticsfromourtrainingsettoperform
                        an adaptive normalization of speech inputs during online transcription. We can solve the other
                        problems by modifying our network and decoding procedure to produce a model that performs
                        almost as well while having much lower latency. We focus on our Mandarin system since some
                        aspects of that system are more challenging to deploy (e.g. the large character set), but the same
                        techniques could also be applied in English.
                        In this section, latency refers to the computational latency of our speech system as measured from
                        the end of an utterance until the transcription is produced. This latency does not include data trans-
                        mission over the internet, and does not measure latency from the beginning of an utterance until the
                        Ô¨Årst transcription is produced. We focus on latency from end of utterance to transcription because it
                        is important to applications using speech recognition.
                        7.1  Batch Dispatch
                        In order to deploy our relatively large deep neural networks at low latency, we have paid special at-
                        tention to efÔ¨Åciency during deployment. Most internet applications process requests individually as
                        they arrive in the data center. This makes for a straightforward implementation where each request
                        can be managed by one thread. However, processing requests individually is inefÔ¨Åcient computa-
                        tionally, for two main reasons. Firstly, when processing requests individually, the processor must
                        loadall the weights of the network for each request. This lowers the arithmetic intensity of the work-
                        load, and tends to make the computation memory bandwidth bound, as it is difÔ¨Åcult to effectively
                        use on-chip caches when requests are presented individually. Secondly, the amount of parallelism
                        that can be exploited to classify one request is limited, making it difÔ¨Åcult to exploit SIMD or multi-
                        core parallelism. RNNs are especially challenging to deploy because evaluating RNNs sample by
                                                                   20
                                                        0.4                                         10streams
                                                                                                    20streams
                                                       y0.3                                         30streams
                                                       t
                                                       i
                                                       l
                                                       i
                                                       b
                                                       a
                                                       ob0.2
                                                       r
                                                       P
                                                        0.1
                                                        0.0
                                                           0   1    2    3    4    5   6    7    8    9   10   11
                                                                                 Batchsize
                                                Figure 5: Probability that a request is processed in a batch of given size
                              sample relies on sequential matrix vector multiplications, which are bandwidth bound and difÔ¨Åcult
                              to parallelize.
                              To overcome these issues, we built a batching scheduler called Batch Dispatch that assembles
                              streams of data from user requests into batches before performing forward propagation on these
                              batches. In this case, there is a tradeoff between increased batch size, and consequently improved
                              efÔ¨Åciency, and increased latency. The more we buffer user requests to assemble a large batch, the
                              longer users must wait for their results. This places constraints on the amount of batching we can
                              perform.
                              Weuse an eager batching scheme that processes each batch as soon as the previous batch is com-
                              pleted, regardless of how much work is ready by that point. This scheduling algorithm has proved
                              to be the best at reducing end-user latency, despite the fact that it is less efÔ¨Åcient computationally,
                              since it does not attempt to maximize batch size.
                              Figure 5 shows the probability that a request is processed in a batch of given size for our production
                              system running on a single NVIDIA Quadro K1200 GPU, with 10-30 concurrent user requests. As
                              expected, batching works best when the server is heavily loaded: as load increases, the distribution
                              shifts to favor processing requests in larger batches. However, even with a light load of only 10
                              concurrent user requests, our system performs more than half the work in batches with at least 2
                              samples.
                                                                   50%ile
                                                        100        98%ile
                                                       )
                                                       s
                                                       m
                                                       (
                                                       y
                                                       c
                                                       en50
                                                       t
                                                       a
                                                       l
                                                          0
                                                           0            10           20           30           40
                                                                       Numberofconcurrentstreams
                                                Figure 6: Median and 98 percentile latencies as a function of server load
                              WeseeinFigure6,that our system achieves a median latency of 44 ms, and a 98 percentile latency
                              of 70 ms when loaded with 10 concurrent streams. As the load increases on the server, the batching
                              scheduler shifts work to more efÔ¨Åcient batches, which keeps latency low. This shows that Batch
                              Dispatch makes it possible to deploy these large models at high throughput and low latency.
                                                                                  21
                                                  0.5
                                                                                            nervana
                                                  0.4                                       baidu
                                                s
                                                /
                                                P 0.3
                                                O
                                                L
                                                F
                                                a 0.2
                                                er
                                                T
                                                  0.1
                                                  0.0
                                                    1    2     3    4     5    6    7     8    9    10
                                                                        Batchsize
                           Figure 7: Comparison of kernels that compute Ax = b where A is a matrix with dimension 2560√ó2560, and
                           xis a matrix with dimension 2560 √ó Batch size, where Batch size ‚àà [1,10]. All matrices are in half-precision
                           format.
                           7.2  DeploymentOptimizedMatrixMultiplyKernels
                           Wehavefoundthatdeployingourmodelsusinghalf-precision(16-bit)Ô¨Çoating-pointarithmeticdoes
                           not measurably change recognition accuracy. Because deployment does not require any updates to
                           the network weights, it is far less sensitive to numerical precision than training. Using half-precision
                           arithmetic saves memory space and bandwidth, which is especially useful for deployment, since
                           RNNevaluation is dominated by the cost of caching and streaming the weight matrices.
                           As seen in Section 7.1, the batch size during deployment is much smaller than in training. We
                           found that standard BLAS libraries are inefÔ¨Åcient at this batch size. To overcome this, we wrote our
                           own half-precision matrix-matrix multiply kernel. For 10 simultaneous streams over 90 percent of
                           batches are for N ‚â§ 4, a regime where the matrix multiply will be bandwidth bound. We store the A
                           matrix transposed to maximize bandwidth by using the widest possible vector loads while avoiding
                           transposition after loading. Each warp computes four rows of output for all N output columns. Note
                           that for N ‚â§ 4 the B matrix Ô¨Åts entirely in the L1 cache. This scheme achieves 90 percent of peak
                           bandwidth for N ‚â§ 4 but starts to lose efÔ¨Åciency for larger N as the B matrix stops Ô¨Åtting into the
                           L1 cache. Nonetheless, it continues to provide improved performance over existing libraries up to
                           N=10.
                           Figure 7 shows that our deployment kernel sustains a higher computational throughput than those
                           from Nervana Systems [44] on the K1200 GPU, across the entire range of batch sizes that we use
                           in deployment. Both our kernels and the Nervana kernels are signiÔ¨Åcantly faster than NVIDIA
                           CUBLASversion7.0,moredetailsarefoundhere[20].
                           7.3  BeamSearch
                           Performingthebeamsearchinvolvesrepeatedlookupsinthen-gramlanguagemodel,mostofwhich
                           translate to uncached reads from memory. The direct implementation of beam search means that
                           each time-step dispatches one lookup per character for each beam. In Mandarin, this results in over
                           1Mlookups per 40ms stride of speech data, which is too slow for deployment. To deal with this
                           problem, we use a heuristic to further prune the beam search. Rather than considering all characters
                           as viable additions to the beam, we only consider the fewest number of characters whose cumulative
                           probability is at least p. In practice, we have found that p = 0.99 works well. Additionally, we limit
                           ourselves to no more than 40 characters. This speeds up the Mandarin language model lookup time
                           byafactor of 150x, and has a negligible effect on the CER (0.1-0.3% relative).
                           7.4  Results
                           Wecan deploy our system at low latency and high throughput without sacriÔ¨Åcing much accuracy.
                           Onaheld-outsetof2000utterances, our research system achieves 5.81 character error rate whereas
                           the deployed system achieves 6.10 character error rate. This is only a 5% relative degradation for
                                                                         22
             the deployed system. In order to accomplish this, we employ a neural network architecture with low
             deploymentlatency,reducetheprecisionofournetworkto16-bit,builtabatchingschedulertomore
             efÔ¨Åciently evaluate RNNs, and Ô¨Ånd a simple heuristic to reduce beam search cost. The model has
             Ô¨Åve forward-only recurrent layers with 2560 hidden units, one row convolution layer (Section 3.7)
             with œÑ = 19, and one fully-connected layer with 2560 hidden units. These techniques allow us to
             deploy Deep Speech at low cost to interactive applications.
             8 Conclusion
             End-to-end deep learning presents the exciting opportunity to improve speech recognition systems
             continually with increases in data and computation. Indeed, our results show that, compared to the
             previousincarnation, DeepSpeechhassigniÔ¨Åcantlyclosedthegapintranscriptionperformancewith
             human workers by leveraging more data and larger models. Further, since the approach is highly
             generic, we‚Äôve shown that it can quickly be applied to new languages. Creating high-performing
             recognizers for two very different languages, English and Mandarin, required essentially no expert
             knowledge of the languages. Finally, we have also shown that this approach can be efÔ¨Åciently
             deployed by batching user requests together on a GPU server, paving the way to deliver end-to-end
             DeepLearning technologies to users.
             To achieve these results, we have explored various network architectures, Ô¨Ånding several effective
             techniques: enhancements to numerical optimization through SortaGrad and Batch Normalization,
             evaluation of RNNs with larger strides with bigram outputs for English, searching through both
             bidirectional and unidirectional models. This exploration was powered by a well optimized, High
             Performance Computing inspired training system that allows us to train new, full-scale models on
             our large datasets in just a few days.
             Overall, we believe our results conÔ¨Årm and exemplify the value of end-to-end Deep Learning meth-
             ods for speech recognition in several settings. In those cases where our system is not already com-
             parable to humans, the difference has fallen rapidly, largely because of application-agnostic Deep
             Learning techniques. We believe these techniques will continue to scale, and thus conclude that the
             vision of a single speech system that outperforms humans in most scenarios is imminently achiev-
             able.
             Acknowledgments
             Weare grateful to Baidu‚Äôs speech technology group for help with data preparation and useful con-
             versations. We would like to thank Scott Gray, Amir Khosrowshahi and all of Nervana Systems for
             their excellent matrix multiply routines and useful discussions. We would also like to thank Natalia
             Gimelshein of NVIDIA for useful discussions and thoughts on implementing our fast deployment
             matrix multiply.
             References
             [1] O. Abdel-Hamid, A.-r. Mohamed, H. Jang, and G. Penn. Applying convolutional neural networks con-
               cepts to hybrid nn-hmm model for speech recognition. In ICASSP, 2012.
             [2] D.Bahdanau,K.Cho,andY.Bengio. Neuralmachinetranslationbyjointlylearningtoalignandtranslate.
               In ICLR, 2015.
             [3] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio. End-to-end attention-based large
               vocabulary speech recognition. abs/1508.04395, 2015. http://arxiv.org/abs/1508.04395.
             [4] J. Barker, E. Marxer, RicardVincent, andS.Watanabe. Thethird‚ÄôCHiME‚Äôspeechseparationandrecogni-
               tion challenge: Dataset, task and baselines. 2015. SubmittedtoIEEE2015AutomaticSpeechRecognition
               and Understanding Workshop (ASRU).
             [5] S. Baxter. Modern GPU. https://nvlabs.github.io/moderngpu/.
             [6] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In International Conference
               onMachineLearning, 2009.
             [7] H. Bourlard and N. Morgan. Connectionist Speech Recognition: A Hybrid Approach. Kluwer Academic
               Publishers, Norwell, MA, 1993.
                                  23
                               [8] W. Chan, N. Jaitly, Q. Le, and O. Vinyals.       Listen, attend, and spell.    abs/1508.01211, 2015.
                                   http://arxiv.org/abs/1508.01211.
                               [9] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro, and E. Shelhamer. cuDNN:
                                   EfÔ¨Åcient primitives for deep learning.
                              [10] T.Chilimbi,Y.Suzue,J.Apacible,andK.Kalyanaraman. Projectadam: BuildinganefÔ¨Åcientandscalable
                                   deeplearningtrainingsystem. In USENIXSymposiumonOperatingSystemsDesignandImplementation,
                                   2014.
                              [11] K.Cho,B.VanMerrienboer,C.Gulcehre,D.Bahdanau,F.Bougares,H.Schwenk,andY.Bengio. Learn-
                                   ingphraserepresentationsusingrnnencoder-decoderforstatisticalmachinetranslation. InEMNLP,2014.
                              [12] J. Chorowski, D. Bahdanau, K. Cho, and Y. Bengio. End-to-end continuous speech recognition using
                                   attention-based recurrent nn: First results. abs/1412.1602, 2015. http://arxiv.org/abs/1412.1602.
                              [13] C. Cieri, D. Miller, and K. Walker. The Fisher corpus: a resource for the next generations of speech-to-
                                   text. In LREC, volume 4, pages 69‚Äì71, 2004.
                              [14] A.Coates, B. Carpenter, C. Case, S. Satheesh, B. Suresh, T. Wang, D. J. Wu, and A. Y. Ng. Text detection
                                   andcharacterrecognitioninsceneimageswithunsupervisedfeaturelearning. InInternationalConference
                                   onDocumentAnalysis and Recognition, 2011.
                              [15] A. Coates, B. Huval, T. Wang, D. J. Wu, A. Y. Ng, and B. Catanzaro. Deep learning with COTS HPC. In
                                   International Conference on Machine Learning, 2013.
                              [16] G. Dahl, D. Yu, and L. Deng. Large vocabulary continuous speech recognition with context-dependent
                                   DBN-HMMs. InProc.ICASSP,2011.
                              [17] G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large
                                   vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 2011.
                              [18] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker,
                                   K.Yang,andA.Ng.Largescaledistributeddeepnetworks. InAdvancesinNeuralInformationProcessing
                                   Systems 25, 2012.
                              [19] D. Ellis and N. Morgan. Size matters: An empirical study of neural network training for large vocabulary
                                   continuous speech recognition. In ICASSP, volume 2, pages 1013‚Äì1016. IEEE, 1999.
                              [20] E. Elsen. Optimizing RNN performance. http://svail.github.io/rnn_perf. Accessed: 2015-11-24.
                              [21] M. J. F. Gales, A. Ragni, H. Aldamarki, and C. Gautier. Support vector machines for noise robust ASR.
                                   In ASRU, pages 205‚Äì2010, 2009.
                              [22] A.Graves,S.Fern√°ndez,F.Gomez,andJ.Schmidhuber. ConnectionisttemporalclassiÔ¨Åcation: Labelling
                                   unsegmented sequence data with recurrent neural networks. In ICML, pages 369‚Äì376. ACM, 2006.
                              [23] A.GravesandN.Jaitly. Towardsend-to-endspeechrecognitionwithrecurrentneuralnetworks. InICML,
                                   2014.
                              [24] A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In
                                   ICASSP, 2013.
                              [25] H. H. Sak, A. Senior, and F. Beaufays. Long short-term memory recurrent neural network architectures
                                   for large scale acoustic modeling. In Interspeech, 2014.
                              [26] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta,
                                   A. Coates, and A. Y. Ng. Deep speech: Scaling up end-to-end speech recognition. 1412.5567, 2014.
                                   http://arxiv.org/abs/1412.5567.
                              [27] A. Y. Hannun, A. L. Maas, D. Jurafsky, and A. Y. Ng. First-pass large vocabulary continuous speech
                                   recognition using bi-directional recurrent DNNs. abs/1408.2873, 2014. http://arxiv.org/abs/1408.2873.
                              [28] K. HeaÔ¨Åeld, I. Pouzyrevsky, J. H. Clark, and P. Koehn. Scalable modiÔ¨Åed Kneser-Ney language model
                                   estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,
                                   SoÔ¨Åa, Bulgaria, 8 2013.
                              [29] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
                                   T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE
                                   Signal Processing Magazine, 29(November):82‚Äì97, 2012.
                              [30] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735‚Äî1780,
                                   1997.
                              [31] N. Jaitly and G. Hinton. Vocal tract length perturbation (VTLP) improves speech recognition. In ICML
                                   Workshop on Deep Learning for Audio, Speech, and Language Processing, 2013.
                              [32] R.Jozefowicz,W.Zaremba,andI.Sutskever. Anempiricalexplorationofrecurrentnetworkarchitectures.
                                   In ICML, 2015.
                                                                                  24
                           [33] O. Kapralova, J. Alex, E. Weinstein, P. Moreno, and O. Siohan. A big data approach to acoustic model
                                training corpus selection. In Interspeech, 2014.
                           [34] K. C. Knowlton. A fast storage allocator. Commun. ACM, 8(10):623‚Äì624, Oct. 1965.
                           [35] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur. Audio augmentation for speech recognition. In Inter-
                                speech, 2015.
                           [36] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiÔ¨Åcation with deep convolutional neural net-
                                works. In Advances in Neural Information Processing Systems 25, pages 1106‚Äì1114, 2012.
                           [37] C. Laurent, G. Pereyra, P. Brakel, Y. Zhang, and Y. Bengio. Batch normalized recurrent neural networks.
                                abs/1510.01378, 2015. http://arxiv.org/abs/1510.01378.
                           [38] Q. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. Corrado, J. Dean, and A. Ng. Building high-level
                                features using large scale unsupervised learning. In International Conference on Machine Learning, 2012.
                           [39] Y. LeCun, F. J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance
                                to pose and lighting. In Computer Vision and Pattern Recognition, volume 2, pages 97‚Äì104, 2004.
                           [40] A. Maas, Z. Xie, D. Jurafsky, and A. Ng. Lexicon-free conversational speech recognition with neural
                                networks. In NAACL, 2015.
                           [41] Y. Miao, M. Gowayyed, and F. Metz. EESEN: End-to-end speech recognition using deep rnn models and
                                wfst-based decoding. In ASRU, 2015.
                           [42] A.Mohamed,G.Dahl,andG.Hinton. Acousticmodelingusingdeepbeliefnetworks. IEEETransactions
                                onAudio, Speech, and Language Processing, (99), 2011.
                           [43] A. S. N. Jaitly, P. Nguyen and V. Vanhoucke. Application of pretrained deep neural networks to large
                                vocabulary speech recognition. In Interspeech, 2012.
                           [44] Nervana Systems. Nervana GPU. https://github.com/NervanaSystems/nervanagpu. Accessed:
                                2015-11-06.
                           [45] J. Niu, L. Xie, L. Jia, and N. Hu. Context-dependent deep neural networks for commercial mandarin
                                speech recognition applications. In APSIPA, 2013.
                           [46] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: an asr corpus based on public domain
                                audio books. In ICASSP, 2015.
                           [47] R. Pascanu, T. Mikolov, and Y. Bengio.   On the difÔ¨Åculty of training recurrent neural networks.
                                abs/1211.5063, 2012. http://arxiv.org/abs/1211.5063.
                           [48] P. Patarasuk and X. Yuan. Bandwidth optimal all-reduce algorithms for clusters of workstations. J.
                                Parallel Distrib. Comput., 69(2):117‚Äì124, Feb. 2009.
                           [49] R. Raina, A. Madhavan, and A. Ng. Large-scale deep unsupervised learning using graphics processors.
                                In 26th International Conference on Machine Learning, 2009.
                           [50] S. Renals, N. Morgan, H. Bourlard, M. Cohen, and H. Franco. Connectionist probability estimators in
                                HMMspeechrecognition. IEEETransactions on Speech and Audio Processing, 2(1):161‚Äì174, 1994.
                           [51] T. Robinson, M. Hochberg, and S. Renals. The use of recurrent neural networks in continuous speech
                                recognition. pages 253‚Äì258, 1996.
                           [52] T. Sainath, O. Vinyals, A. Senior, and H. Sak. Convolutional, long short-term memory, fully connected
                                deep neural networks. In ICASSP, 2015.
                           [53] T. N. Sainath, A. rahman Mohamed, B. Kingsbury, and B. Ramabhadran. Deep convolutional neural
                                networks for LVCSR. In ICASSP, 2013.
                           [54] H. Sak, A. Senior, K. Rao, and F. Beaufays. Fast and accurate recurrent neural network acoustic models
                                for speech recognition. abs/1507.06947, 2015. http://arxiv.org/abs/1507.06947.
                           [55] H. Sak, O. Vinyals, G. Heigold, A. Senior, E. McDermott, R. Monga, and M. Mao. Sequence discrimina-
                                tive distributed training of long shortterm memory recurrent neural networks. In Interspeech, 2014.
                           [56] B. Sapp, A. Saxena, and A. Ng. A fast data collection and augmentation procedure for object recognition.
                                In AAAI Twenty-Third Conference on ArtiÔ¨Åcial Intelligence, 2008.
                           [57] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal
                                Processing, 45(11):2673‚Äì2681, 1997.
                           [58] F. Seide, G. Li, and D. Yu. Conversational speech transcription using context-dependent deep neural
                                networks. In Interspeech, pages 437‚Äì440, 2011.
                           [59] J. Shan, G. Wu, Z. Hu, X. Tang, M. Jansche, and P. Moreno. Search by voice in mandarin chinese. In
                                Interspeech, 2010.
                           [60] H.Soltau,G.Saon,andT.Sainath. Jointtrainingofconvolutionalandnon-convolutionalneuralnetworks.
                                In ICASSP, 2014.
                                                                           25
                       [61] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of momentum and initialization in
                           deep learning. In 30th International Conference on Machine Learning, 2013.
                       [62] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. 2014.
                           http://arxiv.org/abs/1409.3215.
                       [63] C. Szegedy and S. Ioffe. Batch normalization: Accelerating deep network training by reducing internal
                           covariate shift. abs/1502.03167, 2015. http://arxiv.org/abs/1502.03167.
                       [64] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-
                           novich. Going deeper with convolutions. 2014.
                       [65] R. Thakur and R. Rabenseifner. Optimization of collective communication operations in mpich. Interna-
                           tional Journal of High Performance Computing Applications, 19:49‚Äì66, 2005.
                       [66] K. Vesely, A. Ghoshal, L. Burget, and D. Povey. Sequence-discriminative training of deep neural net-
                           works. In Interspeech, 2013.
                       [67] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. Phoneme recognition using time-delay
                                        Àò Àô
                           neural networks,√¢AI acoustics speech and signal processing. IEEE Transactions on Acoustics, Speech
                           and Signal Processing, 37(3):328‚Äì339, 1989.
                       [68] R. Williams and J. Peng. An efÔ¨Åcient gradient-based algorithm for online training of recurrent network
                           trajectories. Neural computation, 2:490‚Äì501, 1990.
                       [69] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. F. C. Yu, W. J. Fabian, M. Espi, T. Higuchi,
                           S. Araki, and T. Nakatani. The ntt chime-3 system: Advances in speech enhancement and recognition for
                           mobile multi-microphone devices. In IEEE ASRU, 2015.
                       [70] W. Zaremba and I. Sutskever. Learning to execute. abs/1410.4615, 2014. http://arxiv.org/abs/1410.4615.
                       A Scalability improvements
                       In this section, we discuss some of our scalability improvements in more detail.
                       A.1  Nodeandclusterarchitecture
                       The software stack runs on a compute dense node built from 2 Intel CPUs and 8 NVIDIA Titan
                       XGPUs,withpeaksingle-precision computational throughput of 53 teraFLOP/second. Each node
                       also has 384 GB of CPU memory and an 8 TB storage volume built from two 4 TB hard disks in
                       RAID-0conÔ¨Åguration. We use the CPU memory to cache our input data so that we are not directly
                       exposed to the low bandwidth and high latency of spinning disks. We replicate our English and
                       Mandarindatasets on each node‚Äôs local hard disk. This allows us to use our network only for weight
                       updates and avoids having to rely on centralized Ô¨Åle servers.
                                                       CPU          CPU
                                         PLX           PLX          PLX           PLX
                                  GPU    GPU    GPU    GPU          GPU    GPU    GPU    GPU
                       Figure 8: Schematic of our training node where PLX indicates a PCI switch and the dotted box includes all
                       devices that are connected by the same PCI root complex.
                       Figure 8 shows a schematic diagram of one our nodes, where all devices connected by the same PCI
                       root complex are encapsulated in a dotted box. We have tried to maximize the number of GPUs
                       within the root complex for faster communication between GPUs using GPUDirect. This allows us
                       to use an efÔ¨Åcient communication mechanism to transfer gradient matrices between GPUs.
                                                              26
                                    All the nodes in our cluster are connected through Fourteen Data Rate (FDR) InÔ¨Åniband which is
                                    primarily used for gradient transfer during back-propagation.
                                    A.2     GPUImplementationofCTCLossFunction
                                    The CTC loss function that we use to train our models has two passes: forward and backward, and
                                    the gradient computation involves element-wise addition of two matrices, Œ± and Œ≤, generated during
                                    the forward and backward passes respectively. Finally, we sum the gradients using the character in
                                    the utterance label as the key, to generate one gradient per character. These gradients are then back-
                                    propagated through the network. The input to the CTC loss function are probabilities calculated by
                                    the softmax function which can be very small, so we compute in log probability space for better
                                    numerical stability.
                                    The forward pass of the CTC algorithm calculates the Œ± matrix, which has S rows and T columns,
                                    where S = 2(L + 1). The variable L is the number of characters in the label and T is the number
                                    of time-steps in the utterance. Our CPU-based implementation of the CTC algorithm assigns one
                                    thread to each utterance label in a minibatch, performing the CTC calculation for the utterances in
                                    parallel. Each thread calculates the relevant entries of the matrix sequentially. This is inefÔ¨Åcient for
                                    two reasons.
                                    Firstly, since the remainder of our network is computed on the GPU, the output of the softmax
                                    function has to be copied to the CPU for CTC calculation. The gradient matrices from the CTC
                                    function then has to be copied back to the GPU for backpropagation. For languages like Mandarin
                                    with large character sets, these matrices have hundreds of millions of entries, making this copy
                                    expensive. Furthermore, we need as much interconnect bandwidth as possible for synchronizing the
                                    gradient updates with data parallelism, so this copy incurs a substantial opportunity cost.
                                    Secondly, although entries in each column of the Œ± matrix can be computed in parallel, the number
                                    of entries to calculate in each column depends both on the column and the number of repeated
                                    characters in the utterance label. Due to this complexity, the CPU implementation does not use
                                    SIMDparallelism optimally, making the computation inefÔ¨Åcient.
                                    WewroteaGPU-basedimplementation of CTC in order to overcome these two problems. The key
                                    insight behind our implementation is that we can compute all elements in each column of the Œ±
                                    matrix, rather than just the valid entries. If we do so, Figure 9 shows that invalid elements either
                                    contain a Ô¨Ånite garbage value (G), or ‚àí‚àû (I), when we use a special summation function that adds
                                    probabilities in log space that discards inputs that are ‚àí‚àû. This summation is shown in Figure 9
                                    wherearrowsincident on a circle are inputs and the result is stored in the circle. However, when we
                                    computetheÔ¨Ånalgradientbyelement-wisesummingŒ±andŒ≤,allÔ¨Ånitegarbagevalueswillbeadded
                                    withacorresponding‚àí‚àûvaluefromtheothermatrix,whichresultsin‚àí‚àû,effectivelyignoringthe
                                    garbage value and computing the correct result. One important observation is that this element-wise
                                    sumofŒ±andŒ≤isasimplesumanddoesnotuseoursummationfunction.
                                    To compute the gradient, we take each column of the matrix generated from element-wise addition
                                    of Œ± and Œ≤ matrices, and do a key-value reduction using the character as key, using the ModernGPU
                                    library [5]. This means elements of the column corresponding to the same character will sum up
                                    their values. In the example shown in Figure 9, the blank character, B, is the only repeated character
                                    and at some columns, say for t = 1 of t = 2, both valid elements (gray) and ‚àí‚àû correspond to
                                    it. Since our summation function in log space effectively ignores the ‚àí‚àû elements, only the valid
                                    elements are combined in the reduction.
                                    In our GPU implementation, we map each utterance in the minibatch to a CUDA thread block.
                                    Since there are no dependencies between the elements of a column, all of them can be computed in
                                    parallel by the threads in a threadblock. There are dependencies between columns, since the column
                                    corresponding to time-step t+1 cannot be computed before the column corresponding to time-step
                                    t.  The reverse happens when computing the Œ≤ matrix, when column corresponding to time-step
                                    t cannot be computed before the column corresponding to time-step t + 1. Thus, in both cases,
                                    columns are processed sequentially by the thread block.
                                    MappingtheforwardandbackwardpassestocorrespondingCUDAkernelsisstraightforwardsince
                                    there are no data dependencies between elements of a column. The kernel that does the backward
                                    passalsocomputesthegradient. However,sincethegradientsmustbesummedupbasedonthelabel
                                                                                                    27
                                                              1      2     3      4              T-3   T-2    T-1     T
                                                         B                                               G     G      G
                                                         C                                                     G      G
                                                         B    I                                                G      G
                                                         A    I                                                       G   ‚Üµ
                                                         B    I      I                                                G
                                                         T    I      I
                                                         B    I      I      I
                                                         B                                               I      I     I
                                                         C                                                      I     I
                                                         B    G                                                 I     I
                                                         A    G                                                       I   
                                                         B    G      G                                                I
                                                         T    G      G
                                                         B    G      G      G
                                Figure 9: Forward and backward pass for GPU implementation of CTC. Gray circles contain valid values,
                                circle with I contain ‚àí‚àûandcirclewithGcontaingarbagevaluesthatareÔ¨Ånite. B standfortheblankcharacter
                                that the CTC algorithm adds to the input utterance label. Column labels on top show different time-steps going
                                from 1 to T.
                                values, with each character as key, we must deal with data dependencies due to repeated characters
                                in an utterance label. For languages with small character sets like English, this happens with high
                                probability. Even if there are no repeated characters, the CTC algorithm adds L+1 blank characters
                                to the utterance label. We solve this problem by performing a key-value sort, where the keys are the
                                characters in the utterance label, and the values are the indices of each character in the utterance.
                                After sorting, all occurrences of a given character are arranged in contiguous segments. We only
                                need to do the sort once for each utterance. The indices generated by the sort are then used to
                                sequentially sum up the gradients for each character. This sum is done once per column and in
                                parallel over all characters in the utterance. Amortizing the cost of key-value sort over T columns is
                                a key insight that makes the gradient calculation fast.
                                OurGPUimplementationusesfast shared memory and registers to achieve high performance when
                                performing this task. Both forward and backward kernels store the Œ± matrix in shared memory.
                                Since shared memory is a limited resource, it is not possible to store the entire Œ≤ matrix. However,
                                as we go backward in time, we only need to keep one column of the Œ≤ matrix as we compute the
                                gradient, adding element-wise the column of the Œ≤ matrix with the corresponding column of the
                                Œ± matrix. Due to on-chip memory space constraints, we read the output of the softmax function
                                directly from off-chip global memory.
                                DuetoinaccuraciesinÔ¨Çoating-pointarithmetic,especiallyintranscendentalfunctions, ourGPUand
                                CPUimplementation are not bit-wise identical. This is not an impediment in practice, since both
                                implementations train models equally well when coupled with the technique of sorting utterances
                                bylength mentioned in Section 3.3.
                                                                                         28
