                             A TrainingandExperimentalDetails
                             Wecontinuepre-training all our models from the TinyLlama3 [23] checkpoint on the RedPajama [24]
                             dataset. All models undergo the same training process, with differences only in their positional
                             encoding and attention patterns. Each model is trained on 16 A800 GPUs over two days. Detailed
                             training parameters are provided in Table 5.
                                                               Table 5: Training Details of Models.
                                                           Training Data               RedPajama[24]
                                                           Tokens                            50B
                                                           Parameters                        1.3B
                                                           Context Window Size               2048
                                                           Decaystyle                       cosine
                                                           Learning Rate                     2e-5
                                                           MinLearningRate                   1e-6
                                                           Optimizer                  AdamW(0.95,0.9)
                                                           WarmupSteps                       3000
                                                           Batch size                         48
                                                           Gradient clipping                  1.0
                             In addition, all the subsequent experiments are computed in 8 A800 GPUs.
                             B ThePositionalVectorsAftertheFirstLayer
                             Thoughprevious work [29, 28] have proven that implicit positional information can be encoded in
                             hidden states after one attention module, they only set the attention logits are equal regardless of
                             queries and keys, which does not hold in actual Transformers. In this section, we demonstrate the
                             preference in attention scores promotes the formation of different positional information in the initial
                             tokens.
                             B.1    Details
                                                                               s   ˆs
                             For the s-th sample in the corpus, we denote v       , h   as the value and output of the first attention
                                                                           s   1,i   1,i
                             head and the first layer at position i, and a    as the attention score between position i and position
                                                                           i,j
                                                 ˆ
                             j. We also denote p      as the positional vector of the attention output, and u as the mean vector of
                                                   1,i                                                        v
                             values. The positional vector can be represented as the formula:
                                                                         N              N
                                                            ˆ        1 Xˆs           1 X s
                                                            p    =          h    =          v    =u                                 (8)
                                                              1,1   N         1,1   N        1,1     v
                                                                        s=1             s=1
                                                                         N              N i
                                                            ˆ        1 Xˆs           1 XX s s
                                                            p =             h =                 a v                                 (9)
                                                              1,i   N         1,i   N            i,j 1,j
                                                                        s=1            s=1j=1
                             For the first token, the output is equal to the value of itself, so the positional vector is equal to the
                                                                      s
                             mean of values. In addition, When a         =1/i, positional information of all positions is equal as
                             follows:                                 i,j
                                                            N i                  i      N               i
                                                ˆ       1 XX1 s               1 X 1 X s             1 X
                                               p =                   v    =                 v    =        u =u .                  (10)
                                                 1,i    N           i  1,j    i     N        1,j    i       v      v
                                                           s=1j=1               j=1     s=1           j=1
                             However, due to the preferences in attention scores, values that can be decomposed into more vector
                             ˆ
                             p −u willbeassignedlargerweights,makingthepositional information of the following tokens
                               1,i    v
                             differ from the beginning token. In summary, the preferences of attention scores force the formation
                             of different positional vectors of the following tokens with the initial ones.
                                 3https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
                                                                                 14
