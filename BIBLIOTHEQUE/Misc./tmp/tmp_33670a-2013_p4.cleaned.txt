                             Algorithm 1 Minibatch version of the Auto-Encoding VB (AEVB) algorithm. Either of the two
                             SGVBestimatorsinsection 2.3 can be used. We use settings M = 100 and L = 1 in experiments.
                                Œ∏,œÜ‚ÜêInitialize parameters
                                repeat
                                    XM‚ÜêRandomminibatchofM datapoints(drawnfromfulldataset)
                                     ‚ÜêRandomsamplesfromnoisedistribution p()
                                                 M           M
                                                e
                                    g ‚Üê‚àá L (Œ∏,œÜ;X ,)(Gradientsofminibatchestimator(8))
                                            Œ∏,œÜ
                                    Œ∏,œÜ‚ÜêUpdateparametersusinggradientsg (e.g. SGD or Adagrad [DHS10])
                                until convergence of parameters (Œ∏,œÜ)
                                return Œ∏,œÜ
                             Often, the KL-divergence DKL(qœÜ(z|x(i))||pŒ∏(z)) of eq. (3) can be integrated analytically (see
                                                                                                                      (i)   
                             appendix B), such that only the expected reconstruction error E             (i)  logp (x |z) requires
                                                                                                   qœÜ(z|x  )       Œ∏
                             estimation by sampling. The KL-divergence term can then be interpreted as regularizing œÜ, encour-
                             aging the approximate posterior to be close to the prior p (z). This yields a second version of the
                                                                                           Œ∏
                                                  B         (i)               (i)
                                                e
                             SGVBestimator L (Œ∏,œÜ;x ) ' L(Œ∏,œÜ;x ), corresponding to eq. (3), which typically has less
                             variance than the generic estimator:
                                                                                                  L
                                             B         (i)                     (i)             1 X             (i)  (i,l)
                                            e
                                           L (Œ∏,œÜ;x )=‚àíD (q (z|x )||p (z))+                          (logp (x |z        ))
                                                                   KL œÜ              Œ∏         L           Œ∏
                                                                                                 l=1
                                                       (i,l)        (i,l) (i)            (l)
                                            where     z     =g (      , x   )   and       ‚àºp()                                   (7)
                                                                œÜ
                             Given multiple datapoints from a dataset X with N datapoints, we can construct an estimator of the
                             marginal likelihood lower bound of the full dataset, based on minibatches:
                                                                                              M
                                                                       M          M       N X               (i)
                                                                      e                           e
                                                     L(Œ∏,œÜ;X)'L (Œ∏,œÜ;X )= M                       L(Œ∏,œÜ;x )                         (8)
                                                                                              i=1
                                                       M          (i) M
                             where the minibatch X         = {x }         is a randomly drawn sample of M datapoints from the
                                                                      i=1
                             full dataset X with N datapoints. In our experiments we found that the number of samples L
                             per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M = 100.
                                                 e      M
                             Derivatives ‚àá      L(Œ∏;X ) can be taken, and the resulting gradients can be used in conjunction
                                             Œ∏,œÜ
                             with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a
                             basic approach to compute the stochastic gradients.
                             A connection with auto-encoders becomes clear when looking at the objective function given at
                             eq. (7). The Ô¨Årst term is (the KL divergence of the approximate posterior from the prior) acts as a
                             regularizer, while the second term is a an expected negative reconstruction error. The function gœÜ(.)
                                                                           (i)                               (l)
                             is chosen such that it maps a datapoint x        and a random noise vector         to a sample from the
                                                                            (i,l)        (l)  (i)          (i,l)           (i)
                             approximate posterior for that datapoint: z         =gœÜ( ,x )wherez               ‚àºqœÜ(z|x ). Subse-
                                                     (i,l)                                    (i) (i,l)
                             quently, the sample z       is then input to function logp (x      |z    ), which equals the probability
                                                                                          Œ∏
                             density (or mass) of datapoint x(i) under the generative model, given z(i,l). This term is a negative
                             reconstruction error in auto-encoder parlance.
                             2.4   Thereparameterization trick
                             In order to solve our problem we invoked an alternative method for generating samples from
                             qœÜ(z|x). The essential parameterization trick is quite simple. Let z be a continuous random vari-
                             able, and z ‚àº qœÜ(z|x) be some conditional distribution. It is then often possible to express the
                             random variable z as a deterministic variable z = gœÜ(,x), where  is an auxiliary variable with
                             independent marginal p(), and g (.) is some vector-valued function parameterized by œÜ.
                                                                 œÜ
                             This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t
                             qœÜ(z|x) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. œÜ. A proof
                             is as follows. Given the deterministic mapping z = gœÜ(,x) we know that qœÜ(z|x)Qidzi =
                             p()Q d . Therefore1, R q (z|x)f(z)dz = R p()f(z)d = R p()f(g (,x))d. It follows
                                     i   i                   œÜ                                                 œÜ
                                 1Note that for inÔ¨Ånitesimals we use the notational convention dz = Q dz
                                                                                                    i   i
                                                                                  4
