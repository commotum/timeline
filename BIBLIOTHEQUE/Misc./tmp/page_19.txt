References

1.

10.

11.

12.

13.

14.

15.

16.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
http: //www.deeplearningbook. org.

Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 770-778, 2015.

. Lena Strobl. Average-hard attention transformers are constant-depth uniform threshold

circuits, 2023.

Tom Bylander. Complexity results for planning. In Proceedings of the 12th International Joint
Conference on Artificial Intelligence - Volume 1, IJCAY91, page 274-279, San Francisco,
CA, USA, 1991. Morgan Kaufmann Publishers Inc. ISBN 1558601600.

. William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. In

Neural Information Processing Systems, 2023.

. David Chiang. Transformers in DLOGTIME-uniform TC°. Transactions on Machine

Learning Research, 2025.

. Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael

Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search
dynamics bootstrapping. In First Conference on Language Modeling, 2024.

. Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex

Vitvitskyi, Razvan Pascanu, and Petar Velivckovi’c. Transformers meet neural algorithmic
reasoners. ArXiv, abs/2406.09308, 2024.

. William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision

transformers. Transactions of the Association for Computational Linguistics, 11:531-545,
2023. doi: 10.1162/tacl_a_00562.

Jason Wei, Yi Tay, et al. Chain-of-thought prompting elicits reasoning in large language
models, 2022. arXiv preprint arXiv:2201.11903.

William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of
thought. In JCLR, 2024.

Xinyun Chen, Ryan A. Chi, Xuezhi Wang, and Denny Zhou. Premise order matters in
reasoning with large language models. ArXiv, abs/2402.08939, 2024.

Rongwu Xu, Zehan Qi, and Wei Xu. Preemptive answer “attacks” on chain-of-thought
reasoning. In Annual Meeting of the Association for Computational Linguistics, 2024.

Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius
Hobbhahn. Will we run out of data? limits of Ilm scaling based on human-generated data.
arXiv preprint arXiv:2211,04325, 2022.

Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang,
Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive
survey on latent chain-of-thought reasoning, 2025.

Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu.
Training large language models to reason in a continuous latent space. arXiv preprint
arXiv:2412.07423, 2024.

19
