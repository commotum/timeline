                                    Methods                     Present at     Modality    mAP(val)      NDS(val)     mAP(test)     NDS(test)     latency (ms)
                                    PointPainting [32]          CVPR’20          C+L          65.8          69.6           -             -              -
                                    MVP[44]                    NeurIPS’21        C+L          66.1          70.0         66.4          70.5             -
                                    TransFusion [1]             CVPR’22          C+L          67.5          71.3         68.9          71.6             -
                                    AutoAlignV2 [7]             ECCV’22          C+L          67.1          71.2         68.4          72.4             -
                                    UVTR[17]                   NeurIPS’22        C+L          65.4          70.2         67.1          71.1           264
                                    BEVFusion(PKU)[22]         NeurIPS’22        C+L          67.9          71.0         69.2          71.8             -
                                    DeepInteraction [42]       NeurIPS’22        C+L          69.9          72.6         70.8          73.4           594
                                    FUTR3D[5]                  CVPRW’23          C+L          64.5          68.3           -             -              -
                                    BEVFusion(MIT)[24]          ICRA’23          C+L          68.5          71.4         70.2          72.9           221
                                    CMT[39]                     ICCV’23          C+L          70.3          72.9         72.0          74.0           180
                                    UniTR[34]                   ICCV’23          C+L          70.5          73.3         70.9          74.5           196
                                    ECFusion[8]                 ICRA’24          C+L          70.7          73.4         71.5          73.9             -
                                    ISFusion [43]               CVPR’24          C+L          72.8          74.0         73.0          75.2           214
                                    GraphBEV[30]                ECCV’24          C+L          70.1          72.9         71.7          73.6           234
                                    SparseVoxFormer                  -           C+L          72.2          74.4         72.9          75.3           179
                   Table 5. Performance comparison in 3D object detection on nuScenes (val and test sets) [3]. Latency (ms) is measured by averaging
                   the inference times over first 1,000 samples of nuScenes val set using a single NVIDIA A100 GPU. Notion of modality: Camera (C),
                   LiDAR(L).
                              SemanticBEVFusion [13]    CMT         Ours-base       Ours             ranges (Table 6). It is noteworthy that, as shown in the
                                mAP        NDS      mAP NDS mAP NDS mAP NDS                          comparison of CMT [39] and Ours-base, our architecture
                     Whole      69.5       72.0      70.3   72.9   70.8   73.2   72.2   74.4         enhances a long-range performance from CMT more than
                      Near      79.9       78.1      80.5   78.9   80.3   79.1   82.9   81.0
                     Middle     66.1       70.2      66.5   71.0   67.3   71.1   68.3   72.0         near- and mid-range performances.
                       Far      37.0       49.1      37.3   49.3   38.2   49.9   39.4   50.9
                              Table 6. Range-wise performance comparison.
                                                                                                     Smaller image backbones                In this work, we focus on
                           Img. backbone           Model            mAP        NDS                   the LiDAR backbone and transformer decoder suitable
                                None           VoxelNeXt [6]        60.5       66.6                  for SparseVoxformer, while adhering to the settings of
                            (LiDARonly)          CMT[39]            62.1       68.6                  CMT [39] regarding the image backbone. However, as
                                             SparseVoxFormer        65.7       70.9                  showninapreviousexperimentoncomputationalcostanal-
                              ResNet-50          CMT[39]            67.9       70.8                  ysis in Table 4, the cost of an image backbone is significant
                                             SparseVoxFormer        69.3       72.7                  in the case of using VoVNet [15]. To demonstrate the practi-
                               VoVNet            CMT[39]            70.3       72.9                  cal utility of our approach in fields requiring lower compu-
                                             SparseVoxFormer        72.2       74.4
                   Table 7. Performance evaluation according to different image                      tational resources, we have prepared additional model vari-
                   backbones on the nuScenes val set [3].                                            ants without an image backbone (LiDAR only) or with a
                                                                                                     smaller image backbone (ResNet-50 [10]). Table 7 shows
                   comparableorevenbetterperformancethanrecentstate-of-                              that our variants of SparseVoxFormer outperform all the
                   the-art methods [8, 43] employing complicated multi-stage                         variants of CMT. It is noteworthy that our LiDAR-only
                   architectures including instance-level information process-                       modelstill outperform the previous state-of-the-art LiDAR-
                   ing for higher accuracies.                                                        based model [6].
                   Range-wise performance               Our explicit multi-modal fu-                 5. Conclusion
                   sion uses sparse image features instead of dense ones. As a                       This paper introduces a new architecture, referred to as
                   result, there might be questions regarding the performance                        SparseVoxFormer, for multi-modal 3D object detection.
                   of our models concerning long-range object detection. Gen-                        This is based on our key observation that using 3D voxel
                   erally, it is understood that detection of long-range object                      features with higher resolution can result in smaller com-
                   detection depends more on the camera modality, since such                         putational expenses compared to using BEV features, by
                   objects may contain only a few LiDAR points [13]. To                              leveraging the sparse nature of LiDAR data. With our foun-
                   address these questions, we measure the range-wise accu-                          dational designs for integrating sparse voxel features into
                   racy of 3D object detection. Following SemanticBEVFu-                             a transformer-based detector and explicit multi-modal fu-
                   sion [13], we divide the detection range of 0∼54m from                            sion with the sparse features, our SparseVoxFormer ex-
                   an ego vehicle into near (0∼18m), middle (18∼36m), and                            hibits a performance comparable to previous state-of-the-
                   far (36∼54m) ranges.                                                              art approaches, but with significantly reduced computa-
                       Ourapproachoutperformspreviousapproachesinallthe                              tional cost. We further facilitate more sparse but well-fused
