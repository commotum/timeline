               8th ICML Workshop on Automated Machine Learning (2021)
                                 PonderNet: Learning to Ponder
               Andrea Banino∗
               DeepMind
               London, UK
               abanino@deepmind.com
               Jan Balaguer*
               DeepMind
               London, UK
               jan@deepmind.com
               Charles Blundell
               DeepMind
               London, UK
               cblundell@deepmind.com
                                               Abstract
                   In standard neural networks the amount of computation used grows with the size of the in-
                   puts, but not with the complexity of the problem being learnt. To overcome this limitation
                   we introduce PonderNet, a new algorithm that learns to adapt the amount of computa-
                   tion based on the complexity of the problem at hand. PonderNet learns end-to-end the
                   number of computational steps to achieve an eﬀective compromise between training pre-
                   diction accuracy, computational cost and generalization. On a complex synthetic problem,
                   PonderNet dramatically improves performance over previous adaptive computation meth-
                   ods and additionally succeeds at extrapolation tests where traditional neural networks fail.
                   Also, our method matched the current state of the art results on a real world question and
                   answering dataset, but using less compute. Finally, PonderNet reached state of the art
                   results on a complex task designed to test the reasoning capabilities of neural networks.
               1. Introduction
               The time required to solve a problem is a function of more than just the size of the inputs.
               Commonlyproblems also have an inherent complexity that is independent of the input size:
               it is faster to add two numbers than to divide them. Most machine learning algorithms do
               not adjust their computational budget based on the complexity of the task they are learning
               to solve, or arguably, such adaptation is done manually by the machine learning practitioner.
               This adaptation is known as pondering. In prior work, Adaptive Computation Time (ACT;
               Graves, 2016) automatically learns to scale the required computation time via a scalar
               halting probability. This halting probability modulates the number of computational steps,
               called the “ponder time”, needed for each input. Unfortunately ACT is notably unstable and
                ∗. contributed equally
                c
               2021 Andrea Banino, Jan Balaguer, Charles Blundell.
