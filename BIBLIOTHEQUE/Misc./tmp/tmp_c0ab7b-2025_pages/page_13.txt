                            A Appendix                                                                                  A.2       Results for GPT-2 Small and Tiny
                            A.1        Bert Models Training                                                             Tables 4 and 5 present the extrapolation results for
                                                                                                                        GPT-2 Small and GPT-2 Tiny models evaluated
                            Figure 6 shows the masked language modeling loss                                            on the FineWeb-Edu-10B dataset. As shown in
                            during BERT pre-training with different positional                                          Table 4, CABLE consistently outperforms other
                            encodings. Traditional methods like learnable and                                           positional encoding methods across all sequence
                            sinusoidal fail to match the loss achieved by RPEs,                                         lengths, particularly at extrapolated lengths beyond
                            highlighting the effectiveness of RPEs. CABLE                                               1024 tokens. Both GPT-2 Small and GPT-2 Tiny
                            also converges faster than other methods.                                                   models trained with CABLE achieve significantly
                                Figure 7 shows the contrastive loss during fine-                                        lower perplexity than those using ALiBi, RoPE,
                            tuning BERT models with different positional en-                                            T5-bias, and other baselines. Notably, standard
                            coding methods on the MS-MARCO training set.                                                methods such as sinusoidal or learnable encodings
                            Onceagain, learnable and sinusoidal methods lag                                             degradesharplyatlongerlengths, whereas CABLE
                            behind, while CABLE achieves the lowest loss                                                maintains stable and superior performance. These
                            amongall methods.                                                                           results further confirm CABLEâ€™s effectiveness in
                                                                                                                        enhancing length extrapolation, even in smaller
                                            Comparison of BERT MLM Pre-training Loss on FineWeb-Edu-10B                 modelregimes.
                                                                                                       ALiBi
                               10                                                                      CABLE
                                                                                                       Learnable
                                9                                                                      RoPE             A.3       Visualizations
                                                                                                       Sinusoidal
                                8                                                                                       Figures 8 to 14 illustrate the additive biases incor-
                              alue7                                                                                     porated into the attention scores (B in Equation 1)
                              oss V6                                                                                    for the additive RPEs Cable, DAPEv2+Cable, Ker-
                              L                                                                                         ple, DAPEv2+Kerple, ALiBi, Fire, and T5-bias.
                                5
                                4                                                                                       Thevisualizations are derived from the final layer
                                3                                                                                       of GPT-2 Small model using a randomly selected
                                                                                                                        input example. Since RoPE does not introduce
                                     0       2000       4000      6000       8000      10000     12000      14000
                                                                   Training Steps                                       an explicit additive bias but instead modifies the
                            Figure 6: Masked language modeling (MLM) loss dur-                                          query and key representations, the corresponding
                            ing BERTpre-training on FineWeb-Edu-10B with dif-                                           attention scores after this transformation are shown
                            ferent positional encoding methods. CABLE achieves                                          separately in Figure 15.
                            the fastest convergence and lowest final loss, demon-
                            strating superior training efficiency over traditional and
                            other RPE methods.
                               1.0          Comparison of BERT Contrastive Fine-tuning Loss on MS-MARCO
                                                                                                       ALiBi
                                                                                                       CABLE
                                                                                                       Learnable
                               0.8                                                                     RoPE
                                                                                                       Sinusoidal
                              alue0.6
                              oss V
                              L0.4
                               0.2
                               0.0  0          100         200         300         400         500         600
                                                                   Training Steps
                            Figure 7: Contrastive fine-tuning loss on the MS-
                            MARCOdatasetforBERTmodelsusingdifferentposi-
                            tional encoding methods. CABLE achieves the lowest
                            loss, while learnable and sinusoidal encodings underper-
                            form.
                                                                                                                30375
