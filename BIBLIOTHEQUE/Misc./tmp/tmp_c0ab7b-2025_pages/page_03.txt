                  most popular methods in RPEs, is rotary positional       pairwise distances into the pre-softmax attention
                  embedding (RoPE) (Su et al., 2024). RoPE rotates         scores. However, the function rapidly approaches
                  a query and key pair vectors with an angle pro-          the zero point (Chi et al., 2022a), hence may not
                  portional to their relative positions before the dot     be a realistic assumption.
                  product attention, which results in attention being a      T5-bias (Raffel et al., 2020). The kernel func-
                  function of the relative distance between the tokens,    tion is defined as b(i,j) = r           , where K is
                  capturing the relative positional information. One                                    min{i−j,K}
                                                                                                      K
                                                                           a hyperparameter and {r }      are learnable scalars.
                  of the primary arguments for the effectiveness of                                 i i=0
                  RoPE—and a key reason it is widely adopted in            For positions beyond the training sequence length,
                  modernLLMs—wasputforthbySuetal.(2024),                   the model reuses the maximum learned relative
                  who claimed that RoPE enables attention scores           bias. While this approach allows some extrapola-
                  to decay as the relative distance between tokens         tion, it suffers from latency issues on modern ac-
                  increases. However, Barbero et al. (2024) later pro-     celerators due to inefficient vectorized operations
                  vided a mathematical analysis showing that this         with long sequences.
                  claim is flawed: attention weights under RoPE do           Kerple (Chi et al., 2022a). The kernel function
                  not necessarily decay proportionally with relative       is defined as b(i,j) = −r1log(1 + r2|i − j|) in
                  query-key distances. This insight offers a possi-        its logarithmic form and −r |i − j|r2 in its power
                                                                                                        1
                  ble explanation for RoPE’s limitations in length         form, where r ,r > 0 are learnable scalars. This
                                                                                         1   2
                  extrapolation. In RoPE-based methods, Yarn (Peng         approach employs a shift-invariant kernel for the
                  et al., 2023) modifies RoPE by integrating atten-        bias terms.
                  tion scaling and Neural Tangent Kernel (NTK) in-           Fire (Li et al., 2023). The kernel function is
                  terpolation (Jacot et al., 2018), and (Chen et al.,      defined as b(i,j) = f      ψ(i−j)    , where f is
                  2023a) extends the context window size of RoPE                                 θ   ψ(max{L,i})            θ
                  by interpolating positions in the range seen during      an MLP with θ parameters, ψ: x 7→ log(cx + 1)
                  training. However, recent studies have shown that        is a monotonically increasing function and L >
                  RoPE-based language models perform poorly on             0 is a learnable scaler. This formulation allows
                  sequences longer than those seen during training         Fire to assign more attention to distant query-key
                  (Press et al., 2021; Kazemnejad et al., 2024). To        pairs—contrary to methods like ALiBi, and Kerple,
                  address this limitation, several positional encod-      which tend to focus on nearby tokens.
                  ing methods with better length extrapolation ca-           Data-dependent Positional Encoding: Re-
                  pabilities have been proposed (Chen et al., 2023a).      cently, several data-dependent RPEs have been
                  Amongthese,additiveapproacheshavegainedpop-              proposed. CoPE (Golovneva et al., 2024) applies
                  ularity—where a bias matrix is directly added to         fixed ALiBi-style biases with a learned binary gate.
                  the pre-softmax attention logits. This design is        While effective in its domain (mathematics), it
                  typically intended to enforce a decay in attention       lacks the flexibility of continuous, learnable span
                  weights proportional to the relative distance be-        control. DAPE (Zheng et al., 2024a,b) functions
                  tween query-key pairs, as shown in the following         as an augmentation to existing additive positional
                  formula:                                                 bias methods and is not a standalone mechanism.
                                                        T                  Furthermore, its architecture imposes a relatively
                         A      (X)=XW (XW ) +B                     (1)
                           RPE               Q       K                     high computational cost, as it uses feedforward or
                  Thebiasmatrixforaninputsequencewithttokens               convolutional layers over the full attention matrix.
                            t×t                                            FoX (Lin et al., 2025), a near-concurrent work,
                  is B ∈ R     , generated by a positional encoding        introduces forgetting mechanisms while minimiz-
                                 2
                  function b : N → R, where the (i,j)-th entry of          ing reliance on positional encodings by learning
                  Bisgivenbyb(i,j). Naturally, different formula-          token-wise biases.
                  tions of the function b lead to different variants of
                  Relative Positional Encodings (RPEs). Below are            Ourmethoddiffers from these baselines, as CA-
                  a few examples of additive RPEs that are capable         BLEavoidsbinarization and instead uses dynamic
                  of extrapolating:                                        biases (unlike CoPE), functions as a standalone
                    ALiBi(Pressetal.,2021). Thekernelfunctionis            RPEandappliesalightweight MLP over the input
                  defined as b(i,j) = −r|i−j|, where r > 0 is a hy-        (unlike DAPE),andadditionallyconditionsthebias
                  perparameter. ALiBiincorporatesbiasbasedonthe            slope on the query token (unlike FoX).
                                                                      30365
