                                                     1e6                    Training Speed (ms)                                                                            Memory (GB)                                                                        Inference Speed (ms)
                                                1.4                                                                                                                                                                              500
                                                1.2                                                                                      40
                                                1.0                                                                                                                                                                              400
                                              )                                                                                         )30                                                                                     )
                                                0.8                                                                                                                                                                              300
                                              TPS (0.6                                                                                  GB (20                                                                                  TPS (200
                                                0.4                                                                                      10
                                                0.2                                                                                                                                                                              100
                                                0.0                                                                                        0                                                                                        0
                                                                                      e                                                                                        e                                                                                        e
                                                                                    ir                                                                                        ir                                                                                       ir
                                                                  oPE              F                ALiBi                                                  oPE               F               ALiBi                                                  oPE               F               ALiBi
                                                                R        erple                              Cable    nable                                R       erple                               Cable    nable                               R        erple                              Cable    nable
                                                                        K                T5-bias                  ear                                            K                T5-bias                   ear                                            K               T5-bias                   ear
                                                   Sinusoidal                                                     L                         Sinusoidal                                                     L                         Sinusoidal                                                     L
                                          Figure 3: Comparison of batched training time, memory usage in training, and unbatched inference time in GPT-2
                                          MediumamongtheSinusoidal,RoPE,Kerple,Fire, T5-bias, ALiBi, CABLE, and Learnable positional encoding
                                          methods.
                                          in CABLEcapture contextual information more ef-                                                                                            5.3            Data-dependent RPEs: CABLEvs. DAPE
                                          fectively than ALiBi, leading to superior length ex-                                                                                       As discussed in the related work section, a few
                                          trapolation, especially on very long sequences. No-                                                                                        context-aware RPEs have already been proposed,
                                          tably, even CABLE                                             outperforms ALiBi, high-
                                                                                              NW                                                                                     with DAPE(Zhengetal., 2024a) and its successor
                                          lighting the strength of our context-aware design.                                                                                         DAPEv2(Zhengetal.,2024b)beingthemostno-
                                          Additionally, the improved performance with the                                                                                            table. In this section, we compare CABLE against
                                          full CABLEmodelunderscores the benefit of the                                                                                              DAPEv2, since the latter represents a direct im-
                                          learned weight function g (X).
                                                                                                            θ                                                                        provement over the former.
                                          5.2            RuntimeandMemoryOverhead                                                                                                           DAPEandDAPEv2aredata-dependent RPEs
                                                                                                                                                                                     that act as augmentations to existing additive posi-
                                          Wealsoevaluate our method against existing meth-                                                                                           tional bias methods rather than standalone mecha-
                                          odsintermsoftraining/inferenceruntimeandmem-                                                                                               nisms. However, they come with considerable com-
                                          ory usage. As shown in Figure 3, our method                                                                                                putational cost: DAPE relies on MLP layers, while
                                          achieves the same training Token Per Second (TPS)                                                                                          DAPEv2appliesconvolutional layers over the full
                                          as ALiBi and Kerple. However, both our method                                                                                              attention matrix of shape [B,nh,T,T], leading to
                                          andALiBihaveslightlyhigher overhead compared                                                                                               higher training and backward-pass overhead. In
                                          to RoPE,Sinusoidal, andLearnablemethods,while                                                                                              contrast, CABLE applies a lightweight MLP over
                                          T5-bias exhibits significant overhead. During infer-                                                                                       the input representations of shape [B,T,D], result-
                                          ence, our method achieves faster performance than                                                                                          ing in significantly lower computational overhead.
                                          other RPEs. It is the third fastest overall—trailing                                                                                              Here we augment DAPEv2 with CABLE and
                                          only Sinusoidal and Learnable encodings—while                                                                                              Kerple for the comparison. Table 2 presents extrap-
                                          outperforming ALiBi in speed. Moreover, CABLE                                                                                              olation results on the FineWeb-Edu-10B dataset,
                                          uses almost the same GPU memory as other meth-                                                                                             with models trained at sequence length 512 using
                                          ods during training and adds negligible overhead                                                                                           GPT-2Small. TheresultsshowthatwhileDAPEv2
                                          compared to methods like ALiBi. It should be                                                                                               improvestheperformanceofbothCABLEandKer-
                                          noted that, due to the extrapolation ability of CA-                                                                                        ple, the overall ranking remains unchanged: CA-
                                          BLE,it can be trained on shorter sequence lengths                                                                                          BLEconsistently outperforms Kerple, both with
                                          and effectively tested on much longer sequences.                                                                                           and without DAPEv2 augmentation.
                                          This approach addresses training overhead by re-                                                                                                  Kerple suffers from severe degradation at longer
                                          ducing the sequence length during training, mak-                                                                                           sequence lengths (from 4096 onward), whereas
                                          ing it feasible on commonly available GPUs. The                                                                                            CABLEmaintainsrobustperformanceevenwith-
                                          overhead of our method is primarily related to the                                                                                         out augmentation. Applying DAPEv2 to kerple
                                          cumulative sum operation in our computations. Im-                                                                                          can alleviate this weakness, but DAPEv2 intro-
                                          portantly, for inference, we cache the cumulative                                                                                          duces substantial computational overhead. For in-
                                          sums,sothereisnoneedtore-calculatethemforall                                                                                               stance, GPT-2 Small with CABLE trains at 1.9M
                                          tokens each time. This optimization helps CABLE                                                                                            tokens/sec, while DAPEv2+CABLEdropsto 0.6M
                                          achieving superior inference time to other methods,                                                                                        tokens/sec (about 3× slower). Also at inference,
                                          such as ALiBi.                                                                                                                             DAPEv2+CABLE is 1.7× slower than CABLE
                                                                                                                                                                         30369
