                  pression (Muetal., 2024; Tan et al., 2024), data for-         alization to sequences longer than those seen
                  matting techniques (Shen et al., 2023; Zhou et al.,           during training.
                  2023), and Relative Positional Encodings (RPE)
                  (Press et al., 2021; Raffel et al., 2020; Su et al.,     2 RelatedWork
                  2024). Amongthese, RPEshaveemergedasoneof                In this section, we review key approaches to po-
                  the most prominent and widely adopted solutions          sitional encoding, including absolute and relative
                  for improving length extrapolation in transformer        methods, as well as recent work exploring trans-
                  models.                                                  former models without any explicit positional en-
                    Recently, a numberofRPEvariantshavebeenin-             coding.
                  troduced. Rotary Positional Encoding (RoPE) (Su            NoPositional Encoding (NoPE). Surprisingly,
                  et al., 2024) encodes token positions by rotating        Havivetal.(2022)showedthatdecoder-onlyTrans-
                  query and key vectors, while ALiBi (Press et al.,        formers with causal attention can implicitly learn
                  2021) introduces a linear bias to attention scores.      positional information without explicit encodings.
                  Manysubsequentworkshavebuiltuponthesefoun-               Kazemnejad et al. (2024) further supported this
                  dations, either enhancing RoPE (Xu et al., 2024;         NoPEapproach, especially in out-of-distribution
                  Peng et al., 2023; Chen et al., 2023a) or refining       (OOD)settings, suggesting that the causal mecha-
                  ALiBi-style additive biases (Chi et al., 2022b,a; Li     nism alone can suffice (Wang et al., 2024). How-
                  et al., 2023; Gao, 2024; Zhu et al., 2025).              ever, Li et al. (2023) found that NoPE generally
                    In this work, based on ALiBi, we propose an            underperforms compared to models with explicit
                  additive RPE method which dynamically learns             positional encodings. In a concurrent effort with
                  biases for tokens on each head of attention mecha-       our work, FoX (Lin et al., 2025) suggested a sim-
                  nismintransformers. In contrast to ALiBi that uses       ilar idea by not using positional encoding and in-
                  constant linear biases, our method, Context-aware        stead proposed a forgetting gate. While NoPE is
                  biases for length extrapolation (CABLE), learns          compatible with arbitrary sequence lengths, its per-
                  slopes for each head, enabling the model to create       formance often degrades when extrapolating far
                  dynamic biases for each token. CABLE adds negli-         beyond training lengths.
                  gible time and memory burden to the conventional           AbsolutePositionalEncoding(APE).APEwas
                  transformer (Vaswani, 2017), while achieving bet-
                  ter performance. As shown in Figure 1, while the         one of the earliest approaches introduced to incor-
                  performance of existing positional encodings de-         porate positional information into Transformers.
                  grades with increasing sequence length, CABLE           Vaswani (2017) proposed both fixed (sinusoidal)
                  achieves even lower perplexity as sequence length        and learned encodings, while Gehring et al. (2017)
                  increases. Our method is simple and easy to im-          applied learnable absolute embeddings in convolu-
                  plement, and can be integrated into any existing         tional architectures. Later, Devlin et al. (2019)
                  transformer model easily.                                adopted learned absolute embeddings in BERT,
                    Contributions of this paper are as follows:            adding them to token embeddings. Chen et al.
                                                                           (2021) further refined APE with a decoupled atten-
                     • Wepropose CABLE,anadditive relative posi-           tion mechanism to better separate content and posi-
                       tional encoding method that, in contrast to ex-     tional signals. In general, APE assigns a fixed or
                                                                                                 d
                       isting methods, uses context-aware positional       learned vector ei ∈ R to each position i, forming
                                                                           amatrixE = [e ,e ,...,e ]T thatisaddedelement-
                       information by learning token-specific biases                       1   2      t
                       in each attention head. CABLE is also sim-         wise to token embeddings (Vaswani, 2017; Devlin
                       ple, easy to implement, and have relatively         et al., 2019; Kiyono et al., 2021; Likhomanenko
                       fast inference time compared to the previous        et al., 2021). A key limitation of APE methods
                       methods.                                            is their poor generalization to sequence lengths
                                                                           beyond those seen during training, making them
                     • Weevaluate our proposed method on several           unsuitable for length extrapolation.
                       benchmark datasets, using GPT-2 variants for          Relative Positional Encoding (RPE). RPE is
                       next-token prediction and BERT models for           an increasingly popular way to encode positional
                       long-context retrieval. Our approach consis-        information for Transformers. (Shaw et al., 2018)
                       tently outperforms existing positional encod-      wasthefirst to propose learning relative positional
                       ing methods and demonstrates superior gener-        information within a clipping distance. Among the
                                                                      30364
