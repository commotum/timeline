                                                                   GPT-2Small
                          Sequence Length CABLE CABLENW ALiBi          Fire  T5-bias Kerple RoPE Learnable Sinusoidal
                          512               21.19     21.42    21.55  21.84   22.17   21.46  21.43    22.16      22.38
                          1024              20.63     20.89    20.99  21.26   21.57   20.86  20.87    21.56      21.83
                          2048              20.02     20.34    20.67  22.48   29.37   20.38  58.59      -       207.53
                          4096              19.24     19.67    21.23  53.25   131.36  21.11 225.78      -       956.41
                          8192              19.31     19.81    22.42 155.32 405.94    26.59 554.12      -      2376.51
                          15360             19.28     19.82    22.89 333.91 757.36    34.91 957.87      -      3589.97
                                                                    GPT-2Tiny
                          Sequence Length CABLE CABLENW ALiBi          Fire  T5-bias Kerple RoPE Learnable Sinusoidal
                          512               29.37     30.12    29.88  30.23   30.78   29.60  29.44    30.73      30.67
                          1024              28.73     29.57    29.25  29.56   30.08   28.95  28.81    30.11      30.03
                          2048              27.96     28.88    28.82  29.60   33.81   28.32  76.29      —       275.28
                          4096              26.90     27.85    28.28  37.86   86.33   28.31 239.95      —      1166.46
                          8192              26.97     27.92    26.80  70.72   222.60  32.00 452.67      —      2561.54
                          15360             26.80     27.75    28.52 124.29 448.08    37.67 652.52      —      3679.78
                  Table 4: Perplexity comparison on the FineWeb-Edu-10B evaluation sets. The upper table shows GPT-2 Small
                  variants, and the lower table shows GPT-Tiny variants—both trained on the FineWeb-Edu-10B training set for 19k
                  steps with a sequence length of 1024.
                                                                  GPT-2Medium
                          Sequence Length CABLE CABLENW ALiBi          Fire  T5-bias Kerple RoPE Learnable Sinusoidal
                          512               20.33     20.80    20.80  22.18   23.33   20.96  20.81    22.52      24.16
                          1024              19.12     19.63    19.62  20.89   22.00   19.73  19.60    21.23      22.77
                          2048              18.36     18.91    19.04  21.86   35.46   19.01  20.78      —       143.58
                          4096              17.87     18.47    18.91  46.60   124.19  18.63  31.22      —       467.51
                          8192              17.58     18.23    18.89 106.27 363.59    18.61  51.54      —      1006.37
                          15360             17.36     17.95    18.60 195.93 726.18    18.79  91.53      —      1516.97
                                                                   GPT-2Small
                          Sequence Length CABLE CABLENW ALiBi          Fire  T5-bias Kerple RoPE Learnable Sinusoidal
                          512               20.93     21.34    21.46  22.03   22.80   21.50  21.38    22.51      23.09
                          1024              19.71     20.15    20.22  20.74   21.49   20.22  20.13    21.21      21.73
                          2048              18.95     19.42    19.62  21.28   33.85   19.45  30.14      —       163.49
                          4096              18.47     18.98    19.39  36.02   123.05  19.04  63.75      —       580.86
                          8192              18.20     18.75    19.29  81.77   347.22  18.91 117.81      —      1121.08
                          15360             17.92     18.48    19.06 163.91 659.72    19.00 202.23      —      1652.53
                  Table 5: Perplexity comparison on the WikiText-103 evaluation set. The upper table shows GPT-2 Medium variants
                  trained for 3k steps, and the lower table shows GPT-2 Small variants trained for 5k steps. All models use a sequence
                  length of 1024.
                                                                      30376
