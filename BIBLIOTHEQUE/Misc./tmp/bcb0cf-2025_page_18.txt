                                  Published as a conference paper at ICLR 2025
                                  Toimprovethescalability, we adapt the asynchronous methods of Barham et al. (2022). The gradients for
                                  each layer l in an asynchronous setting are calculated as:
                                                                                              ∇l,asyncL = ∇lL+∆async                                                                      (17)
                                  where∆             is the asynchronouscorrectionterm. EchoingthesentimentsofMetzetal.(2022),weincorporate
                                               async
                                  auxiliary metrics M alongside gradients, optimized as:
                                                                                                     O=∇L+αM                                                                              (18)
                                  where α is a tunable parameter.
                                  Algorithm 2 Backward Propagation in MIND model
                                    1: procedure BACKWARD(ctx, grad_output)
                                    2:        z ,layer ← ctx.saved_tensors
                                                ⋆
                                    3:        max_iter ← ctx.max_iter
                                    4:        tol ← 1×10−5
                                    5:        dz ←grad_output.detach().clone()
                                    6:        I ←Identity matrix of d                                                                                    ▷ Initialize identity matrix
                                                                                  z
                                    7:        ∇introspection ← 0                                                        ▷ Initialize gradient for the introspection network
                                    8:        dphantom ← PhantomGradient(d ,z )                                                                      ▷ Initialize phantom gradient
                                                z                                           z    ⋆
                                    9:        for k = 1,...,max_iter do
                                  10:              f ←layer(z )
                                                     z                 ⋆
                                  11:              J ← ∂fz                                                                                             ▷ Compute Jacobian matrix
                                                            ∂z⋆
                                  12:              ∆J ←I−J                                                                                           ▷ Implicit differentiation step
                                                                  phantom
                                  13:              d       ←d               ×∆J                                              ▷Updatethegradient using phantom gradient
                                                     z            z
                                                       new
                                                           ∥dz     −dz∥
                                  14:              δ ←          new
                                                              ∥dz     ∥
                                                                  new
                                  15:              if δ < tol then
                                  16:                    break
                                  17:              endif
                                  18:              dz ←dz
                                  19:              ∇            new  ←OrthogonalMethod(∇                                , J, ∆J)       ▷ Update introspection model gradient
                                                       introspection                                      introspection
                                  20:         endfor
                                  21:         Update introspection network using ∇
                                                                                                       introspection
                                  22:         return dphantom,None,None
                                                           z
                                  23: end procedure
                                  B PROOFFORFIXEDPOINTITERATION
                                  TheBanachfixed-point theorem, also known as the contraction mapping theorem, is a fundamental result in
                                  the theory of metric spaces (Agarwal et al., 2018). It guarantees the existence and uniqueness of fixed points
                                  for certain self-maps of complete metric spaces and provides a constructive method to find these fixed points.
                                  Let (X,d) be a non-empty complete metric space with a contraction mapping T : X → X. A contraction
                                  mappingsatisfies the following inequality for some κ < 1:
                                                                               d(T(x),T(y)) ≤ κ·d(x,y)                      for all x,y ∈ X
                                  The Banach fixed-point theorem states that T admits a unique fixed point x∗ in X such that T(x∗) = x∗,
                                  formalized as:
                                                                                                                               ∗                  ∗         ∗
                                                                    If T : X → X is a contraction, then ∃!x ∈ X : T(x ) = x                                                               (19)
                                                                                                         18
