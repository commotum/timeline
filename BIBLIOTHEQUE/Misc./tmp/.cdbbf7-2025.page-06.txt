                   Figure 4. Feature processing to produce BEV features in a LiDAR backbone in CMT [39]. Our sparse voxel features are intermediate
                   results in this process (in 3D sparse encoder). Voxelization, voxel layer, and voxel encoder do not contain learnable parameters.
                                    Model                 mAP         NDS           Feat. Res          #oftokens        Detector cost (GFLOPs)           Modelsize (M)
                                 UVTR[17]                  65.4       70.2       180×180×5               162,000                      -                         88.9
                                  CMT[39]                  70.3       72.9       180×180×1                62,400                    163.6                       83.9
                          SparseVoxFormer-base             70.8       73.2      180×180×11                18,000                    61.3                        77.5
                                                         Table 1. Effect of sparse multi-modal features on the nuScenes val set [3].
                       Application of DSVT              CMT                 SparseVoxFormer               # of tokens      mAP         NDS
                                                  mAP         NDS          mAP         NDS                    Full         72.3        74.5      Voxel size (m)    mAP      NDS
                              None                70.3         72.9        70.8         73.2                  Half         72.2        74.4           0.20         70.5     72.7
                              LiDAR            70.8(+0.5)   73.2(+0.3)  71.7(+0.9)   74.4(+1.2)             10,000         72.2        74.4           0.10         71.3     73.7
                     Multi-modal (deep fusion)      -           -       72.3(+1.5)   74.5(+1.3)              7,500         72.1        74.3           0.075        72.2     74.4
                   Table 2. Effect of sparse feature refinement via DSVT (deep fu-                           5,000         70.0        73.4           0.05         72.5     74.9
                   sion) according to an applied modality.                                                   2,500         55.5        65.8
                                                                                                       Table 3. Accuracy according to the number of remaining trans-
                   of fully sparse features may still introduce redundant com-                         former tokens (left) and input voxel resolutions (right).
                   putations, particularly since many of these features pertain
                   in backgrounds like buildings and roads, which are irrele-                          train our models for 20 epochs, specifically with GT sam-
                   vantfor3Dobjectdetection.Moreover,distinctfromthe2D                                 pling for first 15 epochs and without the sampling for later
                   object detection case [26, 47], the different sparsity of Li-                       5 epochs. We use VoVNet [15] as a image backbone, and
                   DARdata may cause computational instability by produc-                              a part of VoxelNet [48] as a LiDAR backbone as shown in
                   ing different number of transformer tokens. To handle these                         Fig. 4. We use voxel features with the final resolution of
                   problems,inspiredby[26,47],wepresentanadditionalfea-                                180×180×11withtheinputvoxelsizeof0.075mforthe
                   ture elimination scheme which removes the majority of our                           following experiments unless we notify.
                   sparse features before they are fed into the detector.
                       Tothis end, we employ an auxiliary binary classification                        Evaluation metrics           Weusetwoevaluation metric in this
                   head before the transformer decoder. Profiting from the na-                         paper.FirstismAP(meanAveragePrecision),whichissim-
                   ture of our sparse features, each of which carries central co-                      ilar to 2D object detection, but defined by using an overlap
                   ordinates (x,y,z), each feature can be directly supervised                          between3Dcuboidsofapredictionanditslabelinstead2D
                   by whether the coordinates is belong to the cuboids of ob-                          boxesofthem.SecondisNDS(NuscenesDetectionScore),
                   ject detection annotations. To train the auxiliary head, we                         which considers five factors: translation, scale, orientation,
                   utilize focal loss [23] with a binary label. A label is as-                         velocity, and attribute errors of the cuboid of each instance.
                   signed a value of 1 if a voxel feature belongs to any bound-
                   ing cuboid of the annotations, and vice versa. To prevent                           4.1. Component Analysis
                   a true negative case, we use more generous positive labels                          Effectiveness of using sparse voxels (baseline)                   This pa-
                   by dilating the size of the cuboids by 50%. We can elimi-                           per presents a new paradigm to use sparse 3D voxel fea-
                   nate redundant background features by retaining the Top-K                           tures from the multi-modal input of LiDAR and cameras
                   features based on the confidence score of the trained head,                         for 3D object detection. Our SparseVoxFormer uses high-
                   implying that the detector uses a fixed number of tokens.                           resolution features more effectively than UVTR [17], which
                   4. Experimental Results                                                             utilizes full 3D voxel features, thanks to the sparse nature of
                                                                                                       LiDARdata(refer to Table 1).
                   Implementation details              Following to CMT, we use bi-                        Furthermore, compared to the state-of-the-art BEV-
                   partite matching [4] for set prediction, focal loss [23] for                        based model (CMT [39]), the only modifications from the
                   classification , L1 loss for cuboid regression, and query de-                       CMTmodeltoadopttheuseofmulti-modalsparsefeatures
                   noising [16]. We use the nuScenes training dataset [3] and                          allow our base model to achieve higher accuracy (mAP)
