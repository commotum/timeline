                                        BSuite                Proprio Control             Visual Control               Atari100k                   DMLab                    ProcGen                      Atari                  Minecraft
                                       23 tasks                  20 tasks                    20 tasks                   26 tasks                  30 tasks                  16 tasks                   57 tasks                  1 task
                                   <1 million steps            1 million steps            1 million steps            400,000 steps            100 million steps          50 million steps          200 million steps        100 million steps
                               70                        900                       900                         130                        70                        70                        900                        9
                               50                        600                       600                          90                        50                        50                        600                        6
                             Score30                   Score300                   Score300                   Score50                    Score30                   Score30                   Score300                    Score3
                               10     PPO DQN Boot DQNDreamer    PPO TD-MPC2DMPODreamer      PPOTD-MPC2DrQ-v2Dreamer10  PPOTWM IRISDreamer10      PPOR2D2+IMPALADreamer10   PPO RainbowPPGDreamer      PPO RainbowMuZeroDreamer PPORainbowIMPALADreamer
                                                           0                          0                                                                                                          0                       0
                                                                                                                Tuned experts             Unied conguration
                  Fig. 4 | Benchmark scores. Using fixed hyperparameters across all domains,                                           high-quality implementation of the widely applicable PPO algorithm. IMPALA 
                  Dreamer outperforms tuned expert algorithms across a wide range of                                                   and R2D2+ use ten times more data on DMLab.
                  benchmarks and data budgets. Dreamer also substantially outperforms a 
                  or model free—in the domains they are applicable to and outperforms                                                  three-dimensional world. Episodes last until the player dies or up to 
                  PPO across all domains.                                                                                              36,000 steps equalling 30 minutes, during which the player needs to 
                  •  Atari. This established benchmark contains 57 Atari 2600 games with                                               discover a sequence of 12 items from sparse rewards by foraging for 
                                                                                                                               35
                    a budget of 200 million frames, posing a diverse range of challenges .                                             resources and crafting tools. It takes experienced human players about 
                                                                                                                                                                                          21
                    We use the sticky action simulator setting. Dreamer outperforms                                                    20 minutes to obtain diamonds . We form a categorical action space 
                    the powerful MuZero algorithm10 while using only a fraction of the                                                 of the actions provided by the MineRL competition, which includes 
                    computational resources. Dreamer also outperforms the widely used                                                  abstract crafting actions. Moreover, we follow previous work in accel-
                                                                 36              37
                    expert algorithms Rainbow  and IQN .                                                                               erating block breaking because learning to hold a button for hundreds 
                                                                                                                                                                                                                                         20
                  •  ProcGen. This benchmark of 16 games features randomized levels                                                    of consecutive steps would be infeasible for stochastic policies , allow-
                    and visual distractions to test the robustness and generalization of                                               ing us to focus on the essential challenges inherent in Minecraft. Refer 
                                38
                    agents . Within the budget of 50 million frames, Dreamer outper-                                                   to Supplementary Information for details and a comparison with video 
                                                                                      34                       38                                                                       21,47
                    forms the tuned expert algorithms PPG  and Rainbow . Our PPO                                                       pretraining (VPT) and Voyager                         .
                    agent with fixed hyperparameters matches the published score of                                                        Because of the training time in this complex domain, extensive 
                                                                                              34
                    the highly tuned official PPO implementation .                                                                     tuning would be difficult for Minecraft. Instead, we apply Dreamer 
                  •  DMLab. This suite of 30 tasks features three-dimensional environ-                                                 out of the box with its default hyperparameters. As shown in Fig. 5, 
                                                                                                      39
                    ments that test spatial and temporal reasoning . In 100 million                                                    Dreamer is, to our knowledge, the first algorithm to collect diamonds 
                    frames, Dreamer exceeds the performance of the scalable IMPALA                                                     in Minecraft from scratch without using human data, as required by 
                                                  33                                                                                          21                                     20
                    and R2D2+ agents  at 1 billion environment steps, amounting to a                                                   VPT , or adaptive curricula . All the Dreamer agents discover dia-
                    data-efficiency gain of over 1,000%. We note that these baselines were                                             monds within 100 million environment steps. Success rates of all items 
                    not designed for data efficiency but serve as a valuable comparison                                                are shown in Extended Data Fig. 2. Although several strong baselines 
                    point for the performance previously achievable at scale.                                                          progress to advanced items such as the iron pickaxe, none of them 
                  •  Atari100k. This data-efficiency benchmark contains 26 Atari games                                                 discover a diamond.
                    and a budget of only 400,000 frames, amounting to 2 hours of game 
                            18                       40                                                                                Ablations
                    time . EfficientZero  holds the state of the art by combining online 
                    tree search, prioritized replay and hyperparameter scheduling, but                                                 In Fig. 6, we ablate the robustness techniques and learning signals on 
                    also resets levels early to increase data diversity, making a compari-                                             a diverse set of 14 tasks to understand their importance. The training 
                    son difficult. Without this complexity, Dreamer outperforms the                                                    curves of individual tasks are included in Supplementary Information. 
                    best remaining methods, including the model-based IRIS, TWM and                                                    We observe that all robustness techniques contribute to performance, 
                    SimPLe agents, and the model-free SPR41.                                                                           most notably the Kullback–Leibler balancing and free bits of the world 
                  •  Proprio Control Suite. This benchmark contains 20 simulated robot                                                 model objective, followed by return normalization and symexp two-hot 
                    tasks with continuous actions, proprioceptive vector inputs and a                                                  regression for reward and value prediction. In general, we find that 
                    budget of 1 million environment steps42. The tasks range from clas-                                                each individual technique is critical on a subset of tasks but may not 
                    sical control over locomotion to robot manipulation tasks, featuring                                               affect performance on other tasks. To investigate the effect of the 
                    dense and sparse rewards. Dreamer matches the state of the art on                                                  world model, we ablate the learning signals of Dreamer by stopping 
                                                                       31                       43
                    this benchmark, such as DMPO  and TD-MPC2 .                                                                        either the task-specific reward and value prediction gradients or the 
                  •  Visual Control Suite. This benchmark consists of the same 20 con-                                                 task-agnostic reconstruction gradients from shaping its representa-
                    tinuous control tasks but the agent receives only high-dimensional                                                 tions. Whereas previous reinforcement-learning algorithms often rely 
                                             42                                                                                                                                                   9,10
                    images as input . Dreamer establishes a state of the art on this bench-                                            only on task-specific learning signals                        , Dreamer rests predominantly 
                                                                   44                       43
                    mark, outperforming DrQ-v2  and TD-MPC2 , which are specialized                                                    on the unsupervised objective of its world model. This finding could 
                    to visual environments by leveraging data augmentation.                                                            allow for future algorithm variants that leverage pretraining on unsu-
                  •  BSuite. This benchmark includes 23 environments with a total of 468                                               pervised data.
                    configurations that are specifically designed to test credit assign-
                    ment, robustness to reward scale and stochasticity, memory, gen-                                                   Scaling properties
                                                                  45
                    eralization, and exploration . Dreamer establishes a state of the art                                              To investigate whether Dreamer can scale robustly, we train 6 model 
                                                                                                                              46
                    on this benchmark, outperforming Boot DQN and other methods .                                                      sizes ranging from 12 million to 400 million parameters, as well as 
                                                                                                                                                                                               48                               39
                    Dreamer improves over previous algorithms especially in the scale                                                  different replay ratios on Crafter  and a DMLab task . The replay 
                    robustness category.                                                                                               ratio affects the number of gradient updates performed by the agent.  
                                                                                                                                       Figure 6 shows robust learning with fixed hyperparameters across 
                  Minecraft                                                                                                            the compared model sizes and replay ratios. Moreover, increasing the 
                  Collecting diamonds in the popular game Minecraft has been a                                                         model size directly translates to both higher task performance and a 
                                                                                                   19–21
                  long-standing challenge in artificial intelligence                                    . Every episode                lower data requirement. Increasing the number of gradient steps fur-
                  in this game is set in a unique randomly generated and infinite                                                      ther reduces the interactions needed to learn successful behaviours. 
                                                                                                                                                                                         Nature | Vol 640 | 17 April 2025 | 651
