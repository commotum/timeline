                                        Transformer-XL: Attentive Language Models
                                                   BeyondaFixed-LengthContext
                                                 ⇤12                 ⇤12                   1                       1
                                   ZihangDai         , Zhilin Yang       , Yiming Yang , Jaime Carbonell ,
                                                                   2                              1
                                                     QuocV.Le ,RuslanSalakhutdinov
                                                 1Carnegie Mellon University, 2Google Brain
                                {dzihang,zhiliny,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com
                                        Abstract                             Term Memory (LSTM) networks (Hochreiter and
                       Transformers have a potential of learning             Schmidhuber, 1997), have been a standard solu-
                       longer-term dependency, but are limited by a          tion to language modeling and obtained strong
                       ﬁxed-length context in the setting of language        results on multiple benchmarks.           Despite the
                       modeling.    We propose a novel neural ar-            wide adaption, RNNs are difﬁcult to optimize
                       chitecture Transformer-XL that enables learn-         due to gradient vanishing and explosion (Hochre-
                       ing dependency beyond a ﬁxed length with-             iter et al., 2001), and the introduction of gat-
                       out disrupting temporal coherence.     It con-        ing in LSTMs and the gradient clipping tech-
                       sists of a segment-level recurrence mechanism         nique (Graves, 2013) might not be sufﬁcient to
                       and a novel positional encoding scheme. Our           fully address this issue.      Empirically, previous
                       methodnotonlyenablescapturinglonger-term              work has found that LSTM language models use
                       dependency, but also resolves the context frag-       200 context words on average (Khandelwal et al.,
                       mentation problem. As a result, Transformer-
                       XLlearnsdependencythat is 80% longer than             2018), indicating room for further improvement.
                       RNNs and 450% longer than vanilla Trans-                 On the other hand, the direct connections be-
                       formers, achieves better performance on both          tween long-distance word pairs baked in atten-
                       short and long sequences, and is up to 1,800+         tion mechanisms might ease optimization and en-
                       times faster than vanilla Transformers during         able the learning of long-term dependency (Bah-
                       evaluation. Notably, we improve the state-of-         danau et al., 2014; Vaswani et al., 2017).         Re-
                       the-art results of bpc/perplexity to 0.99 on en-      cently, Al-Rfou et al. (2018) designed a set of aux-
                       wiki8, 1.08 on text8, 18.3 on WikiText-103,
                       21.8 on One Billion Word, and 54.5 on Penn            iliary losses to train deep Transformer networks
                       Treebank (without ﬁnetuning). When trained            for character-level language modeling, which out-
                       only on WikiText-103, Transformer-XL man-             perform LSTMs by a large margin. Despite the
                       ages to generate reasonably coherent, novel           success, the LM training in Al-Rfou et al. (2018)
                       text articles with thousands of tokens.   Our         is performed on separated ﬁxed-length segments
                       code, pretrained models, and hyperparameters          of a few hundred characters, without any informa-
                       are available in both Tensorﬂow and PyTorch1.
                                                                             tion ﬂow across segments. As a consequence of
                   1   Introduction                                          the ﬁxed context length, the model cannot capture
                   Language modeling is among the important prob-            any longer-term dependency beyond the prede-
                   lemsthatrequire modeling long-term dependency,            ﬁned context length. In addition, the ﬁxed-length
                   with successful applications such as unsupervised         segments are created by selecting a consecutive
                   pretraining (Dai and Le, 2015; Peters et al., 2018;       chunk of symbols without respecting the sentence
                   Radford et al., 2018; Devlin et al., 2018). How-          or any other semantic boundary. Hence, the model
                   ever, it has been a challenge to equip neural             lacks necessary contextual information needed to
                   networks with the capability to model long-term           well predict the ﬁrst few symbols, leading to inef-
                   dependency in sequential data.       Recurrent neu-       ﬁcient optimization and inferior performance. We
                   ral networks (RNNs), in particular Long Short-            refer to this problem as context fragmentation.
                                                                                To address the aforementioned limitations of
                     ⇤Equal contribution. Order determined by swapping the   ﬁxed-length contexts, we propose a new architec-
                   one in Yang et al. (2017).
                      1https://github.com/kimiyoung/                         ture called Transformer-XL (meaning extra long).
                   transformer-xl                                            We introduce the notion of recurrence into our
                                                                         2978
                          Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988
                                                                        c
                                 Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics
                 deep self-attention network. In particular, instead     as an additional input.     Existing works range
                 of computing the hidden states from scratch for         from ones where context representations are man-
                 each new segment, we reuse the hidden states ob-        ually deﬁned (Mikolov and Zweig, 2012; Ji et al.,
                 tained in previous segments. The reused hidden          2015; Wang and Cho, 2015) to others that rely on
                 states serve as memory for the current segment,         document-level topics learned from data (Dieng
                 which builds up a recurrent connection between          et al., 2016; Wang et al., 2017).
                 the segments. As a result, modeling very long-            More broadly, in generic sequence modeling,
                 term dependency becomes possible because in-            how to capture long-term dependency has been a
                 formation can be propagated through the recur-          long-standing research problem. From this per-
                 rent connections.    Meanwhile, passing informa-        spective, since the ubiquitous adaption of LSTM,
                 tion from the previous segment can also resolve         many efforts have been spent on relieving the
                 the problem of context fragmentation. More im-          vanishing gradient problem, including better ini-
                 portantly, we show the necessity of using relative      tialization (Le et al., 2015), additional loss sig-
                 positional encodings rather than absolute ones, in      nal (Trinh et al., 2018), augmented memory struc-
                 order to enable state reuse without causing tem-        ture (Ke et al., 2018) and others that modify the in-
                 poral confusion. Hence, as an additional techni-        ternal architecture of RNNs to ease the optimiza-
                 cal contribution, we introduce a simple but more        tion (Wu et al., 2016; Li et al., 2018). Different
                 effective relative positional encoding formulation      from them, our work is based on the Transformer
                 that generalizestoattentionlengthslongerthanthe         architecture and shows that language modeling as
                 one observed during training.                           a real-world task beneﬁts from the ability to learn
                    Transformer-XL obtained strong results on ﬁve        longer-term dependency.
                 datasets, varying from word-level to character-
                 level language modeling. Transformer-XL is also         3   Model
                 able to generate relatively coherent long text arti-    Given a corpus of tokens x =(x ,...,x ), the
                 cles with thousands of tokens (see Appendix E),                                             1       T
                 trained on only 100M tokens.                            task of language modeling is to estimate the joint
                                                                         probability P(x), which is often auto-regressively
                    Our main technical contributions include intro-                             Q
                 ducing the notion of recurrence in a purely self-       factorized as P(x)= tP(xt | x<t). With the
                 attentive model and deriving a novel positional en-     factorization, the problem reduces to estimating
                 codingscheme. Thesetwotechniquesformacom-               each conditional factor. In this work, we stick to
                 plete set of solutions, as any one of them alone        the standard neural approach to modeling the con-
                 does not address the issue of ﬁxed-length con-          ditional probability. Speciﬁcally, a trainable neu-
                 texts.  Transformer-XL is the ﬁrst self-attention       ral network is used to encode the context x<t into
                 model that achieves substantially better results        a ﬁxed size hidden state, which is multiplied with
                 than RNNsonbothcharacter-level and word-level           the wordembeddingstoobtainthelogits. Thelog-
                 language modeling.                                      its are then fed into the Softmax function, yielding
                                                                         a categorical probability distribution over the next
                 2    Related Work                                       token.
                 In the last few years, the ﬁeld of language mod-        3.1   Vanilla Transformer Language Models
                 eling has witnessed many signiﬁcant advances,           In order to apply Transformer or self-attention to
                 including but not limited to devising novel ar-         language modeling, the central problem is how to
                 chitectures to better encode the context (Bengio        train a Transformer to effectively encode an arbi-
                 et al., 2003; Mikolov et al., 2010; Merity et al.,      trarily long context into a ﬁxed size representation.
                 2016; Al-Rfou et al., 2018), improving regulariza-      Given inﬁnite memory and computation, a sim-
                 tion andoptimizationalgorithms(GalandGhahra-            ple solution would be to process the entire con-
                 mani, 2016) , speeding up the Softmax computa-          text sequence using an unconditional Transformer
                 tion(Graveetal.,2016a),andenrichingtheoutput            decoder, similar to a feed-forward neural network.
                 distribution family (Yang et al., 2017).                However,thisisusuallyinfeasiblewiththelimited
                    To capture the long-range context in language        resource in practice.
                 modeling, a line of work directly feeds a repre-          Onefeasible but crude approximation is to split
                 sentation of the wider context into the network         the entire corpus into shorter segments of man-
                                                                    2979
                                      x       x       x        x            x       x       x        x      x       x       x       x       x       x              x        x       x       x       x       x                x       x       x       x       x       x
                                       1       2       3       4             5       6       7       8       1       2       3       4       5       6              1       2        3       4       5       6                1       2       3       4       5       6
                                              Segment 1                              Segment 2                   Limited Context                                                 Limited Context                                                   Limited Context
                                                        (a) Train phase.                                                                                              (b) Evaluation phase.
                                                                                    Figure 1: Illustration of the vanilla model with a segment length 4.
                                     ageable sizes, and only train the model within                                                                        3.2          Segment-Level Recurrence with State
                                     each segment, ignoring all contextual information                                                                                  Reuse
                                     from previous segments. This is the idea adopted                                                                      To address the limitations of using a ﬁxed-length
                                     by Al-Rfou et al. (2018). We call it the vanilla                                                                      context, we propose to introduce a recurrence
                                     model and visualize it in Fig.                                            1a.         Under this                      mechanism to the Transformer architecture. Dur-
                                     training paradigm, information never ﬂows across                                                                      ing training, the hidden state sequence computed
                                     segments in either the forward or backward pass.                                                                      for the previous segment is ﬁxed and cached to
                                     There are two critical limitations of using a ﬁxed-                                                                   be reused as an extended context when the model
                                     length context. First, the largest possible depen-                                                                    processes the next new segment, as shown in Fig.
                                     dency length is upper bounded by the segment                                                                          2a. Although the gradient still remains within a
                                     length, which is a few hundred on character-level                                                                     segment, this additional input allows the network
                                     language modeling (Al-Rfou et al., 2018). There-                                                                      to exploit information in the history, leading to an
                                     fore, although the self-attention mechanism is less                                                                   ability of modeling longer-term dependency and
                                     affected by the vanishing gradient problem com-                                                                       avoiding context fragmentation. Formally, let the
                                     pared to RNNs, the vanilla model is not able to                                                                       two consecutive segments of length L be s⌧ =
                                     fully exploit this optimization advantage. Second,                                                                    [x⌧,1,···,x                       ] and s⌧+1 =[x⌧+1,1,···,x                                                  ]
                                     though it is possible to use padding to respect the                                                                                              ⌧,L                                                                   ⌧+1,L
                                     sentence or other semantic boundaries, in practice                                                                    respectively. Denoting the n-th layer hidden state
                                                                                                                                                           sequence produced for the ⌧-th segment s⌧ by
                                     it has been standard practice to simply chunk long                                                                    hn 2 RL⇥d, where d is the hidden dimension.
                                     text into ﬁxed-length segments due to improved                                                                            ⌧
                                     efﬁciency (Peters et al., 2018; Devlin et al., 2018;                                                                  Then, the n-th layer hidden state for segment s⌧+1
                                                                                                                                                           is produced (schematically) as follows,
                                     Al-Rfou et al., 2018). However, simply chunking                                                                           en 1             ⇥           n 1              n 1⇤
                                                                                                                                                               h           = SG(h                   )   h              ,
                                     a sequence into ﬁxed-length segments will lead to                                                                            ⌧+1                       ⌧                ⌧+1
                                                                                                                                                                  n           n           n               n 1          > en 1               > en 1               >
                                     the context fragmentation problem as discussed in                                                                         q⌧+1,k⌧+1,v⌧+1 = h⌧+1Wq ,h⌧+1Wk,h⌧+1Wv ,
                                                                                                                                                               hn          =Transformer-Layer(qn                              , kn        , vn        ).
                                     Section 1.                                                                                                                   ⌧+1                                                  ⌧+1         ⌧+1         ⌧+1
                                                                                                                                                           where the function SG(·) stands for stop-gradient,
                                                                                                                                                           the notation [hu   hv] indicates the concatenation
                                                                                                                                                           of two hidden sequences along the length dimen-
                                          During evaluation, at each step, the vanilla                                                                     sion, and W· denotes model parameters. Com-
                                     modelalsoconsumesasegmentofthesamelength                                                                              pared to the standard Transformer, the critical dif-
                                     as in training, but only makes one prediction at the                                                                  ference lies in that the key kn                                             and value vn
                                                                                                                                                                                                                             ⌧+1                                 ⌧+1
                                                                                                                                                                                                                                                     en 1
                                     last position. Then, at the next step, the segment                                                                    are conditioned on the extended context h⌧+1 and
                                     is shifted to the right by only one position, and the                                                                 hence hn 1 cached from the previous segment.
                                                                                                                                                                              ⌧
                                     newsegmenthastobeprocessedall from scratch.                                                                           Weemphasize this particular design by the green
                                     As shown in Fig. 1b, this procedure ensures that                                                                      paths in Fig. 2a.
                                     each prediction utilizes the longest possible con-                                                                          With this recurrence mechanism applied to ev-
                                     text exposedduringtraining,andalsorelievescon-                                                                        ery two consecutive segments of a corpus, it es-
                                     text fragmentation issue encountered in training.                                                                     sentially creates a segment-level recurrence in the
                                     However, this evaluation procedure is extremely                                                                       hidden states. As a result, the effective context be-
                                     expensive. We will show that our proposed archi-                                                                      ing utilized can go way beyond just two segments.
                                     tecture is able to substantially improve the evalua-                                                                  However,noticethat the recurrent dependency be-
                                     tion speed.                                                                                                           tween hn                    and hn 1 shifts one layer downwards
                                                                                                                                                                             ⌧+1                    ⌧
                                                                                                                                                  2980
                                              x       x        x       x       x       x       x        x            x       x       x       x       x        x       x       x       x       x       x       x        x       x       x       x       x        x       x       x       x      x        x       x
                                               1       2        3       4       5       6       7        8            1       2       3       4       5        6       7       8       9       10      11      12       1       2       3       4       5        6       7       8       9      10       11      12
                                                    Fixed (No Grad)                   New Segment                                                          Fixed (No Grad)                   New Segment                                                            Extended Context
                                                                                                           (a) Training phase.                                                                                                                (b) Evaluation phase.
                                                                                       Figure 2: Illustration of the Transformer-XL model with a segment length 4.
                                           per-segment, which differs from the same-layer                                                                                              der to reuse the hidden states. That is, how can
                                           recurrence in conventional RNN-LMs.                                                                            Conse-                       wekeepthepositional information coherent when
                                           quently, the largest possible dependency length                                                                                             we reuse the states? Recall that, in the standard
                                           grows linearly w.r.t. the number of layers as well                                                                                          Transformer, the information of sequence order is
                                           as the segment length, i.e., O(N ⇥ L), as vi-                                                                                               provided by a set of positional encodings, denoted
                                           sualized by the shaded area in Fig.                                                                     2b.           This                  as U 2 RLmax⇥d, where the i-th row Ui corre-
                                           is analogous to truncated BPTT (Mikolov et al.,                                                                                             sponds to the i-th absolute position within a seg-
                                           2010), a technique developed for training RNN-                                                                                              ment and Lmax prescribes the maximum possible
                                           LMs. However, different from truncated BPTT,                                                                                                length to be modeled. Then, the actual input to the
                                           our method caches a sequence of hidden states in-                                                                                           Transformer is the element-wise addition of the
                                           stead of the last one, and should be applied to-                                                                                            wordembeddingsandthepositional encodings. If
                                           gether with the relative positional encoding tech-                                                                                          we simply adapt this positional encoding to our
                                           nique described in Section 3.3.                                                                                                             recurrence mechanism, the hidden state sequence
                                                  Besides achieving extra long context and re-                                                                                         would be computed schematically by
                                           solving fragmentation, another beneﬁt that comes                                                                                                                          h⌧+1 = f(h⌧,Es                                  +U1:L)
                                                                                                                                                                                                                                                            ⌧+1
                                           with the recurrence scheme is signiﬁcantly faster                                                                                                                              h⌧ = f(h⌧ 1,Es⌧ +U1:L),
                                           evaluation.                        Speciﬁcally, during evaluation, the                                                                      where E                         2 RL⇥d is the word embedding se-
                                           representations from the previous segments can                                                                                                                     s⌧
                                           be reused instead of being computed from scratch                                                                                            quence of s⌧, and f represents a transformation
                                           as in the case of the vanilla model. In our ex-                                                                                             function. Notice that, both Es⌧ and Es⌧+1 are as-
                                           periments on enwiki8, Transformer-XL is up to                                                                                               sociated with the same positional encoding U1:L.
                                           1,800+ times faster than the vanilla model during                                                                                           As a result, the model has no information to dis-
                                           evaluation (see Section 4).                                                                                                                 tinguish the positional difference between x⌧,j and
                                                  Finally, notice that the recurrence scheme does                                                                                      x⌧+1,j for any j =1,...,L, resulting in a sheer
                                           not need to be restricted to only the previous seg-                                                                                         performance loss.
                                           ment. In theory, we can cache as many previous                                                                                                    In order to avoid this failure mode, the funda-
                                           segments as the GPU memory allows, and reuse                                                                                                mental idea is to only encode the relative posi-
                                           all of them as the extra context when processing                                                                                            tional information in the hidden states. Concep-
                                           the current segment. Thus, we can cache a prede-                                                                                            tually, the positional encoding gives the model a
                                           ﬁned length-M old hidden states spanning (pos-                                                                                              temporal clue or “bias” about how information
                                           sibly) multiple segments, and refer to them as the                                                                                          should be gathered, i.e., where to attend. For the
                                           memorymn 2RM⇥d,duetoaclearconnectionto                                                                                                      same purpose, instead of incorporating bias stati-
                                                                        ⌧                                                                                                              cally into the initial embedding, one can inject the
                                           the memory augmented neural networks (Graves                                                                                                same information into the attention score of each
                                           et al., 2014; Weston et al., 2014). In our experi-                                                                                          layer. More importantly, it is more intuitive and
                                           ments, we set M equal to the segment length dur-                                                                                            generalizable to deﬁne the temporal bias in a rela-
                                           ing training, and increase it by multiple times dur-                                                                                        tive manner. Forinstance, whenaqueryvectorq
                                           ing evaluation.                                                                                                                                                                                                                                                        ⌧,i
                                                                                                                                                                                       attends on the key vectors k⌧,i, it does not need
                                           3.3            Relative Positional Encodings                                                                                                to know the absolute position of each key vector
                                                                                                                                                                                       to identify the temporal order of the segment. In-
                                           While we found the idea presented in the pre-                                                                                               stead, it sufﬁces to know the relative distance be-
                                           vious subsection very appealing, there is a cru-                                                                                            tweeneachkeyvectork⌧,j anditselfq⌧,i, i.e. i j.
                                           cial technical challenge we haven’t solved in or-                                                                                           Practically, one can create a set of relative posi-
                                                                                                                                                                           2981
                    tional encodings R 2 RLmax⇥d, where the i-th row              • Finally, we deliberately separate the two weight
                    Ri indicates a relative distance of i between two                matrices Wk,E and Wk,R for producing the
                    positions. By injecting the relative distance dy-                content-based key vectors and location-based
                    namicallyintotheattentionscore,thequeryvector                    key vectors respectively.
                    can easily distinguish the representations of x⌧,j            Under the new parameterization, each term has
                    and x⌧+1,j from their different distances, making             an intuitive meaning: term (a) represents content-
                    the state reuse mechanism feasible. Meanwhile,                based addressing, term (b) captures a content-
                    wewon’tloseanytemporalinformation,astheab-                    dependent positional bias, term (c) governs a
                    solute position can be recovered recursively from             global content bias, and (d) encodes a global po-
                    relative distances.                                           sitional bias.
                       Previously, the idea of relative positional encod-            In comparison, the formulation in Shaw et al.
                    ings has been explored in the context of machine              (2018) only has terms (a) and (b), dropping the
                    translation (Shaw et al., 2018) and music gener-              two bias terms (c) and (d). Moreover, Shaw et al.
                    ation (Huang et al., 2018). Here, we offer a dif-             (2018) merge the multiplication WkR into a sin-
                                                                                                          ˆ
                    ferent derivation, arriving at a new form of rel-             gle trainable matrix R, which abandons the induc-
                    ative positional encodings, which not only has a              tive bias built into the original sinusoid positional
                    one-to-one correspondence to its absolute coun-               encoding (Vaswani et al., 2017). In contrast, our
                    terpart but also enjoys much better generalization            relative positional embedding R adapts the sinu-
                    empirically (see Section 4). Firstly, in the standard         soid formulation. As a beneﬁt of the inductive
                    Transformer (Vaswani et al., 2017), the attention             bias, a model trained on a memory of some certain
                    score between query qi and key vector kj within               length can automatically generalize to a memory
                    the same segment can be decomposed as                         several times longer during evaluation.
                           Aabs = E>W>WkEx +E>W>WkUj                                 Equipping the recurrence mechanism with our
                             i,j    xi   q        j     xi   q
                                   |      {z      }   |      {z      }            proposedrelativepositionalembedding,weﬁnally
                                          (a)                (b)                  arrive at the Transformer-XL architecture.            For
                                +U>W>WkEx +U>W>WkUj.
                                    i    q        j     i   q
                                  |      {z       }   |     {z      }             completeness, we summarize the computational
                                         (c)                (d)                   procedure for a N-layer Transformer-XL with a
                       Following the idea of only relying on rela-                single attention head here. For n =1,...,N:
                    tive positional information, we propose to re-                         en 1    ⇥       n 1     n 1⇤
                    parameterize the four terms as follows                                h⌧     = SG(m⌧ ) h⌧
                                                                                       n   n  n     n 1    n> en 1      n > en 1      n>
                          rel    >    >                >    >                        q⌧,k⌧,v⌧ =h⌧       Wq ,h⌧ Wk,E ,h⌧ Wv
                        Ai,j = ExiWq Wk,EExj +ExiWq Wk,RRi j                                n       n > n        n >    n
                                |       {z       }   |        {z        }                 A⌧,i,j =q⌧,i k⌧,j +q⌧,i Wk,RRi j
                                        (a)                   (b)
                                                                                                   +u>k⌧,j +v>Wn Ri j
                             +u>W E +v>W R .                                                                          k,R
                                     k,E xj          k,R   i j                                n                       n   n
                               |     {z     }  |      {z     }                               a⌧ =Masked-Softmax(A⌧)v⌧
                                     (c)             (d)                                     on =LayerNorm(Linear(an)+hn 1)
                                                                                              ⌧                        ⌧      ⌧
                    • The ﬁrst change we make is to replace all ap-                           n                                 n
                                                                                             h =Positionwise-Feed-Forward(o )
                      pearances of the absolute positional embedding                          ⌧                                 ⌧
                                                                                           0  :
                      Uj for computing key vectors in term (b) and                with h⌧      = Es⌧ deﬁned as the word embed-
                      (d) with its relative counterpart Ri j. This es-            ding sequence. In addition, it is worth mention-
                      sentially reﬂects the prior that only the relative          ing that a naive way to compute A requires com-
                      distance matters for where to attend. Note that             puting Wn Ri j for all pairs (i,j), whose cost
                                                                                              k,R
                      Risasinusoidencodingmatrix(Vaswanietal.,                    is quadratic w.r.t.     the sequence length.        How-
                      2017) without learnable parameters.                         ever, noticing that the value of i   j only ranges
                    • Secondly, we introduce a trainable parameter                from zero to the sequence length, we show a sim-
                      u 2 Rd to replace the query U>W> in term                    ple computation procedure in Appendix B, which
                                                             i    q               reduces the cost to be linear w.r.t. the sequence
                      (c). In this case, since the query vector is the            length.
                      samefor all query positions, it suggests that the
                      attentive bias towards different words should re-           4    Experiments
                      main the same regardless of the query position.             4.1    MainResults
                      With a similar reasoning, a trainable parameter
                      v 2 Rd is added to substitute U>W> in term                  WeapplyTransformer-XL to a variety of datasets
                                                             i    q
                      (d).                                                        on both word-level and character-level language
                                                                             2982
                     Model                                       #Param PPL           Model                                   #Param bpc
                     Grave et al. (2016b) - LSTM                    -      48.7       Cooijmans et al. (2016) - BN-LSTM           -     1.36
                     Bai et al. (2018) - TCN                        -      45.2       Chungetal. (2016) - LN HM-LSTM            35M     1.29
                     Dauphin et al. (2016) - GCNN-8                 -      44.9       Zilly et al. (2016) - RHN                 45M     1.27
                     Grave et al. (2016b) - Neural cache            -      40.8       Krause et al. (2016) - Large mLSTM        45M     1.27
                     Dauphin et al. (2016) - GCNN-14                -      37.2       Al-Rfou et al. (2018) - 12L Transformer   44M     1.18
                     Merity et al. (2018) - QRNN                  151M     33.0       Al-Rfou et al. (2018) - 64L Transformer   235M 1.13
                     Raeetal. (2018) - Hebbian + Cache              -      29.9       Ours - 24L Transformer-XL                 277M 1.08
                     Ours - Transformer-XL Standard               151M     24.0
                     Baevski and Auli (2018) - Adaptive Input⇧    247M     20.5     Table 3: Comparison with state-of-the-art results on
                     Ours - Transformer-XL Large                  257M     18.3     text8.
                     Table 1: Comparison with state-of-the-art results on
                     WikiText-103. ⇧ indicates contemporary work.                  Model                                        #Param PPL
                                                                                   Shazeer et al. (2014) - Sparse Non-Negative    33B    52.9
                      Model                                    #Param bpc          Chelba et al. (2013) - RNN-1024 + 9 Gram       20B    51.3
                                                                                   Kuchaiev and Ginsburg (2017) - G-LSTM-2         -     36.0
                      Haetal. (2016) - LN HyperNetworks          27M     1.34      Dauphin et al. (2016) - GCNN-14 bottleneck      -     31.9
                      Chungetal. (2016) - LN HM-LSTM             35M     1.32      Jozefowicz et al. (2016) - LSTM               1.8B    30.6
                      Zilly et al. (2016) - RHN                  46M     1.27      Jozefowicz et al. (2016) - LSTM + CNN         1.04B   30.0
                      Mujika et al. (2017) - FS-LSTM-4           47M     1.25      Shazeer et al. (2017) - Low-Budget MoE        ⇠5B 34.1
                      Krause et al. (2016) - Large mLSTM         46M     1.24      Shazeer et al. (2017) - High-Budget MoE       ⇠5B 28.0
                      Knol (2017) - cmix v13                       -     1.23      Shazeer et al. (2018) - Mesh Tensorﬂow        4.9B    24.0
                      Al-Rfou et al. (2018) - 12L Transformer    44M     1.11      Baevski and Auli (2018) - Adaptive Input⇧     0.46B   24.1
                      Ours - 12L Transformer-XL                  41M     1.06      Baevski and Auli (2018) - Adaptive Input⇧     1.0B    23.7
                      Al-Rfou et al. (2018) - 64L Transformer   235M 1.06          Ours - Transformer-XL Base                    0.46B   23.5
                      Ours - 18L Transformer-XL                  88M     1.03      Ours - Transformer-XL Large                   0.8B    21.8
                      Ours - 24L Transformer-XL                 277M 0.99
                                                                                  Table4: Comparisonwithstate-of-the-artresultsonOne
                    Table2: Comparisonwithstate-of-the-artresultsonen-            Billion Word. ⇧ indicates contemporary work.
                    wik8.
                                                                                    performing the 12-layer vanilla Transformer from
                    modeling to have a comparison with state-of-the-                Al-Rfou et al. (2018) by 0.05, while both Trans-
                    art systems, including WikiText-103(Merityetal.,                former variants have a large margin over conven-
                    2016), enwik8 (LLC, 2009), text8 (LLC, 2009),                   tional RNN-based models. Notably, our 12-layer
                    One Billion Word (Chelba et al., 2013), and Penn                architecture achieves the same result as the 64-
                    Treebank (Mikolov and Zweig, 2012).                             layer network from Al-Rfou et al. (2018), using
                       WikiText-103isthelargestavailableword-level                  only 17% of the parameter budget. In order to see
                    language modeling benchmark with long-term de-                  whether better performances can be obtained by
                    pendency. It contains 103M training tokens from                 increasing the model size, we train 18-layer and
                    28K articles, with an average length of 3.6K to-                24-layer Transformer-XLs with increased model
                    kens per article, which allows testing the abil-                sizes. With the attention length 784 during train-
                    ity of long-term dependency modeling. We set                    ing and 3,800 during evaluation, we obtained a
                    the attention length to 384 during training and                 new SoTA result and our method is the ﬁrst to
                    1600duringevaluation. Weadoptedadaptivesoft-                    break through 1.0 on widely-studied character-
                    max and input representations (Baevski and Auli,                level benchmarks. Different from Al-Rfou et al.
                    2018; Grave et al., 2016a). As shown in Table 1,                (2018), Transformer-XL does not need any auxil-
                    Transformer-XLreducesthepreviousstate-of-the-                   iary losses, and thus all beneﬁts are credited to a
                    art (SoTA) perplexity from 20.5 to 18.3, which                  better architecture.
                    demonstrates the superiority of the Transformer-                   Similar to but different from enwik8, text8 con-
                    XLarchitecture.                                                 tains 100M processed Wikipedia characters cre-
                       Thedataset enwik8 contains 100M bytes of un-                 ated by lowering case the text and removing any
                    processed Wikipedia text.         We compare our ar-            character other than the 26 letters a through z, and
                    chitecture with the previous results in Table 2.                space. Due to the similarity, we simply adapt the
                    Under the model size constraint, the 12-layer                   best model and the same hyper-parameters on en-
                    Transformer-XL achieves a new SoTA result, out-                 wik8totext8withoutfurthertuning. Thecompari-
                                                                              2983
                    Model                                         #Param PPL         et al. (2018) are absolute. “Full” and “half” losses
                    Inan et al. (2016) - Tied Variational LSTM      24M     73.2     refer to applying a cross entropy loss to all or the
                    Zilly et al. (2016) - Variational RHN           23M     65.4     recent half positions in the segment. We found
                    ZophandLe(2016)-NASCell                         25M     64.0     that absolute encodings only work well with half
                    Merity et al. (2017) - AWD-LSTM                 24M     58.8     losses because half losses exclude positions with
                    Phametal. (2018) - Efﬁcient NAS                 24M     58.6
                    Liu et al. (2018) - Differentiable NAS          23M     56.1     veryshortattentionlengthsduringtrainingforbet-
                    Yangetal. (2017) - AWD-LSTM-MoS                 22M 55.97        ter generalization.      Table 6 shows that both the
                    Melis et al. (2018) - Dropout tuning            24M     55.3     recurrence mechanism and our encoding scheme
                    Ours - Transformer-XL                           24M 54.52        are necessary to achieve the best performance, as
                    Merity et al. (2017) - AWD-LSTM+Finetune†       24M     57.3     well as generalizing to longer attention sequences
                                                      †
                    Yangetal. (2017) - MoS+Finetune                 22M 54.44        during evaluation time. Although the backprop-
                   Table 5: Comparison with state-of-the-art results on              agation length during training is only 128, with
                   PennTreebank. † indicates using two-step ﬁnetuning.               the two techniques the attention length can be in-
                                                                                     creased to 640 at test time. In the standard setting
                                                                                     with151Mparameters,theperplexitydecreasesas
                    sonwithpreviousmethodsissummarizedinTable                        the attention length increases.
                    3. Again, Transformer-XLachievesthenewSoTA                          Since the recurrence mechanism costs addi-
                    result with a clear margin.                                      tional memory, we also compare Transformer-XL
                       One Billion Word does not preserve any long-                  with baselines under the same GPU memory con-
                    term dependency because sentences have been                      straints.   As shown in Table 10 in Appendix A,
                    shufﬂed. Consequently, this dataset mainly tests                 despite using a shorter backpropagation length,
                    the ability of modeling only short-term depen-                   Transformer-XLremainssuperiortothebaselines.
                    dency. The comparison between Transformer-XL                        The second study targets at isolating the ef-
                    and the other methods is shown in Table 4. Al-                   fects of resolving the context fragmentation prob-
                    thoughTransformer-XLismainlydesignedtobet-                       lem from the beneﬁt of capturing longer context
                    ter capture longer-term dependency, it dramati-                  length. In order to achieve this goal, we deliber-
                    cally improves the single-model SoTA from 23.7                   ately choose a dataset that does not require long-
                    to 21.8.      Speciﬁcally, Transformer-XL signiﬁ-                term dependency, so that any improvement from
                    cantly outperforms a contemporary method using                   establishing the recurrence can be attributed to
                    vanilla Transformers (Baevski and Auli, 2018),                   solving the context fragmentation.            Speciﬁcally,
                    suggesting the advantage of Transformer-XL is                    weperformthis controlled experiment on the One
                    generalizable to modeling short sequences.                       Billion Word dataset, which can only beneﬁt from
                       We also report the results on word-level Penn                 removing the context fragmentation.              We train
                    Treebank in Table 5.          Similar to AWD-LSTM                a 20-layer Transformer-XL with ⇠0.3B parame-
                    (Merity et al., 2017), we apply variational dropout              ters for 400K steps. As shown in Table 7, using
                    and weight average to Transformer-XL. With                       segment-level recurrence substantially improves
                    proper regularization, Transformer-XL achieves a                 performance even when long-term dependency is
                    new SoTA result among models without two-step                    not needed, which is consistent with our previous
                    ﬁnetuning. Penn Treebank has only 1M training                    discussion that the recurrence mechanism resolves
                    tokens, which implies that Transformer-XL also                   the context fragmentation problem. Moreover, our
                    generalizes well even on small datasets.                         relative positional encodings is also superior to
                    4.2    Ablation Study                                            Shawetal. (2018) on short sequences.
                    Weconduct two sets of ablation studies to exam-                  4.3    Relative Effective Context Length
                    ine the effects of two proposed techniques used in
                    Transformer-XL: the recurrence mechanism and                     Khandelwal et al. (2018) proposed a method to
                    the new positional encoding scheme.                              evaluate the Effective Context Length (ECL) of a
                       The ﬁrst study is performed on WikiText-103,                  sequence model.         ECL is the longest length to
                    which requires modeling long-term dependency.                    which increasing the context span would lead to
                    The results are reported in Table 6. Among the                   a gain more than a threshold. However, ECL ig-
                    comparedencodingschemes,Shawetal.(2018)is                        nores the fact that it is harder to get improve-
                    relative, while Vaswani et al. (2017) and Al-Rfou                ment when a model already achieves a lower per-
                                                                                2984
                            Remark                      Recurrence           Encoding           Loss    PPLinit     PPLbest      Attn Len
                            Transformer-XL (128M)            3                  Ours            Full      27.02       26.77         500
                            -                                3           Shawetal. (2018)       Full      27.94       27.94         256
                            -                                3                  Ours            Half      28.69       28.33         460
                            -                                7                  Ours            Full      29.59       29.02         260
                            -                                7                  Ours            Half      30.10       30.10         120
                            -                                7           Shawetal. (2018)       Full      29.75       29.75         120
                            -                                7           Shawetal. (2018)       Half      30.50       30.50         120
                            -                                7         Vaswani et al. (2017)    Half      30.97       30.97         120
                                                †
                            Transformer (128M)               7          Al-Rfou et al. (2018)   Half      31.16       31.16         120
                                                                                                                      23.09         640
                            Transformer-XL (151M)            3                  Ours            Full      23.43       23.16         450
                                                                                                                      23.35         300
                    Table 6: Ablation study on WikiText-103. For the ﬁrst two blocks, we use a slightly smaller model (128M parame-
                    ters). † indicates that the corresponding row is reduced to the same setting as the Transformer network in (Al-Rfou
                    et al., 2018), except that two auxiliary losses are not implemented in our experiments. “PPL init” refers to using
                    the same length as training. “PPL best” indicates the perplexity obtained by using the optimal length. “Attn Len”
                    is the shortest possible attention length during evaluation to achieve the corresponding result (PPL best). Increas-
                    ing the attention length during evaluation improves performance only when our positional encoding is used. The
                    “Transformer-XL (151M)” setting uses a standard parameter budget as previous work (Merity et al., 2018), where
                    weobserveasimilar effect when increasing the attention length during evaluation.
                              Method                               PPL                son. RECL also has a parameter r, which means
                              Ours                                 25.2               constraining the comparison on top-r hard exam-
                              With Shaw et al. (2018) encodings    25.7               ples. See AppedixCformoredetailsaboutRECL.
                              Without recurrence                   27.1               As shown in Table 8, Transformer-XL manages
                    Table 7: Ablation study on One Billion Word, a dataset            to model dependency of 900 words long on av-
                    without long-term dependency.                                     erage with r =0.1. The RECL of Transformer-
                                                                                      XL is 80% and 450% longer than recurrent net-
                     Model                             r =0.1 r =0.5 r =1.0           works and Transformer respectively. Both the re-
                     Transformer-XL 151M                  900      800      700       currence mechanism and our positional encodings
                     QRNN                                 500      400      300       contribute to a longer RECL. This further substan-
                     LSTM                                 400      300      200       tiates our argument that Transformer-XL is able to
                     Transformer-XL 128M                  700      600      500       modellonger-term dependency.
                     - use Shaw et al. (2018) encoding    400      400      300
                     - remove recurrence                  300      300      300       4.4    Generated Text
                     Transformer                          128      128      128       Trained only on WikiText-103 which is medium-
                    Table8: Relativeeffectivecontextlength(RECL)com-                  sized, Transformer-XL is already able to generate
                    parison. See text for the deﬁnition of RECL and r. The            relatively coherent articles with thousands of to-
                    ﬁrst three models and the last four models are com-               kens without manual cherry picking, despite mi-
                    pared as two model groups when we calculate RECL                  nor ﬂaws. Please refer to Appendix E for samples.
                    (RECLiscomputedonamodelgroupratherthanasin-
                    glemodel). Eachgrouphasthesameparameterbudget.                    4.5    Evaluation Speed
                                                                                      Finally, we compare the evaluation speed of our
                    plexity using only a shorter context, and thus it                 model with the vanilla Transformer model (Al-
                    is not suitable for fair comparison among mul-                    Rfou et al., 2018). As shown in Table 9, due to
                    tiple models. We instead propose a new metric                     the state reuse scheme, Transformer-XL achieves
                    called Relative Effective Context Length (RECL).                  an up to 1,874 times speedup during evaluation.
                    RECL is deﬁned on a model group instead of a                      5    Conclusions
                    single model, and the gain of a long context is
                    measurebytherelativeimprovementoverthebest                        Transformer-XL obtains strong perplexity results,
                    short context model. As such, the model group                     models longer-term dependency than RNNs and
                    shares the same baseline to enable fair compari-                  Transformer, achieves substantial speedup during
                                                                                2985
                      Attn Len    HowmuchAl-Rfouetal.(2018)isslower               Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
                       3,800                       1,874x                           2016. Hierarchical multiscale recurrent neural net-
                       2,800                       1,409x                           works. arXiv preprint arXiv:1609.01704.
                       1,800                        773x                                                                               ˘
                        800                         363x                          TimCooijmans, Nicolas Ballas, César Laurent, Çaglar
                                                                                    Gülçehre,    and Aaron Courville. 2016.            Re-
                   Table 9: Slowdown in terms of running time during                current batch normalization.           arXiv preprint
                   evaluation. Evaluation is based on per-token time on             arXiv:1603.09025.
                   one GPU.                                                       AndrewMDaiandQuocVLe.2015. Semi-supervised
                                                                                    sequence learning. In Advances in neural informa-
                                                                                    tion processing systems, pages 3079–3087.
                   evaluation, and is able to generate coherent text              Yann N Dauphin, Angela Fan, Michael Auli, and
                   articles.   We envision interesting applications of              David Grangier. 2016.       Language modeling with
                   Transformer-XL in the ﬁelds of text generation,                  gated convolutional networks.          arXiv preprint
                   unsupervised feature learning, image and speech                  arXiv:1612.08083.
                   modeling.                                                      Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
                   Acknowledgments                                                  KristinaToutanova.2018. Bert: Pre-trainingofdeep
                                                                                    bidirectional transformers for language understand-
                   ZD and YY were supported in part by National                     ing. arXiv preprint arXiv:1810.04805.
                   Science Foundation (NSF) under the grant IIS-                  Adji B Dieng, Chong Wang, Jianfeng Gao, and John
                   1546329 and by the DOE-Ofﬁce of Science un-                      Paisley. 2016.    Topicrnn: A recurrent neural net-
                   der the grant ASCR #KJ040201.               ZY and RS            work with long-range semantic dependency. arXiv
                   were supported in part by the Ofﬁce of Naval                     preprint arXiv:1611.01702.
                   Research grant N000141812861, the NSF grant                    Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
                   IIS1763562, the Nvidia fellowship, and the Siebel                ically grounded application of dropout in recurrent
                   scholarship.                                                     neural networks. In Advances in neural information
                                                                                    processing systems, pages 1019–1027.
                   References                                                     Edouard Grave, Armand Joulin, Moustapha Cissé,
                                                                                    David Grangier, and Hervé Jégou. 2016a. Efﬁcient
                   Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy                  softmax approximation for gpus.         arXiv preprint
                      Guo, and Llion Jones. 2018. Character-level lan-              arXiv:1609.04309.
                      guage modeling with deeper self-attention.       arXiv      Edouard    Grave,    Armand Joulin,        and   Nicolas
                      preprint arXiv:1808.04444.                                    Usunier. 2016b.         Improving neural language
                   Alexei Baevski and Michael Auli. 2018. Adaptive in-              models with a continuous cache.         arXiv preprint
                      put representations for neural language modeling.             arXiv:1612.04426.
                      arXiv preprint arXiv:1809.10853.                            Alex Graves. 2013.         Generating sequences with
                                                                                    recurrent   neural   networks.        arXiv    preprint
                   DzmitryBahdanau,KyunghyunCho,andYoshuaBen-                       arXiv:1308.0850.
                      gio. 2014.    Neural machine translation by jointly
                      learning to align and translate.       arXiv preprint       Alex Graves, Greg Wayne, and Ivo Danihelka.
                      arXiv:1409.0473.                                              2014.     Neural turing machines.      arXiv preprint
                                                                                    arXiv:1410.5401.
                   Shaojie Bai, J Zico Kolter, and Vladlen Koltun.                David Ha, Andrew Dai, and Quoc V Le. 2016. Hyper-
                      2018. An empirical evaluation of generic convolu-             networks. arXiv preprint arXiv:1609.09106.
                      tional and recurrent networks for sequence model-
                      ing. arXiv preprint arXiv:1803.01271.                       Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jür-
                   YoshuaBengio,RéjeanDucharme,PascalVincent,and                    gen Schmidhuber, et al. 2001. Gradient ﬂow in re-
                      Christian Jauvin. 2003. A neural probabilistic lan-           current nets: the difﬁculty of learning long-term de-
                      guagemodel. Journalofmachinelearningresearch,                 pendencies.
                      3(Feb):1137–1155.                                           Sepp Hochreiter and Jürgen Schmidhuber. 1997.
                                                                                    Long short-term memory.          Neural computation,
                   CiprianChelba,TomasMikolov,MikeSchuster,QiGe,                    9(8):1735–1780.
                      Thorsten Brants, Phillipp Koehn, and Tony Robin-
                      son. 2013. One billion word benchmark for measur-           Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob
                      ing progress in statistical language modeling. arXiv          Uszkoreit, Noam Shazeer, Curtis Hawthorne, An-
                      preprint arXiv:1312.3005.                                     drewMDai,MatthewDHoffman,andDouglasEck.
                                                                            2986
                     2018. An improved relative self-attention mecha-      Stephen Merity, Nitish Shirish Keskar, and Richard
                     nism for transformer with application to music gen-      Socher.2017. Regularizingandoptimizinglstmlan-
                     eration. arXiv preprint arXiv:1809.04281.                guage models. arXiv preprint arXiv:1708.02182.
                  HakanInan, Khashayar Khosravi, and Richard Socher.       Stephen Merity, Nitish Shirish Keskar, and Richard
                     2016.   Tying word vectors and word classiﬁers:          Socher. 2018.    An analysis of neural language
                     A loss framework for language modeling.     arXiv        modeling at multiple scales.       arXiv preprint
                     preprint arXiv:1611.01462.                               arXiv:1803.08240.
                  YangfengJi,TrevorCohn,LingpengKong,ChrisDyer,            Stephen Merity, Caiming Xiong, James Bradbury, and
                     and Jacob Eisenstein. 2015. Document context lan-        Richard Socher. 2016.    Pointer sentinel mixture
                     guage models. arXiv preprint arXiv:1511.03962.           models. arXiv preprint arXiv:1609.07843.
                  Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam     Tomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan
                                                                              ˇ       `
                     Shazeer, and Yonghui Wu. 2016.          Exploring        Cernocky, and Sanjeev Khudanpur. 2010. Recur-
                     the limits of language modeling.   arXiv preprint        rent neural network based language model.       In
                     arXiv:1602.02410.                                        Eleventh Annual Conference of the International
                                                                              Speech Communication Association.
                  Nan Rosemary Ke, Anirudh Goyal ALIAS PARTH               Tomas Mikolov and Geoffrey Zweig. 2012. Context
                     GOYAL,     Olexa    Bilaniuk,   Jonathan    Binas,       dependentrecurrentneuralnetworklanguagemodel.
                     Michael C Mozer, Chris Pal, and Yoshua Ben-              SLT, 12(234-239):8.
                     gio. 2018. Sparse attentive backtracking: Temporal
                     credit assignment through reminding. In Advances      Asier Mujika, Florian Meier, and Angelika Steger.
                     in Neural Information Processing Systems, pages          2017. Fast-slow recurrent neural networks. In Ad-
                     7650–7661.                                               vances in Neural Information Processing Systems,
                  Urvashi Khandelwal, He He, Peng Qi, and Dan Ju-             pages 5915–5924.
                     rafsky. 2018. Sharp nearby, fuzzy far away: How       Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
                     neural language models use context. arXiv preprint       Gardner, Christopher Clark, Kenton Lee, and Luke
                     arXiv:1805.04623.                                        Zettlemoyer. 2018. Deep contextualized word rep-
                                                                              resentations. arXiv preprint arXiv:1802.05365.
                  Bryon Knol. 2017.      cmix v13.     http://www.
                     byronknoll.com/cmix.html.                             Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V
                                                                              Le, and Jeff Dean. 2018. Efﬁcient neural architec-
                  Ben Krause, Liang Lu, Iain Murray, and Steve Renals.        ture search via parameter sharing.  arXiv preprint
                     2016. Multiplicative lstm for sequence modelling.        arXiv:1802.03268.
                     arXiv preprint arXiv:1609.07959.                      AlecRadford,KarthikNarasimhan,TimSalimans,and
                  Oleksii Kuchaiev and Boris Ginsburg. 2017. Factor-          Ilya Sutskever. 2018.  Improving language under-
                     ization tricks for lstm networks.  arXiv preprint        standing by generative pre-training. URL https://s3-
                     arXiv:1703.10722.                                        us-west-2.amazonaws.com/openai-assets/research-
                                                                              covers/languageunsupervised/language       under-
                  Quoc V Le, Navdeep Jaitly, and Geoffrey E Hin-              standing paper. pdf.
                     ton. 2015.   A simple way to initialize recurrent     Jack W Rae, Chris Dyer, Peter Dayan, and Tim-
                     networks of rectiﬁed linear units.  arXiv preprint       othy P Lillicrap. 2018.    Fast parametric learn-
                     arXiv:1504.00941.                                        ing with activation memorization.   arXiv preprint
                  Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo         arXiv:1803.10049.
                     Gao. 2018. Independently recurrent neural network     Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
                     (indrnn): Building a longer and deeper rnn. In Pro-      2018. Self-attention with relative position represen-
                     ceedings of the IEEE Conference on Computer Vi-          tations. arXiv preprint arXiv:1803.02155.
                     sion and Pattern Recognition, pages 5457–5466.
                                                                           Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin
                  Hanxiao Liu, Karen Simonyan, and Yiming Yang.               Tran, Ashish Vaswani, Penporn Koanantakool, Peter
                     2018.   Darts:  Differentiable architecture search.      Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff
                     arXiv preprint arXiv:1806.09055.                         Young,etal. 2018. Mesh-tensorﬂow: Deep learning
                                                                              for supercomputers. In Advances in Neural Infor-
                  MultiMedia LLC. 2009.        Large text compression         mation Processing Systems, pages 10434–10443.
                     benchmark.
                                                                           NoamShazeer, Azalia Mirhoseini, Krzysztof Maziarz,
                                                                 ˇ   `
                  Gábor Melis, Charles Blundell, Tomáš Kocisky,               Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
                     Karl Moritz Hermann, Chris Dyer, and Phil Blun-          Dean. 2017. Outrageously large neural networks:
                     som. 2018. Pushing the bounds of dropout. arXiv          The sparsely-gated mixture-of-experts layer. arXiv
                     preprint arXiv:1805.09208.                               preprint arXiv:1701.06538.
                                                                      2987
                  Noam Shazeer, Joris Pelemans, and Ciprian Chelba.
                     2014. Skip-gram language modeling using sparse
                     non-negative matrix probability estimation.  arXiv
                     preprint arXiv:1412.1454.
                  Trieu H Trinh, Andrew M Dai, Thang Luong, and
                     Quoc V Le. 2018. Learning longer-term dependen-
                     cies in rnns with auxiliary losses. arXiv preprint
                     arXiv:1803.00144.
                  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
                     Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
                     Kaiser, and Illia Polosukhin. 2017. Attention is all
                     you need. In Advances in Neural Information Pro-
                     cessing Systems, pages 5998–6008.
                  Tian Wang and Kyunghyun Cho. 2015.            Larger-
                     context language modelling.        arXiv preprint
                     arXiv:1511.03729.
                  Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen,
                     Jiaji Huang, Wei Ping, Sanjeev Satheesh, and
                     Lawrence Carin. 2017. Topic compositional neural
                     language model. arXiv preprint arXiv:1712.09783.
                  Jason Weston, Sumit Chopra, and Antoine Bor-
                     des. 2014.   Memory networks.       arXiv preprint
                     arXiv:1410.3916.
                  Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua
                     Bengio, and Ruslan R Salakhutdinov. 2016.      On
                     multiplicative integration with recurrent neural net-
                     works. In Advances in neural information process-
                     ing systems, pages 2856–2864.
                  Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and
                     William WCohen.2017. Breakingthesoftmaxbot-
                     tleneck: A high-rank rnn language model. arXiv
                     preprint arXiv:1711.03953.
                  Julian  Georg Zilly,    Rupesh Kumar Srivastava,
                     Jan Koutník, and Jürgen Schmidhuber. 2016.
                     Recurrent highway networks.        arXiv preprint
                     arXiv:1607.03474.
                  Barret Zoph and Quoc V Le. 2016. Neural architecture
                     search with reinforcement learning. arXiv preprint
                     arXiv:1611.01578.
                                                                       2988
