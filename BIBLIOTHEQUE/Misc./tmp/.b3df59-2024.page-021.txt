                               TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
           F. BIG-Bench Hard Details
           F.1. Further Experimental Details
           Wewriteourownevaluationfunction for BIG-Bench Hard available in our codebase. We found that existing evaluation
           frameworksdidnotproperlymeasurezero-shotperformanceduetoinsufficientanswer-extractionparsingandanswer-format
           prompting. We also wanted more control in splitting each individual task’s dataset into demonstration examples and
           evaluation sets. For all results, we average results over different selections of the 10 few-shot examples with the following 5
           randomseeds: 42,43,44,45,46. The full TTT and inference process takes approximately 15 minutes on an NVIDIA A100
           GPU.
           Thestandard error of the mean for each method in Figure 8 over the 5 seeds is given in Table 7.
                                    Table 7. Standard Error of the Mean for each method in Figure 8.
                                    Method                  StandardErroroftheMean
                                    Zero-Shot                         0.01
                                    ICL                               0.19
                                    TTT                               0.20
                                    NoExamplePermutation              0.32
                                    E2E                               0.66
                                    Shared TTT                        0.72
                                    NoDemoLoss                        0.69
                                    Loss on Inputs and Outputs        0.35
           Wesearchoverthefollowing hyperparameters:
                                          Table 8. BBH TTT Fine-tuning Hyperparameters
                                          Hyperparameter   Search Space
                                          learning rate    [1e-5, 5e-5, 1e-4, 3e-4]
                                          r LoRArank       [64, 128]
                                          αLoRAalpha       [16, 32, 64, 128]
                                          epochs           1
                                          batch size       5
                                          training steps   [20, 40, 60]
                                          optimizer        AdamW
                                          scheduler        Cosine LR Schedule
           We similarly use the torchtune(torchtune Maintainers & Contributors, 2024) library for test-time training and the
           vLLM(Kwonetal., 2023) library for inference.
           F.2. Task-specific Results
           Thefull results for all tasks over all methods and ablations are shown in Figure 17.
                                                           21
