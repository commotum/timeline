                        Published as a conference paper at ICLR 2015
                        The most important distinguishing feature of this approach from the basic encoder–decoder is that
                        it does not attempt to encode a whole input sentence into a single ﬁxed-length vector. Instead, it en-
                        codes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively
                        while decoding the translation. This frees a neural translation model from having to squash all the
                        information of a source sentence, regardless of its length, into a ﬁxed-length vector. We show this
                        allows a model to cope better with long sentences.
                        In this paper, we show that the proposed approach of jointly learning to align and translate achieves
                        signiﬁcantly improved translation performance over the basic encoder–decoder approach. The im-
                        provement is more apparent with longer sentences, but can be observed with sentences of any
                        length. On the task of English-to-French translation, the proposed approach achieves, with a single
                        model, a translation performance comparable, or close, to the conventional phrase-based system.
                        Furthermore, qualitative analysis reveals that the proposed model ﬁnds a linguistically plausible
                        (soft-)alignment between a source sentence and the corresponding target sentence.
                        2   BACKGROUND: NEURAL MACHINE TRANSLATION
                        From a probabilistic perspective, translation is equivalent to ﬁnding a target sentence y that max-
                        imizes the conditional probability of y given a source sentence x, i.e., argmax p(y | x). In
                                                                                                  y
                        neural machine translation, we ﬁt a parameterized model to maximize the conditional probability
                        of sentence pairs using a parallel training corpus. Once the conditional distribution is learned by a
                        translation model, given a source sentence a corresponding translation can be generated by searching
                        for the sentence that maximizes the conditional probability.
                        Recently, a number of papers have proposed the use of neural networks to directly learn this condi-
                        tional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al.,
                                                           ˜
                        2014; Cho et al., 2014b; Forcada and Neco, 1997). This neural machine translation approach typ-
                        ically consists of two components, the ﬁrst of which encodes a source sentence x and the second
                        decodes to a target sentence y. For instance, two recurrent neural networks (RNN) were used by
                        (Cho et al., 2014a) and (Sutskever et al., 2014) to encode a variable-length source sentence into a
                        ﬁxed-length vector and to decode the vector into a variable-length target sentence.
                        Despitebeingaquitenewapproach,neuralmachinetranslationhasalreadyshownpromisingresults.
                        Sutskever et al. (2014) reported that the neural machine translation based on RNNs with long short-
                        term memory (LSTM) units achieves close to the state-of-the-art performance of the conventional
                                                                                                 1
                        phrase-based machine translation system on an English-to-French translation task. Adding neural
                        components to existing translation systems, for instance, to score the phrase pairs in the phrase
                        table (Cho et al., 2014a) or to re-rank candidate translations (Sutskever et al., 2014), has allowed to
                        surpass the previous state-of-the-art performance level.
                        2.1   RNNENCODER–DECODER
                        Here, we describe brieﬂy the underlying framework, called RNN Encoder–Decoder, proposed by
                        Choet al. (2014a) and Sutskever et al. (2014) upon which we build a novel architecture that learns
                        to align and translate simultaneously.
                        In the Encoder–Decoder framework, an encoder reads the input sentence, a sequence of vectors
                        x=(x ,··· ,x ),intoavectorc.2 The most common approach is to use an RNN such that
                               1      Tx
                                                            h =f(x ,h      )                                 (1)
                        and                                  t       t  t−1
                                                         c = q({h ,··· ,h   }),
                                                                  1      Tx
                        where ht ∈ Rn is a hidden state at time t, and c is a vector generated from the sequence of the
                        hidden states. f and q are some nonlinear functions. Sutskever et al. (2014) used an LSTM as f and
                        q({h ,··· ,h }) = h , for instance.
                             1      T       T
                           1 We mean by the state-of-the-art performance, the performance of the conventional phrase-based system
                        without using any neural network-based component.
                           2 Althoughmostofthepreviousworks(see,e.g.,Choetal.,2014a;Sutskeveretal.,2014;Kalchbrennerand
                        Blunsom, 2013) used to encode a variable-length input sentence into a ﬁxed-length vector, it is not necessary,
                        and even it may be beneﬁcial to have a variable-length vector, as we will show later.
                                                                   2
