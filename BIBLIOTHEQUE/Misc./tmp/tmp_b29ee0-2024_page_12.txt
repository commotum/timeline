                                    12       S. He et al.
                                    Table 3: 3D referring segmentation benchmark results on ScanRefer [4],
                                    Nr3D [1], and Multi3Drefer [79] evaluated by mIoU. ∗ represents adding an auxil-
                                    iary mask head through our implementation.
                                       Stage     Method                   Reference       ScanRefer     Nr3D      Multi3DRefer
                                        Two      M3DRef-CLIP [79]         [ICCV’23]         35.7         27.0         32.6
                                        Two      3D-STMN [63]             [AAAI’24]         39.5          -             -
                                       Single    TGNN[22]                 [AAAI’21]         27.8          -             -
                                                                  ∗
                                       Single    BUTD-DETR[23]            [ECCV’22]         35.4         27.5         26.2
                                                          ∗
                                       Single    EDA[67]                  [CVPR’23]         36.2         29.3         28.9
                                       Single    X-RefSeg3D [49]          [AAAI’24]         29.9          -             -
                                       Single    RefMask3D [18]         [ACMMM’24]          44.8          -             -
                                       Single    SegPoint                 [ECCV’24]         41.7        32.2          36.1
                                    Table 4: 3D open-vocabulary semantic segmentation benchmark results on
                                    val split of ScanNet++ [74].
                                               Type               Method                    Reference          ScanNet++
                                                                  PointNet [45]            [CVPR’17]               7.0
                                             Supervised           PointNet++ [46]          [NeurIPS’17]            15.0
                                                                  MinkUNet [6]             [CVPR’19]               28.0
                                                                  KPConv [58]               [ICCV’19]              30.0
                                          Open-Vocabulary         OpenScene [42]           [CVPR’23]               12.8
                                                                  PLA[12]                  [CVPR’23]               14.2
                                                                  RegionPLC [71]           [CVPR’24]               14.9
                                              Unified             SegPoint                 [ECCV’24]              19.3
                                    4.6    Results on Open-vocabulary Semantic Segmentation
                                    Table 4 shows our method’s open-vocabulary segmentation performance which is
                                    directly evaluated on ScanNet++ [74] following the setting in prevalent method-
                                    ologies in the 2D domain [13,69]. It demonstrates our superiority over both exist-
                                    ing open-vocabulary techniques and even several supervised approaches, showing
                                    our model’s robust generalization capabilities. It effectively aligns and interprets
                                    categories with visual scenes, underscoring the reasoning prowess of large lan-
                                    guage models. A notable issue is the potential misalignment between output
                                    categories of SegPoint and val split category names. To address this, we employ
                                    GPT-4 to match its most similar category names in the val split.
                                    4.7    Ablation Study
                                    We conduct extensive experiments to verify the effectiveness of our proposed
                                    components in Table 5 (a) on both Instruct3D and ScanRefer [4] dataset. We
                                    established a vanilla baseline as described in Sec. 3.2 following the paradigm
                                    of LISA [28], which cannot achieve satisfactory performance and only obtain
                                    16.1%/30.3 mIoU on Instruct3D/ScanRefer, respectively. Further analysis, both
                                    qualitative and quantitative, of our proposed components reveals that their in-
                                    tegration substantially outperforms the baseline.
                                    Geometric Enhancer Module Integrating the Geometric Enhancer Module
                                    (GEM) into our point encoder results in a notable 5.3%/5.5% mIoU improve-
                                    ment on Instruct3D/ScanRefer, effectively addressing compatibility issues with
                                    dense prediction tasks. An ablation study, shown in Table 5 (b), shows that
                                    our improvement is not due to an increase in parameters. Our approach out-
                                    performs traditional full fine-tuning, LoRA [21] strategies, and the addition of
