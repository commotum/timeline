                                        Published as a conference paper at ICLR 2021
                                                                   ÀÜ
                                        In turn, the value (w) that solves this approximation is given by the solution to a classical dual
                                        normproblem(|¬∑|q‚àí1 denotes elementwise absolute value and power)2:
                                                                                                                                                         
                                                                                                                                                            1/p
                                                              ÀÜ                                                           q‚àí1                           q
                                                              (w) = œÅ sign(‚àá L (w))|‚àá L (w)|                                   / k‚àá L (w)k                                        (2)
                                                                                        w S                 w S                          w S            q
                                        where 1/p+1/q = 1. Substituting back into equation (1) and differentiating, we then have
                                                                                                                                ÀÜ
                                                                  SAM                                  ÀÜ             d(w+(w))
                                                          ‚àá L            (w) ‚âà ‚àá L (w+(w))=                                              ‚àá L (w)|
                                                             w                         w S                                                   w S                ÀÜ
                                                                  S                                                          dw                            w+(w)
                                                                                                                       ÀÜ
                                                                                =‚àá L (w)|                       +d(w)‚àá L (w)|                             .
                                                                                       w S               ÀÜ                       w S                ÀÜ
                                                                                                     w+(w)            dw                       w+(w)
                                                                                 SAM
                                        This approximation to ‚àá L                       (w)canbestraightforwardly computed via automatic differentia-
                                                                            w S
                                        tion, as implemented in commonlibrariessuchasJAX,TensorFlow,andPyTorch. Thoughthiscom-
                                                                                                                                ÀÜ
                                        putationimplicitlydependsontheHessianofL (w)because(w)isitselfafunctionof‚àá L (w),
                                                                                                           S                                                             w S
                                        the Hessian enters only via Hessian-vector products, which can be computed tractably without ma-
                                        terializing the Hessian matrix. Nonetheless, to further accelerate the computation, we drop the
                                        second-order terms. obtaining our Ô¨Ånal gradient approximation:
                                                                                    ‚àá LSAM(w)‚âà‚àá L (w)|                                    .                                        (3)
                                                                                       w                         w S                ÀÜ
                                                                                            S                                  w+(w)
                                        AsshownbytheresultsinSection3, this approximation (without the second-order terms) yields an
                                        effective algorithm. In Appendix C.4, we additionally investigate the effect of instead including the
                                        second-order terms; in that initial experiment, including them surprisingly degrades performance,
                                        and further investigating these terms‚Äô effect should be a priority in future work.
                                        Weobtain the Ô¨Ånal SAM algorithm by applying a standard numerical optimizer such as stochastic
                                        gradient descent (SGD) to the SAM objective LSAM(w), using equation 3 to compute the requisite
                                                                                                             S
                                        objective function gradients. Algorithm 1 gives pseudo-code for the full SAM algorithm, using SGD
                                        as the base optimizer, and Figure 2 schematically illustrates a single SAM parameter update.
                                                                           n
                                        Input: Training set S , ‚à™              {(xi,yi)}, Loss function
                                                                           i=1
                                                  l : W √óX √óY ‚ÜíR+,Batchsizeb,StepsizeŒ∑ > 0,
                                                  Neighborhood size œÅ > 0.
                                        Output: Model trained with SAM                                                                                             w
                                                                                                                                                         L(w )       t+1
                                        Initialize weights w , t = 0;                                                                                        t
                                                                  0
                                        while not converged do                                                                                        wt                    wSAM
                                                                                                                                           L(w )                              t+1
                                                                                                                                 || L(w )||     t
                                             Samplebatch B = {(x1,y1),...(xb,yb)};                                                    t 2
                                             Computegradient ‚àá L (w)ofthebatch‚Äôstraining loss;
                                                          ÀÜ             w B                                                      wadv                            L(wadv)
                                             Compute(w)perequation2;
                                             Computegradient approximation for the SAM objective
                                               (equation 3): g = ‚àá L (w)|                        ;
                                                                          w B              ÀÜ
                                                                                       w+(w)
                                             Update weights: w              =w ‚àíŒ∑g;
                                                                      t+1         t
                                             t = t + 1;                                                                       Figure2: SchematicoftheSAMparam-
                                        end                                                                                   eter update.
                                        return wt         Algorithm 1: SAM algorithm
                                        3      EMPIRICAL EVALUATION
                                        In order to assess SAM‚Äôs efÔ¨Åcacy, we apply it to a range of different tasks, including image clas-
                                        siÔ¨Åcation from scratch (including on CIFAR-10, CIFAR-100, and ImageNet), Ô¨Ånetuning pretrained
                                        models, and learning with noisy labels. In all cases, we measure the beneÔ¨Åt of using SAM by simply
                                        replacing the optimization procedure used to train existing models with SAM, and computing the
                                        resulting effect on model generalization. As seen below, SAM materially improves generalization
                                        performance in the vast majority of these cases.
                                            2In the case of interest p = 2, this boils down to simply rescaling the gradient such that its norm is œÅ.
                                                                                                               4
