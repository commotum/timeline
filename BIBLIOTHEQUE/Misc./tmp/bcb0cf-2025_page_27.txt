                     Published as a conference paper at ICLR 2025
                     Our LSTM-based MIND model consists of three LSTM layers. The LSTM layers are parameterized as
                     follows:
                                                       f =σ(W ·[h       , x ] + b )
                                                        t       f    t−1  t     f
                                                       i =σ(W ·[h       , x ] + b )
                                                        t       i    t−1  t    i
                                                       o =σ(W ·[h       , x ] + b )
                                                        t       o    t−1  t    o                                   (21)
                                                       ˜
                                                      C =tanh(W ·[h         , x ] + b )
                                                        t          C     t−1  t    C
                                                                            ˜
                                                      C =f ∗C        +i ∗C
                                                        t    t    t−1   t    t
                                                       h =o ∗tanh(C )
                                                        t    t         t
                     In this equation, f ,i ,o are the forget, input, and output gates, respectively. h is the hidden state, C is the
                                      t t  t                                                t                   t
                     cell state, and x is the input at time t.
                                    t
                     During our experiments on checking the frequency at which layers are selected, remarkably, the “No Layer”
                     or Straightforward option emerges as the most frequently selected, indicating its ability to capture essential
                     features across a broad spectrum of tasks and that the samples becomes easier once they are learned so the
                     layers don’t have to spend a lot of time to process them. The diminishing frequency of Layer3 selections
                     implies that the model tends to minimize reliance on this layer’s fixed-point operations as it stabilizes.
                     Figure 10 elucidates the frequency distribution of each layer’s selection during the evaluation process.
                     Figure 10: Frequency distribution of layer choices during CIFAR10 training. The bar chart quantifies
                     the selection frequency of each layer across numerous training epochs, offering insights into their relative
                     importance for CIFAR10 performance.
                     WeemployFixed-Point Iteration (FPI) on individual LSTM layers just as in our CNN experiments. The
                     results are summarized in Figure 11, which shows that the LSTM model with FPI on the third layer achieves
                     the best performance in terms of both accuracy and computational cost.
                     Throughtheseexperiments,wedemonstratethatMINDmodel’sadaptivelayerselectionmechanismisequally
                     effective for text-based tasks, thereby confirming its versatility across different data modalities and tasks.
                     F ABLATIONSTUDIES
                     F.1   ABLATION ON MIND-TRANSFORMERS
                     We conducted ablation studies on the MIND-Transformer model to investigate the impact of different
                     componentsonperformance. Eachpartofthearchitecture was evaluated separately by removing key modules
                     suchastheattention-basedFixed-PointIteration (FPI), feed-forward network (FFN) FPI, and the introspection
                     mechanism. Theseablations were performed on the WikiText-103 dataset (Merity et al., 2016), and the results
                     are summarized in Table 9. Perplexity, parameter count, and FLOPs are reported to measure the impact of
                     each ablation on the model’s efficiency and performance.
                                                                 27
