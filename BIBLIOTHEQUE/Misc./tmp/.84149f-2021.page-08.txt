                Table 5: Comparisons to state-of-the-art across multiple datasets. For “views”, x × y denotes x temporal crops and y spatial crops. We
                report the TFLOPs to process all spatio-temporal views. “FE” denotes our Factorised Encoder model.
                                       (a) Kinetics 400                                       (b) Kinetics 600                   (d) Epic Kitchens 100 Top 1 accuracy
                  Method                   Top1    Top5    Views   TFLOPs          Method                  Top1   Top5          Method            Action  Verb   Noun
                  blVNet [18]               73.5   91.2      –         –           AttentionNAS [75]       79.8    94.4         TSN[71]            33.2   60.2   46.0
                  STM[32]                   73.7   91.6      –         –           LGD-3DR101[50]          81.5    95.6         TRN[85]            35.3   65.9   45.4
                  TEA[41]                   76.1   92.5   10×3       2.10          SlowFast R101-NL [20]   81.8    95.1         TBN[35]            36.7   66.0   47.2
                  TSM-ResNeXt-101[42]       76.3     –       –         –           X3D-XL[19]              81.9    95.5         TSM[42]            38.3   67.9   49.0
                  I3DNL[74]                 77.7   93.3   10×3       10.77         TimeSformer-L [4]       82.2    95.6         SlowFast [20]      38.5   65.6   50.0
                  CorrNet-101 [69]          79.2     –    10×3       6.72          ViViT-L/16x2 FE         82.9    94.6         ViViT-L/16x2 FE    44.0   66.4   56.8
                  ip-CSN-152 [65]           79.2   93.8   10×3       3.27          ViViT-L/16x2 FE (JFT)   84.3    94.9
                  LGD-3DR101[50]            79.4   94.4      –         –           ViViT-H/16x2 (JFT)      85.8    96.5              (e) Something-Something v2
                  SlowFast R101-NL [20]     79.8   93.9   10×3       7.02                  (c) Moments in Time                   Method                Top1     Top5
                  X3D-XXL[19]               80.4   94.6   10×3       5.82
                  TimeSformer-L [4]         80.7   94.7    1×3       7.14                                 Top1     Top5          TRN[85]                48.8     77.6
                  ViViT-L/16x2 FE           80.6   92.7    1×1       3.98          TSN[71]                 25.3     50.1         SlowFast [19, 79]      61.7      –
                  ViViT-L/16x2 FE           81.7   93.8    1×3       11.94         TRN[85]                 28.3     53.4         TimeSformer-HR[4]      62.5      –
                  Methods with large-scale pretraining                             I3D[8]                  29.5     56.1         TSM[42]                63.4     88.5
                                                                                   blVNet [18]             31.4     59.3         STM[32]                64.2     89.8
                  ip-CSN-152 [65] (IG [43]) 82.5   95.3   10×3       3.27          AssembleNet-101 [53]    34.3     62.7         TEA[41]                65.1      –
                  ViViT-L/16x2 FE (JFT)     83.5   94.3    1×3       11.94                                                       blVNet [18]            65.2     90.3
                  ViViT-H/16x2 (JFT)        84.9   95.8    4×3       47.77         ViViT-L/16x2 FE         38.5     64.1         ViVIT-L/16x2 FE        65.9     89.9
                achieves the highest accuracy.                                                  optical ﬂow as an additional input modality [42, 49]. Fur-
                4.3. Comparison to state-of-the-art                                             thermore, all variants of our model presented in Tab. 2 out-
                                                                                                performed the existing state-of-the-art on action accuracy.
                    Based on our ablation studies in the previous section,                      We note that we use the same model to predict verbs and
                wecompare to the current state-of-the-art using two of our                      nounsusingtwoseparate“heads”,andforsimplicity,wedo
                model variants. We primarily use our Factorised Encoder                         not use separate loss weights for each head.
                model(Model2),asitcanprocessmoretokensthanModel                                 Something-Something v2 (SSv2)                 Finally, Tab. 5e shows
                1 to achieve higher accuracy.                                                   that we achieve state-of-the-art Top-1 accuracy with our
                Kinetics      Tables 5a and 5b show that our spatio-temporal                    Factorised encoder model (Model 2), albeit with a smaller
                attention models outperform the state-of-the-art on Kinetics                    margin compared to previous methods. Notably, our Fac-
                400 and 600 respectively. Following standard practice, we                       torised encoder modelsigniﬁcantlyoutperformstheconcur-
                take 3 spatial crops (left, centre and right) [20, 19, 65, 74]                  rent TimeSformer[4]methodby2.9%,whichalsoproposes
                for each temporal view, and notably, we require signiﬁ-                         a pure-transformer model, but does not consider our Fac-
                cantly fewer views than previous CNN-based methods.                             torised encoder variant or our additional regularisation.
                    Wesurpass the previous CNN-based state-of-the-art us-                           SSv2differs from other datasets in that the backgrounds
                ing ViViT-L/16x2 Factorised Encoder (FE) pretrained on                          and objects are quite similar across different classes, mean-
                ImageNet, and also outperform [4] who concurrently pro-                         ing that recognising ﬁne-grained motion patterns is neces-
                posedapure-transformerarchitecture. Moreover, by initial-                       sary to distinguish classes from each other. Our results sug-
                ising our backbones from models pretrained on the larger                        gest that capturing these ﬁne-grained motions is an area of
                JFT dataset [57], we obtain further improvements.                    Al-        improvement and future work for our model. We also note
                though these models are not directly comparable to previ-                       an inverse correlation between the relative performance of
                ous work, we do also outperform [65] who pretrained on                          previous methods on SSv2 (Tab. 5e) and Kinetics (Tab. 5a)
                the large-scale, Instagram dataset [43]. Our best model uses                    suggesting that these two datasets evaluate complementary
                aViViT-HbackbonepretrainedonJFTandsigniﬁcantlyad-                               characteristics of a model.
                vances the best reported results on Kinetics 400 and 600 to
                84.9%and85.8%,respectively.                                                     5. Conclusion and Future Work
                Moments in Time We surpass the state-of-the-art by a
                signiﬁcant margin as shown in Tab. 5c. We note that the                             We have presented four pure-transformer models for
                videos in this dataset are diverse and contain signiﬁcant la-                   video classiﬁcation, with different accuracy and efﬁciency
                bel noise, making this task challenging and leading to lower                    proﬁles, achieving state-of-the-art results across ﬁve pop-
                accuracies than on other datasets.                                              ular datasets.      Furthermore, we have shown how to ef-
                Epic Kitchens 100           Table 5d shows that our Factorised                  fectively regularise such high-capacity models for training
                Encoder model outperforms previous methods by a signiﬁ-                         on smaller datasets and thoroughly ablated our main de-
                cant margin. In addition, our model obtains substantial im-                     sign choices. Future work is to remove our dependence on
                provements for Top-1 accuracy of “noun” classes, and the                        image-pretrained models, and to extend our model to more
                only method which achieves higher “verb” accuracy used                          complex video understanding tasks.
                                                                                             6843
