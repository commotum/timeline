                 test sets each contains 1,000 sequence samples with     32Kand16K,respectively, which corresponds to
                 length 256. The sequences in the training, valida-      a scaling factor s = 8 and s = 4 for the position
                 tion and test sets do not overlap in the first j + k    embeddings of the two models.
                 tokens. For all YaRN and RESONANCE YARN set-              For both the long-sequence perplexity evalua-
                 tings, we train the model with YaRN and RESO-           tion in Section 6.2.2 and real-world task evalua-
                 NANCE YARN applied to the model with a scal-            tions in Section 6.2.3, the hyperparameters for the
                 ing factor s = 4, which corresponds to the TSTL         LLMexperiments follow the configurations pro-
                                                                                                   2
                 setting of our evaluation. Each model is trained       videdinPengetal.(2024) ,withtheonlymodifica-
                 on each subtask for 150 epochs with a language          tion that we fine-tune the model on approximately
                 modeling-style cross-entropy loss. Training was        100M tokens. More specifically, we use α = 1
                 done with AdamWoptimizer (Loshchilov and Hut-           and β = 32 for YaRN and RESONANCE YARNas
                                                      −4
                 ter, 2019), using learning rate 2×10    and weight      suggested by Peng et al. (2024). The model was
                 decay 1 × 10−2. We use a batch size of 128 for          trained with a language modeling-style cross en-
                 all experiments. All hyperparameters were tuned         tropy loss. Training was done with the AdamW op-
                 to maximize YaRN’s validation set performance           timizer (Loshchilov and Hutter, 2019) using learn-
                 onthe Semi-Recurrent subtask. All synthetic task        ing rate 2 × 10−5 and weight decay 1 × 10−2. We
                 evaluations were performed on a single NVIDIA           use a batch size of 1 on each of the GPUs. The
                 V10032GGPU.                                             learning rate warm-up is applied to the first 5% of
                 C.2    LLMEvaluations                                   the total training steps. Models were fine-tuned
                                                                        with BF16 precision, FlashAttention 2 (Dao, 2023)
                 For the LLM-based evaluations in Section 6.2,           and DeepSpeed ZeRO-3 Offload (Ren et al., 2021)
                 wefine-tune LLaMA2-Chat 7B or LLaMA2-Chat               onfour NVIDIAA10040GGPUs.
                 13B(Touvronet al., 2023b) after replacing its orig-       For the real-world task evaluations in Sec-
                 inal RoPE position embedding with RoPE scaled           tion 6.2.3, we further compare two different fine-
                 with different strategies:                              tuning strategies:
                    • NTK-Aware Scaling (bloc97, 2023; Xiong               1. Fine-tuning on long sequences for less
                      et al., 2023; Liu et al., 2024), which scales the       epochs. We directly fine-tune the model on
                      base b in Equation 1 to s · b, where s is the           the target sequence lengths after applying the
                      scaling factor. We evaluate the performance             scaled position embeddings. For LLaMA2-
                      without fine-tuning as used in bloc97 (2023).           Chat 7B and 13B, we fine-tune the model on
                                                                              sequences with length 32,768 for 50 steps and
                    • Dynamic NTK-AwareScaling (Peng et al.,                  sequences with length 16,384 for 100 steps,
                      2024; Rozière et al., 2023). This method dy-            respectively.
                      namically computes the scaling factor consid-
                      ering the current sequence length Lc and the         2. Finetuning on short sequences for more
                                                                  L
                      original context window length L: s =        c .        epochs. We fine-tune the model on the origi-
                                                                  L           nal pre-training sequence length after apply-
                      Duetothehighcostoffrequently recomput-
                      ing RoPE features, we evaluated its perfor-             ing the scaled position embeddings. For both
                      mancewithout fine-tuning.                               LLaMA2-Chat7Band13B,wefine-tunethe
                                                                              modelonsequenceswithlength4,096for400
                    • YaRN (Peng et al., 2024). We evaluate its               steps.
                      performance after fine-tuning.
                    For NTK-Aware scaling and Dynamic NTK-
                 Aware scaling settings, we replace the original
                 RoPEposition embeddings in the model with the
                 scaled ones and test their performance without fine-
                 tuning following (bloc97, 2023; Peng et al., 2024).
                 For YaRN and RESONANCE YARN settings, we
                 fine-tune the model for approximately 100M to-
                 kens on PG19’s training set (Rae et al., 2020). Our
                 target scaled length for the 7B and 13B models is          2https://github.com/jquesnelle/yarn.
                                                                     598
