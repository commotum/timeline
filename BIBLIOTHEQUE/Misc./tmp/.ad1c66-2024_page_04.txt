           T. Han et al.                                                                 International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
           Fig. 4. Overall framework diagram of Adaptive-Structure Graph Transformer (ASGFormer) for point cloud semantic segmentation; The network is designed as an end-to-end
           pyramid architecture, from Section 3.1; Multi-layer Adaptive Graph Transformer (AGT) blocks are incorporated into the architecture to dynamically learn the structural weights and
           feature relationships of points, from Section 3.2; The utilization of Virtual Nodes for Graph Optimizing learning (VNGO) are applied between network hierarchies, from Section 3.3.
               Point Transformer V2 (Wu et al., 2022) presented the more ef-
           fective group vector attention with weighted encoding and additional
           position encoding multiplier. Further, the Point Transformer V3 (Wu
           et al., 2023) has focused on overcoming the trade-off between accuracy
           and efficiency within the Transformer architectures. Considering the
           implicit relationship between semantic and instance segmentation, the
           unified Transformer-based framework (Kolodiazhnyi et al., 2023) is
           established to perform semantic and instance segmentation consistently
           with the learnable kernels. ConDaFormer (Duan et al., 2024) built the
           local window by three orthogonal 2D planes to capture local priors. The                          Fig. 5. Illustration of graph pooling.
           inter-connections between neighboring local regions remain underex-
           plored, despite their significance in Transformer-based 3D point cloud
           models.                                                                      enhance scalability of graph. AutoGT (Zhang et al., 2022b) proposes an
               LCPFormer (Huang et al., 2023) exploited the message passing in          encoding-aware estimation strategy to jointly optimized Transformer
           neighbor regions and made their representations more discriminative          and graph learning. However, these studies have not yet been ap-
           and informative. Local Transformer (Wang et al., 2022) used cross-skip       plied to point cloud analysis and understanding. Upon modeling graph
           selection of neighbors to capture similarities and geometric structure in    representation with learnable weights, Transformer is embedded to
           a larger receptive fields. Stratified Transformer (Lai et al., 2022) adopt   encode weights and point features, determining the similarity between
           contextual relative position encoding to adaptively capture position         points. Semantic segmentation is achieved by combining graph and
           information in a stratified way. SPoTr (Park et al., 2023) designed          Transformer, and the relationships between points are dynamically
           self-positioning point-based global cross-attention to adaptively locate     optimized to separate adherent objects.
           points based on the input shape. OctFormer (Wang, 2023) introduced
           the dilated octree attention to expand the receptive field for shape-        3. Method
           robust of point cloud understanding. Transformer broke the limitations
           of local receptive fields and could learn the relative positional rela-         To tackle the challenge of effectively distinguishing structurally
           tionship between points. However, the weights influenced by relative         adherent objects in existing point cloud semantic segmentation al-
           position and features remained non-learnable in previous methods.            gorithms, we introduce a Graph Transformer network named ASG-
           2.3. Incorporating graph into transformer                                    Former with dynamic adaptive capabilities. The overall framework is
                                                                                        illustrated in Fig. 4.
               In the graph, nodes and edges represent objects and relations be-        3.1. ASGFormer architecture
           tween objects, respectively. In Transformer, nodes can function as
           sequential units, and edges can serve as the links between these units.         The proposed ASGFormer is designed as an end-to-end semantic
           Applying Transformer to graph significantly enhances the representa-         segmentation architecture with pyramid structure. The backbone net-
           tional capability of the graph, making it suitable for searching relation-   workcomprisesencoderanddecodermodulesemployinguniformscale
           ships between points with larger non-local receptive fields. RT (Diao        strategy, as depicted in Fig. 4(a). The five stages in the encoder are
           and Loynd, 2022) generalizes Transformer attention to consider and           denoted as {í µí±† ,í µí±† ,í µí±† ,í µí±† ,í µí±† }, where í µí±† is a MLP layer, and í µí±† âˆ’í µí±† are
           update edge vectors in each Transformer layer, making it successful to                     1  2  3   4  5           1                      2   5
           greater expressivity of graph. GTNs (Yun et al., 2019) is Graph Trans-       composed of MLP and AGT blocks with {2,4,2,2} layers, respectively.
           former Networks, which involve identifying significant connections           We incorporated virtual nodes between each stage to facilitate global
           among unconnected nodes on the original graph, while learning node           message passing across the entire graph.
           representation. GraphGPS (RampÃ¡Å¡ek et al., 2022) employed position              To preserve the spatial structure of point cloud while reducing its
           embedding, local message passing and global attention mechanism to           resolution, we devised graph pooling to construct feature pyramid, as
                                                                                     4 
