Figure 5. mAP under sensor misalignment cases. The X axis refers to the translational discrepanc y between tw o sensors. are misaligned by 1 m , the mAP of our model only drops by 0.49%, while the mAP of P A and CC de grades by 2.33% and 2.85%, respecti v ely . In our method, the calibration ma- trix is only used for projecting the object queries onto im- ages, and the fusion module is not strict wi th the projected locations since the attention mechanism could adapti v ely the r ele v ant image features around based on the con- te xt information. The i nsensiti vity to w ards sensor calibra- tion also enables the possibility to pipelining the 2D and 3D backbones such that the LiD AR features are fused with the features from the pre vious images [ 46 ]. 5.3. Ablation Studies W e conduct ablation studies on the nuScenes v alidation set to study the ef fecti v eness of the proposed components. C.A. I.D. #Layers #Epochs mAP NDS a\ X X 1 12 60.0 66.8 b\ X 1 12 54.3 63.9 c\ X X 3 12 59.9 67.1 d\ X 1 12 24.0 33.8 e\ X 3 12 28.3 43.4 f\ X 3 36 46.9 57.8 T able 6. Ablation of the query initialization module. C.A.: cate gory-a w are; I.D.: input-dependent. Query Initialization. In T able 6 , we study ho w the query initialization strate gy af fects the performance of the initial bounding box prediction. a\ the ro w is T ransFusion-L. b\ when the cate gory-embedding is remo v ed, NDS drops to 63.9%. d\ sho ws the performance of the models trained without the input-dependent strate gy . Specically , we mak e the query posit ions as a set of learnable parame- ters N 2 \ to capture the statistics of potential object lo- cations in the dataset. The model under this setting only achie v es 33.8% NDS. Increasing the number of decoder layers or the number of training epochs boosts the perfor - mance, b ut T ransFusion-L still outperforms the model in f\ by 9.0% NDS. a\ c\ : In contrast, with the proposed query initialization strate gy , our T ransFusion-L does not require more decoder layers. Fusion Components. T o study ho w the image informa- tion benets the detection results, we ablate the proposed fusion components by remo ving the feature fusion mod- mAP NDS P arams M\ Latenc y ms\ CenterP oint 57.4 65.2 8.54 117.2 T ransFusion-L 60.0 66.8 7.96 114.9 CC 63.3 67.6 8.01 + 18.34 212.3 P A 64.2 68.7 13.9 + 18.34 288.2 w/o Fusion 61.6 67.4 9.08 + 18.34 215.0 w/o Guide 64.8 69.3 8.35 + 18.34 236.9 T ransFusion 65.6 69.7 9.47 + 18.34 265.9 T able 7. Ablation of the proposed fusion components. 18.34 rep- resents the parameter size of the 2D backbone. The latenc y is measured on an Intel Core i7 CPU and a T itan V100 GPU. F or CenterPoint, we use re-implementations in MMDetection3D. ule denoted as w/o Fusion \ and the image-guided query initialization denoted as w/o Guide \ As sho wn in T able 7 , the image feature fusion and image-guided query initializa- tion bring 4.8% and 1.6% mAP g ain, respecti v ely . The for - mer pro vides more distincti v e instance features, which are particularly critical for classication on nuScenes, where some cate gories are challenging to distinguish, such as trailer and construction v ehicle. The latter af fec ts less, since T ransFusion-L already has enough recall. W e belie v e the latter will be more useful when point clouds are sparser . Compared with other fusion methods, our fusion strate gy brings a lar ger performance g ain with a modestly increas- ing number of parameters and latenc y . T o better understand where the impro v ements are from, we sho w the mAP break- do wn on dif ferent subsets based on the range in T able 8 . Our fusion method gi v es lar ger performance boost for di stant re- gions where 3D objects are dif cult to detect or classify in LiD AR modality . < 15m 15-30m > 30m T ransFusion-L 70.4 59.5 35.3 T ransFusion 75.5 +5.1\ 66.9 +7.4\ 43.7 +8.4\ T able 8. mAP breakdo wn o v er BEV distance between object cen- ter and e go v ehicle in meters. 6. Conclusion W e ha v e designed an ef fecti v e and rob ust transformer - based LiD AR-camera 3D detection frame w ork with a soft- association mechanism to adapti v ely determine where and what information should be tak en from images. Our T rans- Fusion sets the ne w state-of-the-art results on the nuScenes detection and tracking leaderboards, and sho ws competi- ti v e results on W aymo detection benchmark. The e xten- si v e ablati v e e xperiments demonstrate the rob ustness of our method ag ainst inferior image conditions. W e hope that our w ork will inspire further in v estig ation of LiD AR-camera fu- sion for dri ving-scene perception, and the application of a soft-association based fusion strate gy to other tasks, such as 3D se gmentation. Ackno wledgements. This w ork is supported by Hong K ong RGC GRF 16206819, 16203518, T22-603/15N\ Guangzhou Okay Information T echnology with the project GZETDZ18EG05, and City Uni v ersity of Hong K ong No. 7005729\ Refer ences [1] Holger Caesar , V arun Bankiti, A le x H. Lang, Sourabh V ora, V enice Erin Liong, Qiang Xu, Anush Krishnan, Y u P an, Gi- ancarlo Baldan, and Oscar Beijbom. nuScenes: A multi- modal dataset for autonomous dri ving. CVPR , 2020. 1 [2] Nicolas Carion, Francisco Massa, Gabriel Synnae v e, Nicolas Usunier , Ale xander Kirillo v , and Ser ge y Zagoruyk o. DETR: End-to-end object detection with transformers. ECCV , 2020. 2 , 3 [3] Qi Chen, Lin Sun, Ernest C. H. Cheung, and A. Y uille. Ev ery V ie w Counts: Cross-vie w consistenc y in 3d object detection with h ybrid-c ylindrical-spherical v ox elization. NeurIPS , 2020. 2 [4] Qi Chen, Lin Sun, Zhixin W ang, K. Jia, and A. Y uille. Object as Hotspots: An anchor -free 3d object detection approach via ring of hotspots. ECCV , 2020. 2 [5] Xiaozhi Chen, Huimin Ma, Jixiang W an, B. Li, and T ian Xia. Multi-vie w 3d object detection netw ork for autonomous dri ving. CVPR , 2017. 1 , 2 [6] MMDete ction3D Contrib utors. MMDetection3D: Open- MMLab ne xt-generation platform for general 3D object detection. https : / / github . com / open - mmlab / mmdetection3d , 2020. 5 [7] M. Ev eringham, L. Gool, Christopher K. I. W illiams, J. W inn, and Andre w Zisserman. The pascal visual object classes v oc\ challenge. IJCV , 2009. 6 [8] Lue F an, Xuan Xiong, Feng W ang, Naiyan W ang, and Zhaoxiang Zhang. RangeDet: In defense of range vie w for lidar -based 3d object detection. ICCV , 2021. 2 [9] Peng Gao, Minghang Zheng, Xiaog ang W ang, Jifeng Dai, and Hongsheng Li. F ast con v er gence of detr with spatially modulated co-attention. ICCV , 2021. 3 , 4 [10] T engteng Huang, Zhe Liu, Xiwu Chen, and X. Bai. EPNet: Enhancing point features with image semantics for 3d object detection. ECCV , 2020. 1 , 2 [11] Aleks andr Kim, Aljosa Osep, and Laura Leal-T aix e. Ea- gerMO T : 3d multi-object tracking via sensor fusion. ICRA , 2021. 7 [12] Jas on K u, Melissa Mozian, Jungw ook Lee, Ali Harak eh, and Ste v en L. W aslander . Joint 3d proposal generation and object detection from vie w aggre g ation. IR OS , 2018. 1 [13] H. K uhn. The hung arian method for the assignment problem. Naval Resear c h Lo gistics Quarterly , 1955. 5 [14] Ale x H. Lang, Sourabh V ora, Holger Caesar , Lubing Zhou, Jiong Y ang, and Oscar Beijbom. PointPillars: F ast encoders for object detection from point clouds. CVPR , 2019. 1 , 2 , 6 [15] Zhicha o Li, Feng W ang, and Naiyan W ang. LiD AR R-CNN: An ef cient and uni v ersal 3d object detector . CVPR , 2021. 7 [16] Ming Liang, Binh Y ang, Y un Chen, Rui Hu, and R. Urta- sun. Multi-task multi-sensor fusion for 3d object detection. CVPR , 2019. 1 [17] Ming Liang, Binh Y ang, Shenlong W ang, and R. Urtasun. Deep continuous fusion for multi-sensor 3d object detection. ECCV , 2018. 1 [18] Tsung-Y i Lin, Priya Go yal, Ross B. Girshick, Kaiming He, and Piotr Doll ar . F ocal loss for dense object detection. ICCV , 2017. 5 [19] Zili Liu, Guodong Xu, Honghui Y ang, Minghao Chen, K uoliang W u, Zheng Y ang, Haifeng Liu, and Deng Cai. Suppress-and-rene frame w ork for end-to-end 3d object de- tection. arXiv , 2021. 2 [20] Ze Liu, Zheng Zhang, Y ue Cao, Han Hu, and Xin T ong. Group-free 3d object detection via transformers. ICCV , 2021. 2 , 3 [21] Jia geng Mao, Y ujing Xue, Minzhe Niu, Hao yue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. V ox el transformer for 3d object detection. ICCV , 2021. 2 [22] Gre gory P . Me yer , J ak e Charland, Darshan He gde, Ankita Gajanan Laddha, and Carlos V allespi-Gonzalez. Sen- sor fusion for joint 3d object detection and semantic se gmen- tation. CVPR W , 2019. 1 [23] Ishan Misra, Rohit Girdhar , and Armand Joulin. An End- to-End T ransformer Model for 3D Object Detection. ICCV , 2021. 2 , 3 , 4 , 5 [24] Xuran P an, Zhuof an Xia, Shiji Song, L. Li, and Gao Huang. 3d object detection with pointformer . CVPR , 2021. 2 [25] Adam P aszk e, Sam Gross, Soumith Chintala, Gre gory Chanan, Edw ard Y ang, Zachary DeV ito, Zeming Lin, Al- ban Desmaison, Luca Antig a, and Adam Lerer . Automatic dif ferentiation in p ytorch. NeurIPS-W , 2017. 5 [26] Jonah Philion and S. Fidler . Lift, Splat, Shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. ECCV , 2020. 5 [27] C. Qi, Xinlei Chen, O. Litan y , and L. Guibas. ImV oteNet: Boosting 3d object detection in point clouds with image v otes. CVPR , 2020. 2 [28] C. Qi, O. Litan y , Kaiming He, and L. Guibas. Deep hough v oting for 3d object detection in point clouds. ICCV , 2019. 2 [29] C. Qi, W . Liu, Chenxia W u, Hao Su, and L. Guibas. Frustum pointnets for 3d object detection from r gb-d data. CVPR , 2018. 1 , 2 [30] C. Qi, Hao Su, Kaichun Mo, and L. Guibas. Point Net: Deep learning on poi nt sets for 3d cla ssication and se gmentation. CVPR , 2017. 1 [31] Shaoqing Ren, Kaiming He, Ross B. Girshick, and J. Sun. F aster R-CNN: T o w ards real-time object detection with re- gion proposal netw orks. TP AMI , 2015. 1 [32] Thomas Roddick and R. Cipolla. Predicting semantic map representations from images using p yramid occupanc y net- w orks. CVPR , 2020. 5 [33] Thomas Roddick, Ale x K endall, and R. Cipol la. Ortho- graphic feature transform for monocular 3d object detection. BMVC , 2019. 5 [34] P aul-Edoua rd Sarlin, Daniel DeT one, T omasz M alisie wicz, and Andre w Rabino vich. SuperGlue: Learning feature matching with graph neural netw orks. CVPR , 2020. 4 [35] Huali an Sheng, Sijia Cai, Y uan Liu, Bing Deng, Jianqiang Huang, Xiansheng Hua, and Min-Jian Zhao. Impro ving 3d object detection with channel-wise transformer . ICCV , 2021. 2 [36] Shaoshua i Shi, Chaoxu Guo, Li Jiang, Zhe W ang, Jianping Shi, Xiaog ang W ang, and Hongsheng Li. PV -RCNN: Point- v ox el feature set abstraction for 3d object detection. CVPR , 2020. 2 , 7 [37] Shaoshuai Shi, Xiaog ang W ang, and Hongsheng Li. PointR- CNN: 3d object proposal generation and detection from point cloud. CVPR , 2019. 2 [38] Shaoshuai Shi, Zhe W ang, Jianping Shi, Xiaog ang W ang, and Hongsheng Li. From Points to P arts: 3d object detec- tion from point cloud with part-a w are and part-aggre g ation netw ork. TP AMI , 2021. 2 [39] Kiw oo Shin, Y . Kw on, and M. T omizuka. RoarNet: A ro- b ust 3d object detection based on re gion approximation re- nement. IV , 2019. 1 , 2 , 7 [40] V ishw anath A. Sindagi, Y in Zhou, and Oncel T uzel. MVX- Net: Multimodal v ox elnet for 3d object detection. ICRA , 2019. 1 , 2 [41] Jiami ng Sun, Zehong Shen, Y uang W ang, Hujun Bao, and Xiao wei Zhou. LoFTR: Detector -free local feature matching with transformers. CVPR , 2021. 4 [42] Pei Sun, Henrik Kret zschmar , Xerx es Dotiw alla, Aurelien Chouard, V ijaysai P atnaik, P . Tsui, James Guo, Y in Zhou, Y uning Chai, Benjamin Caine, V ijay V asude v an, W ei Han, Jiquan Ngiam, Hang Zhao, Al eksei T imofee v , S. Ettinger , Maxim Kri v ok on, A. Gao, Aditya Joshi, Y . Zhang, Jonathon Shlens, Zhifeng Chen, and D ragomir Anguelo v . Scalability in perception for autonomous dri ving: W aymo open dataset. CVPR , 2020. 1 [43] Pei Sun, W eiyue W ang, Y uning Chai, Gamaleldin F . Elsayed, Ale x Be wle y , Xiao Zhang, Cristian Sm inchisescu, and Drago Anguelo v . RSN: Range sparse net for ef cient, accurate lidar 3d object detection. CVPR , 2021. 2 [44] Pei Sun, Rufeng Zhang, Y i Jiang, T . K ong, Chenfeng Xu, W . Zhan, M. T omizuka, L. Li, Zehuan Y uan, C. W ang, and Ping Luo. Sparse R-C NN: End-to-end object detection with learnable proposals. CVPR , 2021. 2 , 3 [45] Ashish V asw a ni, N oam M. Shazeer , Niki P armar , Jak ob Uszk oreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser , and Illia Polosukhin. Attention is all you need. NeurIPS , 2017. 3 [46] Sourabh V ora, Ale x H. Lang, Bassam Helou, and Oscar Bei- jbom. PointP ainting: Sequential fusion for 3d object detec- tion. CVPR , 2020. 1 , 2 , 4 , 6 , 7 , 8 [47] Chunwei W ang, Chao Ma, Ming Zhu, and Xiaokang Y ang. PointAugmenting: Cross-modal augmentation for 3d object detection. CVPR , 2021. 1 , 2 , 4 , 5 , 6 , 7 [48] Y ue W ang, Alireza F athi, Abhijit K undu, Da vid A. Ross, Caroline P antof aru, Thomas A. Funkhouser , and Justin M. Solomon. Pillar -based object detection for autonomous dri v- ing. In ECCV , 2020. 2 [49] Y ue W ang and Justin Solomon. Object DGCNN: 3d object detection using dynamic graphs. NeurIPS , 2021. 3 [50] Liang Xie, Chao Xiang, Zhengxu Y u, Guodong Xu, Zheng Y ang, Deng Cai, and Xiaofei He. PI-RCNN: An ef cient multi-sensor 3d object detector with point-based attenti v e cont-con v fusion module. AAAI , 2020. 1 [51] Shaoqing Xu, Dingfu Zhou, Jin F ang, Junbo Y in, Bin Zhou, and Liangjun Zhang. FusionP ainting: Multimodal fusion with adapti v e attention for 3d object detection. ITSC , 2021. 1 , 6 [52] Y an Y an, Y uxing Mao, and B. Li. SECOND: Sparsely em- bedded con v olutional detection. Sensor s , 2018. 2 , 5 [53] Binh Y ang, W enjie Luo, and R. Urtasun. PIXOR: Re al-time 3d object detection from point clouds. CVPR , 2018. 2 [54] Zetong Y ang, Y . Sun, Shu Liu, and Jiaya Jia. 3DSSD: Point- based 3d single stage object detector . CVPR , 2020. 2 [55] Zetong Y ang, Y . Sun, Shu Liu, Xiao yong Shen, and Jiaya Jia. STD: Sparse-to-dense 3d object detector for point cloud. ICCV , 2019. 2 [56] Z. Y ao, Jiangbo Ai, Boxun Li, and Chi Zhang. Ef cient DETR: Im pro vi ng end-to-end object detector with dense prior . arXiv , 2021. 3 [57] T ianwei Y in, Xingyi Zhou, and Philipp Kr ahenb uhl. Center - based 3d object detection and tracking. CVPR , 2021. 2 , 4 , 5 , 6 , 7 [58] T ianwei Y in, Xingyi Zhou, and Philipp Kr ahenb uhl. Multi- modal virtual point 3d detection. NeurIPS , 2021. 6 [59] Jin Hyeok Y oo, Y eocheol Kim, Ji Song Kim, and J. Choi. 3D-CVF: Generating joint camera and lidar features using cross-vie w spatial feature fusion for 3d object detection. ECCV , 2020. 1 , 6 [60] F . Y u, Dequan W ang, and T re v or Darrell. Deep layer aggre- g ation. CVPR , 2018. 5 [61] Y ihan Zeng, Chao Ma, Ming Zhu, Zhiming F an, and Xi- aokang Y ang. Cross-modal 3d object detection and tracking for auto-dri ving. IR OS , 2021. 7 [62] W enwei Zhang, Zhe W ang, and Chen Change Lo y . Multi- modality cut and paste for 3d object detection. arXiv , 2020. 1 [63] Lin Zhao, Hui Zhou, Xinge Zhu, Xiao Song, Hongsheng Li, and W enbing T ao. LIF-Se g: Lidar and camer a image fusion for 3d lidar semantic se gmentation. arXiv , 2021. 1 [64] Dingfu Zhou, Jin F ang, Xibin Song, Chen ye Guan, Junbo Y in, Y uchao Dai , and Ruig ang Y ang. Iou loss for 2d/3d ob- ject detection. 3D V , 2019. 5 [65] Xingyi Zhou, Dequan W ang, and Philipp Kr ahenb uhl. Ob- jects as points. arXiv , 2019. 3 , 4 [66] Y in Zhou, Pei Sun, Y . Zhang, Dragomir Angue lo v , J. Gao, T om Y . Ouyang, James Guo, Jiquan Ngiam, and V ijay V a- sude v an. MVF: End-to-end multi-vie w fusion for 3d object detection in lidar point clouds. CoRL , 2019. 2 [67] Y in Zhou and Oncel T uzel. V ox elNet: End-to-end learning for point cloud based 3d object detection. CVPR , 2018. 1 , 2 , 5 [68] Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Y u. Class-balanced grouping and sampling for point cloud 3d object detection. arXiv , 2019. 2 , 5 , 6 [69] Xinge Zhu, Y ue xin Ma, T ai W ang, Y an Xu, Jianping Shi, and Dahua Lin. SSN: Shape signature netw orks for multi-class object detection from point clouds. ECCV , 2020. 2 [70] Xizhou Zhu, W eijie Su, Le wei Lu, Bin Li, Xiaog ang W ang, and Jifeng Dai. Deformable DETR: Deformable transform- ers for end-to-end object detection. ICLR , 2021. 3 Figure 1. Left: An e xample of bad illumination conditions. Right: Due to the sparsity of point clouds, t he ha rd-association based fu- sion methods w aste man y image features and are sensiti v e to sen- sor calibration, since the projected points may f all outside objects due to a small calibration error . detection frame w ork in this paper . Our k e y idea is to repo- sition the focus of the fusion process, from hard-association to soft-association, leading to the rob ustness ag ainst de gen- erated image quality and sensor misalignment. Specically , we design a sequential fusi on method that uses tw o transformer decoder layers as the detection head. T o our best kno wledge, we are the to use transformer for LiD AR-camera 3D detection. Our decoder layer le v erages a sparse set of object queries to produce ini- tial bounding box es from LiD AR features. Unlik e input- independent object queries in 2D [ 2 , 44 ], we mak e the ob- ject queries input-dependent and cate gory-awar e so that the queries are enriched with better position and cate gory infor - mation. Ne xt, the second transformer decoder layer adap- ti v ely fuses object queries with useful image features as- sociated by spatial and conte xtual relationships. W e le v er - age a locality inducti v e bias by spatially cons training the cross attention around the initial bounding box es to help the netw ork better visit the related positions. Our fusion mod- ule not only pro vides rich semantic informati on to object queries, b ut also is more rob ust to inferior image conditions since the association between LiD AR points and image pix- els are established in a soft and adapti v e w ay . Finally , to handle objects that are dif cult to detect in point clouds, we introduce an image-guided query initialization module to in v olv e image guidance on the query initialization stage. Ov erall, the corporation of these components signicantly impro v es the ef fecti v eness and rob ustness of our LiD AR- camera 3D detector . T o summarize, our contrib utions are fourfold: 1. Our studies in v estig ate the inherent dif culties of LiD AR-camera fusion and re v eal a crucial aspect to rob ust fusion, namely , the soft-association mechanism. 2. W e propose a no v el transformer -based LiD AR-camera fusion model for 3D detection, which performs grained fusion in an attenti v e manner and sho ws supe- rior rob ustness ag ainst de generated image quality and sensor misalignment. 3. W e introduce se v eral simple yet ef fecti v e adjustments for object queries to boost the quality of initial bound- ing box predictions for image fusion. An image- guided query initialization module is also designed to handle objects that are hard to detect in point clouds. 4. W e achie v e the state-of-the-art 3D detection per - formance on nuScenes and compet iti v e results on W aymo. W e also e xtend our model to the 3D track- ing task and achie v e the 1st place in the leaderboard of the nuScenes tracking challenge. 2. Related W ork LiD AR-only 3D Detection aims to predict 3D bounding box es of objects in gi v en point clouds [ 3 , 4 , 27 , 28 , 38 , 46 , 53 , 66 , 68 , 69 ]. Due to the unordered, irre gular nature of point clouds, man y 3D detectors project them onto a re gular grid such as 3D v ox els [ 52 , 67 ], pillars [ 14 ] or range images [ 8 , 43 ]. After that, standard 2D or 3D con v olutions are used to compute the features in the BEV plane, where objects are naturally separated, with their ph ysical sizes pre- serv ed. Other w orks [ 36 , 37 , 54 , 55 ] directly operate on ra w point clouds without quantization. The mainstream of 3D detection head is based on anchor box es [ 14 , 67 ] follo w- ing the 2D counterparts, while [ 48 , 57 ] adopt a center -based representation for 3D objects, lar gely simplifying the 3D detection pipeline. Despite the popularity of adopting the transformer architecture as a detection head in 2D [ 2 ], 3D detection models for outdoor scenarios mostly utilize the transformer for feature e xtraction [ 21 , 24 , 35 ]. Ho we v er , the attenti on operation in each transformer layer requires a computation comple xity of O N 2 \ for N points, requir - ing a carefully designed memory reduction operation when handling LiD AR point clouds with millions of points per frame. In contrast, our model retains an ef cient con v olu- tion backbone for feature e xtraction and le v erages a trans- former decoder with a small set of object queries as the detection head, making the c o m putation cost manageable. The concurrent w orks [ 19 , 20 , 23 ] adopt transformer as a detection head b ut focus on indoor scenarios and e xtending these methods to outdoor scenes is non-tri vial. LiD AR-Camera 3D Detection has g ained increasing atten- tion due to the complementary roles of point clouds and im- ages. Early w orks [ 5 , 29 , 39 ] adopt result-le v el or proposal- le v el fusion, where the fusion granularity is too coarse to release the full potential of tw o modalities. Since Point- P ainting [ 46 ] w as proposed, the point-le v el fusion meth- ods [ 10 , 40 , 47 ] ha v e sho wn great adv antages and promising results. Ho we v er , such methods are easily af fected by the sensor misalignment due to the hard association between points and pix els established by cal ibration matrices. More- o v er , the s imple point-wise concatenation ignores the qual- ity of real data and conte xtual relationships between tw o modalities, and thus leads to de graded performance when the image features are defecti v e. In our w ork, we e xplore a more rob ust and ef fecti v e fusion mechanism to mitig ate these limitations during LiD AR-camera fusion. T ransFusion: Rob ust LiD AR-Camera Fusion f or 3D Object Detection with T ransf ormers Xuyang Bai 1 Ze yu Hu 1 Xinge Zhu 2 Qingqiu Huang 2 Y ilun Chen 2 Hongbo Fu 3 Chie w-Lan T ai 1 1 Hong K ong Uni v ersity of Science and T echnology 2 ADS, IAS B U, Hua wei 3 City Uni v ersity of Hong K ong Abstract LiD AR and camer a ar e two important sensor s for 3D ob- ject detection in autonomous driving . Despite the incr eas- ing popularity of sensor fusion in this eld, the r ob ustness a gainst inferior ima g e conditions, e .g ., bad illumination and sensor misalignment, is under -e xplor ed. Existing fu- sion methods ar e easily af fected by suc h conditions, mainly due to a har d association of LiD AR points and ima g e pixels, established by calibr ation matrices. W e pr opose T r ansFusion, a r ob ust solution to LiD AR- camer a fusion with a soft-association mec hanism to han- dle inferior ima g e conditions. Specically , our T r ansFu- sion consists of con volutional ba c kbones and a detection head based on a tr ansformer decoder . The st layer of the decoder pr edicts initial bounding boxes fr om a LiD AR point cloud using a spar se set of object queries, and its second decoder layer adaptively fuses the object queries with use- ful ima g e featur es, le ver a ging both spatial and conte xtual r elationships. The attention mec hanism of the tr ansformer enables our model to adaptively determine wher e and what information should be tak en fr om the ima g e , leading to a r ob ust and ef fective fusion str ate gy . W e additionally design an ima g e-guided query initialization str ate gy to deal with objects that ar e dif cult to detect in point clouds. T r ansFu- sion ac hie ves state-of-the-art performance on lar g e-scale datasets. W e pr o vide e xtensive e xperiments to demonstr ate its r ob ustness a gainst de g ener ated ima g e quality and cali- br ation err or s. W e also e xtend the pr oposed method to the 3D tr ac king task and ac hie ve the 1st place in the leader - boar d of nuScenes tr ac king , showing its ef fectiveness and g ener alization capability . [ code r elease ] 1. Intr oduction As one of the fundamental tasks in self-dri ving, 3D ob- ject detection aims to localize a set of objects in 3D space and recognize their cate gories. Thanks to the accurate depth information pro vided by LiD AR, early w orks such as V ox elNet [ 67 ] and PointPillar [ 14 ] achie v e reasonably good results using only point clouds as input. Ho we v er , these LiD AR-only methods are generally surpassed by the methods using both LiD AR and camera data on lar ge-scale datasets with sparser point clouds, such as nuScenes [ 1 ] and W aymo [ 42 ]. LiD AR-only methods are surely insuf cient for rob ust 3D detection due to the sparsity of point clouds. F or e xample, small or distant objects are dif cult to detect in LiD AR modality . In contrast, such objects are still clearly visible and distinguishable in high-resolution images. The complementary roles of point clouds and images moti v ate researchers to design detectors utilizing the best of the tw o w orlds, i.e., multi-modal detectors. Existing LiD AR-camera fusion methods roughly f all into three cate gories: r esult-le vel , pr oposal-le vel , and point- le vel . The result-le v el methods, including FPointNet [ 29 ] and RoarNet [ 39 ], use of f-the-shelf 2D detectors to seed 3D proposals, follo wed by a PointNet [ 30 ] for object lo- calization. The proposal-le v el fusion methods, including MV3D [ 5 ] and A V OD [ 12 ], perform fusion at the re gion proposal le v el by applying RoIPool [ 31 ] in each modality for shared proposals. These coarse-grained fusion meth- ods sho w unsatisf actory results since rectangular re gions of interest RoI\ usually contain lots of background noise. Recently , a majority of approaches ha v e tried to do point- le v el fusion and achie v ed promising results. The y a hard association between LiD AR points and image pix- els based on calibration matrices, and then augment LiD AR features with the se gmentation scores [ 46 , 51 ] or CNN fea- tures [ 10 , 22 , 40 , 47 , 62 ] of the associated pix els through point-wise concatenation. Similarly , [ 16 , 17 , 50 , 59 ] project a point cloud onto the bird' s e ye vie w BEV\ plane and then fuse the image features with the BEV pix els. Despite the impressi v e impro v ements, these point-le v el fusion methods suf fer from tw o major problems, as sho wn in Fig. 1 . First, the y simply fuse the LiD AR features and image features through element-wise addition or concate- nation, and thus their performance de grades seriously with lo w-quality image features, e.g., images in bad illumina- tion conditions. Second, nding the hard association be- tween sparse LiD AR points and dense image pix els not only w astes man y image features with rich semantic information, b ut also hea vily relies on high-quality calibration between tw o sensors, which is usually hard to acquire due to the in- herent spatial-temporal misalignment [ 63 ]. T o address the shortcomings of the pre vious fusion ap- proaches, we introduce an ef fecti v e and rob ust multi-modal center computed by projecting the query prediction onto the image plane, r is the radius of the minimum circumscribed circle of the projected corners of the 3D bounding box, and is the h yper -parameter to modulate the bandwidth of the Gaussian distrib ution. Then this weight map is element- wisely multiplied with the cross-attention map among all the attention heads. In this w ay , each object query only at- tends to the related re gion around the projected 2D box, so that the netw ork can learn where to select image features based on the input LiD AR features better and f aster . The visualization of the attention map is sho wn in Fig. 3 . The netw ork typically tends to focus on the fore ground pix els close to the object center and ignore the irrele v ant pix els, pro viding v aluable semantic information for object classi- cation and bounding box re gression. After SMCA, we use another FFN to produce the bound box predicti ons us- ing the object queries containing both LiD AR and image information. 3.5. Label Assignment and Losses F ollo wing DETR [ 23 ], we the bipartite matching be- tween the predictions and ground truth objects through the Hung arian algorithm [ 13 ], where the matching cost is de- by a weighted sum of classication, re gression, and IoU cost: C match = 1 L cl s p; ^ p \ + 2 L r eg b; ^ b \ + 3 L iou b; ^ b \ ; 1\ where L cl s is the binary cross entrop y loss, L r eg is the L1 loss between the predicted BEV centers and the ground- truth centers both normalized in [0 ; 1] \ and L iou is the IoU loss [ 64 ] between the predicted bo x es and ground-truth box es. 1 ; 2 ; 3 are the coef cients of the indi vidual cost terms. W e pro vide sensiti vity analysis of these terms in the supplementary Sec. ?? . Since the number of predictions is usually lar ger than that of GT box es, the unmatched predic- tions are considered as ne g ati v e samples. Gi v en all matched pairs, we compute a focal loss [ 18 ] for the classication branch. The bounding box re gression is supervised by an L1 loss for only positi v e pairs. F or the heatmap predic- tion, we adopt a penalty-reduced focal loss follo wing Cen- terPoint [ 57 ]. The total loss is the weighted sum of losses for each component. W e adopt the same label assignment strate gy and loss formulation for both decoder layers. 3.6. Query Initialization Since our object queries are currently selected using only LiD AR fea tures, it potentially leads to sub-optimality in terms of the detection recall. Empirically , our model al- ready achie v es high recall and sho ws superior performance o v er the baselines Sec. 5 \ Ne v ertheless, to further le v er - age the ability of high-resolution images in detecting small objects and mak e our algorithm more rob ust ag ainst sparse LiD AR point clouds, we propose an image-guided query Figure 4. W e condense the image features along the v ertical dimension, and then project the features onto the BEV plane us- ing cross attention with the LiD AR BEV features. Each image is processed by a separate multi-hea d attention layer, which captures the relation between image column and BEV locations. initialization strate gy , which selects object queries le v erag- ing both the LiD AR and camera information. Specically , we generate a LiD AR-camera BEV feature map F LC by projecting the image features F C onto the BEV plane through cross attention with LiD AR BEV features F L . Inspired by [ 32 ], we use the multi vie w image features collapsed along the height axis as the k e y-v alue sequence of the attent ion mechanism, as sho wn in Fig. 4 . The collaps- ing operation is based on the observ ation that the relation between BEV locations and image columns can be estab- lished easily using camera geometry , and usually there is at most one object along each image column. Therefore, collapsing along the height axis can signicantly reduce the computation without losing critical information. Although some ne-grained image feat ures might be lost during this process, it already meets our need as only a hint on poten- tial object positions is required. Afterw ard, similar to Sec. 3.2 , we use F LC to predict the heatmap, which is a v eraged with the LiD AR-only heatmap ^ S as the heatmap ^ S LC . Using ^ S LC to select and initialize the object queries, our model is able to detect objects that are dif cult to detect in LiD AR point clouds. Note that proposing a no v el method to project the image features onto the BEV plane is be yond the scope of this pa- per . W e belie v e that our method could benet from more research progress [ 26 , 32 , 33 ] in this direction. 4. Implementation Details T raining . W e implement our netw ork in PyT orch [ 25 ] us- ing the open-sourced MMDetection3D [ 6 ]. F or nuScenes, we use the DLA34 [ 60 ] of the pretrained CenterNet as our 2D backbone and k eep its weights frozen during training, follo wing [ 47 ]. W e set the image size to 448 800 , which performs comparably with full resolution 896 1600 \ V ox elNet [ 52 , 67 ] is chosen as our 3D backbone. Our train- ing consists of tw o stages: 1\ W e train the 3D backbone with the decoder layer and FFN for 20 epochs, which only needs the LiD AR point clouds as input and produces the initial 3D bounding box predictions. W e adopt the same data augmentation and training schedules as prior LiD AR- only w orks [ 57 , 68 ]. Note that we also the cop y-and- paste augmentation strate gy [ 52 ] benets the con v er gence Figure 2. Ov erall pipe line of T ransFusion. Our model relies on standard 3D and 2D backbones to e xtract LiD AR BEV feature map and image feature map. Our detection head consists of tw o transformer decoder layers sequentially: 1\ The layer produces initial 3D bounding box es using a sparse set of object queries, initialized in a input-dependent and cate gory-a w are manner . 2\ The second layer attenti v ely associates and fuses the object queries with initial predictions\ from the stage with the image features, producing rich te xture and color c ues for bett er detection results. A spatially modulated cross attention SMCA\ mechanism is introduced to in v olv e a locality inducti v e bias and help the netw ork better attend to the related image re gions. W e additionally propose an image-guided query initialization strate gy to in v olv e image guidance on LiD AR BEV . This strate gy helps produce object queries that are dif cult to detect in the sparse LiD AR point clouds. 3. Methodology In this section, we present the proposed method T ransFu- sion for LiD AR-camera 3D object detection. As sho wn in Fig. 2 , gi v en a LiD AR BEV feature map and an ima ge fea- ture map from con v olutional backbones, our transformer - based detection head decodes object queries into ini- tial bounding box predictions using the LiD AR information, and then perf o r ms LiD AR-camera fusion by attenti v ely fus- ing object queries with useful image features. Belo w we will pro vide the preliminary kno wledge about a trans- former architecture for det ection and then present the detail of T ransFusion. 3.1. Pr eliminary: T ransf ormer f or 2D Detection T ransformer [ 45 ] has been widely used for 2D object de- tection [ 9 , 44 , 56 , 70 ] since DETR [ 2 ] w as proposed. DETR uses a CNN backbone to e xtract image features and a trans- former architecture to con v ert a small set of learned embed- dings called object queries\ into a set of predictions. The follo w-up w orks [ 44 , 56 , 70 ] further equip the object queries with positional information 1 . The pre d i ctions of box es are the relati v e of fsets w .r .t. the query positions to reduce optimization dif culty . W e refer readers to the original pa- pers [ 2 , 70 ] for more details. In our w ork, each object query contains a query position pro viding the localization of the object and a query feature encoding instance information, such as the box' s size, orientation, etc. 1 Slightly dif ferent concepts might be introduced, e.g., reference points in Deformable-DETR [ 70 ] and proposal box es in Sparse-RCNN [ 44 ]. 3.2. Query Initialization Input-dependent. The query positions in the seminal w orks [ 2 , 44 , 70 ] are randomly generated or learned as net- w ork parameters, r e g ardle ss of the input data. Such input- independent query positions will tak e e xtra stages decoder layers\ for their models [ 2 , 70 ] to learn the mo ving process to w ards the real obj ect centers. Recently , it has been ob- serv ed in 2D object detection [ 56 ] that with a better ini- tialization of object quer ies, the g ap between 1-layer struc- ture and 6-layer structure could be bridged. Inspired by this observ ation, we propose an input-dependent initialization strate gy based on a center heatmap to achie v e competiti v e performance using only one decoder layer . Specically, gi v en a d dimensional LiD AR BEV fea- ture map F L 2 R X Y d , we predict a class-specic heatmap ^ S 2 R X Y K , where X Y describes the size of the BEV feat ure map and K is the number of cate gories. Then we re g ard the heatmap as X Y K object candi- dates and select the top- N candidates for all the cate gories as our initial object queries. T o a v oid spat ially too closed queries, follo wing [ 65 ], we select the local maximum ele- ments as our object queries, whose v alues are greater than or equal to their 8-connected neighbors. Otherwise, a lar ge number of queries are needed to co v er the BEV plane. The positions and features of the selected candidates are used to initialize the query positions and query features. In this w ay , our initial object queries wil l locate at or close to the poten- tial object centers, eliminating the need of multiple decoder layers [ 20 , 23 , 49 ] to rene the locations. Category-awar e. Unlik e their 2D projections on the image Method Modality V ox el Size m\ mAP NDS Car T ruck C.V . Bus T railer Barrier Motor . Bik e Ped. T .C. P ointPillar [ 14 ] L 0 : 2 ; 0 : 2 ; 8\ 40.1 55.0 76.0 31.0 11.3 32.1 36.6 56.4 34.2 14.0 64.0 45.6 CBGS [ 68 ] L 0 : 1 ; 0 : 1 ; 0 : 2\ 52.8 63.3 81.1 48.5 10.5 54.9 42.9 65.7 51.5 22.3 80.1 70.9 CenterP oint [ 57 ] L 0 : 075 ; 0 : 075 ; 0 : 2\ 60.3 67.3 85.2 53.5 20.0 63.6 56.0 71.1 59.5 30.7 84.6 78.4 P ointP ainting [ 46 ] LC 0 : 2 ; 0 : 2 ; 8\ 46.4 58.1 77.9 35.8 15.8 36.2 37.3 60.2 41.5 24.1 73.3 62.4 3D-CVF [ 59 ] LC 0 : 05 ; 0 : 05 ; 0 : 2\ 52.7 62.3 83.0 45.0 15.9 48.8 49.6 65.9 51.2 30.4 74.2 62.9 P ointA ugmenting [ 47 ] LC 0 : 075 ; 0 : 075 ; 0 : 2\ 66.8 71.0 87.5 57.3 28.0 65.2 60.7 72.6 74.3 50.9 87.9 83.6 MVP [ 58 ] LC 0 : 075 ; 0 : 075 ; 0 : 2\ 66.4 70.5 86.8 58.5 26.1 67.4 57.3 74.8 70.0 49.3 89.1 85.0 FusionP ainting [ 51 ] LC 0 : 075 ; 0 : 075 ; 0 : 2\ 68.1 71.6 87.1 60.8 30.0 68.5 61.7 71.8 74.7 53.5 88.3 85.0 T ransFusion-L L 0 : 075 ; 0 : 075 ; 0 : 2\ 65.5 70.2 86.2 56.7 28.2 66.3 58.8 78.2 68.3 44.2 86.1 82.0 T ransFusion LC 0 : 075 ; 0 : 075 ; 0 : 2\ 68.9 71.7 87.1 60.0 33.1 68.3 60.8 78.1 73.6 52.9 88.4 86.7 T able 1. Comparison with SO T A methods on the nuScenes test set. `C.V . ', `Ped. ', and `T .C. ' are short for construction v ehicle, pedestrian, and traf cone, respecti v ely . `L ' and `C' represent LiD AR and Camera, respecti v ely . The best results are in boldf ace Best LiD AR-only results are mark ed blue and best LC results are mark ed r ed \ F or FusionP ainting [ 51 ], we report the results on the nuScenes website, which are better than what the y reported in their paper . Note that CenterPoint [ 57 ] and PointAugmenting [ 47 ] utilize double-ip tes ting while we do not use an y test time augmentation. Please detailed results here. 2 b ut could disturb the real dat a distrib ution, so we disable this augment ation for the last 5 epochs follo wing [ 47 ] the y called a f ade strate gy\ 2\ W e then trai n the LiD AR-camera fusion and the image-guided query initialization module for another 6 epochs. W e that this tw o-step training scheme performs better than joint training, since we can adopt more xible augmentations for the training stage. See sup- plementary Sec. ?? for the detailed h yper -parameters and settings on W aymo. T esting . During inference, the score is computed as the geometric a v erage of the heatmap score ^ S ij and the classi- cation score ^ p t . W e use all the outputs as our predic- tions without Non-maximum Suppression NMS\ see the ef fect of NMS in s upp l ementary Sec. ?? \ It is note w orth y that pre vious point-le v el fusion methods such as PointAug- menting [ 47 ] rely on tw o dif ferent models for camera FO V and LiD AR-only re gions if the cameras are not 360-de gree cameras, because only points in the camera FO V could fetch the corresponding image features. In contrast, we use a sin- gle model to deal with both camera FO V and LiD AR-only re gions, since object queries located outs ide camera FO V will direc tly ignore the fusion stage and the initial predic- tions from the decoder layer will be a safe guard. 5. Experiments In this section, we mak e comparisons with the state- of-the-art methods on nuScenes and W aymo. Then we con- duct e xtensi v e ablation studies to demonstrate the impor - tance of each k e y component of T ransFusion. Moreo v er , we design tw o e xperiments to sho w the rob ustness of our T rans- Fusion ag ainst inferior image conditions. Besides T ransFu- sion, we also include a model v ariant, which is based on the rst training stage, i.e., producing the initial bounding box predictions using only point clouds. W e denote it as T ransFusion-L and belie v e that it can serv e as a strong base- line for LiD AR-only detection. W e pro vide the qualitati v e results in supplementary Sec. ?? . nuScenes Dataset. The nuScenes dataset is a lar ge-scale autonomous-dri ving dataset for 3D detection and track- 2 https://www.nuscenes.org/object- detection ing, consisting of 700, 150, and 150 scenes for train- ing, v alidation, and testing, respecti v ely . Each frame con- tains one point cloud and six calibrated images co v er - ing the 360-de gree horizontal FO V . F or 3D detection, the main metrics are mean A v erage Precision mAP\ [ 7 ] and nuScenes detection score NDS\ The mAP is dened by the BEV center distance instead of the 3D IoU, and the nal mAP is computed by a v eraging o v er distance thresholds of 0 : 5 m; 1 m; 2 m; 4 m across ten classes. NDS is a consol- idated metric of mAP and other attrib ute metrics, includ- ing translation, scale, orientation, v elocity , and other box attrib utes. F ollo wing CenterPoint [ 57 ], we set the v ox el size to 0 : 075 m; 0 : 075 m; 0 : 2 m \ . W aymo Open Dataset. This dataset consists of 798 scenes for training and 202 scenes for v alidation. The of cial met- rics are mAP and mAPH mAP weighted by heading accu- rac y\ The mAP and mAPH are dened based on the 3D IoU threshold of 0.7 for v ehicles and 0.5 for pedestrians and c yclists. These metrics are further brok en do wn into tw o dif culty le v els: LEVEL1 for box es with more than v e LiD AR points and LEVEL2 for box es with at least one LiD AR point . Unlik e the 360-de gree cameras in nuScenes, the cameras in W aymo only co v er around 250 de grees hor - izontally . The v ox el size is set to 0 : 1 m; 0 : 1 m; 0 : 15 m \ . 5.1. Main Results nuScenes Results. W e submitted our detection results to the nuScenes e v aluation serv er . W ithout an y test time aug- mentation or model ensemble, our T ransFusion outperforms all competing non-ensembled methods on the nuScenes leaderboard at the time of submission. As sho wn in T able 1 , our T ransFusion-L already outperforms the state-of-the- art LiD AR-only methods by a signicant mar gin +5.2% mAP , +2.9% NDS\ and e v en surpasses some multi-modality methods. W e ascribe this performance g ain to the rela- tion modeli ng po wer of the transformer decoder as well as the proposed query initialization strate gies, which are ab- lated in Sec. 5.3 . Once enabling the proposed fusion com- ponents, our T ransFusion recei v es remarkable performance boost +3.4% mAP , +1.5% NDS\ and outperforms all the pre vious methods, including FusionP ainting [ 51 ], which plane, the objects on the BEV plane are all in absolute scale and has small scale v ariance among the same cate gories . T o le v erage such properties for better multi-class detection, we mak e the object queries cate gory-a w are by equipping each query with a cate gory embedding. Specically , using the cate gory of each selected candidate e.g. ^ S ij k belonging to the k -th cate gory\ we element-wisely sum the query feature with a cate gory embedding produced by linearly projecting the one-hot cate gory v ector into a R d v ector . The cate gory embedding brings benets in tw o aspects: on the one hand, it serv es as a useful side information when modelling the object-object relations in the self-attention modules and the object-conte xt relations in the cross-attention modules. On the other hand, during prediction, it could deli v er v aluable prior kno wledge of the object, making the netw ork focus on intra-cate gory v ariance and thus beneting the property prediction. 3.3. T ransf ormer Decoder and FFN The decoder layer follo ws the design of DETR [ 23 ] and the detailed architecture is pro vided in the supplementary Sec. ?? . The cross attention between object queries and the feature maps either from point clouds or images\ ag- gre g ates rele v ant conte xt onto the object candidates, while the self attention between object queries reasons pairwise relations between dif ferent object candidates. The query positions are embedded into d -dimensional positional en- coding with a Multilayer Perceptron MLP\ and element- wisely summed wit h the query features. This enables the netw ork to reason about both conte xt and position jointly . The N object queries containing rich instance informa- tion are then independently decoded into box es and class labels by a feed-forw ard netw ork FF N\ F ollo wing Center - Point [ 57 ], our FFN predicts the center of fset from the query position as x; y , bounding box height as z , size l ; w ; h as log l \ ; log w \ ; log h \ , ya w angle as sin \ ; cos \ and the v elocity if a v ailable\ as v x ; v y . W e also predict a per - class probability ^ p 2 [0 ; 1] K for K semantic classes. Each attrib ute is computed by a separate tw o-layer 1 1 con v olu- tion. By decoding each object query into prediction in par - allel, we get a set of predictions f ^ b t ; ^ p t g N t as output, where ^ b t is the predicted bounding box for the i -th query . F ol- lo wing [ 23 ], we adopt the auxiliary decoding mechanism, which adds FFN and supervision after each decoder layer. Hence, we can ha v e initial bounding box predictions from the decoder layer . W e le v erage such initial predictions in the LiD AR-camera fusion module to constrain the cross attention, as e xplained in the ne xt section. 3.4. LiD Fusion Image F eatur e F etching . Although impressi v e impro v e- ment has been brought by point-le v el fusion methods [ 46 , 47 ], their fusion quality is lar gely limited by the sparsity of Figure 3. The ro w sho ws the input images and the predic- tions of object queries projected on the images, and the second ro w sho ws the cross-attention maps. Our fusion strate gy is able to dynamically choose rele v ant image pix els and is not limited by the number of LiD AR points. The tw o images are pick ed from nuScenes and W aymo, respecti v ely . LiD AR points. When an object only contains a small num- ber of LiD AR points, it can fetch only the same number of image features, w asting the rich semantic information of high-resolution images. T o mitig ate this issue, we do not fetch the multi vie w image features based on the hard asso- ciation between LiD AR points and image pix els. Instead, we retain all the image features F C 2 R N v H W d as our memory bank, and use the cross-attention mechanism in the transformer decoder to perform feature fusion in a sparse- to-dense and adapti v e manner , as sho wn in Fig. 2 . SMCA f or I mage F eatur e Fusion. Multi-head attention is a popular mechanism to perform information e xchange and b uild a soft association between tw o sets of inputs, and it has been widely used for the feature matching task [ 34 , 41 ]. T o mitig ate the sensiti vity to w ards sensor calibration and infe- rior image features brought by the hard-association strate gy , we le v erage the cross-attention mechanism to b uild the soft association between LiD AR and images, enabling the net- w ork to adapti v ely determine where and what information should be tak en from the images. Specically , we identify the specic image in which the object queries are located using pre vious predictions as well as the calibration matrices, and then perform cross at- tention between the object queries and the corresponding image feature map. Ho we v er , as the LiD AR features and image features are from compl etely dif ferent domains, the object queries might attend to visual re gions unrelated to the bounding box to be predicted, leading to a long train- ing time for the netw ork to accurately identify the proper re gions on images. Inspired by [ 9 ], we design a spatially modulated cross attention SMCA\ module, which weighs the cross attention by a 2D circular Gaussian mask around the projected 2D center of each query . The 2D Gaussian weight mask M is generated in a similar w ay as Center - Net [ 65 ], M ij = i c x \ 2 j c y \ 2 r 2 \ ; where i; j \ is the spatial indices of the weight mask M , c x ; c y \ is the 2D V ehicle P edestrian Cyclist Ov erall P ointPillar [ 46 ] 62.5 50.2 59.9 57.6 PVRCNN [ 36 ] 64.8 46.7 - - LiD AR-RCNN [ 15 ] 64.2 51.7 64.4 60.1 CenterP oint [ 57 ] 66.1 62.4 67.6 65.3 P ointA ugmenting [ 47 ] 62.2 64.6 73.3 66.7 T ransFusion-L 65.1 63.7 65.9 64.9 T ransFusion 65.1 64.0 67.4 65.5 T able 2. LEVEL 2 mAPH on W aymo v alidation set. F or Center - Point, we report the performance of single-frame one-stage model trained in 36 epochs. AMO T A " TP " FP # FN # IDS # CenterP oint [ 57 ] 63.8 95877 18612 22928 760 EagerMO T [ 11 ] 67.7 93484 17705 24925 1156 AlphaT rack [ 61 ] 69.3 95851 18421 22996 718 T ransFusion-L 68.6 95235 17851 23437 893 T ransFusion 71.8 96775 16232 21846 944 T able 3. Comparison of the tracking results on nuScenes test set. Please detailed results here. 3 uses e xtra data to train their se gmentation sub-netw orks. Moreo v er , thanks to our soft-association mechanism, T rans- Fusion is rob ust to inferior image conditions including de- generated image quality and sensor misalignment, as sho wn in the ne xt section. W aymo Results. W e report the performance of our model o v er all three classes on W aymo v alidation set in T able 2 . Our fusion strate gy impro v es the mAPH of pedestrian and c yclist classes by 0.3 and 1.5x, respecti v ely . W e suspect tw o reasons for the relati v ely small impro v ement brought by the image components. First, the semantic information of im- ages might ha v e less impact on the coarse-grained cate go- rization of W aymo. Second, the initial bounding box es from the decoder layer are already with accurate locations since the point clouds in W aymo are denser than those in nuScenes see more discussions in supplementary Sec. ?? \ Note that CenterPoint achie v es a better performance with a multi-frame input and a second-stage renement mod- ule. Such components are orthogonal to our method and we lea v e a more po werful T ransFusion for W aymo as the future w ork. PointAugmenting achie v es better performance than ours b ut relies on CenterPoint to get the predictions outside camera FO V for a full-re gion detection, making their sys- tem less xible. Extend to T racking . T o further demonstrate the general- ization capability , we e v aluate our model in a 3D multi- object tracking MO T\ task by performing tracking-by- detection with the same tracking algorithms adopted by CenterPoint. W e refer readers to the original paper [ 57 ] for details. As sho wn in T able 3 , our model signicantly outperforms CenterPoint and sets the ne w state-of-the-art results on the leaderboard of nuScenes tracking. 5.2. Rob ustness against Inferior Image Conditions W e design three e xperiments to demonstrate the rob ust- ness of our proposed fusion module. Since the nuScenes 3 https://www.nuscenes.org/tracking Nighttime Daytime T ransFusion-L 49.2 60.3 CC 49.4 +0.2\ 63.4 +3.1\ P A 51.0 +1.8\ 64.3 +4.0\ T ransFusion 55.2 +6.0\ 65.7 +5.4\ T able 4. mAP breakdo wn o v er daytime and nighttime. W e e xclude cate gories that do no ha v e an y labeled samples. test set only allo ws at most three submissions, all the e x- periments are conducted on the v alidation set. F or f ast it- eration, we reduce the stage training to 12 epochs and remo v e the f ade strate gy . All the other parameters are the same as the main e xperiments. T o a v oid o v erstatement, we additionally b uild tw o baseline LiD AR-camera detectors by equipping our T ransFusion-L with tw o representati v e fu- sion methods on nuScenes: fusing LiD AR and im age fea- tures by point-wise concatenation denoted as CC \ and the fusion strate gy of PointAugmenting denoted as P A \ Nighttime. W e split the v alidation set into daytime and nighttime based on scene descriptions pro vided by nuScenes and sho w the performance g ain under dif ferent situations in T able 4 . Our method brings a much lar ger per - formance g ain during nighttime, where the w orse lighting ne g ati v ely af fects the hard-association based fusion strate- gies CC and P A . Degenerated Image Quality . In T able 5 , we randomly drop se v eral images for each frame by setting the image features of such images to zero during inference. Since both CC and P A fuse LiD AR and image features in a tightly-coupled w ay , their performance drops signicantly when some im- ages are not a v ailable during inference. In contrast, our T ransFusion is able to maintain a high mAP under all cases. When all the six images are not a v ailable, CC and P A suf- fer from 23 : 8% and 17 : 2% mAP de gradation, respecti v ely , while T ransFusion still k eeps the mAP at a competiti v e le v el of 61 : 7% . This adv antage comes from the sequential des ign and the attenti v e fusion strate gy , which generates ini- tial predictions based on LiD AR data and then only g athers useful information from image features adapti v ely . More- o v er , we could e v en directly disable the fusion module if the camera m alfunctioning is kno wn, such that the whole system could still w ork seamlessly in a LiD AR-only mode. # Dr opped Images 0 1 3 6 CC 63.3 59.8 -3.5\ 50.9 -12.4\ 39.5 -23.8\ P A 64.2 61.6 -2.6\ 55.4 -8.8\ 47.0 -17.2\ T ransFusion 65.6 65.1 -0.5\ 63.9 -1.7\ 61.7 -3.9\ T able 5. mAP under dif ferent numbers of droppe d images. The number in each brack et is the mAP drop from the standard input. Sensor Misalignment. W e e v aluate dif ferent fusion meth- ods under a setting where LiD AR and images are not well- calibrated follo wing RoarNet [ 39 ]. Specically , we ran- domly add a translation of fset to the transformation matrix from camera to LiD AR sensor . As sho wn in Fig. 5 , T rans- Fusion achie v es better rob ustness ag ainst the calibration er - ror compared with other fusion methods. When tw o sensors