                   experiments illustrates how while the neural Queue solves copying and the Stack solves reversal, a
                   simple LSTMcontroller can learn to operate a DeQue as either structure, and solve both tasks.
                   Theresults of the Bigram Flipping task for all models are consistent with the failure to consistently
                   correctly generate the last two symbols of the sequence. We hypothesise that both Deep LSTMs and
                   our models economically learn to pairwise ﬂip the sequence tokens, and attempt to do so half the
                   time when reaching the EOS token. For the two ITG tasks, the success of Deep LSTM benchmarks
                   relative to their performance in other tasks can be explained by their ability to exploit short local
                   dependencies dominating the longer dependencies in these particular grammars.
                   Overall, the rapid convergence, where possible, on a general solution to a transduction problem
                   in a manner which propagates to longer sequences without loss of accuracy is indicative that an
                   unboundedmemory-enhancedcontroller can learn to solve these problems procedurally, rather than
                   memorising the underlying distribution of the data.
                   6  Conclusions
                   The experiments performed in this paper demonstrate that single-layer LSTMs enhanced by an un-
                   bounded differentiable memory capable of acting, in the limit, like a classical Stack, Queue, or
                   DeQue, are capable of solving sequence-to-sequence transduction tasks for which Deep LSTMs
                   falter. Even in tasks for which benchmarks obtain high accuracies, the memory-enhanced LSTMs
                   converge earlier, and to higher accuracies, while requiring considerably fewer parameters than all
                   but the simplest of Deep LSTMs. We therefore believe these constitute a crucial addition to our neu-
                   ral network toolbox, and that more complex linguistic transduction tasks such as machine translation
                   or parsing will be rendered more tractable by their inclusion.
                                                                 ´ˇ  ˇ  ´        ¨
                   Acknowledgements WethankAlexGraves,DemisHassabis, Tomas Kocisky , Tim Rocktaschel,
                   SamRitter, Geoff Hinton, Ilya Sutskever, Chris Dyer, and many others for their helpful comments.
                                                   10
