                  Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
                  Figure 4: AIRVprocessappliedtoasimpleARCriddle. Startingfromtheoriginalriddle(bluepanel), thepipeline(1)
                  Augments the grids via rotations and flips, (2) runs inference on each transformed instance, (3) reverses every prediction
                  back to the original frame of reference, and finally (4) votes on the most consistent output.
                  4   Results
                  4.1  ARCdatasetsplits
                  TheARCdatasetconsists of 400 training riddles, 400 public evaluation riddles, and 100 private evaluation riddles that
                  are not accessible to the public [37]. The training set riddles are the easiest, then public evaluation riddles are harder,
                  and finally the private test set have been shown to be harder than the public evaluation set [38].
                  4.1.1  Testing setup
                  Wereportourresultsontheprivatetest set, with the following test-time compute limitations imposed by the competition
                  compute environments available [7, 37]: Namely 2 hours of runtime on a single P100 GPU (16 GB VRAM).
                  Accuracy between predicted output grids and the ground truth output grids is measured by only counting exact matches
                  over the whole predicted grid, allowing for two grid attempts per task (top-2).
                  4.2  Analysis and Discussion
                  Toevaluate the effectiveness of our methodology, we compare the following configurations:
                         • Zero Shot (No TTFT/AIRV): Direct prediction using the ARC-trained LongT5 model.
                         • AIRVOnly: ApplyingtheAIRVtechnique(withbeamsearchdecoding)butwithout TTFT.
                         • TTFT+AIRV:CombineduseofTTFTandAIRV(withbeamsearchdecoding).
                  Wealsotrain small and Large LongT5 variants on our training data. Due to pre-training compute constraints, only train
                  those models on around 10% of the total training data.
                                                                          8
