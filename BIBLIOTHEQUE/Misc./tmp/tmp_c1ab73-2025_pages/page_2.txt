        Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
        Figure 1: An example of a single easy ARC task (one datapoint). This task is solved by surrounding the red pixels with
        four yellow corners, blue pixels with four orange side pixels, whereas cyan and magenta input pixels remain unchanged.
        transformation based on the few examples provided. Crucially, the model must learn a new transformation, which limits
        the degree to which the model can rely on pretrained knowledge or zero shot performance. This puts a heavy emphasis
        onthe need for contextual reasoning (reaching the correct associations) during the evaluation. Therefore, this dataset
        becomes a reliable test of the efficiency of the learning process itself.
        Large-scale foundation models (both vision based and LLMs) do not perform well out of the box, when prompted
        with these problems [3, 4]. Moreover, even though neural nets generally achieve state-of-the-art in natural language
        processing and difficult visual classification and detection tasks [5, 6], these are perceptual/qualitative-type problems
        that require highly contextual and dynamic reasoning. There is significant uncertainty in the research community on
        whether neural nets can ever be trained or enhanced to perform well on ARC or similar datasets [1, 3].
        Ourperspective that these ARC problems are actually more “perceptual” and qualitative than quantitative in nature.
        There are no performant quantitative approaches to searching the space of possible input to output transformations since
        there are near infinite possible transformations even with only a few basic priors [1].
        Learning performant abstractions from data is what the deep learning paradigm is known for, specifically when applied
        ondifficult perceptual or qualitative problems. Deep learning solutions produce state-of-the-art results on perceptual
        problems, for example NLP and vision. The deep learning paradigm consists of an untrained neural network combined
        with an optimizer (e.g., AdamW, SGD). When these two components are enabled with sufficient amounts of data and
        compute, highly-skilled (accurate), artifacts are produced. Artifacts that possess the right abstractions for the task at
        hand. The artifacts we refer to are the trained neural networks.
        This points us to the idea that this paradigm, namely both the untrained NN and the optimizer algorithm (as opposed to
        just a well-trained NN) can be what creates the novel abstractions needed for correct predictions on the ARC private
        test set. Indeed, we are the first to find success on ARC by implementing this combination of optimizer and NN in the
        evaluation loop.
        Weareabletoexplore what kind of training data, architecture decisions, model size, and other factors that impact test
        time tuning and the model’s ability to create abstractions on the fly, to solve novel ARC tasks in the forward pass.
        Wecontribute the following:
           • Wemotivate and present architecture and pre-training recipe decisions for a performant ARC neural network
           in subsection 3.1.
           • Weproposethemethodologyfor creating training data for Test-Time Fine-Tuning (TTFT) in subsection 3.2.
           • We motivate and propose the Augment Inference Reverse-augmentation and Vote (AIRV) and (TTFT) as
           test-time methods for improving ARC performance (section 3), showing a 2.6 fold increase and a further 3
           fold increase over baseline ARC-pre-training in the ARC private set accuracy respectively.
           • This method helps achieve first place in the 2023 ARCathon competition, and achieves the highest score on
           the ARCprivate test set during the 2024 ARC kaggle competition. Unlike previous work, we achieve this on
           the completely novel ARC problems in the ARC private test set. We achieve the best score in the time and
           compute restricted kaggle test environment [7].
                               2
