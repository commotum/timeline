                                    optimization process, where the memory is optimized by simple gradient descent algorithm. This
                                    process is closely related to Fast Weight Programs (FWPs) [62], where the weight update process
                                    (i.e., Equation 9) is the slow network that its momentum weight is generated by a fast network (i.e.,
                                    Equation 10).
                                    Concluding the above examples, we observed that the training process of a 1-layer MLP with:
                                    (1) Gradient descent is a 1-level associative memory that learns how to map data points to their
                                    corresponding LSS-value; and (2) Gradient descent with momentum is a 2-level associative memory
                                    (or optimization process) that the inner-level learns to store gradient values into its parameters, and
                                    then the outer-level updates the slow weight (i.e., Wt) with the value of the inner-level memory.
                                    While these are the most simple examples with respect to both architecture and optimizer algorithms,
                                    one might ask if similar conclusion can be made in more complex setups.
                                    AnExampleofArchitecturalDecomposition. Inthenextexample,wereplacetheMLPmodule
                                    with a linear attention [60]. That is, we aim to train a 1-layer linear attention for task T and on a
                                    sequenceofD             ={x ,...,x               }byoptimizingtheobjectiveLwithgradientdescent. Recalling
                                                       train        1         |Dtrain|
                                    the unnormalized linear attention formulation:
                                                                       k =xW ,                v =xW ,                 q =xW ,                                      (12)
                                                                         t      t   k           t       t   v           t      t    q
                                                                       M =M +vk⊤,                                                                                  (13)
                                                                           t        t−1        t  t
                                                                       y =Mq .                                                                                     (14)
                                                                        t         t  t
                                    Asdiscussed in earlier studies [58, 59], the recurrence in Equation 13 can be reformulated as the
                                    optimization process of a matrix-valued associative memory Mt(·), in which, it aims to compress
                                    the mappings of keys and values into its parameters. In more details, in Definition 1, if we let
                                     ˜
                                    L(M ;k,v):=−⟨M k,v⟩andaimtooptimizethememorywithgradientdescent, the
                                            t−1     t    t               t−1 t      t ˜                               ⊤
                                    memoryupdateruleis: (Note that ∇L(M                          ; k ,v ) = v k          and we let learning rate η = 1)
                                                                                             t−1     t    t       t   t                                   t
                                                      M =argmin ⟨Mk ,v ⟩+∥M−M∥2 withgradientdescent,                                                               (15)
                                                          t+1             M            t+1     t+1                    t 2
                                                                                ˜                                               ⊤
                                                  ⇒M =M−∇L(M;k ,v )=M +v k ,                                                                                       (16)
                                                          t+1          t               t    t+1     t+1           t      t+1 t+1
                                    which is equivalent to the update rule of an unnormalized linear attention in Equation 13. Also, note
                                    that as we observed in the first example, training a linear layer with gradient descent is a 1-layer
                                    optimization problem of an associative memory (see Equation 3) and so the general training/updating
                                    process of projection layers (i.e., W ,W , and W ) is itself an optimization process of associative
                                                                                    k      v           q
                                    memory. Accordingly, this setup, i.e., training a linear attention with gradient descent, can be seen as
                                    a two-level optimization process, where the outer-loop (also known as training process) optimizes the
                                    projection layers with gradient descent, while the inner-loop optimizes the inner memory of M with
                                                                                                                                                                 t
                                    gradient descent.
                                    Note that, as discussed above, here, we have two associative memories, and so each of which
                                    has their own optimization process and gradient flow. That is, in the optimization of outer-level
                                    parameters of W ,W , and W there is no gradient with respect to parameter M(·) and so there is
                                                           k     v           q
                                    no backpropagation through it. Similarly, in the inner-level, there is no backpropagation through
                                    projection layers and they are considered frozen. Furthermore, it is notable that in this example, the
                                    above formulation is also closely connected to FWPs perspective of linear attentions [63], where
                                    projections are considered slow weights, and memory update in Equation 13 is the fast weight update
                                    rule.
                                    Architectural Decomposition with More Levels. In both above examples, we discussed simple
                                    cases, where they can be translated into 2-level optimization processes, which also coincides with their
                                    FWPsinterpretations. In practice, however, we need to use more powerful optimization algorithms to
                                    train the model, and/or use more powerful recurrent update rule for memory. As a simple example,
                                    assume we use gradient descent with momentum to train a linear attention model. In the above
                                    examples, we show that how the linear attention component can be decomposed into two nested
                                    optimization problem. Similarly, here the model can be represented as a 2-level optimization problem,
                                    where (1) the inner level optimizes the memory to compress the context using gradient descent (see
                                    Equation 15), and (2) the outer level optimizes the projection layers with gradient descent with
                                    momentum. Interestingly, from the first example, we know that “gradient descent with momentum”
                                    algorithm itself is indeed a 2-level optimization problem where the momentum term itself is an
                                    associative memory that compress the past gradients into its parameters.
                                                                                                      6
