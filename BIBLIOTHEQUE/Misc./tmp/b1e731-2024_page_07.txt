                                                  Thismeanswesearchforthelatentthatwouldmostlikelymakethedecodergeneratetherightoutputs
                                                  given the corresponding inputs. By finding a latent that can explain all the input-output pairs, the
                                                  latent solution to the optimization problem is more likely to generalize to a new input-output pair. We
                                                  describe here two instantiations of the search process, namely a random search and a gradient ascent
                                                  algorithm, both acting in the latent space of programs. We leave for future work the exploration of
                                                  other search methods like evolutionary strategies [Hansen and Ostermeier, 2001, Chalumeau et al.,
                                                  2023] that could better trade-off exploration and exploitation of the latent space.
                                                  RandomSearch Aninitial version of the latent search process is to sample some latents from
                                                  either the prior distribution p(z) or around the approximate Bayesian posterior q (z|x ,y ) and select
                                                                                                                                                                                                ϕ          i    i
                                                  the latent that gives the highest log likelihood of the input-output pairs according to the decoder.
                                                                                                                                                                     n
                                                                                ∀k ∈ [1,K], z ∼ p(z)                                     z′ ∈ argmaxXlogp (y |x ,z )                                                             (7)
                                                                                                           k                                                z                      θ     i    i     k
                                                                                                                                                              k    i=1
                                                  Randomsearchasymptotically converges to the true maximum likelihood latent (equation 6) and can
                                                  prove useful when the function to optimize (here, the decoder log-likelihood) is not differentiable or
                                                  smooth. However, the efficiency of random search decreases exponentially with the dimension of the
                                                  latent space, which makes it impractical for most applications.
                                                  Algorithm1LPNTest-TimeInferencewithGradientAscent                                                                                                                          5
                                                  Latent Optimization
                                                                                                                                                                                                                             10
                                                    1: Input: n input-output pairs (x ,y ), a test input x                                                   ,
                                                                                                                   i    i                             n+1                                                                    15
                                                         the number of gradient steps K
                                                    2: for i = 1,...,n do                                        ▷ Can be done in parallel                                                                                   20
                                                    3:          Sample z ∼ q (z|x ,y )                                                                                                                                       25
                                                    4: end for                   i         ϕ         i     i
                                                                                                    P                                                                                                                        30
                                                    5: Initialize latent z′ = 1                         n      z
                                                                                                n       i=1 i
                                                    6: for k = 1,...,K doP                                     ▷ Perform gradient ascent                                                                                     35
                                                                   ′        ′                        n
                                                    7:          z =z +α·∇                                   logp (y |x ,z)|                    ′
                                                    8: end for                               z       i=1            θ      i    i        z=z
                                                    9: Generate output: y                           ∼p (y|x                 , z′)                             Figure 3: Gradient field of the decoder
                                                                                           n+1             θ         n+1                                      log-likelihood f(z) = logpθ(y|x,z)
                                                  Gradient Ascent                       Since the decoder is a differentiable neural network, its log-likelihood
                                                  logp (y|x,z)isalsodifferentiable with respect to z and one can use first-order methods like gradient-
                                                           θ
                                                  ascent to efficiently search through the latent space for a solution to the latent optimization problem
                                                  (equation 6). Figure 3 shows the gradient field and the landscape of the decoder log-likelihood of a
                                                  2Dlatent space and displays how gradient-ascent can be performed to find a local optimum in the
                                                  latent space. This visualization highlights that only a small portion of the latent space can explain
                                                  all the input-output pairs, corresponding to high decoding likelihood. Notably, poor initialization
                                                  can lead the search to converge to different local minima, highlighting the importance of amortized
                                                  inference from the encoder.
                                                                                 n                                                                                       n
                                                                z′ = 1 Xz                               ∀k ∈ [1,K], z′ = z′                          +α·∇ Xlogp (y|x ,z)|                                        ′               (8)
                                                                   0       n             i                                         k         k−1                    z                  θ     i    i        z=zk−1
                                                                                i=1                                                                                    i=1
                                                  Theseries(z )                           shouldexhibit increasing decoding likelihood if the step size α is small enough.
                                                                         k k∈[1,K]
                                                  In practice, we generate the output with the best latent found during the gradient ascent algorithm,
                                                  which may not always be the one that is obtained after taking the last gradient step (z ).
                                                                                                                                                                                                            K
                                                  4.3       Training
                                                  Totrain the LPN system end-to-end, we assume to have a dataset of tasks, where a task is defined
                                                  as n input-output pairs (x ,y ) generated by the same program. To simulate the test conditions of
                                                                                                 i     i
                                                  predicting a new input from a given specification, we design the training procedure to reconstruct
                                                                                                                                           7
