                       TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
               are often trained with reinforcement learning with human        plore their potential in handling table reasoning. This group
               feedback (RLHF) (Dong et al., 2024), they already exhibit       includes Qwen2.5-Coder-7B-Instruct (Hui et al., 2024),
               strong human alignment. This approach is called “LLM-as-        Deepseek-Coder-7B-Instruct-v1.5, Deepseek-Coder-33B-
               a-judge” (Gu et al., 2024; Zheng et al., 2023), which has       Instruct (Guo et al., 2024), Seed-Coder-8B-Instruct, and
               been tested in various fields to replace human labor as a       Yi-Coder-9B-Chat (Young et al., 2024).
               decision-making model (Dubois et al., 2023; Chiang & Lee,       Deep Thinking LLMs: We also include a set of deep
               2023).                                                          thinking LLMs, which are designed to excel in complex
               This method addresses several challenges inherent in table      problem analysis and self-reflective reasoning. This group
               reasoning evaluation. LLMs on table reasoning often allow       includes DeepSeek-distilled variants of Qwen-7B/14B/32B
               nuanced reasoning and open-ended outputs. Using LLM-as-         and Llama-8B/70B (DeepSeek-AI, 2025), as well as
               a-Judge reduces the risk of penalizing semantically correct     QwQ-32B (Team, 2025) and the latest models Qwen3-
               but stylistically different outputs, ensuring more objective    8B/14B/32B (Yang et al., 2025a). These models are rele-
               evaluation. Moreover, the judging focuses solely on the         vant for tasks that involve multi-step reasoning and intricate
               final answer’s correctness, avoiding undue penalties caused     query processing.
               by intermediate reasoning steps or formatting errors unless     MathOptimizedLLMs: Weevaluate LLMs specialized
               they directly affect the final response.                        in mathematical reasoning, which are particularly suited
               3.4.3. EXACT MATCH ACCURACY                                     for tasks involving numerical computation. This category
                                                                               includes Kimina-Prover-Preview-Distill-7B (Wang et al.,
               Accuracy is utilized for tasks where LLMs are required to       2025), Qwen2.5-Math-7B/72B-Instruct (Yang et al., 2024b),
               generate outputs in a predefined format, enabling direct and    and Deepseek-Math-7b-Instruct (Shao et al., 2024).
               unambiguous comparison with ground truth. For instance,         Table Reasoning Optimized LLMs: We incorporate three
               in tasks involving numeric calculations, table cell retrieval,  specific LLMs, TableGPT2-7B (Su et al., 2024) and Table-
               or structured outputs, accuracy measures the exact match        R1-SFT/Zero-7B (Yang et al., 2025b), which are fine-tuned
               between the generated response and the expected result.         specifically for the table reasoning. These modelsaretrained
               4. Experiments                                                  on datasets and tasks closely aligned with TableQA require-
                                                                               ments, making them specialized for this benchmark.
               In this section, we conduct a comprehensive evaluation of       4.1.2. IMPLEMENTATION DETAILS
               over 20 state-of-the-art LLMs on our benchmark, providing
               an in-depth analysis of their performance across various        Inference Mode: Inference modes are configured based on
               table reasoning tasks.                                          task requirements, as not all tasks are suitable for PoT or
                                                                               ICoTreasoning. Specifically, certain tasks exclusively use
               4.1. Experiment Setup                                           the TCoTmode,includingalltasks under Natural Language
               4.1.1. LLMS                                                     Understandingproficiency, as well as Table Summary, Table
                                                                               ColumnNaming,TableTitleNaming,andTablePlausibility
               Weevaluate a total of 26 LLMs, covering a diverse range         Verification. These tasks focus on text generation and table
               of models designed for different purposes. These include        content understanding, making code-based reasoning un-
               general LLMs, code-optimized LLMs, deep thinking LLMs,          necessary and less suitable. For all other tasks, we evaluate
               math and structured data analysis optimized LLMs, and           models using three inference modes: TCoT, PoT, and ICoT.
               LLMsspecifically fine-tuned for table reasoning tasks. The      In TCoT, models receive table content in Markdown/HTML
               evaluated models range in size from 7B to 72B parameters,       formats and directly generate answers. In contrast, the PoT
               ensuring a comprehensive comparison across model scales         and ICoT do not provide the model with plaintext table
               and specializations.                                            content. Instead, the model writes codes to read the table,
               General LLMs: The general LLMs, which represent the             extracts the required information, and finally answers the
               baseline performance of language models on table reason-        questions. Clearly, PoT and ICoT place greater demands on
               ing, include Llama-3.1-8B/70B-Instruct (Grattafiori et al.,     the model’s foundational coding abilities, as the model must
               2024), Qwen2.5-7B/72B-Instruct (Yang et al., 2024c), and        write code to extract and process information from the table.
               Mistral-7B-Instruct-v0.3 (Jiang et al., 2023). These models     However, these capabilities are essential for handling larger
               are designed for broad language understanding and genera-       tables and tackling more complex tasks.
               tion tasks.                                                     Evaluation Metrics: In the following experiments, we
               Code Optimized LLMs: The code-optimized LLMs,                   primarily use ROUGE-L (Lin, 2004) and LLM-as-a-
               trained with a focus on code generation, are evaluated to ex-   judge (Zheng et al., 2023) to evaluate model performance
                                                                            8
