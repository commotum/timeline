                       TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
              based on two strict criteria: (1) only QA pairs classified     3.3. Inference Modes
              as high-quality are eligible for selection; (2) only tables    Theframeworksupportsthree distinct inference modes to
              with single-row headers are included to avoid parsing er-      fully evaluate LLMs in various table analysis scenarios:
              rors. Each instance undergoes manual annotation and a dual     TCoT,PoT,andICoT.
              quality-control process to ensure maximum precision.
              3. Evaluation Framework                                        3.3.1. TEXTUAL CHAIN-OF-THOUGHT (TCOT)
                                                                             TCoT(Weietal.,2022)isareasoningmodeinwhichLLMs
              3.1. Overview                                                  solve data analysis problems step by step through pure tex-
              Our evaluation framework is designed to systematically         tual reasoning. The final answer is output exclusively in text
              assess the performance of LLMs on Table tasks. As shown        form. This mode relies heavily on the model’s intrinsic abil-
              in Fig. 1, the framework begins with a database containing     ity to perform logical and sequential text-based reasoning,
              organized data, which includes tabular data, user questions,   without external computational support. Formally, TCoT
              ground-truth answers, and optional additional information.     reasoning mode can be represented as follows:
              This structured data serves as the foundation for generating                     M(T,Q)→− {C,A},                      (1)
              prompts and evaluating model outputs.
              Then, the framework incorporates three distinct inference      where C denotes a chain-of-thoughts derived from model
              modes: Textual Chain-of-Thought (TCoT), Programmatic           M.
              Chain-of-Thought (PoT), and Interleaved Chain-of-Thought       TCoT is well-suited for text generation tasks, such as ta-
              (ICoT). Each mode employs a uniquely designed hybrid           ble summarization or descriptive analysis, where the focus
              prompt that constrains the model’s reasoning process, ensur-   is on interpreting and explaining tabular data. However,
              ing that it follows the appropriate pathway for generating     due to its reliance on text-based reasoning alone, TCoT is
              answers. In the TCoT mode, the model operates on textual       less effective for tasks requiring complex calculations or
              reasoning, generating answers in plain text. The PoT mode      programmatic execution, as it lacks the ability to leverage
              prompts the model to produce executable code by prompt-        external tools to validate or refine LLM’s outputs.
              ing it with basic tabular information and system instructions.
              TheICoTmodeallowsforadvancedreasoning by interleav-            3.3.2. PROGRAM-OF-THOUGHT (POT)
              ing textual and programmatic steps. This mode enables the
              modeltoengageinplanning, iterative step-by-step reason-        PoTisareasoningmodeinwhichLLMssolvedataanaly-
              ing, and self-reflection.                                      sis problems by generating executable code. In this mode,
              Uponcompletingtheinference process, the framework eval-        the model combines textual reasoning with programmatic
              uates the model-generated outputs (referred to as student an-  outputs and ultimately producing a code block as its final
              swers) by comparing them against the ground-truth answers.     answer. This code block is then executed within a code
              Theevaluation process employs multiple reliable metrics to     sandbox environment, which serves as a secure runtime to
              comprehensivelyassessthequalityofthemodel’sresponses.          validate the functionality and correctness of the generated
              Unlike traditional natural language similarity metrics that    code. The execution results are returned as the final answer,
              may penalize models for producing valid but stylistically      ensuring the solution is both logically sound and compu-
              different outputs, such rigorous methodology ensures that      tationally accurate. Formally, PoT reasoning mode can be
              the framework captures both the correctness and contextual     represented as follows:
              relevance of the model’s outputs, providing a holistic view                                        E(P)
              of its performance.                                                          M(T,Q)→− {C,P}−−−→A,                     (2)
                                                                             where P denotes a program code block generated by model
              3.2. Problem Formulation                                       M,andE isacodeexecutor.
              For one specific table-reasoning task, there are two kinds     ComparedwiththeTCoTmode,PoTofferssignificantad-
              of inputs, i.e., tabular data T, and a question Q. A model     vantages, particularly in tasks requiring precise calculations
              Misasked to generate the corresponding answer A ac-            or complex data manipulations. By leveraging program-
              cording to T and Q, where the ground-truth answer is de-       matic reasoning, PoT allows the model to offload compu-
              noted as G. Given N tasks, the goal is to compute scalar       tational tasks to the code interpreter, thereby reducing the
              metrics to evaluate the discrepancy between model pre-         risk of errors in manual calculations. However, a key limita-
              dictions {Ai|i = 1,2,...,N} and ground-truth answers           tion of PoT is its reliance on the model’s ability to generate
              {G |i = 1,2,...,N}.
                 i                                                           syntactically correct and executable code, which may fail
                                                                             if the model lacks sufficient programming proficiency or
                                                                          6
