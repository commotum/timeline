                       TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
              Lu,W.,Zhang,J.,Fan,J.,Fu,Z.,Chen,Y.,andDu,X. Large                Proceedings of the 17th ACM International Conference
                 language model for table processing: A survey. Frontiers       onWebSearchandDataMining,pp.645–654,2024.
                 of Computer Science, 19(2):192350, 2025.                    Team, Q. Qwq-32b: Embracing the power of reinforcement
              Luo,Z.,Xu,C.,Zhao,P.,Sun,Q.,Geng,X.,Hu,W.,Tao,C.,                 learning. URL: https://qwenlm. github. io/blog/qwq-32b,
                 Ma,J., Lin, Q., and Jiang, D. WizardCoder: Empowering          2025.
                 code large language models with evol-instruct. In The
                 Twelfth International Conference on Learning Represen-      Wang, B., Xu, C., Wang, S., Gan, Z., Cheng, Y., Gao, J.,
                 tations (ICLR), 2024.                                          Awadallah, A. H., and Li, B. Adversarial glue: A multi-
                                                                                task benchmark for robustness evaluation of language
              Nan, L., Hsieh, C., Mao, Z., Lin, X. V., Verma, N., Zhang,        models. arXiv preprint arXiv:2111.02840, 2021.
                        ´  ´
                 R., Kryscinski, W., Schoelkopf, H., Riley, K., Tang, X.,
                 et al. Fetaqa: Free-form table question answering. Trans-   Wang, B., Ren, C., Yang, J., Liang, X., Bai, J., Chai, L.,
                 actions of the Association for Computational Linguistics,      Yan, Z., Zhang, Q.-W., Yin, D., Sun, X., et al. Mac-sql: A
                 10:35, 2022.                                                   multi-agentcollaborativeframeworkfortext-to-sql. arXiv
              Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu:          preprint arXiv:2312.11242, 2023.
                 a method for automatic evaluation of machine transla-       Wang, H., Unsal, M., Lin, X., Baksys, M., Liu, J., Santos,
                 tion. In Proceedings of the 40th annual meeting of the         M. D., Sung, F., Vinyes, M., Ying, Z., Zhu, Z., et al.
                 Association for Computational Linguistics, pp. 311–318,        Kimina-prover preview: Towards large formal reason-
                 2002.                                                          ing models with reinforcement learning. arXiv preprint
              Parikh, A., Wang, X., Gehrmann, S., Faruqui, M., Dhingra,         arXiv:2504.11354, 2025.
                 B., Yang, D., and Das, D. Totto: A controlled table-        Wang,Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S.,
                 to-text generation dataset. In Proceedings of the 2020         Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro:
                 Conference on Empirical Methods in Natural Language            Amorerobust and challenging multi-task language un-
                 Processing (EMNLP), pp. 1173–1186, 2020.                       derstanding benchmark. In The Thirty-eight Conference
              Pasupat, P. and Liang, P. Compositional semantic parsing          on Neural Information Processing Systems Datasets and
                 on semi-structured tables. In Proceedings of the 53rd          Benchmarks Track, 2024.
                 Annual Meeting of the Association for Computational         Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
                 Linguistics and the 7th International Joint Conference on      E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting
                 Natural Language Processing (Volume 1: Long Papers),           elicits reasoning in large language models. Advances in
                 pp. 1470–1480, 2015.                                           neural information processing systems, 35:24824–24837,
              Qiu,Z.,Peng,Y.,He,G.,Yuan,B.,andWang,C.Tqa-bench:                 2022.
                 Evaluating llms for multi-table question answering with     Wei, Y., Wang, Z., Liu, J., Ding, Y., and Zhang, L. Magi-
                 scalable context and symbolic extension. arXiv preprint        coder: Empowering code generation with oss-instruct.
                 arXiv:2411.19504, 2024.                                        In International Conference on Machine Learning, pp.
              Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.         52632–52657. PMLR, 2024.
                 Winogrande: An adversarial winograd schema challenge        Wu, J., Yang, L., Li, D., Ji, Y., Okumura, M., and Zhang,
                 at scale. Communications of the ACM, 64(9):99–106,             Y. Mmqa: Evaluating llms with multi-table multi-hop
                 2021.                                                          complex questions. In The Thirteenth International Con-
              Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang,     ference on Learning Representations (ICLR), 2025a.
                 H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Push-   Wu,X.,Yang,J., Chai, L., Zhang, G., Liu, J., Du, X., Liang,
                 ing the limits of mathematical reasoning in open language      D., Shu, D., Cheng, X., Sun, T., et al. Tablebench: A
                 models. arXiv preprint arXiv:2402.03300, 2024.                 comprehensive and complex benchmark for table ques-
              Su, A., Wang, A., Ye, C., Zhou, C., Zhang, G., Chen, G.,          tion answering. In Proceedings of the AAAI Conference
                 Zhu, G., Wang, H., Xu, H., Chen, H., et al. Tablegpt2:         onArtificial Intelligence, volume 39, pp. 25497–25506,
                 Alargemultimodal model with tabular data integration.          2025b.
                 arXiv preprint arXiv:2411.02059, 2024.                      Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou,
              Sui, Y., Zhou, M., Zhou, M., Han, S., and Zhang, D. Table         C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei,
                 meets llm: Can large language models understand struc-         H., et al.  Qwen2 technical report.      arXiv preprint
                 tured table data? a benchmark and empirical study. In          arXiv:2407.10671, 2024a.
                                                                          18
