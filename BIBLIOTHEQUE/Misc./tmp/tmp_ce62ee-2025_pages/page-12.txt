                                 TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
                                                                 Table 5: Number Answer Tasks with Exact Match Accuracy
                                                                      TU                     TBO                      TCO                      DA                      ADA
                                    ModelName                TCoT     PoT    ICoT    TCoT     PoT    ICoT    TCoT      PoT    ICoT    TCoT     PoT    ICoT    TCoT     PoT     ICoT   Overall
                                                                                                     General LLMs
                          Llama-3.1-8B-Instruct              22.37   33.46   40.00   60.66    66.93   81.55   76.94   81.61   84.24   25.00   15.00   25.00   56.67    58.43   53.67     52.10
                          Llama-3.1-70B-Instruct             31.47   45.00   44.62   68.02    85.44   82.54   79.50   91.30   94.17   20.00   25.00   25.00   27.03    65.11   71.07     57.02
                          Qwen2.5-7B-Instruct                22.76   37.31   40.77   46.17    70.91   67.59   69.31   84.83   85.32   22.50   15.00    5.00   50.10    64.96   52.93     49.03
                          Qwen2.5-72B-Instruct               31.86   45.39   46.54   58.15    85.63   83.32   82.87   94.20   92.85   25.00   25.00   15.00   64.94    71.33   65.58     59.18
                          Mistral-7B-Instruct-v0.3           28.78   33.08   21.93   46.66    71.22   39.50   69.89   75.93   56.46   22.50   20.00   20.00   46.20    62.26   52.83     44.48
                                                                                                  CodeOptimizedLLMs
                          Qwen2.5-Coder-7B-Instruct          29.94   36.16   36.93   52.84    71.01   66.92   71.04   87.29   83.48   22.50   25.00    5.00   44.99    68.15   64.58     51.05
                          Deepseek-Coder-7B-Instruct-v1.5     9.49   19.62   14.62   64.84    43.24   34.95   64.12   44.48   42.68   20.00    5.00    5.00   63.64    20.19   62.43     34.28
                          Deepseek-Coder-33B-Instruct        19.68   34.23   21.16   51.37    68.81   52.96   62.64   79.79   57.40   17.50   15.00   20.00   52.05    61.46   40.56     43.64
                          Seed-Coder-8B-Instruct             28.85   38.85   38.08   54.79    79.50   71.95   66.58   88.14   84.39   17.50   25.00   15.00   63.41    63.26   53.33     52.57
                          Yi-Coder-9B-Chat                   29.87   34.62   35.00   73.55    75.33   55.72   74.52   83.84   67.65   22.50   20.00    5.00   63.59    54.65   50.00     49.72
                                                                                                  DeepThinkingLLMs
                          Deepseek-R1-Distill-Qwen-7B        30.06   27.31   35.77   60.91    67.76   54.79   80.66   75.71   79.82   22.50   20.00   20.00   63.21    59.46   53.32     50.08
                          Deepseek-R1-Distill-Qwen-14B       31.41   41.16   42.69   57.14    75.02   70.30   82.30   80.87   88.71   17.50   20.00   10.00   44.27    60.13   65.96     52.50
                          Deepseek-R1-Distill-Qwen-32B       31.99   43.85   44.23   68.81    75.61   78.76   82.69   83.81   77.54   17.50   25.00   15.00   49.17    64.37   67.50     55.05
                          Deepseek-R1-Distill-Llama-8B       30.77   32.69   35.39   57.93    61.55   60.46   79.54   68.73   76.56   25.00   20.00   25.00   61.32    54.56   66.17     50.38
                          Deepseek-R1-Distill-Llama-70B      31.60   44.23   44.62   66.48    85.56   75.84   84.82   89.23   93.31   15.00   20.00   15.00   47.28    62.59   62.46     55.87
                          QwQ-32B                            32.37   46.16   46.54   84.16    85.32   81.08   92.15   87.39   83.53   20.00   25.00   15.00   57.25    67.43   71.61     59.66
                          Qwen3-8B                           31.47   42.69   43.85   65.08    70.87   68.06   90.41   83.49   80.53   25.00   25.00   15.00   72.09    70.26   65.50     56.62
                          Qwen3-14B                          31.22   43.46   45.00   69.87    72.48   74.07   90.07   91.93   83.43   25.00   25.00   10.00   51.36    64.98   70.74     56.57
                          Qwen3-32B                          31.86   45.77   46.54   70.33    77.80   80.73   89.93   87.84   84.98   25.00   25.00   15.00   52.49    72.00   72.33     58.51
                                                                                                 MathOptimizedLLMs
                          Kimina-Prover-Preview-Distill-7B   30.90    1.54   36.54   91.13    0.58    75.67   88.10    3.94   76.08   25.00    0.00   20.00   73.25    4.17    72.80     39.98
                          Qwen2.5-Math-7B-Instruct           32.50   17.69   34.23   92.01    43.72   50.51   94.17   55.98   78.99   25.00   20.00   10.00   80.32    39.28   62.24     49.11
                          Qwen2.5-Math-72B-Instruct          32.05   33.46   41.54   88.93    56.47   71.81   93.17   73.42   89.07   25.00   20.00   10.00   77.96    57.74   63.72     55.62
                          Deepseek-Math-7B-Instruct           8.21    3.08   13.08   48.58    5.85    26.99   67.13   15.42   32.10   20.00    0.00    5.00   48.51    1.93    52.65     23.23
                                                                                            Table Reasoning Optimized LLMs
                          TableGPT2-7B                       31.28   42.31   44.62   54.64    72.63   70.31   76.02   87.36   91.33   25.00   25.00   20.00   56.75    60.41   73.85     55.43
                          Table-R1-SFT-7B                    31.67   35.39   12.31   63.95    61.43   16.90   89.01   84.15   26.42   20.00   10.00    5.00   69.59    40.80   51.26     41.19
                          Table-R1-Zero-7B                   31.92   31.16   44.23   61.14    50.37   65.39   80.61   70.50   88.09   20.00   15.00   15.00   72.09    65.22   65.17     51.72
                     models, such as Qwen2.5-72B-Instruct and TableGPT2-7B,                                            ROUGE-L=28.5butLLM-as-a-judge=100.
                     also excel in numerical tasks. Notably, the exact match ac-
                     curacy represents an objective evaluation method, as it elim-                                  • ContextualRetrievalTask: Aninformationallysuperior
                     inates randomness in scoring by strictly assessing whether                                        table retrieval result obtained ROUGE-L=28.5 versus
                     the predicted value matches the ground truth. Despite this                                        LLM-as-a-judge=100.
                     rigorous evaluation, the best-performing models achieve a
                     maximumscoreofonly59.66,indicating that there remains                                     These cases expose several fundamental limitations of
                     substantial room for improvement in LLMs’ capabilities on                                  ROUGE-L: failure to recognize numerical precision due
                     table reasoning. This highlights the significant challenges                                to rigid lexical matching, inability to acknowledge seman-
                     LLMsfaceintablereasoning, particularly in precise numer-                                   tic equivalence that with different words, and systematic
                     ical understanding and accurate data retrieval.                                            penalization of contextually enriched responses. The LLM-
                                                                                                                as-a-judge resolves these limitations through its capacity for:
                     4.2.6. CASE STUDY                                                                          semantic understanding, context-aware assessment and task-
                     In this section, we present two detailed case studies to ana-                              adaptive evaluation protocol. This comparative analysis
                     lyze the limitations of existing evaluation metrics — partic-                              substantiates the necessity of integrating LLM-as-a-judge
                     ularly ROUGE-L, and demonstrate the necessity and effec-                                  with conventional metrics to establish a comprehensive eval-
                     tiveness of the proposed ICoT reasoning mode.                                              uation framework. The hybrid approach enables multidi-
                                                                                                                mensional assessment (syntactic-semantic balance) of both
                     Firstly, we have thoroughly checked and analyzed the perfor-                               technical accuracy and contextual appropriateness.
                     manceofvarious LLMsacross different tasks, with partic-                                    Furthermore, to highlight the necessity and effectiveness
                     ular emphasis on manual verification of low-scoring cases.                                 of the proposed ICoT, we present case 2, which centers on
                    As shown in Fig. 3, two representative scenarios expose                                     a high-level analytical task named Multi Step Operation.
                     ROUGE-L’sfundamentallimitations:                                                           In this case, the model under evaluation is QwQ. Figure
                                                                                                               4 illustrates the input question, the input table (omitted
                         • Numerical Precision Task: A Python-calculated corre-                                 due to space limitations), and the main inference processes
                            lation coefficient (0.947 vs. ground truth 0.95) received                           under three reasoning modes: PoT, TCoT, and ICoT. In
                                                                                                          12
