                             TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
                  Table 6: Representative benchmarks for table reasoning capabilities. DP is short for direct prompting, and Markup indicates
                  formats including html, xml and markdown.
                                   Benchmark               Table Reasoning Capabilities         Data source       NewQApairs           Infer modes           Table format
                                                         TU TBO TCO DA ADA
                                      WTQ                ✓      ×       ×      ×      ×          Wikipedia                                                       JSON
                                     TabFact             ✓      ×       ×      ×      ×          Wikipedia                                                       JSON
                                     FeTaQA              ✓      ×       ×      ×      ×          Wikipedia                                                       JSON
                                       SQA               ✓      ×       ×      ×      ×          Wikipedia                                                       JSON
                                    HybridQA             ✓      ×       ×      ×      ✓          Wikipedia                        ——                             JSON
                        raditional    ToTTo              ✓      ×       ×      ×      ×          Wikipedia                                                       JSON
                        T             FinQA              ✓      ×       ✓      ×      ×          FinTabNet                                                       JSON
                                     AIT-QA              ✓      ×       ✓      ×      ×      Airline Companies                                                   JSON
                                      Spider             ×      ✓       ✓      ×      ×       Crowdsourcing                                                      JSON
                                      BIRD               ×      ✓       ✓      ×      ×           Kaggle                                                         JSON
                                   TableBench            ✓      ✓       ✓      ✓      ✓          6 sources              ✓          DP/TCoT/PoT/SCoT              JSON
                                       SUC               ✓      ×       ×      ×      ×          5 sources              ✓                   DP             NL/JSON/Markup
                                 RealTableBench          ✓      ✓       ×      ×      ✓           BI data               ✓                   DP             NL/JSON/Markup
                                   TableQAKit            ×      ×       ✓      ×      ✓          7 sources              ×             DP/TCoT/PoT             NL/Markup
                         LLM       TQA-Bench             ×      ✓       ✓      ×      ✓          3 sources              ✓                   DP               CSV/Markup
                                     MMQA                ✓      ✓       ✓      ×      ✓            Spider               ✓                   DP                    NL
                             Tables as Texts or Images   ✓      ✓       ✓      ×      ×          6 sources              ×                DP/TCoT           NL/JSON/Image
                                TableVQA-Bench           ✓      ✓       ×      ×      ×          3 sources              ✓                   DP                   Image
                  uation workflow. Beyond these efforts, other benchmarks                          Onelimitation of our framework is its reliance on LLM-as-
                  have extended the scope of table reasoning to multi-table                        a-judge, which may inadvertently introduce biases. These
                  reasoning, multi-step execution, and visual table understand-                    biases stem from the inherent tendencies of LLMs to favor
                  ing (Wu et al., 2025a; Qiu et al., 2024; Kim et al., 2024;                       certain reasoning styles or answer formats over others, po-
                  Dengetal., 2024). Despite these advances, existing bench-                        tentially impacting the fairness and generalizability of the
                  marks still lack end-to-end evaluation pipelines that cover                      evaluations. We have taken multiple steps to mitigate these
                  the full spectrum of table understanding—from perception                         biases by carefully designing and refining system prompts
                  and reasoning to execution and verification.                                     to ensure neutrality and consistency in scoring. However,
                  In summary, table reasoning has evolved from basic struc-                        despite these efforts, some residual biases may remain. De-
                  tural comprehension toward multidimensional integration,                         spite this, extensive experimental analyses show that in the
                  with methodologies shifting from static retrieval to dy-                         vast majority of cases, LLM-as-a-judge achieves more ob-
                  namic, executable reasoning. This trend reflects broader                         jective results compared to other evaluation methods. At
                  developments in task complexity, methodological diver-                           present, LLM-as-a-judge remains the most comprehensive
                  sity, and fine-grained evaluation design. However, current                       and relatively fair approach for evaluating LLMs across a
                  benchmarks remain limited by fragmented capability di-                           variety of table reasoning tasks.
                  mensions, incomplete reasoning chains, and restricted data                       Furthermore, our current evaluations are confined to tex-
                  structures—hindering their ability to comprehensively as-                        tual and tabular data, excluding other modalities such as
                  sess general-purpose models in real-world tabular environ-                       image-based table representations or multimodal inputs.
                  ments. To bridge this gap, there is an urgent need for a                         This limitation restricts the framework’s applicability to
                  more open, realistic, and complex evaluation framework                           real-world scenarios where data often exists in a variety
                  that enables systematic assessment of model performance                          of formats, such as scanned documents, charts, or hybrid
                  across heterogeneous, multi-source tabular data.                                 representations. Expanding the framework to accommodate
                                                                                                   these modalities would enhance its versatility and enable
                  6. Limitations and Future Work                                                   a more comprehensive assessment of model capabilities
                                                                                                   across diverse table reasoning tasks.
                  6.1. Limitations
                  Arigorous design process has been employed to develop                            6.2. Future Work
                  this benchmark, aiming to comprehensively and objectively                        Several promising directions for future work can further
                  evaluate diverse table reasoning capabilities. Nevertheless,                     enhance the scope and utility of this benchmark:
                  as with all benchmarking frameworks, inherent limitations                        Evaluation of Image-Based Tables: One key area for im-
                  persist, which are critically analyzed in the following sec-                     provement is expanding the benchmark to include tasks
                  tions.                                                                           involving the generation, interpretation, or reasoning over
                                                                                               15
