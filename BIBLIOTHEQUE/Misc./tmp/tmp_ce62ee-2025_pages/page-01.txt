                       TReB:AComprehensiveBenchmarkforEvaluatingTableReasoning
                                                Capabilities of Large Language Models
                            1 †               1 †               1 †         1 †              1 †                 1 †                    1 †
                     CeLi       Xiaofan Liu       ZhiyanSong        CeChi        ChenZhao        Jingjing Yang       ZhendongWang
                                                   1 †              1 †              1 †*              1               1 *
                                      KexinYang        BoshenShi        XingWang          ChaoDeng JunlanFeng
                                             1JIUTIANTeam,ChinaMobileResearchInstitute, Beijing, China
               {lice, liuxiaofan, songzhiyan, chice, zhaochen, yangjingjing, wangzhendongai, yangkexin, shiboshen, wangxing, dengchao, fengjunlan}@chinamobile.com
                                         Abstract                                 2025), and has huge potentials in real-world applications
                    Themajority of data in businesses and industries              such as Business Intelligence and Healthcare (Cheng et al.,
                     is stored in tables, databases, and data warehouses.         2025).Traditional approaches mostly focus on encoding the
                     Reasoningwithtable-structureddataposessignifi-               semantics of tables through structure embeddings and at-
                     cant challenges for large language models (LLMs)             tention mechanisms, enabling pretrained models to better
                     due to its hidden semantics, inherent complexity,            understand the content of tabular data (Kim et al., 2025; Su
                     and structured nature. One of these challenges is            et al., 2024; Zhu et al., 2023). In recent years, the advent
                     lacking an effective evaluation benchmark fairly             of large language models (LLMs), like GPT-3.5 and GPT-
                     reflecting the performances of LLMs on broad                 4 (Brown et al., 2020; Achiam et al., 2023), has redefined
                     table reasoning abilities. In this paper, we fill in         the paradigm of table reasoning methodology. Instead of
                     this gap, presenting a comprehensive table rea-              relying solely on table semantic embeddings, LLMs lever-
                     soning evolution benchmark, TReB, which mea-                 age prompt engineering, external tools such as SQL and
                     sures both shallow table understanding abilities             Python (Wang et al., 2023; Chai et al., 2024), and complex
                     and deep table reasoning abilities, a total of 26            reasoning techniques such as chain-of-thought (CoT) (Wei
                     sub-tasks. We construct a high quality dataset               et al., 2022) to understand and analyze the tabular data.
                     through an iterative data processing procedure.              These developments have demonstrated the remarkable rea-
                    We create an evaluation framework to robustly                 soning capability of LLMs to perform table-related data
                     measure table reasoning capabilities with three              analysis, even without task-specific modifications.
                     distinct inference modes, TCoT, PoT and ICoT.                Owing to the growing potential of LLMs in table analy-
                     Further, we benchmark over 20 state-of-the-art               sis, several benchmarks, such as TableBench (Wu et al.,
                     LLMsusingthisframeworkandproveitseffec-                      2025b) and RealTableBench (Su et al., 2024), have been
                     tiveness. Experimental results reveal that existing          developed to evaluate their reasoning capabilities. These
                     LLMsstillhavesignificantroomforimprovement                   benchmarks evaluate LLMs across multiple dimensions,
                     in addressing the complex and real world Table               including information retrieval, structural understanding,
                     related tasks. Both the dataset and evaluation               numerical computation, etc. Despite these advancements,
         arXiv:2506.18421v2  [cs.CL]  14 Jul 2025frameworkarepublicly available, with the datasetfully evaluating the table-reasoning capabilities of LLMs
                     hosted on huggingface.co/datasets/                           remains challenging due to data&task quality, inference
                    JT-LM/JIUTIAN-TReBandtheframeworkon                           paradigms, and evaluation metrics. These key factors have
                     github.com/JT-LM/jiutian-treb.                               not been fully considered by previous works.
                                                                                  First, the quality and practicality of current datasets remain
               1. Introduction                                                    problematic. Many benchmarks are generated using auto-
                                                                                  mated scripts or LLMs themselves, which can introduce
               Table reasoning refers to the core capability of a model to        noise into the data, such as malformed tables or incorrect
               interpret, manipulate, and deduce insights from tabular data       answers. Furthermore, existing datasets often contain small,
               through logical operations (Zhang et al., 2025). It is promi-      overly simplified tables that fail to reflect the complexity
               nent in the field of natural language processing (Lu et al.,       of real-world tabular structures. In addition, most existing
                                                                                  benchmarks focus on limited task categories, such as table
                    *Corresponding author                                         fact verification or simple numerical calculation, without
                    †Equal contribution
                                                                               1
