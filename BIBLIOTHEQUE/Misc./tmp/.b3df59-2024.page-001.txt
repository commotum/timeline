                  TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                    ¨    1                  *1                  *1             1            1                1
                         EkinAkyurek       MehulDamani          AdamZweiger         Linlu Qiu    HanGuo JyothishPari
                                                            YoonKim1 JacobAndreas1
                                      Abstract
                                                                                   0.7          ARC                  BBH
                   Language models (LMs) have shown impressive
                   performance on tasks within their training distri-              0.6                                    57.8%
                   bution, but often struggle with structurally novel              0.5                         50.5%
                   tasks even when given a small number of in-                                       45.0%
                                                                                   0.4                                           Zero-
                   context task examples. We investigate the effec-                                                              Shot
                   tiveness of test-time training (TTT)—temporarily                0.3
                   updating model parameters during inference us-                 Accuracy
                   ing a loss derived from in-context examples—                    0.2    17.5%
                   as a mechanism for improving LMs’ reason-                       0.1
                   ing and few-shot learning capabilities. On the
                   Abstraction and Reasoning Corpus (ARC), per-                    0.0     FT       FT + TTT    Base    Base + TTT
                   forming TTT with in-context examples yields up             Figure 1. Pass@2 accuracy on a subset of 80 randomly selected
                   to 6× higher accuracy compared to fine-tuned               ARCvalidation tasks and overall accuracy on BIG-Bench Hard.
                   baselines—reaching 53.0% on the public valida-             Thezero-shotbaselineis0forARCand40.9%forBBH,indicated
                   tion set with an 8B-parameter LM and 61.9%                 by the dashed line. TTT boosts the performance of fine-tuned
                   when ensembled with program-synthesis meth-                models (FT) on ARC by 27.5 percentage points and increases
                   ods, matching average human performance. On                accuracy on BBH by 7.3 percentage points.
                   BIG-Bench Hard (BBH), TTT on in-context ex-
                   amples surpasses standard few-shot prompting
                   in the 10-shot setting by 7.3 percentage points            from their pre-training distributions. This question is funda-
                   (50.5%to57.8%). Ourfindings highlight the lim-             mental to understanding how, and whether, LMs can exhibit
                   itations of in-context learning for novel tasks and        the sort of flexible, novel-skill acquisition that has been
                   demonstrate the potential of test-time training to         proposed as a measure of intelligence (Chollet, 2019).
                   enhance language model adaptability.                       Solving complex and novel tasks remains extremely chal-
                                                                              lenging for LMs, and simple sampling approaches often
              1. Introduction                                                 yield poor performance on such problems (Wu et al., 2024;
                                                                              McCoyetal.,2024). However, recent progress has shown
              Large-scale neural language models (LMs) have demon-            that LMs can be substantially improved by adding extra test-
         arXiv:2411.07279v2  [cs.AI]  25 Mar 2025strated remarkable success on few-shot learning of taskstime computation. Several methods fall into this category,
              related to those seen during pre-training, as well as ele-      such as chain-of-thought prompting (Wei et al., 2022), sam-
              mentary variations or compositions of those tasks (Brown        pling with majority voting (self-consistency; Wang et al.,
              et al., 2020; Todd et al., 2024). When given natural lan-       2023), code execution (Brown et al., 2024; Snell et al., 2025;
              guage specifications or a small number of examples, LMs         Damanietal., 2025), and search (Yao et al., 2023).
              can often infer the desired task and generate appropriate out-  The idea of updating model parameters using instance-
              puts. However, an open question is whether these models         specific data at test time has roots in the literature on local
              can truly acquire new skills for which they have not been       learning (Bottou & Vapnik, 1992) and transductive learn-
              trained—particularly, tasks involving non-trivial reasoning,    ing (Joachims, 1999). In these methods, a learner refines its
              planning, and abstraction in domains that differ significantly  parameters or hypotheses after observing test inputs, adapt-
                 *Equal contribution 1Massachusetts Institute of Technology.  ing to individual examples or small clusters of examples.
              Correspondence to: Ekin Akyurek <akyurek@mit.edu>.              Suchapproaches inherently blur the line between training
                                                                              and inference, and can lead to robust adaptation in low-data
                                                                              scenarios or under distribution shift.
                                                                           1
