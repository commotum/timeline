                          Section C.4 we show that Adam with a small modification is the optimal associative memory for the
                          models’ gradients. Next, we show that how this perspective can result in designing more expressive
                          optimizers:
                          Extension: More Expressive Association. As discussed earlier, momentum is a value-less asso-
                          ciative memory and so has limited expressive power. To address this issue, following the original
                          definition of associative memory (i.e., mapping keys to values), we let value parameter vi = Pi and
                          so the momentum aims to minimize:
                                                                               ⊤
                                                          min ⟨m∇L(W ;x ) ,P ⟩,                                    (19)
                                                            m             i  i     i
                          using gradient descent, resulting in the update rule:
                                                      W =W+m
                                                        i+1     i     i+1
                                                      m =α m −ηP∇L(W;x).                                           (20)
                                                        i+1     i+1  i    t  i      i  i
                          This formulation is equivalent to using preconditioning the momentum GD. In fact, preconditioning
                          meansthat the momentum term is an associative memory that learns how to compress the mappings
                          between P and the gradient term ∇L(W ;x ). While any reasonable choice (e.g., random features)
                                    i                           i  i
                          of preconditioning can improve the expressivity of the initial version of GD with momentum per se is
                          a value-less memory (i.e., mapping all gradients to a single value), the above perspective gives more
                          intuition about what preconditioning are more useful. That is, the momentum acts as a memory that
                          aims to map gradients to their corresponding values, and so a function of gradients (e.g., information
                          about Hessian) can provide the memory with a more meaningful mappings.
                          Extension: More Expressive Objectives. As discussed by Behrouz et al. [58], optimizing an
                          inner objective of dot-product similarity results in Hebbian-like update rule, which can cause the
                          memorytobelesseffective. A natural extension of this internal objective is to use ℓ2(·) regression
                          loss (for measuring the corresponding key-value mapping fitness) and minimize the loss func-
                          tion ∥m∇L(W ;x )⊤ −P ∥2,resulting in the update rule of:
                                        i  i       i 2
                                     W =W+m ,                                                                      (21)
                                        i+1    i     i+1                        
                                     m = α I−∇L(W;x)⊤∇L(W;x) m −ηP∇L(W;x),                                         (22)
                                        i+1      i+1           i  i          i  i    i    t  i       i  i
                          This update is based on delta-rule [64] and so it allows the memory (momentum) to better manage its
                          limited capacity and better memorize the series of past gradients.
                          Extension: More Expressive Memory. As discussed earlier, momentum can be viewed as a meta
                          memory model that uses a linear layer (i.e., matrix-valued) to compress the past gradient values.
                          Due to the linear nature of momentum, only linear functions of past gradients can be learned by
                          its internal objective. To increase the learning capacity of this module, one alternative is to use
                          alternative powerful persistent learning modules: i.e., replacing a linear matrix-valued memory for
                          momentumwithanMLP.Therefore,momentumastheamemoryforthepastgradients,hasmore
                          capacity to capture the underlying dynamics of the gradients. To this end, we extend the formulation
                          in Equation 17 as:
                                    W =W+m (u), and m =α m −η∇L(2)(m;u,I),                                         (23)
                                       i+1     i     i+1   i            i+1     i+1  i    t         i  i
                                                          (2)
                          where u = ∇L(W ;x ) and ∇L (·) is the internal objective of momentum (e.g., dot product
                                  i          i  i
                                         ⊤
                          similarity ⟨m(u ),1⟩). We refer to this variant as Deep Momentum Gradient Descent (DMGD).
                                         i
                          Extension: None Linear Outputs. Building upon the above perspective, in which we see the
                          momentum as a neural architecture, one common technique to enhance the representation power
                          of momentum memory module is to use non-linearity on top of its output [28, 65]. That is, we
                          re-formulate Equation 23 as:
                                   W =W+σ(m (u)), and m =α m −η∇L(2)(m;u,I),                                       (24)
                                     i+1     i        i+1   i             i+1     i+1  i    t         i  i
                          where σ(·) is an arbitrary non-linearity. As an example, we let σ(·) = Newton-Schulz(·), where
                          Newton-Schulz(·) is the iterative Newton-Schulz method [66], and m(·) be a linear layer; the
                          resulted optimizer is equivalent to Muon optimizer [34].
                                                                       8
