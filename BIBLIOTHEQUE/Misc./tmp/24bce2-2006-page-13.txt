              AFastLearningAlgorithmforDeepBeliefNets                           1539
              the values of the variables in the layer below. A variant of the wake-sleep
              algorithm described in Hinton et al. (1995) can then be used to allow the
              higher-levelweightstoinﬂuencethelower-levelones.Inthe“up-pass,”the
              recognition weights are used in a bottom-up pass that stochastically picks
              a state for every hidden variable. The generative weights on the directed
              connections are then adjusted using the maximum likelihood learning rule
                             5
              in equation 2.2. The weights on the undirected connections at the top
              level are learned as before by ﬁtting the top-level RBM to the posterior
              distribution of the penultimate layer.
                 The “down-pass” starts with a state of the top-level associative mem-
              oryandusesthetop-downgenerativeconnectionstostochasticallyactivate
              each lower layer in turn. During the down-pass, the top-level undirected
              connectionsandthegenerativedirectedconnectionsarenotchanged.Only
              the bottom-up recognition weights are modiﬁed. This is equivalent to the
              sleep phase of the wake-sleep algorithm if the associative memory is al-
              lowed to settle to its equilibrium distribution before initiating the down-
              pass. But if the associative memory is initialized by an up-pass and then
              only allowed to run for a few iterations of alternating Gibbs sampling be-
              fore initiating the down-pass, this is a “contrastive” form of the wake-sleep
              algorithm that eliminates the need to sample from the equilibrium distri-
              bution of the associative memory. The contrastive form also ﬁxes several
              other problems of the sleep phase. It ensures that the recognition weights
              arebeinglearnedforrepresentationsthatresemblethoseusedforrealdata,
              and it also helps to eliminate the problem of mode averaging. If, given a
              particulardatavector,thecurrentrecognitionweightsalwayspickapartic-
              ularmodeatthelevelaboveandignoreotherverydifferentmodesthatare
              equally goodatgeneratingthedata,thelearninginthedown-passwillnot
              trytoalterthoserecognitionweightstorecoveranyoftheothermodesasit
              wouldif the sleep phase used a pure ancestral pass. A pure ancestral pass
              wouldhavetostart by using prolonged Gibbs sampling to get an equilib-
              rium sample from the top-level associative memory. By using a top-level
              associative memory, we also eliminate a problem in the wake phase: inde-
              pendenttop-level units seem to be required to allow an ancestral pass, but
              theymeanthatthevariationalapproximationisverypoorforthetoplayer
              of weights.
                 Appendix B speciﬁes the details of the up-down algorithm using
              MATLAB-style pseudocode for the network shown in Figure 1. For sim-
              plicity, there is no penalty on the weights, no momentum, and the same
              learning rate for all parameters. Also, the training data are reduced to a
              single case.
                 5 Becauseweightsarenolongertiedtotheweightsabovethem,vˆ0 mustbecomputed
                                                                    i
              usingthestatesofthevariablesinthelayerabovei andthegenerativeweightsfromthese
              variables to i.
