                                                10          H. Wang et al.
                                                      79                                                           79
                                                      78                                                           78
                                                      77                                                           77
                                                      76                       Full Axial-Attention                76                       Full Axial-Attention
                                                                               Conv-Stem + Axial-Attention                                  Conv-Stem + Axial-Attention
                                                     Top-1 Accuracy (%)75      Conv-Stem + PS-Attention           Top-1 Accuracy (%)75      Conv-Stem + PS-Attention
                                                                               Conv-Stem + Attention                                        Conv-Stem + Attention
                                                                               Full Attention                                               Full Attention
                                                      74                       ResNet-50                           74                       ResNet-50
                                                              10       20       30       40       50                       2        4        6       8       10       12
                                                                           Parameters (M)                                                 M-Adds (B)
                                                Fig.3. Comparing parameters and M-Adds against accuracy on ImageNet classiﬁ-
                                                cation. Our position-sensitive self-attention (Conv-Stem + PS-Attention) and axial-
                                                attention (Conv-Stem + Axial-Attention) consistently outperform ResNet-50 [31,65]
                                                and attention models [65] (both Conv-Stem + Attention, and Full Attention), across a
                                                range of network widths (i.e., diﬀerent channels). Our Full Axial-Attention works the
                                                best in terms of both parameters and M-Adds
                                                                           Table 2. COCO val set. MS: Multi-scale inputs
                                                  Method                                  Backbone            MS Params M-Adds PQ PQTh PQSt
                                                  DeeperLab [89]                        Xception-71                                            33.8        -         -
                                                  SSAP [28]                              ResNet-101            3                               36.5        -         -
                                                  Panoptic-DeepLab [19]                 Xception-71                   46.7M        274.0B 39.7 43.9               33.2
                                                  Panoptic-DeepLab [19]                 Xception-71            3 46.7M 3081.4B 41.2 44.9                          35.7
                                                  Axial-DeepLab-S                     Axial-ResNet-S                  12.1M        110.4B 41.8 46.1               35.2
                                                  Axial-DeepLab-M                    Axial-ResNet-M                   25.9M        209.9B 42.9 47.6               35.8
                                                  Axial-DeepLab-L                     Axial-ResNet-L                  44.9M        343.9B 43.4 48.5               35.6
                                                  Axial-DeepLab-L                     Axial-ResNet-L           3 44.9M 3867.7B 43.9 48.6                          36.8
                                                being 3.8× parameter-eﬃcient and 27× computation-eﬃcient (in M-Adds). In-
                                                creasing the backbone capacity (via large channels) continuously improves the
                                                performance. Speciﬁcally, our multi-scale Axial-DeepLab-L attains 43.9% PQ,
                                                outperforming Panoptic-DeepLab [19] by 2.7% PQ.
                                                      Test-dev set: As shown in Tab. 3, our Axial-DeepLab variants show con-
                                                sistent improvements with larger backbones. Our multi-scale Axial-DeepLab-L
                                                attains the performance of 44.2% PQ, outperforming DeeperLab [89] by 9.9%
                                                PQ, SSAP [28] by 7.3% PQ, and Panoptic-DeepLab [19] by 2.8% PQ, setting
                                                a new state-of-the-art among bottom-up approaches. We also list several top-
                                                performingmethodsadoptingthetop-downapproachesinthetableforreference.
                                                      Scale Stress Test: In order to verify that our model learns long range in-
                                                teractions, we perform a scale stress test besides standard testing. In the stress
                                                test, we train Panoptic-DeepLab (X-71) and our Axial-DeepLab-L with the stan-
                                                dard setting, but test them on out-of-distribution resolutions (i.e., resize the in-
