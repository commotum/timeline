                2  H. Wang et al.
                byhandorsearchedbyalgorithms[99,11,57].Anotherlineofworksadoptsatten-
                tion mechanisms. Attention shows its ability of modeling long range interactions
                in language modeling [80,85], speech recognition [21,10], and neural captioning
                [88]. Attention has since been extended to vision, giving signiﬁcant boosts to
                image classiﬁcation [6], object detection [36], semantic segmentation [39], video
                classiﬁcation [84], and adversarial defense [86]. These works enrich CNNs with
                non-local or long-range attention modules.
                 Recently, stacking attention layers as stand-alone models without any spatial
                convolution has been proposed [65,37] and shown promising results. However,
                naive attention is computationally expensive, especially on large inputs. Ap-
                plying local constraints to attention, proposed by [65,37], reduces the cost and
                enables building fully attentional models. However, local constraints limit model
                receptive ﬁeld, which is crucial to tasks such as segmentation, especially on
                high-resolution inputs. In this work, we propose to adopt axial-attention [32,39],
                which not only allows eﬃcient computation, but recovers the large receptive
                ﬁeld in stand-alone attention models. The core idea is to factorize 2D attention
                into two 1D attentions along height- and width-axis sequentially. Its eﬃciency
                enables us to attend over large regions and build models to learn long range
                or even global interactions. Additionally, most previous attention modules do
                not utilize positional information, which degrades attention’s ability in modeling
                position-dependent interactions, like shapes or objects at multiple scales. Recent
                works [65,37,6] introduce positional terms to attention, but in a context-agnostic
                way. In this paper, we augment the positional terms to be context-dependent,
                making our attention position-sensitive, with marginal costs.
                 We show the eﬀectiveness of our axial-attention models on ImageNet [70]
                for classiﬁcation, and on three datasets (COCO [56], Mapillary Vistas [62], and
                Cityscapes [22]) for panoptic segmentation [45], instance segmentation, and se-
                mantic segmentation. In particular, on ImageNet, we build an Axial-ResNet by
                replacing the 3 × 3 convolution in all residual blocks [31] with our position-
                sensitive axial-attention layer, and we further make it fully attentional [65] by
                adopting axial-attention layers in the ‘stem’. As a result, our Axial-ResNet at-
                tains state-of-the-art results among stand-alone attention models on ImageNet.
                For segmentation tasks, we convert Axial-ResNet to Axial-DeepLab by replac-
                ing the backbones in Panoptic-DeepLab [18]. On COCO [56], our Axial-DeepLab
                outperforms the current bottom-up state-of-the-art, Panoptic-DeepLab [19], by
                2.8% PQ on test-dev set. We also show state-of-the-art segmentation results on
                Mapillary Vistas [62], and Cityscapes [22].
                 To summarize, our contributions are four-fold:
                – The proposed method is the ﬁrst attempt to build stand-alone attention
                  models with large or global receptive ﬁeld.
                – We propose position-sensitive attention layer that makes better use of posi-
                  tional information without adding much computational cost.
                – We show that axial attention works well, not only as a stand-alone model
                  on image classiﬁcation, but also as a backbone on panoptic segmentation,
                  instance segmentation, and segmantic segmentation.
