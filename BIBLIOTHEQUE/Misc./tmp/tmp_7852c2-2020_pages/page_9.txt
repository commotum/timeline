                                                                                    Axial-DeepLab      9
                              Table 1. ImageNet validation set results. BN: Use batch normalizations in atten-
                              tion layers. PS: Our position-sensitive self-attention. Full: Stand-alone self-attention
                              models without spatial convolutions
                                Method                       BN PS Full Params         M-Adds    Top-1
                                                          Conv-Stem methods
                                ResNet-50 [31,65]                             25.6M      4.1B     76.9
                                Conv-Stem + Attention [65]                    18.0M      3.5B     77.4
                                Conv-Stem + Attention        3                18.0M      3.5B     77.7
                                Conv-Stem + PS-Attention     3     3          18.0M      3.7B     78.1
                                Conv-Stem + Axial-Attention  3     3          12.4M      2.8B     77.5
                                                      Fully self-attentional methods
                                LR-Net-50 [37]                           3    23.3M      4.3B     77.3
                                Full Attention [65]                      3    18.0M      3.6B     77.6
                                Full Axial-Attention         3     3     3    12.5M     3.3B      78.1
                              Our proposed position-sensitive self-attention (Conv-Stem + PS-Attention) fur-
                              ther improves the performance by 0.4% at the cost of extra marginal compu-
                              tation. Our Conv-Stem + Axial-Attention performs on par with Conv-Stem +
                              Attention [65] while being more parameter- and computation-eﬃcient. When
                              comparing with other full self-attention models, our Full Axial-Attention out-
                              performs Full Attention [65] by 0.5%, while being 1.44× more parameter-eﬃcient
                              and 1.09× more computation-eﬃcient.
                                 Following [65], we experiment with diﬀerent network widths (i.e., Axial-
                              ResNets-{S,M,L,XL}), exploring the trade-oﬀ between accuracy, model parame-
                              ters, and computational cost (in terms of M-Adds). As shown in Fig. 3, our pro-
                              posed Conv-Stem + PS-Attention and Conv-Stem + Axial-Attention already
                              outperforms ResNet-50 [31,65] and attention models [65] (both Conv-Stem +
                              Attention, and Full Attention) at all settings. Our Full Axial-Attention further
                              attains the best accuracy-parameter and accuracy-complexity trade-oﬀs.
                              4.2  COCO
                              The ImageNet pretrained Axial-ResNet model variants (with diﬀerent channels)
                              are then converted to Axial-DeepLab model variant for panoptic segmentation
                              tasks. We ﬁrst demonstrate the eﬀectiveness of our Axial-DeepLab on the chal-
                              lenging COCO dataset [56], which contains objects with various scales (from less
                              than 32×32 to larger than 96×96).
                                 Val set: In Tab. 2, we report our validation set results and compare with
                              other bottom-up panoptic segmentation methods, since our method also belongs
                              to the bottom-up family. As shown in the table, our single-scale Axial-DeepLab-S
                              outperforms DeeperLab [89] by 8% PQ, multi-scale SSAP [28] by 5.3% PQ, and
                              single-scale Panoptic-DeepLab by 2.1% PQ. Interestingly, our single-scale Axial-
                              DeepLab-S also outperforms multi-scale Panoptic-DeepLab by 0.6% PQ while
