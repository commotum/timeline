              Backbone structure. The feature encoder in point trans-              4. Experiments
              former networks for semantic segmentation and classifica-               We evaluate the effectiveness of the presented Point
              tion has five stages that operate on progressively downsam-          Transformer design on a number of domains and tasks. For
              pled point sets. The downsampling rates for the stages are           3Dsemanticsegmentation,weusethechallengingStanford
              [1, 4, 4, 4, 4], thus the cardinality of the point set produced      Large-Scale 3D Indoor Spaces (S3DIS) dataset [1]. For
              byeachstageis[N,N/4,N/16,N/64,N/256],whereNisthe                     3D shape classification, we use the widely adopted Mod-
              number of input points. Note that the number of stages and           elNet40 dataset [47]. And for object part segmentation, we
              the downsampling rates can be varied depending on the ap-            use ShapeNetPart [52].
              plication, for example to construct light-weight backbones
              for fast processing. Consecutive stages are connected by             Implementation details. We implement the Point Trans-
              transition modules: transition down for feature encoding             former in PyTorch [24]. We use the SGD optimizer with
              and transition up for feature decoding.                              momentumandweightdecaysetto0.9and0.0001,respec-
                                                                                   tively. For semantic segmentation on S3DIS, we train for
              Transition down. A key function of the transition down               40Kiterations with initial learning rate 0.5, dropped by 10x
              module is to reduce the cardinality of the point set as re-          at steps 24K and 32K. For 3D shape classification on Mod-
              quired, for example from N to N/4 in the transition from             elNet40 and 3D object part segmentation on ShapeNetPart,
              the first to the second stage. Denote the point set provided         we train for 200 epochs. The initial learning rate is set to
              as input to the transition down module as P and denote               0.05 and is dropped by 10x at epochs 120 and 160.
                                                               1
              the output point set as P . We perform farthest point sam-
                                        2                                          4.1. Semantic Segmentation
              pling [27] in P to identify a well-spread subset P       ⊂P
                              1                                      2      1      Dataandmetric. TheS3DIS[1]datasetforsemanticscene
              with the requisite cardinality. To pool feature vectors from
              P onto P , we use a kNN graph on P . (This is the same               parsing consists of 271 rooms in six areas from three differ-
                1        2                              1
              k as in Section 3.2. We use k = 16 throughout and report             ent buildings. Each point in the scan is assigned a semantic
              a controlled study of this hyperparameter in Section 4.4.)           label from 13 categories (ceiling, floor, table, etc.). Follow-
              Eachinputfeaturegoesthroughalineartransformation,fol-                ing a common protocol [36, 27], we evaluate the presented
              lowed by batch normalization and ReLU, followed by max               approach in two modes: (a) Area 5 is withheld during train-
              pooling onto each point in P from its k neighbors in P .             ing and is used for testing, and (b) 6-fold cross-validation.
                                             2                             1
              The transition down module is schematically illustrated in           For evaluation metrics, we use mean classwise intersection
              Figure 4(b).                                                         over union (mIoU), mean of classwise accuracy (mAcc),
                                                                                   and overall pointwise accuracy (OA).
              Transition up. For dense prediction tasks such as seman-             Performancecomparison. TheresultsarepresentedinTa-
              tic segmentation, we adopt a U-net design in which the               bles 1 and 2. The Point Transformer outperforms all prior
              encoder described above is coupled with a symmetric de-              models according to all metrics in both evaluation modes.
              coder [27, 3]. Consecutive stages in the decoder are con-            On Area 5, the Point Transformer attains mIoU/mAcc/OA
              nected by transition up modules. Their primary function is           of 70.4%/76.5%/90.8%, outperforming all prior work by
              to map features from the downsampled input point set P               multiple percentage points in each metric. The Point Trans-
                                                                            2
              onto its superset P    ⊃ P . To this end, each input point           former is the first model to pass the 70% mIoU bar, outper-
                                   1      2
              feature is processed by a linear layer, followed by batch            forming the prior state of the art by 3.3 absolute percent-
              normalization and ReLU, and then the features are mapped             age points in mIoU. The Point Transformer outperforms
              onto the higher-resolution point set P      via trilinear inter-     MLPs-based frameworks such as PointNet [25], voxel-
                                                        1
              polation. These interpolated features from the preceding             based architectures such as SegCloud [36], graph-based
              decoder stage are summarized with the features from the              methods such as SPGraph [15], attention-based methods
              corresponding encoder stage, provided via a skip connec-             such as PAT [50], sparse convolutional networks such as
              tion. The structure of the transition up module is illustrated       MinkowskiNet [3], and continuous convolutional networks
              in Figure 4(c).                                                      such as KPConv [37]. Point Transformer also substantially
                                                                                   outperforms all prior models under 6-fold cross-validation.
              Outputhead. Forsemanticsegmentation,thefinaldecoder                  The mIoU in this mode is 73.5%, outperforming the prior
              stage produces a feature vector for each point in the input          state of the art (KPConv) by 2.9 absolute percentage points.
              point set. We apply an MLP to map this feature to the final          The number of parameters in Point Transformer (4.9M) is
              logits. For classification, we perform global average pool-          muchsmaller than in current high-performing architectures
              ing over the pointwise features to get a global feature vec-         such as KPConv (14.9M) and SparseConv (30.1M).
              tor for the whole point set. This global feature is passed           Visualization. Figure 5 shows the Point Transformer’s pre-
              through an MLP to get the global classification logits.              dictions. We can see that the predictions are very close to
                                                                               16263
