SA-IB LYIS vi coco ADE20K Open Images
— 11Mimages — 0.120M images —— 0.123Mimages —— 0.028Mimages —— IM images

1129M (1.1B) masks 1.5M masks 0.9M masks 0.7M masks 2.7M masks

3 215

S80 4

— z 10° E10

5 40] ‘s s

2 2102 #3

Bo 2 Bo

5 a0 1130 51-100 101-200 >20 2 000 025 050 O75 2 00 02 04 06 O08

Number of masks per image

Relative segmentation mask size

Concavity

Figure 6: Dataset mask properties. The legend references the number of images and masks in each dataset. Note, that SA-1B
has 11x more images and 400x more masks than the largest existing segmentation dataset Open Images [58].

Mask properties. In Fig. 5 we plot the spatial distribution
of object centers in SA-1B compared to the largest existing
segmentation datasets. Common photographer biases are
present in all datasets. We observe that SA-1B has greater
coverage of image corners compared to LVIS v1 [43] and
ADE20K [115], the two most similarly distributed datasets,
while COCO [64] and Open Images V5 [58] have a more
prominent center bias. In Fig. 6 (legend) we compare these
datasets by size. SA-1B has 11x more images and 400x
more masks than the second largest, Open Images. On av-
erage, it has 36x more masks per image than Open Images.
The closest dataset in this respect, ADE20K, still has 3.5x
fewer masks per image. Fig. 6 (left) plots the masks-per-
image distribution. Next, we look at image-relative mask
size (square root of the mask area divided by image area)
in Fig. 6 (middle). As expected, since our dataset has more
masks per image, it also tends to include a greater percent-
age of small and medium relative-size masks. Finally, to
analyze shape complexity, we look at mask concavity (1
minus mask area divided by area of mask’s convex hull) in
Fig. 6 (right). Since shape complexity is correlated with
mask size, we control for the datasets’ mask size distribu-
tions by first performing stratified sampling from binned
mask sizes. We observe that the concavity distribution of
our masks is broadly similar to that of other datasets.

6. Zero-Shot Transfer Experiments

In this section, we present zero-shot transfer experiments
with SAM, the Segment Anything Model. We consider five
tasks, four of which differ significantly from the promptable
segmentation task used to train SAM. These experiments
evaluate SAM on datasets and tasks that were not seen dur-
ing training (our usage of “zero-shot transfer” follows its
usage in CLIP [80]). The datasets may include novel image
distributions, such as underwater or ego-centric images that,
to our knowledge, do not appear in SA-1B.

Our experiments begin by testing the core goal of
promptable segmentation: producing a valid mask from any
prompt. We emphasize the challenging scenario of a single
foreground point prompt, since it is more likely to be am-
biguous than other more specific prompts. Next, we present
a sequence of experiments that traverse low, mid, and high-

level image understanding and roughly parallel the histori-
cal development of the field. Specifically, we prompt SAM
to (1) perform edge detection, (2) segment everything, i.e.
object proposal generation, (3) segment detected objects,
Le. instance segmentation, and (4), as a proof-of-concept,
to segment objects from free-form text. These four tasks
differ significantly from the promptable segmentation task
that SAM was trained on and are implemented via prompt
engineering. We report zero-shot single point valid mask
evaluation and zero-shot text to mask proof-of-concept in
the main text. We refer readers to the supplement for our
experiments with zero-shot edge detection, object proposal,
and instance segmentation. In addition, we report a set of
ablations in the supplement. We analyze SAM performance
with respect to the size and composition of its training data
as well as the image encoder architecture.

Implementation. Unless otherwise specified: (1) SAM
uses an MAE [46] pre-trained ViT-H [32] image encoder
and (2) SAM was trained on SA-1B, noting that this dataset
includes only automatically generated masks from the final
stage of our data engine. For all other model and training
details, such as hyperparameters, refer to §B.

6.1. Zero-Shot Single Point Valid Mask Evaluation

Task. We evaluate segmenting an object from a single fore-
ground point. This task is ill-posed as one point can refer
to multiple objects. Ground truth masks in most datasets
do not enumerate all possible masks, which can make au-
tomatic metrics unreliable. Therefore, we supplement the
standard mloU metric (ie., the mean of all IoUs between
predicted and ground truth masks) with a human study in
which annotators rate mask quality from 1 (nonsense) to 10
(pixel-perfect). See §E.1, §F, and §H for additional details.
By default, we sample points from the “center” of ground
truth masks (at a maximal value of the mask’s interior dis-
tance transform), following the standard evaluation proto-
col in interactive segmentation [90]. Since SAM is capable
of predicting multiple masks, we evaluate only the model’s
most confident mask by default. The baselines are all
single-mask methods. We compare mainly to RITM [90],
a strong interactive segmenter that performs best on our
benchmark compared to other strong baselines [65, 17].

4021
