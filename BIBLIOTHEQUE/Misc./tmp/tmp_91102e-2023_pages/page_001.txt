frame transforms in the same way as the underlying geome-
try, then we can estimate the local rigid motion via compar-
ing such frames among corresponding regions across times-
tamps. Such local frames allow us to factorize the local
geometry understanding from its motion in a similar spirit
to equivariant 3D analysis [22, |4]. In particular, once we
establish temporally-consistent local coordinate frames, we
essentially have a way to align the geometry regardless of
the underlying motion toward a more canonical and com-
plete geometry understanding. In addition, the motion-
agnostic geometric features also allow easier temporal as-
sociation toward a better motion understanding.

Based upon the above observations, we present a novel
4D learning framework named LeaF. LeaF contains a frame
learning module and a frame-guided 4D learning module.
The frame learning module aims at producing geometry-
based temporally consistent coordinate frames at different
scales so that corresponding regions across timestamps can
be well-aligned. This allows for factorizing geometry learn-
ing from motion learning. And then the frame-guided 4D
learning module leverages the learned frames to facilitate a
more effective spatial-temporal feature aggregation. Specif-
ically, in the frame learning module, we design a hierar-
chical FrameNet which is essentially a rotation-equivariant
neural network able to produce coordinate frames equivari-
ant to the rotation of input geometry. The frame learning
process is very challenging though since apart from rota-
tions different observations from a point cloud sequence c:
vary significantly due to sampling differences, density vari-
ation, or sensor noises. To make sure the rotation equivari-
ance of hierarchical FrameNet is not broken in practice, we
introduce a frame stabilization scheme to further regularize
the frame learning process. In the frame-guided 4D learn-
ing module, we first modify popular 4D operators (e.g. 4D
point conv) into their frame-guided version and then lever-
age frame-guided 4D convolution to process 4D point cloud
sequences. Since the motion information is factorized away
while convolving with the learned region frames, we addi-
tionally process the 4D sequences with just a globally con-
stant camera frame so that the motion information is faith-
fully kept. We fuse the learned features both from using
region frames and from using camera frames, allowing a
very effective 4D feature learning.

To verify the effectiveness of LeaF, we conduct experi-
ments on a wide range of 4D understanding tasks. And we
demonstrate significant improvements over previous state-
of-the-art methods (+2.0% accuracy on HOI4D action seg-
mentation [21], +1.51% accuracy on MSR action recog-
nition [17], +2.4% mloU on HOMD indoor semantic seg-
mentation [21], and +1.81% mloU on Synthia4D outdoor
semantic segmentation [25]).

Our contributions are threefold: 1) we propose to learn
effective spatial-temporal 4D features via learning and ex-

605

ploiting region-wise coordinate frames and our framework
LeaF achieves state-of-the-art performance on a wide range
of 4D understanding benchmarks; 2) we design a hierar-
chical FrameNet along with a frame stabilization scheme
to learn equivariant region frames for motion-invariant ge-
ometry feature learning; 3) we present a frame-guided 4D
learning method that is able to benefit from equivariant re-
gion frames without losing the motion information.

2. Related Work

4D point cloud sequence understanding. Compared with
understanding static 3D point clouds, understanding 4D
point cloud sequences requires more on aggregating and
leveraging spatial-temporal information to perceive the ge-
ometry and dynamics. Several 4D backbones have been
proposed to address such challenges, and they can be di-
vided into two categories based on their representations.
The first is to voxelize raw point clouds and extract features
on 4D voxels, including MinkwoskiNet [3] which employs
4D spatial-temporal convolutions on 4D voxels. The second
is to perform directly on raw points, including Meteor Net
[19] which extends PointNet++[23] with a temporal dimen-
sion and explicitly tracks points’ motion for grouping, and
PSTNet[! |] which constructs a point tube along temporal
dimension for 4D point convolution. State of The Art meth-
ods such as Point 4d Transformer [9] and PPTr [30] belong
to the second category and introduce transformer architec-
ture, in order to avoid point-tracking and to better capture
spatio-temporal correlation. There are works [7] focusing
on improving the optimization of a transformer.

Equivariant feature learning. The seminal work of Group
Equivariant Convolution Networks [4] (G-CNN) starts a
trend of leveraging group equivariance for neural networks.
For 3D data, rotation group equivariance or $O(3) equivari-
ance is significant, and there are roughly two categories of
SO(3) equivariant networks. The first category is the “filter
orbit” method. Inspired by group-equivariant convolution
on 2D images proposed in G-CNN [4], Equivariant Point
Network [2] and successive works [16, |8, 32] discretizes
S0(3) and analogously design group-equivariant operations
on point clouds. However, as such networks extend the orig-
inal features to multiple oriented features, they encounter
memory issues and may not be suitable for 4D point cloud
sequences understanding. The second category is the “filter
design” method. These methods [28, 5, |4] design the filter
forms to achieve exact SO(3) equivariance. “Filter design”
methods get rid of memory issues, but their representation
capability is usually limited due to the restricted filter form.

Rotation-invariant local descriptors. Local descriptor is
a synonym for local feature, which is widely used in tasks
such as 3D point cloud matching. Some of the rotation-

