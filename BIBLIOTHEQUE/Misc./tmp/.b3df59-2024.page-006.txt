                                             TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                     Individual             Aggregated 32                    PS         Fine-tuned LM       TTTMethod           Score
                     30                                    27      29                        X          Ours                X                  18.3%
                            23              21      22                                       X          Ours                Ours               47.1%
                     20             18                                                       X          BARC                Ours               53.0%
                                                                                             BARC Ours                      Ours               58.5%
                                                                                             BARC BARC                      Ours              62.8%
                    asks Solved10                                                                                            Avg. Human        60.2%
                    T                                                                                                         Best Human      97.8%
                       0                                                                                               BARC(ensemble)          54.4%
                         Rotate           Flip   anilla                Oracle                                     BARC(nosynthesizer)          39.3%
                              Transpose         V     Flattened                                                        Claude 3.5 Sonnet       21.0%
                                                            Hierarchical                                                          GPT-4o        9.0%
                Figure 7. Accuracy of different transformations and voting                                            OpenAIo1preview          21.0%
                schemes. While individual transformations generally perform                                                  DeepSeekr1        20.5%
                at a modest level and are comparable to one another, aggregat-                                                 OpenAIo3       82.8%
                ing across them through voting yields substantial improvements.         Table 1. Pass@2 Scores of different systems on the ARC vali-
                Notably, a hierarchical voting strategy with two voting stages sur-     dation set. Our TTT pipeline improves base models consistently.
                passes a flat voting approach. Our hierarchical method approaches       Weachieve 47.1% accuracy when applied to our fine-tuned model
                oracle-level performance, demonstrating its effectiveness in accu-      and 53.0% when applied to the BARC model (Li et al., 2025). We
                rately selecting the correct answer when present.                       ensemble our method with program synthesis (PS) based models,
                4.5. Impact of Augmented Inference                                      where we achieve score of 61.9%, comparable to the average hu-
                                                                                        manperformanceof60.2%.
                Toanalyze the impact of augmented inference and voting,
                werunseveral ablations: (1) Vanilla, which generates two                racybycombiningneuralandprogramsynthesisapproaches.
                predictions without transformations or advanced voting; (2)             While their fully neural approach shares similarities with
                TransformedInference, applying a single transformation                  our system, our TTT and inference pipeline has several ad-
                (Rotate, Transpose, or Flip) to measure its isolated effect;            ditional components (per-task LoRA, more augmentations,
                (3) Hierarchical Voting, our full pipeline combining aug-               hierarchical voting) that boost performance. To validate
                mented inference and structured voting; (4) Flattened Vot-              our improvements, we applied our TTT pipeline to BARC’s
                ing, which selects the top-2 predictions from a single voting           fully neural model, achieving 53.0% accuracy—a 35% im-
                round over all generated outputs; and (5) Oracle, an upper              provement over their original TTT method.
                bound that selects the correct answer if present.                       Building on these results, we explored combinations of
                Asshown,individual transformations are modestly effective               our approach with BARC. Combining our TTT pipeline
                ontheir own (with Transpose performing worst), but their                and neural model with BARC’s synthesizer raised accuracy
                aggregation improves results markedly. Hierarchical voting              to 58.5%. Combining our TTT pipeline with BARC’s
                further outperforms a flattened voting approach and closely             neural model and synthesizer raised accuracy to 61.9%.
                approaches the oracle’s accuracy, suggesting that our two-              This configuration matches average human performance of
                stage aggregation effectively identifies the correct solution           60.2%(LeGris et al., 2024) on the benchmark.
                whenit is present.                                                      Comparingprogramgenerationandend-to-endmodel-
                                                                                        ing    Li et al. (2025) found that program synthesis and fully
                4.6. Comparison to Other Systems                                        neural predictors for ARC are highly complementary. Their
                Following our experiments on 80 tasks, we present com-                  end-to-end neural model can only solve 42.2% of the tasks
                prehensive results on the full ARC public evaluation set,               solved by the program synthesis model. However, we find
                comparing our system against existing approaches. Our                   that when equipped with our TTT pipeline, BARC’s fine-
                analysis focuses on three key aspects: the impact of our                tuned fully neural model solves 73.5% of the tasks that are
                TTTmethodology,the benefits of combining our approach                   solved by the program synthesis model. This suggests that
                with existing methods, and the differences between fully                our TTT pipeline significantly improves the neural model’s
                neural and program synthesis methods.                                   ability to learn systematic reasoning patterns similar to those
                                                                                        captured by program synthesis models.
                TTT WeapplyTTTandaugmentedinferenceprocedure                            Semi-private evaluation          ARC-AGIchallengeprovides
                to our base fine-tuned model (fine-tuned 8B model). TTT                 a hidden “semi-private dataset” and performs external tests
                significantly improves accuracy from 18.3% to 47.1%.                    for submissions. We submitted our ensemble solution to
                Integration with existing methods            Aconcurrent work           the official ARC-AGI semi-private evaluation and observed
                byLietal.(2025)introducedBARC,achieving54.4%accu-                       47.5% accuracy. This decline may be attributed to more
                                                                                     6
