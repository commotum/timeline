             Results
             Weﬁrstexperimentallyinvestigate DreamCoder within two classic benchmark domains: list processing
             and text editing. In both cases we solve tasks speciﬁed by a conditional mapping (i.e., input/output
             examples), starting with a generic functional programming basis, including routines like map, fold,
             cons, car, cdr, etc. Our list processing tasks comprise 218 problems taken from (17), split 50/50
             test/train, each with 15 input/output examples. In solving these problems, DreamCoder composed
             around 20 new library routines (S1.1), and rediscovered higher-order functions such as filter. Each
             round of abstraction built on concepts discovered in earlier sleep cycles — for example the model ﬁrst
             learns filter, then uses it to learn to take the maximum element of a list, then uses that routine to
             learn a new library routine for extracting the nth largest element of a list, which it ﬁnally uses to sort
             lists of numbers (Fig. 1B).
                Synthesizing programs that edit text is a classic problem in the programming languages and AI
             literatures (18), and algorithms that synthesize text editing programs ship in Microsoft Excel (7).
             These systems would, for example, see the mapping “Alan Turing” → “A.T.”, and then infer a
             program that transforms “Grace Hopper” to “G.H.”. Prior text-editing program synthesizers rely
             on hand-engineered libraries of primitives and hand-engineered search strategies. Here, we jointly
             learn both these ingredients and perform comparably to a state-of-the-art domain-general program
             synthesizer. We trained our system on 128 automatically-generated text editing tasks, and tested on
                                                                                               2
             the 108 text editing problems from the 2017 SyGuS (24) program synthesis competition. Prior to
             learning, DreamCoder solves 3.7% of the problems within 10 minutes with an average search time
             of 235 seconds. After learning, it solves 79.6%, and does so much faster, solving them in an average
             of 40 seconds. The best-performing synthesizer in this competition (CVC4) solved 82.4% of the
             problems —buthere, the competition conditions are 1 hour & 8 CPUs per problem, and with this more
             generous compute budget we solve 84.3% of the problems. SyGuS additionally comes with a different
             hand-engineered library of primitives for each text editing problem. Here we learned a single library of
             text-editing concepts that applied generically to any editing task, a prerequisite for real-world use.
                We next consider more creative problems: generating images, plans, and text. Procedural or
             generative visual concepts — from Bongard problems (25), to handwritten characters (5,26), to
             Raven’s progressive matrices (27) — are studied across AI and cognitive science, because they offer a
             bridge between low-level perception and high-level reasoning. Here we take inspiration from LOGO
             Turtle graphics (28), tasking our model with drawing a corpus of 160 images (split 50/50 test/train;
             Fig. 4A) while equipping it with control over a ‘pen’, along with imperative control ﬂow, and arithmetic
             operations on angles and distances. After training DreamCoder for 20 wake/sleep cycles, we inspected
             the learned library (S1.1) and found interpretable parametric drawing routines corresponding to the
             families of visual objects in its training data, like polygons, circles, and spirals (Fig. 4B) – without
             supervision the system has learned the basic types of objects in its visual world. It additionally learns
             more abstract visual relationships, like radial symmetry, which it models by abstracting out a new
             higher-order function into its library (Fig. 4C).
                Visualizing the system’s dreams across its learning trajectory shows how the generative model
             bootstraps recognition model training: As the library grows and becomes more ﬁnely tuned to the
             domain, the neural net receives richer and more varied training data. At the beginning of learning,
             random programs written using the library are simple and largely unstructured (Fig. 4D), offering
               2Wecomparewiththe2017benchmarksbecause2018onwardintroducednon-stringmanipulation problems; custom
             string solvers such as FlashFill (7) and the latest custom SyGuS solvers are at ceiling for these newest problems.
                                                         9
