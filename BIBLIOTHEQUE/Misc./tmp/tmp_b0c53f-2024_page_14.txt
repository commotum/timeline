                               14      C. He et al.
                               Table 6: Performance with various kernel configurations in the Cross-Window Inte-
                               gration (CWI) module. Each configuration is specified by the notation “KxDy", where
                               “Kx" refers to the kernel size, and “Dy" denotes the dilation rate. The APH (L2) scores
                               on different categories are reported.
                                                   Kernel          Latency Veh. Ped. Cyc.
                                                   Conv1DK13 ×2 47ms 71.1 70.9 73.1
                                                   Conv2DK5D1       49ms 69.5 69.0 71.1
                                                   Conv2DK5D2       49ms 70.7 70.4 72.8
                                                   Conv2DK7D1       65ms 71.5 71.0 72.5
                               Table 7: Performance with different linear attention designs. The APH (L2) scores on
                               different categories are reported.
                                                   Linear Attention        Veh. Ped. Cyc.
                                                   EfÏcient Attn [3]        71.1 70.9 73.1
                                                   Gated Linear Attn [32]   69.9 69.0 72.5
                                                   Focused Linear Attn [13] 70.4 69.5 72.3
                                                   XCA[1]                   71.5 70.4 72.8
                               Pedestrian and Cyclist classes. Focused Linear Attention introduces additional
                               computational overhead, while Gated Linear Attention adds more learnable pa-
                               rameters to the model.
                               4.6   Limitations
                               ScatterFormer relies on our customized operators, which have not yet been im-
                               plemented as plugins in TensorRT. Therefore, deploying ScatterFormer on in-
                               vehicle devices will require additional engineering efforts. Despite this, Scatter-
                               Former is more efÏcient than current models based on sparse convolution and
                               traditional attention when used on consumer-grade GPU cards. Furthermore,
                               ScatterFormer can be optimized by dynamically partitioning matrices according
                               to different GPU architectures to leverage TensorCore for hardware acceleration.
                               5    Conclusion
                               We introduced ScatterFormer, an innovative architecture designed for 3D ob-
                               ject detection using point clouds, specifically targeting the challenges associated
                               with processing sparse and unevenly distributed data from LiDAR sensors. The
                               cornerstone of our approach is the Scattered Linear Attention (SLA) module,
                               which effectively addresses the limitations of conventional attention mechanisms
                               in managing voxel features of varying lengths. SLA ingeniously combines linear
                               attention with a chunk-wise matrix multiplication algorithm, tailored to meet
                               the distinct requirements of processing voxels grouped by windows. By integrat-
                               ing SLA with a novel cross-window interaction (CWI) module, ScatterFormer
                               achieves higher accuracy and lower latency, surpassing traditional transformer-
                               based and sparse CNN-based detectors in extensive 3D detection tasks.
