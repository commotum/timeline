85.

86.

87.

88.

89.

90.

91.

92.

93.

94.

95.

96.

97.
98.

99.
100.

101.

Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In CLR, 2016.

Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R.
Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time
compute with latent reasoning: A recurrent depth approach, 2025.

Tiedong Liu and Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic
tasks. ArXiv, abs/2305.14201, 2023.

Alex Graves. Adaptive computation time for recurrent neural networks. ArXiv,
abs/1603.08983, 2016.

Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. ArXiv,
abs/2107.05407, 2021.

Chris Eliasmith, Terrence C Stewart, Xuan Choo, Trevor Bekolay, Travis DeWolf, Yichuan
Tang, and Daniel Rasmussen. A large-scale model of the functioning brain. science, 338
(6111):1202-1205, 2012.

James CR Whittington, Timothy H Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil
Burgess, and Timothy EJ Behrens. The tolman-eichenbaum machine: unifying space and
relational memory through generalization in the hippocampal formation. Cell, 183(5):1249â€”
1263, 2020.

Lars Buesing, Johannes Bill, Bernhard Nessler, and Wolfgang Maass. Neural dynamics as
sampling: a model for stochastic computation in recurrent networks of spiking neurons. PLoS
computational biology, 7.1 1):e1002211, 2011.

Salah Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term
dependencies. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, Advances in Neural
Information Processing Systems, volume 8. MIT Press, 1995.

Jan Koutnik, Klaus Greff, Faustino J. Gomez, and Jiirgen Schmidhuber. A clockwork rnn. In
International Conference on Machine Learning, 2014.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser.
Universal transformers, 2018. arXiv preprint arXiv:1807.03819.

Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng,
Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du,
and Yelong Shen. Reinforcement learning for reasoning in large language models with one
training example, 2025. URL https: //arxiv.org/abs/2504.20571.

Niklas Muennighoff. s1: Simple test-time scaling. arXiv preprint arXiv:2502.23456, 2025.
Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu,
Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng
Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025.
Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling, 2025.

Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms
through structured state space duality. ArXiv, abs/2405.21060, 2024.

Han Guo, Songlin Yang, Tarushii Goel, Eric P Xing, Tri Dao, and Yoon Kim. Log-linear
attention. arXiv preprint arXiv:2506.04761, 2025.

4
