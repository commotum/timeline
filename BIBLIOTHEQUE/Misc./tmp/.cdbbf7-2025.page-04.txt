                Figure 2. A key idea to use sparse voxel features instead of BEV features. BEV features, obtained from lower-resolution and z-axis
                suppressed features, rather can produce a comparable number of tokens to that of higher-resolution voxel features.
                the effectiveness of utilizing sparse multi-modal features in      model contains more sub-modules, in this backbone equa-
                this context.                                                      tion, for notational simplicity, we represent key modules.
                                                                                   Theentire process will be visualized in Fig. 4.
                3. Architecture of SparseVoxFormer
                Asourworkpresentanewparadigmof3Dobjectdetection                    Ourapproach Distinctfromthepreviousapproachesthat
                architecture (Fig. 1), which directly utilizes sparse voxel        useBEVfeatures,wedirectlyfeedsparse3Dvoxelfeatures
                features instead of BEV features, for comprehensive un-            into our 3D object detector (Fig. 2a). We can easily obtain
                derstanding, we first present our basic architecture essen-        3Dvoxelfeatureswithahigherresolutionbyomittingcom-
                tial for handling sparse features and then describe more           putations for sparse encoding (Sparse encoderb) and BEV
                sparsefeatures-specificarchitecture.Beforedelvingintothe           feature refinement (FCN) from Eq. 1:
                details, we first visit the key difference between obtaining           F      =
                                                                                        lidar                                                (2)
                BEVfeatures and our sparse voxel features.                             ϕsparse(L) = Sparse encoder (Voxelize(L)).
                                                                                        lidar                          f
                Previous BEV-based approaches          As a LiDAR data for            Theextraction of sparse 3D voxel features is straightfor-
                autonomous driving usually cover a wide area such as the           ward. The voxel features of a sparse LiDAR point cloud are
                range of [-54m, +54m] in x-, y-axes and the range of [-5m,         also sparse, signifying that the sparse features can be ob-
                +3m]inz-axis, raw LiDAR features need 3D voxels with a             tained by omitting zero-filled features and serializing valid
                highresolution(e.g.1440×1440×40)toeffectivelycapture               feature cells as follows:
                fine-level geometric details. However, utilizing such voxel                Fsparse = Flatten({f ∈ F          |f ̸= 0}).
                                                                                             lidar                       lidar
                features directly would be very cost-intensive, so that pre-       3.1. Transforemr Tokens from Sparse Voxel Fea-
                vious works reduce the feature resolution by transforming                tures
                the voxel features into BEV features with the resolution of
                180×180(Fig. 2b). In this process, the voxel features can          Thanks to the property of transformers that accepts serial-
                lose fine-level structural information and z-axis height in-       ized tokens from input data in any form, the transformer-
                formation. This LiDAR backbone is described as:                    based decoder can directly process our sparse 3D features
                                                                                   without the need for a regular topology. i.e., we can intactly
                          ϕbev (L) = FCN(Sparse encoder (                          employ a previous DETR-like transformer architecture [4]
                            lidar                                b        (1)
                             Sparse encoderf(Voxelize(L)))),                       used in CMT [39]. We feed the sparse feature with posi-
                                                                                   tional embedding to the transformer decoder of 3D object
                where L is input LiDAR sweeps and ϕbev             denotes a       detector as:
                                                             lidar
                LiDAR backbone model of the previous BEV-based ap-                                  oˆ = ϕ       (F′sparse,q),               (3)
                proach [39]. FCN denotes a fully convolutional net-                                       decoder   lidar
                work used in [40]. For the next derivation, we divide              where F′ denotes features F that is combined with the po-
                Sparse encoder into front and back parts and denote them           sitional embedding, and oˆand q denotes output cuboids and
                with subscripts f and b, respectively. While the backbone          initial transformer queries, respectively.
