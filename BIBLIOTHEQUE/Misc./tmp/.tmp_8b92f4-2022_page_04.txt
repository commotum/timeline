                             Under review as a conference paper at ICLR 2025
                    162      2018; Raffel et al., 2020; Li et al.) that measure the relative positions between tokens by modifying
                    163      the attention logits, and various hybrid methods (Su et al., 2024; Zhou et al., 2024). Vision Trans-
                    164      former research has adapted these concepts, implementing both APEs (Dosovitskiy et al., 2021) and
                    165      RPEs(Wuetal.,2021)toincorporate positional information about the image patches.
                    166
                    167      Solvers for the ARC.       Since the introduction of the ARC (Chollet, 2019), the development of
                    168      solvers has been an active research area. The earliest successful approaches consisted of an ex-
                    169      pressive Domain Specific Language (DSL) and a program synthesis algorithm that searched for a
                    170      validsolutionprogramexpressedintheDSL.TheseincludeDAG-basedsearch(Wind,2020),graph-
                    171      basedconstraint-guidedsearch(Xuetal.,2023),grammaticalevolution(Fischeretal.,2020),library
                    172      learning (Alford et al., 2021), compositional imagination (Assouel et al., 2022), inductive logic pro-
                    173      gramming(Hocquette&Cropper,2024),decisiontransformers(Parketal.,2023),generalizedplan-
                    174      ning (Lei et al., 2024), reinforcement learning (Lee et al., 2024), and several others (Ainooson et al.,
                                        ´
                    175      2023; Ferre, 2021). These models achieved up to 30% on the private ARC test set (Chollet et al.,
                    176      2020; Lab42, 2023).
                    177      Recently, Transformer-based Large Language Models (LLMs) were shown to exhibit an apparent
                    178      ability to perform “reasoning” (Wei et al., 2022) spurring interest in using LLMs as part of an ARC
                    179      solver. Such methods were prompted to perform program synthesis on a DSL (Min Tan & Motani,
                    180      2024; Barke et al., 2024) as well as general-purpose languages such as Python (Butt et al., 2024;
                    181      Wang et al., 2024), with the best-performing model achieving 42% on the public ARC evaluation
                    182      set (Greenblatt, 2024). LLMs were also explored as standalone solvers, where they were asked
                             to produce the output grids directly instead of outputting a program. Although pre-trained LLMs
                    183      proved ineffective when generating the output grid pixels directly (Camposampiero et al., 2023;
                    184      Mirchandani et al., 2023; Moskvichev et al., 2023), its performance was shown to be improved by
                    185      object representation (Xu et al., 2024). The vision variant of a state-of-the-art LLM, GPT-4V was
                    186      showntobeineffective (Mitchell et al., 2023; Xu et al., 2024).
                    187      Thecurrent state-of-the-art solver has achieved 46% on the private test set at the time of writing (ar-
                    188      cprize, 2024) but is not publicly available or described in detail. We do know that it is a pre-trained
                    189      LLM that is fine-tuned on millions of synthetic ARC tasks generated using the RE-ARC gener-
                    190      ator (Hodel, 2024) and combined with test-time fine-tuning (Cole & Osman, 2023). Despite the
                    191      visual nature of ARC tasks, Transformer-based LLM approaches convert the images into strings,
                    192      which does not fully capture all relevant structural information (Xu et al., 2024).
                    193
                    194      3    VANILLA VISION TRANSFORMER FOR THE ARC: AN INITIAL APPROACH
                    195
                    196      Wefirst implement a vanilla Vision Transformer architecture as detailed in Dosovitskiy et al. (2021)
                    197      and Touvron et al. (2021) as a solver for the ARC. Consider an input image I divided into P × P
                    198      non-overlapping patches. Each patch p is flattened in raster order and indexed by i before being
                    199                                                i
                             projected into a d-dimensional embedding space. Let h0 denote the initial input to the Transformer
                    200                                                                i
                    201      for patch pi. For the n-th Transformer layer, n ∈ {1,...,N}, and for a single attention head, the
                             following operations are performed:
                    202                                         0
                                                               h =E +E                                                            (1)
                                                                i      p      pos
                    203                                                 i       i
                    204                                       ˆn                   n−1
                                                              h =LayerNorm(h            )                                         (2)
                    205                                         i                  i
                                                        n   n   n    ˆn    n    ˆn    n     ˆn    n
                                                       q ,k ,v =h W , h W , h W                                                   (3)
                    206                                 i   i   i     i   q       i   k      i   v
                    207                                              qn · kn
                                                               n      i√ j
                                                             A =                                                                  (4)
                    208                                        i,j       d
                    209                                         n    X               n    n     n−1
                                                              o =        Softmax(A )v +h                                          (5)
                    210                                         i                    i,j  j     i
                    211                                               j
                                                                n                                 n       n
                                                              f =FeedForward(LayerNorm(o ))+o                                     (6)
                    212                                        i                                  i       i
                                                                n                  n
                                                              h =LayerNorm(f )                                                    (7)
                    213                                         i                  i
                    214
                             Here, E    is the embedding of patch p and E          is the positional encoding. Following the stan-
                    215               p                               i        pos
                                       i                                         i
                             dard ViT implementation of Dosovitskiy et al. (2021), the Absolute Positional Encoding (APE) is
                                                                                4
