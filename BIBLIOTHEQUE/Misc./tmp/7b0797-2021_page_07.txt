                                               Training data-efﬁcient image transformers & distillation through attention
                 Table 5. Throughput (images/s) vs accuracy on Imagenet (Rus-               Table 7. Ablation study on training methods on ImageNet (top-1
                 sakovsky et al., 2015), Imagenet Real (Beyer et al., 2020) and             acc.). The top row (”none”) corresponds to our default conﬁgura-
                 Imagenet V2 matched frequency (Recht et al., 2019) of models               tion employed for DeiT. The symbols ✓ and ✗ indicate that we
                 trained without external data. We compare DeiT and Vit-B (Doso-            use and do not use the corresponding method, respectively. We
                 vitskiy et al., 2020) to several state-of-the-art convnets: ResNet (He     report the accuracy scores (%) after the initial training at resolution
                 et al., 2016), Regnet (Radosavovic et al., 2020), EfﬁcientNet (Tan         224×224,andafter ﬁne-tuning at resolution 384×384. The hyper-
                 &Le, 2019; Cubuk et al., 2019; Wei et al., 2020). We use for               parameters are ﬁxed according to Table 9, and may be suboptimal.
                 each model the deﬁnition in the same GitHub (Wightman, 2019)               * indicates that the model did not train well, possibly because
                 repository. The reported results are from corresponding papers.            hyper-parameters are not adapted.
                                         nb of  image            ImNet    Real    V2                                                               vg.
                   Network             param.    size    im/s     top-1   top-1  top-1                                                             A
                                                                                                                                                        224     384
                   ResNet-18             12M     224    4458.4    69.8    77.3    57.1                                                     Aug.    ving
                   ResNet-50             25M     224    1226.1    76.2    82.5    63.3                                                Depth
                   ResNet-101            45M     224     753.6    77.4    83.7    65.7                                                             Mo
                   ResNet-152            60M     224     526.4    78.3    84.1    67.0
                   RegNetY-4GF⋆          21M     224    1156.7    80.0    86.4    69.4           Pre-trainingFine-tuningRand-AugmentAutoAugMixupCutMixErasingStoch.RepeatedDropoutExp.pre-trainedﬁne-tuned
                   RegNetY-8GF⋆          39M     224     591.6    81.7    87.4    70.8       adamw adamw       ✓ ✗ ✓ ✓ ✓             ✓ ✓ ✗ ✗ 81.8±0.2 83.1±0.1
                   RegNetY-16GF⋆         84M     224     334.7    82.9    88.1    72.4
                   EfﬁcientNet-B0         5M     224    2694.3    77.1    83.5    64.3        SGD adamw        ✓ ✗ ✓ ✓ ✓             ✓ ✓ ✗ ✗ 74.5 77.3
                   EfﬁcientNet-B1         8M     240    1662.5    79.1    84.9    66.9       adamw SGD         ✓ ✗ ✓ ✓ ✓             ✓ ✓ ✗ ✗ 81.8 83.1
                   EfﬁcientNet-B2         9M     260    1255.7    80.1    85.9    68.8       adamw adamw       ✗ ✗ ✓ ✓ ✓             ✓ ✓ ✗ ✗ 79.6 80.4
                   EfﬁcientNet-B3        12M     300     732.1    81.6    86.8    70.6       adamw adamw       ✗ ✓ ✓ ✓ ✓             ✓ ✓ ✗ ✗ 81.2 81.9
                   EfﬁcientNet-B4        19M     380     349.4    82.9    88.0    72.3       adamw adamw       ✓ ✗ ✗ ✓ ✓             ✓ ✓ ✗ ✗ 78.7 79.8
                   EfﬁcientNet-B5        30M     456     169.1    83.6    88.3    73.6
                   EfﬁcientNet-B6        43M     528       96.9   84.0    88.8    73.9       adamw adamw       ✓ ✗ ✓ ✗ ✓             ✓ ✓ ✗ ✗ 80.0 80.6
                   EfﬁcientNet-B7        66M     600       55.1   84.3                       adamw adamw       ✓ ✗ ✗ ✗ ✓             ✓ ✓ ✗ ✗ 75.8 76.7
                   EfﬁcientNet-B5 RA     30M     456       96.9   83.7                       adamw adamw       ✓ ✗ ✓ ✓ ✗             ✓ ✓ ✗ ✗            4.3*    0.1
                   EfﬁcientNet-B7 RA     66M     600       55.1   84.7                       adamw adamw       ✓ ✗ ✓ ✓ ✓             ✗ ✓ ✗ ✗            3.4*    0.1
                   KDforAA-B8            87M     800       25.2   85.8                       adamw adamw       ✓ ✗ ✓ ✓ ✓             ✓ ✗ ✗ ✗ 76.5 77.4
                                     Transformers: training 300 epochs                       adamw adamw       ✓ ✗ ✓ ✓ ✓             ✓ ✓ ✓ ✗ 81.3 83.1
                   ViT-B/16              86M     384       85.9   77.9    83.6               adamw adamw       ✓ ✗ ✓ ✓ ✓             ✓ ✓ ✗ ✓ 81.9 83.1
                   ViT-L/16             307M     384       27.3   76.5    82.2
                   DeiT-Ti                5M     224    2536.5    72.2    80.1    60.4
                   DeiT-S                22M     224     940.4    79.8    85.7    68.5
                   DeiT-B                86M     224     292.3    81.8    86.7    71.5
                   DeiT-B↑384            86M     384       85.9   83.1    87.7    72.4              Method      RegNetY-16GF         DeiT-B     DeiT-B
                                                                                                                                                        ⚗
                   DeiT-Ti                6M     224    2529.5    74.5    82.1    62.9               Top-1            98.0            97.5         98.5
                         ⚗
                   DeiT-S                22M     224     936.2    81.2    86.8    70.0
                         ⚗
                   DeiT-B                87M     224     290.9    83.4    88.3    73.2
                         ⚗                                                                  For this experiment, we tried we get as close as possible
                   DeiT-B ↑384           87M     384       85.8   84.5    89.0    74.8
                         ⚗                                                                  to the Imagenet pre-training counterpart, meaning that (1)
                                     Transformers: training 1000 epochs
                                                                                            weconsider longer training schedules (up to 7200 epochs,
                   DeiT-Ti                6M     224    2529.5    76.6    83.9    65.4
                         ⚗                                                                  which corresponds to 300 Imagenet epochs) so that the
                   DeiT-S                22M     224     936.2    82.6    87.8    71.7
                         ⚗
                   DeiT-B                87M     224     290.9    84.2    88.7    73.9
                         ⚗                                                                  network has been fed a comparable number of images in
                   DeiT-B ↑384           87M     384       85.8   85.2    89.3    75.2
                         ⚗                                                                  total; (2) we re-scale images to 224 × 224 to ensure that we
                 ⋆: our trained teachers with SGD, whose optimization procedure is closer to DeiT
                                                                                            have the same augmentation. The results are not as good
                                                                                            as with Imagenet pre-training (98.5% vs 99.1%), which is
                 Table 6. WecompareTransformersbasedmodelsondifferenttrans-                 expected since the network has seen a much lower diversity.
                 fer learning task with ImageNet pre-training. We also give results         Howevertheyshowthatitis possible to learn a reasonable
                 obtained with Efﬁcient-B7 for reference (Tan & Le, 2019).                  transformer on CIFAR-10 only.
                                             AR-10 AR-100 wers                              6. Training details & ablation
                   Model              ImageNetCIF  CIF    Flo   Cars   iNat-18iNat-19im/sec
                   EfﬁcientNet-B7   84.3  98.9   91.7  98.8   94.7                 55.1     This section discusses the DeiT training strategy to learn vi-
                   ViT-B/32         73.4  97.8   86.3  85.4                       394.5     sion transformers in a data-efﬁcient manner. We build upon
                                                                                                                                               3
                   ViT-B/16         77.9  98.1   87.1  89.5                        85.9     PyTorch (Paszke et al., 2019) and the timm library (Wight-
                   ViT-L/32         71.2  97.9   87.1  86.4                       124.1     man, 2019). We provide hyper-parameters and an ablation
                   ViT-L/16         76.5  97.9   86.4  89.7                        27.3     study in which we analyze the impact of each choice.
                   DeiT-B           81.8  99.1   90.8  98.4   92.1  73.2   77.7   292.3
                   DeiT-B↑384       83.1  99.1   90.8  98.5   93.3  79.5   81.4    85.9         3The timm implementation includes a training procedure that
                   DeiT-B           83.4  99.1   91.3  98.8   92.9  73.7   78.4   290.9
                         ⚗                                                                  improved the accuracy of ViT-B from 77.91% to 79.35% top-1,
                   DeiT-B ↑384      84.4  99.2   91.4  98.9   93.9  80.1   83.0    85.9
                         ⚗                                                                  and trained on Imagenet-1k with a 8xV100 GPU machine.
