                                                  Training data-efﬁcient image transformers & distillation through attention
                  Table 4. Disagreement analysis between convnet, image transform-
                  ers and distillated transformers: We report the fraction of sample
                  classiﬁed differently for all classiﬁer pairs, i.e., the rate of different
                  decisions. We include two models without distillation (a RegNetY
                  and DeiT-B), so that we can compare how our distilled models and
                  classiﬁcation heads are correlated to the RegNetY teacher.
                                              no distillation          DeiT student
                                                                            ⚗
                                             convnet     DeiT     class    distil.  DeiT
                                                                                          ⚗                                                                         ⚗
                   groundtruth                0.171     0.182     0.170    0.169     0.166                                                                          ⚗↑
                   convnet (RegNetY)          0.000     0.133     0.112    0.100     0.102
                   DeiT                       0.133     0.000     0.109    0.110     0.107
                   DeiT –class only           0.112     0.109     0.000    0.050     0.033
                         ⚗
                   DeiT –distil. only         0.100     0.110     0.050    0.000     0.019
                         ⚗
                   DeiT –class+distil.        0.102     0.107     0.033    0.019     0.000
                         ⚗
                                                                                                  Figure 3. Distillation on ImageNet1k with DeiT-B: top-1 accuracy
                                                                                                  as a function of the training epochs. The performance without
                                                                                                  distillation (horizontal dotted line) saturates after 400 epochs.
                  similar but not identical.
                  Weveriﬁed that our distillation token adds something to
                  the model, compared to simply adding an additional class                        Our method DeiT is slightly below EfﬁcientNet, which
                  token associated with the same target label: instead of a                       shows that we have almost closed the gap between vision
                  teacher pseudo-label, we experimented with a transformer                        transformers and convnets when training with Imagenet
                  with two class tokens. Even if we initialize them randomly                      only. These results are a major improvement (+6.3% top-1
                  and independently, during training they converge towards                        in a comparable setting) over previous ViT models trained
                  the same vector (cos=0.999), and the output embedding are                       onImagenet1konly(Dosovitskiyetal.,2020). Furthermore,
                  also quasi-identical. In contrast to our distillation strategy,                 when DeiT beneﬁts from the distillation from a relatively
                                                                                                  weaker RegNetY to produce DeiT , it outperforms Efﬁ-
                  an additional class token does not bring anything to the                                                                     ⚗
                  classiﬁcation performance.                                                      cientNet. It also outperforms by 1% (top-1 acc.) the Vit-B
                                                                                                  model pre-trained on JFT300M at resolution 384 (85.2% vs
                  Numberofepochs. Increasingthenumberofepochssig-                                 84.15%), while being signiﬁcantly faster to train.
                  niﬁcantly improves the performance of training with distilla-                   Table 5 reports the numerical results in more details and
                                                               2                                  additional evaluations on ImageNet V2 and ImageNet Real,
                  tion, see Figure 3. With 300 epochs , our distilled network
                  DeiT-B       is already better than DeiT-B. But while for the                   that have a test set distinct from the ImageNet validation,
                            ⚗
                  latter the performance saturates with longer schedules, the                     which reduces overﬁtting on the validation set. Our results
                                                                                                  showthat DeiT-B          and DeiT-B ↑384outperform, by some
                  distilled network beneﬁts from a longer training time.                                                ⚗                ⚗
                                                                                                  margin, the state of the art on the trade-off between accuracy
                  5.3. Efﬁciency vs accuracy: a comparison to convnets                            and inference time on GPU.
                  In the literature, image classiﬁcaton methods are often com-                    5.4. Transfer learning to downstream tasks
                  pared as a compromise between accuracy and another cri-
                  terion, such as FLOPs, number of parameters, size of the                        Although DeiT perform very well on ImageNet it is impor-
                  network, etc. We focus in Figure 1 on the tradeoff between                      tant to evaluate them on other datasets with transfer learning
                  the throughput (images per second) and the top-1 classiﬁ-                       in order to measure the power of generalization of DeiT.
                  cation accuracy on ImageNet. The throughput is measured                         Weevaluated this on transfer learning tasks by ﬁne-tuning
                  as the number of images that we can process per second on                       onthe datasets in Table 8. Table 6 compares DeiT transfer
                  one 16GB V100 GPU: we take the largest possible batch                           learning results to those of ViT and EfﬁcientNet. DeiT is on
                  size and average the processing time over 30 runs. We focus                     par with competitive convnet models, which is in line with
                  on the popular EfﬁcientNet convnet, which has beneﬁted                          our previous conclusion on ImageNet1k.
                  from years of research on convnets and was optimized by
                  architecture search on the ImageNet validation set.                             Comparison vs training from scratch.                   We investigate
                      2Formally we have 100 epochs, but each is 3x longer because                 the performance when training from scratch on a small
                  of the repeated augmentations. We prefer to refer to this as 300                dataset, without Imagenet pre-training. We get the following
                  epochsinordertohaveadirectcomparisonontheeffectivetraining                      results on the small CIFAR-10, which is small both w.r.t.
                  time with and without repeated augmentation.                                    the number of images and labels:
