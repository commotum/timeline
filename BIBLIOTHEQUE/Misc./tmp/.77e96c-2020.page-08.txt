                                        Table 1: A summary of the quantitative evaluation for the ﬁve PDE scenarios. SOL denotes a variant
                                                                                                                                                               s
                                                                                                            ∗
                                        with shorter look-ahead compared to SOL. ( For the CG solver scenario, iterations to reach an
                                        accuracy of 0.001 are given. Here, SOLs denotes the physics-based loss version.)
                                                 Exp.                                 Meanabsoluteerrorofvelocity                                          Rel. improvement
                                                                  SRC              PRE              NON              SOL                SOL          PRE     NON SOL            SOL
                                                                                                                          s                                                s
                                             WakeFlow        0.146±0.004       0.031±0.010      0.049±0.012       0.041±0.009      0.013±0.003       79%     67%       72%      91%
                                              Buoyancy       1.590±1.033       1.373±0.985      1.080±0.658       0.944±0.614      0.620±0.390       19%     29%       41%      60%
                                              Adv.-diff.     0.248±0.019       0.218±0.017      0.159±0.015       0.152±0.015      0.158±0.017       12%     36%       39%      36%
                                             ∗CGSolver       121.6±13.44             -                 -          79.03±10.02      29.59±14.83         -       -       35%      76%
                                               3DWake        0.167±0.061             -          0.144±0.074             -          0.130±0.058         -     14%         -      22%
                                        128 yields average improvements of 60%. Our tests consistently show that, without changing the
                                        numberofweightsorthearchitecture of a network, the gradients provided by the longer rollout times
                                        allow the network to anticipate the behavior of the physical system better and react to it. Throughout
                                        our tests, similar performances could not be obtained by other means.
                                        Generalization             Thebuoyancyscenario also highlights the very good generalizing capabilities of
                                        the resulting models. All test simulations were generated with an out-of-distribution parametrization
                                        of the initial conditions, leading to substantially different structures, and velocity ranges over time.
                                        Training with Noise                Aninteresting variant to stabilize physical predictions in the context of Graph
                                        Network-based Simulators was proposed by Sanchez et al. [48]. They report that perturbations
                                        of input features with noise lead to more stable long-term rollouts. We mimic this setup in our
                                        Eulerian setting by perturbing the inputs to the neural networks with N(0,σ) for varying strengths
                                                                                                                                                                  −4
                                        σ. While a sweet spot with improvements of 34.5% seems to exist around σ = 10                                                 , the increase
                                        in performance is small compared to a model with less perturbations (30.6%), as training with an
                                        increased look-ahead for the SOL models gives improvements up to 60.0%.
                                        Training Stability              Thephysical models we employ introduce a large amount of complexity into
                                        the training loop. Especially during the early stages of training, an inferred correction can overly
                                        distort the physical state. Performing time integration via the PDE then typically leads to exponential
                                        increases of existing oscillations and a diverging calculation. Hence, we found it important to pre-train
                                        networks with small look-aheads (we usually use SOL models), and then continue training with
                                                                                                                          2
                                        longer recurrent iterations for the look-ahead. While this scheme can be applied hierarchically, we
                                        sawnospeciﬁcgains from, e.g., starting a SOL                            training with a SOL model versus a SOL                           model.
                                                                                                            32                               2                               16
                                        RuntimePerformance Thetrainingviadifferentiable physics incurs an increased computational
                                        cost at training time, as the PDE model has to be evaluated for n steps for each learning iteration, and
                                        the calculation of the gradients is typically of similar complexity as the evaluation of the PDE itself.
                                        However, this incurs only moderate costs in our tests. For example, for the buoyancy-driven ﬂow, the
                                        training time increases from 0.21 seconds per iteration on average for SOL to 0.42s for SOL , and
                                                                                                                                                      2                          4
                                        1.25s for SOL . The look-ahead additionally provides n times more gradients at training time, and
                                                             16
                                        the inference time of the resulting models is not affected. Hence, the training cost can quickly pay off
                                        in practical scenarios by yielding more accurate results without any increase in cost at inference time.
                                        Computingsolutions with the resulting hybrid method which alternates PDE evaluations and ANN
                                        inference also provides beneﬁts in terms of evaluation performance: A pre-trained, fully convolutional
                                        CNNhasanO(n)costforndegreesoffreedom,incontrasttomanyPDE-solverswithasuper-linear
                                        complexity. For example, a simulation as shown in Fig. 1 involving the trained model took 13.3s on
                                        average for 100 time steps, whereas a CPU-based reference simulation required 913.2s. A speed-up
                                        of more than 68×.
                                        6      Conclusions
                                        Wehavedemonstratedhowtoachievesigniﬁcant reductions of numerical errors in PDE-solvers by
                                        training ANNs with long look-ahead rollouts and differentiable physics solvers. The resulting models
                                        yield substantially lower errors than models trained with pre-computed data. We have additionally
                                        provided a ﬁrst thorough evaluation of different methodologies for letting PDE-solvers interact with
                                        recurrent ANN evaluations.
                                                                                                                8
