           Preprint, Under Review.
           ShunyuYao,Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao.
            React: Synergizing reasoning and acting in language models. In The Eleventh International Confer-
            ence on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,
            2023b. URLhttps://openreview.net/forum?id=WE_vluYUL-X.
           Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with
            reasoning. Advances in Neural Information Processing Systems, 35:15476â€“15488, 2022.
           Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman.
            Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint
            arXiv:2403.09629, 2024.
           AndrewZhao,YiranWu,YangYue,TongWu,QuentinXu,YangYue,MatthieuLin,ShenzhiWang,
            Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with
            zero data. arXiv preprint arXiv:2505.03335, 2025.
           Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and
            Xian Li. SWEET-RL: training multi-turn LLM agents on collaborative reasoning tasks. CoRR,
            abs/2503.15478, 2025. URL https://doi.org/10.48550/arXiv.2503.15478.
                               14
