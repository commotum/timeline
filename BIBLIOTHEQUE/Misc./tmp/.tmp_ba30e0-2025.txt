                                                                   ARCIsaVisionProblem!
                                                  KeyaHu           Ali Cy        Linlu Qiu         XiaomanDeloresDing
                                            Runqian Wang              Yeyin Eva Zhu            Jacob Andreas            Kaiming He
                                                                                       MIT
                                              Abstract                                           Training tasks                               Test task
                  The Abstraction and Reasoning Corpus (ARC) is designed
                  to promote research on abstract reasoning, a fundamen-
                  tal aspect of human intelligence. Common approaches to
                  ARCtreat it as a language-oriented problem, addressed by
                  large language models (LLMs)orrecurrentreasoningmod-
                  els. However, although the puzzle-like tasks in ARC are in-
                  herently visual, existing research has rarely approached the
                  problem from a vision-centric perspective. In this work, we
                                                                                                                                                   model prediction
                  formulate ARC within a vision paradigm, framing it as an                                       [TASK]
                  image-to-image translation problem. To incorporate visual
                                                                                                                           VARC network
                  priors, we represent the inputs on a “canvas” that can be
                  processed like natural images. It is then natural for us to
                  apply standard vision architectures, such as a vanilla Vi-                  Figure 1. The ARC benchmark (top) consists of a collection of
                  sion Transformer (ViT), to perform image-to-image map-                      many different tasks, where each task has a few (e.g., 2-4) exam-
                  ping. Our model is trained from scratch solely on ARC                       ples. We propose the Vision ARC (VARC) framework, which ad-
                  dataandgeneralizestounseentasksthroughtest-timetrain-                       dresses the ARC problem as an image-to-image translation prob-
                  ing. Our framework, termed Vision ARC (VARC), achieves                      lem, from a computer vision perspective (bottom). In this illus-
                  60.4% accuracy on the ARC-1 benchmark, substantially                        tration, the underlying concepts of the three tasks can be roughly
                  outperforming existing methods that are also trained from                   described by humans as: “reflection” (left), “symmetry” (middle),
                                                                                              and “gravity” (right). These concepts are closely related to the vi-
                  scratch. Our results are competitive with those of leading                  sual and physical world.
                  LLMsandclosethegaptoaveragehumanperformance.1
                  1. Introduction                                                             Among a wide variety of methods, those based on large
                                                                                              language models (LLMs) have proven highly competi-
                  Learning and abstracting concepts from a small number of                    tive. These methods generally convert ARC inputs into se-
                  demonstrations is a key feature of intelligence. The Ab-                    quences of text tokens for language modeling. Representa-
                  stractionandReasoningCorpus(ARC)benchmark[12]was                            tive methods may involve inductive reasoning [54, 7, 50, 6],
          arXiv:2511.14761v1  [cs.CV]  18 Nov 2025designed to incentivize machine learning research aimed attransductive reasoning [1, 19, 45], or a combination of both
                  improving these capabilities. ARC consists of a collection                  [35,8,40]. TheLLMsarepre-trainedoninternet-scaledata,
                  of puzzle-like tasks (Fig. 1, top), each containing only a                  from which they learn transferable common sense.
                  few examples governed by a unique underlying transfor-                          Most recently, research on recurrent models [53, 27]
                  mation rule. The model is expected to make predictions                      has achieved impressive results on ARC without relying on
                  on each unseen task given a few examples. While humans                      internet-scale data. These models are trained from scratch
                  are capable of solving various ARC tasks [25, 31, 32], the                  onARCdataonlyandperforminferencethroughrecurrent,
                  benchmark remains highly challenging for today’s leading                    iterative reasoning. Although they do not rely on large-scale
                  machine learning systems [44, 42].                                          language pre-training, these recurrent models draw strong
                     TheARCproblemhasattractedsignificantattention,and                        inspiration from the success of language modeling.
                  substantial progress has been made in recent years [13].                        Interestingly, although the ARC puzzles are typically
                                                                                              presentedvisually, existing research has rarely framed ARC
                     1Project webpage: https://github.com/lillian039/VARC.                    as a vision-centric problem. In fact, many concepts in ARC
                                                                                          1
                                      ARC-1                                                                                    ARC-2
                                                                                                                                                                                      Combining VARCmodelsthrough ensembling [29] further
                                                                                                                                                                                      improvesaccuracyto60.4%,matchingthereportedaverage
                                                                                                                                                                                      humanperformance[31] on the ARC-1 dataset.
                                                                                                                                                                                             Wehopeour research will shed light on the ARC prob-
                                                                                                                                                                                      lem, and more broadly, on the field of abstract reasoning.
                                                                                                                                                                                      Ontheonehand,thedesignoftheARCbenchmarkisbased
                                                                                                                                                                                      on human observations and induced rules abstracted from
                                                           model prediction                            model prediction                               model prediction                the visual and physical world. It is natural to explore vision-
                                                                                                                                                                                      driven approaches for ARC. On the other hand, human rea-
                                                                                                                                                                                      soning is not confined to language or vision in isolation,
                                                                                                                                                                                      but instead should integrate information across modalities.
                                  Figure 2. Examples of unseen tasks solved by VARC. Each                                                                                             Withourcomplementaryvision-basedperspective,wehope
                                  panel shows an unseen test task, with demonstrations on the top                                                                                     the scope of abstract reasoning will be further broadened.
                                  and the model’s prediction on the bottom. VARC correctly solves                                                                                     Weinvite the vision community to study the ARC problem
                                  these challenging tasks.                                                                                                                            and to advance research on abstract reasoning.
                                  are inherently visual and physical: e.g., reflection, symme-                                                                                        2. Related Work
                                  try, and gravity, as shown in Fig. 1. Humans can solve these                                                                                        Visual reasoning. Visual reasoning is a long-standing re-
                                  tasks not merely from the demonstrations, but by reason-                                                                                            search problem. It involves not only perceiving scenes and
                                  ing through analogy to their common sense obtained from                                                                                             objects, but also inferring and abstracting the relations and
                                  external experience. Such common sense can be acquired                                                                                              transformations among them.                                                The advancement of ma-
                                  through observing the world, particularly, the visual world.                                                                                        chine learning methods has led to the development of a
                                         Motivated by its visual nature, we approach ARC from                                                                                         variety of challenging protocols, such as VQA [5, 56, 20],
                                  a vision-centric perspective. We frame each puzzle as an                                                                                            CLEVR[26],andWinoground[51].
                                  image-to-image translation problem. Abstraction and infer-                                                                                                 The visual reasoning methods developed under these
                                  encecanarisedirectlyfromvisuallearning,withoutexplicit                                                                                              protocols typically consist of a visual perception mod-
                                  linguistic intermediates. This perspective connects ARC to                                                                                          ule and a language-like recurrent module, e.g., within the
                                  classical image-to-image problems, ranging from low-level                                                                                           neuro-symbolic framework [4, 23, 3, 41]. These methods
                                  imageprocessing (e.g., [16, 43]) to high-level image under-                                                                                         have evolved into modern vision-language models (VLMs,
                                  standing (e.g., [38, 46]). With this connection, we can ap-                                                                                         e.g., [2, 33, 37]), in which images are converted into tokens
                                  ply standard vision models (e.g., Vision Transformers [17]                                                                                          and processed jointly with text.
                                  or convolutional networks [30]) to tackle the ARC problem.                                                                                                 Unlike ARC, classical visual reasoning protocols gener-
                                         We demonstrate that incorporating visual priors is cru-                                                                                      ally involve a training set and a test set, both of which can
                                  cial.         These priors include 2D spatial locality, translation                                                                                 be viewed as instances of the same task. In contrast, ARC
                                  invariance, and scale invariance. To facilitate learning these                                                                                      consists of a large collection of distinct tasks, each defined
                                  priors, we represent the inputs on a “canvas” with flexible                                                                                         byonlyafewexamples.
                                  geometric transformations, allowing the inputs to be pro-                                                                                           Approaches to ARC. Owing to the “few-shot, many-task”
                                  cessed as if they were natural images. A patch on the can-                                                                                          nature of ARC, LLMshavebeenregardedasanaturalsolu-
                                  vas can consist of exponentially many color combinations,                                                                                           tion. A newtaskcanbeconvertedintoasequenceoftokens,
                                  whichhelps reduce overfitting and encourages the model to                                                                                           treated as a prompt, and processed by LLMs via in-context
                                  learn spatial priors rather than merely memorize.                                                                                                   few-shot learning [55, 10]. We refer the reader to [13] for a
                                         With the vision-centric formulation, we train our model                                                                                      comprehensive survey.
                                  fromscratchusingARC-onlydata. Atinferencetime,when                                                                                                         Recently, recurrent models [53, 27] have been proven
                                  presented with a new, unseen task, we perform test-time                                                                                             effective for ARC, without the requirement of internet-scale
                                  training [9, 24, 49, 1, 53, 27] to adapt the model to the task,                                                                                     pre-training. These models aim to mimic the hierarchical
                                  enabling it to generalize from only a few examples.                                                                                                 and multi-timescale processing of the human brain [53] for
                                         Our framework, termed Vision ARC (VARC), shows                                                                                               reasoning. At inference time, these methods adopt test-time
                                  strong performance on the ARC benchmarks (e.g., Fig. 2).                                                                                            training [9, 24, 49] on the few demonstration examples.
                                  VARCachieves54.5%accuracyontheARC-1benchmark,                                                                                                              Related to our work, the ViT-ARC method [34] attempts
                                  using a small model with only 18 million parameters. This                                                                                           to address the ARC problem using vision models. How-
                                  result substantially surpasses the best recurrent methods                                                                                           ever, this method has only shown the ability to fit individual
                                  [53, 27] that are also trained from scratch on ARC. It is                                                                                           tasks in the training set; it is unable to generalize or solve
                                  also competitive with many popular LLM-based methods.                                                                                               any unseen test task. As such, this method has not been
                                                                                                                                                                              2
                                                                                                                                                              T
                            Training Set                            Test Set                                      there also exists a demo set D                   , and the pairs (x,y) in
                                                                                                                                                              demo
                             Task 1              Task 400
                                                                                         Task 400
                                                                     Task 1                                       DT       are given to the model at inference time. The model
                                                                                                                    demo
                                                                                                                  should make use of DT                  to infer the output of the given
                                                                                                                                                  demo
                                                                                                                  xinfer for this new task.
                                              ......                                 ......                           The presence of new (x,y) pairs in DT                          at inference
                                                                                                                                                                              demo
                                                                                                                  time allows to perform test-time training [49, 1, 9, 24],
                                                                                                                  which we adopt and will discuss.
                     Figure 3. The ARC problem definition. ARC is a collection of                                 3.2. Image-to-Image Translation
                     manydifferenttasks. Foreachtask,afew(e.g.,2-4)demonstration                                  Withthesedefinitions, we formulate reasoning on each task
                     pairs (x,y) are given, and the model is required to infer the output                         as an image-to-image translation problem. We frame the
                     fromx        . ThetrainingsetT            is a collection of 400 tasks, which
                              infer                       train                                                   problemasper-pixelclassification, analogous to the seman-
                     can be used for model training. The test set T                contains 400 new
                                                                               test
                     tasks: the demo pairs of a new task are given only at inference                              tic segmentation problem [38].
                     time, based on which the model performs inference on x                       .                   Formally, welearnaneuralnetworkf parameterizedby
                                                                                              infer                                                                        θ
                                                                                                                  θ. The network f takes an image x as input, conditioned
                                                                                                                                           θ                          i
                     able to satisfy the ARC protocol, whose essence lies pre-                                    on a task token associated with the task T. The task token
                     cisely in few-shot, cross-task generalization. Unlike [34],                                  is represented as a learnable embedding dependent on T.
                     our framework is designed to address the “few-shot, many-                                    The output of fθ is a grid where each position represents
                     task” nature of ARC.                                                                         a categorical distribution. The overall objective function is
                                                                                                                  simply the per-pixel cross-entropy loss [38]:
                     3. ARCasaVisionProblem                                                                                                                                     
                                                                                                                                    L(θ) = E           D(y , f (x | T)) .                         (1)
                                                                                                                                                  T,i        i    θ    i
                     3.1. ARC Problem Definition                                                                  Here, D denotes the per-pixel cross-entropy loss between
                     TheARCbenchmarkconsistsofseveral hundred very few-                                           the ground-truth y and the network output.
                     shot (e.g., 2 to 4-shot) reasoning tasks. Each task, denoted                                                          i
                     by T, involves a unique underlying transformation rule,                                      3.3. Visual Modeling
                     mapping from an input x to an output y. Here, x and y are                                    Previous methods on ARC generally operate in the space of
                     both 2D grids with maximum size 30×30, in which each                                         discrete-valued tokens, motivated by the design of language
                     location has one of C different color indexes (e.g., C=10).                                  models. In our formulation of image-to-image translation,
                     The ARC problem definition is illustrated in Fig. 3, which                                   weexplore native designs developed for vision.
                     wediscuss next.                                                                              Canvas. While it is straightforward to view the raw H×W
                     Atask. A “task” is the basic unit in ARC. Each task in-                                      grid as an H×W image, weproposemoreflexibletransfor-
                     cludes a few demonstration examples. For a demonstration                                     mationstorepresentitinamannersimilartonaturalimages.
                     pair (x,y), both x and y are known to the model. We denote
                     the demonstration set of task T as: DT                       =(x ,y )	m ,                       We define the concept of a “canvas”. A canvas has a
                                                                             demo          i   i    i=1           predefined and sufficiently large size, e.g., 64×64. The raw
                     where m is the number of pairs (e.g., m is 2 to 4). Each                                     input is transformed and placed onto this canvas. This for-
                     task T also contains a few inference examples, denoted as:
                                           	                                                                     mulationnaturallyaccommodatestranslationandscaleaug-
                     DT = (x ,y ) n                 (n is 1 or 2). At inference time, only
                        infer        i    i   i=1                                                                 mentations, which are common strategies for introducing
                     the demopairs DT               andoneinputx                ∈DT aregiven,
                                              demo                        infer       infer                       translation and scale invariance in vision, discussed next.
                     and the model is required to infer the desired output y                          .
                                                                                                  infer           Weset the background of the canvas to an additional back-
                     Training set. The training set consists of multiple tasks                                    ground color, i.e., the (C+1)-th color.
                     used to train the model offline (i.e., before a new task is                                      When applying a ViT model (discussed next), if we
                                                                                          k
                     given). Wedenotethetrainingsetas: T                         ={T }         , where               ¨
                                                                            train      i i=1                      naıvely treat each raw pixel as a token, there would be only
                     k is the number of tasks (400 in ARC-1). Following stan-                                     C distinct tokens. In contrast, our canvas formulation sup-
                     dard machine learning protocols, samples in DT                            for any            ports a much larger set of local, patch-level configurations.
                                                                                         demo
                     T ∈Ttrain can be used for training. The “inference” samples                                  For example, with a patch size of 2×2 (see Fig. 5), a single
                     in the training set, that is, DT              for any task T ∈ T              , are          patch can contain multiple colors and, in principle, has an
                                                              infer                            train
                     used for validating the training process only.                                               exponentially large cardinality, O(C2×2). This formulation
                     Test set. The test set is a collection of new tasks, which are                               is important for improving generalization performance.
                     not seen during offline training. We denote the test set as:                                 Translation and scale invariance. The “canvas” concept
                                    l
                     Ttest={Ti}          , with l different test tasks. Note that any test                        enables us to flexibly apply translation and scale augmen-
                                    i=1
                     task is a “complete” and new task: that is, for any T ∈ T                           ,        tations, which are critical in standard vision models. The-
                                                                                                     test
                                                                                                             3
                   Scale                  Translation
                                                                                                 place on canvas
                Figure 4. The raw input undergoes random scale and translation           [task]
                transformations and is placed on the “canvas” (denoted in gray).
                                                                                                                  ...                 ...
                ses data augmentations encourage the model to learn un-                                        patch embedding
                derlying mappings invariant to geometric transformations
                                                                                                                  ...                 ...
                grounded in the visual world. Formally, we perform:
                • Scale augmentation: Given a raw input, we randomly                                         Transformer block
                                                                                                                  ...
                   resize it by an integer scaling ratio s, duplicating each
                                                                                                             Transformer block
                   raw pixel into s×s (see Fig. 4, left). This is analo-
                                                                                                                  ...
                   gous to nearest-neighbor interpolation in natural im-                                                              ...
                   ages. However, note that “colors” in ARC do not cor-
                                                                                                                  predictor
                   respond to real-world colors, so it is not meaningful to
                                                                                                                                      ...
                   perform other interpolations (such as bilinear).                                               ...
                • Translation augmentation: given the scaled grid, we
                   randomly place it on the fixed-size canvas. We ensure
                   all pixels are visibile. See Fig. 4 (right).
                Weempirically show that these visual priors are important
                for generalization to unseen tasks.                                                  off canvas
                Vision Transformer. Given a canvas with an input ran-
                domly placed, we perform image-to-image translation by a            Figure 5. The ViT architecture in VARC. The input is randomly
                standard vision model. By default, we use a ViT [17].               placed on a canvas, which is then treated as a natural image and
                   The principle of ViT is Transformer on patches. For-             processed by a standard ViT, conditioned on the task token.
                mally, the input canvas is divided into non-overlapping
                patches (e.g., 2×2), projected by a linear embedding, added         vertical coordinate. This can be applied both to additive po-
                with positional embedding [52], and processed by a stack            sitional embeddings for encoding absolute positions and to
                of Transformer blocks [52]. The model has a linear projec-          the encoding of relative positions (e.g., RoPE [48]).
                tion layer as the output, which performs per-pixel classifica-      Alternative: convolutional networks. Beyond ViT, we
                tion for each patch. Note that unlike natural images where          also study the more classical vision-based architecture, i.e.,
                each raw pixel has continuous values, in our case, the raw          convolutional neural networks [30]. Specifically, we adopt
                pixels have discrete values. Therefore, before patchifica-          the U-Netmodel[46],ahierarchicalconvolutionalnetwork.
                tion, we first map each pixel’s discrete index into a learnable     The original U-Net was proposed precisely for the image-
                continuous-valued embedding.                                        to-image translation problem of segmentation [46], making
                   Conceptually, patchification can be viewed as a special          it a natural candidate for the problem we consider.
                form of convolution. Like convolution, it incorporates sev-
                eral critical inductive biases in vision: most notably, local-      3.4. Two-stage Training
                ity (i.e., grouping nearby pixels) and translation invariance
                (i.e., weight sharing across locations).                            Weadoptatwo-stagetrainingparadigmtolearntheparam-
                2Dpositional embedding. Unlike language data, which is              eters of the neural network.
                generally modeled as 1D sequences, images are inherently            Offline training. This stage is applied on the entire train-
                                                               ¨                                                         T
                2D. This 2D structure can be lost if we naıvely treat the           ing set T    . It is on all demos D       for any T ∈ T      .
                                                                                             train                       demo                 train
                embedded patches as a 1D sequence. We empirically show              Wetrain one model f jointly for all k training tasks (e.g.,
                                                                                                           θ
                that explicitly modeling positions in 2D is essential.              k=400), based on the loss in Eq. (1). All tasks share the
                   Formally, we adopt separable 2D positional embed-                same parameters, only except that each task has its own
                dings, following [11]: with D channels for positional em-           task-conditional token.    We do not use the inference set
                beddings, we use the first half of the channels to embed            DT from the training tasks (i.e., T ∈ Ttrain) to train the
                                                                                      infer
                the horizontal coordinate and the second half to embed the          model. These sets are used only for validation purposes.
                                                                                4
                                                                                       nearly free to use many views. We use 510 random views
                                                                                       (details are in appendix). Predictions from different views
                                                                                       are consolidated by majority voting [1].2
                                                                       prediction
                                                                                       Pass@2accuracy. The ARC benchmark by default adopts
                                                                                       the pass@2 accuracy metric: i.e., two different solutions
                                                                                       can be produced for evaluation, and a task is considered
                                                                                       correct if one is correct. To support this metric, we adopt
                                                  TTT process                          majority voting in multi-view inference and retain the top-2
                Figure 6. Effect of test-time training. (Top): Demonstration ex-       most populated output solutions.
                amples for the current task. (Bottom left): An inference example
                x    . (Bottom right): During test-time training, the prediction
                  infer                                                                4. Implementation Details
                from x     becomes progressively more accurate, with the model
                       infer
                finally generating the correct prediction.                             Wedescribe the major implementation choices in this sec-
                                                                                       tion. The configuration details can be found in appendix.
                Test-time training (TTT). Given a single new, unseen                   Canvas. In our best-performing model, the canvas size is
                task T    ∈ T      from the test set, we perform inference
                               test                                                    64×64. In the case of ViT, the patch size is 2×2, resulting
                by test-time training.     At inference time, we are given             in a sequence length of 322. For scale augmentation, an in-
                   T             	m
                D      = (x ,y )         with both input and output accessi-
                  demo       i  i   i=1                                                teger scaling ratio is randomly sampled, such that the scaled
                ble; the model is required to make prediction for a given              grid is no larger than the canvas size. For translation aug-
                xinfer in this new task T.        The test-time training fol-          mentation, the upper-left corner is randomly sampled under
                lowed by inference can be viewed abstractly as a function              the constraint that the placed image is fully visible.
                F(x       | DT    ) 7→ y    .
                     infer   demo       infer                                          Offline training. We use the standard ARC-1 training set
                    Weperformtest-time training for each new task T inde-              T    for training: it has 400 tasks with 2-4 demo pairs each.
                pendently. It has a new task token whose parameters are                 train
                randomly initialized. As there are very few demo pairs in              Following common practice on ARC, we also expand our
                DT     (e.g., 2 to 4), we also perform data augmentation. We           training set with the RE-ARC set [22], from which we sam-
                  demo                                                                 ple 1,000 additional demo pairs per task. Put together, our
                elaborate on the details in the next section and in appendix.          full training set has about 400k sample pairs. We apply
                    In summary, at inference time, the model is initialized            translation and scale augmentation in offline training.
                fromoffline training, fine-tuned with test-time training only
                for the single new task T, and then performs inference on              Test-time training. Given an unseen task T ∈ Ttest, we
                xinfer. As the new demo pairs in DT         are very few, even         have 2-4 sample pairs in DT      . To make test-time training
                                                       demo                                                         demo
                with data augmentation, this test-time training process re-            more feasible, we also augment the single task T into mul-
                mains reasonably fast (e.g., 70 seconds per task on a single           tiple auxiliary tasks. We do this by using standard augmen-
                GPU).Fig. 6 visualizes the effect of test-time training.               tation from existing ARC methods: flip, rotation (by 90◦,
                3.5. Inference                                                         180◦, or 270◦), and color permutation. We treat each of
                                                                                       these test-time training augmentations as an auxiliary task,
                After test-time training, we apply f to x          to obtain the       each assigned a task embedding. We also apply translation
                                                       θ      infer                    and scale augmentation in test-time training, but we do not
                final prediction. This process is analogous to the classical
                recognition problems [29, 38]. Accordingly, we adopt post-             view them as a new auxiliary task (under the assumption
                processing strategies inspired by recognition methods.                 that all auxiliary tasks are translation and scale invariant).
                Single-view inference. Given x            and a single “view”
                                                     infer                             5. Experimental Results
                (i.e., with a given scale and translation), we place xinfer on
                the canvas and apply fθ to predict the output. Since one               Ourexperimentsareprimarilyconductedonthebenchmark
                output location in the raw grid may be predicted by multi-             of ARC-1 [12]. We report the pass@2 accuracy (referred
                ple pixels on the canvas (e.g., due to rescaling; see Fig. 5),         to simply as “accuracy” hereafter) in percentage (%). To
                weaggregate all predictions (from softmax outputs) at this             support pass@2 evaluation, we adopt multi-view inference.
                location by average pooling.                                           Wealsoreport final results on ARC-2 [14].
                Multi-viewinference. It was a common practice to consol-                  WeevaluateourmodelontheARC-1evaluationset(i.e.,
                idate the predictions from multiple views (e.g., see AlexNet           Teval). This set is conceptually a test set (see Fig. 3), but with
                [29]). Analogously, we adopt multi-view inference to im-               ground truth available only for computing accuracy.
                prove accuracy, where the views are sampled with different               2In majority voting, two output grids are considered “consistent” only
                augmentations. As the multi-view inference cost is negli-              when they are identical across the entire grid. The winner is the grid that
                gible compared with test-time training cost, it is virtually           is “consistent” with the largest number of other output grids.
                                                                                   5
                                                                                                      model     width    depth    #params    Gflops    acc.
                     (a) naïve baseline
                                                 26.8
                                                                                                                 384       5          6M         10    44.4
                                                        32.8
                     (b) w/ 2D absolute pos embed                                                      ViT       512      10         18M         28    54.5
                                                                                                                 768      20         66M         99    53.0
                                                                     43.0
                     (c) w/ 2D RoPE
                                                                                                                  setting (a)         7M         18    42.8
                                                                        45.4
                     (d) 1x1 patch on 32x32  → 2x2 patch on 64x64                                     U-Net       setting (b)        17M         33    47.5
                     (e) w/ translation aug. on canvas
                                                                           48.3                                   setting (c)        55M         87    48.3
                     (f) w/ scale aug. on canvas
                                                                                   54.5       Table 1. Vision backbones. We compare variants of ViTs and U-
                                                                                    (%)
                    0          10          20          30          40          50
                                                                                              Nets of similar sizes. U-Net settings are in appendix.
                  Figure 7. Effects of visual priors in VARC. Accuracy is reported
                  on the ARC-1 evaluation set. The model used is ViT-18M. En-
                                                                                                 60                                  80
                  tries (a-c) use a patch size of 1×1 on a 32×32 canvas, whereas                        Depth = 5                            w/ offline training
                                                                                                                                     70
                                                                                                        Depth = 10                           wo/ offline training
                  entries (d-f) use a patch size of 2×2 on a 64×64 canvas. Each                  55
                                                                          ¨                                                          60                 54.5
                  entry modifies the one above it. We start from a naıve baseline
                                                                                                )
                                                                                                                         54.5        50
                                                                                                %
                                                                                                 50
                                                                                                (
                                                                                                                         18M              44.8
                  with components (b-f) removed. These vision priors cumulatively                
                                                                                                               51.6
                                                                                                y
                                                                                                c
                                                                                                a                                    40
                                                                                                               12M
                  yield 27.7 improvement (a→f), in which the canvas-based designs               r
                                                                                                u
                                                                                                c
                                                                                                c
                                                                                                 45
                                                                                                      47.0               47.0        30        29.1
                  (c→f) contribute an 11.5 gain.                                                A                                   Accuracy (%)              26.4
                                                                                                      6M
                                                                                                                         10M
                                                                                                               44.4                  20
                                                                                                                6M
                                                                                                 40
                                                                                                      41.0                           10
                                                                                                      3M
                                                                                                                                      0
                  5.1. Visual Priors                                                             35                                       TTT jointly  TTT independently
                                                                                                      256       384      512
                  Fig. 7 summarizes the effects of visual priors, starting from               Figure 8.    Scalability: ViTs       Figure 9.     TTT strategies:
                  a baseline (a) without the other components in this figure.                 with different width (x-axis)        with vs. without offline train-
                  These priors jointly have a gain of 27.7 points, where the                  and depth. The circle areas de-      ing, and joint vs. independent
                  canvas-based designs (c→f) has a gain of 11.5 points. We                    note model sizes.                    for each task.
                  discuss these components as follows.
                  2D positional embedding.             Extending from 1D posi-                Translation and scale augmentation. In image recogni-
                  tional embedding to its 2D counterpart is beneficial: see                   tion, even highly capable network architectures still benefit
                  Fig. 7(b)(c). This is observed in both (b) absolute and (c)                 greatly from translation and scale augmentations. We draw
                  relative positional embeddings.                                             similar observations in ARC. See Fig. 7(e,f).
                     To demonstrate this effect on a stronger baseline, we re-                   In Fig. 7(e), we apply fully flexible translation augmen-
                  place the 2D RoPEinFig.7(f)witha1DRoPEandobserve                            tation on the canvas. Compared with the “one-pixel” aug-
                  a degradation of 3.5 points, from 54.5 to 51.0.                             mentation in Fig. 7(d), this setting yields an additional gain
                  Patchification. A key design principle of our method is                     of 2.9 points (from 45.4 to 48.3). In Fig. 7(f), we further ap-
                  to prepare the input as a natural image. This enables the                   ply the scale augmentation enabled by the concept of can-
                  expansionofthetokensetfromaverylimitedsize(e.g.,10)                         vas.   Scale augmentation yields a substantial gain of 6.2
                  to an exponentially large number. The entries Fig. 7(d-f) all               points. Unlike translation invariance, which can be partially
                  benefit from this design.                                                   addressed by patchification (i.e., a special form of convo-
                     In Fig. 7(d), we advance from 1×1 patches on a 32×32                     lution), the ViT architecture has little to no inductive bias
                  canvas to 2×2 patches on a 64×64 canvas. Doing so does                      aboutscaleinvariance. This can explain why scale augmen-
                  not increase the computational cost of the Transformer. In                  tation yields a substantial gain.
                  this ablation (d), the scaling ratio is fixed as 2×. As such,               5.2. Other Ablation Experiments
                  if we constrain each 2×2 patch to cover only one raw pixel,                 ViT vs. U-Net. In Tab. 1, we compare ViT with U-Nets,
                  it becomes equivalent to the 1×1 patch counterpart on the                   a type of convolutional network. We evaluate three model
                  32×32 canvas. Therefore, to ensure a meaningful compar-                     sizes for each architecture. Although ViTs consistently per-
                  ison, we do not impose this constraint, allowing each 2×2                   formbetter,allU-Netvariantsachievedecentaccuracy,sug-
                  patch to cover multiple colors. This can be interpreted as
                  one-pixel translation augmentation on the canvas.                           gesting that this problem can also be effectively addressed
                     Evenso,the2×2patchificationleadstoanoticeablegain                        byclassical vision backbones.
                  of 2.4 points, improving from 43.0 to 45.4; see Fig. 7(c,d).                Scalability. In Fig. 8, we show ViTs with varying depths
                  In spite of the small one-pixel augmentation, each patch can                and widths. In this regime, our method demonstrates good
                  cover multiple colors (as in natural images), which substan-                scalability: increasing depth and/or width leads to higher
                  tially enriches the data space for learning.                                accuracy as a result of better fitting.        Going beyond this
                                                                                          6
                      single-view, pass@1  multi-view, pass@1   multi-view, pass@2                   system                  #params    ARC-1     ARC-2
                             35.9                 49.8                 54.5                          large language models (LLMs)
                                                                                                     Deepseek R1 [21]         671B         15.8       1.3
                           Table 2. Single-view vs. multi-view inference.                            Claude 3.7 8k [18]       N/A          21.2       0.9
                                                                                                     o3-mini-high [18]        N/A          34.5       3.0
                                                                                                     GPT-5[18]                N/A          44.0       1.9
                                                                                                     Grok-4-thinking [18]     1.7T         66.7      16.0
                  regime can lead to overfitting in our current setting, as                          Bespoke (Grok-4) [8]     1.7T         79.6      29.4
                  shown in Tab. 1 for the 66M ViT model. We observe that                             recurrent models
                  this larger model achieves higher training accuracy, sug-                          HRM[53]                  27M          40.3       5.0
                  gesting that future research should focus on generalization.                       TRM[27]                   7M          44.6       7.8
                  Test-timetraining(TTT)strategies. InFig.9(b),westudy                               vision models
                  TTTwithandwithout offline training, and TTT performed                              VARC                     18M          54.5       8.3
                                                                                                     VARC(ensemble)           73M          60.4      11.1
                  jointly on all test tasks vs. independently for each test task.                    humanresults
                     As expected, offline training greatly improves the per-                         avg. human [31]            -          60.2         -
                  formance of TTT, suggesting that common sense about the                            best human [18]            -          98.0     100.0
                  visual world can be learned from the training set. We also                 Table 3. System-level comparisons on the ARC-1 and ARC-2
                  note that even without offline training, our TTT strategy                  benchmarks. LLM-based results are from the ARC-AGI leader-
                  canachievenontrivialaccuracy(26.4),suggestingthatsome                      board [18]. HRM, TRM, and our VARC are trained from scratch
                  tasks in this benchmark can be solved tabula rasa. This re-                only on ARC data. Our single-model result is based on ViT, with
                  sult outperforms that in [36] under a similar setting.                     mean±std of 54.5±0.7 (ARC-1) and 8.3±0.4 (ARC-2) over four
                     Surprisingly, performing TTT independently for each                     runs. Our ensemble result aggregates an 18M ViT and a 55M
                                                                                  ∼
                  test task yields substantially better performance (by            10        U-Net, each with test-time training performed four times.
                  points) than doing so jointly across all test tasks, even
                  though the latter relies on a stronger assumption about the                orders of magnitude smaller.
                  availability of multiple test tasks at once.3 We hypothesize
                  that overtraining on the test tasks may cause the model to                    In the controlled setting of training from scratch on
                  forget the knowledge acquired during offline training.                     ARCdata, our method substantially outperforms the recur-
                                                                                             rent models: HRM [53] and TRM [27]. Our VARC with
                  Single-view vs. multi-view inference.            As discussed in                                 ∼
                                                                                             18Mparameters is 10 points better than TRM on ARC-1,
                  Sec. 3.5, we adopt multi-view inference by default. For                    a >20% relative improvement. Note that, once test-time
                  completeness, we also examine the single-view inference                    training is completed, our model performs fully feedfor-
                  accuracy. Since single-view inference cannot produce mul-                  ward inference, with no recurrence involved in reasoning.
                  tiple predictions, we compare pass@1 accuracy. See Tab. 2.                    Following the classical ensembling practice in vision
                     Single-view inference has a decent pass@1 accuracy of                   (e.g., AlexNet [29]), we ensemble one ViT and one U-Net,
                  35.9; multi-view inference further boosts to 49.8, thanks to               eachwithtest-time training run four times. Doing so boosts
                  majority voting. Unlike typical computer vision applica-                   our result to 60.4. This result closes the gap with the re-
                  tions such as semantic segmentation, in ARC, a mistake on                  ported average human performance (60.2 [31]).
                  even a single pixel renders the entire prediction incorrect.
                  This may explain the large gain seen here.                                 6. Visualization and Analysis
                  5.3. System-level Comparisons                                              Beyond numerical metrics, we provide additional qualita-
                  In Tab. 3 we compare with leading results using LLMs or                    tive results that help reveal the model’s behavior. We refer
                  recurrent models, on ARC-1 and ARC-2.4                                     readers to the appendix for more visualizations.
                     Our model compares favorably with some of the most                      Attention patterns. Fig. 10 shows the attention patterns of
                  powerful LLMs at the time their results were reported: in-                 our ViT model in a test task. These attention maps show
                  cluding Deepseek, Claude, o3, and GPT-5 (we note that                      that our model can correctly reason about the relationship
                  given the rapid progress of LLMs, these models may have                    between a source pixel and its target pixel to copy from.
                  stronger results by the time our paper is public). LLMs are                   Figure11visualizesthelayer-wiseattentionmapsforan-
                  pre-trained on internet-scale data, and some may also incor-               other test task. A layer-wise map is the softmax attention
                  porate multimodal data that include images. Our method                     mapaveragedacrossall pixels in the layer: it reveals which
                  does not rely on such data and uses a model that is several                pixels receive the most attention in that layer. In this task,
                     3In general, it cannot be assumed that multiple unseen tasks will be       4Our ARC-2 models are trained only on the ARC-1 dataset, with test-
                  presented all at once.                                                     time training and inference on the ARC-2 set.
                                                                                         7
                                                                                                                                                                                                                                                                                                                             model prediction
                                                                                                                                                      model prediction
                                      one pixel, different layers                                                                       -1                                                  +1
                                                                             layer 3                                             layer 4                                              layer 8
                                       one layer, different pixels
                                                                             layer 8                                              layer 8                                             layer 8
                                                                                                                                                                                                               Figure 11. Visualization of layer-wise attention maps. For each
                                                                                                                                                                                                               layer, we compute pixel-to-pixel attention and then average the
                                                                                                                                                                                                               softmax maps across all pixels to obtain a single map per layer.
                                       Figure 10. Visualization of pixel-to-pixel attention. (Top): a                                                                                                          This map reveals which pixels are most attended in this layer. We
                                       test task from ARC-1 eval: showing demo pairs, inference input,                                                                                                         showatesttaskfromARC-1eval. Inthistask,somelayersexhibit
                                       and model prediction. (Middle): attention maps for a single pixel                                                                                                       strong attention to the 3×3 neighborhood, reflecting the influence
                                       across different layers. With the highlighted pixel as query, we                                                                                                        ofthepattern’score. Incomparison,someotherlayers(e.g.,layers
                                       show pre-softmax logits. Different layers exhibit different behav-                                                                                                      7–9)focusontheoutward-radiatingrays,correspondingtotherule
                                       ior. (Bottom): attention maps in layer 8 with other query pixels.                                                                                                       that extends colored pixels along the eight directions.
                                       All of them correctly attend to their corresponding palette pixel.
                                                                                                                                                                                                                    “Use the blue layout in the         “Use the green layout in the 
                                                                                                                                                                                                                                                                                                “Copy the small colored           “Copy the small colored 
                                                                                                                                                                                                                    blue box as the blueprint,          blue box as the blueprint, 
                                       different layers exhibit different specialties: some layers at-                                                                                                                                                                                          pattern center to the gray        pattern center to the blue 
                                                                                                                                                                                                                    fill its four regions with the      fill its four regions with the 
                                                                                                                                                                                                                                                                                                pixel.”                           pixel.”
                                       tend to the pixels that are to be copied, and some layers                                                                                                                    four corner colors”                 small square of four colors.”
                                       attend to the target lines alone the eight directions.
                                       t-SNE of task embeddings. Our model is conditioned on
                                       a task token, with an embedding learned to represent each
                                       task. With 400 training tasks in ARC-1, our model learns
                                       400 distinct task embeddings in offline training. We visu-
                                       alize these 400 embeddings in the 2D space by t-SNE [39]
                                       (see Fig. 12). Each point corresponds to a task..
                                              Interestingly, we observe that nearby points in the task                                                                                                               “If a cell is black in both     “If a cell is black in both        “Expand the red pixel vertically  “Expand the gray pixel diagonally 
                                                                                                                                                                                                                     input grids, make it green.  input grids, make it green.           with blue pixels, and expand          with blue pixels, and expand 
                                       embedding space exhibit similar semantics. For example,                                                                                                                       Otherwise leave it black.”      Otherwise leave it black.”         pattern in four dimensions.”          pattern in four dimensions.” 
                                       the top-left corner in Fig. 12 shows two tasks related to                                                                                                               Figure 12. t-SNE of task embeddings, on the 400 task tokens
                                       coloring; the bottom-left corner shows two tasks related to                                                                                                             learned from the ARC-1 training set. Each point represents a sin-
                                       generalized logic operations (i.e., AND/OR/XOR). This vi-                                                                                                               gle task. To aid the reader, we provide human-written descriptions
                                       sualization suggests that our method attempts to learn the                                                                                                              for the tasks (which are not used in any form by our method).
                                       relations between different tasks, which is an essential abil-
                                       ity for abstraction and reasoning.                                                                                                                                      lem, emphasizing abstraction and reasoning emerging di-
                                                                                                                                                                                                               rectly from image pixels.
                                       7. Conclusion                                                                                                                                                                   We hope this work will encourage the community to
                                                                                                                                                                                                               leverage ARC not only as a symbolic reasoning problem,
                                       Our work explores a previously overlooked perspective in                                                                                                                but also as a testbed for promoting the generalization ca-
                                       theARCtaskbyframingitasanimage-to-imagetranslation                                                                                                                      pacity of visual methods. Future research may extend this
                                       problem. Itnaturallyenablestheadaptationofvisualframe-                                                                                                                  direction through more expressive architectures, richer vi-
                                       works and yields strong few-shot generalization competi-                                                                                                                sual priors, or larger-scale image pre-training. We envision
                                       tive with recent approaches, while remainingordersofmag-                                                                                                                that vision-centric reasoning will play a key role in build-
                                       nitudesmallerthanmostLLM-basedmodels. Thisopensup                                                                                                                       ing AI systems capable of learning and applying abstract
                                       a new possibility of treating ARC as a vision-centric prob-                                                                                                             concepts in a human-like manner.
                                                                                                                                                                                                      8
                 References                                                                     works. IEEE Transactions on Pattern Analysis and Machine
                                 ¨                                                              Intelligence, 2015.
                  [1] Ekin Akyurek, Mehul Damani, Adam Zweiger, Linlu Qiu,                [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
                       Han Guo, Jyothish Pari, Yoon Kim, and Jacob Andreas.                     Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
                       Thesurprisingeffectivenessoftest-timetrainingforfew-shot                 Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
                       learning. In ICML, 2025.                                                 vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
                  [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine                 worth 16x16 words: Transformers for image recognition at
                       Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-                   scale. In ICLR, 2021.
                       sch, Katherine Millican, Malcolm Reynolds, Roman Ring,             [18] ARC Prize Foundation. ARC-AGI benchmarking: Leader-
                       Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,                  board and dataset for the ARC-AGI benchmark. https:
                       Sina Samangooei, Marianne Monteiro, Jacob L. Menick,                     //arcprize.org/leaderboard, 2025. Accessed:
                       Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sa-                     2025-11-01.
                       hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,             [19] Daniel Franzen, Jan Disselhoff, and David Hartmann. Prod-
                                                                     ´
                       Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.                     uct of experts with LLMs: Boosting performance on ARC is
                       Flamingo: a visual language model for few-shot learning.                 a matter of perspective. arXiv:2505.07859, 2025.
                       In NeurIPS, 2022.                                                  [20] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
                  [3] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan                   tra, and Devi Parikh. Making the V in VQA matter: El-
                       Klein. Learning to compose neural networks for question                  evating the role of image understanding in visual question
                       answering. In ACL, 2016.                                                 answering. In CVPR, 2017.
                  [4] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan             [21] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,
                       Klein. Neural module networks. In CVPR, 2016.                            Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi
                  [5] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret                   Wang, Xiao Bi, et al.     Deepseek-R1: Incentivizing rea-
                       Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi                     soning capability in LLMs via reinforcement learning.
                       Parikh. VQA: visual question answering. In ICCV, 2015.                   arXiv:2501.12948, 2025.
                  [6] JeremyBerman. HowIcameinfirstonARC-AGI-Pubusing                     [22] Michael Hodel.        Addressing the abstraction and rea-
                       Sonnet 3.5 with evolutionary test-time compute. Substack,                soning   corpus    via   procedural   example     generation.
                       2024. Accessed: 2025-10-13.                                              arXiv:2404.07353, 2024.
                  [7] Jeremy Berman. How I got a record 53.6% on ARC-AGI.                 [23] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor
                       Substack, 2024. Accessed: 2025-10-13.                                    Darrell, and Kate Saenko. Learning to reason: End-to-end
                  [8] Jeremy Berman. How I got the highest score on ARC-AGI                     module networks for visual question answering. In ICCV,
                       again swapping Python for English. Substack, 2025.                       2017.
                        ´
                  [9] Leon Bottou and Vladimir Vapnik.        Local learning algo-        [24] Thorsten Joachims. Transductive inference for text classifi-
                       rithms. Neural Computation, 1992.                                        cation using support vector machines. In ICML, 1999.
                 [10] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-               [25] Aysja Johnson, Wai Keen Vong, Brenden M. Lake, and
                       biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-                  ToddM.Gureckis. Fastandflexible: Humanprograminduc-
                       tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-                   tion in abstract reasoning tasks. arXiv:2103.05823, 2021.
                       hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom            [26] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,
                       Henighan, RewonChild, Aditya Ramesh, Daniel M. Ziegler,                  Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick.
                       Jeffrey Wu,ClemensWinter,ChristopherHesse,MarkChen,                      CLEVR: A diagnostic dataset for compositional language
                       Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,                 and elementary visual reasoning. In CVPR, 2017.
                       JackClark,ChristopherBerner,SamMcCandlish,AlecRad-                 [27] AlexiaJolicoeur-Martineau. Lessismore: Recursivereason-
                       ford, Ilya Sutskever, and Dario Amodei. Language models                  ing with tiny networks. arXiv:2510.04871, 2025.
                       are few-shot learners. In NeurIPS, 2020.                           [28] Diederik P Kingma and Jimmy Ba. Adam: A method for
                 [11] Xinlei Chen, Saining Xie, and Kaiming He.         An empiri-              stochastic optimization. In ICLR, 2015.
                       cal study of training self-supervised vision transformers. In      [29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
                       ICCV,2021.                                                               ImageNet classification with deep convolutional neural net-
                 [12] Franc¸ois Chollet.       On the measure of intelligence.                  works. In NeurIPS, 2012.
                       arXiv:1911.01547, 2019.                                            [30] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
                 [13] Francois Chollet, Mike Knoop, Gregory Kamradt, and                        Howard, W. Hubbard, and L. D. Jackel. Backpropagation
                       Bryan Landers.      ARC Prize 2024:        Technical report.             applied to handwritten zip code recognition. Neural Compu-
                       arXiv:2412.04604, 2024.                                                  tation, 1989.
                 [14] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan                [31] Solim LeGris, Wai Keen Vong, Brenden M. Lake, and
                       Landers, and Henry Pinkard. ARC-AGI-2: A new challenge                   Todd M. Gureckis. H-ARC: A robust estimate of human
                       for frontier AI reasoning systems. arXiv:2505.11831, 2025.               performance on the abstraction and reasoning corpus bench-
                 [15] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion                   mark. arXiv:2409.01374, 2024.
                       models beat GANs on image synthesis. In NeurIPS, 2021.             [32] Solim LeGris, Wai Keen Vong, Brenden M. Lake, and
                 [16] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou                        Todd M. Gureckis. A comprehensive behavioral dataset for
                       Tang. Image super-resolution using deep convolutional net-               the abstraction and reasoning corpus. Scientific Data, 2025.
                                                                                      9
                [33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.          [51] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet
                     Hoi. BLIP: bootstrapping language-image pre-training for             Singh, Adina Williams, Douwe Kiela, and Candace Ross.
                     unified vision-language understanding and generation.   In           Winoground: Probing vision and language models for visio-
                     ICML,2022.                                                           linguistic compositionality. In CVPR, 2022.
                [34] Wenhao Li, Yudong Xu, Scott Sanner, and Elias Boutros          [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
                     Khalil. Tackling the abstraction and reasoning corpus with           reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
                     vision transformers: the importance of 2D representation,            Polosukhin. Attention is all you need. In NeurIPS, 2017.
                     positions, and objects. arXiv:2410.06405, 2024.                [53] Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu,
                [35] Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon                Yue Wu, Meng Lu, Sen Song, and Yasin Abbasi Yadkori.
                     Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Wei-Long               Hierarchical reasoning model. arXiv:2506.21734, 2025.
                     Zheng,YewenPu,andKevinEllis. Combininginductionand             [54] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu,
                     transduction for abstract reasoning. In ICLR, 2025.                  Nick Haber, and Noah D. Goodman. Hypothesis search: In-
                [36] Isaac Liao and Albert Gu.     ARC-AGI without pretrain-              ductive reasoning with language models. In ICLR, 2024.
                     ing.   https://iliao2345.github.io/blog_                       [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
                     posts/arc_agi_without_pretraining/arc_                               Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and
                     agi_without_pretraining.html,2025.                                   Denny Zhou. Chain-of-thought prompting elicits reasoning
                [37] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.                      in large language models. In NeurIPS, 2022.
                     Visual instruction tuning. NeurIPS, 2023.                      [56] Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv
                [38] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully             Batra, and Devi Parikh. Yin and yang: Balancing and an-
                     convolutional networks for semantic segmentation.       In           swering binary visual questions. In CVPR, 2016.
                     CVPR,2015.
                [39] Laurens van der Maaten and Geoffrey Hinton. Visualizing
                     data using t-SNE. Journal of machine learning research,
                     2008.
                                                   ´
                [40] Matthew V Macfarlane and Clement Bonnet. Searching la-
                     tent program spaces. arXiv:2411.08706, 2024.
                [41] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B.
                     Tenenbaum, and Jiajun Wu. The neuro-symbolic concept
                     learner: Interpreting scenes, words, and sentences from nat-
                     ural supervision. In ICLR, 2019.
                [42] Arseny Moskvichev, Victor Vikram Odouard, and Melanie
                     Mitchell.    The ConceptARC benchmark:         Evaluating
                     understanding and generalization in the ARC domain.
                     arXiv:2305.07141, 2023.
                                                ¨    ¨
                [43] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
                     Darrell, and Alexei A. Efros.  Context encoders: Feature
                     learning by inpainting. In CVPR, 2016.
                [44] Rolf Pfister and Hansueli Jud. Understanding and bench-
                     marking artificial intelligence: OpenAI’s o3 is not AGI.
                     arXiv:2501.07458, 2025.
                [45] Jean-Francois Puget. A 2D nGPT model for ARC Prize.
                     2024.
                [46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
                     net: Convolutional networks for biomedical image segmen-
                     tation. In International Conference on Medical image com-
                     puting and computer-assisted intervention. Springer, 2015.
                [47] Yang Song and Stefano Ermon. Generative modeling by es-
                     timating gradients of the data distribution. In NeurIPS, 2019.
                [48] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen
                     Bo,andYunfengLiu. Roformer: Enhancedtransformerwith
                     rotary position embedding. Neurocomputing, 2024.
                [49] YuSun,XiaolongWang,ZhuangLiu,JohnMiller,AlexeiA.
                     Efros, and Moritz Hardt.     Test-time training with self-
                     supervision for generalization under distribution shifts. In
                     ICML,2020.
                [50] Hao Tang, Keya Hu, Jin Zhou, Sicheng Zhong, Wei-Long
                     Zheng, Xujie Si, and Kevin Ellis. Code repair with LLMs
                     gives an exploration-exploitation tradeoff. In NeurIPS, 2024.
                                                                                10
                           offline training                                                                   U-net                        7M     17M     55M
                           epochs                                              100                            #stages                        3       3        3
                           warmupepochs                                         10                            layers per stage               1       1        2
                           optimizer                 Adam[28],betas=(0.9, 0.999)                              #channels at resolution 1     80     120     160
                           batch size                                           32                            attention at resolution 1    No      No       No
                           learning rate                                      3e-4                            #channels at resolution 2   160      240     320
                           learning rate scheduler                          cosine                            attention at resolution 2   Yes      Yes     Yes
                           weight decay                                          0                            #channels at resolution 3   160      240     320
                           dropout                                             0.1                            attention at resolution 3   Yes      Yes     Yes
                           test-time training                                                                 midblock                     No      No      Yes
                           epochs                                              100
                           warmupepochs                                         10                Table7. ConfigurationoftheU-Netarchitecture. Thedefinition
                           optimizer                 Adam[28],betas=(0.9, 0.999)                  follows standard U-Nets used in generative models [47, 15].
                           batch size                                            8
                           learning rate                                      3e-4
                           learning rate scheduler                          cosine
                           weight decay                                          0
                           dropout                                             0.1
                                         Table 4. Configurations.
                     offline training                    test-time training                       Figure 13. Shape Handling. The gray pixels denote the back-
                     GPUtype              H100           GPUtype                   H100           ground tokens [BG], which keep the canvas size fixed (64×64 by
                     GPUnumber                8          GPUnumber                     1          default). The white pixels denote the border tokens [BD], which
                     GPUtime          4.8 hours          GPUtime          0.7s per epoch          indicate the output shape. (Left): a pair (x,y) with a scaling ratio
                  Table 5. Running time of the ViT-18M model. The reported time                   of 1×. (Right): a pair (x,y) with a scaling ratio of 2×.
                  is obtained with torch.compile optimization.
                                                                                                  inal task. We train for 100 epochs on these 51 tasks, covering
                               ViT                        6M     18M      66M                     100 × 51 × 3 = 15.3k samples in total for test-time training for
                               hidden dim                 384     512     768                     one test task T (assuming 3 raw samples in this task).
                               Transformer blocks          5      10       20
                               #heads                      8       8       12                     A.3. Shape Handling
                               MLPblockhiddendim                  512
                               dropout                             0.1                            Unlikestandardsemanticsegmentation,inARC,therawinputand
                               patch size                         2×2                             outputsizesarenotalwaysidentical(e.g.,seeFig.3,TestSet,Task
                               canvas size                       64×64                            1). This issue can be addressed on the canvas in a unified frame-
                  Table 6. Configuration of the ViT architecture. The 18M model                   work. In our method, the input/output canvas always has a fixed
                  is our default setting.                                                         size and is filled with a background token [BG]. In addition, when
                                                                                                  the raw output is placed on the canvas (serving as the ground truth
                                                                                                  during training), we always use an extra border token, [BD], to
                  A. Additional Implementation Details                                            indicate the right and bottom edges. Specifically, the token [BD]
                                                                                                  is filled along the one-pixel-wide edge on the right and bottom
                  A.1. Configurations                                                             sides. During inference, we locate the rightmost and bottommost
                                                                                                  [BD]tokens and crop the output accordingly to recover the final
                  Wereport the training configurations in Tab. 4. The running time                predicted shape. This is illustrated in Fig. 13.
                  under this configuration is profiled in Tab. 5.                                     Since the number of background pixels [BG] can dominate
                      The hyperparameters for our ViT models are listed in Tab. 6,                in some examples, we apply attention masks in the self-attention
                  and those for our U-net models are shown in Tab. 7.                             blocks to encourage the model to focus on the foreground pixels.
                  A.2. Test-time Training Augmentation                                            The attention masks are applied after the query-key dot-product
                                                                                                  computation,addingalargenegativevaluetothekeyscorrespond-
                  During test-time training, we augment the single test task T into               ing to background inputs. The resulting softmax attention scores
                  multiple auxiliary tasks. We use a distinct task embedding for                  are therefore zero at those key positions. Moreover, during train-
                  each auxiliary task, as not all of these augmentations correspond               ing, the loss is computed only on locations where the inputs are not
                  to the same underlying rule (e.g., consider “gravity” under a 90◦               background pixels [BG]. These designs encourage the model to
                  rotation). We apply 2 flippings (horizontal and vertical) or 3 rota-            paymoreattentiontoforegroundsandthereforeimproveaccuracy,
                  tions (in multiples of 90◦), and 10 predefined color index permu-               although we note that even without them, our method still per-
                  tations, resulting in (2+3)×10=50 auxiliary tasks with the orig-                forms competitively, as observed in our preliminary experiments.
                                                                                             11
                                                                          60                                                                                                                                                      Pass@k Curve for ARC-1                                                    Pass@k Curve for ARC-2
                                                                                                                                                                                                                                                                                       10.5
                                                                          50                                                                                                                                 65.0
                                                                                                                                                                                                            ate)                                                                      ate)10.0
                                                                                                                                                                                                             62.5                                                                       9.5
                                                                          40
                                                                                                                                                                                                             60.0                                                                       9.0
                                                                        Accuracy (%)30                                                                                                                       57.5                                                                       8.5
                                                                          20                                                                                                                                 55.0                                                                       8.0
                                                                                                                                                                                                            ass@k (Cumulative Success R                                               ass@k (Cumulative Success R7.5
                                                                                                                                                                                                            P52.5                                                                     P
                                                                        Evaluation 10                                                                                                                                                                                                   7.0
                                                                                                                                                                                                             50.0                                                                       6.5
                                                                           0                                                                                                                                       0       50      100      150     200      250      300                    0       50       100     150      200     250      300
                                                                             0                      10                    100                    1000                                                                                        k                                                                         k
                                                                           Offline Training RE-ARC Pairs Per Task (Log-scaled)                                                                                               Pass@k Curve for Ensemble ARC-1                                           Pass@k Curve for Ensemble ARC-2
                                     Figure 14. Offline training data scaling: effect of varying the                                                                                                         72.5                                                                       15
                                     numberofRE-ARCsamplespertask,evaluatedontheARC-1eval                                                                                                                   ate)70.0                                                                   ate)14
                                                                                                                                                                                                             67.5
                                                                                                                                                                                                                                                                                        13
                                     set. Increasing the amount of offline training data is beneficial,                                                                                                      65.0
                                     although even without it, our model can achieve decent accuracy.                                                                                                        62.5                                                                       12
                                                                                                                                                                                                             60.0                                                                       11
                                                                                                                                                                                                            ass@k (Cumulative Success R                                                ass@k (Cumulative Success R
                                                                          60                                                                                                                                P57.5                                                                      P10
                                                                                                                                                                                                             55.0
                                                                          50                                                                                                                                       0     250    500    750    1000   1250   1500   1750                      0      20      40      60     80     100     120     140
                                                                                                                                                                                                                                             k                                                                         k
                                                                          40                                                                                                                            Figure 16. Pass@k results in the ARC-1 (left) and ARC-2 (right)
                                                                        Accuracy (%)30                                                                                                                  evaluation sets. Results are obtained with majority voting from
                                                                          20                                                                                                                            multi-view inference, using 510 views.                                                           (Top): using a single
                                                                        Evaluation 10                                                                                                                   modelofViT-18M.(Bottom): using an ensemble of one ViT-18M
                                                                                                                                                                                                        and one U-Net-55M, each with test-time training run four times.
                                                                           00                                 16                80                400
                                                                                  Offline Training Task Diversity (Log-scaled)                                                                          (66.3) of our method, even if oracle voting were applied. Beyond
                                     Figure 15. Offline training task diversity scaling: effect of vary-                                                                                                voting, future efforts should focus on improving the fundamental
                                     ing the number of training tasks, evaluated on the ARC-1 eval set.                                                                                                 ability of the model on each individual view.
                                     Increasing task diversity is beneficial.
                                     B. Additional Experiments                                                                                                                                          C. Additional Visualizations
                                     B.1. Offline Training Data Scaling                                                                                                                                 C.1. Successful and Failed Examples
                                                                                                                                                                                                        Weshowsuccessful and failed examples on ARC-1 (Fig. 17) and
                                     Since we use the RE-ARC dataset [22] in our offline training, we                                                                                                   ARC-2 (Fig. 18). See captions for detailed descriptions. Our
                                     can examine the effect of data scale provided by RE-ARC. See                                                                                                       method can solve some highly challenging tasks, but still makes
                                     Fig. 14. Using only the original ARC training data, without any                                                                                                    mistakes on some tasks that are simple for humans.
                                     RE-ARC data, our method achieves a decent accuracy of 31.5.                                                                                                        C.2. Ambiguous Examples.
                                     By adding 10, 100, and 1,000 pairs per task from RE-ARC, the
                                     accuracy increases to 38.6, 52.3, and 54.0, respectively. This com-                                                                                                Although most ARC tasks are unambiguous, some may admit
                                     parison suggests that increasing the amount of offline training data                                                                                               multiple plausible explanations or rules. We show an example in
                                     is beneficial, although the returns diminish beyond a certain point.                                                                                               Fig. 19, in which our method uncovers different solutions that are
                                             BeyondscalingthedatapertaskusingRE-ARC,wealsoexam-                                                                                                         plausible. Here, the rule can be interpreted as either “turn the red
                                     ine the scalability of the offline training task diversity. See Fig. 15.                                                                                           box blue only if the extended blue lines go through the box” (our
                                     Whentrained on 0, 16, 80, and 400 tasks, the accuracy increases                                                                                                    method’s first guess) or “turn the red box blue if the extended blue
                                     from 26.4 to 43.1, 49.6, and 54.5, respectively, suggesting that the                                                                                               lines touch the box in any form” (our method’s second guess).
                                     diversity of training tasks is helpful for generalization.
                                     B.2. Pass@k Results                                                                                                                                                C.3. Attention Maps
                                     Bydefault, the ARC protocol evaluates the pass@2 accuracy. We                                                                                                      Pixel-wise Attention Maps. In Fig. 20, we visualize the attention
                                     further examine the pass@k accuracy, thanks to our multi-view                                                                                                      maps of a single pixel specified as the query. See captions for
                                     inference with many views (510). This metric reflects whether at                                                                                                   detailed descriptions.
                                     least one of the k predicted solutions is correct. It can be viewed                                                                                                Layer-wise Attention Maps. In Fig. 21, we visualize the layer-
                                     as a recall-like measure.                                                                                                                                          wise attention maps averaged across all pixels. See captions for
                                             Figure 16 provides the pass@k results on ARC-1 and ARC-2                                                                                                   detailed descriptions.
                                     eval sets. As expected, as the number of proposals (k) increases,                                                                                                  C.4. Test-time Training Visualization
                                     the pass@k accuracy increases. On ARC-1, the pass@k accu-
                                     racy is 49.8, 54.5, and 66.3, when k is 1, 2, and 300, respectively                                                                                                Figure 22 illustrates the evolution of model predictions during the
                                     (Fig. 16, top-left). This result indicates that our model produces                                                                                                 test-time training process. Each row corresponds to a distinct test
                                     correct predictions in some of the many views, although such cor-                                                                                                  task from the ARC benchmark. It shows how our method progres-
                                     rect cases are not sufficiently populated to be retained after voting.                                                                                             sively refines its prediction through test-time training.
                                     Ontheotherhand,thisresultrevealstheupper-boundperformance
                                                                                                                                                                                             12
                              ARC-1  Solved
                              Demonstration  1                       Demonstration 2                           Input                Attempt 1           Attempt 2           Ground truth
                                                                                                                                    Vote: 109
                                                                                                                                                       Vote: 57
                              15663ba9
                                                                                                                                    Vote: 399           Vote: 35
                              981571dc
                                                                                                                                    Vote: 456           Vote: 10
                              15696249
                                                                                                                                    Vote: 233          Vote: 123
                              67c52801
                              ARC-1  Unsolved
                              Demonstration  1                       Demonstration 2                           Input                Attempt 1           Attempt 2           Ground truth
                              8dae5dfc                                                                                              Vote: 9             Vote: 6
                              67636eac
                                                                                                                                    Vote: 14
                                                                                                                                                        Vote: 8
                              aa4ec2a5                                                                                              Vote: 13            Vote: 9
                              b457fec5                                                                                              Vote: 3             Vote: 2
                     Figure 17. Successful and failed examples on ARC-1. (Top): Examples of test tasks successfully solved by VARC. (Bottom): Examples
                     of test tasks unsolved by VARC. (Left): Two demonstration example pairs shown for each task (some have more demonstrations not shown
                     here). (Right): Inference input and the first and second solutions proposed by VARC. The green box indicates the correct output.
                                                                                                           13
                          ARC-2  Solved
                           Demonstration  1                       Demonstration 2                          Input                 Attempt 1          Attempt 2            Ground truth
                                                                                                                                Vote: 99
                                                                                                                                                    Vote: 82
                           800d221b
                                                                                                                                Vote: 410          Vote: 16
                           7666fa5d
                                                                                                                                Vote: 30           Vote: 17
                           221dfab4
                                                                                                                               Vote: 168           Vote: 44
                           7b80bb43
                           ARC-2  Unsolved
                           Demonstration  1                       Demonstration 2                           Input                Attempt 1           Attempt 2            Ground truth
                           2b83f449
                                                                                                                                 Vote: 21           Vote: 20
                           2d0172a1
                                                                                                                                 Vote: 7            Vote: 6
                           3e6067c3
                                                                                                                                 Vote: 14           Vote: 12
                                                                                                                                 Vote: 67           Vote: 51
                           7ed72f31
                     Figure 18. Successful and failed examples on ARC-2. (Top): Examples of test tasks successfully solved by VARC. (Bottom): Examples
                     of test tasks unsolved by VARC. (Left): Two demonstration example pairs shown for each task (some have more demonstrations not shown
                     here). (Right): Inference input and the first and second solutions proposed by VARC. The green box indicates the correct output.
                                                                                                        14
                                                         Demonstration Examples
                                                         Input                               Attempt 1                      Attempt 2                          Ground truth
                                  Figure 19. Ambiguous examples. Although most ARC tasks are unambiguous, some may admit multiple plausible explanations or rules.
                                  Here, in the given three demonstration examples of a test task (top panel), it is unclear whether a blue line “touching” (but not “going
                                  through”) a red rectangle should render that rectangle blue. The inference example (bottom panel) involves this situation (“touching”), and
                                  our model attempts to interpret the rule as either “going-through-only” (attempt 1) or “touching” (attempt 2).
                                                    Task                                                                                                            Intermediate Heatmaps
                                                                                                                                                                               Early                                               Mid                                                Late
                                                    09c534e7*
                                                    Fill in the interior of the
                                                    connected boxes with the
                                                    same color.
                                                                                                                                                                    Block 2                           Block 4                          Block 5                            Block 6  Attention 
                                                                                                                                                                                                                                                                          is focused within the rectangle.
                                                    506d28a5
                                                    Logical OR between the
                                                    upper and lower parts
                                                    of the input.
                                                                                                                                                                     Block 1                          Block 4                           Block 5                           Block 7  Attention focuses on
                                                                                                                                                                                                                                                                          the corresponding cell in the 
                                                                                                                                                                                                                                                                          lower part of the grid.
                                                    0607ce86
                                                    Remove visual noise
                                                    from a regular, repeating
                                                    grid pattern.
                                                                                                                                                                    Block 2                           Block 4                           Block 5                           Block 7  Attention focuses on
                                                                                                                                                                                                                                                                          other cells in the same relative
                                                                                                                                                                                                                                                                          position within the grid.
                                                    070dd51e
                                                    Connect horizontal
                                                    or vertical lines, with
                                                    vertical lines being on
                                                    top.
                                                                                                                                                                    Block 3  Attention focuses on      Block 4                          Block 5                           Block 7  Attention focuses on
                                                                                                                                                                    endpoints of all lines for all                                                                        nearby, relevant lines only,
                                                                                                                                                                    pixels.                                                                                               focusing strongest on the
                                                                                                                                                                                                                                                                          correct line.
                                  Figure 20. Additional visualization: pixel-level attention maps. The maps are shown for different Transformer blocks, with a query
                                  pixel highlighted by a red-yellow border. Here we show 4 test tasks in ARC eval. Layers at different depths tend to focus on different
                                  structures. Early layers tend to focus on local transformations and context. Middle layers tend to perform a more non-local connection,
                                  e.g., horizontally or vertically. The deep layers are more task-specialized. The red asterisk indicates the task that was not correctly solved.
                                  (Here, the text descriptions are written by humans solely to help readers interpret the tasks.)
                                                                                                                                                                           15
               demonstrations             inference
               demonstrations             inference
               demonstrations             inference
               demonstrations             inference
               Figure 21. Additional visualization: layer-wise attention maps. Each map is the per-pixel softmax attention maps averaged across all
               pixels in that layer. The corresponding demonstration examples (on the left) are provided for reference.
                                                                          16
                    55059096
                                     Demonstration 2
                    Demonstration  1
                                     TTT process
                    Input
                                                         Ground truth
                    0c786b71
                    Demonstration  1
                                     Demonstration 2
                                     TTT process
                    Input
                                                         Ground truth
                    ac3e2b04
                    Demonstration  1
                                     Demonstration 2
                                     TTT process
                    Input
                                                         Ground truth
        Figure 22. Visualization of the test-time training process. Here, we visualize the grid augmented with a given scale ratio of 2× (the
        full canvas is not shown for brevity). As the test-time training progresses, the model’s predictions gradually converge toward the correct
        output. In early epochs, the model produces coarse and imprecise structures; in later epochs, the model can improve the solutions, e.g.,
        by refining color and spatial arrangement. This visualization illustrates the model’s behavior of adapting to task-specific transformations
        through few-shot test-time training.
                                         17
