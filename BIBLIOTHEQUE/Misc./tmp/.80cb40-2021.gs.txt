                                 Per-Pixel ClassiÔ¨Åcation is Not All You Need
                                              for Semantic Segmentation
                                                  1,2‚àó                       2                    1
                                     BowenCheng        Alexander G. Schwing     Alexander Kirillov
                            1Facebook AI Research (FAIR)     2University of Illinois at Urbana-Champaign (UIUC)
                                                                Abstract
                                Modernapproachestypicallyformulatesemanticsegmentationasaper-pixelclassi-
                                Ô¨Åcation task, while instance-level segmentation is handled with an alternative mask
                                classiÔ¨Åcation. Our key insight: mask classiÔ¨Åcation is sufÔ¨Åciently general to solve
                                both semantic- and instance-level segmentation tasks in a uniÔ¨Åed manner using
                                the exact same model, loss, and training procedure. Following this observation,
                                wepropose MaskFormer, a simple mask classiÔ¨Åcation model which predicts a
                                set of binary masks, each associated with a single global class label prediction.
                                Overall, the proposed mask classiÔ¨Åcation-based method simpliÔ¨Åes the landscape
                                of effective approaches to semantic and panoptic segmentation tasks and shows
                                excellent empirical results. In particular, we observe that MaskFormer outperforms
                                per-pixel classiÔ¨Åcation baselines when the number of classes is large. Our mask
                                classiÔ¨Åcation-based method outperforms both current state-of-the-art semantic
                                (55.6 mIoUonADE20K)andpanopticsegmentation(52.7PQonCOCO)models.1
                        1   Introduction
                        The goal of semantic segmentation is to partition an image into regions with different semantic
                        categories. Starting from Fully Convolutional Networks (FCNs) work of Long et al. [28], most deep
                        learning-based semantic segmentation approaches formulate semantic segmentation as per-pixel
                        classiÔ¨Åcation (Figure 1 left), applying a classiÔ¨Åcation loss to each output pixel [8, 46]. Per-pixel
                        predictions in this formulation naturally partition an image into regions of different classes.
                        MaskclassiÔ¨Åcation is an alternative paradigm that disentangles the image partitioning and classiÔ¨Åca-
                        tion aspects of segmentation. Instead of classifying each pixel, mask classiÔ¨Åcation-based methods
                        predict a set of binary masks, each associated with a single class prediction (Figure 1 right). The
                        more Ô¨Çexible mask classiÔ¨Åcation dominates the Ô¨Åeld of instance-level segmentation. Both Mask
                        R-CNN[19] and DETR [3] yield a single class prediction per segment for instance and panoptic
                        segmentation. In contrast, per-pixel classiÔ¨Åcation assumes a static number of outputs and cannot
                        return a variable number of predicted regions/segments, which is required for instance-level tasks.
                        Ourkeyobservation: mask classiÔ¨Åcation is sufÔ¨Åciently general to solve both semantic- and instance-
                        level segmentation tasks. In fact, before FCN [28], the best performing semantic segmentation
                        methods like O2P [4] and SDS [18] used a mask classiÔ¨Åcation formulation. Given this perspective, a
                        natural question emerges: can a single mask classiÔ¨Åcation model simplify the landscape of effective
                        approaches to semantic- and instance-level segmentation tasks? And can such a mask classiÔ¨Åcation
                        model outperform existing per-pixel classiÔ¨Åcation methods for semantic segmentation?
                        Toaddress both questions we propose a simple MaskFormer approach that seamlessly converts any
                        existing per-pixel classiÔ¨Åcation model into a mask classiÔ¨Åcation. Using the set prediction mechanism
                        proposed in DETR [3], MaskFormer employs a Transformer decoder [37] to compute a set of pairs,
                           ‚àóWorkpartly done during an internship at Facebook AI Research.
                           1Project page: https://bowenc0221.github.io/maskformer
                        35th Conference on Neural Information Processing Systems (NeurIPS 2021).
                                            per-pixel classification loss                                 building                           sky                                     √ò
                                                                         K                                                                                                              K + 1
                                                                        }                                                                                                              }
                                                                                         prediction 1                    prediction 2                          prediction N
                                                               per-pixel class
                                                                 predictions                                                                                    per-pixel binary mask loss
                                                                                                                                                                per-mask classification loss
                                         Figure 1: Per-pixel classiÔ¨Åcation vs. mask classiÔ¨Åcation. (left) Semantic segmentation with per-
                                         pixel classiÔ¨Åcation applies the same classiÔ¨Åcation loss to each location. (right) Mask classiÔ¨Åcation
                                         predicts a set of binary masks and assigns a single class to each mask. Each prediction is supervised
                                         with a per-pixel binary mask loss and a classiÔ¨Åcation loss. Matching between the set of predictions
                                         and ground truth segments can be done either via bipartite matching similarly to DETR [3] or by
                                         Ô¨Åxed matching via direct indexing if the number of predictions and classes match, i.e., if N = K.
                                         each consisting of a class prediction and a mask embedding vector. The mask embedding vector is
                                         used to get the binary mask prediction via a dot product with the per-pixel embedding obtained from
                                         an underlying fully-convolutional network. The new model solves both semantic- and instance-level
                                         segmentation tasks in a uniÔ¨Åed manner: no changes to the model, losses, and training procedure are
                                         required. SpeciÔ¨Åcally, for semantic and panoptic segmentation tasks alike, MaskFormer is supervised
                                         with the same per-pixel binary mask loss and a single classiÔ¨Åcation loss per mask. Finally, we design
                                         a simple inference strategy to blend MaskFormer outputs into a task-dependent prediction format.
                                         Weevaluate MaskFormer on Ô¨Åve semantic segmentation datasets with various numbers of categories:
                                         Cityscapes [13] (19 classes), Mapillary Vistas [31] (65 classes), ADE20K [49] (150 classes), COCO-
                                         Stuff-10K [2] (171 classes), and ADE20K-Full [49] (847 classes). While MaskFormer performs on
                                         parwithper-pixelclassiÔ¨ÅcationmodelsforCityscapes,whichhasafewdiverseclasses,thenewmodel
                                         demonstrates superior performance for datasets with larger vocabulary. We hypothesize that a single
                                         class prediction per mask models Ô¨Åne-grained recognition better than per-pixel class predictions.
                                         MaskFormerachievesthenewstate-of-the-art on ADE20K (55.6 mIoU) with Swin-Transformer [27]
                                         backbone, outperforming a per-pixel classiÔ¨Åcation model [27] with the same backbone by 2.1 mIoU,
                                         while being more efÔ¨Åcient (10% reduction in parameters and 40% reduction in FLOPs).
                                         Finally, we study MaskFormer‚Äôs ability to solve instance-level tasks using two panoptic segmentation
                                         datasets: COCO [26, 22] and ADE20K [49]. MaskFormer outperforms a more complex DETR
                                         model[3] with the same backbone and the same post-processing. Moreover, MaskFormer achieves
                                         the new state-of-the-art on COCO (52.7 PQ), outperforming prior state-of-the-art [38] by 1.6 PQ.
                                         Ourexperiments highlight MaskFormer‚Äôs ability to unify instance- and semantic-level segmentation.
                                         2      Related Works
                                         Both per-pixel classiÔ¨Åcation and mask classiÔ¨Åcation have been extensively studied for semantic
                                         segmentation. In early work, Konishi and Yuille [23] apply per-pixel Bayesian classiÔ¨Åers based on
                                         local image statistics. Then, inspired by early works on non-semantic groupings [11, 33], mask
                                         classiÔ¨Åcation-based methods became popular demonstrating the best performance in PASCAL VOC
                                         challenges [16]. Methods like O2P [4] and CFM [14] have achieved state-of-the-art results by
                                         classifying mask proposals [5, 36, 1]. In 2015, FCN [28] extended the idea of per-pixel classiÔ¨Åcation
                                         to deep nets, signiÔ¨Åcantly outperforming all prior methods on mIoU (a per-pixel evaluation metric
                                         which particularly suits the per-pixel classiÔ¨Åcation formulation of segmentation).
                                         Per-pixel classiÔ¨Åcation became the dominant way for deep-net-based semantic segmentation since
                                         the seminal work of Fully Convolutional Networks (FCNs) [28]. Modern semantic segmentation
                                         models focus on aggregating long-range context in the Ô¨Ånal feature map: ASPP [6, 7] uses atrous
                                         convolutions with different atrous rates; PPM [46] uses pooling operators with different kernel sizes;
                                         DANet[17],OCNet[45],andCCNet[21]usedifferentvariants of non-local blocks [39]. Recently,
                                         SETR[47]andSegmenter[34]replacetraditional convolutional backbones with Vision Transformers
                                         (ViT) [15] that capture long-range context starting from the very Ô¨Årst layer. However, these concur-
                                         rent Transformer-based [37] semantic segmentation approaches still use a per-pixel classiÔ¨Åcation
                                                                                                                    2
                             formulation. Note, that our MaskFormer module can convert any per-pixel classiÔ¨Åcation model to the
                             maskclassiÔ¨Åcation setting, allowing seamless adoption of advances in per-pixel classiÔ¨Åcation.
                             MaskclassiÔ¨Åcation is commonly used for instance-level segmentation tasks [18, 22]. These tasks
                             require a dynamic number of predictions, making application of per-pixel classiÔ¨Åcation challenging
                             as it assumes a static number of outputs. Omnipresent Mask R-CNN [19] uses a global classiÔ¨Åer to
                             classify mask proposals for instance segmentation. DETR [3] further incorporates a Transformer [37]
                             design to handle thing and stuff segmentation simultaneously for panoptic segmentation [22]. How-
                             ever, these mask classiÔ¨Åcation methods require predictions of bounding boxes, which may limit their
                             usage in semantic segmentation. The recently proposed Max-DeepLab [38] removes the dependence
                             onboxpredictions for panoptic segmentation with conditional convolutions [35, 40]. However, in
                             addition to the main mask classiÔ¨Åcation losses it requires multiple auxiliary losses (i.e., instance
                             discrimination loss, mask-ID cross entropy loss, and the standard per-pixel classiÔ¨Åcation loss).
                             3    FromPer-PixeltoMaskClassiÔ¨Åcation
                             In this section, we Ô¨Årst describe how semantic segmentation can be formulated as either a per-pixel
                             classiÔ¨Åcation or a mask classiÔ¨Åcation problem. Then, we introduce our instantiation of the mask
                             classiÔ¨Åcation model with the help of a Transformer decoder [37]. Finally, we describe simple
                             inference strategies to transform mask classiÔ¨Åcation outputs into task-dependent prediction formats.
                             3.1   Per-pixel classiÔ¨Åcation formulation
                             For per-pixel classiÔ¨Åcation, a segmentation model aims to predict the probability distribution over all
                                                                                                          K H¬∑W            K
                             possibleK categoriesforeverypixelofanH√óW image: y = {p |p ‚àà ‚àÜ }                      . Here‚àÜ istheK-
                                                                                                 i  i        i=1
                             dimensional probability simplex. Training a per-pixel classiÔ¨Åcation model is straight-forward: given
                                                              gt      gt  gt                 H¬∑W
                             ground truth category labels y      ={y |y ‚àà {1,...,K}}               for every pixel, a per-pixel cross-
                                                                      i   i                  i=1                 P
                             entropy (negative log-likelihood) loss is usually applied, i.e., L       (y,ygt) =     H¬∑W ‚àílogp (ygt).
                                                                                               pixel-cls            i=1          i  i
                             3.2   MaskclassiÔ¨Åcation formulation
                             MaskclassiÔ¨Åcationsplitsthesegmentationtaskinto1)partitioning/groupingtheimageintoN regions
                             (N does not need to equal K), represented with binary masks {m |m ‚àà [0,1]H√óW}N ; and 2)
                                                                                                      i   i                i=1
                             associating each region as a whole with some distribution over K categories. To jointly group and
                             classify a segment, i.e., to perform mask classiÔ¨Åcation, we deÔ¨Åne the desired output z as a set of N
                                                                           N
                             probability-mask pairs, i.e., z = {(p ,m )}       . In contrast to per-pixel class probability prediction,
                                                                    i   i  i=1
                             for mask classiÔ¨Åcation the probability distribution p ‚àà ‚àÜK+1 contains an auxiliary ‚Äúno object‚Äù label
                                                                                   i
                             (‚àÖ) in addition to the K category labels. The ‚àÖ label is predicted for masks that do not correspond to
                             any of the K categories. Note, mask classiÔ¨Åcation allows multiple mask predictions with the same
                             associated class, making it applicable to both semantic- and instance-level segmentation tasks.
                             Totrain a mask classiÔ¨Åcation model, a matching œÉ between the set of predictions z and the set of Ngt
                                                       gt      gt   gt   gt                  gt          H√óW Ngt                2
                             ground truth segments z     ={(c ,m )|c ‚àà{1,...,K},m ‚àà{0,1}                       }    is required. Here
                              gt                               i    i    i                   i                  i=1
                             c is the ground truth class of the ith ground truth segment. Since the size of prediction set |z| = N
                              i                      gt      gt                                      gt
                             andgroundtruth set |z | = N       generally differ, we assume N ‚â• N       andpadthesetofgroundtruth
                             labels with ‚Äúno object‚Äù tokens ‚àÖ to allow one-to-one matching.
                             Forsemanticsegmentation, atrivial Ô¨Åxed matching is possible if the number of predictions N matches
                             the number of category labels K. In this case, the ith prediction is matched to a ground truth region
                             with class label i and to ‚àÖ if a region with class label i is not present in the ground truth. In our
                             experiments, we found that a bipartite matching-based assignment demonstrates better results than
                             the Ô¨Åxed matching. Unlike DETR [3] that uses bounding boxes to compute the assignment costs
                             between prediction z and ground truth zgt for the matching problem, we directly use class and mask
                                                   i                    j
                                                     gt                  gt
                             predictions, i.e., ‚àíp (c ) + L      (m ,m ),whereL            is a binary mask loss.
                                                  i  j       mask    i   j            mask
                             Totrain model parameters, given a matching, the main mask classiÔ¨Åcation loss L               is composed
                                                                                                                  mask-cls
                             of a cross-entropy classiÔ¨Åcation loss and a binary mask loss L         for each predicted segment:
                                                                       h                       mask                   i
                                                               X
                                                         gt        N                  gt                            gt
                                           L        (z,z ) =            ‚àílogp       (c )+1 gt      L     (m     , m ) .             (1)
                                             mask-cls                           œÉ(j)  j      c 6=‚àÖ mask     œÉ(j)    j
                                                                   j=1                        j
                                 2Different mask classiÔ¨Åcation methods utilize various matching rules. For instance, Mask R-CNN [19] uses
                             a heuristic procedure based on anchor boxes and DETR [3] optimizes a bipartite matching between z and zgt.
                                                                                 3
                                            transformer module                                                         Ì†µ„åµ          classification loss           segmentation module
                                                                                transformer                        MLP             Ì†µ„åµ class predictions       semantic segmentation
                                                                                  decoder                  Ì†µ„åµ mask embeddings          Ì†µ„åµ√ó(Ì†µ„åµ + 1)            inference only
                                                                                   Ì†µ„åµ queries                                                                      drop ‚àÖ       semantic
                                            pixel-level module                                                ‚Ñ∞       Ì†µ„åµ √óÌ†µ„åµ        binary mask loss                         segmentation
                                                                                    pixel                  ‚Ñ∞ '()*       ‚Ñ∞                                                       Ì†µ„åµ√óÌ†µ„åµ√óÌ†µ„åµ
                                              backbone                           decoder                     "#$%&                Ì†µ„åµ mask predictions
                                                                                                         Ì†µ„åµ √óÌ†µ„åµ√óÌ†µ„åµ
                                                             image features ‚Ñ±                              ‚Ñ∞                            Ì†µ„åµ√óÌ†µ„åµ√óÌ†µ„åµ
                                                                                           per-pixel embeddings
                                          Figure 2: MaskFormer overview. We use a backbone to extract image features F. A pixel decoder
                                          gradually upsamples image features to extract per-pixel embeddings E                                               . A transformer decoder
                                                                                                                                                       pixel
                                          attendstoimagefeaturesandproducesN per-segmentembeddingsQ. Theembeddingsindependently
                                          generate N class predictions with N corresponding mask embeddings E                                              . Then, the model predicts
                                                                                                                                                     mask
                                          Npossiblyoverlapping binary mask predictions via a dot product between pixel embeddings E
                                                                                                                                                                                             pixel
                                          and mask embeddings E                       followed by a sigmoid activation. For semantic segmentation task we
                                                                                mask
                                          can get the Ô¨Ånal prediction by combining N binary masks with their class predictions using a simple
                                          matrix multiplication (see Section 3.4). Note, the dimensions for multiplication N are shown in gray.
                                          Note, that most existing mask classiÔ¨Åcation models use auxiliary losses (e.g., a bounding box
                                          loss [19, 3] or an instance discrimination loss [38]) in addition to L                                             . In the next section we
                                                                                                                                                   mask-cls
                                          present a simple mask classiÔ¨Åcation model that allows end-to-end training with L                                                       alone.
                                                                                                                                                                       mask-cls
                                          3.3      MaskFormer
                                          WenowintroduceMaskFormer,thenewmaskclassiÔ¨Åcationmodel,whichcomputesN probability-
                                                                                     N
                                          mask pairs z = {(p ,m )}                        .  The model contains three modules (see Fig. 2): 1) a pixel-level
                                                                          i     i    i=1
                                          modulethat extracts per-pixel embeddings used to generate binary mask predictions; 2) a transformer
                                          module, where a stack of Transformer decoder layers [37] computes N per-segment embeddings;
                                                                                                                                                       N
                                          and 3) a segmentation module, which generates predictions {(p ,m )}                                                from these embeddings.
                                                                                                                                            i      i   i=1
                                          During inference, discussed in Sec. 3.4, p and m are assembled into the Ô¨Ånal prediction.
                                                                                                          i           i
                                          Pixel-level module takes an image of size H √ó W as input. A backbone generates a (typically)
                                                                                                           C √óH√óW
                                          low-resolution image feature map F ‚àà R F                                 S     S , where CF is the number of channels and S
                                          is the stride of the feature map (CF depends on the speciÔ¨Åc backbone and we use S = 32 in this
                                          work). Then, a pixel decoder gradually upsamples the features to generate per-pixel embeddings
                                                         C √óH√óW
                                          E        ‚ààR E                 , where C is the embedding dimension. Note, that any per-pixel classiÔ¨Åcation-
                                            pixel                                      E
                                          based segmentation model Ô¨Åts the pixel-level module design including recent Transformer-based
                                          models [34, 47, 27]. MaskFormer seamlessly converts such a model to mask classiÔ¨Åcation.
                                          Transformer module uses the standard Transformer decoder [37] to compute from image features
                                          FandN learnablepositional embeddings (i.e., queries) its output, i.e., N per-segment embeddings
                                          Q‚ààRCQ√óN ofdimensionCQ that encode global information about each segment MaskFormer
                                          predicts. Similarly to [3], the decoder yields all predictions in parallel.
                                          Segmentation module applies a linear classiÔ¨Åer, followed by a softmax activation, on top of the
                                                                                                                                                         K+1 N
                                          per-segment embeddings Q to yield class probability predictions {p ‚àà ‚àÜ                                                }       for each segment.
                                                                                                                                                i                 i=1
                                          Note, that the classiÔ¨Åer predicts an additional ‚Äúno object‚Äù category (‚àÖ) in case the embedding does
                                          not correspond to any region. For mask prediction, a Multi-Layer Perceptron (MLP) with 2 hidden
                                          layers converts the per-segment embeddings Q to N mask embeddings E                                                 ‚ààRCE√óN ofdimension
                                                                                                                                                       mask
                                                                                                                                           H√óW
                                          C . Finally, we obtain each binary mask prediction m ‚àà [0,1]                                               via a dot product between the
                                             E                                                                               i
                                          ith mask embedding and per-pixel embeddings E                                     computedbythepixel-level module. The dot
                                                                                                                     pixel
                                                                                                                                                                   T
                                          product is followed by a sigmoid activation, i.e., m [h,w] = sigmoid(E                                             [:, i]   ¬∑ E      [:, h, w]).
                                                                                                                        i                              mask               pixel
                                          Note, we empirically Ô¨Ånd it is beneÔ¨Åcial to not enforce mask predictions to be mutually exclusive to
                                          each other by using a softmax activation. During training, the Lmask-cls loss combines a cross entropy
                                          classiÔ¨Åcation loss and a binary mask loss L                             for each predicted segment. For simplicity we use the
                                                                                                           mask
                                          sameL             as DETR[3], i.e., a linear combination of a focal loss [25] and a dice loss [30] multiplied
                                                     mask
                                          byhyper-parameters Œªfocal and Œªdice respectively.
                                                                                                                     4
                                             3.4      Mask-classiÔ¨Åcation inference
                                             First, we present a simple general inference procedure that converts mask classiÔ¨Åcation outputs
                                                              N
                                             {(p ,m )}               to either panoptic or semantic segmentation output formats. Then, we describe a
                                                  i      i    i=1
                                             semantic inference procedure speciÔ¨Åcally designed for semantic segmentation. We note, that the
                                             speciÔ¨Åc choice of inference strategy largely depends on the evaluation metric rather than the task.
                                             General inference partitions an image into segments by assigning each pixel [h,w] to one of the N
                                             predicted probability-mask pairs via argmax                                         p (c ) ¬∑ m [h,w]. Here c is the most likely class
                                                                                                                      i:c 6=‚àÖ      i   i         i                       i
                                             label c = argmax                                     p (c) for eachiprobability-mask pair i. Intuitively, this procedure
                                                        i                    c‚àà{1,...,K,‚àÖ} i
                                             assigns a pixel at location [h,w] to probability-mask pair i only if both the most likely class probability
                                             p (c ) and the mask prediction probability m [h,w] are high. Pixels assigned to the same probability-
                                               i    i                                                                i
                                             maskpairiformasegmentwhereeachpixelislabelledwithci. Forsemanticsegmentation,segments
                                             sharing the same category label are merged; whereas for instance-level segmentation tasks, the index
                                             i of the probability-mask pair helps to distinguish different instances of the same class. Finally, to
                                             reduce false positive rates in panoptic segmentation we follow previous inference strategies [3, 22].
                                             SpeciÔ¨Åcally, weÔ¨Ålteroutlow-conÔ¨Ådencepredictionspriortoinferenceandremovepredictedsegments
                                             that have large parts of their binary masks (m > 0.5) occluded by other predictions.
                                                                                                                       i
                                             Semantic inference is designed speciÔ¨Åcally for semantic segmentation and is done via a simple
                                             matrix multiplication. We empirically Ô¨Ånd that marginalization over probability-mask pairs, i.e.,
                                                                            P
                                             argmax                             N p(c)¬∑m [h,w],yieldsbetterresults than the hard assignment of each pixel
                                                           c‚àà{1,...,K}          i=1 i                i
                                             to a probability-mask pair i used in the general inference strategy. The argmax does not include the
                                            ‚Äúno object‚Äù category (‚àÖ) as standard semantic segmentation requires each output pixel to take a label.
                                                                                                                                      P
                                             Note, this strategy returns a per-pixel class probability                                    N p(c)¬∑m [h,w]. However,weobserve
                                                                                                                                          i=1 i                i
                                             that directly maximizing per-pixel class likelihood leads to poor performance. We hypothesize, that
                                             gradients are evenly distributed to every query, which complicates training.
                                             4      Experiments
                                             We demonstrate that MaskFormer seamlessly uniÔ¨Åes semantic- and instance-level segmentation
                                             tasks by showing state-of-the-art results on both semantic segmentation and panoptic segmentation
                                             datasets. Then, we ablate the MaskFormer design conÔ¨Årming that observed improvements in semantic
                                             segmentation indeed stem from the shift from per-pixel classiÔ¨Åcation to mask classiÔ¨Åcation.
                                             Datasets.           We study MaskFormer using four widely used semantic segmentation datasets:
                                             ADE20K [49] (150 classes) from the SceneParse150 challenge [48], COCO-Stuff-10K [2] (171
                                             classes), Cityscapes [13] (19 classes), and Mapillary Vistas [31] (65 classes). In addition, we use
                                             the ADE20K-Full [49] dataset annotated in an open vocabulary setting (we keep 874 classes that are
                                             presentinbothtrainandvalidationsets). ForpanoticsegmenationevaluationweuseCOCO[26,2,22]
                                             (80 ‚Äúthings‚Äù and 53 ‚Äústuff‚Äù categories) and ADE20K-Panoptic [49, 22] (100 ‚Äúthings‚Äù and 50 ‚Äústuff‚Äù
                                             categories). Please see the appendix for detailed descriptions of all used datasets.
                                             Evaluationmetrics. ForsemanticsegmentationthestandardmetricismIoU(meanIntersection-over-
                                             Union) [16], a per-pixel metric that directly corresponds to the per-pixel classiÔ¨Åcation formulation.
                                             Tobetter illustrate the difference between segmentation approaches, in our ablations we supplement
                                             mIoUwithPQSt (PQstuff)[22], a per-region metric that treats all classes as ‚Äústuff‚Äù and evaluates
                                             each segment equally, irrespective of its size. We report the median of 3 runs for all datasets, except
                                             for Cityscapes where we report the median of 5 runs. For panoptic segmentation, we use the standard
                                             PQ(panoptic quality) metric [22] and report single run results due to prohibitive training costs.
                                             Baseline models. On the right we                                                                per-pixel                        transformer            per-pixel
                                             sketchtheusedper-pixelclassiÔ¨Åcation                                                               loss                             decoder         Ì†µ„åµ√óÌ†µ„åµ   loss
                                             baselines. The PerPixelBaselineuses                                                                  t                                                       t
                                             thepixel-levelmoduleofMaskFormer                                                       pixel         u                            pixel                      u
                                                                                                               backbone                           p      backbone                                         p
                                                                                                                                  decoder         t                          decoder                      t
                                             and directly outputs per-pixel class                                                                 ou                                                      ou
                                             scores. For a fair comparison, we de-                                                           Ì†µ„åµ√óÌ†µ„åµ√óÌ†µ„åµ                                  Ì†µ„åµ√óÌ†µ„åµ√óÌ†µ„åµ      Ì†µ„åµ√óÌ†µ„åµ√óÌ†µ„åµ
                                             sign PerPixelBaseline+ which adds                                       (a) PerPixelBaseline                             (b) PerPixelBaseline+
                                             the transformer module and mask em-
                                             bedding MLPtothePerPixelBaseline. Thus, PerPixelBaseline+ and MaskFormer differ only in the
                                             formulation: per-pixel vs. mask classiÔ¨Åcation. Note that these baselines are for ablation and we
                                             compare MaskFormer with state-of-the-art per-pixel classiÔ¨Åcation models as well.
                                                                                                                             5
                                       4.1     Implementation details
                                       Backbone. MaskFormer is compatible with any backbone architecture. In our work we use the stan-
                                       dard convolution-based ResNet [20] backbones (R50 and R101 with 50 and 101 layers respectively)
                                       and recently proposed Transformer-based Swin-Transformer [27] backbones. In addition, we use the
                                       R101cmodel[6]whichreplaces the Ô¨Årst 7√ó7 convolution layer of R101 with 3 consecutive 3 √ó 3
                                       convolutions and which is popular in the semantic segmentation community [46, 7, 8, 21, 44, 10].
                                       Pixel decoder. The pixel decoder in Figure 2 can be implemented using any semantic segmentation
                                       decoder (e.g., [8‚Äì10]). Many per-pixel classiÔ¨Åcation methods use modules like ASPP [6] or PSP [46]
                                       to collect and distribute context across locations. The Transformer module attends to all image
                                       features, collecting global information to generate class predictions. This setup reduces the need
                                       of the per-pixel module for heavy context aggregation. Therefore, for MaskFormer, we design a
                                       light-weight pixel decoder based on the popular FPN [24] architecture.
                                       Following FPN, we 2√ó upsample the low-resolution feature map in the decoder and sum it with the
                                       projected feature map of corresponding resolution from the backbone; Projection is done to match
                                       channel dimensions of the feature maps with a 1 √ó 1 convolution layer followed by GroupNorm
                                       (GN)[41]. Next, we fuse the summed features with an additional 3 √ó 3 convolution layer followed
                                       byGNandReLUactivation. Werepeatthis process starting with the stride 32 feature map until we
                                       obtain a Ô¨Ånal feature map of stride 4. Finally, we apply a single 1 √ó 1 convolution layer to get the
                                       per-pixel embeddings. All feature maps in the pixel decoder have a dimension of 256 channels.
                                       Transformer decoder. We use the same Transformer decoder design as DETR [3]. The N query
                                       embeddings are initialized as zero vectors, and we associate each query with a learnable positional
                                       encoding. We use 6 Transformer decoder layers with 100 queries by default, and, following DETR,
                                       we apply the same loss after each decoder. In our experiments we observe that MaskFormer is
                                       competitive for semantic segmentation with a single decoder layer too, whereas for instance-level
                                       segmentation multiple layers are necessary to remove duplicates from the Ô¨Ånal predictions.
                                       Segmentation module. The multi-layer perceptron (MLP) in Figure 2 has 2 hidden layers of 256
                                       channels to predict the mask embeddings E                         , analogously to the box head in DETR. Both per-pixel
                                                                                                    mask
                                       E       and mask E             embeddings have 256 channels.
                                         pixel                  mask
                                                                                                                                                                              gt
                                       Loss weights. We use focal loss [25] and dice loss [30] for our mask loss: L                                                 (m,m ) =
                                                                gt                            gt                                                               mask
                                       Œª       L      (m,m )+Œª L (m,m ),andsetthehyper-parameterstoŒª                                                      =20.0andŒª                =
                                         focal   focal                    dice  dice                                                                focal                     dice
                                       1.0. Following DETR [3], the weight for the ‚Äúno object‚Äù (‚àÖ) in the classiÔ¨Åcation loss is set to 0.1.
                                       4.2     Training settings
                                       Semantic segmentation. We use Detectron2 [42] and follow the commonly used training settings
                                       for each dataset. More speciÔ¨Åcally, we use AdamW [29] and the poly [6] learning rate schedule
                                                                                      ‚àí4                                       ‚àí4
                                       with an initial learning rate of 10                 and a weight decay of 10                 for ResNet [20] backbones, and an
                                                                                ‚àí5                                       ‚àí2
                                       initial learning rate of 6 ¬∑ 10               and a weight decay of 10                 for Swin-Transformer [27] backbones.
                                       Backbones are pre-trained on ImageNet-1K [32] if not stated otherwise. A learning rate multiplier of
                                       0.1 is applied to CNN backbones and 1.0 is applied to Transformer backbones. The standard random
                                       scale jittering between 0.5 and 2.0, random horizontal Ô¨Çipping, random cropping as well as random
                                       color jittering are used as data augmentation [12]. For the ADE20K dataset, if not stated otherwise,
                                       weuseacropsizeof512√ó512,abatchsizeof16andtrainallmodelsfor160kiterations. For the
                                       ADE20K-Fulldataset, we use the same setting as ADE20K except that we train all models for 200k
                                       iterations. For the COCO-Stuff-10k dataset, we use a crop size of 640 √ó 640, a batch size of 32
                                       and train all models for 60k iterations. All models are trained with 8 V100 GPUs. We report both
                                       performance of single scale (s.s.) inference and multi-scale (m.s.) inference with horizontal Ô¨Çip and
                                       scales of 0.5, 0.75, 1.0, 1.25, 1.5, 1.75. See appendix for Cityscapes and Mapillary Vistas settings.
                                       Panoptic segmentation. We follow exactly the same architecture, loss, and training procedure as
                                       weuseforsemantic segmentation. The only difference is supervision: i.e., category region masks
                                       in semantic segmentation vs. object instance masks in panoptic segmentation. We strictly follow
                                       the DETR[3]setting to train our model on the COCO panoptic segmentation dataset [22] for a fair
                                       comparison. On the ADE20K panoptic segmentation dataset, we follow the semantic segmentation
                                       setting but train for longer (720k iterations) and use a larger crop size (640 √ó 640). COCO models
                                       are trained using 64 V100 GPUs and ADE20K experiments are trained with 8 V100 GPUs. We use
                                                                                                              6
                                    Table 1: Semantic segmentation on ADE20K val with 150 categories. Mask classiÔ¨Åcation-based
                                    MaskFormeroutperforms the best per-pixel classiÔ¨Åcation approaches while using fewer parameters
                                    and less computation. We report both single-scale (s.s.) and multi-scale (m.s.) inference results with
                                    ¬±std. FLOPs are computed for the given crop size. Frames-per-second (fps) is measured on a V100
                                    GPUwithabatchsizeof1.3 Backbonespre-trained on ImageNet-22K are marked with ‚Ä†.
                                             method                  backbone      crop size     mIoU(s.s.)      mIoU(m.s.)       #params.     FLOPs        fps
                                             OCRNet[44]                R101c     520√ó520           -              45.3                -           -          -
                                             DeepLabV3+[8]              R50c     512√ó512         44.0             44.9              44M         177G       21.0
                                         backbones                     R101c     512√ó512         45.5             46.4              63M         255G       14.2
                                             MaskFormer(ours)           R50      512√ó512         44.5 ¬±0.5        46.7 ¬±0.6         41M          53G       24.5
                                         CNN                           R101      512√ó512         45.5 ¬±0.5        47.2 ¬±0.2         60M          73G       19.5
                                                                       R101c     512√ó512         46.0 ¬±0.1        48.1 ¬±0.2         60M          80G       19.0
                                                                            ‚Ä†
                                             SETR[47]                 ViT-L      512√ó512           -              50.3             308M           -          -
                                                                      Swin-T     512√ó512           -              46.1              60M         236G       18.5
                                             Swin-UperNet [27, 43]    Swin-S‚Ä†    512√ó512           -              49.3              81M         259G       15.2
                                                                      Swin-B     640√ó640           -              51.6             121M         471G         8.7
                                         backbones                    Swin-L‚Ä†    640√ó640           -              53.5             234M         647G         6.2
                                                                      Swin-T     512√ó512         46.7 ¬±0.7        48.8 ¬±0.6         42M          55G       22.1
                                             MaskFormer(ours)         Swin-S     512√ó512         49.8 ¬±0.4        51.0 ¬±0.4         63M          79G       19.6
                                         ransformer                   Swin-B     640√ó640         51.1 ¬±0.2        52.3 ¬±0.4        102M         195G       12.6
                                         T                            Swin-B‚Ä†    640√ó640         52.7 ¬±0.4        53.9 ¬±0.2        102M         195G       12.6
                                                                      Swin-L‚Ä†    640√ó640         54.1 ¬±0.2        55.6 ¬±0.1        212M         375G         7.9
                                    Table 2: MaskFormervs.per-pixelclassiÔ¨Åcationbaselineson4semanticsegmentationdatasets.
                                    MaskFormer improvement is larger when the number of classes is larger. We use a ResNet-50
                                    backbone and report single scale mIoU and PQSt for ADE20K, COCO-Stuff and ADE20K-Full,
                                    whereas for higher-resolution Cityscapes we use a deeper ResNet-101 backbone following [7, 8].
                                                           Cityscapes (19 classes)    ADE20K(150classes)        COCO-Stuff(171classes)     ADE20K-Full(847classes)
                                                                             St                          St                         St                         St
                                                            mIoU          PQ           mIoU           PQ           mIoU          PQ           mIoU           PQ
                                    PerPixelBaseline      77.4         58.9          39.2          21.6          32.4         15.5          12.4           5.8
                                    PerPixelBaseline+     78.5         60.2          41.9          28.3          34.2         24.6          13.9           9.0
                                    MaskFormer(ours) 78.5(+0.0)        63.1 (+2.9)   44.5 (+2.6)   33.4 (+5.1)   37.1 (+2.9)  28.9 (+4.3)   17.4 (+3.5)   11.9 (+2.9)
                                    the general inference (Section 3.4) with the following parameters: we Ô¨Ålter out masks with class
                                    conÔ¨Ådence below 0.8 and set masks whose contribution to the Ô¨Ånal panoptic segmentation is less
                                    than 80% of its mask area to VOID. We report performance of single scale inference.
                                    4.3    Mainresults
                                    Semantic segmentation. In Table 1, we compare MaskFormer with state-of-the-art per-pixel classi-
                                    Ô¨Åcation models for semantic segmentation on the ADE20K val set. With the same standard CNN
                                    backbones(e.g., ResNet [20]), MaskFormer outperforms DeepLabV3+ [8] by 1.7 mIoU. MaskFormer
                                    is also compatible with recent Vision Transformer [15] backbones (e.g., the Swin Transformer [27]),
                                    achieving a new state-of-the-art of 55.6 mIoU, which is 2.1 mIoU better than the prior state-of-the-
                                    art [27]. Observe that MaskFormer outperforms the best per-pixel classiÔ¨Åcation-based models while
                                    having fewer parameters and faster inference time. This result suggests that the mask classiÔ¨Åcation
                                    formulation has signiÔ¨Åcant potential for semantic segmentation. See appendix for results on test set.
                                    Beyond ADE20K, we further compare MaskFormer with our baselines on COCO-Stuff-10K,
                                    ADE20K-Full as well as Cityscapes in Table 2 and we refer to the appendix for comparison with
                                    state-of-the-art methods on these datasets. The improvement of MaskFormer over PerPixelBase-
                                    line+ is larger when the number of classes is larger: For Cityscapes, which has only 19 categories,
                                    MaskFormerperforms similarly well as PerPixelBaseline+; While for ADE20K-Full, which has 847
                                    classes, MaskFormer outperforms PerPixelBaseline+ by 3.5 mIoU.
                                    Although MaskFormer shows no improvement in mIoU for Cityscapes, the PQSt metric increases
                                                  St                                                                                                      St
                                    by 2.9 PQ . We Ô¨Ånd MaskFormer performs better in terms of recognition quality (RQ ) while
                                    lagging in per-pixel segmentation quality (SQSt) (we refer to the appendix for detailed numbers).
                                    This observation suggests that on datasets where class recognition is relatively easy to solve, the main
                                    challenge for mask classiÔ¨Åcation-based approaches is pixel-level accuracy (i.e., mask quality).
                                        3It isn‚Äôt recommended to compare fps from different papers: speed is measured in different environments.
                                    DeepLabV3+fpsarefromMMSegmentation[12],andSwin-UperNetfpsarefromtheoriginalpaper[27].
                                                                                                    7
                                  Table 3: Panoptic segmentation on COCO panoptic val with 133 categories. MaskFormer seam-
                                   lessly uniÔ¨Åes semantic- and instance-level segmentation without modifying the model architecture
                                   or loss. Our model, which achieves better results, can be regarded as a box-free simpliÔ¨Åcation of
                                   DETR[3]. The major improvement comes from ‚Äústuff‚Äù classes (PQSt) which are ambiguous to
                                   represent with bounding boxes. For MaskFormer (DETR) we use the exact same post-processing
                                   as DETR. Note, that in this setting MaskFormer performance is still better than DETR (+2.2 PQ).
                                   Ourmodelalsooutperforms recently proposed Max-DeepLab [38] without the need of sophisticated
                                   auxiliary losses, while being more efÔ¨Åcient. FLOPs are computed as the average FLOPs over 100
                                  validation images (COCO images have varying sizes). Frames-per-second (fps) is measured on a
                                  V100 GPU with a batch size of 1 by taking the average runtime on the entire val set including
                                   post-processing time. Backbones pre-trained on ImageNet-22K are marked with ‚Ä†.
                                        method                  backbone       PQ         PQTh          PQSt        SQ       RQ     #params.   FLOPs      fps
                                        DETR[3]                R50+6Enc       43.4     48.2          36.3           79.3    53.8        -         -        -
                                        MaskFormer(DETR)       R50+6Enc       45.6     50.0 (+1.8)   39.0 (+2.7)    80.2    55.8        -         -        -
                                      backbonesMaskFormer(ours)R50+6Enc       46.5     51.0 (+2.8)   39.8 (+3.5)    80.4    56.8      45M       181G     17.6
                                        DETR[3]               R101+6Enc       45.1     50.5          37.0           79.9    55.5        -         -        -
                                      CNNMaskFormer(ours)     R101+6Enc       47.6     52.5 (+2.0)   40.3 (+3.3)    80.7    58.0      64M       248G     14.0
                                        Max-DeepLab[38]          Max-S        48.4     53.0          41.5            -        -       62M       324G      7.6
                                                                 Max-L        51.1     57.0          42.2            -        -      451M      3692G       -
                                      backbones                 Swin-T        47.7     51.7          41.7           80.4    58.3      42M       179G     17.0
                                                                Swin-S        49.7     54.4          42.6           80.9    60.4      63M       259G     12.4
                                        MaskFormer(ours)        Swin-B        51.1     56.3          43.2           81.4    61.8     102M       411G      8.4
                                                                Swin-B‚Ä†       51.8     56.9          44.1           81.4    62.6     102M       411G      8.4
                                      ransformer                       ‚Ä†
                                      T                         Swin-L        52.7     58.5          44.0           81.8    63.5     212M       792G      5.2
                                   Panoptic segmentation. In Table 3, we compare the same exact MaskFormer model with DETR [3]
                                   ontheCOCOpanopticvalset. TomatchthestandardDETRdesign,weadd6additionalTransformer
                                   encoder layers after the CNN backbone. Unlike DETR, our model does not predict bounding boxes
                                   but instead predicts masks directly. MaskFormer achieves better results while being simpler than
                                   DETR.Todisentangle the improvements from the model itself and our post-processing inference
                                   strategy we run our model following DETR post-processing (MaskFormer (DETR)) and observe that
                                   this setup outperforms DETR by 2.2 PQ. Overall, we observe a larger improvement in PQSt compared
                                   to PQTh. This suggests that detecting ‚Äústuff‚Äù with bounding boxes is suboptimal, and therefore, box-
                                   basedsegmentationmodels(e.g., MaskR-CNN[19])donotsuitsemanticsegmentation. MaskFormer
                                   also outperforms recently proposed Max-DeepLab [38] without the need of special network design
                                   as well as sophisticated auxiliary losses (i.e., instance discrimination loss, mask-ID cross entropy
                                   loss, and per-pixel classiÔ¨Åcation loss in [38]). MaskFormer, for the Ô¨Årst time, uniÔ¨Åes semantic- and
                                   instance-level segmentation with the exact same model, loss, and training pipeline.
                                  Wefurther evaluate our model on the panoptic segmentation version of the ADE20K dataset. Our
                                   model also achieves state-of-the-art performance. We refer to the appendix for detailed results.
                                   4.4    Ablation studies
                                  Weperformaseries of ablation studies of MaskFormer using a single ResNet-50 backbone [20].
                                   Per-pixel vs. mask classiÔ¨Åcation. In Table 4, we verify that the gains demonstrated by MaskFromer
                                   comefromshifting the paradigm to mask classiÔ¨Åcation. We start by comparing PerPixelBaseline+
                                   and MaskFormer. The models are very similar and there are only 3 differences: 1) per-pixel vs.
                                   mask classiÔ¨Åcation used by the models, 2) MaskFormer uses bipartite matching, and 3) the new
                                   model uses a combination of focal and dice losses as a mask loss, whereas PerPixelBaseline+
                                   utilizes per-pixel cross entropy loss. First, we rule out the inÔ¨Çuence of loss differences by training
                                   PerPixelBaseline+ with exactly the same losses and observing no improvement. Next, in Table 4a, we
                                   comparePerPixelBaseline+withMaskFormertrainedusingaÔ¨Åxedmatching(MaskFormer-Ô¨Åxed),i.e.,
                                   N=Kandassignmentdonebasedoncategorylabelindicesidenticallytotheper-pixelclassiÔ¨Åcation
                                   setup. We observe that MaskFormer-Ô¨Åxed is 1.8 mIoU better than the baseline, suggesting that
                                   shifting from per-pixel classiÔ¨Åcation to mask classiÔ¨Åcation is indeed the main reason for the gains of
                                   MaskFormer. In Table 4b, we further compare MaskFormer-Ô¨Åxed with MaskFormer trained with
                                   bipartite matching (MaskFormer-bipartite) and Ô¨Ånd bipartite matching is not only more Ô¨Çexible
                                  (allowing to predict less masks than the total number of categories) but also produces better results.
                                                                                                8
                                      Table 4: Per-pixel vs. mask classiÔ¨Åcation for semantic segmentation. All models use 150 queries
                                      for a fair comparison. We evaluate the models on ADE20K val with 150 categories. 4a: PerPixel-
                                      Baseline+ and MaskFormer-Ô¨Åxed use similar Ô¨Åxed matching (i.e., matching by category index), this
                                      result conÔ¨Årms that the shift from per-pixel to mask classiÔ¨Åcation is the key. 4b: bipartite matching is
                                      not only more Ô¨Çexible (can make less prediction than total class count) but also gives better results.
                                                   (a) Per-pixel vs. mask classiÔ¨Åcation.                   (b) Fixed vs. bipartite matching assignment.
                                                                                            St                                                               St
                                                                         mIoU            PQ                                                 mIoU          PQ
                                                  PerPixelBaseline+    41.9           28.3               MaskFormer-Ô¨Åxed                 43.7          30.3
                                                  MaskFormer-Ô¨Åxed      43.7 (+1.8)    30.3 (+2.0)        MaskFormer-bipartite (ours) 44.2 (+0.5)       33.4 (+3.1)
                                      Number of queries. The table to the right                                                   ADE20K        COCO-Stuff    ADE20K-Full
                                                                                                                                           St             St              St
                                      shows results of MaskFormer trained with a                               # of queries     mIoU PQ        mIoU PQ        mIoU PQ
                                      varying number of queries on datasets with dif-                       PerPixelBaseline+    41.9   28.3    34.2   24.6    13.9    9.0
                                      ferent number of categories. The model with                                   20           42.9   32.6    35.0   27.6    14.1    10.8
                                     100queriesconsistentlyperformsthebestacross                                    50           43.9   32.7    35.5   27.9    15.4    11.1
                                      the studied datasets. This suggest we may not                                100           44.5   33.4    37.1   28.9    16.0    11.9
                                      need to adjust the number of queries w.r.t. the                              150           44.2   33.4    37.0   28.9    15.5    11.5
                                      numberofcategories or datasets much. Interest-                               300           43.5   32.3    36.1   29.1    14.2    10.3
                                      ingly, even with 20 queries MaskFormer outper-                              1000           35.4   26.7    34.4   27.6     8.0     5.8
                                      forms our per-pixel classiÔ¨Åcation baseline.
                                      Wefurther calculate the number of classes which are on average present in a training set image.
                                      WeÔ¨Åndthesestatistics to be similar across datasets despite the fact that the datasets have different
                                      numberoftotal categories: 8.2 classes per image for ADE20K (150 classes), 6.6 classes per image
                                      for COCO-Stuff-10K (171 classes) and 9.1 classes per image for ADE20K-Full (847 classes). We
                                      hypothesize that each query is able to capture masks from multiple categories.
                                      TheÔ¨Åguretothe right shows the num-
                                      ber of unique categories predicted by                        25
                                      each query (sorted in descending or-                         20
                                                                                              by   15
                                      der) of our MaskFormer model on the                          10
                                      validation sets of the corresponding                         5
                                      datasets. Interestingly, the number of                     set0   0            20            40            60            80           100
                                                                                              predicted                     (a) ADE20K(150classes)
                                      unique categories per query does not                         35
                                                                                                 alidation30
                                      follow a uniform distribution: some                        v 25
                                                                                              classes20
                                      queries capture more classes than oth-                     on15
                                                                                                   10
                                                                                                   5
                                      ers.   We try to analyze how Mask-                      uniquequery00          20            40            60            80           100
                                      Former queries group categories, but                    of                        (b) COCO-Stuff-10K (171 classes)
                                      wedonotobserveanyobviouspattern:                           each100
                                                                                                   80
                                      there are queries capturing categories                  Number60
                                      with similar semantics or shapes (e.g.,                      40
                                                                                                   20
                                     ‚Äúhouse‚Äù and ‚Äúbuilding‚Äù), but there are                         0   0            20            40            60            80           100
                                      also queries capturing completely dif-                                              (c) ADE20K-Full (847 classes)
                                      ferent categories (e.g., ‚Äúwater‚Äù and ‚Äúsofa‚Äù).
                                      NumberofTransformerdecoderlayers. Interestingly,MaskFormerwithevenasingleTransformer
                                      decoder layer already performs well for semantic segmentation and achieves better performance than
                                      our 6-layer-decoder PerPixelBaseline+. For panoptic segmentation, however, multiple decoder layers
                                      are required to achieve competitive performance. Please see the appendix for a detailed discussion.
                                      5     Discussion
                                      OurmaingoalistoshowthatmaskclassiÔ¨Åcation is a general segmentation paradigm that could be a
                                      competitive alternative to per-pixel classiÔ¨Åcation for semantic segmentation. To better understand its
                                      potential for segmentation tasks, we focus on exploring mask classiÔ¨Åcation independently of other
                                      factors like architecture, loss design, or augmentation strategy. We pick the DETR [3] architecture
                                      as our baseline for its simplicity and deliberately make as few architectural changes as possible.
                                      Therefore, MaskFormer can be viewed as a ‚Äúbox-free‚Äù version of DETR.
                                                                                                         9
                              Table 5: Matching with masks vs. boxes. We compare DETR [3] which uses box-based matching
                              with two MaskFormer models trained with box- and mask-based matching respectively. To use
                               box-based matching in MaskFormer we add to the model an additional box prediction head as in
                               DETR.Note,that with box-based matching MaskFormer performs on par with DETR, whereas with
                               mask-based matching it shows better results. The evaluation is done on COCO panoptic val set.
                                                                                                           Th       St
                                                    method              backbone   matching     PQ      PQ        PQ
                                                    DETR[3]            R50+6Enc     by box     43.4      48.2     36.3
                                                    MaskFormer(ours)   R50+6Enc     by box     43.7      49.2     35.3
                                                                       R50+6Enc     by mask    46.5      51.0     39.8
                               In this section, we discuss in detail the differences between MaskFormer and DETR and show how
                               these changes are required to ensure that mask classiÔ¨Åcation performs well. First, to achieve a
                               pure mask classiÔ¨Åcation setting we remove the box prediction head and perform matching between
                               prediction and ground truth segments with masks instead of boxes. Secondly, we replace the compute-
                               heavy per-query mask head used in DETR with a more efÔ¨Åcient per-image FPN-based head to make
                               end-to-end training without box supervision feasible.
                               Matching with masks is superior to matching with boxes. We compare MaskFormer models
                               trained using matching with boxes or masks in Table 5. To do box-based matching, we add to
                               MaskFormeranadditional box prediction head as in DETR [3]. Observe that MaskFormer, which
                               directly matches with mask predictions, has a clear advantage. We hypothesize that matching with
                               boxes is more ambiguous than matching with masks, especially for stuff categories where completely
                               different masks can have similar boxes as stuff regions often spread over a large area in an image.
                               MaskFormer mask head reduces computation. Results in Table 5 also show that MaskFormer
                               performsonparwithDETRwhenthesamematchingstrategyisused. Thissuggeststhatthedifference
                               in mask head designs between the models does not signiÔ¨Åcantly inÔ¨Çuence the prediction quality. The
                               newhead, however, has signiÔ¨Åcantly lower computational and memory costs in comparison with the
                               original mask head used in DETR. In MaskFormer, we Ô¨Årst upsample image features to get high-
                               resolution per-pixel embeddings and directly generate binary mask predictions at a high-resolution.
                               Note, that the per-pixel embeddings from the upsampling module (i.e., pixel decoder) are shared
                               amongallqueries. In contrast, DETR Ô¨Årst generates low-resolution attention maps and applies an
                               independent upsampling module to each query. Thus, the mask head in DETR is N times more
                               computationally expensive than the mask head in MaskFormer (where N is the number of queries).
                               6   Conclusion
                              The paradigm discrepancy between semantic- and instance-level segmentation results in entirely
                               different models for each task, hindering development of image segmentation as a whole. We show
                               that a simple mask classiÔ¨Åcation model can outperform state-of-the-art per-pixel classiÔ¨Åcation models,
                               especially in the presence of large number of categories. Our model also remains competitive for
                               panoptic segmentation, without a need to change model architecture, losses, or training procedure.
                              WehopethisuniÔ¨Åcation spurs a joint effort across semantic- and instance-level segmentation tasks.
                               AcknowledgmentsandDisclosureofFunding
                              WethankRossGirshick for insightful comments and suggestions. Work of UIUC authors Bowen
                               ChengandAlexanderG.SchwingwassupportedinpartbyNSFunderGrant#1718221,2008387,
                               2045586, 2106825, MRI #1725729, NIFA award 2020-67021-32799 and Cisco Systems Inc. (Gift
                              AwardCG1377144-thanksforaccesstoArcetri).
                               References
                                [1] Pablo Arbel√°ez, Jordi Pont-Tuset, Jonathan T Barron, Ferran Marques, and Jitendra Malik. Multiscale
                                    combinatorial grouping. In CVPR, 2014. 2
                                [2] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-Stuff: Thing and stuff classes in context. In
                                    CVPR,2018. 2, 5
                                [3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
                                    Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 1, 2, 3, 4, 5, 6, 8, 9, 10
                                                                                    10
             [4] Joao Carreira, Rui Caseiro, Jorge Batista, and Cristian Sminchisescu. Semantic segmentation with
               second-order pooling. In ECCV, 2012. 1, 2
             [5] Joao Carreira and Cristian Sminchisescu. CPMC: Automatic object segmentation using constrained
               parametric min-cuts. PAMI, 2011. 2
             [6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. DeepLab:
               Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs.
               PAMI,2018. 2, 6
             [7] Liang-ChiehChen,GeorgePapandreou,FlorianSchroff,andHartwigAdam. Rethinkingatrousconvolution
               for semantic image segmentation. arXiv:1706.05587, 2017. 2, 6, 7
             [8] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder
               with atrous separable convolution for semantic image segmentation. In ECCV, 2018. 1, 6, 7
             [9] Bowen Cheng, Liang-Chieh Chen, Yunchao Wei, Yukun Zhu, Zilong Huang, Jinjun Xiong, Thomas S
               Huang, Wen-MeiHwu,andHonghuiShi. SPGNet: Semanticpredictionguidance for scene parsing. In
               ICCV,2019.
             [10] BowenCheng,MaxwellDCollins,YukunZhu,TingLiu,ThomasSHuang,HartwigAdam,andLiang-
               Chieh Chen. Panoptic-DeepLab: A simple, strong, and fast baseline for bottom-up panoptic segmentation.
               In CVPR, 2020. 6
             [11] Dorin Comaniciu and Peter Meer. Robust Analysis of Feature Spaces: Color Image Segmentation. In
               CVPR,1997. 2
             [12] MMSegmentation Contributors. MMSegmentation: OpenMMLab semantic segmentation toolbox and
               benchmark. https://github.com/open-mmlab/mmsegmentation,2020. 6,7
             [13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benen-
               son, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene
               understanding. In CVPR, 2016. 2, 5
             [14] Jifeng Dai, Kaiming He, and Jian Sun. Convolutional feature masking for joint object and stuff segmenta-
               tion. In CVPR, 2015. 2
             [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
               Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
               16x16words: Transformers for image recognition at scale. In ICLR, 2021. 2, 7
             [16] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew
               Zisserman. The PASCAL visual object classes challenge: A retrospective. IJCV, 2015. 2, 5
             [17] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention
               network for scene segmentation. In CVPR, 2019. 2
             [18] Bharath Hariharan, Pablo Arbel√°ez, Ross Girshick, and Jitendra Malik. Simultaneous detection and
               segmentation. In ECCV, 2014. 1, 3
             [19] Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick. Mask R-CNN. In ICCV, 2017. 1, 3, 4, 8
             [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
               In CVPR, 2016. 6, 7, 8
             [21] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. CCNet:
               Criss-cross attention for semantic segmentation. In ICCV, 2019. 2, 6
             [22] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll√°r. Panoptic segmentation.
               In CVPR, 2019. 2, 3, 5, 6
             [23] Scott Konishi and Alan Yuille. Statistical Cues for Domain SpeciÔ¨Åc Image Segmentation with Performance
               Analysis. In CVPR, 2000. 2
             [24] Tsung-Yi Lin, Piotr Doll√°r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature
               pyramid networks for object detection. In CVPR, 2017. 6
             [25] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll√°r. Focal loss for dense object
               detection. In ICCV, 2017. 4, 6
             [26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,
               and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 2, 5
             [27] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
               transformer: Hierarchical vision transformer using shifted windows. arXiv:2103.14030, 2021. 2, 4, 6, 7
             [28] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-
               tion. In CVPR, 2015. 1, 2
             [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 6
             [30] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net: Fully convolutional neural networks
               for volumetric medical image segmentation. In 3DV, 2016. 4, 6
             [31] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul√≤, and Peter Kontschieder. The mapillary vistas
               dataset for semantic understanding of street scenes. In CVPR, 2017. 2, 5
             [32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
               Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large
               Scale Visual Recognition Challenge. IJCV, 2015. 6
             [33] Jianbo Shi and Jitendra Malik. Normalized Cuts and Image Segmentation. PAMI, 2000. 2
             [34] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic
               segmentation. arXiv:2105.05633, 2021. 2, 4
             [35] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In ECCV,
               2020. 3
                                  11
             [36] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeulders. Selective search
               for object recognition. IJCV, 2013. 2
             [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
               Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 1, 2, 3, 4
             [38] HuiyuWang,YukunZhu,HartwigAdam,AlanYuille,andLiang-ChiehChen. MaX-DeepLab: End-to-end
               panoptic segmentation with mask transformers. In CVPR, 2021. 2, 3, 4, 8
             [39] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR,
               2018. 2
             [40] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. SOLOv2: Dynamic and fast instance
               segmentation. NeurIPS, 2020. 3
             [41] Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. 6
             [42] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https:
               //github.com/facebookresearch/detectron2,2019. 6
             [43] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. UniÔ¨Åed perceptual parsing for scene
               understanding. In ECCV, 2018. 7
             [44] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation.
               In ECCV, 2020. 6, 7
             [45] Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. OCNet: Object
               context for semantic segmentation. IJCV, 2021. 2
             [46] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing
               network. In CVPR, 2017. 1, 2, 6
             [47] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng
               Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence
               perspective with transformers. In CVPR, 2021. 2, 4, 7
             [48] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing
               challenge 2016. http://sceneparsing.csail.mit.edu/index_challenge.html, 2016. 5
             [49] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing
               through ADE20Kdataset. In CVPR, 2017. 2, 5
                                  12
