                      Published as a conference paper at ICLR 2025
                      Figure 11: Above plots representing the performance of LSTM-based MIND model configurations on
                      WikiText-2 and IMDB datasets. Left plot indicates the test accuracy for both datasets and Right plot shows
                      the loss for both datasets
                      Table 9: Ablation Study Results on WikiText-103. The table compares the performance of the full MIND-
                      Transformer model and its variants with specific components removed.
                                    ModelVariant                Perplexity â†“   Params(M)      Avg. FLOPs(G)
                                    MIND-Transformer (Full)         16.5           112              76.8
                                       - w/o Attention FPI          17.1           111              79.2
                                       - w/o FFN FPI                16.9           111              80.5
                                       - w/o Introspection          17.3           110              89.4
                                                                    28
