                 paradigm, our model develops the capability to per-    computational budget to enable multiple reason-
                 form comprehensive and transparent analyses for        ing attempts, coupled with majority voting mecha-
                 evaluating complex solution steps of challenging       nismsforanswerselection, can achieve remarkable
                 questions. Second, we explore the self-evolution of    accuracy improvements.
                 our model through preference optimization, which
                 encourages the model to generate reasoning trajec-     2.2  RewardModelingofReasoning
                 tories that yield correct evaluations. This approach   Rewardmodelsareintroduced to further improve
                 enables our model to improve its capabilities with-    mathematical reasoning by enhancing training data
                 out requiring additional annotated data. Finally,      quality, guiding model learning (Lightman et al.,
                 wefurther exploit the reasoning capabilities of our    2023; Cobbe et al., 2021; Uesato et al., 2022),
                 modelat inference time, allowing multiple evalua-      and guiding the policy modelâ€™s reasoning pro-
                 tion trajectories to be sampled for a more compre-     cess through Best-of-N and Guided-Search meth-
                 hensive and robust assessment without training.        ods (Wang et al., 2024b; Zhang et al., 2025b).
                   When evaluated on ProcessBench and PRM-                Currently, reward models are typically catego-
                 Bench, our R-PRM achieves F1 score improve-            rized into Outcome Reward Models (ORMs) and
                 ments of 13.9 and 8.5 points, respectively, over the   Process Reward Models (PRMs) (Lightman et al.,
                 strongestbaselinetrainedonthesamedata. Further-        2023). ORMsfocus on providing an overall evalu-
                 more, when used to guide policy model reasoning        ation based on whether the correct answer is ulti-
                 via Best-of-N and Guided Search strategies, our        mately obtained (Cobbe et al., 2021). In contrast,
                 approach improves accuracy by average margins of       PRMsprovide a fine-grained evaluation for each
                 8.6 and 8.4 points over the Pass@1 baseline across     reasoning step, and many works have shown that
                 six challenging math datasets, outperforming both      they can achieve better results (Lightman et al.,
                 majority voting and all existing PRM baselines.        2023; Uesato et al., 2022). Data for PRM is ex-
                 Further analysis reveals our three key additional ad-  tremely scarce, and its annotation is costly (Light-
                 vantages: (1) comprehensive evaluation coverage        man et al., 2023; Wang et al., 2024b; Luo et al.,
                 through multi-dimensional analysis, (2) enhanced       2024b; Zhang et al., 2025a). Some studies ex-
                 generalization capability across diverse datasets,     plore automatic synthesis strategies, such as us-
                 and (3) progressive accuracy improvement with in-      ing Monte Carlo (MC) estimation methods (Wang
                 creased reasoning budgets, suggesting significant      et al., 2024b; Luo et al., 2024b). However, MC
                 potential for reasoning-system optimization.           methods incur a large computational cost and in-
                 2 RelatedWork                                          evitably introduce bias and noise (Zheng et al.,
                                                                        2024). (Zhang et al., 2025b) propose combining
                 2.1   Mathematical Reasoning                           MCwithLLMasajudge,helpingtoreducenoise.
                 Recent studies have demonstrated that LLMs ex-        The quality and quantity of step-level reasoning
                 hibit enhanced reasoning capabilities when gener-      evaluation data are still limited, and this remains
                 ating step-by-step solutions before providing the      an unsolved challenge.
                 final answers (Wei et al., 2023). Building on this     3 Method
                 insight, several pioneering works have focused on
                 developing large-scale mathematical datasets with      In this section, we propose a novel reasoning-
                 high-quality reasoning annotations for fine-tuning     driven process-level reward modeling framework.
                 of LLMs (Luo et al., 2025; Wang et al., 2023;          Its core objective is to fully leverage the inher-
                 Shao et al., 2024; Yang et al., 2024). However,        ent reasoning capabilities of LLMs to evaluate
                 even when models arrive at correct final answers,      the given reasoning steps, achieved through three
                 their intermediate reasoning steps may contain crit-   stages: cold start with limited labeled data, self-
                 ical errors. This discrepancy undermines the re-       evolution via preference optimization, and infer-
                 liability of their problem-solving processes and       ence time scaling.
                 poses significant obstacles for future model im-       3.1  Reasoning for Process Reward Modeling
                 provements (Zheng et al., 2024).
                   Parallel advancements (Snell et al., 2024; O1,       GivenamathematicalproblemQ,thepolicymodel
                 2023; DeepSeek-AI, 2025; QwQ, 2023) in infer-          generates a sequential chain-of-reasoning process
                 ence time have demonstrated that increasing the        S ={s ,s ,...,s }, where each reasoning step s
                                                                               1  2      n                               i
                                                                   13451
