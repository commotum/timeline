                        improvements in in-distribution multiplication performance, and making headway on the challenging
                        problem of variable length array sorting.
                        Contrasting with prior work, our experiments explore types of extrapolation well beyond just length
                        generalization for addition, presenting an architecture modification that improves performance on
                        multiple algorithmic reasoning tasks simultaneously. We hope that our work deepens the community’s
                        understanding of these problems and paves the way for further advancements in the algorithmic
                        reasoning capabilities of large language models.
                        Limitations  There are some intrinsic limitations that accompany any study involving language
                        model training from scratch under compute constraints. However, the primary point of relevance for
                        this study is that although we show the compatibility of Abacus Embeddings with FIRE and RoPE
                        embeddings, we do not actually explore any natural language tasks. In the future, a larger scale study
                        including natural language would be needed to understand further how Abacus Embeddings would
                        perform on heterogeneous tasks comprising both numerical and natural language inputs.
                        AcknowledgmentsandDisclosureofFunding
                        This work was made possible by the ONR MURI program and the AFOSR MURI program. Com-
                        mercial support was provided by Capital One Bank, the Amazon Research Award program, and Open
                        Philanthropy. Further support was provided by the National Science Foundation (IIS-2212182), and
                        by the NSF TRAILS Institute (2229885). Computing resources were furnished by the Department of
                        Energy INCITE Allocation Program, and Lawrence Livermore National Labs.
                        Furthermore, this work was performed under the auspices of the U.S. Department of Energy by
                        Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344 and was supported
                        bythe LLNL-LDRDProgramunderProjectNo. 24-ERD-010(LLNL-CONF-2000175).
                        References
                        CemAnil,AshwiniPokle,KaiquLiang,Johannes Treutlein, Yuhuai Wu, Shaojie Bai, J Zico Kolter,
                          andRogerBGrosse. Pathindependentequilibriummodelscanbetterexploittest-timecomputation.
                          Advances in Neural Information Processing Systems, 35:7796–7809, 2022a.
                        Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh,
                          AmbroseSlone,GuyGur-Ari,EthanDyer,andBehnamNeyshabur.Exploringlengthgeneralization
                          in large language models. Advances in Neural Information Processing Systems, 35:38546–38556,
                          2022b.
                        Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
                          arXiv:1607.06450, 2016.
                        Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and
                          TomGoldstein. End-to-end algorithm synthesis with recurrent networks: Logical extrapolation
                          without overthinking. Advances in Neural Information Processing Systems, 35, 2022.
                        TomBrown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
                          Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
                          few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
                        Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and Alexander Rudnicky. Kerple: Kernelized relative
                          positional embedding for length extrapolation. In Advances in Neural Information Processing
                          Systems, 2022.
                        Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer
                          length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual
                          Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13522–
                          13537, 2023.
                        Artur Back de Luca and Kimon Fountoulakis. Simulation of graph algorithms with looped transform-
                          ers. arXiv preprint arXiv:2402.01107, 2024.
                                                                  10
