

===== PAGE 1 =====

Available online at www.sciencedirect.com

ScienceDirect

NEUROCOMPUTING

ELSEVIER

Neurocomputing 71 (2008) 1180-1190
www.elsevier.com/locate/neucom

Natural Actor-Critic

Jan Peters*”*, Stefan Schaal?*

* Max-Planck-Institute for Biological Cybernetics, Tuebingen, Germany
> University of Southern California, Los Angeles, CA 90089, USA
°ATR Computational Neuroscience Laboratories, Kyoto 619-0288, Japan

Available online 1 February 2008

Abstract

In this paper, we suggest a novel reinforcement learning architecture, the Natural Actor-Critic. The actor updates are achieved using
stochastic policy gradients employing Amari’s natural gradient approach, while the critic obtains both the natural policy gradient and
additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy
gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be
estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by
the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the
original Actor-Critic and Bradtke’s Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations
illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning

control on an anthropomorphic robot arm.
© 2008 Elsevier B.V. All rights reserved.

Keywords: Policy-gradient methods; Compatible function approximation; Natural gradients; Actor-Critic methods; Reinforcement learning; Robot

learning

1. Introduction

Reinforcement learning algorithms based on value func-
tion approximation have been highly successful with discrete
lookup table parameterization. However, when applied with
continuous function approximation, many of these algo-
rithms failed to generalize, and few convergence guarantees
could be obtained [24]. The reason for this problem can
largely be traced back to the greedy or é-greedy policy
updates of most techniques, as it does not ensure a policy
Improvement when applied with an approximate value
function [8]. During a greedy update, small errors in the
value function can cause large changes in the policy which in
return can cause large changes in the value function. This
process, when applied repeatedly, can result in oscillations or
divergence of the algorithms. Even in simple toy systems,

*Corresponding author at: Max-Planck-Institute for Biological Cybernetics,
Department of Empirical Inference, Spemannstr. 38, 72076 Tuebingen,
Germany.

E-mail address: jan.peters@tuebingen.mpg.de (J. Peters).

0925-2312/$-see front matter © 2008 Elsevier B.V. All rights reserved.
doi:10.1016/j.neucom.2007.11.026

such unfortunate behavior can be found in many well-known
greedy reinforcement learning algorithms [6,8].

As an alternative to greedy reinforcement learning, policy-
gradient methods have been suggested. Policy gradients have
rather strong convergence guarantees, even when used in
conjunction with approximate value functions, and recent
results created a theoretically solid framework for policy-
gradient estimation from sampled data [25,15]. However,
even when applied to simple examples with rather few states,
policy-gradient methods often turn out to be quite inefficient
[14], partially caused by the large plateaus in the expected
return landscape where the gradients are small and often do
not point directly towards the optimal solution. A simple
example that demonstrates this behavior is given in Fig. 1.

Similar as in supervised learning, the steepest ascent with
respect to the Fisher information metric [3], called the
‘natural’ policy gradient, turns out to be significantly more
efficient than normal gradients. Such an approach was first
suggested for reinforcement learning as the ‘average
natural policy gradient’ in [14], and subsequently shown
in preliminary work to be the true natural policy gradient
[21,4]. In this paper, we take this line of reasoning one step


===== PAGE 2 =====

J. Peters, S. Schaal {| Neurocomputing 71 (2008) 1180-1190 1181

a ‘Vanilla’ policy gradients b Natural policy gradients

05 PAT 4 OO Ye
i, 0.4 YE =N\ I 0.4 EN
s 04 \ La NY
8 03 § 03 BN
= 02 1 £ 02 LN
gf 8 Pw
fi 0.1 & 0.1

0.0 0.0 2

—2 -15 -10 0.5 0.0 —2 -15 -10 -0.5 0.0

Controller gain 6,=k Controller gain 6,=k

Fig. 1. When plotting the expected return landscape for simple problem as
ld linear-quadratic regulation, the differences between (a) ‘vanilla’ and
(b) natural policy gradients becomes apparent [21].

further in Section 2.2 by introducing the “Natural Actor-Critic
(NAC) which inherits the convergence guarantees from
gradient methods. Furthermore, in Section 3, we show
that several successful previous reinforcement learning
methods can be seen as special cases of this more general
architecture. The paper concludes with empirical evalua-
tions that demonstrate the effectiveness of the suggested
methods in Section 4.

2. Natural Actor-Critic
2.1. Markov decision process notation and assumptions

For this paper, we assume that the underlying control
problem is a Markov decision process (MDP) in discrete
time with continuous state set X = R”, and a continuous
action set U = R” [8]. The assumption of an MDP comes
with the limitation that very good state information and
Markovian environment are assumed. However, similar as
in [1], the results presented in this paper might extend to
problems with partial state information.

The system is at an initial state xo € X at time r=0
drawn from the start-state distribution p(xo). At any state
x, € X at time ¢, the actor will choose an action uw, € U
by drawing it from a stochastic, parameterized policy
n(u;|x;) = p(u;|x;,0) with parameters @¢R%, and the
system transfers to a new state x,,; drawn from the state
transfer distribution p(x,41|x;,4,). The system yields a
scalar reward r,=r(x,,u,)€R after each action. We
assume that the policy zg is continuously differentiable
with respect to its parameters 0, and for each considered
policy 79, a state-value function V’*(x), and the state-action
value function Q"(x,u) exist and are given by

lo. @)
t=0
lo. @)
Q"(x,u) = {3 y'r;|Xo = X, Mo = i
1=0

where y € (0,1) denotes the discount factor, and t a
trajectory. It is assumed that some basis functions (x)
are given so that the state-value function can be approxi-

mated with linear function approximation V7(x) = @(x)'v.
The general goal is to optimize the normalized expected

return
°

J(0) = e.fo —y>o yn
t=0

= [ew / n(u|x)r(x, u) dx du,
x U

where

oo
d(x) =(1—y) 5° y'p(x: = ¥)
1=0
is the discounted state distribution.

2.2. Actor improvement with natural policy gradients

Actor-Critic and many other policy iteration architec-
tures consist of two steps, a policy evaluation step and a
policy improvement step. The main requirements for the
policy evaluation step are that it makes efficient usage of
experienced data. The policy improvement step is required
to improve the policy on every step until convergence while
being efficient.

The requirements on the policy improvement step rule
out greedy methods as, at the current state of knowledge, a
policy improvement for approximated value functions
cannot be guaranteed, even on average. ‘Vanilla’ policy
gradient improvements (see e.g. [25,15]) which follow the
gradient VgJ(@) of the expected return function J(@) (where
Vof = [Of /061,...,0f/00n]) denotes the derivative of
function f with respect to parameter vector (0) often get
stuck in plateaus as demonstrated in [14]. Natural gradients
VoJ(8) avoid this pitfall as demonstrated for supervised
learning problems [3], and suggested for reinforcement
learning in [14]. These methods do not follow the steepest
direction in parameter space but the steepest direction with
respect to the Fisher metric given by

VoJ(0) = G'(0)VoJ(0), (1)

where G(@) denotes the Fisher information matrix. It is
guaranteed that the angle between natural and ordinary
gradient is never larger than 90°, i.e., convergence to the
next local optimum can be assured. The ‘vanilla’ gradient is
given by the policy-gradient theorem (see e.g. [25,15])

VoJ(0) = [ew / Von(u|x)( QO" (x, u) — b"(x)) du dx,
(2)

where 6"(x) denotes a baseline. Refs. [25,15] demonstrated
that in Eq. (2), the term Q”(x, u) — b*(x) can be replaced by
a compatible function approximation

Fix, w) = (Vo log x(ulx))" w = O(x, u) — B(x), (3)

parameterized by the vector w, without affecting the
unbiasedness of the gradient estimate and irrespective of
the choice of the baseline b”(x). However, as mentioned in


===== PAGE 3 =====

1182 J. Peters, S. Schaal {| Neurocomputing 71 (2008) 1180-1190

[25], the baseline may still be useful in order to reduce the
variance of the gradient estimate when Eq. (2) is approxi-
mated from samples. Based on Eqs. (2) and (3), we derive
an estimate of the policy gradient as

VoJ(0) = [ d"(x) I 7(u|x)Vo log x(u|x)Vo log n(ul|x)' dudxw
=> Fow (4)

as Von(ulx) = 2(ul|x)V_q log x(u|x). Since z(u|x) 1s chosen by
the user, even in sampled data, the integral

F(0,x) = / n(u|x)Vo log n(ulx)Vo log x(ulx)! du (5)

can be evaluated analytically or empirically without
actually executing all actions. It is also noteworthy that
the baseline does not appear in Eq. (4) as it integrates out,
thus eliminating the need to find an optimal selection of
this open parameter. Nevertheless, the estimation of Fg =
Jy U(x) F(0, x) dx is still expensive since d"(x) is not
known. However, Eq. (4) has more surprising implications
for policy gradients, when examining the meaning of the
matrix F’g in Eq. (4). Kakade [14] argued that F(0, x) is the
point Fisher information matrix for state x, and that
F(0) = f,.d"(x) F(0,x)dx, therefore, denotes a weighted
‘average Fisher information matrix’ [14]. However, going
one step further, we demonstrate in Appendix A that Fy is
indeed the true Fisher information matrix and does not
have to be interpreted as the ‘average’ of the point Fisher
information matrices. Eqs. (4) and (1) combined imply that
the natural gradient can be computed as

VoJ(0) = G'(0)Fow = w, (6)

since Fg = G(@) (cf. Appendix A). Therefore we only need
estimate w and not G(@). The resulting policy improvement
step is thus 0;,; = 6; + aw where « denotes a learning rate.
Several properties of the natural policy gradient are
worthwhile highlighting:

e@ Convergence to a local minimum guaranteed as for
‘vanilla gradients’ [3].

@ By choosing a more direct path to the optimal solution
In parameter space, the natural gradient has, from
empirical observations, faster convergence and avoids
premature convergence of ‘vanilla gradients’ (cf. Fig. 1).

@ The natural policy gradient can be shown to be covariant,
Le., Independent of the coordinate frame chosen for
expressing the policy parameters (cf. Section 3.1).

@ As the natural gradient analytically averages out the
influence of the stochastic policy (including the baseline of
the function approximator), it requires fewer data point
for a good gradient estimate than ‘vanilla gradients’.

2.3. Critic estimation with compatible policy evaluation

The critic evaluates the current policy z in order to
provide the basis for an actor improvement, i.e., the change

Aé of the policy parameters. As we are interested in natural
policy gradient updates A@ = aw, we wish to employ the
compatible function approximation f7,(x, 4) from Eq. (3) in
this context. At this point, a most important observation is
that the compatible function approximation f7,(x,) is
mean-zero w.r.t. the action distribution, 1.e.,

/ n(ulx) fy (x, w) du = wt | Von(u|x) du = 0, (7)
U U

since from tu n(u|x)du = 1, differention w.r.t. to @ results
in fi) Vor(ulx) du = 0. Thus, f{(x, a) represents an advan-
tage function A”(x,#) = Q"(x,u)— V"(x) in general. The
essential differences between the advantage function and
the state-action value function is demonstrated in Fig. 2.
The advantage function cannot be learned with TD-like
bootstrapping without knowledge of the value function as
the essence of TD is to compare the value V(x) of the two
adjacent states—but this value has been subtracted out in
A”(x,u). Hence, a TD-like bootstrapping using exclusively
the compatible function approximator is impossible.

As an alternative, [25,15] suggested to approximate
f,,(x,u) from unbiased estimates O'(x, u) of the action
value function, e.g., obtained from rollouts and using least-
squares minimization between f,, and Q . While possible in
theory, one needs to realize that this approach implies a
function approximation problem where the parameteri-
zation of the function approximator only spans a much
smaller subspace of the training data—e.g., imagine
approximating a quadratic function with a line. In practice,
the results of such an approximation depends crucially on
the training data distribution and has thus unacceptably
high variance—e.g., fit a line to only data from the right
branch of a parabula, the left branch, or data from both
branches.

Furthermore, in continuous state-spaces a state (except
for single start-states) will hardly occur twice; therefore, we
can only obtain unbiased estimates O'(x, u) of O7(x, x).
This means the state-action value estimates O'(x, u) have to

a b

Value Function Q* (z,u) Advantage Function A"(x,u)

Action u
Action u

5 -5
-5 5 -5 5
State x State z

Fig. 2. The state-action value function in any stable linear-quadratic
Gaussian regulation problems can be shown to be a bowl (a). The
advantage function is always a saddle as shown in (b); it is straightforward
to show that the compatible function approximation can exactly represent
the advantage function—but projecting the value function onto the
advantage function is non-trivial for continuous problems. This figure
shows the value function and advantage function of the system described
in the caption of Fig. 1.


===== PAGE 4 =====

J. Peters, S. Schaal {| Neurocomputing 71 (2008) 1180-1190 1183

be projected onto the advantage function A”(x, a). This
projection would have to average out the state-value offset
V"(x). For example, for linear-quadratic regulation, it is
straightforward to show that the advantage function is
saddle while the state-action value function is bowl—we
therefore would be projecting a bowl onto a saddle; both
are illustrated in Fig. 2. In this case, the distribution of the
data has a drastic impact on the projection.

To remedy this situation, we observe that we can write
the Bellman equations (e.g., see [5]) in terms of the
advantage function and the state-value function

QO" (x, u) = A*(x,u) + V"(x)
=r(x,u)+y) il p(x’ |x, uw) V"(x') dx’. (8)
x

Inserting A" (x, u) = f (x, #) and an appropriate basis
functions representation of the value function as V"(x) =
¢(x)'v, we can rewrite the Bellman Equation, Eq. (8), as a
set of linear equations

Va log m(u;|x;)' w + P(x,)'v
= (x1, Mr) + P(r) B+ (Xp, Me X41), (9)

where é(x;, #;, X;41) denotes an error term which mean-zero
as can be observed from Eq. (8). These equations enable us
to formulate some novel algorithms in the next sections.

The linear appearance of w and » hints at a least squares
to obtain. Thus, we now need to address algorithms that
estimate the gradient efficiently using the sampled equa-
tions (such as Eq. (9)), and how to determine the additional
basis functions @(x) for which convergence of these
algorithms is guaranteed.

2.3.1. Critic evaluation with LSTD-Q(2)

Using Eq. (9), a solution to Eq. (8) can be obtained by
adapting the LSTD() policy evaluation algorithm [9]. For
this purpose, we define
, = [p(x,)'. Vo log n(u,|X;)' |",

, = [6(a1)',0')'" (10)
as new basis functions, where 0 is the zero vector. This
definition of basis function reduces bias and variance of the
learning process in comparison to SARSA and previous
LSTD(A) algorithms for state-action value functions [9] as
the basis functions @, do not depend on stochastic future
actions ,.;, 1e., the input variables to the LSTD re-
gression are not noisy due to m4; (e.g., as in [10]}—such
input noise would violate the standard regression model
that only takes noise in the regression targets into account.
Alternatively, Bradtke et al. [10] assume V"(x) = Q(x, i)
where #@ is the average future action, and choose their basis
functions accordingly; however, this is only given for
deterministic policies, i.e., policies without exploration and
not applicable in our framework. LSTD(A) with the basis
functions in Eq. (10), called LSTD-Q(/) from now on, is
thus currently the theoretically cleanest way of applying
LSTD to state-value function estimation. It is exact for

Table 1
Natural Actor-Critic Algorithm with LSTD-Q(A)

Input: Parameterized policy z(u|x) = p(u|x, 6) with initial parameters
0 = Op, its derivative Vg log z(u|x) and basis functions @(x) for the value
function V(x)

1: Draw initial state xo~p(xo), and select parameters
Ani = 0b) = 41 = 9.
2: For f= 0,1,2,... do
3: Execute: Draw action u,~7(u;|x;), observe next state
Xi ~p(*1 |X, 4), and reward r; = r(x;, u;).
4: Critic Evaluation (LSTD-Q(/)): Update
4.1: basis functions: d, = [b(xu1)'.0'T,
>, = [b(x)", Vologn(udx)"1",
4.2: statistics: 24) = Az, + 6, Ani = A, + 241(b; -— 7)":
Buy) = Be + els

4,3: critic parameters: [v7,,,w1,,]' = 4G) bu1-

5: Actor: If gradient estimate is accurate, 4 (w;, w;_1)<é, update
5.1: policy parameters: 0.4; = 0; + «wi41,

5.2: forget statistics: 741 <— P2411, 461 <— BA, 841 <— Bia.
6: end.

deterministic or weekly noisy state transitions and arbi-
trary stochastic policies. As all previous LSTD suggestions,
it loses accuracy with increasing noise in the state
transitions since @, becomes a random variable. The
complete LSTD-Q(4) algorithm is given in the Critic
Evaluation (lines 4.1—4.3) of Table 1.

Once LSTD-Q(4) converges to an approximation of
A"(x,,u;) + V"(x,), we obtain two results: the value function
parameters v, and the natural gradient w. The natural
gradient w serves in updating the policy parameters
Ad, = aw,. After this update, the critic has to forget at least
parts of its accumulated sufficient statistics using a forgetting
factor f € [0,1] (cf. Table 1). For fB =0, Le., complete
resetting, and appropriate basis functions (x), convergence
to the true natural gradient can be guaranteed. The complete
NAC algorithm is shown in Table 1.

However, it becomes fairly obvious that the basis
functions can have an influence on our gradient estimate.
When using the counterexample in [7] with a typical Gibbs
policy, we will realize that the gradient is affected for A<1;
for 4 = 0 the gradient is flipped and would always worsen
the policy. However, unlike in [7], we at least could
guarantee that we are not affected for 7 = 1.

2.3.2. Episodic NAC

Given the problem that the additional basis functions
(x) determine the quality of the gradient, we need
methods which guarantee the unbiasedness of the natural
gradient estimate. Such method can be determined by
summing up Eq. (9) along a sample path, we obtain

N-1
S7 yA™(x1, ur)
t=0

N-1

= V"(x0)+ 5) y'r(xrus) —y" Vey). (lL)
t=0


===== PAGE 5 =====

1184 J. Peters, S. Schaal {| Neurocomputing 71 (2008) 1180-1190

Table 2
Episodic Natural Actor-Critic Algorithm (eNAC)

Input: Parameterized policy 2(u|x) = p(u|x, 6) with initial parameters
0 = Oo, and derivative Vg log z(u|x).

For u = 1,2,3,... do
For e = 1,2,3,... do
Execute Rollout: Draw initial state x9 ~p(xo).
For ¢ = 1,2,3,...,N do
Draw action #,~7(u;|x;), observe next state x441~p(%1411%;, &),
and reward r; = r(x;, u;).
end.
end.
Critic Evaluation (Episodic): Determine value function
J = V"(xo), compatible function approximation f/,(%:, 4:).
Update: Determine basis functions: @, = oxy y'Vo log n(u,|x,)", 1]':
reward statistics: R; = yw oY
Actor-Update: When the natural gradient is converged,
A (wi, W-1) Se, update the policy parameters: 6,4; = 0; + owr41.
6: end.

It is fairly obvious that the last term disappears for N > oo
or episodic tasks (where r(xy_1,#y_1) 1s the final reward);
therefore each rollout would yield one equation. If we
furthermore assume a single start-state, an additional
scalar value function of (x)= 1 suffices. We therefore
get a straightforward regression problem:

N-1 N-1
S> y'Vilog n(u,,x,)'w+J = S~ y'r(x;.u;) (12)
1=0 1=0

with exactly dim @+ 1 unknowns. This means that for non-
stochastic tasks we can obtain a gradient after dimé@+ 1
rollouts. The complete algorithm is shown in Table 2.

3. Properties of NAC

In this section, we will emphasize certain properties of
the NAC. In particular, we want to give a simple proof of
covariance of the natural policy gradient, and discuss [14]
observation that in his experimental settings the natural
policy gradient was non-covariant. Furthermore, we will
discuss another surprising aspect about the NAC which is
its relation to previous algorithms. We briefly demonstrate
that established algorithms like the classic Actor-Critic
[24], and Bradtke’s Q-Learning [10] can be seen as special
cases of NAC.

3.1. On the covariance of natural policy gradients

When [14] originally suggested natural policy gradients,
he came to the disappointing conclusion that they were not
covariant. As counterexample, he suggested that for two
different linear Gaussian policies (one in the normal form,
and the other in the information form) the probability
distributions represented by the natural policy gradient
would be affected differently, ie., the natural policy
gradient would be non-covariant. We intend to give a

proof at this point showing that the natural policy gradient
is in fact covariant under certain conditions, and clarify
why [14] experienced these difficulties.

Theorem 1. Natural policy-gradients updates are covariant
for two policies xg parameterized by 0 and 1, parameterized
by h if (i) for all parameters 6; there exists a function
6;=f(,...,4x), (ii) the derivative V,0 and its inverse
v.01.

For the proof see Appendix B. Practical experiments
show that the problems occurred for Gaussian policies in
[14] are in fact due to the selection the stepsize « which
determines the length of A@. As the linearization A@ =
V,0'Ah does not hold for large A@, this can cause
divergence between the algorithms even for analytically
determined natural policy gradients which can partially
explain the difficulties occurred by Kakade [14].

3.2. NAC’s relation to previous algorithms

Original Actor-Critic. Surprisingly, the original Actor-
Critic algorithm [24] is a form of the NAC. By choosing
a Gibbs policy m(u;\x:) = exp(@x)/>_, exp(Gxs), with
all parameters 6,,, lumped in the vector @ (denoted as
6 = [6,,]) in a discrete setup with tabular representations
of transition probabilities and rewards. A linear function
approximation V7(x) = @(x)'» with v = [v,] and unit basis
functions @(x) =u, was employed. Sutton et al. online
update rule is given by

Oo = Oi, + 1 (H(X, W) + Yow — Vy),

t+1
x

vy) = vl. + 02(1(x, uw) + pox — Vy),

where «1, % denote learning rates. The update of the critic
parameters vi, equals the one of the NAC in expectation as
TD(0) critics converges to the same values as LSTD(O) and
LSTD-Q(0) for discrete problems [9]. Since for the Gibbs
policy we have 0log z(b\a)/00,,, = 1 — x(b|a) if a= x and
b=u, Olog x(b|a)/06., = —n(bla) if a= x and bAu, and
Olog x(b|a)/0@,.,, = 0 otherwise, and as >> ,2(5|x)A(x, 6) =
0, we can evaluate the advantage function and derive

A(x,u) = A(x,u) — S~ n(b|x)A(x, 5)
5

_ 0 log x(b|x)
=) -6,, A(x, b).

Since the compatible function approximation represents
the advantage function, Le., f(x, u) = A(x, u), we realize
that the advantages equal the natural gradient, Le.,
w= [A(x,u)]. Furthermore, the TD(0) error of a state-
action pair (x,u) equals the advantage function in
expectation, and therefore the natural gradient update
Wy, = A(x, U) = Ey {r(x,u) + pV’) — V(x)| x, u}, corre-
sponds to the average online updates of Actor-Critic. As
both update rules of the Actor-Critic correspond to the
ones of NAC, we can see both algorithms as equivalent.


===== PAGE 6 =====

J. Peters, S. Schaal {| Neurocomputing 71 (2008) 1180-1190 1185

SARSA. SARSA with a tabular, discrete state-action
value function Q"(x,u) and an ¢-soft policy improvement

n(u;|x;) = exp(Q"(x, u)/e)/ » exp(Q"(x,u)/)

can also be seen as an approximation of NAC. When treating
the table entries as parameters of a policy 0,,, = Q"(x,™), we
realize that the TD update of these parameters corresponds
approximately to the natural gradient update since wy, =
bA(x,u) © cE {r(x u) + yO’, u’) — O(x, w)|x, u}. However,
the SARSA-TD error equals the advantage function only for
policies where a single action u* has much better action values
O(x, u*) than all other actions; for such special cases, s-soft
SARSA can be seen as an approximation of NAC. This also
corresponds to Kakade’s [14] observation that greedy update
step (such as the «-soft greedy update), approximates the
natural policy gradient.

Bradtke’s Q-Learning. Bradtke [10] proposed an algo-
rithm with policy x(u;|x,) = V (u;|; Xt. og?) and parameters
0; = [k} oi]! (where go; denotes the exploration, and i the
policy update time step) in a linear control task with linear
state transitions x,,; = Ax,+ bu,, and quadratic rewards
r(X;,U;) = x) Hx,+ Ru?. They evaluated Q*(x,,u,) with
LSTD(0) using a quadratic polynomial expansion as basis
functions, and applied greedy updates:

jBradtke

T
i = arg max O"(x;,,u; = k;, )X;)

Rist
= —(R+ yb'P;b) 'ybP;A,

where P; denotes policy-specific value function parameters
related to the gain k;; no update the exploration a; was
included. Similarly, we can obtain the natural policy
gradient w =[w;,wo]', as yielded by LSTD-Q(A) analyti-
cally using the compatible function approximation and the
same quadratic basis functions. As discussed in detail in
[21], this gives us

w, = (yA'Pib + (R+ yb! P,b)k)' 07,

We = 0.5(R + yb" P;b)o?.

Similarly, it can be derived that the expected return is
J(0;) = —(R+ yb" P;b)c? for this type of problems, see [21].
For a learning rate x; = 1/||J(0;)||, we see
Kiny = kj + awe = kj — (ki + (R + yb" Pb) yA™ Pb)
_ fBradtke
i+l ?

which demonstrates that Bradtke’s Actor Update is a
special case of the NAC. NAC extends Bradtke’s result as it
gives an update rule for the exploration—which was not
possible in Bradtke’s greedy framework.

4. Evaluations and applications

In this section, we present several evaluations comparing
the episodic NAC architectures with previous algorithms.

We compare them in optimization tasks such as Cart-Pole
Balancing and simple motor primitive evaluations and
compare them only with episodic NAC. Furthermore, we
apply the combination of episodic NAC and the motor
primitive framework to a robotic task on a real robot, Le.,
‘hitting a T-ball with a baseball bat’.

4.1. Cart-Pole Balancing

Cart-Pole Balancing is a well-known benchmark for
reinforcement learning. We assume the cart as shown in
Fig. 3(a) can be described by

mlx cos 0 + mi°6 — mgl sind = 0,

m+ me)x + mlbcos 6 — mii’ sin 0 = F,
(

with /=0.75m, m=0.15kg, g=9.81m/s* and m=
1.0kg. The resulting state is given by x = [x,x,6,6]', and
the action uw = F. The system is treated as if it was sampled
at a rate of h = 60 Hz, and the reward is given by r(x, uw) =
x'Qx + u' Ru with Q = diag(1.25, 1, 12,0.25), R = 0.01.

The policy is specified as x(u|x) = (Kx, 7). In order to
ensure that the learning algorithm cannot exceed an
acceptable parameter range, the variance of the policy is
defined as ¢=0.1+1/(1+exp(y)). Thus, the policy
parameter vector becomes @=[K',y]' and has the
analytically computable optimal solution K ~ [5.71, 11.3,
—82.1,—21.6]', and o = 0.1, corresponding to 7 > oo. As
4 —> oo Is hard to visualize, we show a in Fig. 3(b) despite
the fact that the update takes place over the parameter 7.

For each initial policy, samples (x, #;,7141,X;41) are
being generated using the start-state distributions, transi-
tion probabilities, the rewards and the policy. The samples
arrive at a sampling rate of 60 Hz, and are immediately sent
to the NAC module. The policy is updated when
A (W141, Ws) <e = 2/180. At the time of update, the true
‘vanilla’ policy gradient, which can be computed analyti-
cally, is used to update a separate policy. The true ‘vanilla’
policy gradients these serve as a baseline for the
comparison. If the pole leaves the acceptable region
of —x/6<¢<a/6, and —1.5m<x<+1.5m, it is reset
to a new starting position drawn from the start-state
distribution.

Results are illustrated in Fig. 3. In Fig. 3(b), a sample
run is shown: the NAC algorithms estimates the optimal
solution within less than 10min of stmulated robot trial
time. The analytically obtained policy gradient for
comparison takes over 2h of robot experience to get to
the true solution. In a real world application, a significant
amount of time would be added for the vanilla policy
gradient as it is more unstable and leaves the admissible
area more often. The policy gradient is clearly outperformed

'The true natural policy gradient can also be computed analytically.
However, it is not shown as the difference in performance to the Natural
Actor-Critic gradient estimate is negligible.


===== PAGE 7 =====

1186 J. Peters, S. Schaal {| Neurocomputing 71 (2008) 1180-1190
a b
Cart-pole problem A sample learning run
. 200
ip ~ 400 Pe
= oc
xo
——. «200
100 §.
0 &
= -200 |
-400
, 20 F
150
08
s 04
0 50 100 150
Time in minutes
Cc Expected return J (0)
0.04
0.05 #
_~ 0.06
2
" 0.07
0.08 +
0.09 & 1 ! 1 1 1 ! 1 !
0 20 40 60 80 100 120 140 160

Time in minutes

Fig. 3. This figure shows the performance of Natural Actor-Critic in the Cart-Pole Balancing framework. In (a), you can see the general setup of the pole
mounted on the cart. In (b), a sample learning run of the both Natural Actor-Critic and the true policy gradient is given. The dashed line denotes the
Natural Actor-Critic performance while the solid line shows the policy gradients performance. In (c), the expected return of the policy is shown. This is an

average over 100 randomly picked policies as described in Section 4.1.

by the NAC algorithm. The performance difference
between the true natural gradient and the NAC algorithm
is negligible and, therefore, not shown separately. By the
time of the conference, we hope to have this example
implemented on a real anthropomorphic robot. In
Fig. 3(c), the expected return over updates is shown
averaged over all hundred initial policies.

In this experiment, we demonstrated that the NAC is
comparable with the ideal natural gradient, and outperforms
the ‘vanilla’ policy gradient significantly. Greedy policy
Improvement methods do not compare easily. Discretized
greedy methods cannot compete due to the fact that the
amount of data required would be significantly increased.
The only suitable greedy improvement method, to our
knowledge, is Bradtke’s Adaptive Policy Iteration [10].
However, this method is problematic in real-world applica-
tion due to the fact that the policy in Bradtke’s method is
deterministic: the estimation of the action-value function is
an ill-conditioned regression problem with redundant para-
meters and no explorative noise. Therefore, it can only work
in simulated environments with an absence of noise in the
state estimates and rewards.

4.2. Motor primitive learning for baseball

This section will turn towards optimizing nonlinear
dynamic motor primitives for robotics. In [13], a novel
form of representing movement plans (q,,.q,) for the
degrees of freedom (DOF) robot systems was suggested in
terms of the time evolution of the nonlinear dynamical
systems

ae = Wage Bk> Jest Ok), (13)

where (q7x;4a,) denote the desired position and velocity
of a joint, z, the internal state of the dynamic system, g,
the goal (or point attractor) state of each DOF, 1 the
movement duration shared by all DOFs, and 6; the open
parameters of the function 4. The original work in [13]
demonstrated how the parameters 6, can be learned
to match a template trajectory by means of supervised
learning—this scenario is, for instance, useful as the first
step of an imitation learning system. Here we will add the
ability of self-improvement of the movement primitives in


===== PAGE 8 =====

J. Peters, S. Schaal {| Neurocomputing 71 (2008) 1180-1190 1187

Eq. (13) by means of reinforcement learning, which is the
crucial second step in imitation learning. The system in
Eq. (13) is a point-to-point movement, 1.e., this task is
rather well suited for episodic NAC.

In Fig. 4, we show a comparison with GPOMDP for
simple, single DOF task with a reward of

N

i) 2

r(Xo:n Uo.N) = 5 14a + 2(Gack:n — Ik) >
i=0

where c) = 1, c2 = 1000, and g, is chose appropriately.
In Fig. 4(a), we show how the expected cost decreases for
both GPOMDP and the episodic NAC. The positions
of the motor primitives are shown in Fig. 4(b) and in
Fig. 4(c) the accelerations are given. In 4(b,c), the dashed
line shows the initial configurations, which is accomplished
by zero parameters for the motor primitives. The solid line

shows the analytically optimal solution, which is unac-
hievable for the motor primitives, but nicely approximated
by their best solution, presented by the dark dot-dashed
line. This best solution is reached by both learning
methods. However, for GPOMDP, this requires approxi-
mately 10° learning steps while the NAC takes less than 10°
to converge to the optimal solution.

We also evaluated the same setup in a challenging
robot task, 1e., the planning of these motor primitives
for a seven DOF robot task. The task of the robot is to
hit the ball properly so that it flies as far as possible.
Initially, it is taught in by supervised learning as can
be seen in Fig. 5(b); however, it fails to reproduce the
behavior as shown in Fig. 5(c); subsequently, we improve
the performance using the episodic NAC which yields
the performance shown in Fig. 5(a) and the behavior in
Fig. 5(d).

a b c
20
1 _-- 7s \
1000 - _ by
S GPOMDP 08 %
> 800 z / 5
a fant
jo) a /
(o) 0.6 c
2 600 § / 3
— 7) pee
3 400 o 0.4 / xo
QO
i / s
200 0.2 /
Episodic Natural Actor-Gritic /
0 Fr ee 0 -10
0 200 400 600 800 1000 0 1 0 0.5 1
Number of Episodes time [s] time [s]

Expected Cost Position of motor primitives Controls of motor primitives

Fig. 4. This figure illustrates the task accomplished in the toy example. In (a), we show how the expected cost decreases for both GPOMDP and the
episodic Natural Actor-Critic. The positions of the motor primitives are shown in (b) and in (c) the accelerations are given. In (b,c), the dashed line shows
the initial configurations, which is accomplished by zero parameters for the motor primitives. The solid line shows the analytically optimal solution, which
is unachievable for the motor primitives, but nicely approximated by their best solution, presented by the dark dot-dashed line. This best solution is
reached by both learning methods. However, for GPOMDP, this requires approximately 10° learning steps while the NAC takes less than 10? to converge
to the optimal solution.

ie)

x 108

Performance J (8)

0 100 200 300 400 i
Episodes

Fig. 5. The figure shows (a) the performance of a baseball swing task when using the motor primitives for learning. In (b), the learning system is initialized
by imitation learning, in (c) it is initially failing at reproducing the motor behavior, and (d) after several hundred episodes exhibiting a nicely learned
batting.


===== PAGE 9 =====

1188 J. Peters, S. Schaal {| Neurocomputing 71 (2008) 1180-1190

5. Conclusion

In this paper, we have summarized novel developments
in policy-gradient reinforcement learning, and based on
these, we have designed a novel reinforcement learning
architecture, the NAC algorithm. This algorithm comes in
(at least) two forms, 1e., the LSTD-Q(A) form which
depends on sufficiently rich basis functions, and the
Episodic form which only requires a constant as additional
basis function. We compare both algorithms and apply the
latter on several evaluative benchmarks as well as on a
baseball swing robot example.

Recently, our NAC architecture [19,21] has gained a lot of
traction in the reinforcement learning community. According
to Aberdeen, the NAC is the ‘Current method of choice’ [2].
Additional to our work presented at ESANN 2007 in [19]
and its earlier, preliminary versions (see e.g. [22,21,18,20]),
the algorithm has found a variety of applications in largely
unmodified form in the last year. The current range of
additional applications includes optimization of constrained
reaching movements of humanoid robots [12], traffic-light
system optimization [23], multi-agent system optimization
[11,28], conditional random fields [27] and gait optimization
in robot locomotion [26,17]. All these new developments
indicate that the NAC is about to become a standard
architecture in the area of reinforcement learning as it is
among the few approaches which have scaled towards
interesting applications.

Appendix A. Fisher information property

In Section 6, we explained that the all-action matrix Fg
equals in general the Fisher information matrix G(@). In
[16], we can find the well-known lemma that by differ-
entiating fp, p(x)dx = 1 twice with respect to the para-
meters 0, we can obtain

[rei togn(y de

=- [ _plx)¥p log p(x)Vo log p(x)" dx (A.1)

for any probability density function p(x). Furthermore, we
can rewrite the probability p(zo.,) of a rollout or trajectory
Ton = [xo, Ug, To, X1, 1, 11, -.-, Xn, Un, Vn, Xnult as

P(tom) = P(Xo) |] pees lar ws )e(u |r)
t=0

which implies that

V5 log p(to:n) = S_ Vo log x(u)|x,)-
1=0

Using Eq. (A.1), and the definition of the Fisher informa-
tion matrix [3], we can determine Fisher information

matrix for the average reward case by

G(8) = lim n7!E,{Vo log p(t)Vo log p(to-n)"}
= — lim a 'E,{Vjlog p(s},

n> oo

= — lim med Vitventais)|

(A.2)

t=0

= | d"(x) / n(u|x)Vo log x(u|x) dudx (A.3)
x U

[ew | n(u|x)Vo log x(u|x)
x U

Vo log x(ul|x)! dudx = Fy. (A.4)

This proves that the all-action matrix is indeed the Fisher
information matrix for the average reward case. For the
discounted case, with a discount factor y we realize that we
can rewrite the problem where the probability of rollout is
given by

P, (ton) = P(Ton) (: HI)

i=0

and derive that the all-action matrix equals the Fisher
information matrix by the same kind of reasoning as in
Eq. (A.4). Therefore, we can conclude that in general, 1.e.,
G(0) = Fo.

Appendix B. Proof of the covariance theorem

For small parameter changes Ad and AO, we have
AO = V,,0' Ah. If the natural policy gradient is a covariant
update rule, a change Ah along the gradient V,J(#) would
result in the same change A@ along the gradient V»J(@) for
the same scalar step-size «. By differentiation, we can
obtain V;,/(4) = V,0VeJ(0). It is straightforward to show
that the Fisher information matrix includes the Jacobian
V,,0 twice as factor

F(h) = [rw] n(u|x)V;, log x(u|x)V;, log x(u|x)' dudx,

=vi0 fais) [ mais log x(u|x)

V_ log x(u\x)! dudxV,0',
= V,0F(0)V,0'.

This shows that natural gradient in the A parameterization
Is given by

V,J(h) = F~'()V,J(h)
= (V,0F(0)V,0') 'V,0VoJ(0).


===== PAGE 10 =====

J. Peters, S. Schaal {| Neurocomputing 71 (2008) 1180-1190 1189

This has a surprising implication as it makes it straightfor-
ward to see that the natural policy is covariant since

AO = 0V,0' Ah = oV,0'V;,J(h),
aV,0'(V,0F(0)V,0') 'V,0Ve(0),
aF!(0)VoJ(0) = aVoJ(8),

assuming that V,@ is invertible. This concludes that the
natural policy gradient is in fact a covariant gradient
update rule.

The assumptions underlying this proof require that the
learning rate is very small in order to ensure a covariant
gradient descent process. However, single update steps will
always be covariant and, thus, this requirement is only
formally necessary but barely matters in practice. Similar
as in other gradient descent problems, learning rates can be
chosen to optimize the performance without changing the
fact that the covariance of a single update step direction
will not be affected.

References

[1] D. Aberdeen, Policy-gradient algorithms for partially observable
Markov decision processes, Ph.D. Thesis, Australian National
Unversity, 2003.

[2] D. Aberdeen, POMDPs and policy gradients, in: Proceedings of the
Machine Learning Summer School (MLSS), Canberra, Australia,
2006.

[3] S. Amari, Natural gradient works efficiently in learning, Neural
Comput. 10 (1998) 251-276.

[4] J. Bagnell, J. Schneider, Covariant policy search, in: International
Joint Conference on Artificial Intelligence, 2003.

[5] L.C. Baird, Advantage updating, Technical Report WL-TR-93-1146,
Wright Lab., 1993.

[6] L.C. Baird, A.W. Moore, Gradient descent for general reinforcement
learning, in: Advances in Neural Information Processing Systems,
vol. 11, 1999.

[7] P. Bartlett, An introduction to reinforcement learning theory: value
function methods, in: Machine Learning Summer School, 2002,
pp. 184-202.

[8] D.P. Bertsekas, J.N. Tsitsiklis, Neuro-Dynamic Programming,
Athena Scientific, Belmont, MA, 1996.

[9] J. Boyan, Least-squares temporal difference learning, in: Machine
Learning: Proceedings of the Sixteenth International Conference,
1999, pp. 49-56.

[10] S. Bradtke, E. Ydstie, A.G. Barto, Adaptive Linear Quadratic
Control Using Policy Iteration, University of Massachusetts,
Ambherst, MA, 1994.

[11] O. Buffet, A. Dutech, F. Charpillet, Shaping multi-agent systems with
gradient reinforcement learning, Autonomous Agents Multi-Agent
Syst. 15 (2) (October 2007) 1387-2532.

[12] F. Guenter, M. Hersch, S. Calinon, A. Billard, Reinforcement
learning for imitating constrained reaching movements, RSJ Adv.
Robotics 21 (13) (2007) 1521-1544.

[13] A. Ijspeert, J. Nakanishi, S. Schaal, Learning rhythmic movements by
demonstration using nonlinear oscillators, in: IEEE International
Conference on Intelligent Robots and Systems (IROS 2002), 2002,
pp. 958-963.

[14] S.A. Kakade, Natural policy gradient, in: Advances in Neural
Information Processing Systems, vol. 14, 2002.

[15] V. Konda, J. Tsitsiklis, Actor-critic algorithms, in: Advances in
Neural Information Processing Systems, vol. 12, 2000.

[16] T. Moon, W. Stirling, Mathematical Methods and Algorithms for
Signal Processing, Prentice-Hall, Englewood Cliffs, NJ, 2000.

[17] J. Park, J. Kim, D. Kang, An RLS-based Natural Actor-Critic
algorithm for locomotion of a two-linked robot arm, in: Proceedings
of Computational Intelligence and Security: International Conference
(CIS 2005), Xi’an, China, December 2005, pp. 15-19.

[18] J. Peters, S. Schaal, Policy gradient methods for robotics, in:
Proceedings of the IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), Beijing, China, 2006.

[19] J. Peters, S. Schaal, Applying the episodic natural actor-critic
architecture to motor primitive learning, in: Proceedings of the 2007
European Symposium on Artificial Neural Networks (ESANN), 2007.

[20] J. Peters, S. Vijayakumar, S. Schaal, Scaling reinforcement learning
paradigms for motor learning, in: Proceedings of the 10th Joint
Symposium on Neural Computation (ISNC), Irvine, CA, May 2003.

[21] J. Peters, S. Vijaykumar, S. Schaal, Reinforcement learning for
humanoid robotics, in: IEEE International Conference on Human-
doid Robots, 2003.

[22] J. Peters, S. Vijayakumar, S. Schaal, Natural Actor-Critic, in:
Proceedings of the European Machine Learning Conference
(ECML), Porto, Portugal, 2005.

[23] S. Richter, D. Aberdeen, J. Yu, Natural Actor-Critic for road traffic
optimisation, in: Advances in Neural Information Processing
Systems, 2007.

[24] R.S. Sutton, A.G. Barto, Reinforcement Learning, MIT Press,
Cambridge, MA, 1998.

[25] R.S. Sutton, D. McAllester, S. Singh, Y. Mansour, Policy gradient
methods for reinforcement learning with function approximation, in:
Advances in Neural Information Processing Systems, vol. 12, 2000.

[26] T. Ueno, Y. Nakamura, T. Shibata, K. Hosoda, S. Ishii, Fast and
Stable learning of quasi-passive dynamic walking by an unstable
biped robot based on off-policy Natural Actor-Critic, in: TEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),
2006.

[27] S.V. N Vishwanathan, X. Zhang, D. Aberdeen, Conditional random
fields for reinforcement learning, in: Y. Bengio, Y. LeCun (Eds.),
Proceedings of the 2007 Snowbird Learning Workshop, San Juan,
Puerto Rico, March 2007.

[28] X. Zhang, D. Aberdeen, S.V.N. Vishwanathan, Conditional random
fields for multi-agent reinforcement learning, in: Proceedings of the
24th International Conference on Machine Learning (ICML 2007),
ACM International Conference Proceeding Series, Corvalis, Oregon,
2007, pp. 1143-1150.

Jan Peters heads the Robot Learning Lab
(RoLL) at the Max-Planck Institute for Biologi-
cal Cybernetics (MPI) while being an invited
researcher at the Computational Learning and
Motor Control Lab at the University of Southern
California (USC). Before joining MPI, he grad-
uated from University of Southern California
with a Ph.D. in Computer Science in March 2007.
~ * Jan Peters studied Electrical Engineering, Com-
l > puter Science and Mechanical Engineering. He
holds two German M.Sc. degrees in Informatics and in Electrical
Engineering (Dipl-Informatiker from Hagen University and Diplom-
Ingenieur from Munich University of Technology/TUM) and two M.Sc.
degrees in Computer Science and Mechanical Engineering from University
of Southern California (USC). During his graduate studies, Jan Peters has
been a visiting researcher at the Department of Robotics at the German
Aerospace Research Center (DLR) in Oberpfaffenhofen, Germany, at
Siemens Advanced Engineering (SAE) in Singapore, at the National
University of Singapore (NUS), and at the Department of Humanoid
Robotics and Computational Neuroscience at the Advanced Telecommu-
nication Research (ATR) Center in Kyoto, Japan. His research interests
include robotics, nonlinear control, machine learning, and motor skill
learning.


===== PAGE 11 =====

J. Peters, S. Schaal {| Neurocomputing 71 (2008) 1180-1190

Stefan Schaal is an Associate Professor at the
Department of Computer Science and the Neu-
roscience Program at the University of Southern
California, and an Invited Researcher at the ATR
Human Information Sciences Laboratory in
Japan, where he held an appointment as Head
of the Computational Learning Group during an
international ERATO project, the Kawato Dy-
namic Brain Project (ERATO/JST). Before join-
ing USC, Dr. Schaal was a postdoctoral fellow at
the Department of Brain and Cognitive Sciences

and the Artificial Intelligence Laboratory at MIT, an Invited Researcher
at the ATR Human Information Processing Research Laboratories in
Japan, and an Adjunct Assistant Professor at the Georgia Institute of
Technology and at the Department of Kinesiology of the Pennsylvania
State University. Dr. Schaal’s research interests include topics of statistical
and machine learning, neural networks, computational neuroscience,
functional brain imaging, nonlinear dynamics, nonlinear control theory,
and biomimetic robotics. He applies his research to problems of artificial
and biological motor control and motor learning, focusing on both
theoretical investigations and experiments with human subjects and
anthropomorphic robot equipment.
