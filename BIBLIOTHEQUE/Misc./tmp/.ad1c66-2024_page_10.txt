           T. Han et al.                                                                  International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
           Fig. 13. Visualizations results of proposed method on the ScanNet dataset. (For interpretation of the references to color in this figure legend, the reader is referred to the web
           version of this article.)
           walls (See Fig. 14(b)). The doors adhered to the walls (even cases where      a greater exploration space for the combination of Graph and Trans-
           doors intersect with walls) and bookshelves are accurately identified.        former. The advantages of ASGFormer are more focused on addressing
           Moreover, the outlines and details of furniture such as tables, chairs,       the segmentation of structural adherent objects, which will be reflected
           and sofas are well-preserved, as shown in Fig. 14(c). City-Facade: The        in class-level metrics.
           representative semantic segmentation of our proposed method on City-             Additionally, we compared the proposed method with point cloud
           Facade are shown in Fig. 15, where ROIs in red boxes are represented          segmentation approaches based on different strategies, including voxel-
           the details. Obviously, for building facades, walls, windows, balconies,      based methods like SegCloud (addressing information loss caused by
           and advertisements share similar structural features, posing challenges       voxelization), continuous convolution-based methods like KPConv (mit-
           for semantic segmentation. The proposed method accurately identifies          igating the limited receptive field due to convolutional locality), and
           the contours of adhesive structures and segments components embed-            attention-based methods like PAT (simultaneously adjusting weights
           ded in the wall surface. Additionally, air conditioners mounted on the        for feature vectors and channel relationships to eliminate attention
           wall can also be recognized.                                                  shift under uneven density influence). Our architecture significantly
               Toronto 3D: The representative semantic segmentation of our pro-          enhances the effectiveness of graph model and Transformer framework
           posed method on Toronto 3D dataset are shown in Fig. 16. Four scenes          in indoor 3D point cloud semantic segmentation applications.
           werecombinedfromlefttorighttoformacompletearea.Theproposed                       AsshowninTable2,theclass-levelevaluation further highlights su-
           methodaccuratelysegmentsbuildingoutlinesfromdensevegetation.In                periority of proposed ASGFormer. Among thirteen classes in the S3DIS
           areas with complex feature relationships, different types of objects such     dataset, six (floor, ceiling, door, chair, board, clutter) are achieved
           as poles, trees, and cars are identified with clear structure. This part      as the top-1 by our method, while three (wall, sofa, bookcase) be-
           demonstrates the potential of the proposed ASGFormer in large-scale           come second-best. ASGFormer effectively distinguishes adherent ob-
           complex outdoor scenes.                                                       jects within homogeneous structures: (1) Completely segmented ar-
                                                                                         chitectural structures such as wall, floor, and ceiling. (2) Segmented
           4.4. Quantitative evaluation                                                  columns that are consistent with the architectural structure. (3) High-
                                                                                         precision segmentation of door, bookcase, and board that are connected
               S3DIS Area-5: Quantitative evaluation results for comprehensive           to the building structure. The unsatisfactory performance occurs in
           testing and class-level comparison are shown in Tables 1 and 2, respec-       the category of windows, where our method exhibits a significant
           tively. Following the S3DIS protocol, we validate using Area-5 of the         gap compared to the SOTA approach. This indicates that our method
           data and compare with the state-of-the-art models (See Table 1). The          has a certain degree of neglect in detecting window borders (thin
           OA, mAcc, and mIoU of our ASGFormer are 91.3%, 78.0%, and 72.3%,              and small objects), and a similar situation occurs in ScanNet as well.
           respectively. ASGFormer combines the strengths of 3D-GCN and Point            The competitive sampling strategy of the PAT adds confidence scores
           Transformer. The metrics reflect the performance improvement relative         to each sampled point, effectively avoiding the neglect of window
           to 3D-GCN (mIoU +20.4%) and Point Transformer (mIoU +1.9%).                   boundary points by the farthest point sampling. The presented method
           Compared to the previously leading graph algorithms AGConv, pro-              is compared with the Point Transformer and 3D-GCN, as shown in
           posed ASGFomer has demonstrated improvements of 1.3%, 4.8%, and               Fig. 17(a). This is because our approach is inspired by the combination
           4.4% in the three metrics. This implies that graph attention based on         of these two methods. We exhibit a significant improvement compared
           dynamic learning weights effectively combines global message passing          to 3D-GCN, and thought there are some shortcomings in certain cat-
           and local topological structure, providing more robust representation         egories compared to Point Transformer, we still maintain a dominant
           capabilities for graph learning. Although ASGFormer has a 1.1% lower          advantage.
                                                                                            S3DIS 6-fold: We also conduct a 6-fold experiment on the S3DIS
           mIoU compared to the latest SOTA Point Transformer V3, there is still         dataset. We achieved OA of 91.5% and mIoU of 77.2% ranking second
                                                                                     10 
