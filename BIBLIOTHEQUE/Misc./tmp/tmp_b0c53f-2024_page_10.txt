                                10     C. He et al.
                                            75.0               Performance vs. Speed
                                                                         ScatterFormer
                                                                      HEDNet
                                            72.5           DSVT-Voxel
                                                                 DVST-Pillar
                                            70.0       PV-RCNN++
                                            67.5            VoxSet
                                                               SST              CenterPoint-Voxel
                                            65.0 Part-A2
                                            mance (mAP@L2)PV-RCNN
                                            erfor62.5                                 CenterPoint-Pillar
                                            P
                                            60.0
                                                                                        Second
                                            57.5                                            PointPillars
                                                    5      10      15      20      25      30
                                                                Frame per second (FPS)
                                Fig.6: The detection performance (mAPH/L2) vs. speed (FPS) of different methods
                                on Waymo validation set. The speeds are measured on an NVIDIA A100 GPU.
                                4.2   Implementation Details
                                Ourapproachisimplementedusingtheopen-sourceframeworkOpenPCDet[43].
                                Toconstruct ScatterFormer, we set the voxel size to (0.32m, 0.32m, 0.1875m) for
                                the Waymodataset and (0.3m, 0.3m, 8m) for the NuScenes dataset. The window
                                sizes (S ,S ) for the two datasets are set to (12,12) and (20,20), respectively. We
                                       w h
                                stack six building blocks for the backbone network. We configure our attention
                                module to have 4 heads with a dimensionality of 128. ScatterFormer is trained
                                for 24 epochs with a learning rate of 0.006 on Waymo Dataset and 20 epochs
                                with a learning rate of 0.004 on NuScenes Dataset. In the last 4 epochs, we
                                disabled the data augmentation strategy of ground-truth sampling. The model
                                is trained on 8 RTX A6000 GPUs with a batch size of 32. Other settings for
                                training and inference adhere strictly to DSVT [47].
                                4.3   Comparison with State-of-the-Arts
                                Results on Waymo Open Dataset (WOD). We conduct a detailed com-
                                parison of ScatterFormer against the published results on the validation set of
                                the Waymo Open Dataset. In line with standard practices, we distinctly cate-
                                gorize and list the methods utilizing single-frame and multi-frame approaches.
                                To ensure comprehensive coverage, we also include comparisons with methods
                                incorporating long-term temporal modeling, such as those described in [4,15,61].
                                As demonstrated in Table 1, our single-stage model outperforms most two-stage
                                methods and achieves better performance than state-of-the-art methods such
                                as DSVT [47] and HEDNet [53]. Moreover,our model achieves 76.0 and 76.7
                                level 2 mAPH on 3 and 4 frame settings, respectively, outperforming previous
                                multiframe methods by a margin of 1.1. Furthermore, as shown in Figure 6,
