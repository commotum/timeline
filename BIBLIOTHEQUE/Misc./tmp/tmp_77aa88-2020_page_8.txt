                                  Published as a conference paper at ICLR 2021
                                            SGD                     SAM
                                             max=62.9                max=18.6
                                             max/ 5=2.5              max/ 5=3.6       0.08        m                        0.050
                                  Epoch: 1                                                                                                 Task 1
                                   0     20     40    60   0     20     40    60      0.07        1                        0.045           Task 2  0.17
                                                                                                  4
                                             max=12.5                max=8.9                      16                       0.040                   0.16
                                             max/ 5=1.7              max/ 5=1.9       0.06        64
                                  Epoch: 50                                                       256                      0.035                   0.15
                                    0       5      10       0       5      10        Error rate (%)0.05
                                                =24.2                   =1.0                                              Mutual information0.030         Mutual information
                                             max/ =11.4              max/ =2.6                                                                     0.14
                                             max 5                   max 5            0.04
                                  Epoch: 300010    20      0       10      20            0.00    0.05    0.10    0.15            1   4 16 64 256
                                            p( )                    p( )                                                                 m
                                  Figure 3: (left) Evolution of the spectrum of the Hessian during training of a model with standard
                                  SGD(lefthand column) or SAM (righthand column). (middle) Test error as a function of œÅ for dif-
                                  ferent values of m. (right) Predictive power of m-sharpness for the generalization gap, for different
                                  values of m (higher means the sharpness measure is more correlated with actual generalization gap).
                                  4     SHARPNESS AND GENERALIZATION THROUGH THE LENS OF SAM
                                  4.1    m-SHARPNESS
                                  ThoughourderivationofSAMdeÔ¨ÅnestheSAMobjectiveovertheentiretrainingset,whenutilizing
                                  SAMinpractice, we compute the SAM update per-batch (as described in Algorithm 1) or even by
                                  averaging SAMupdatescomputedindependentlyper-accelerator (where each accelerator receives a
                                  subset of size m of a batch, as described in Section 3). This latter setting is equivalent to modifying
                                  the SAM objective (equation 1) to sum over a set of independent  maximizations, each performed
                                  on a sum of per-data-point losses on a disjoint subset of m data points, rather than performing the
                                   maximization over a global sum over the training set (which would be equivalent to setting m
                                  to the total training set size). We term the associated measure of sharpness of the loss landscape
                                  m-sharpness.
                                  To better understand the effect of m on SAM, we train a small ResNet on CIFAR-10 using SAM
                                  with a range of values of m. As seen in Figure 3 (middle), smaller values of m tend to yield models
                                  having better generalization ability. This relationship fortuitously aligns with the need to parallelize
                                  across multiple accelerators in order to scale training for many of today‚Äôs models.
                                  Intriguingly, the m-sharpness measure described above furthermore exhibits better correlation with
                                                                                                                                              7
                                  models‚Äô actual generalization gaps as m decreases, as demonstrated by Figure 3 (right) . In partic-
                                  ular, this implies that m-sharpness with m < n yields a better predictor of generalization than the
                                  full-training-set measure suggested by Theorem 1 in Section 2 above, suggesting an interesting new
                                  avenue of future work for understanding generalization.
                                  4.2     HESSIAN SPECTRA
                                  Motivated by the connection between geometry of the loss landscape and generalization, we con-
                                  structed SAM to seek out minima of the training loss landscape having both low loss value and low
                                  curvature (i.e., low sharpness). To further conÔ¨Årm that SAM does in fact Ô¨Ånd minima having low
                                  curvature, we compute the spectrum of the Hessian for a WideResNet40-10 trained on CIFAR-10
                                  for 300 steps both with and without SAM(withoutbatchnorm,whichtendstoobscureinterpretation
                                  of the Hessian), at different epochs during training. Due to the parameter space‚Äôs dimensionality, we
                                  approximate the Hessian spectrum using the Lanczos algorithm of Ghorbani et al. (2019).
                                  Figure 3 (left) reports the resulting Hessian spectra. As expected, the models trained with SAM
                                  converge to minima having lower curvature, as seen in the overall distribution of eigenvalues, the
                                      7We follow the rigorous framework of Jiang et al. (2019), reporting the mutual information between
                                  the m-sharpness measure and generalization on the two publicly available tasks from the Predicting gen-
                                  eralization in deep learning NeurIPS2020 competition.               https://competitions.codalab.org/
                                  competitions/25301
                                                                                               8
