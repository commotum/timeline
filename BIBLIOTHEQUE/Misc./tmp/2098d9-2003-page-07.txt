                                             ANEURALPROBABILISTIC LANGUAGEMODEL
                    The y are the unnormalized log-probabilities for each output word i, computed as follows, with
                          i
                    parameters b,W,U,d and H:
                                                     y =b+Wx+Utanh(d+Hx)                                        (1)
                    where the hyperbolic tangent tanh is applied element by element, W is optionally zero (no direct
                    connections), and x is the word features layer activation vector, which is the concatenation of the
                    input word features from the matrix C:
                                                x =(C(w     ),C(w    ),···,C(w      )).
                                                         t−1      t−2          t−n+1
                    Lethbethenumberofhiddenunits, andmthenumberoffeaturesassociated witheachword. When
                    no direct connections from word features to outputs are desired, the matrix W is set to 0. The free
                    parameters of the model are the output biases b (with |V| elements), the hidden layer biases d (with
                    helements), the hidden-to-output weightsU (a |V|×h matrix), the word features to output weights
                    W (a |V|×(n−1)m matrix), the hidden layer weights H (a h×(n−1)m matrix), and the word
                    features C (a |V|×m matrix):         θ=(b,d,W,U,H,C).
                    The number of free parameters is |V|(1+nm+h)+h(1+(n−1)m). The dominating factor is
                    |V|(nm+h). Note that in theory, if there is a weight decay on the weights W and H but not on C,
                    then W and H could converge towards zero whileC would blow up. In practice we did not observe
                    such behavior when training with stochastic gradient ascent.
                        Stochastic gradient ascent on the neural network consists in performing the following iterative
                    update after presenting the t-th word of the training corpus:
                                                                 ˆ
                                                           ∂logP(w |w     ,···w     )
                                                  θ←θ+ε             t  t−1     t−n+1
                                                                       ∂θ
                    where ε is the “learning rate”. Note that a large fraction of the parameters needs not be updated
                    or visited after each example: the word features C(j) of all words j that do not occur in the input
                    window.
                        Mixtureofmodels. Inourexperiments(seeSection4)wehavefoundimprovedperformanceby
                    combining the probability predictions of the neural network with those of an interpolated trigram
                    model, either with a simple ﬁxed weight of 0.5, a learned weight (maximum likelihood on the
                    validation set) or a set of weights that are conditional on the frequency of the context (using the
                    same procedure that combines trigram, bigram, and unigram in the interpolated trigram, which is a
                    mixture).
                    3. Parallel Implementation
                    Although the number of parameters scales nicely, i.e. linearly with the size of the input window and
                    linearly with the size of the vocabulary, the amount of computation required for obtaining the output
                    probabilities is much greater than that required from n-gram models. The main reason is that with
                    n-gram models, obtaining a particular P(w |w   ,...,w      ) does not require the computation of
                                                             t  t−1      t−n+1
                    the probabilities for all the words in the vocabulary, because of the easy normalization (performed
                    when training the model) enjoyed by the linear combinations of relative frequencies. The main
                    computational bottleneck with the neural implementation is the computation of the activations of
                    the output layer.
                                                                 1143
