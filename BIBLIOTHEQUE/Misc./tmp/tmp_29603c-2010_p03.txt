                             Relative Entropy Policy Search                           Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShannon (BFGS) method (de-
                                                                                      noted in this paper by fmin BFGS(g,‚àÇg,[Œ∏ ,Œ∑ ]) with
                                                                                                                                         0   0
                 input: features œÜ(s), maximal information loss .                    ‚àÇg = [‚àÇ g,‚àÇ g]). The resulting method is given in Table
                                                                                                Œ∏    Œ∑
                 for each policy update                                               1.
                                                           0                          Sample-based Policy Iteration with REPS
                    Sampling: Obtain samples (s ,a ,s ,r ), e.g.,
                                                    i   i  i  i
                    byobserving another policy or being on-policy.                    If the REPS algorithm is used in a policy iteration scenario,
                    Counting: Count samples to obtain the                             one can re-use parts of the sampling distribution q(s,a). As
                                                                                                                  œÄ
                                                         P                            weknowthatq(s,a) = ¬µ l(s)œÄ (a|s) where œÄ denotes the
                    sampling distribution q(s,a) = 1        N Ii .                                                       l              l
                                                       N    i=1 sa                    last policy in a policy iteration scenario, we can also write
                    Critic: Evaluate policy for Œ∑ and Œ∏.                              our new policy as
                       DeÔ¨ÅneBellmanErrorFunction:                                                                           1          
                                      a +P       a    T       TŒ∏                                                œÄl(a|s)exp Œ∑Œ¥Œ∏(s,a)
                       Œ¥ (s,a) = R            0 P 0œÜ 0Œ∏ ‚àíœÜ
                         Œ∏            s      s   ss   s       s                                œÄl+1(a|s) = P                             .
                                                                                                                  œÄ (a|s)exp 1Œ¥ (s,b)
                       ComputeDualFunction:                                                                     b   l            Œ∑ Œ∏
                                                                  
                                          P             Œµ+1Œ¥ (s,a)
                       g(Œ∏,Œ∑) = Œ∑log            q(s,a)e    Œ∑ Œ∏                        As a result, we can also evaluate our policy at states where
                                            s,a                                       no actions have been taken. Setting œÄ to good locations
                       ComputetheDualFunction‚Äôs Derivative :                                                                     l
                                            Œµ+1Œ¥ (s,a)                                allows encoding prior knowledge on the policy. This up-
                                      q(s,a)e  Œ∑ Œ∏     (   0 Pa œÜ 0‚àíœÜ )
                       ‚àÇ g = Œ∑     s,a                    s  ss0 s    s               date has the intuitive interpretation that an increase in log-
                         Œ∏                           Œµ+1Œ¥ (s,a)
                                               q(s,a)e  Œ∑ Œ∏                           probability of an action is determined by the Bellman error
                                           s,a               
                                     P             Œµ+1Œ¥ (s,a)                         minus a baseline similar to its mean, i.e., logœÄ       (a|s) =
                       ‚àÇ g = log           q(s,a)e    Œ∑ Œ∏                                                                                 l+1
                         Œ∑             s,a                                            logœÄ (a|s) + 1Œ¥ (s,a)‚àíb(s).
                                         Œµ+1Œ¥ (s,a) 1                                       l        Œ∑ Œ∏
                                  q(s,a)e  Œ∑ Œ∏        Œ¥ (s,a)
                           ‚àí s,a                   Œ∑2 Œ∏                                  Obviously, the algorithm as presented in the previous sec-
                                             Œµ+1Œ¥ (s,a)
                                       q(s,a)e  Œ∑ Œ∏                                   tion would be handicapped by maintaining a high accuracy
                                    s,a                                                                                              a   a
                                     ‚àó   ‚àó                                            model of the Markov decision problem (R ,P 0). Model
                       Optimize: (Œ∏ ,Œ∑ ) = fmin BFGS(g,‚àÇg,[Œ∏ ,Œ∑ ])                                                                   s   ss
                                                                      0   0           estimation would require covering prohibitively many states
                                                        ‚àó        T ‚àó
                       Determine Value Function: VŒ∏ (s) = œÜ Œ∏
                                                                 s                    andactions, and it is hard to obtain an error-free model from
                    Actor: Compute new policy œÄ(a|s).                                 data (Deisenroth 2009; Sutton and Barto 1998). Furthere-
                                    q(s,a)exp 1 Œ¥ ‚àó(s,a)                              more,inmostinterestingcontrolproblems,wedonotintend
                       œÄ(a|s) =              (Œ∑‚àó Œ∏       ) ,
                                      q(s,b)exp( 1 Œ¥ ‚àó(s,b))                          to visit all states and take all actions ‚Äî hence, the number
                                     b          Œ∑‚àó Œ∏                                  of samples N may often be smaller than the number of all
                 Output: Policy œÄ(a|s).                                               state-action pairs mn. Thus, in order to become model-free,
                                                                                      we need to rephrase the algorithm in terms of sample aver-
               Table 1: Algorithmic description of Relative Entropy Pol-              ages instead of the system model.
               icy Search. This algorithm reÔ¨Çects the proposed solution                  The next step is hence to replace the summations over
               clearly.  Note that Ii    is an indicator function such that           states s, s0, and actions a by summations over samples
                i                     sa                   i
               I    = 1 if s = s and a = a while I            = 0 otherwise.                   0
                sa                 i             i         sa                         (s ,a ,s ,r ).    It turns out that this step can be accom-
               In Table 2, we show a possible application of this method in             i   i  i   i
               policy iteration.                                                      plished straightforwardly as all components of REPS can
                                                                                      be expressed using sample-based replacements such as
                                                                                      P                            P
                                                                                            q(s,a)f(s,a) = 1          N f(s ,a ). As the Bellman
               function for the critic in Eq.(10) differs substantially from             s,a                    N     i=1     i  i
                                                                                      error Œ¥ (s ,a ) only needs to be maintained for the executed
               traditional temporal difference errors, residual gradient er-                 Œ∏   i  i
               rors and monte-carlo rollout Ô¨Åttings (Sutton and Barto 1998;           actions, we can also approximate it using sample averages.
               Sutton et al. 2000). The presented solution is derived for ar-            Usingthesetwoinsights,wecandesignageneralizedpol-
               bitrary stationary features and is therefore sound with func-          icy iteration algorithm that is based on samples while using
               tion approximation. The derived policy is similar to the               the main insights of Relative Entropy Policy Search. The re-
               Gibbs policy used in policy gradient approaches (Sutton et             sulting method is shown in Table 2. Note that Table 2 does
               al. 2000) and in SARSA (Sutton and Barto 1998).                        not include sample re-use in REPS policy iteration. How-
                  In order to turn proposed solution into algorithms, we              ever, this step may be included straightfowardly as we can
               needtoefÔ¨Åcientlydeterminethesolution(Œ∏‚àó,Œ∑‚àó)ofthedual                   mixdatafrompreviousiterationswiththecurrentonebyus-
               function g. Eq. (10) can be rewritten as                               ing all data in the critic and the sum of all previous policies
                                       X                                              in the actor update. While such remixing will require more
                                ‚àí1                                                    policy update steps, it may improve robustness and allow
               ming(Œ∏,Œ∑Àú) = Œ∑Àú      log     exp(logq(s,a)+Œµ+Œ∑ÀúŒ¥Œ∏(s,a)),
                Œ∏,Œ∑Àú                    s,a                                           updates after fewer sampled actions.
               which is known to be convex (Boyd and Vandenberghe                                            Experiments
               2004) as Œ¥ (s,a) is linear in Œ∏. Given that g is convex and
                          Œ∏
               smoothly differentiable, we can determine the optimal solu-            In the following section, we test our Sample-based Policy It-
               tion g(Œ∏‚àó,Œ∑‚àó)efÔ¨Åcientlywithanystandardoptimizersuchas                  eration with Relative Entropy Policy Search approach using
                                                                                1609
