                                                     Test-Time Learning for Large Language Models
               Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,       openreview.net/forum?id=yzkSU5zdwD. Sur-
                                           `
                 M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E.,          vey Certification.
                 Azhar, F., et al. Llama: Open and efficient foundation lan-   Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
                 guage models. arXiv preprint arXiv:2302.13971, 2023a.            E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting
               Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,         elicits reasoning in large language models. Advances in
                 A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,          neural information processing systems, 35:24824–24837,
                 Bhosale, S., et al. Llama 2: Open foundation and fine-           2022b.
                 tuned chat models. arXiv preprint arXiv:2307.09288,           Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu,
                 2023b.                                                           B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5
               Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal,         technical report. arXiv preprint arXiv:2412.15115, 2024.
                 A. Interleaving retrieval with chain-of-thought reasoning     Yi, C., Chen, H., Zhang, Y., Xu, Y., Zhou, Y., and Cui,
                 for knowledge-intensive multi-step questions. In Rogers,         L. From question to exploration: Can classic test-time
                 A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceed-            adaptation strategies be effectively applied in semantic
                 ings of the 61st Annual Meeting of the Association for           segmentation? In Proceedings of the 32nd ACM Inter-
                 Computational Linguistics (Volume 1: Long Papers), pp.           national Conference on Multimedia, pp. 10085–10094,
                 10014–10037, Toronto, Canada, July 2023. Association             2024.
                 for Computational Linguistics. doi: 10.18653/v1/2023.
                 acl-long.557. URLhttps://aclanthology.org/                    Zhang, M., Levine, S., and Finn, C. Memo: Test time
                 2023.acl-long.557/.                                              robustness via adaptation and augmentation. Advances in
               Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,        neural information processing systems, 35:38629–38642,
                 L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-             2022a.
                 tention is all you need. Advances in neural information       Zhang, Q., Bian, Y., Kong, X., Zhao, P., and Zhang, C.
                 processing systems, 30, 2017.                                    Come: Test-time adaption by conservatively minimizing
               Wang,D.,Shelhamer,E.,Liu,S.,Olshausen,B.,andDarrell,               entropy. In The Thirteenth International Conference on
                 T. Tent: Fully test-time adaptation by entropy minimiza-         Learning Representations, 2025.
                 tion. International Conference on Learning Representa-        Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
                 tions, 2021.                                                     Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,
               Wang, Q., Hu, J., Li, Z., Wang, Y., Hu, Y., Tan, M., et al.        et al. Opt: Open pre-trained transformer language models.
                 Generating long-form story using dynamic hierarchical            arXiv preprint arXiv:2205.01068, 2022b.
                 outlining with memory-enhancement. The 2025 Annual            Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi,
                 Conference of the Nations of the Americas Chapter of the         Y. Bertscore: Evaluating text generation with bert. arXiv
                 ACL,2025a.                                                       preprint arXiv:1904.09675, 2019.
               Wang, Y., Li, P., Sun, M., and Liu, Y. Self-knowledge           Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,
                 guided retrieval augmentation for large language mod-            Min,Y.,Zhang,B.,Zhang,J.,Dong,Z.,etal. Asurveyof
                 els.  In The 2023 Conference on Empirical Methods                large language models. arXiv preprint arXiv:2303.18223,
                 in Natural Language Processing, 2023. URL https:                 2023.
                 //openreview.net/forum?id=MoEfm3iPMy.
               Wang, Y., Hu, J., Huang, Z., Lin, K., Zhang, Z., Chen,          Zhong, W., Guo, L., Gao, Q., Ye, H., and Wang, Y. Memo-
                 P., Hu, Y., Wang, Q., Yu, Z., Sun, B., Xing, X., Zheng,          rybank: Enhancinglargelanguagemodelswithlong-term
                 Q., and Tan, M. Enhancing user-oriented proactivity in           memory. In Proceedings of the AAAI Conference on Arti-
                 open-domaindialogueswithcriticguidance. Proceedings              ficial Intelligence, volume 38, pp. 19724–19731, 2024.
                 of the Thirty-Fourth International Joint Conference on        Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-
                 Artificial Intelligence, 2025b.                                  sun, R., Torralba, A., and Fidler, S. Aligning books and
               Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,             movies: Towards story-like visual explanations by watch-
                 Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-            ing movies and reading books. In Proceedings of the
                 zler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,         IEEE international conference on computer vision, pp.
                 P., Dean, J., and Fedus, W. Emergent abilities of large          19–27, 2015.
                 language models. Transactions on Machine Learning
                 Research, 2022a. ISSN 2835-8856. URL https://
                                                                            13
