        146              5.  MONOTONE COMPLEXITY
        Then the next letter is read and we add — logp to the complexity, where p is the 
        declared probability of that letter (i.e., its probability with respect to the guessed 
        distribution).
          If we believe that the behavior of the reader is computable,  the result  is an 
        upper  bound  for  the  complexity.  Indeed,  the  reader  provides  (some  part  of)  a 
        computable  probability  distribution  on  the  set  of strings  telling  the  conditional 
        probabilities along some path, and the complexity of text does not exceed the sum 
        of negative logarithms of these probabilities (Theorem 89).
          Of course,  it  is not  practical to require that the reader provides at each step 
        the  list  of probabilities  for  all  the  letters;  one  can  suggest  some  standard  types 
        of answers such as  “the next letter is A with probability 0.5,  all other vowels are 
        équiprobable  and  have  total  probability  0.3,  all  other  letters  are  équiprobable”. 
        Note also that we get an upper bound for the conditional complexity of the text 
        where the condition is the background of the reader.  (For example, if the reader 
        knows the text by heart or is just familiar with the author’s writings, the bound 
        can be very small.)
          The same trick used in compression algorithms is called arithmetic coding and 
        was even patented (many years after Kolmogorov’s experiments in the 1970s).
          Now we are ready to formulate the criterion of Martin-Löf randomness that 
        uses monotone complexity:  a sequence is ML-random if and only if the inequality 
        of Theorem 89 becomes an equality for its prefixes.
          Let us formulate this statement precisely.  Let /i be a computable probability 
        distribution on the set Q of all infinite bit sequences, and let p(x) be the measure 
        of the interval Q,x:  p(x) — p>(Q,x).
          Theorem 90 (Levin-Schnorr).  A sequence со e Q, is ML-random with respect 
        to  a computable probability distribution /i if and only if
                          — log p(x) — KM(x) ^ c 
        for some c and for every prefix x of со.
          Proof.  We have to prove this theorem in both directions.  Let us show first 
        that if (for a given sequence со)  the difference — logp(x) — KM(x)  is unbounded, 
        then this sequence is not ML-random (i.e., the set {cu} is an effectively null set).
          Fix some constant c and consider all strings x such that — logp(x) — KM(x) > c. 
        (This difference is sometimes called randomness deficiency, but this term has differ­
        ent meanings.  We have already used it in the previous chapter, and in Chapter 14 
        it  is used in a different way.)  This set is denoted by Dc.
          The set Dc is enumerable (since p is computable and KM is upper semicom- 
        putable, the difference is lower semicomputable).
          Lemma  1.  The  set  of all  infinite  sequences  that  have  a prefix  in  Dc  has /i- 
        measure at most 2~c.
          Informally speaking, this is true because on this set the measure /i is 2C times 
        smaller  than  the  a  priori  probability  (and  the  latter  does  not  exceed  1).  More 
        formally this argument can be explained as follows.
          We are interested in the measure of the union of intervals flx  for all x  G Dc. 
        Without changing this union, we may keep only minimal x G Dc (i.e., strings x e Dc 
        such that no prefix of x belongs to Dc).  Let xq,x\, ... be these minimal elements
