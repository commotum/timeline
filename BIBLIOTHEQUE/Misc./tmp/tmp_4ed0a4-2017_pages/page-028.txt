                   SOME APPLICATIONS OF KOLMOGOROV COMPLEXITY  11
         why this theory could be applied to the real-world phenomena and how it should 
         be applied.
           Assume that we toss  a  coin  a thousand  times  (or  test  some other hardware 
         random number generator) and get a bit string of length 1000.  If this string contains 
        only zeros or equals 0101010101...  (zeros and ones alternate), then we definitely 
        will conclude that the generator is bad.  Why?
           The usual explanation:  the probability of obtaining a thousand zeros is negli­
        gible  (2“100°) provided the coin is fair.  Therefore,  the conjecture of a fair coin is 
        refuted by the experiment.
           The problem with this explanation is that we do not always reject the generator: 
        there should be some sequence a of a thousand zeros and ones which is consistent 
        with this conjecture.  Note, however, that the probability of obtaining the sequence 
        a as a result of fair coin tossing is also 2“1000.  So what is the reason behind our 
        complaints?  What is the difference between the sequence of a thousand zeros and 
        the sequence a?
           The reason is revealed when we compare the Kolmogorov complexities of these 
        sequences.
           Proving theorems  of probability  theory.  As an example,  consider  the 
        Strong Law of Large Numbers.  It claims that for almost all (according to the the 
        uniform Bernoulli probability distribution)  infinite binary sequences,  the limit of 
        frequencies of ones in their initial segments equals 1/ 2.
           More formally,  let  ft  be  the  set  of all  infinite  sequences  of zeros  and  ones. 
        The uniform Bernoulli measure on ft is defined as follows.  For every finite binary 
        string X, consider the set ftx consisting of all infinite sequences that start with x. 
        For example,  ft a  =  ft-  The  measure of ftx  is equal to  2~l(x'>.  For example,  the 
        measure of the set floi, that consists of all sequences starting with 01, equals 1/4.
           For each sequence со = u)qcvicv2 . . .  consider the limit of the frequencies of ones 
        in the prefixes of co, that is,
                              u>o + CJi + ... + <vn_i
                           hm  -------------------------- .
                           n—>oo     Tl
        We say that u> satisfies the Strong Law of Large Numbers (SLLN) if this limit exists 
        and is equal to 1/2.  For instance, the sequence 010101..., having period 2, satisfies 
        the SLLN, and the sequence 011011011..., having period 3, does not.
           The SLLN says that the set of sequences that do not satisfy SLLN has measure 
        0.  Recall that  a set  A  C  ft has measure 0 if for all e  >  0 there is  a sequence of 
        strings то, x\, X2,...  such that
                           A C ftXo U ftXl UftX2U...
        and the sum of the series
                          2~i(xo) _|_ 2~l(xi) -|- 2~dx2) _|_
        (the sum of the measures of ftXi) is less than e.
           One can prove SLLN using the notion of a Martin-Löf random sequence men­
        tioned above.  The proof consists of two parts.  First, we show that every Martin-Löf 
        random sequence satisfies SLLN. This can be done using Levin-Schnorr random­
        ness criterion  (if the limit does not exist or differs from 1/2, then the complexity 
        of some prefix is less than it should be for a random sequence).
           The second part is rather general and does not depend on the specific law of 
        probability theory.  We prove that the set of all Martin-Löf non-random sequences
