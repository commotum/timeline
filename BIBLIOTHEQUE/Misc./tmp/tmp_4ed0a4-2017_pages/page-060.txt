                                               2.3.    COMPLEXITY AS  THE AMOUNT  OF  INFORMATION                                                            45
                                             2.3.  Complexity as the amount of information
                            As we know (Theorem 18), the conditional complexity C(y\x) does not exceed 
                    the unconditional one C(y)  (up to a constant).  The difference C(y) — C(y\x) tells 
                    us how the knowledge of y makes x easier to describe.  So this difference can be 
                    called the amount of information in x about y.  We use the notation I(x:y).
                           Theorem 18 says that I(x:y) is non-negative (up to a constant):  there exists 
                    some c such that I(x:y) ^ c for all x and y.
                             58 Let / be a computable function.  Prove that I(f(x):y)  ^ I{x:y) + c for 
                    some c and for all x, y such that /(x) is defined.
                           A generalization of this statement to probabilistic algorithms is possible.
                             59 Let /(x, r) be a computable function of two arguments, and let r be chosen 
                    at random uniformly among n-bit strings for some n.  Then for each I the probability 
                    of the event
                                                                     I(f(x,r):y) > I{x:y)+l
                    does not exceed 2~l+ot'C^ +c^ .
                            (Hint:  Use the conditional version of Problem 41.)
                           These properties of information can be described as  conservation laws for in­
                    formation (about something)  in algorithmic or random processes.  As Levin once 
                    put it,  “by torturing an uninformed person you do not get any evidence about the 
                    crime.”  He discusses this property (for different notions of information) in [100].
                           Recall that
                                                       C(x, y) = C(x) + C(y I x) + О (log C(x, y))
                    (Theorem 22, p. 39).  This allows us to express conditional complexity in terms of 
                    an unconditional one:  C(y|x) = C(x,y) — C(x) + 0(logC(x,y)).  Then we get the 
                    following expression for the information:
                                 I(x:y) = C(y) - C(y\x) = C(x) + C(y) - C(x,y) + 0(logC(x,y)).
                    This expression immediately implies the following theorem:
                           THEOREM 23 (Information symmetry).
                                                             I(x:y) = I(y:x) + 0(logC(x,y)).
                           So the difference between I(x:y) and I(y:x) is logarithmically small compared 
                    to C(x, y).  The following problem shows that at the same time this difference could 
                    be comparable with the values I(x:y) and I(y:x) if they are much less than C(x, y).
                             60  Let x be a string of length n such that C{x\n) ^ n.  Show that 
                                                      /(x:n) = C{n) + 0(1) and /(n:x) = 0(1).
                           The property of information symmetry  (up  to  a logarithmic  term)  explains 
                    why I(x:y)  (or I(y:x))  is sometimes called  mutual information in two strings x 
                    and y.  The connection between mutual information, conditional and unconditional 
                    complexities, and pair complexity can be illustrated by a (rather symbolic) picture 
                    (see Figure 3).
                           It shows that strings x and у have I(x:y) ~ I(y:x) bits of mutual information. 
                    Adding C(x\y) bits (information that is present in x but not in y, the left part), 
                    we obtain
                                            I(y:x) + C(x\y) « (C(x) - C(x\y)) + C(x\y) = C(x)
