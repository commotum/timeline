                             10.  INEQUALITIES  FOR ENTROPY,  COMPLEXITY  AND SIZE
              346
              is not less than the mutual information between 7 and aß.  On the other hand, the 
              condition W(a,ß, 7) = 0 implies that the mutual information between aß and 7 
              equals I(a:ß:7) = I(a:ß).  Also, the conditions H(£\a) = #(£|/3) — 0 imply that 
              the mutual information between aß and £ equals I (a :/?:£) (see again the diagram). 
              Therefore,  I(a:ß)  ^  /(cc :/?:£)  ^  I(a:ß:7)  =  I(a:ß),  and  all  inequalities  here 
              become equalities.  The equality  between  the two  first  terms  means  that  a  and 
             ß  are  independent  when £  is  given.  Finally,  the  entropy  #(£ 17)  is  bounded  by 
              #(£|q!,7) + #(£|/?,7) + I(a:ß I7) and all three terms in the last sum are zeros.)
                  In [154] a Kolmogorov complexity version of Theorem 219 is proven.  Assume 
             that a, b, c are strings,  and three quantities I(a:b\c), I(a:c\b),  and I(b:c\a)  are 
             small,  e.g.,  are  bounded  by  0(log(|a| + |b| + |c|)).  Then  there  exists  a string  d 
             such  that  the  conditional  complexities  (7(d|a),  C(d\b),  C(d\c),  as  well  as  the 
             mutual information I(a:b\d), I(b:c\d), and I(a:c\d)  are also small  (bounded by 
             0 (log(|o| + \b\ + |c|))).
                  Artificial independence.  We considered some special cases of Theorem 218. 
             Now we will prove Theorem 208 in the general case using some trick  (by making 
             some variables independent).
                  Proof.  Let us split the variables in the inequality into three groups:
                                            (1)  a.ßi    (2)4,*;  (3) e.
             Note that in our inequality (2)-variables never appear in the same tuple with (3)- 
             variables, though variables of both groups are used together with (Invariables.  So 
             without loss of generality we may assume that the pair (7, S)  and e are independent 
             given  (a,ß).  Indeed,  consider  a different joint  distribution  for  all  the  variables, 
             obtained  in  the  following  way:  First  we  generate  values  of  (a,ß)  according  to 
             the existing distribution,  and then we independently generate values of (7, S)  and 
             e  according to their  (existing)  conditional distributions given  (a,ß).  Indeed,  for
             (l)  + (2)-variables the distribution remains the same, so the entropies do not change; 
             the same is true for (1) + (3)-variables.
                  Knowing this,  we  see  that  it  is  enough  to  prove  a  weaker  inequality  (with 
             additional terms in the right-hand side):
                             I (a :ß)^I(a:ß\'y) + I(a:ß\S) + /(7 : Ô) + W(a, ß, e)
                                        + /((7, S):e\(a,ß))
                                        + I('y:e\(a,ß))
                                        + I(6:e\(a,ß)).
             Indeed, if the pair  (7, S)  is independent with e given (a,ß), then the same in true 
             for its components 7 and 5, so the last three terms vanish after making the groups 
             artificially independent  (and the other terms remain the same).
