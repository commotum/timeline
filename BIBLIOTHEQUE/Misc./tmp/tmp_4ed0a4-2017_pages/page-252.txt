                                       8.4.  LAWS  OF  LARGE  NUMBERS                        239
                 However, we can go in the other direction.  Namely,  we may first prove that 
             for any ML-random sequence the frequencies converge to 1/2 using the randomness 
             criterion  in  terms  of complexity  (Theorem  90,  p.  146).  This  criterion  says  that 
             for an ML-random (with respect to the uniform Bernoulli measure) sequence u> the 
             monotone complexity of its prefix (cj)n of length n is n+0( 1).  This property implies 
             that the frequency of ones in (ui)n (i.e., pn) converges to 1/2.  Indeed, Theorem 146 
             (p.  226) says that the complexity of u>n does not exceed nh(pn, 1 — pn) + O(logn), 
            so  h(pn, 1 — pn)  —  1 + 0(\ogn/n)  for any ML-random sequence.  (Note that the 
            difference between plain and prefix complexity of u>n  is O(logn),  so any of them 
            can be used.)  This implies that pn —> 1/2 as n —> oo (see the graph of the entropy 
            function, Figure 8, p. 57).  So the SLLN is true for all ML-random sequences, which 
            form a set of full measure.
                 The skeptical observer would say that this is not a different proof, or we have 
            just  repeated the same arguments using different language.  And she is probably 
            right.  If we recall the proof of Theorem 146, we see that it uses the same estimate 
             (based on Stirling’s approximation) that was used for the proof of SLLN. (Another 
            argument, where monotone complexity is bounded by a negative logarithm of the 
            measure, Theorem 89, also has a direct translation in the probabilistic language; it 
            was discussed in Section 3.2 after the proof of Theorem 27 on p. 56.)
                 So what do we get by using complexity language?  First, we find a broader class 
            of sequences that satisfy the SLLN:
                 Theorem 155.  Letu> be a binary sequence such that C((u>)n) = n + o(n).  Then 
             the sequence pn  (the frequency of ones in (cu)n)  converges to 1/2.
                 P r o o f.  The proof remains essentially unchanged:  in this case h(pn, 1 —pn) is
            still  1 + o(l).                                                                  □
                Second,  we  not  only  can  prove  that  pn  —>  1/2  but  we  also  can  give  some 
            estimates for the convergence speed.  The corresponding result in probability theory 
            is known as the Law of the Iterated Logarithm, and V. Vovk [208] has shown that it 
            is valid for ML-random sequences.  Following his argument, let us use Kolmogorov 
            complexity to give a (rather simple) proof of the upper bound provided by this law.
                Theorem  156.  Let u>  be an ML-random sequence with respect to the uniform 
            measure.  Let pn  be  the frequency  of ones  in  (cj)n.  Then for  every e  >  0,  the 
            inequality
            holds for any sufficiently large n.
                P r o o f.  Let  us  first  look  at  what  bound  can  be  obtained  by  the  argument 
            above (that uses Kolmogorov complexity).  We know that
                            n -  0(1) < KM((u)n) ^ nh(pn, 1 ~Pn) + O(logn)
            therefore
                                      h(pn, 1 -  pn) > 1 -  0(\ogn/n).
            The function
                            p    h(p, 1 -  p) = p (-logp) + (1 -  p )(-log(l -  p))
