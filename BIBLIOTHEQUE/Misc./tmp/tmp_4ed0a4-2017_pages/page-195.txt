                      5.9.  RANDOMNESS  WITH RESPECT TO DIFFERENT MEASURES        181
               (Hint:  We interleave two sequences:  at  positions  0,2,4,...  we put  a generic 
           sequence  7;  and  at  positions  1,3,5,...  we  put  an  ML-random  sequence  ы  that 
           computes 7.  The resulting sequence is Turing-equivalent to ui.  Note also that if a 
           sequence is ML-random with respect to some measure P, that its subsequence with 
           even indices is ML-random with respect to the projection of P on these coordinates; 
           see Theorem 123.)
               The sequences that are not random with respect to any computable measure, 
           are similar  (in a sense)  to non-stochastic objects in the sense of Kolmogorov (see 
           Section 14.2).  Moreover, one can show that if a sequence a is random with respect 
           to  a computable measure,  then its prefixes  are stochastic  objects  (Problem  349, 
           p. 430).
               5.9.3.    Image randomness.  We started this chapter by considering a proba­
           bilistic machine that consists of a (fair) random bit generator and an algorithm that 
           transforms this sequence of random bits into a finite or infinite output sequence.  Let 
           us return to this scheme and assume that with probability 1 the output sequence 
           is infinite.  In this case we get a computable output distribution ß.
               A (slightly philosophical) question arises:  Which infinite sequences are plausible 
           as outcomes of such a machine? There are two possible answers.
               First, we have a definition of Martin-Löf randomness that can be applied to the 
           computable distribution ß.  We can say that plausible sequences are the sequences 
           that are ML-random with respect to this distribution.  On the other hand, we can 
           look inside the machine and ask, Which sequences are plausible as the outputs of a 
           random bit generator?  The natural answer is ML-random sequences with respect 
           to uniform distribution.  According to this answer, plausible output sequences are 
           images of ML-random sequences (with respect to uniform distribution) under the 
           computable transformation performed by the machine.
              Which of these two answers is more philosophically convincing?  Fortunately, 
           we do not need to make a choice here, since these two classes coincide.  Here are 
           the exact statements and proofs.
              Let ß be a computable probability distribution on Q, and let /: E —> E be a 
           continuous computable mapping.  Consider the image of the measure ß with respect 
           to /, i.e., a measure и on the set E such that
                                       v(U) = li(rl(U))
           for  any  U  С  E.  In other words,  v is the probability distribution of the random 
           variable     where w is a random variable that has distribution ß.  In the general 
           case the distribution и is not  concentrated on S7  and may assign positive proba­
           bilities to finite sequences;  in our terminology и may be a semimeasure (and this 
           semimeasure is lower semicomputable),  not a measure.  Let us assume,  however, 
           that it is not the case and that и is a measure on Q.  (It is easy to see that in this 
           case v is a computable measure.)
              Theorem 123.  (a) For any sequence ui E fl that is ML-random with respect to 
           measure ß,  its image f(ui)  is  an infinite sequence  that is ML-random with respect 
           to measure v.
               (b)    Any sequence r that is ML-random with respect to v can be obtained in this 
           way,  i.e.,  there  exists a sequence w  that is ML-random with respect to ß such that 
           f  M  -  T.
