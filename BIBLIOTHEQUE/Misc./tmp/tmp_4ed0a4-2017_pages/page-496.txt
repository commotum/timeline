        488          2.  FOUR  ALGORITHMIC FACES  OF  RANDOMNESS
        about Solomonoff’s work when publishing his  1969 paper,25  and he cited it).  So 
        we  called  the  statement  about  existence  of an  optimal  description  language  the 
        Solomonoff-Kolmogorov theorem.  At the same  time  (the middle of 1960s)  Kol­
        mogorov suggested in his seminar talks that the growth of complexity of prefixes 
        can be used to define randomness for individual infinite sequences.  However, the 
        family of description languages introduced by Kolmogorov turned out to be unsuit­
        able for this,  and (as we have said before)  a suitable family was found in 1973 by 
        Leonid Levin who defined the notion of monotone entropy.
           Typical sequences were defined  (and called  “random”)  by Per Martin-Löf in 
        1966, as we have said earlier.
           The existence of a Kolmogorov stochastic sequence that is not typical (= not 
        chaotic) was proven by Alexander Shen (see [6] or [2, Section 6.2.4]).26
           Let  К be one of the entropy functions  (many of them were studied,  includ­
        ing plain,  a priori,  monotone,  process, prefix, and decision entropies;  the versions 
        mentioned are different in the sense that the difference between any two of these 
        entropy functions is not bounded).  We may try to define chaotic sequences (with 
        respect to the uniform distribution) using К by requiring that
                        3c Mn (K(ai, a2, a3,..., an) > n -  c).
        (Just for the record:  for plain and decision entropy no sequences with this property 
        exist,  and for four other versions we get a definition that is equivalent to typical­
        ness.)  The equivalence of chaoticness for monotone entropy and typicalness was 
        shown by Levin in the same paper where monotone entropy was introduced.  In­
        dependently Claus-Peter Schnorr in his 1973 paper27  (the conference version was 
        published in 1972) introduced another version of entropy, process entropy (Schnorr 
        used  the  name  “process  complexity”)  and  proved  (by  a  similar  argument)  that 
        the corresponding notion of chaoticness is equivalent to typicalness.  Process en­
        tropy and monotone entropy differ significantly (their difference is unbounded, as 
        Vladimir Vyugin showed in [7]); later Schnorr switched to monotone entropy, and 
        the equivalence between chaoticness based on monotone entropy and typicalness is 
        sometimes called the Levin-Schnorr theorem.
           Prefix entropy was introduced by Levin in his Ph.D. thesis submitted in 1971, 
        but the thesis was rejected28 and the definition was published only in 1974.29  Later 
        Gregory J. Chaitin independently discovered the same definition (see his paper “A 
        theory of program size formally identical to  information theory”,  Journal  of the 
        Association of Computing Machinery,  1975, v.  22,  no.  3,  329-340)  where he also 
        introduced chaoticness definition using prefix entropy and claimed (without proof) 
        that  this  version  of chaoticness  is  equivalent  to  typicalness;  the  proof was  first 
        published in Vyugin’s paper  [7,  Corollary 3.2].  Prefix entropy can be defined as
           25See item [79]  in the main list of references.
           26The  main idea of this  proof was  invented  by  M.  van  Lambalgen  for  monotone selection 
        rules and can be easily generalized to non-monotone ones.  — A.  Shen.
           27See item [169] in the main list of references.
           28Levin was a USSR citizen.  The rejection of his thesis, having been approved by Kolmogorov 
        who was the thesis advisor and all the reviewers, took place for political reasons.  He emigrated 
        in  1978 and earned a Ph.D. at the Massachusetts Institute of Technology (MIT)  in 1979.
           29See  item  [94]  in  the  main  list  of references,  where  the  prefix  entropy  was  called  prefix 
        complexity, we use the same name in the main part of this book.
