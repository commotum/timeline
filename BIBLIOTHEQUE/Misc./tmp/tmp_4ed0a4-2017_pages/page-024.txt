                      COMPLEXITY AND INFORMATION       7
          Note also that we can exchange p and q and thus prove that 
                    C(xy) ^ C(x) + C(y) + 2 log2 C(y) + c.
          How tight  is  the  inequality  of Theorem  4?  Can  C(xy)  be  much  less  than 
       C(x) + C(y)? According to our intuition, this happens when x and y have much in 
       common.  For example, if x — y, we have C(xy) = C(xx) = C(x) + 0(1), since xx 
       can be algorithmically obtained from x and vice versa (Theorem 3).
          To refine this observation,  we will define the notion of the quantity of infor­
       mation in x that  is  missing  in y  (for  all  strings  x  and  y).  This  value  is  called 
       the Kolmogorov complexity of x  conditional to y  (or  “given y” ) and is denoted by 
       C(x\y).  Its definition is similar to the definition of the unconditional complexity. 
       This time the decompressor D has access not only to the (compressed) description, 
       but also to the string y.  We will discuss this notion later in Section 2.  Here we 
       mention only that the following equality holds:
                     C(xy) = C(y) + C(x I у) + О (log n)
       for all strings x and у of complexity at most n.  The equality reads as follows:  the 
       amount of information in xy is equal to the amount of information in у plus the 
       amount of new information in x (“new”  = missing in y).
          The difference C(x) — C(x\ y) can be considered as “the quantity of information 
       in у about x”.  It indicates how much the knowledge of у simplifies x.
          Using the notion of conditional complexity, we can ask questions like this:  How 
       much new information does the DNA of some organism have compared to that of 
       another organism’s DNA? If d\ is the binary string that encodes the first DNA and 
       d2 is the binary string that encodes the second DNA, then the value in question is 
       C(d\ |^2)-  Similarly we can ask what percentage of information has been lost when 
       translating a novel into another language:  this percentage is the fraction
                     C (original | translation) / C (original).
          The questions about  information in different objects were studied before the 
       invention of algorithmic information theory.  The information was measured using 
       the  notion of Shannon entropy.  Let  us recall its definition.  Let  £  be a random 
       variable that takes n values with probabilities p\,... ,pn-  Then its Shannon entropy 
       #(£) is defined as
                        #(£) = X ^ ( “ log2Pi)-
       Informally, the outcome having probability pi carries log(l/pj)  = — log2Pi bits of 
       information (=surprise).  Then #(£) can be understood as the average amount of 
       information in an outcome of the random variable.
          Assume that we want to use Shannon entropy to measure the amount of infor­
       mation contained in some English text.  To do this, we have to find an ensemble of 
       texts and a probability distribution on this ensemble such that the text is “typical” 
       with respect to this distribution.  This makes sense for a short telegram, but for a 
       long text (say, a novel) such an ensemble is hard to imagine.
          The same difficulty arises when we try to define the amount of information in 
       the genome of some species.  If we consider as the ensemble the set of the genomes 
       of all existing species (or even all species that ever existed), then the cardinality of 
       this set is rather small  (it does not exceed 21000  for sure).  And if we consider all
