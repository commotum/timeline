       374      12.  MULTISOURCE  ALGORITHMIC INFORMATION THEORY
          Now we can formulate a combinatorial equivalent of Theorem 229.
          Theorem  230.  There exists  a constant c such that for every positive integer 
       a, b, m with m ^ a the described game with parameters a, b, m, and c has a winning 
       strategy for M.
          Let us show that this combinatorial statement is indeed equivalent to Theo­
       rem 229.  Assume that it is true for some c.  Consider a blind adversary that does 
       not look at M’s moves and for each В declares as simple all strings of length a that 
       have conditional complexity (given В) less than m.  This behavior is algorithmic, 
       and the algorithm is determined by a, b, m.  The winning strategy for M can be 
       found by a brute-force search (the game is essentially finite,  and we assume that 
       such  a strategy  exists),  so  we  may  assume  that  the  winning  strategy  is  simple. 
       So  the strings  declared by  M  as simple  are  indeed simple,  i.e.,  they  have  small 
       (conditional)  complexity.  Indeed, to specify such a string, one may specify its or­
       dinal  number in the list  of strings  declared simple  (for  a given  condition),  using 
       loge + clog(a + b)  bits,  and  also  specify  a,  6,  m  (additional  0 (log(a + b))  bits). 
       So we get the statement of Theorem 229.  (A technical comment:  The factor c in 
       c(a + b)c is needed for small values of c and corresponds to the 0 (l)-term in the 
       complexity bounds that should be added to О (logn) for the case when n — 1 and 
       logn — 0.)
          In the other direction, assume that the statement of Theorem 229 is true with 
       some constant  d  in  О (logn).  We want to prove that  for sufficiently large c the 
       combinatorial statement is true.  Assume it is not the case and for every c there 
       exist a, b, m for which A can win the game.  This strategy (together with a, b, c) can 
       be found by a search.  So if this strategy declares some A as simple with respect 
       to  B,  then indeed the conditional complexity C(A\B)  is small—it is bounded by 
       m + 0(C(c)).  We get a contradiction if A plays this strategy against the following 
       blind strategy for M:  Declare X as simple for A if C(X\ A) < clog(a + b) + loge, 
       and declare A simple for В, X if C(A \ В, X) < clog(a + b) + loge.
          Playing this strategy, M does not violate the quantitative restrictions (on the 
       number of simple strings).  To get  the desired contradiction,  it  remains  to  show 
       that  M wins the game.  Let A, В be strings of lengths a,  b,  and assume that A 
       is  declared simple for В  (by A).  Theorem 229 says that there exists a string X' 
       of length C(A\B) + c'log(a + b) for which the statement of that theorem is true. 
       Since C(A IB) is bounded by m + 0(C'(c)), the string X' is only slightly longer that 
       m.  Let X be the first m bits of X'.  The complexities C{X'\A) and C(A\B,X') 
       are  small  as  Theorem  229  says,  and  the  number  of discarded  bits  is  also  small. 
       Therefore the  complexities  C(X\A)  and  C(A | В, X)  are  also  small,  and for the 
       right choice of c they are less than clog(a + b) + loge, so M wins.
          Let is provide the necessary bounds.  The conditional complexity C(X \ A) is at 
       most
                         d log(a + b) + O(logm)
       (the complexity of X'  given A plus the length of the prefix-free encoding of m). 
       The conditional complexity C(A\B,X) does not exceed the sum
                         d log(a + b) + 0(C(c))
       (the complexity of A given В, X plus the length of the discarded suffix of X').  Now 
       we see that one can choose c of the form 2г in such a way that both sums do not
