        390      12.  MULTISOURCE ALGORITHMIC  INFORMATION  THEORY
                               A
                               P 
                               A
                   Figure 49.  Two channels of bounded capacity
                      12.12.  Minimal sufficient statistics
          Tn this section we consider another request where cut-flow conditions are not 
        sufficient  (Figure 49).  Here the output string is again (like in Muchnik’s theorem) 
        one of the input strings, but now both capacities are limited.  In addition, we allow 
        use of the full information about В when encoding A.
           This problem is related to the notion of minimal sufficient statistics in probabil­
        ity theory.  Let us explain this connection (though it is not important for the proofs, 
        so  one may skip these explanations).  Consider a pair of two random variables 9 
        and X with some joint distribution.  The variable 9 is considered a parameter, and 
        for each value of 9 we consider the conditional distribution of X.  For example, we 
        may first choose 9 uniformly distributed in [0,1], and then choose an n-bit string X 
        according to the Bernoulli distribution on n-bit strings with parameter 9.  In this 
        way we get a joint distribution of 9 and X.
           Assume that we observe X in a pair (9, X) generated according to this distri­
        bution, and want to guess 9.  (As usual, we assume that some a priori distribution 
        on the space [0,1] of parameters is given.)  Not all information in X is really useful 
        for that—it is enough to know how many ones are among the outcomes, and it does 
        not matter what their positions are.  More formally,  the random variable N(X), 
        the number of ones in X , extracts all information about 9 that is in X,
                           I(N(X):9) = I(X:9).
        For an arbitrary function N the left-hand side does not exceed the right-hand side; 
        the functions N that transform this inequality into an equality are called sufficient 
        statistics.  The same condition can be formulated in a different way:  9 and X and 
        independent given N(X).  One more reformulation is H(9\N(X)) = H(9\X).
           By definition, the random variable X itself is a sufficient statistic; our example 
        shows that it may contain a lot of irrelevant information.  A sufficient statistic is 
        called a minimal sufficient statistic if it is a function of all other sufficient statistics. 
        For the random variables with finitely many values, a minimal sufficient statistic 
        always exists (and is unique up to permutations):  one should identify those values 
        of X that lead to the same conditional distributions on 9.  The minimal sufficient 
        statistic has minimal entropy among all sufficient statistics.
           330  Assume that all values of 9 have positive probabilities.  Prove that the 
        notion of sufficient  statistics depends only on the values of conditional probabil­
        ities  P[X  =  X  I  9  =  t]  for  all  pairs  x,t  (so  the  distribution  for  9  itself is  not 
        important).
