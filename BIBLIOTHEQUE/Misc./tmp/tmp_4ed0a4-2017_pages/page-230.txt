                                    7.2.  PAIRS  AND  CONDITIONAL  ENTROPY                      217
             We obtain the recursive algorithm that finds the optimal prefix code as follows:
                 • combine the two rarest letters into one (adding their probabilities);
                 • find the optimal prefix code for the resulting probabilities (a recursive call);
                 •  replace  the  codeword  x  for  a  “virtual”  combined  letter  by  two  codewords 
             xO and .xl  which are one bit  longer  (note that  this  replacement keeps the prefix 
             property).
                 The optimal code constructed by this algorithm is called the Huffman code for 
             a given distribution pi,... ,pn-
                 7.1.4.  Kraft—McMillan inequality.  So far we have studied prefix codes.  It 
             turns out that they are as efficient as general uniquely decodable codes, as shown 
             in the following theorem.
                 Theorem 140 (McMillan inequality).  Assum,e that ci,... ,ck  are codewords of 
             a uniquely decodable code,  and let щ = /(c,;)  be their lengths.  Then
                                                 E2'"'<L
                                                  i
                 Therefore (recall the lemma above) for any uniquely decodable code there is a 
             prefix code with the same lengths of codewords.
                 PROOF.  Let us use letters и and v instead of digits 0 and 1 when constructing 
             codewords (e.g., the codes 0, 01, and 11 are now written as u, uv, vv).  Now take a 
             formal sum (c\ + ■ ■ ■ + ck) of all codewords and consider its Nth power (for some N 
             that we choose later).  Then we open the parentheses without changing the order 
             of factors и and v (as if и and v were two non-commuting variables).  For example, 
             the code above gives (for N = 2) the expression
             (и + uv + vv)(u + uv + vv)
                                 = uu + uuv + uvv + uvu + uvuv + uvvv + vvu + vvuv + vvvv.
             Each term in the right-hand side is a concatenation of some codewords.  The unique 
             decoding property guarantees that all the terms are different.  Now we let и = v = 
             1/2.  The left-hand side  (ci + ■ ■ • + ck)N  becomes  (2~711  + • • • + 2~nk)N.  For the 
             right-hand side we have an upper bound:  if it consisted of all strings of length t, 
             it would contain 2* terms equal to 2~b  (each), so the sum would be equal to 1  (for 
             each length t).  Therefore, the right-hand side does not exceed the maximal length 
             of strings in the right-hand side, which equals N тах.(щ).
                 if £ 2 - 'n'  >  1, we immediately get a contradiction, since for large enough N the 
             left-hand side grows exponentially in N while the right-hand side is linear in N.   □
                 This proof looks like an extremely artificial trick (though a nice one).  A more 
             natural proof (or, better to say, a more natural version of the same proof) is given 
             below; see p. 222.
                                   7.2.  Pairs and conditional entropy
                 7.2.1.  Pairs of random variables.  Dealing with Shannon entropies, we use 
             the terminology which is standard for probability theory.  Let £ be a random variable 
             which takes finitely many values £i,... ,£& with probabilities pi,... ,Pk■  Then the 
             Shannon entropy of a random variable £ is defined as
                                   H(£) =Pi(-logPi) + •• • +pk(-logpk).
