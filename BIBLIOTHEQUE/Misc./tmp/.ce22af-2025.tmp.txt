                sceneinastreamingsetup,wefocusonthechallengingtask                   perior robustness compared to other streaming perception
                of streaming 4D panoptic segmentation. Given a stream-               methods shown in Fig. 1, particularly under high-FPS
                ing sequence of point clouds, the goal is to predict panoptic        scenarios. These results highlight the effectiveness and
                segmentation on each frame within a strict time budget, en-          value of our method for 4D streaming segmentation.
                abling real-time scene perception. This task is particularly
                difﬁcult due to the computational overhead and ﬁne-grained         2. Related Work
                perception requirements. Most existing 4D methods [2, 8–           2.1. Streaming Perception
                11, 21, 25, 29, 34, 39, 40, 43, 45, 47, 48] fail to achieve
                real-time perception and the ﬂuctuations in computing re-             In the streaming perception, the inherent challenge lies
                sourcesintroduceadditionallatencyinconsistencies, further          in predicting results in the future state, in order to mini-
                complicating streaming 4D panoptic segmentation task.              mize the temporal gap between input and output timestep.
                   To address the challenges of real-time dense percep-            Most previous studies concentrate on developing forecast-
                tion in streaming 4D panoptic segmentation, we intro-              ing modules speciﬁcally tailored for this streaming setting.
                duce 4DSegStreamer, a general system designed to en-               Stream [26] ﬁrstly introduces the streaming setting and uti-
                able existing segmentation methods to operate in real time.        lizes the Kalman Filters to predict future bounding boxes.
                4DSegStreamer utilizes a novel dual-thread system with a           StreamYOLO [44] designs a dual-ﬂow perception module,
                predictive thread maintaining geometry and motion mem-             which incorporates dynamic and static ﬂows from previous
                ories in the scene and an inference thread facilitating            and current features to predict the future state. DAMO-
                rapid inference at each time step.     The key idea behind         StreamNet [17] and LongShortNet [23] leverages spatial-
                4DSegStreamer involves dividing the streaming input into           temporal information by extracting long-term temporal mo-
                key frames and non-key frames based on the model’s la-             tion from previous multi-frames and short-term spatial in-
                tency. In the predictive thread, we meticulously compute           formation from the current frame for future prediction. Dif-
                geometric and motion features at key frames and utilize            ferent from previous researches which only forecast one
                these features to continuously update the memories, en-            frameaheadandthusthepredictionoutputislimitedwithin
                abling long-term spatial-temporal perception. To support           a single frame, DaDe [20] and MTD [19] considering pre-
                efﬁcient memory queries, the memories are also utilized to         vious prediction time, adaptively choose the correspond-
                predict future dynamics, guiding how a future frame can ef-        ing future features. Transtreaming [46] designs an adaptive
                fectively adjust for potential movement when querying the          delay-awaretransformertoselectthepredictionfrommulti-
                geometry memory. In the inference thread, each incoming            frames future that best matches future time.
                frame is ﬁrst positionally aligned with the current geometry          Several studies have explored streaming perception in
                memorybycompensatingfortheforecastedmotion. Itthen                 LiDAR-based 3D detection [1, 6, 12, 16, 24, 37]. Lidar
                swiftly queries the hash table-style memory to obtain per-         Stream [16] segments full-scan LiDAR points into multi-
                point labels. The two threads together allow both fast and         ple slices, processing each slice at a higher frequency com-
                high-quality streaming 4D panoptic segmentation.                   pared to using the full-scan input. Although ASAP [38]
                   Ourcontributions to this work can be summarized as:             introduces a benchmark for online streaming 3D detection,
                • We introduce a new task for streaming 4D panoptic seg-           it relies on camera-based methods using images as input.
                  mentation, advancing real-time, ﬁne-grained perception           2.2. 4D Point Cloud Sequence Perception
                  for autonomous systems in dynamic environments.
                • We propose a novel dual-thread system that includes a               4D point cloud sequence perception methods integrate
                  predictive thread and an inference thread, which is gen-         temporal consistency and spatial aggregation through ad-
                  eral and applicable to existing segmentation methods to          vanced memory mechanisms. These methods are generally
                  achieve real-time performance.      The predictive thread        categorized into voxel-based [8, 25, 43, 45] and point-based
                  continuously updates memories by leveraging historical           [2, 9–11, 21, 21, 28, 29, 31, 34, 39, 40, 47, 48] approaches.
                  motion and geometric features to forecast future dynam-             For the point-based methods, SpSequenceNet [34] ag-
                  ics. The inference thread retrieves relevant features from       gregates 4D information on both a global and local scale
                  the memory through geometric alignment with the fore-            through K-nearest neighbours. NSM4D [10] introduces a
                  casted motion, using ego-pose transformation and inverse         historical memory mechanism that maintains both geomet-
                  ﬂowiteration.                                                    ric and motion features derived from motion ﬂow infor-
                • Through extensive evaluations in outdoor datasets Se-            mation, thereby enhancing perception capabilities. Eq-4D-
                  manticKITTIandnuScenes,aswellastheindoorHOI4D                    StOP [48] introduces a rotation-equivariant neural network
                  dataset, our system signiﬁcantly outperforms existing            that leverages the rotational symmetry of driving scenarios
                  SOTA streaming perception and 4D panoptic segmenta-              onthe ground plane.
                  tion methods. Moreover, our approach demonstrates su-               For the voxel-based methods, SVQNet [8] develops a
                                                                               2
               of this forward ﬂow. The process continues until the dis-         wedividethesPQintofourcomponents: sPQd fordynamic
               tance between current query position p′ and the point p           objects, sPQ for static objects, sPQ   for thing classes, and
                                                                                             s                       th
               closely approximates the inverse of the forward ﬂow. The          sPQst for stuff classes. In the streaming setting, evaluation
               pseudo-code for this process is as follows:                       of each frame must occur at every input timestamp, accord-
                                                                                 ing to the dataset’s frame rate. If the computation for the
               Algorithm 1 Iterative Inverse Forward Flow Method                 current frame is not completed in time, we use the features
               Require: forecast forward ﬂow query Q, stop threshold ǫ,          from the last completed frame to query the results and per-
                    maximumiterations N                                          form the evaluation.
                                           max                                   Implementation details. We choose P3Former [42] and
                 1: for each point p in the non-key frame do
                 2:    Initialize current query position p′ ← p                  Mask4Former [45] as our backbone model, which is origi-
                 3:    Initialize iteration counter n ← 0                        nallyaSOTAmethodfor3Dand4Dpanopticsegmentation.
                 4:    Inverse(f) ← −f                                           Byincorporatingtheegoposeandﬂowalignmentstrategies
                                 ′             ′                                 we proposed, along with memory construction, they can
                 5:    while k(p −p)+Q(p )k ≥ ǫandn < Nmax do
                 6:        Query local forecast forward ﬂow f ← Q(p′)            also achieve good performance in 4D streaming panoptic
                 7:        Update track position: p′ ← p + Inverse(f)            segmentation. We ﬁrst train the model on each dataset, then
                 8:        Increment iteration counter: n ← n + 1                freeze it for feature extraction. The remaining components,
                 9:    endwhile                                                  including ego-pose forecasting, forward ﬂow forecasting,
                10: end for                                                      and history memory aggregation, are trained subsequently.
                                                                                 For the inverse ﬂow iteration, the maximum iterations pa-
                                                                                 tience is set to 10. All models are trained on 4 NVIDIA
               5. Experiments                                                    GTX3090GPUsandevaluated on a single NVIDIA GTX
                                                                                 3090 GPU.
                  We present the experimental setup and benchmark re-            5.2. Streaming 4D Panoptic Segmentation in Out-
               sults on two widely used outdoor LiDAR-based panoptic                   doordatasets
               segmentationdatasets, SemanticKITTI[4]andnuScenes[5],
               as well as the indoor dataset HOI4D[30].                          SemanticKITTI [4]. Tab. 1 and 2 compare streaming 4D
               5.1. Settings                                                     panoptic segmentation on the SemanticKITTI validation
                                                                                 split in the unknown and known pose settings. We compare
               SemanticKITTI [4].        SemanticKITTI is a large-scale          our method with StreamYOLO [44], LongShortNet [23],
               dataset for LiDAR-based panoptic segmentation, contain-           DAMO-StreamNet [17], Mask4Former [45], Eq-4D-StOP
               ing 23,201 outdoor scene frames at 10 fps. Unlike tradi-          [48] and PTv3 [41]. Originally designed for 2D streaming
               tional 4D panoptic segmentation, streaming 4D panoptic            object detection via temporal feature fusion, the ﬁrst three
               segmentation also involves distinguishing between moving          modelsareadaptedto4Dstreamingbyreplacingtheirback-
               and static objects, since the ability to perceive moving ob-      bones with P3Former [42]. Mask4Former and Eq-4D-StOP
               jects is signiﬁcant in streaming perception. This adds 6          are designed for 4D panoptic segmentation but are not op-
               additional classes for moving objects (e.g., ”moving car”)        timized for streaming. PTv3 is a state-of-the-art method
               to the standard 19 semantic classes. In total, there are 25       designed for 3D perception. We adapt it to 4D panoptic
               classes, including 14 thing classes and 11 stuff classes.         segmentation with ﬂow propagation according to [2].
               nuScenes[5]. nuScenes is a publicly available autonomous             Frombothtables,weobservethat2Dstreamingmethods
               driving dataset with 1,000 scenes captured at 2 fps. We ex-       performpoorlyduetotheirrelianceonreal-timebackbones,
               tend the per-point semantic labels to distinguish between         which are difﬁcult to achieve in such a high-granularity
               moving and non-moving objects using ground truth 3D               task.  Similarly, 4D panoptic segmentation methods also
               bounding box attributes. This extension includes 8 mov-           suffer signiﬁcant performance degradation due to computa-
               ing object classes and 16 static object classes, totaling 18      tional latency. PTv3 performs better than 4D methods due
               thing classes and 6 stuff classes.                                to its high efﬁciency, but it still suffers from performance
               HOI4D [
                         30].  HOI4D is a large-scale egocentric dataset         drop. In contrast, our method outperforms all baseline mod-
               focused on indoor human-object interactions. It contains          els by a large margin in the streaming setting. Notably,
               3,865 point cloud sequences, with 2,971 for training and          in the unknown pose setting, our method achieves signiﬁ-
               892 for testing. Each sequence has 300 frames captured at         cantimprovementsof7.7%and15.2%insLSTQoverPTv3
               15fps.                                                            [41]when integrated with P3Former and Mask4Former re-
               Evaluation metrics. We use PQ and LSTQ in streaming               spectively, demonstrating the effectiveness of our alignment
               setting (denoted as sPQ and sLSTQ) as our main metrics to         strategies across both dynamic and static classes. When
               evaluate panoptic segmentation performance. Furthermore,          combined with Mask4Former, our method outperforms its
                                                                             6
                Table 1. SemanticKITTI validation set result in unknown pose streaming setting. The best is highlighted in bold. sX indicates the metric
                Xinthestreaming setting. PQ and PQ refer to the evaluation for dynamic and static points, respectively. PQ   evaluates the thing class
                                             d        s                                                                   th
                and PQst evaluates the stuff class.
                  Method                               sLSTQ      S          S        sPQ      sRQ      sSQ      sPQ      sPQ      sPQ       sPQ
                                                                   assoc      cls                                    d        s        th        st
                  StreamYOLO[44]                        0.415     0.321     0.536    0.373    0.478    0.664    0.429     0.371    0.388     0.364
                  LongShortNet [23]                     0.430     0.341     0.541    0.392    0.472    0.673    0.452     0.391    0.400     0.386
                  DAMO-StreamNet[17]                    0.432     0.341     0.546    0.392    0.472    0.674    0.459     0.391    0.400     0.388
                  Mask4Former[45]                       0.515     0.464     0.572    0.485    0.594    0.691    0.571     0.413    0.538     0.422
                  Eq-4D-StOP[48]                        0.504     0.452     0.563    0.477    0.578    0.691    0.543     0.412    0.529     0.423
                  PTv3[41]                              0.536     0.492     0.586    0.567    0.612    0.704    0.638     0.464    0.575     0.459
                  4DSegStreamer (P3Former)              0.613     0.627     0.599    0.602    0.679    0.723    0.711     0.479    0.625     0.481
                  4DSegStreamer (Mask4Former)           0.688     0.706     0.621    0.634    0.701    0.752    0.744     0.486    0.660     0.497
                Table 2. SemanticKITTI validation set result in known pose streaming setting. The best is highlighted in bold. sX indicates the metric X in
                the streaming setting. PQd and PQs refer to the evaluation for dynamic and static points, respectively. PQth evaluates the thing class and
                PQ evaluates the stuff class.
                   st
                  Method                               sLSTQ      S          S        sPQ      sRQ      sSQ      sPQ      sPQ      sPQ       sPQ
                                                                   assoc      cls                                    d        s        th        st
                  StreamYOLO[44]                        0.439     0.356     0.541    0.384    0.468    0.715    0.432     0.383    0.392     0.382
                  LongShortNet [23]                     0.446     0.360     0.553    0.412    0.489    0.719    0.459     0.410    0.413     0.399
                  DAMO-StreamNet[17]                    0.446     0.362     0.551    0.425    0.489    0.724    0.460     0.412    0.414     0.401
                  Mask4Former[45]                       0.564     0.539     0.592    0.520    0.613    0.734    0.623     0.460    0.592     0.467
                  Eq-4D-StOP[48]                        0.557     0.530     0.585    0.520    0.619    0.732    0.625     0.459    0.594     0.465
                  4DSegStreamer (P3Former)              0.655     0.703     0.610    0.687    0.774    0.816    0.782     0.560    0.704     0.531
                  4DSegStreamer (Mask4Former)           0.701     0.722     0.648    0.704    0.811    0.838    0.803     0.579    0.741     0.552
                Table 3. nuScenes validation set result in unknown pose streaming    Table 4. nuScenes validation set result in known pose streaming
                setting. The best is highlighted in bold.                            setting. The best is highlighted in bold.
                  Method                      sLSTQ       sPQ     sPQ       sPQ        Method                      sLSTQ       sPQ       sPQ      sPQ
                                                                       d        s                                                            d        s
                  StreamYOLO[44]               0.596     0.581    0.569    0.591       StreamYOLO[44]               0.613      0.593    0.583     0.613
                  LongShortNet [23]            0.610     0.603    0.579    0.607       LongShortNet [23]            0.628     0.6116    0.599     0.621
                  DAMO-StreamNet[17]           0.623     0.607    0.601    0.612       DAMO-StreamNet[17]           0.633      0.625    0.607     0.639
                  Mask4Former[45]              0.648     0.636    0.634    0.641       Mask4Former[45]              0.681      0.665    0.655     0.683
                  Eq-4D-StOP[48]               0.650     0.642    0.633    0.658       Eq-4D-StOP[48]               0.695      0.673    0.654     0.693
                  PTv3[41]                     0.662     0.659    0.627    0.670       4DSegStreamer (P3)           0.747      0.723    0.711     0.733
                  4DSegStreamer (P3)           0.693     0.683    0.675    0.690       4DSegStreamer (M4F)          0.765      0.751    0.734     0.786
                  4DSegStreamer (M4F)          0.721     0.733    0.701    0.699
                                                                                     approaches in both known and unknown pose settings. Ad-
                combination with P3Former, as Mask4Former is speciﬁ-                 ditionally, all models perform better in the known pose set-
                cally designed for 4D panoptic segmentation.                         ting, as pose estimation in the unknown pose setting takes
                nuScenes [5]. We also compare the performance of 4D                  moretime, further degrading performance.
                streaming panoptic segmentation on the nuScenes valida-              5.3.Streaming4DPanopticSegmentationinIndoor
                tion split [5].  Compared to SemanticKITTI[4], it has a                     dataset
                slower frame rate, which allows many baseline methods
                to achieve real-time computation. However, in a stream-              HOI4D [30]. We also evaluate our model in indoor sce-
                ing setting, even real-time methods experience at least a            narios. We compare our approach with StreamYOLO [44],
                one-frame delay, leading to performance degradation. As              LongShortNet [23], DAMO-StreamNet [17], NSM4D [10]
                showninTab.3and4,ourmethodoutperformsallbaseline                     and PTv3 [41]. As shown in Tab. 5, our method outper-
                                                                                 7
                 Table 5. HOI4D test set result in unknown pose streaming setting.      Table 8. Ablation study in known pose streaming setting. Pose
                 Thebest is highlighted in bold.                                        is given and Flow is multi-head forecasting. Mem represents the
                                                                                        memorymodule. Flow denotes multi-frame future ﬂow forecast-
                  Method                        sLSTQ       sPQ      sPQd     sPQs      ing.
                  StreamYOLO[44]                 0.373     0.336     0.362    0.324       Method                         sLSTQ      sLSTQ        sLSTQ
                  LongShortNet [23]              0.377     0.335     0.354    0.323                                                         d           s
                  DAMO-StreamNet[17]             0.375     0.335     0.351    0.324       P3+Mem+GTpose                   0.563      0.534        0.592
                  NSM4D[10]                      0.314     0.305     0.315    0.303       P3+Mem+GTpose+Flow              0.655      0.698        0.601
                  PTv3[41]                       0.445     0.417     0.397    0.445
                  4DSegStreamer (P3)             0.483     0.455     0.431    0.490       Table 9. Ablation study of different ﬂow forecasting methods.
                  4DSegStreamer (M4F)            0.511     0.482     0.457    0.533
                                                                                           Method                    sLSTQ      sLSTQ        sLSTQ
                                                                                                                                        d            s
                 Table   6.      General   evaluation  of   different  backbones.          Backwardﬂow                0.565       0.637       0.483
                 w/o streamer is vanilla backbone.        w streamer is 3D or              Forward ﬂow                0.589       0.667       0.497
                 4Dbackbonewithour4DSegStreamer.                                           Inverse forward ﬂow        0.586       0.662       0.502
                  Method                  sLSTQ                     sLSTQ                  Inverse brute search       0.591       0.669       0.501
                                                  w/o streamer              wstreamer      Inverse ﬂow iteration      0.613       0.682       0.516
                  Mask4Former[45]                 0.515                   0.688
                  Eq-4D-StOP[48]                  0.504                   0.674
                  P3former [42]                   0.304                   0.613         and sLSTQ . Building on this, incorporating ﬂow align-
                                                                                                     s
                                                                                        ment further reﬁnes the handling of moving objects, sig-
                 Table 7. Ablation study in unknown pose streaming setting. P3          niﬁcantly boosting the model’s performance on sLSTQd.
                 indicates the P3former backbone. Mem represents the memory             Weevaluate our method under both unknown-pose (Tab. 7)
                 module. Pose and Flow denote multi-frames future pose and              and known-pose settings (Tab. 8), where the latter provides
                 ﬂow forecasting, respectively.  M Flow indicates the moving            ground-truth ego poses. Results demonstrate that our mem-
                 masktoassign non-zero ﬂow only to moving objects.                      ory module, pose alignment, and dynamic object alignment
                  Method                         sLSTQ      sLSTQ        sLSTQ          continuously enhance streaming performance. Moreover,
                                                                    d            s      applyinganon-movingobjectmaskbringsadditionalgains.
                  P3[42]                          0.304       0.265       0.357         Flow Forecasting Strategies. We compare different ﬂow
                  P3+Mem                          0.349       0.292       0.408         forecasting strategies in Tab. 9.       The ”Inverse Forward
                  P3+Mem+Pose                     0.497       0.488       0.501         Flow” represents a single iteration of the Inverse Flow Iter-
                  P3+Mem+Pose+Flow                0.591       0.667       0.514         ation algorithm, while the ”Inverse Brute Search” algorithm
                  P3+Mem+Pose+MFlow               0.613       0.682       0.516         directly searches for the forward ﬂow within a restricted re-
                                                                                        gionthatpointstothetargetposition. Asshowninthetable,
                                                                                        forward ﬂow forecasting does not achieve the best perfor-
                 forms all other approaches, surpassing the runner-up by                mance due to the high time consumption associated with
                 6.6%intermsofsLSTQ.Thisdemonstratesthatourmethod                       repeated kd-tree construction. Additionally, backward ﬂow
                 exhibits strong generalization ability, performing well not            forecasting performs poorly, as it is challenging to predict
                 only in outdoor scenarios but also in indoor scenes.                   thebackwardﬂowwithoutknowledgeofthefutureposition.
                                                                                        In contrast, our proposed Inverse Flow Iteration algorithm
                 5.4. Ablations for System                                              shows superior performance in terms of sLSTQ.
                    In this section, we conduct several groups of ablation              6. Conclusion
                 studies on SemanticKITTI [4] validation set to demonstrate
                 the effectiveness of 4DSegStreamer.                                       In this work, we propose 4DSegStreamer, an efﬁcient
                 Generalto3Dand4Dbackbone. Tab6demonstratesthat                         4Dstreamingpanopticsegmentationmethodthatoptimizes
                 integrating our plug-and-play 4DSegStreamer consistently               accuracy-latency trade-offs. We develop a dual-thread sys-
                 boosts the perfomance across various SOTA 3D and 4D                    tem to synchronize current and future point clouds within
                 backbones, with signiﬁcnt improvements observed. This                  temporal constraints, complemented by an ego-pose fore-
                 highlightsthegeneralityandeffectivenessofourframework                  caster and inverse forward ﬂow iteration for motion align-
                 in enabling real-time capability.                                      ment. Evaluatedacrossdiverseindoorandoutdoorpanoptic
                 Effects of Components. Pose alignment mitigates the ego-               segmentationdatasets, our method demonstrates robust per-
                 pose motion, resulting in improvements to both sLSTQd                  formance in streaming scenarios.
                                                                                    8
                    References                                                                           [13] Weizhen Ge, Xin Wang, Zhaoyong Mao, Jing Ren, and
                     [1] Mazen Abdelfattah, Kaiwen Yuan, Z Jane Wang, and Rabab                                 Junge Shen.        Streamtrack:      real-time meta-detector for
                          Ward. Multi-modal streaming 3d object detection. IEEE                                 streaming perception in full-speed domain driving scenarios.
                          Robotics and Automation Letters, 2023. 1, 2                                           Applied Intelligence, pages 1–17, 2024. 1
                     [2] Mehmet Aygun, Aljosa Osep, Mark Weber, Maxim Maxi-                              [14] Anurag Ghosh, Vaibhav Balloli, Akshay Nambi, Aditya
                                                                                                ´               Singh, and Tanuja Ganu. Chanakya: Learning runtime deci-
                          mov, Cyrill Stachniss, Jens Behley, and Laura Leal-Taixe.                             sions for adaptive real-time perception. Advances in Neural
                          4d panoptic lidar segmentation.             In Proceedings of the                     Information Processing Systems, 36, 2024. 1
                          IEEE/CVF Conference on Computer Vision and Pattern                             [15] AlexGravesandAlexGraves. Longshort-termmemory. Su-
                          Recognition, pages 5527–5537, 2021. 2, 6                                              pervised sequence labelling with recurrent neural networks,
                     [3] Nicolas Ballas, Li Yao, Chris Pal, and Aaron Courville.                                pages 37–45, 2012. 5
                          Delving deeper into convolutional networks for learning                        [16] Wei Han, Zhengdong Zhang, Benjamin Caine, Brandon
                          video representations.        arXiv preprint arXiv:1511.06432,                        Yang, Christoph Sprunk, Ouais Alsharif, Jiquan Ngiam, Vi-
                          2015. 4                                                                               jay Vasudevan, Jonathon Shlens, and Zhifeng Chen. Stream-
                     [4] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-                                 ing object detection for 3-d point clouds. In European Con-
                          zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-                              ference on Computer Vision, pages 423–441. Springer, 2020.
                          mantickitti: A dataset for semantic scene understanding of                            1, 2
                          lidar sequences.       In Proceedings of the IEEE/CVF inter-                   [17] Jun-YanHe,Zhi-QiCheng,ChenyangLi,WangmengXiang,
                          national conference on computer vision, pages 9297–9307,                              Binghui Chen, Bin Luo, Yifeng Geng, and Xuansong Xie.
                          2019. 6, 7, 8                                                                         Damo-streamnet: Optimizing streaming perception in au-
                     [5] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,                               tonomous driving. arXiv preprint arXiv:2303.17144, 2023.
                          Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-                              1, 2, 6, 7, 8
                          ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-                          [18] Xiang Huang, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li,
                          modal dataset for autonomous driving. In Proceedings of                               WangmengXiang,BaiguiSun, and Xiao Wu. Dyronet: Dy-
                          the IEEE/CVF conference on computer vision and pattern                                namicroutingandlow-rankadaptersforautonomousdriving
                          recognition, pages 11621–11631, 2020. 6, 7                                            streaming perception. CoRR, 2024.
                     [6] Qi Chen, Sourabh Vora, and Oscar Beijbom. Polarstream:                          [19] Yihui Huang and Ningjiang Chen. Mtd: Multi-timestep de-
                          Streaming object detection and segmentation with polar pil-                           tector for delayed streaming perception. In Chinese Confer-
                          lars.  Advances in Neural Information Processing Systems,                             ence on Pattern Recognition and Computer Vision (PRCV),
                          34:26871–26883, 2021. 1, 2                                                            pages 337–349. Springer, 2023. 2, 1
                     [7] Xieyuanli Chen, Andres Milioto, Emanuele Palazzolo,                             [20] Wonwoo Jo, Kyungshin Lee, Jaewon Baik, Sangsun Lee,
                          Philippe Giguere, Jens Behley, and Cyrill Stachniss.                                  Dongho Choi, and Hyunkyoo Park.                      Dade:      delay-
                          Suma++: Efﬁcient lidar-based semantic slam.                   In 2019                 adaptive detector for streaming perception. arXiv preprint
                          IEEE/RSJ International Conference on Intelligent Robots                               arXiv:2212.11558, 2022. 1, 2
                          and Systems (IROS), pages 4530–4537. IEEE, 2019. 5                             [21] Lars Kreuzberg, Idil Esen Zulﬁkar, Sabarinath Mahadevan,
                     [8] XuechaoChen,ShuangjieXu,XiaoyiZou,TongyiCao,Dit-                                       Francis Engelmann, and Bastian Leibe. 4d-stop: Panoptic
                          Yan Yeung, and Lu Fang. Svqnet: Sparse voxel-adjacent                                 segmentation of 4d lidar using spatio-temporal object pro-
                          querynetworkfor4dspatio-temporallidarsemanticsegmen-                                  posal generation and aggregation. In European Conference
                          tation. In Proceedings of the IEEE/CVF International Con-                             onComputerVision, pages 537–553. Springer, 2022. 2
                          ference on Computer Vision, pages 8569–8578, 2023. 2                           [22] Bowen Li, Ziyuan Huang, Junjie Ye, Yiming Li, Sebastian
                     [9] Ayush Dewan and Wolfram Burgard.                   Deeptemporalseg:                    Scherer, Hang Zhao, and Changhong Fu. Pvt++: a sim-
                          Temporally consistent semantic segmentation of 3d lidar                               ple end-to-end latency-aware visual tracking framework. In
                          scans. In 2020 IEEE International Conference on Robotics                              Proceedings of the IEEE/CVF International Conference on
                          and Automation (ICRA), pages 2624–2630. IEEE, 2020. 2                                 ComputerVision, pages 10006–10016, 2023. 1
                    [10] Yuhao Dong, Zhuoyang Zhang, Yunze Liu, and Li Yi.                               [23] Chenyang Li, Zhi-Qi Cheng, Jun-Yan He, Pengyu Li, Bin
                          Nsm4d: Neural scene model based online 4d point cloud                                 Luo, Hanyuan Chen, Yifeng Geng, Jin-Peng Lan, and Xuan-
                          sequence understanding. arXiv preprint arXiv:2310.08326,                              song Xie. Longshortnet: Exploring temporal and semantic
                          2023. 2, 7, 8                                                                         features fusion in streaming perception. In ICASSP 2023-
                    [11] HeheFan,YiYang,andMohanKankanhalli. Point4dtrans-                                      2023 IEEE International Conference on Acoustics, Speech
                          formernetworksforspatio-temporalmodelinginpointcloud                                  andSignal Processing (ICASSP), pages 1–5. IEEE, 2023. 1,
                          videos. In Proceedings of the IEEE/CVF conference on com-                             2, 6, 7, 8
                          puter vision and pattern recognition, pages 14204–14213,                       [24] Dianze Li, Jianing Li, and Yonghong Tian.                   Sodformer:
                          2021. 2                                                                               Streamingobjectdetectionwithtransformerusingeventsand
                    [12] Davi Frossard, Shun Da Suo, Sergio Casas, James Tu, and                                frames. IEEETransactionsonPatternAnalysisandMachine
                          Raquel Urtasun. Strobe: Streaming object detection from li-                           Intelligence, 2023. 1, 2
                          dar packets. In Conference on Robot Learning, pages 1174–                      [25] Enxu Li, Sergio Casas, and Raquel Urtasun. Memoryseg:
                          1183. PMLR, 2021. 1, 2                                                                Online lidar semantic segmentation with a latent memory. In
                                                                                                     9
                    Proceedings of the IEEE/CVF International Conference on           segmentationwithpolarpillars,2023. USPatent11,798,289.
                    ComputerVision, pages 745–754, 2023. 2, 3, 4                      1, 2
               [26] Mengtian Li, Yu-Xiong Wang, and Deva Ramanan.        To-     [38] Xiaofeng Wang, Zheng Zhu, Yunpeng Zhang, Guan Huang,
                    wards streaming perception.   In Computer Vision–ECCV             Yun Ye, Wenbo Xu, Ziwei Chen, and Xingang Wang. Are
                    2020: 16th European Conference, Glasgow, UK, August 23–           we ready for vision-centric driving streaming perception?
                    28, 2020, Proceedings, Part II 16, pages 473–488. Springer,       the asap benchmark. In Proceedings of the IEEE/CVF Con-
                    2020. 1, 2                                                        ference on Computer Vision and Pattern Recognition, pages
               [27] Xueqian Li, Jianqiao Zheng, Francesco Ferroni, Jhony Kae-         9600–9610, 2023. 2, 1
                    semodel Pontes, and Simon Lucey. Fast neural scene ﬂow.      [39] Hao Wen, Yunze Liu, Jingwei Huang, Bo Duan, and Li
                    In Proceedings of the IEEE/CVF International Conference           Yi. Point primitive transformer for long-term 4d point cloud
                    onComputerVision, pages 9878–9890, 2023. 5                        video understanding. In European Conference on Computer
               [28] Zhiheng Li, Yubo Cui, Jiexi Zhong, and Zheng Fang.                Vision, pages 19–35. Springer, 2022. 2
                    Streammos: Streaming moving object segmentation with         [40] Xiaopei Wu, Yuenan Hou, Xiaoshui Huang, Binbin Lin,
                    multi-viewperceptionanddual-spanmemory. arXivpreprint             Tong He, Xinge Zhu, Yuexin Ma, Boxi Wu, Haifeng Liu,
                    arXiv:2407.17905, 2024. 2                                         DengCai,etal. Taseg: Temporal aggregation network for li-
               [29] Jiahui Liu, Chirui Chang, Jianhui Liu, Xiaoyang Wu, Lan           dar semantic segmentation. In Proceedings of the IEEE/CVF
                    Ma, and Xiaojuan Qi. Mars3d: A plug-and-play motion-              Conference on Computer Vision and Pattern Recognition,
                    aware model for semantic segmentation on multi-scan 3d            pages 15311–15320, 2024. 2
                    point clouds. In Proceedings of the IEEE/CVF Conference      [41] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xi-
                    on Computer Vision and Pattern Recognition, pages 9372–           hui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang
                    9381, 2023. 2                                                     Zhao. Point transformer v3: Simpler faster stronger. In Pro-
               [30] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan,           ceedings of the IEEE/CVF Conference on Computer Vision
                    HaoShen,BoqiangLiang, Zhoujie Fu, He Wang, and Li Yi.             and Pattern Recognition, pages 4840–4851, 2024. 6, 7, 8
                    Hoi4d: A 4d egocentric dataset for category-level human-     [42] Zeqi Xiao, Wenwei Zhang, Tai Wang, Chen Change Loy,
                    object interaction. In Proceedings of the IEEE/CVF Con-           DahuaLin,andJiangmiaoPang. Position-guidedpointcloud
                    ference on Computer Vision and Pattern Recognition, pages         panoptic segmentation transformer. International Journal of
                    21013–21022, 2022. 6, 7                                           ComputerVision, pages 1–16, 2024. 6, 8
               [31] Rodrigo Marcuzzi, Lucas Nunes, Louis Wiesmann, Jens          [43] Xiuwei Xu, Chong Xia, Ziwei Wang, Linqing Zhao,
                    Behley, andCyrillStachniss. Mask-basedpanopticlidarseg-           Yueqi Duan, Jie Zhou, and Jiwen Lu.       Memory-based
                    mentation for autonomous driving. IEEE Robotics and Au-           adapters for online 3d scene perception.  arXiv preprint
                    tomation Letters, 8(2):1141–1148, 2023. 2                         arXiv:2403.06974, 2024. 2
               [32] Kangan Qian, Zhikun Ma, Yangfan He, Ziang Luo, Tianyu        [44] Jinrong Yang, Songtao Liu, Zeming Li, Xiaoping Li, and
                    Shi, Tianze Zhu, Jiayin Li, Jianhui Wang, Ziyu Chen, Xiao         Jian Sun.   Real-time object detection for streaming per-
                    He, et al. Fasionad: Fast and slow fusion thinking systems        ception.  In Proceedings of the IEEE/CVF conference on
                    for human-like autonomous driving with adaptive feedback.         computer vision and pattern recognition, pages 5385–5395,
                    arXiv preprint arXiv:2411.18013, 2024. 3                          2022. 1, 2, 6, 7, 8
               [33] Gur-Eyal Sela, Ionel Gog, Justin Wong, Kumar Krishna         [45] Kadir Yilmaz, Jonas Schult, Alexey Nekrasov, and Bastian
                    Agrawal, Xiangxi Mo, Sukrit Kalra, Peter Schafhalter, Eric        Leibe.  Mask4former: Mask transformer for 4d panoptic
                    Leong, Xin Wang, Bharathan Balaji, et al. Context-aware           segmentation. In 2024 IEEE International Conference on
                    streamingperceptionindynamicenvironments. InEuropean              Robotics and Automation (ICRA), pages 9418–9425. IEEE,
                    Conference on Computer Vision, pages 621–638. Springer,           2024. 2, 3, 6, 7, 8
                    2022. 1                                                      [46] Xiang Zhang, Yufei Cui, Chenchen Fu, Weiwei Wu, Zihao
               [34] Hanyu Shi, Guosheng Lin, Hao Wang, Tzu-Yi Hung, and               Wang, Yuyang Sun, and Xue Liu. Transtreaming: Adaptive
                    ZhenhuaWang. Spsequencenet: Semanticsegmentationnet-              delay-aware transformer for real-time streaming perception.
                    work on 4d point clouds. In Proceedings of the IEEE/CVF           arXiv preprint arXiv:2409.06584, 2024. 1, 2
                    conference on computer vision and pattern recognition,       [47] Yunsong Zhou, Hongzi Zhu, Chunqin Li, Tiankai Cui, Shan
                    pages 4574–4583, 2020. 2                                          Chang, and Minyi Guo. Tempnet: Online semantic segmen-
               [35] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang,         tation on large-scale point cloud series. In Proceedings of
                    Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and              the IEEE/CVF International Conference on Computer Vi-
                    Hang Zhao. Drivevlm: The convergence of autonomous                sion, pages 7118–7127, 2021. 2
                    driving and large vision-language models. arXiv preprint     [48] Minghan Zhu, Shizhong Han, Hong Cai, Shubhankar Borse,
                    arXiv:2402.12289, 2024. 3                                         MaaniGhaffari, and Fatih Porikli. 4d panoptic segmentation
               [36] Kyle Vedder, Neehar Peri, Nathaniel Chodosh, Ishan Khatri,        as invariant and equivariant ﬁeld prediction. In Proceedings
                    Eric Eaton, Dinesh Jayaraman, Yang Liu, Deva Ramanan,             of the IEEE/CVF International Conference on Computer Vi-
                    and James Hays. Zeroﬂow: Scalable scene ﬂow via distilla-         sion, pages 22488–22498, 2023. 2, 6, 7, 8
                    tion. arXiv preprint arXiv:2305.10424, 2023. 5
               [37] Sourabh Vora and Qi Chen. Streaming object detection and
                                                                             10
