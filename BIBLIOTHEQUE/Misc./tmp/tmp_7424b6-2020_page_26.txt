                                                                                   95%Conﬁdence           t compared to       “I don’t know”
                                                                Meanaccuracy       Interval (low, hi)    control (p-value)     assignments
                         Control (deliberately bad model)            86%               83%–90%                   -                 3.6 %
                         GPT-3Small                                  76%               72%–80%              3.9 (2e-4)             4.9%
                         GPT-3Medium                                 61%               58%–65%             10.3 (7e-21)            6.0%
                         GPT-3Large                                  68%               64%–72%              7.3 (3e-11)            8.7%
                         GPT-3XL                                     62%               59%–65%             10.7 (1e-19)            7.5%
                         GPT-32.7B                                   62%               58%–65%             10.4 (5e-19)            7.1%
                         GPT-36.7B                                   60%               56%–63%             11.2 (3e-21)            6.2%
                         GPT-313B                                    55%               52%–58%             15.3 (1e-32)            7.1%
                         GPT-3175B                                   52%               49%–54%             16.9 (1e-34)            7.8%
                    Table 3.11: Human accuracy in identifying whether short (∼200 word) news articles are model generated. We
                    ﬁndthat human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from 86%
                    onthe control model to 52% on GPT-3 175B. This table compares mean accuracy between ﬁve different models, and
                    showstheresults of a two-sample T-Test for the difference in mean accuracy between each model and the control model
                    (an unconditional GPT-3 Small model with increased output randomness).
                    Meanhumanaccuracy(theratioofcorrect assignments to non-neutral assignments per participant) at detecting that
                    the intentionally bad articles were model generated was ∼ 86% where 50% is chance level performance. By contrast,
                    meanhumanaccuracyatdetecting articles that were produced by the 175B parameter model was barely above chance
                                                 5
                    at ∼ 52% (see Table 3.11). Human abilities to detect model generated text appear to decrease as model size increases:
                    there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance.6
                    This is true despite the fact that participants spend more time on each output as model size increases (see Appendix E).
                    Examples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15.7 Much of the text is—as indicated by the
                    evaluations—difﬁcult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator
                    that an article is model generated since, unlike human authors, the models have no access to the speciﬁc facts that the
                    article titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual
                    phrasings, though these are often subtle enough that they are not noticed.
                    Related work on language model detection by Ippolito et al. [IDCBE19] indicates that automatic discriminators like
                    GROVER [ZHR+19]andGLTR[GSR19]mayhavegreatersuccessatdetecting model generated text than human
                    evaluators. Automatic detection of these models may be a promising area of future research.
                    Ippolito et al. [IDCBE19] also note that human accuracy at detecting model generated text increases as humans observe
                    moretokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated
                    byGPT-3175B,weselected12worldnewsarticlesfromReuterswithanaveragelengthof569wordsandgenerated
                    completions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial
                    experiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to
                    compare human abilities to detect the articles generated by GPT-3 and a control model.
                    Wefound that mean human accuracy at detecting the intentionally bad longer articles from the control model was
                    ∼88%,whilemeanhumanaccuracyatdetectingthelongerarticles that were produced by GPT-3 175B was still barely
                    above chance at ∼ 52% (see Table 3.12). This indicates that, for news articles that are around 500 words long, GPT-3
                    continues to produce articles that humans ﬁnd difﬁcult to distinguish from human written news articles.
                    3.9.5   Learning and Using Novel Words
                    Ataskstudied in developmental linguistics [CB78] is the ability to learn and utilize new words, for example using a
                    wordinasentenceafterseeingitdeﬁnedonlyonce,orconverselyinferringaword’smeaningfromonlyoneusage. Here
                    wequalitatively test GPT-3’s ability to do the former. Speciﬁcally, we give GPT-3 the deﬁnition of a nonexistent word,
                    such as “Gigamuru”, and then ask it to use it in a sentence. We provide one to ﬁve previous examples of a (separate)
                       5Weuseatwo-sampleStudent’s T-Test to test for signiﬁcant difference between the means of the participant accuracies of each
                    modelandthecontrol model and report the normalized difference in the means (as the t-statistic) and the p-value.
                       6If a model consistently produces texts that are more impressive than human articles, it is possible that human performance on
                    this task would drop below 50%. Indeed, many individual participants scored below 50% on this task.
                       7Additional non-news samples can be found in Appendix F.
                                                                                  26
