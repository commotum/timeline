                                                 Model name                        Re-Arc                  public              Concept               ARC-Heavy
                                                                                       [11]               eval [1]             ARC[12]                       [13]
                                                 Llama-rearc [19]                 368 × 400                     -                      -                        -
                                                 Llama-mix                        368 × 400              64 × 300              64 × 176                         -
                                                 Nemo-mix                         368 × 400              64 × 300              64 × 176                         -
                                                 Nemo-full [20]                   736 × 400             128 × 400             128 × 176                         -
                                                 Nemo-heavy                       644 × 400             128 × 400             128 × 176                  1 × 200k
                                            Table 3: Number of training examples (epochs × dataset tasks) from each datasets in
                                            different training runs.
                                            4 epochs. We also tried an additional separate fine-tuning for each task,
                                            which increased the score slightly. However, it did not do enough to be
                                            considered runtime efÏcient in our approach and was discarded. Instead, we
                                            chose to make use of both T4 GPUs available on Kaggle by splitting the test
                                            dataset in two parts, and running the full pipeline in parallel on the halves.
                                            Using this approach we estimate the training took around 5:20 in the Kaggle
                                            notebook.
                                            For a comparison of our training parameters used, see Table 4. In general we
                                            found that modern architectures and larger models performed substantially
                                            better. In the following sections we will present results for three different
                                            models, each trained on different sections of the datasets (see 3). Llama-mix
                                            andNemo-mixaredirectlycomparablemodels,thatshowcasethesubstantial
                                            increase in performance from running the 8B Nemo model, compared to
                                            the 3B Llama model. Llama-rearc provides us with a better evaluation set
                                            performance baseline, as it has not seen any evaluation examples during
                                            training.          And finally, the table includes Nemo-full and Nemo-heavy, our
                                            highest scoring models on the private evaluation set.                                                However, as their
                                            training includes the full evaluation set, we cannot provide any evaluation
                                            performance except for our final Kaggle score (53.5 points Nemo-full and
                                            56.5 for Nemo-heavy).
                                            3.5         Solution Inference
                                            Given a trained decoder-only network, standard solution generation is easy.
                                            Whenprovided with tokens x ,...,x , a model M will calculate the proba-
                                                                                                1            n
                                            bility distributions p                 , . . . , p         for subsequent token predictions. The next
                                                                                x             x
                                                                                  2             n+1
                                            token, px             , can then be selected either greedily (using argmax) or stochas-
                                                            n+1
                                            tically (using multinomial sampling). To generate a solution candidate we
                                            repeat this procedure until we sample an ⟨eos⟩ token, indicating that the
                                            example is done. The output is parsed into an array, with some checks to
                                            make sure it is a valid grid.
                                                                                                             10
