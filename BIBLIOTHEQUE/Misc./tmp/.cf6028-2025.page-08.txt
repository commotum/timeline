                     ModelFamily       Difficulty     Compute     RecommendedRecipe
                     Short-horizon    High/Low          High      MV@N;Nlarge
                                      High/Low          Low       FFS-k@N;k=1,Nlarge
                     Long-horizon     High/Low          High      MV@N;Nlarge
                                      High/Low          Low       SD
                     Non-reasoning    High/Low          High      MV@N;Nlarge
                                      High/Low          Low       FFS-k@N;k=1,Nlarge
                   Table2: Decision matrix outlining optimal TTS strategies based on model family, task difficulty, and
                   computational budget. K denotes the number of shortest/longest traces considered for voting, and
                   Nindicates the total trace count. SD refers to simple decoding, a greedy left-to-right generation
                   procedure analogous to beam search with beam size 1: at each generation step, the model selects
                   only the single most probable continuation.
                   and 5 depict the performance of FFS-k@N and LFS-k@N for the different model types.
                   TheseplotsrevealaninterestingbehaviorwheretheoptimalTTSstrategyalwaysseemsto
                   scale with increasing budget.
                   Furthermore, we find that for the LFS family of methods, the maximum performance for
                   a given amount of total compute is always achieved when k is large (which implies k=N).
                   Notethatk=NissimplyMV-N,andthereforeweconcludethatMV@NisbetterthanLFS-
                   k@Nforanyvalueofk,allwhileconsumingthesamenumberoftokens.
                      Finding
                      LFS is always suboptimal to MV: longest-trace filtering consistently reduces accu-
                      racy at the same compute.
                   For the FFS family of methods, we observe a more nuanced behavior where while per-
                   formance improves for increasing k (while also consuming more tokens) across all model
                   types, the behavior with N for a fixed k is mixed. We find that for short-horizon models
                   largervaluesofNarealwaysbest(higherperformanceatlessertokenconsumption),while
                   for long-horizon and non-reasoning models there is a tradeoff between performance and
                   the compute consumed. Note that while a tradeoff exists for both long-horizon and non-
                   reasoning models, the handles to vary the tradeoff are opposite for them: for long-horizon
                   modelstodrawperformanceatthecostofhighercomputeonehastochoosesmallerN(es-
                   sentially performing simple decoding) while for non-reasoning models one has to choose
                   larger N.
                   5  TheRecipe
                   Ouranalysisrevealsthattheoptimaltest-timescalingstrategyisnotuniversalbutdepends
                   onacombinationofthemodelâ€™sarchitecturalfamily,thedifficulty of the problem at hand,
                   and the available compute budget. To distill our findings into actionable guidance, we
                   present a decision matrix in Table 2. We explain below the rationale for choosing such a
                   recipe below.
                   Short-horizonmodels. Acrossbothlow-andhigh-difficultysettings,short-horizonmod-
                   els consistently prefer shorter traces over longer ones (Table 1). Because FFS-k improves in
                   bothaccuracyandcomputationalcostaskincreases,weselectsmallvaluesofk(specifically
                   k = 1) under low-compute constraints, and large values of k (namely k = N) when ample
                   compute is available. The latter choice is equivalent to MV@N, since selecting the k = N
                   shortest traces from N samples necessarily involves including all traces. Additionally, for
                   short-horizon models, performance increases with larger N for any fixed k. Accordingly,
                   wechooseNtobeaslargeaspermittedbythecomputebudget.
                   Long-horizon models. For high-difficulty settings, long-horizon models prefer longer
                   traces. Because LFS@N improves as N increases, we use large N when compute is abun-
                   dantandsmallN(ideally N = 1)whencomputeislimited. KeepingNfixed,performance
                                                     8
