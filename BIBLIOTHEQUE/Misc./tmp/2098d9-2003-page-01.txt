                           Journal of Machine Learning Research 3 (2003) 1137–1155                                        Submitted 4/02; Published 2/03
                                                      ANeuralProbabilistic Language Model
                           YoshuaBengio                                                                         BENGIOY@IRO.UMONTREAL.CA
                           Réjean Ducharme                                                                   DUCHARME@IRO.UMONTREAL.CA
                           Pascal Vincent                                                                      VINCENTP@IRO.UMONTREAL.CA
                           Christian Jauvin                                                                      JAUVINC@IRO.UMONTREAL.CA
                           Départementd’Informatiqueet RechercheOpérationnelle
                           Centre de RechercheMathématiques
                           Université de Montréal, Montréal, Québec, Canada
                           Editors: Jaz Kandola, ThomasHofmann,Tomaso Poggioand JohnShawe-Taylor
                                                                                   Abstract
                                 Agoal of statistical language modeling is to learn the joint probability function of sequences of
                                words in a language. This is intrinsically difﬁcult because of the curse of dimensionality:aword
                                sequenceonwhichthemodelwillbetestedislikelytobedifferentfromallthewordsequencesseen
                                duringtraining. Traditional but very successful approachesbased on n-grams obtain generalization
                                byconcatenatingveryshortoverlappingsequencesseen in the training set. We proposeto ﬁght the
                                curse of dimensionality by learning a distributed representation for words which allows each
                                training sentence to inform the model about an exponential number of semantically neighboring
                                sentences. The model learns simultaneously (1) a distributed representation for each word along
                                with (2) the probability function for word sequences, expressed in terms of these representations.
                                Generalization is obtained because a sequence of words that has never been seen before gets high
                                probability if it is made of words that are similar (in the sense of having a nearby representation) to
                                wordsforminganalreadyseensentence. Training such large models (with millions of parameters)
                                within a reasonable time is itself a signiﬁcant challenge. We report on experiments using neural
                                networks for the probability function, showing on two text corpora that the proposed approach
                                signiﬁcantly improveson state-of-the-art n-grammodels, and that the proposedapproach allows to
                                take advantage of longer contexts.
                                Keywords: Statistical language modeling, artiﬁcial neural networks, distributed representation,
                                curse of dimensionality
                           1. Introduction
                           Afundamental problem that makes language modeling and other learning problems difﬁcult is the
                           curse of dimensionality. It is particularly obvious in the case when one wants to model the joint
                           distribution between many discrete random variables (such as words in a sentence, or discrete at-
                           tributes in a data-mining task). For example, if one wants to model the joint distribution of 10
                           consecutive words in a natural language with a vocabulary V of size 100,000, there are potentially
                                    10             50
                           100000 −1=10 −1free parameters. When modeling continuous variables, we obtain gen-
                           eralization more easily (e.g. with smooth classes of functions like multi-layer neural networks or
                           Gaussian mixture models) because the function to be learned can be expected to have some lo-
                           cal smoothness properties. For discrete spaces, the generalization structure is not as obvious: any
                           change of these discrete variables may have a drastic impact on the value of the function to be esti-
                           c
                           2003YoshuaBengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin.
