                         Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal
                           transformers. In International Conference on Learning Representations, 2018.
                         NouhaDziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West,
                           Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and fate: Limits of transformers
                           oncompositionality. arXiv preprint arXiv:2305.18654, 2023.
                         Jonas Geiping and Tom Goldstein. Cramming: Training a language model on a single gpu in one day.
                           In International Conference on Machine Learning, pages 11117–11143. PMLR, 2023.
                         Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris
                           Papailiopoulos. Looped transformers as programmable computers. In International Conference on
                           Machine Learning, pages 11398–11442. PMLR, 2023.
                         Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik,
                           Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, et al. xval: A continuous number
                           encoding for large language models. arXiv preprint arXiv:2310.02989, 2023.
                         Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi Bennani, Róbert
                           Csordás, Andrew Joseph Dudzik, Matko Bošnjak, Alex Vitvitskyi, Yulia Rubanova, et al. A
                           generalist neural algorithmic learner. In Learning on graphs conference, pages 2–1. PMLR, 2022.
                         SamyJelassi, Stéphane d’Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and François
                           Charton. Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400,
                           2023.
                         Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva
                           Reddy. The impact of positional encoding on length generalization in transformers. arXiv preprint
                           arXiv:2305.19466, 2023.
                         ZhenzhongLan,MingdaChen,SebastianGoodman,KevinGimpel,PiyushSharma,andRaduSoricut.
                           Albert: A lite bert for self-supervised learning of language representations. In International
                           Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=
                           H1eA7AEtvS.
                         Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, and Dimitris Papailiopoulos.
                           Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.
                         Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit
                           Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for
                           relative positions improves long context transformers. arXiv preprint arXiv:2310.04418, 2023.
                         John Loeber. #16: Notes on Arithmetic in GPT-4, February 2024. URL https://loeber.
                           substack.com/p/16-notes-on-arithmetic-in-gpt-4.
                         Ilya Loshchilov and Frank Hutter.  Decoupled weight decay regularization.   arXiv preprint
                           arXiv:1711.05101, 2017.
                         Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan
                           May, and Luke Zettlemoyer. Mega: moving average equipped gated attention. arXiv preprint
                           arXiv:2209.10655, 2022.
                         Sean McLeish, Avi Schwarzschild, and Tom Goldstein. Benchmarking chatgpt on algorithmic
                           reasoning. arXiv preprint arXiv:2404.03441, 2024.
                         Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,
                           BenMann,AmandaAskell,YuntaoBai,AnnaChen,etal. In-contextlearningandinductionheads.
                           arXiv preprint arXiv:2209.11895, 2022.
                         OpenAI.    Gpt-4 technical report.   ArXiv, abs/2303.08774, 2023.     URL https://api.
                           semanticscholar.org/CorpusID:257532815.
                         BowenPeng,Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window
                           extension of large language models. International Conference on Learning Representations, 2024.
                                                                    11
