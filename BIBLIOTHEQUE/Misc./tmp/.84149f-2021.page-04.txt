                                                                                          Temporal Transformer Encoder                                                                        MLP             Class                                                                                                                   Transformer Block x L
                                          en                                                                                                                                                  Head
                                          k   g
                                              n
                                          To  i
                                              d      C
                                          +                                                                                                 …
                                              d
                                          l   e  0 L     1                                                    2                                                                T
                                          a
                                          r   b      S
                                          o                                                                                                                                                                                                                                                                                                                     K 
                                          p   Em                                                                                                                                                                                                                                                                         K                                       
                                                                                                                                                                                                                                                Positional embedding                                                                                             
                                                                                                                                                                                                                                                                                                                                                                         Mu               La
                                                                                                                                                                                                                                                                                                                                 Mu                     La       
                                                                                                                                                                                                                                                                                                                La                                               
                                                                                                                                                                                                                                                                                                                             At                                      At
                                                                                                                                                                                                                                                                                                                                                                 
                                                                                                                                                                                                                                                                                                                                                                 
                                          Tem                                                                                                                                                                                                                                                                   y                                       y       V    t                    y
                                                                                                                                                                                                                                                                                                                e        V   t   l                      e                l                e        ML
                                                             Spatial Transformer                                 Spatial Transformer                                                                                                                                                                                         e   t                      r            e   t                r
                                                                                                                                                                                  Spatial Transformer                                                                                                           r                i                                       i                 
                                                                                                                                                                                                                                                                                                                             n                                       n   -                N
                                                                                                                                                                                                                                                                                                                                 -                      N        
                                                                                                                                                                                                                                                                                                                N                He                                      He                        P
                                                                                                                                                                                                                                                   Token embedding                                                                                                   t
                                                                                                                                                                                                                                                                                                                             t                                  Q                         or
                                                   n                                                                                                                                                                                                                                                            or       Q   i                          or           i
                                                   e                   Encoder                                             Encoder                                                          Encoder                                                                                                                          o   a                      m            o   a                m
                                                   k   g                                                                                                                                                                                                                                                        m            n   d                                   n   d
                                                   o   n
                                                   T   i
                                                       d                                                          C
                                                   +          C                                                                                                 …                  C
                                                       d
                                                   l   e  0 L       1           … N                           0 L       1           … N                                        0 L                   …
                                                   a   b                                                          S                                                                      1                       N
                                                   n          S                                                                                                                    S
                                                   o
                                                   i                                                                                                                                                                                                                                                       Spatial Self-Attention Block          Temporal Self-Attention Block
                                                   t
                                                   i   Em
                                                   s
                                                   Po                                                                     Embed to tokens
                                                                                                                                                                                                                                          Figure 5: Factorised self-attention (Model 3). Within each trans-
                                                                                                                                                                                                                                          former block, the multi-headed self-attention operation is fac-
                                                                                                                                                                                                                                          torised into two operations (indicated by striped boxes) that ﬁrst
                                                                                                                                                                                                                                          only compute self-attention spatially, and then temporally.
                                      Figure 4: Factorised encoder (Model 2). This model consists of                                                                                                                                      poral index), and then temporally (among all tokens ex-
                                      two transformer encoders in series: the ﬁrst models interactions
                                      betweentokensextractedfromthesametemporalindextoproduce                                                                                                                                             tracted from the same spatial index) as shown in Fig. 5.
                                      a latent representation per time-index. The second transformer                                                                                                                                      Each self-attention block in the transformer thus models
                                      models interactions between time steps. It thus corresponds to a                                                                                                                                    spatio-temporal interactions, but does so more efﬁciently
                                      “late fusion” of spatial- and temporal information.                                                                                                                                                 than Model 1 by factorising the operation over two smaller
                                      tention (MSA) [67] has quadratic complexity with respect                                                                                                                                            sets of elements, thus achieving the same computational
                                      to the number of tokens. This complexity is pertinent for                                                                                                                                           complexity as Model 2. We note that factorising attention
                                      video, as the number of tokens increases linearly with the                                                                                                                                          over input dimensions has also been explored in [28, 77],
                                      number of input frames, and motivates the development of                                                                                                                                            and concurrently in the context of video by [4] in their “Di-
                                      moreefﬁcient architectures next.                                                                                                                                                                    vided Space-Time” model.
                                                                                                                                                                                                                                                   Thisoperationcanbeperformedefﬁcientlybyreshaping
                                                                                                                                                                                                                                                                                                         1×n ·n ·n ·d                                          n ×n ·n ·d
                                      Model 2: Factorised encoder                                                                            As shown in Fig. 4, this                                                                     the tokens z from R                                                       t       h w to R t                                        h w (denoted
                                      model consists of two separate transformer encoders. The                                                                                                                                            byzs)tocomputespatialself-attention. Similarly, the input
                                                                                                                                                                                                                                                                                                                                                                                          n ·n ×n ·d
                                      ﬁrst, spatial encoder, only models interactions between to-                                                                                                                                         to temporal self-attention, z is reshaped to R h w                                                                                                                       t       .
                                                                                                                                                                                                                                                                                                                                 t
                                      kensextracted from the same temporal index. A representa-                                                                                                                                           Hereweassumetheleadingdimensionisthe“batchdimen-
                                      tion for each temporal index, hi ∈ Rd, is obtained after Ls                                                                                                                                         sion”. Our factorised self-attention is deﬁned as
                                                                                                                                                                                         L
                                      layers: This is the encoded classiﬁcation token, z s if it was
                                                                                                                                                                                        cls
                                      prepended to the input (Eq. 1), or a global average pooling                                                                                                                                                                                                 `                                                 `                     `
                                                                                                                                                                                                                                                                                             y =MSA(LN(z ))+z                                                                                                       (4)
                                                                                                                                                                                           L                                                                                                      s                                                 s                     s
                                      from the tokens output by the spatial encoder, z s, other-                                                                                                                                                                                             y` = MSA(LN(y`))+y`                                                                                                    (5)
                                      wise. The frame-level representations, h , are concatenated                                                                                                                                                                                                 t                                                  s                      s
                                                                                                                                                                i                                                                                                                          `+1                                                       `                     `
                                      into H ∈ Rnt×d, and then forwarded through a temporal                                                                                                                                                                                            z               =MLP(LN(yt))+yt.                                                                                             (6)
                                      encoder consisting of Lt transformer layers to model in-
                                      teractions between tokens from different temporal indices.                                                                                                                                          We observed that the order of spatial-then-temporal self-
                                      Theoutput token of this encoder is then ﬁnally classiﬁed.                                                                                                                                           attention or temporal-then-spatial self-attention does not
                                               This architecture corresponds to a “late fusion” [33,                                                                                                                                      make a difference, provided that the model parameters are
                                      55, 71, 45] of temporal information, and the initial spa-                                                                                                                                           initialised as described in Sec. 3.4. Note that the number
                                      tial encoder is identical to the one used for image classi-                                                                                                                                         of parameters, however, increases compared to Model 1, as
                                      ﬁcation. It is thus analogous to CNN architectures such                                                                                                                                             there is an additional self-attention layer (cf. Eq. 7). We do
                                      as             [23, 33, 71, 85] which ﬁrst extract per-frame fea-                                                                                                                                   not use a classiﬁcation token in this model, to avoid ambi-
                                      tures, and then aggregate them into a ﬁnal representation                                                                                                                                           guities when reshaping the input tokens between spatial and
                                      before classifying them.                                                          Although this model has more                                                                                      temporal dimensions.
                                      transformer layers than Model 1 (and thus more parame-
                                      ters), it requires fewer ﬂoating point operations (FLOPs), as                                                                                                                                       Model 4: Factorised dot-product attention                                                                                                        Finally, we
                                      the two separate transformer blocks have a complexity of                                                                                                                                            develop a model which has the same computational com-
                                      O((nh ·nw)2 +n2)comparedtoO((nt ·nh ·nw)2).
                                                                                              t                                                                                                                                           plexity as Models 2 and 3, while retaining the same number
                                      Model 3: Factorised self-attention                                                                                     This model, in con-                                                          of parameters as the unfactorised Model 1. The factorisa-
                                      trast, contains the same number of transformer layers as                                                                                                                                            tion of spatial- and temporal dimensions is similar in spirit
                                      Model 1. However, instead of computing multi-headed                                                                                                                                                 to Model 3, but we factorise the multi-head dot-product at-
                                                                                                                                                                       `
                                      self-attention across all pairs of tokens, z , at layer l, we                                                                                                                                       tention operation instead (Fig. 1). Concretely, we compute
                                      factorise the operation to ﬁrst only compute self-attention                                                                                                                                         attention weights for each token separately over the spatial-
                                      spatially (among all tokens extracted from the same tem-                                                                                                                                            and temporal-dimensions using different heads. First, we
                                                                                                                                                                                                                                 6839
