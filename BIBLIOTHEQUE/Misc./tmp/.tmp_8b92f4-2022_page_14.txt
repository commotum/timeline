              Under review as a conference paper at ICLR 2025
          702 Hosung Lee, Sejin Kim, Seungpil Lee, Sanha Hwang, Jihwan Lee, Byung-Jun Lee, and Sundong
          703  Kim. Arcle: The abstraction and reasoning corpus learning environment for reinforcement learn-
          704  ing. arXiv preprint arXiv:2407.20806, 2024.
          705
          706 Chao Lei, Nir Lipovetzky, and Krista A Ehinger. Generalized planning for the abstraction and
          707  reasoning corpus. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,
          708  pp. 20168–20175, 2024.
          709 ShandaLi,ChongYou,GuruGuruganesh,JoshuaAinslie,SantiagoOntanon,ManzilZaheer,Sumit
          710  Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for
          711  relative positions improves long context transformers. In The Twelfth International Conference
          712  onLearning Representations.
          713 Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya Jia. Mat: Mask-aware transformer for
          714  large hole image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision
          715  andPattern Recognition (CVPR), pp. 10758–10768, June 2022.
          716
          717 XiaochuanLi,BaoyuFan,RunzeZhang,LiangJin,DiWang,ZhenhuaGuo,YaqianZhao,andRen-
          718  gangLi. Imagecontentgenerationwithcausalreasoning. InProceedingsoftheAAAIConference
          719  onArtificial Intelligence, volume 38, pp. 13646–13654, 2024.
          720 Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir:
          721  Imagerestoration using swin transformer. In Proceedings of the IEEE/CVF International Confer-
          722  ence on Computer Vision (ICCV) Workshops, pp. 1833–1844, October 2021.
          723
          724 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
          725  Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the
          726  IEEE/CVFinternational conference on computer vision, pp. 10012–10022, 2021.
          727        ´         ´
              Mikołaj Małkinski and Jacek Mandziuk. A review of emerging research directions in abstract visual
          728  reasoning. Information Fusion, 91:713–736, 2023.
          729 JohnChongMinTanandMehulMotani.Llmsasasystemofmultipleexpertagents: Anapproachto
          730  solve the abstraction and reasoning corpus (arc) challenge. In 2024 IEEE Conference on Artificial
          731  Intelligence (CAI), pp. 782–787, 2024. doi: 10.1109/CAI59869.2024.00149.
          732
          733 SuvirMirchandani,FeiXia,PeteFlorence,brianichter,DannyDriess,MontserratGonzalezArenas,
          734  Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern ma-
          735  chines. In 7th Annual Conference on Robot Learning, 2023. URL https://openreview.
          736  net/forum?id=RcZMI8MSyE.
          737 Melanie Mitchell, Alessandro B. Palmarini, and Arsenii Kirillovich Moskvichev. Comparing hu-
          738  mans, GPT-4, and GPT-4v on abstraction and reasoning tasks. In AAAI 2024 Workshop on ”Are
          739  LargeLanguageModelsSimplyCausalParrots?”,2023. URLhttps://openreview.net/
          740  forum?id=3rGT5OkzpC.
          741
          742 Arsenii Kirillovich Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. The conceptARC
          743  benchmark: Evaluating understanding and generalization in the ARC domain. Transactions on
          744  Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/
          745  forum?id=8ykyGbtt2q.
          746 Jaehyun Park, Jaegyun Im, Sanha Hwang, Mintaek Lim, Sabina Ualibekova, Sejin Kim, and Sun-
          747  dong Kim. Unraveling the arc puzzle: Mimicking human solutions with object-centric decision
          748  transformer. In Interactive Learning with Implicit Human Feedback Workshop at ICML 2023.,
          749  2023.
          750 Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases
          751  enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.
          752
          753 ColinRaffel, NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
          754  Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-
          755  text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http:
               //jmlr.org/papers/v21/20-074.html.
                                      14
