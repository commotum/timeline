



´1                          1
Aljosa Osep            Laura Leal-Taixe              TimMeinhardt
2       ¨
Linkoping University
4DInstances
Semantics
Text prompts:                     Text prompts:
{car}





spatio-temporal scene understanding is directly relevant for

calization [33, 39] and neural rendering [63].
Status quo.        In applications that demand precise spa-
tial and dynamic situational scene understanding, e.g., au-
tonomous driving [91], perception stacks rely on Lidar-
based object detection [53, 104, 113] and multi-object
tracking [20, 35, 93, 106] methods to localize objects, with
recent trends moving towards holistic scene understanding
via 4D Lidar Panoptic Segmentation (4D-LPS) [4]. The
progress in these areas has largely been fueled by data-
driven methods [16, 73, 74, 90] that rely on manually la-
beled datasets [7, 24, 86], limiting these methods to lo-
calizing instances of predefined object classes.            On the
other hand, recent developments in single-scan Lidar-based
perception are moving towards utilizing vision foundation
models for pre-training [71, 72, 82] and zero-shot segmen-
tation [62, 67, 99]. However, state-of-the-art methods can
only detect [61] and segment [62, 99] objects in individual
scans. In contrast, embodied agents must continuously in-
terpret sensory data and localize objects in a 4D continuum
to understand the present and predict the future.

Panoptic Segmentation by distilling video-foundation mod-
els to Lidar? Recent advances [78] suggest that Video Ob-
ject Segmentation (VOS) [70] generalize well to arbitrary

bility remains a challenge [21, 110], while data recorded

This formulation limits types of classes that can be rec-

Lidar data is scarce, [67, 99] lift image features to 3D for
zero-shot semantic [67] and panoptic [99] segmentation.











dar [68] data. It is commonly addressed via tracking-by-






on motion and geometry [20, 35, 93, 106]. Unlike our pur-

object, prior Lidar-based tracking methods focus on the
cross-detection association to track instances of pre-defined

Related to our work is class-agnostic multi-object track-

conjunction with zero-shot recognition [17, 50]. Like ours,
these methods must track and, optionally, classify objects
as they enter and exit the sensing area. In contrast to ours,



on spatial prompts (Visual Object Tracking [32, 41, 42, 97]



4D Lidar panoptic segmentation.           4D Lidar Panoptic

standing of (4D) Lidar data. Contemporary methods ap-
proach this task by segmenting short spatio-temporal (4D)
volumes [3, 4, 13, 30, 40, 96, 105, 115], followed by








tionally, semantic recognition of tracked objects (for which

between thing and stuff classes cannot be specified prior to
the model training, we drop this distinction.

components: (i) The pseudo-label engine (Fig. 2) con-
structs a proxy dataset D     , that consists of Lidar data and

self-generated pseudo-labels that localize individual spatio-
temporal instances and their semantic features.       (ii) The

fixed-size 4D volumes by minimizing empirical risk on our
proxy dataset D      . Our model and proxy dataset are con-

structed such that our model learns to segment and recog-
nize a super-set of all objects labeled in existing datasets.


Our pseudo-label engine (Fig. 2) operates with a multi-
modal sensory setup. We assume an input Lidar sequence
T                                             c C
,
c=1
c        c T                          c

t

camera c at time t.     For each point cloud Pt, we pro-
Mt



where m˜     ∈ {0,1} t represents the binary segmentation


id ∈ N is the unique object identifier for spatio-temporal


features aggregated over time.




label each temporal window (see Figure 2a), and then per-

be       pseudo-labels for sequences of arbitrary length. In a nut-


is
(track), lift masks to 4D Lidar sequences (lift), and, finally,
“flatten” overlapping masklets in the 4D volume. Our tem-
poral windows w = {(P ,I ) | t ∈ T } consist of Lidar
k         t  t          k
point clouds and images over specific time frames. Here,


for window wk. We drop the camera index c unless needed.
Track.     For each video, we use a segmentation foun-

dation model [37] to perform grid-prompting in the first
video frame of the window I      to localize objects as masks

k
H×W
, where M      denotes the
t
k i=1        k                             k


k

t        k


i,t        k i=1

t
k                     k
a3DvideovolumeofdimensionsH×W×K,representing

tk                        k

Given masklets {mi,t | t ∈ Tk}            and correspond-


t          k

(b) Cross-window association.





i,t
based suppression) in the 4D space-time volume. However,



t                     k

3.2.2. Labeling Arbitrary-Length Sequences
After labeling each temporal window, we obtain pseudo-

labels for point clouds within overlapping windows of size
K,withlocal instance identifiers id     . To produce pseudo-

labels for the full sequence of length T and account for new
objects entering the scene, we associate instances across
windows in a near-online fashion (with stride S), resulting
our final pseudo-labels {(m˜    , id , f ) | t ∈ T} (Fig. 4).

For each pair of overlapping windows (w           , w ), we
k−1    k

ciation costs from temporal instance overlaps (measured by

3D-IoU) in the overlapping frames T         ∩T :
k−1      k
c   =1−IoU (m˜             , m˜  ),           (1)
ij            3D    i,k−1    j,k

where m˜        and m˜    are the aggregated Lidar masks of
i,k−1        j,k
instances i and j. After association, we update the global

i

i

3.3. SAL-4D Model



|m˜  |, where       Overview.        We follow tracking-before-detection de-


i,t
agnostic fashion. Once localized and tracked, objects can

former decoder-based architecture [12]. In a nutshell, our
network (Fig. 3) consists of a point cloud encoder-decoder















label engine and model and justify our design decisions. In

and supervised baselines on multiple benchmarks for 3D
, superim-       and 4D Lidar Panoptic Segmentation.
t              t +K−1
k
4.1. Experiments

Datasets. For evaluation, we utilize two datasets that pro-

SemanticKITTI [7] and Panoptic nuScenes [11, 24].

SemanticKITTI was recorded in Karlsruhe, Germany, us-
ing a 64-beam Velodyne Lidar sensor at 10Hz and provides
Lidar and front RGB camera, which we use for pseudo-

dataset provides instance-level spatiotemporal labels for 8
thing and 11 stuff classes.


360◦ coverage (covering 48% of all points) at 2Hz. Spatio-
temporal labels are available for 8 thing and 8 stuff classes.
Evaluation metrics.       We follow prior work in 4D Li-


dar Panoptic Segmentation [4] and adopt LSTQ as the

√

terms,associationtermSassoc assessesspatio-temporalseg-

mentation quality, independently of semantics, whereas
classification S   assesses semanticrecognitionqualityand

,         (2)      establishes whether points were correctly classified. This
separation between spatio-temporal segmentation and se-




Panoptic Quality [36], which consists of Segmentation



cover part of the point cloud co-visible in RGB cameras
(“frustum”), we focus our ablations to camera view frus-
tumsandonlyreportbenchmarkresultsonfullpointclouds.
Furthermore, since our approach no longer distinguishes
thing and stuff classes but treats both in a unified manner,


cls             th
Comp

49.2     70.0     34.6    36.0      36.9          Labels      8              51.1    70.3    37.2   37.4    41.5
Ego-motion compensation




Model       2      Mix     52.3    74.8    36.6   47.7    21.3
Model       4      Mix     52.7    76.2    36.4   47.8    25.3








pensation during training by 10%. Bottom: Processing larger tem-
poral sequencesdirectly benefits our model. Overall, we distill our
pseudo-labels (51.1 LSTQ) to a stronger model (53.2 LSTQ).
windows should be preferable. However, errors that arise

degrade the performance. Our analysis confirms this intu-
ition: we generate pseudo-labels with varying window sizes

2
findings in Tab. 1. Our results improve with increasing win-
dowsize, but performance plateaus after K = 8. We obtain


and tracking (Sassoc: 70.3 vs. 69.5). In Fig. 4, we confirm
Our       this visually by contrasting ground-truth labels with single-
scan labels, and our labels, obtained with K = {2,8}.
Gains are most significant in terms of Sassoc, as these re-
sults are reported after cross-window association. The ap-
pendix reports a similar analysis conducted before cross-
windowassociation. For the remainder, we fix K = 8.
Comparison with single-scan pseudo-labels.                Do our
spatio-temporal pseudo-labels improve quality on a single-
scan basis? In Tab. 3, we compare our SAL-4D pseudo-
labels with single-scan labels (SAL [62]), and report zero-
shot and class-agnostic segmentation results.         As can be
seen, our temporally consistent pseudo-labels perform bet-
ter than our single-scan counterparts, especially in terms
of semantics (a relative 15% improvement w.r.t. PQ and
20% improvement w.r.t. mIoU). Our spatio-temporal la-

temporal labels improve precision due to temporal coher-
ence. We conclude that our approach not only unlocks the
training of models for ZS-4D-LPSbutalsosubstantiallyim-
proves pseudo-labels for training ZS-LPS methods [62].
4.2.2. Model and Training

To train the 4D segmentation model, we superimpose
point clouds within fixed-size temporal windows and train
our model to directly segment superimposed point clouds
within these short 4D volumes. For a comparison with our


PQ       SQ      PQth     PQst     mIoU                                                                                     th      st

DS-Net[29]         ×            -        57.7    77.6   61.8    54.8
PolarSeg [114]     ×            -        59.1    78.3   65.7    54.3
GP-S3Net[79]       ×            -        63.3    81.4   70.2    58.3
SupervisedMaskPLS[55]  ×            -        59.8    76.3     -      -
✓        62k / 15.2   33.1    71.3   21.5    41.5
SAL-4D             ✓        61k / 15.1   38.2    78.1   30.9    43.5
Zero-shotSAL[62]       ×        25k / 49.0   25.3    63.8   18.3    30.3
×        18k / 44.0   30.8    76.9   25.5    34.6
Table 4. 3D-LPS evaluation. Training our SAL-4D model on the
temporal consistent 4D pseudo-labels yields superior 3D (single-
scan) performance compared to 3D baselines. We evaluate on the
SemanticKITTI validation set. SAL-4D evaluated not only in the
frustum was trained with the FrankenFrustum [62] augmentation.




(a) Ground Truth (GT).                  (b) 3D Pseudo-Labels.
Refers to the number of scans



(c) 4D Pseudo-Labels (2 frames).       (d) 4D Pseudo-Labels (8 frames).
Figure 4. Qualitative results. We compare our 4D pseudo-labels
(obtained over windows of 2&8 frames) to GT labels, and single-
scan labels. By contrast to GT, our automatically-generated labels
cover both thing and stuff classes. As can be seen, the temporal
coherence of labels improves over larger window sizes.
tation [62], that helps our model, trained on pseudo-labels
generated on 14% of full point cloud, to generalize to the
full 360◦ point clouds. As can be seen in Tab. 4, SAL-4D
consistently outperforms SAL baseline: we obtain 38.2 PQ
within-frustum (+5.1 w.r.t. SAL), and 30.8 PQ on the full
point cloud (+5.5 w.r.t. SAL), and overall reduces the gap
to supervised baselines. Improvements are especially no-
table for thing classes (18.3 vs. 25.5 PQ                  ). We attribute
th
). With          these gains to temporal coherence imposed during pseudo-
labeling and model training.

4.3.2. 4D Lidar Panoptic Segmentation
We compare SAL-4D to several zero-shot baselines and
state-of-the-art 4D-LPS methods trained with ground-truth

datasets.      In contrast, all zero-shot approaches rely only
on single-scan 3D [62] or our 4D pseudo-labels. To com-
pare SAL-4D to baselines that operate on full (360◦) point
We compare two variants of our                        clouds, we train our model on temporal windows of size 2,
with FrankenFrustum augmentation [62], which helps our
model to generalize beyond view frustum.
ZS-4D-LPS baselines.                 We construct several baselines
that associate single-scan 3D SAL [62] predictions in time
(see Appendix B for further details) and require no tempo-

LSTQ    Sassoc    S      IoUst   IoU                        GT                Pseudo-labels              SAL-4D
th
62.7     65.1    60.5    65.4    61.3
67.0     74.4    60.3    65.3    60.9




71.4     75.4    67.5    65.8    69.9
69.1     70.1    68.0    65.7    71.2

30.9     34.4    27.7    41.0    12.9          Figure 5.     Qualitative results on SemanticKITTI. We show
32.7     38.5    27.7    41.0    12.9          ground-truth (GT) labels (first column), our pseudo-labels (mid-

56.1     51.4     -       -        -           dle column), and SAL-4D results (right column). We show se-


30.3     26.9    34.3    43.0    29.9          of the sequence (middle). By contrast to GT labels, our pseudo-
32.8     31.5    34.3    43.0    29.9          label instances are not limited to a subset of thing classes (GT, left
33.2     32.4    34.1    42.8    29.7


labels do not provide semantic labels, only CLIP tokens. For visu-
alization, we prompt individual instances with prompts that con-
form to the SemanticKITTI class vocabulary. Best seen zoomed.
EfficientLPS+KF. Due to the different ratio between static
and moving objects on nuScenes, MOT baseline (32.8
LSTQ) outperforms SW (30.3 LSTQ), as expected. Min-

VISperformsfavorablycomparedtobothandachieves33.2

efits from a larger Panoptic nuScenes dataset. Improve-
ments over baselines are most notable in terms of associ-
ation (Sassoc: 48.8 SAL-4D vs. 32.4 MinVIS).

5. Conclusions
Weintroduced SAL-4D for zero-shot segmentation, track-

ing, and recognition of arbitrary objects in Lidar.                   Our
core component, the pseudo-label engine, distills recent ad-
vancements in image-based video object segmentation to
Lidar. This enables us to improve significantly over prior
single-scanmethodsandunlockZero-Shot4DLidarPanop-
tic Segmentation. However, as evidencedinTab.5, aperfor-
mancegappersists compared to fully-supervised methods.
Challenges.       We observe semantic recognition is the pri-
mary source of this gap, with zero-shot recognition Scls
(34.9) trailing supervised methods (68.0). Second, segmen-
tation consistency degrades over extended temporal hori-
zons, reflecting challenges in maintaining coherence across
superimposed point clouds. Third, segmentation quality is
notably lower for thing classes compared to stuff classes,
mostlikelyduetotheinherentimbalance,mitigatedbyaug-
We note that SAL-                mentation strategies in supervised methods.

Future work. To bridge these gaps, we will focus on (i)
refining the data labeling engine to enhance temporal con-
sistency, (ii) expanding the volume of pseudo-labeled data,
and (iii) curating high-quality labels for fine-tuning. These
steps aim to narrow the divide with supervised methods
while preserving SAL-4D ’s zero-shot scalability.

[16] Christopher Choy, JunYoung Gwak, and Silvio Savarese.
4D spatio-temporal convnets: Minkowski convolutional
neural networks.   In IEEE Conf. Comput. Vis. Pattern
Recog., 2019. 1, 2, 5

Dave, Leonidas Guibas, and Katerina Fragkiadaki. Zero-
shot open-vocabulary tracking with large pre-trained mod-
els. In Int. Conf. Rob. Automat., 2024. 2

wards segmenting anything that moves. In ICCV Work-
shops, 2019. 2


Schindler, Daniel Cremers, Ian Reid, Stefan Roth, and

Laura Leal-Taixe. Motchallenge: A benchmark for single-
camera multiple target tracking. Int. J. Comput. Vis., 2020.

2
[20] Shuxiao Ding, Eike Rehder, Lukas Schneider, Marius
Cordts, and Juergen Gall. 3dmotformer: Graph transformer
for online 3d multi-object tracking. In Int. Conf. Comput.
Vis., 2023. 1, 2
[21] Shuangrui Ding, Rui Qian, Xiaoyi Dong, Pan Zhang,
YuhangZang,YuhangCao,YuweiGuo,DahuaLin,andJi-
aqi Wang. Sam2long: Enhancing sam 2 for long video seg-
mentation with a training-free memory tree. arXiv preprint
arXiv:2410.16268, 2024. 1
Open-
vocabularyuniversalimagesegmentationwithmaskclip. In
Int. Conf. Mach. Learn., 2023. 3
¨

et al. A density-based algorithm for discovering clusters in
large spatial databases with noise. In Rob. Sci. Sys., 1996.
4

ingZhou,HolgerCaesar,OscarBeijbom,andAbhinavVal-
ada. Panoptic nuscenes: A large-scale benchmark for lidar

panoptic segmentation and tracking. RAL, 2021. 1, 2, 5
[25] Stefano Gasperini, Mohammad-Ali Nikouei Mahani, Al-
varo Marcos-Ramiro, Nassir Navab, and Federico Tombari.
Panoster: End-to-end panoptic segmentation of lidar point
clouds. IEEE Rob. Automat. Letters, 2021. 2
[26] GolnazGhiasi,XiuyeGu,YinCui,andTsung-YiLin. Scal-
ing open-vocabulary image segmentation with image-level
labels. In Eur. Conf. Comput. Vis., 2022. 3

Open-vocabulary object detection via vision and language
knowledge distillation. Int. Conf. Learn. Represent., 2022.
3

Thrun, and Silvio Savarese. A probabilistic framework for
real-time 3d segmentation using spatial, temporal, and se-
mantic cues. In Rob. Sci. Sys., 2016. 2

and Ziwei Liu. Lidar-based panoptic segmentation via dy-
namicshiftingnetwork. InIEEEConf.Comput.Vis.Pattern
Recog., 2021. 2, 7

Hongsheng Li, and Ziwei Liu. Unified 3d and 4d panoptic

Anopen-worldpanoptic segmentation and tracking robotic
dataset in crowded human environments. In IEEE Conf.
Comput. Vis. Pattern Recog., 2024. 2
[45] Bastian Leibe, Konrad Schindler, Nico Cornelis, and
LucVanGool. Coupledobject detection and tracking from
static cameras and moving vehicles. IEEE Trans. Pattern
Anal. Mach. Intell., 2008. 2
[46] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Rene Ranftl. Language-driven semantic seg-
mentation. In Int. Conf. Learn. Represent., 2022. 3
[47] Jinke Li, Yang Wen Xiao He, Yuan Gao, Xiaoqiang Cheng,
and Dan Zhang. Panoptic-phnet: Towards real-time and
high-precision lidar panoptic segmentation via clustering
pseudo heatmap.       In IEEE Conf. Comput. Vis. Pattern
Recog., 2022. 2
[48] Shijie Li, Xieyuanli Chen, Yun Liu, Dengxin Dai, Cyrill
Stachniss, and Juergen Gall.      Multi-scale interaction for
real-time lidar data segmentation on an embedded platform.

IEEERob.Automat.Letters, 2021. 2

[49] Siyuan Li, Martin Danelljan, Henghui Ding, Thomas E
Huang, and Fisher Yu. Tracking every thing in the wild.
In Eur. Conf. Comput. Vis., 2022. 2

[50] Siyuan Li, Tobias Fischer, Lei Ke, Henghui Ding, Martin
Danelljan, and Fisher Yu. Ovtrack: Open-vocabulary mul-
tiple object tracking. In IEEE Conf. Comput. Vis. Pattern
Recog., 2023. 2
[51] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yi-
nan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and
Diana Marculescu. Open-vocabulary semantic segmenta-

tion with mask-adapted clip. In IEEE Conf. Comput. Vis.
Pattern Recog., 2023. 3
[52] Yang Liu, Idil Esen Zulfikar, Jonathon Luiten, Achal Dave,

Deva Ramanan, Bastian Leibe, Aljosa Osep, and Laura


Leal-Taixe. Openingupopenworldtracking. InIEEEConf.

Comput. Vis. Pattern Recog., 2022. 2
[53] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong.
Group-free 3d object detection via transformers.        In Int.
Conf. Comput. Vis., 2021. 1, 2
[54] RodrigoMarcuzzi,LucasNunes,LouisWiesmann,Ignacio
Vizzo, Jens Behley, and Cyrill Stachniss. Contrastive in-
stance association for 4d panoptic segmentation using se-
quences of 3d lidar scans.      IEEE Rob. Automat. Letters,
2022. 2
[55] Rodrigo Marcuzzi, Lucas Nunes, Louis Wiesmann, Jens
Behley, and Cyrill Stachniss. Mask-based panoptic lidar
segmentation for autonomous driving. IEEE Rob. Automat.
´ˇ    ´ˇ                   Letters, 2023. 5, 7

[56] Rodrigo Marcuzzi, Lucas Nunes, Louis Wiesmann, Elias

Marks, Jens Behley, and Cyrill Stachniss. Mask4d: End-
to-end mask-based 4d panoptic segmentation for lidar se-
quences. IEEE Rob. Automat. Letters, 2023. 2, 8
[57] Andres Milioto, Ignacio Vizzo, Jens Behley, and Cyrill
Stachniss. RangeNet++: Fast and Accurate LiDARSeman-
tic Segmentation. In Int. Conf. Intel. Rob. Sys., 2019. 2
[58] DimityMiller,LachlanNicholson,FerasDayoub,andNiko

Sunderhauf. Dropout sampling for robust object detection
in open-set conditions. In Int. Conf. Rob. Automat., 2018. 3

[73] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J.
Guibas. Pointnet: Deep learning on point sets for 3d clas-
sification and segmentation. In IEEE Conf. Comput. Vis.
Joint self-              Pattern Recog., 2017. 1
[74] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-
net++: Deep hierarchical feature learning on point sets in a
metric space. Adv. Neural Inform. Process. Syst., 2017. 1
[75] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
AmandaAskell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In Int. Conf. Mach. Learn., 2021. 3, 4

Zero-shotobjectdetection: Learningtosimultaneouslyrec-
ognize and localize novel concepts. Asian Conf. Comput.
Vis., 2018. 3
[77] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong
Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen
Lu.  Denseclip: Language-guided dense prediction with
context-aware prompting. In IEEE Conf. Comput. Vis. Pat-
tern Recog., 2022. 3

Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Ro-

¨
man Radle, Chloe Rolland, Laura Gustafson, et al. Sam
2: Segment anything in images and videos. arXiv preprint
arXiv:2408.00714, 2024. 1, 2, 3, 4
[79] Ryan Razani, Ran Cheng, Enxu Li, Ehsan Taghavi, Yuan
Ren, and Liu Bingbing. Gp-s3net: Graph-based panop-
tic sparse semantic segmentation network. In IEEE Conf.
Comput. Vis. Pattern Recog., 2021. 2, 7
[80] Ryan Razani, Ran Cheng, Ehsan Taghavi, and Liu Bing-
bing. Lite-hdseg: Lidar semantic segmentation using lite
harmonic dense convolutions. In Int. Conf. Rob. Automat.,
2021. 2
[81] Donald B Reid. An algorithm for tracking multiple targets.
Tran. Automat. Contr., 1979. 2

Boulch, Andrei Bursuc, and Renaud Marlet. Image-to-lidar
self-supervised distillation for autonomous driving data. In

IEEEConf.Comput.Vis. Pattern Recog., 2022. 1
[83] Walter J Scheirer, Anderson de Rezende Rocha, Archana
Sapkota, and Terrance E Boult. Toward open set recogni-
tion. IEEE transactions on pattern analysis and machine
intelligence, 35(7):1757–1772, 2012. 3

´

´

to a strong multi-object tracker. In IEEE Conf. Comput. Vis.
Pattern Recog., 2023. 2
´
¨

gard, and Abhinav Valada.      Efficientlps: Efficient lidar
panoptic segmentation.    IEEE Transactions on Robotics,
2021. 7, 8
[86] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aure-
´
lien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin
Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in
perception for autonomous driving: Waymo open dataset.
In IEEE Conf. Comput. Vis. Pattern Recog., 2020. 1


longWang,andShaliniDeMello. Open-vocabularypanop-
tic segmentation with text-to-image diffusion models. In


Xiang Bai. Side adapter network for open-vocabulary se-
mantic segmentation. In IEEE Conf. Comput. Vis. Pattern
Recog., 2023. 3

Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen,
andThomasHuang. YouTube-VOS:Sequence-to-sequence

2018. 2
[104] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely em-
bedded convolutional detection. Sensors, 2018. 1, 2

Leibe. Mask4former: Mask transformer for 4d panoptic
segmentation. In Int. Conf. Rob. Automat., 2024. 2, 5, 8
¨    ¨


Conf. Comput. Vis. Pattern Recog., 2021. 1, 2

Chen, and Chen Change Loy. Open-vocabulary sam: Seg-

In Eur. Conf. Comput. Vis., 2024. 3
[108] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and
Shih-Fu Chang. Open-vocabulary object detection using
captions. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.
3


In IEEE Conf. Comput. Vis. Pattern Recog., 2008. 2
[110] Tiantian Zhang, Zhangjun Zhou, and Jialun Pei. Evaluation
study on sam 2 for class-agnostic instance-level segmenta-
tion. arXiv preprint arXiv:2409.02567, 2024. 1

Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang
Dai, Lu Yuan, Yin Li, et al.   Regionclip: Region-based
language-image pretraining. In IEEE Conf. Comput. Vis.
Pattern Recog., 2022. 3
[112] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free
dense labels from clip. In Eur. Conf. Comput. Vis., 2022. 3

for point cloud based 3d object detection. In IEEE Conf.
Comput. Vis. Pattern Recog., 2018. 1, 2


tation. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 2,
7

Borse, Maani Ghaffari, and Fatih Porikli. 4d panoptic seg-

Int. Conf. Comput. Vis., 2023. 2, 8
[116] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin
Ma,WeiLi,HongshengLi,andDahuaLin. Cylindricaland

tion. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 2
