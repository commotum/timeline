                  Yujie Lu, Xiujun Li, William Yang Wang, and Yejin       Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma,
                    Choi. 2023b. Vim: Probing multimodal large lan-          Yann LeCun, and Saining Xie. 2024b. Eyes wide
                    guage models for visual embedded instruction fol-        shut? exploring the visual shortcomings of multi-
                    lowing. ArXiv preprint, abs/2311.17647.                  modal llms. In Proceedings of the IEEE/CVF Con-
                                                                             ference on Computer Vision and Pattern Recognition,
                  Kenneth Marino, Mohammad Rastegari, Ali Farhadi,           pages 9568–9578.
                    and Roozbeh Mottaghi. 2019. OK-VQA: A visual          YuboWang,XueguangMa,GeZhang,YuanshengNi,
                    question answering benchmark requiring external          Abhranil Chandra, Shiguang Guo, Weiming Ren,
                    knowledge. In IEEE Conference on Computer Vision         Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024.
                    and Pattern Recognition, CVPR 2019, Long Beach,          Mmlu-pro: Amorerobustandchallengingmulti-task
                    CA,USA,June16-20,2019,pages3195–3204.Com-                language understanding benchmark. ArXiv preprint,
                    puter Vision Foundation / IEEE.                          abs/2406.01574.
                  Mistral.      2024.                      Pixtral-12b.   Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
                    https://mistral.ai/news/pixtral-12b.                     Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
                  Masoud Monajatipoor, Liunian Harold Li, Mozhdeh            et al. 2022. Chain-of-thought prompting elicits rea-
                    Rouhsedaghat, Lin Yang, and Kai-Wei Chang. 2023.         soning in large language models. Advances in neural
                    MetaVL: Transferring in-context learning ability         information processing systems, 35:24824–24837.
                    from language models to vision-language models.       Sean Welleck, Amanda Bertsch, Matthew Finlayson,
                    In Proceedings of the 61st Annual Meeting of the         Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia
                    Association for Computational Linguistics (Volume        Kulikov, and Zaid Harchaoui. 2024.      From de-
                    2: Short Papers), pages 495–508, Toronto, Canada.        coding to meta-generation: Inference-time algo-
                    Association for Computational Linguistics.               rithms for large language models. arXiv preprint
                  OpenAI. 2023. Gpt-4v(ision) system card.                   arXiv:2406.16838.
                  OpenAI.2024a. Gpt-4o mini: advancing cost-efficient     Penghao Wu and Saining Xie. 2024. V*: Guided visual
                    intelligence. https://openai.com/index/gpt-4o-mini-      search as a core mechanism in multimodal llms. In
                    advancing-cost-efficient-intelligence/.                  Proceedings of the IEEE/CVF Conference on Com-
                                                                             puter Vision and Pattern Recognition, pages 13084–
                  OpenAI.      2024b.                Hello     gpt4-o.       13094.
                    https://openai.com/index/hello-gpt-4o/.               Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao,
                                                                             ShuoLiu, MengLei, Fanqing Meng, Siyuan Huang,
                  Maxime Oquab, Timothée Darcet, Théo Moutakanni,            YuQiao, and Ping Luo. 2023. Lvlm-ehub: A com-
                    HuyVo,MarcSzafraniec, Vasil Khalidov, Pierre Fer-        prehensive evaluation benchmark for large vision-
                    nandez, Daniel Haziza, Francisco Massa, Alaaeldin        language models. ArXiv preprint, abs/2306.09265.
                    El-Nouby, et al. 2023. Dinov2: Learning robust vi-    An Yang, Baosong Yang, Binyuan Hui, Bo Zheng,
                    sual features without supervision. arXiv preprint        BowenYu,ChangZhou,ChengpengLi,Chengyuan
                    arXiv:2304.07193.                                        Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2
                  Qwen.2024. Qwen2-vl: To see the world more clearly.        technical report. ArXiv preprint, abs/2407.10671.
                    https://qwenlm.github.io/blog/qwen2-vl/ .             Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang,
                  Machel Reid, Nikolay Savinov, Denis Teplyashin,            Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li,
                    Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste        Weilin Zhao, Zhihui He, et al. 2024. Minicpm-v:
                    Alayrac, RaduSoricut, Angeliki Lazaridou, Orhan Fi-      Agpt-4vlevel mllm on your phone. ArXiv preprint,
                    rat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-  abs/2408.01800.
                    locking multimodal understanding across millions of   Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming
                    tokens of context. ArXiv preprint, abs/2403.05530.       Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,
                  Gemini Team, Rohan Anil, Sebastian Borgeaud,               Pengcheng Shi, Yaya Shi, et al. 2023a. mplug-owl:
                    Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,            Modularization empowers large language models
                    Radu Soricut, Johan Schalkwyk, Andrew M Dai,             with multimodality. ArXiv preprint, abs/2304.14178.
                    Anja Hauth, et al. 2023.    Gemini: a family of       Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei
                    highly capable multimodal models. ArXiv preprint,        Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.
                    abs/2312.11805.                                          2023b. mplug-owl2: Revolutionizing multi-modal
                  Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun          large language model with modality collaboration.
                    Woo,ManojMiddepogu,SaiCharithaAkula,Jihan                ArXiv preprint, abs/2311.04257.
                    Yang,ShushengYang,AdithyaIyer,XichenPan,etal.         ShukangYin,ChaoyouFu,SiruiZhao,KeLi,XingSun,
                    2024a. Cambrian-1: A fully open, vision-centric          Tong Xu, and Enhong Chen. 2023a. A survey on
                    exploration of multimodal llms.    ArXiv preprint,       multimodal large language models. ArXiv preprint,
                    abs/2406.16860.                                          abs/2306.13549.
                                                                     15145
