                                        (a) Sequence-to-Sequence                              (b) Ptr-Net
                           Figure 1: (a) Sequence-to-Sequence - An RNN (blue) processes the input sequence to create a code
                           vector that is used to generate the output sequence (purple) using the probability chain rule and
                           another RNN. The output dimensionality is ﬁxed by the dimensionality of the problem and it is the
                           sameduringtraining and inference [1]. (b) Ptr-Net - An encoding RNN converts the input sequence
                           to a code (blue) that is fed to the generating network (purple). At each step, the generating network
                           produces a vector that modulates a content-based attention mechanism over inputs ([5, 2]). The
                           output of the attention mechanism is a softmax distribution with dictionary size equal to the length
                           of the input.
                           ion (i.e., when we only have examples of inputs and desired outputs). The proposed approach is
                           depicted in Figure 1.
                           Themaincontributions of our work are as follows:
                                 • Wepropose a new architecture, that we call Pointer Net, which is simple and effective. It
                                   deals with the fundamental problem of representing variable length dictionaries by using a
                                   softmax probability distribution as a “pointer”.
                                 • WeapplythePointerNetmodeltothreedistinctnon-trivialalgorithmicproblemsinvolving
                                   geometry. We show that the learned model generalizes to test problems with more points
                                   than the training problems.
                                 • Our Pointer Net model learns a competitive small scale (n ≤ 50) TSP approximate solver.
                                   Ourresults demonstrate that a purely data driven approach can learn approximate solutions
                                   to problems that are computationally intractable.
                           2   Models
                           Wereviewthesequence-to-sequence[1]andinput-attentionmodels[5]thatarethebaselinesforthis
                           workinSections 2.1 and 2.2. We then describe our model - Ptr-Net in Section 2.3.
                           2.1  Sequence-to-Sequence Model
                           Given a training pair, (P,CP), the sequence-to-sequence model computes the conditional probabil-
                           ity p(CP|P;θ) using a parametric model (an RNN with parameters θ) to estimate the terms of the
                           probability chain rule (also see Figure 1), i.e.
                                                                  m(P)
                                                    p(CP|P;θ) = Y p (C |C ,...,C           , P;θ).                      (1)
                                                                        θ   i  1       i−1
                                                                   i=1
                                                                          2
