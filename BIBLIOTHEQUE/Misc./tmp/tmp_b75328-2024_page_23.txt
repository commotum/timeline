                          Table 3: Number of parameters, to the nearest million, in a model with Abacus Embeddings and input
                          injection.
                                          Layers in Recurrent Block   Recurrences   Parameters (Millions)
                                                     16                    1                 122
                                                      8                    2                 64
                                                      4                    4                 34
                                                      2                    8                 19
                                                      1                   16                 12
                                         Table 4: Default number of Nvidia GPU hours used to train a model.
                                Dataset          NumberofGPUHours(training)        NumberofGPUHours(testing)
                                Addition                 24-RTXA4000                         65.8 - V100
                                Bitwise OR                1 - RTXA4000                        45-V100
                                Sorting                  24-RTXA4000                       64-RTXA4000
                                Multiplication          192-RTXA4000                      0.83 - RTXA4000
                          addition as they are reused. To fully implement our methodology all numbers also have to be reversed,
                          this can be implemented with simple regular expressions on all inputs and outputs.
                          Weuseacharacter level tokenizer for all experiments and greedy decoding in all testing. We train all
                          modelswithalocalbatchsizewhichisthemaximumbatchsizethatisapoweroftwothatwillfitinto
                          the sixteen gigabytes of GPU memory. For multiplication models we first take the mean loss across
                          samples before taking the mean across all samples in a batch, instead of taking the mean loss across
                          all token in a batch; we find this leads to slightly more stable training. We note that training models
                          to solve multiplication requires more hyperparameter tuning than addition, perhaps implying it is a
                          trickier task to learn. Also, FIRE models require a much greater compute budget for hyperparameter
                          search as compared to Abacus models for multiplication. In Table 3, we present the approximate
                          parameter counts for models trained with input injection and Abacus Embeddings.
                          Compute Usage. We detail the default use of GPUs for each experiment in Table 4. For some
                          experiments, such as extreme length generalization (Figure 10) and index hints (Figure 21) more
                          GPUhoursarerequiredfortesting, these are included in the total number of GPU hours used. Our
                          testing pipeline for addition and Bitise OR uses Nvidia V100 GPUs. Due to a technical problem,
                          ‘torch.compile’ cannot be used on the V100 GPUs we use, therefore others may be able to reduce
                          this compute time in future studies. All compute was provided by internal resources. During the
                          exploratory phase of this project, we used more GPU hours to test and design the experiments shown,
                          using approximately 1.5 terabytes of storage of the entire project. An estimate of the total compute
                          required for all of the results presented in the main paper is 10,039 GPU hours. The appendix results
                          require a further 18,278 GPU hours.
                          A.8.1   Hyperparameters
                          Wedetail what we believe to be an important subset of the default hyperparameter values in Table
                          5. A full list of all hyperparameters and model configurations is contained in the code release. For
                          multiplication models with FIRE embeddings, the learning rate is 0.00006, due to large instabilities
                          in higher learning rates which were not experienced for the Abacus Embeddings.
                          A.8.2   CodeRelease
                          Wewillrelease all code and datasets on GitHub with an MIT License.
                                                                        23
