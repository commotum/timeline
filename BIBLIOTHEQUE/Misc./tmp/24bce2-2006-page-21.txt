             AFastLearningAlgorithmforDeepBeliefNets                     1547
             posteriors—has been replaced by the constraint that the prior must make
             the variational approximation exact.
               Aftereachlayerhasbeenlearned,itsweightsareuntiedfromtheweights
             in higher layers. As these higher-level weights change, the priors for lower
             layers cease to be complementary, so the true posterior distributions in
             lower layers are no longer factorial, and the use of the transpose of the
             generative weights for inference is no longer correct. Nevertheless, we can
             use a variational bound to show that adapting the higher-level weights
             improvestheoverallgenerativemodel.
               To demonstrate the power of our fast, greedy learning algorithm, we
             used it to initialize the weights for a much slower ﬁne-tuning algorithm
             that learns an excellent generative model of digit images and their labels. It
             isnotclearthatthisisthebestwaytousethefast,greedyalgorithm.Itmight
             bebettertoomittheﬁne-tuningandusethespeedofthegreedyalgorithm
             to learn an ensemble of larger, deeper networks or a much larger training
             set. The network in Figure 1 has about as many parameters as 0.002 cubic
             millimeters of mouse cortex (Horace Barlow, personal communication,
             1999), and several hundred networks of this complexity could ﬁt within
             asingle voxel of a high-resolution fMRI scan. This suggests that much big-
             ger networks may be required to compete with human shape recognition
             abilities.
               Ourcurrentgenerativemodelislimitedinmanyways(Lee&Mumford,
             2003).It is designedforimagesinwhichnonbinaryvaluescanbetreatedas
             probabilities (which is not the case for natural images); its use of top-down
             feedbackduringperceptionislimitedtotheassociativememoryinthetop
             two layers; it does not have a systematic way of dealing with perceptual
             invariances;itassumesthatsegmentationhasalreadybeenperformed;and
             it does not learn to sequentially attend to the most informative parts of
             objects whendiscriminationisdifﬁcult.Itdoes,however,illustratesomeof
             the major advantages of generative models as compared to discriminative
             ones:
                 Generative models can learn low-level features without requir-
                  ing feedback from the label, and they can learn many more
                  parameters than discriminative models without overﬁtting. In dis-
                  criminative learning, each training case constrains the parameters
                  only by as many bits of information as are required to specify
                  the label. For a generative model, each training case constrains
                  the parameters by the number of bits required to specify the
                 input.
                  It is easy to see what the network has learned by generating from its
                 model.
                  It is possible to interpret the nonlinear, distributed representations in
                  the deep hidden layers by generating images from them.
