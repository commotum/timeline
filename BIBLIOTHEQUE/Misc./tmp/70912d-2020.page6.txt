                                                                  Generative Pretraining from Pixels
                Table 2. Comparing linear probe accuracies between our models          Table 3. Comparing ﬁne-tuning performance between our models
                and state-of-the-art self-supervised models. A blank input resolu-     and state-of-the-art models utilizing supervised ImageNet transfer.
                tion (IR) corresponds to a model working at standard ImageNet          Wealsoinclude AutoAugment, the best performing model trained
                resolution. We report the best performing conﬁguration for each        end-to-end on CIFAR. Table results: AutoAugment (Cubuk et al.,
                contrastive method, ﬁnding that our models achieve comparable          2019), SimCLR (Chen et al., 2020), GPipe (Huang et al., 2019),
                performance.                                                           EfﬁcentNet (Tan & Le, 2019)
                     Method           IR     Params (M)      Features    Acc               Model             Acc     UnsupTransfer      SupTransfer
                     Rotation        orig.        86          8192      55.4               CIFAR-10
                                      2                                                    AutoAugment       98.5
                     iGPT-L         32 ·3        1362         1536      60.3               SimCLR            98.6          √
                     BigBiGAN        orig.        86          8192      61.3                                                                 √
                                      2                                                    GPipe             99.0
                     iGPT-L         48 ·3        1362         1536      65.2               iGPT-L            99.0          √
                     AMDIM           orig.       626          8192      68.1
                     MoCo            orig.       375          8192      68.6
                                      2                                                    CIFAR-100
                     iGPT-XL        64 ·3        6801         3072      68.7               iGPT-L            88.5          √
                     SimCLR          orig.        24          2048      69.3               SimCLR            89.0          √
                     CPCv2           orig.       303          8192      71.5
                                      2                                                    AutoAugment       89.3
                     iGPT-XL        64 ·3        6801         15360     72.0               EfﬁcientNet       91.7                            √
                     SimCLR          orig.       375          8192      76.5
                Machieves 54.5% accuracy and iGPT-S achieves 41.9%                     onthese datasets, though we do not use sophisticated data
                accuracy.                                                              augmentationtechniques. In fact, 99.0% ties GPipe, the best
                                                                                       model which pre-trains using ImageNet labels.
                Theﬁrst obvious optimization is to increase MR while stay-             OnImageNet,weachieve66.3%accuracyafter ﬁne-tuning
                                                                                2
                ing within accelerator memory limits. With a MR of 48 ,                           2
                iGPT-L achieves a best-layer accuracy of 65.2% using 1536              at MR32 ,abumpof6%overlinearprobing. Whenﬁne-
                                                                                                         2
                features and with a MR of 642, iGPT-XL achieves a best-                tuning at MR 48 , we achieve 72.6% accuracy, with a simi-
                layer accuracy of 68.7% using 3072 features.                           lar 7% bump over linear probing. However, our models still
                                                                                       slightly underperform Isometric Neural Nets (Sandler et al.,
                Since contrastive methods report their best results on 8192            2019), which achieves 70.2% at an IR of 282 × 3.
                features, we would ideally evaluate iGPT with an embed-                Finally, as a baseline for ImageNet ﬁne-tuning, we train
                dingdimension8192forcomparison. Trainingsuchamodel                     the classiﬁcation objective from a random initialization. At
                is prohibitively expensive, so we instead concatenate fea-                     2
                tures from multiple layers as an approximation. However,               MR48 , a model with tuned learning rate and dropout
                our features tend to be correlated across layers, so we need           achieves 53.2% after 18 epochs, 19.4% worse than the pre-
                moreofthemtobecompetitive. If we concatenate features                  trained model. Comparatively, the pre-trained model is
                from 5 layers centered at the best single layer of iGPT-XL,            muchquickertoﬁne-tune, achieving the same 53.2% loss
                we achieve an accuracy of 72.0% using 15360 features,                  in roughly a single epoch.
                which is competitive with recent contrastive learning ap-              When ﬁne-tuning, it is important to search over learning
                proaches (Table 2). Note that we require more parameters               rates again, as the optimal learning rate on the joint training
                and compute to achieve this accuracy, but we work at low               objective is often an order of magnitude smaller than that
                resolution and without utilizing knowledge of the 2D input             for pre-training. We also tried regularizing with dropout,
                structure.                                                             though we did not observe any clear beneﬁts. It is easy to
                4.5. Full Fine-tuning                                                  overﬁt the classiﬁcation objective on small datasets, so we
                                                                                       employ early stopping based on validation accuracy.
                Toachieve even higher accuracy on downstream tasks, we                 4.6. BERT
                adapt the entire model for classiﬁcation through ﬁne-tuning.
                Building off of the previous analysis, we tried attaching the          Given the success of BERT in language, we train iGPT-L
                classiﬁcation head to the layer with the best representations.         at an input resolution of 322 × 3 and a model resolution
                                                                                             2
                Though this setup trains faster than one with the head at-             of 32 (Figure 4). On CIFAR-10, we observe that linear
                tached at the end, the latter is able to leverage greater model        probe accuracy at every layer is worse than that of the auto-
                depth and eventually outperforms.                                      regressive model, with best-layer performance more than
                On CIFAR-10, iGPT-L achieves 99.0% accuracy and on                     1%lower. Best-layer accuracy on ImageNet is 6% lower.
                CIFAR-100, it achieves 88.5% accuracy after ﬁne-tuning.                However, during ﬁne-tuning, BERT makes up much of this
                Weoutperform AutoAugment, the best supervised model                    gap. A fully ﬁne-tuned CIFAR-10 model achieves 98.6%
