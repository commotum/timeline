                                                Stabilizing Equilibrium Models by Jacobian Regularization
               Kelly, J., Bettencourt, J., Johnson, M. J., and Duvenaud, D.    Peaceman, D. W. and Rachford, Jr, H. H. The numerical
                 Learning differential equations that are easy to solve. In      solution of parabolic and elliptic differential equations.
                 Neural Information Processing Systems, 2020.                    Journal of the Society for Industrial and Applied Mathe-
               Krantz, S. G. and Parks, H. R. The implicit function theorem:     matics, 3(1):28–41, 1955.
                 History, theory, and applications. Springer, 2012.            Poli, M., Massaroli, S., Yamashita, A., Asama, H., and Park,
               Krizhevsky, A. and Hinton, G. Learning multiple layers of         J. Hypersolvers: Toward fast continuous-depth models.
                 features from tiny images. Technical report, University         arXiv:2007.09601, 2020.
                 of Toronto, 2009.                                             Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
               Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet         Sutskever, I. Languagemodelsareunsupervisedmultitask
                 classiﬁcation with deep convolutional neural networks.          learners. OpenAI blog, 1(8):9, 2019.
                 In Neural Information Processing Systems, 2012.               Revay, M., Wang, R., and Manchester, I. R. Lipschitz
               Linsley, D., Ashok, A. K., Govindarajan, L. N., Liu, R., and      bounded equilibrium networks. arXiv:2010.01732, 2020.
                 Serre, T. Stable and expressive recurrent vision models.      Roosta-Khorasani, F. and Ascher, U. Improved bounds on
                 arXiv:2005.11362, 2020.                                         sample size for implicit matrix trace estimators. Founda-
                                                                                 tions of Computational Mathematics, 15(5):1187–1212,
               Liu, L., Liu, X., Gao, J., Chen, W., and Han, J.                  2015.
                 Understanding the difﬁculty of training transformers.
                 arXiv:2004.08249, 2020.                                       Salimans, T. and Kingma, D. P. Weight normalization: A
                                                                                 simple reparameterization to accelerate training of deep
               Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient           neural networks. In Neural Information Processing Sys-
                 descent with warm restarts. In International Conference         tems, 2016.
                 onLearning Representations (ICLR), 2017.
                                                                               Santoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski,
               Lu, C., Chen, J., Li, C., Wang, Q., and Zhu, J. Implicit nor-     M., Weber, T., Wierstra, D., Vinyals, O., Pascanu, R., and
                 malizing ﬂows. In International Conference on Learning          Lillicrap, T. Relational recurrent neural networks. In
                 Representations (ICLR), 2021.                                   Neural Information Processing Systems, 2018.
               Marcus, M. P., Marcinkiewicz, M. A., and Santorini, B.          Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,
                 Building a large annotated corpus of English: The Penn          J., and Catanzaro, B.     Megatron-lm: Training multi-
                 treebank. Computational Linguistics, 19(2), 1993.               billion parameter language models using model paral-
               Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer       lelism. arXiv:1909.08053, 2019.
                 sentinel mixture models. In International Conference on              ´
                                                                               Sokolic, J., Giryes, R., Sapiro, G., and Rodrigues, M. R.
                 Learning Representations (ICLR), 2017.                          Robust large margin deep neural networks. IEEE Trans-
               Merity, S., Keskar, N. S., and Socher, R. Regularizing and        actions on Signal Processing, 65(16):4265–4280, 2017.
                 optimizing LSTM language models. In International             Ubaru, S., Chen, J., and Saad, Y. Fast estimation of tr(f(a))
                 Conference on Learning Representations (ICLR), 2018.            via stochastic lanczos quadrature. SIAM Journal on Ma-
               Meyer, R. A., Musco, C., Musco, C., and Woodruff, D. P.           trix Analysis and Applications, 38(4):1075–1099, 2017.
                 Hutch++: Optimal stochastic trace estimation. In Sympo-       Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
                 sium on Simplicity in Algorithms (SOSA), 2021.                  L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
               Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spec-        tion is all you need. In Neural Information Processing
                 tral normalization for generative adversarial networks. In      Systems, 2017.
                 International Conference on Learning Representations          vonMises, R. and Pollaczek-Geiringer, H. Praktische ver-
                 (ICLR), 2018.                                                                              ¨
                                                                                 fahren der gleichungsauﬂosung. ZAMM-Journal of Ap-
                                                                                 plied Mathematics and Mechanics/Zeitschrift fur Ange-
               Novak, R., Bahri, Y., Abolaﬁa, D. A., Pennington, J., and                                                          ¨
                 Sohl-Dickstein, J. Sensitivity and generalization in neural     wandte Mathematik und Mechanik, 9(1):58–77, 1929.
                 networks: An empirical study. arXiv:1802.08760, 2018.         Winston,E.andKolter,J.Z. Monotoneoperatorequilibrium
               Pabbaraju, C., Winston, E., and Kolter, J. Z. Estimating Lip-     networks. In Neural Information Processing Systems,
                 schitz constants of monotone deep equilibrium models.           2020.
                 In International Conference on Learning Representations       Wu, Y. and He, K. Group normalization. In European
                 (ICLR), 2021.                                                   Conference on Computer Vision (ECCV), 2018.
