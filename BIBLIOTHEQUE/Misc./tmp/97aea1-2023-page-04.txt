image
encoder

image

«score
al mask decoder
a , score
‘conv prompt encoder
image i t t , score
embedding ™k points box text

valid masks

Figure 4: Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can
then be efficiently queried by a variety of input prompts to produce object masks at amortized real-time speed. For ambiguous
prompts corresponding to more than one object, SAM can output multiple valid masks and associated confidence scores.

3. Segment Anything Model

‘We next describe the Segment Anything Model (SAM)
for promptable segmentation. SAM has three components,
illustrated in Fig. 4: an image encoder, a flexible prompt
encoder, and a fast mask decoder. We build on Transformer
vision models [13, 32, 19, 60] with specific tradeoffs for
(amortized) real-time performance. We describe these com-
ponents at a high-level here, with details in §B.

Image encoder. Motivated by scalability and powerful pre-
training methods, we use an MAE [46] pre-trained Vision
Transformer (ViT) [32] minimally adapted to process high
resolution inputs [60]. The image encoder runs once per
image and can be applied prior to prompting the model.

Prompt encoder. We consider two sets of prompts: sparse
(points, boxes, text) and dense (masks). We represent
points and boxes by positional encodings [93] summed with
learned embeddings for each prompt type and free-form text
with an off-the-shelf text encoder from CLIP [80]. Dense
prompts (i.e., masks) are embedded using convolutions and
summed element-wise with the image embedding.

Mask decoder. The mask decoder efficiently maps the im-
age embedding, prompt embeddings, and an output token
to a mask. This design, inspired by [13, 19], employs a
modification of a Transformer decoder block [101] followed
by a dynamic mask prediction head. Our modified decoder
block uses prompt self-attention and cross-attention in two
directions (prompt-to-image embedding and vice-versa) to
update all embeddings. After running two blocks, we up-
sample the image embedding and an MLP maps the output
token to a dynamic linear classifier, which then computes
the mask foreground probability at each image location.

Resolving ambiguity. With one output, the model will av-
erage multiple valid masks if given an ambiguous prompt.
To address this, we modify the model to predict multiple
output masks for a single prompt (see Fig. 3). We found
3 mask outputs is sufficient to address most common cases
(nested masks are often at most three deep: whole, part, and
subpart). During training, we backprop only the minimum

loss [14, 44, 62] over masks. To rank masks, the model pre-
dicts a confidence score (i.e., estimated IoU) for each mask.

Efficiency. The overall model design is largely motivated
by efficiency. Given a precomputed image embedding, the
prompt encoder and mask decoder run in a web browser, on
CPU, in ~50ms. This runtime performance enables seam-
less, real-time interactive prompting of our model.

Losses and training. We supervise mask prediction with
the linear combination of focal loss [63] and dice loss [71]
used in [13]. We train for the promptable segmentation task
using a mixture of geometric prompts (for text prompts see
§06.2). Following [90, 36], we simulate an interactive setup
by randomly sampling prompts in 11 rounds per mask, al-
lowing SAM to integrate seamlessly into our data engine.

4. Segment Anything Data Engine

As segmentation masks are not abundant on the inter-
net, we built a data engine to enable the collection of our
1.1B mask dataset, SA-1B. The data engine has three
stages: (1) a model-assisted manual annotation stage, (2) a
semi-automatic stage with a mix of automatically predicted
masks and model-assisted annotation, and (3) a fully auto-
matic stage in which our model generates masks without
annotator input. We go into details of each next.

Assisted-manual stage. In the first stage, resembling clas-
sic interactive segmentation, a team of professional annota-
tors labeled masks by clicking foreground / background ob-
ject points using a browser-based interactive segmentation
tool powered by SAM. Masks could be refined using pixel-
precise “brush” and “eraser” tools. Our model-assisted an-
notation runs in real-time directly inside a browser (using
precomputed image embeddings) enabling a truly interac-
tive experience. We did not impose semantic constraints for
labeling objects, and annotators freely labeled both “stuff”
and “things” [1]. We suggested annotators label objects
they could name or describe, but did not collect these names
or descriptions. Annotators were asked to label objects in
order of prominence and were encouraged to proceed to the
next image once a mask took over 30 seconds to annotate.

4019
