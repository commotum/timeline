                                                                               Point Primitive Transformer       3
                                in Figure 1. On the lower level, PPTr extracts short-term spatial-temporal fea-
                                tures through an intra-primitive point transformer for a short video clip around
                                the frame of interest. Primitive planes are used to restrict the spatial support of
                                attention maps in a point-level transformer. Such geometry-aware locality induc-
                                tive bias is not only beneficial for the optimization of the transformer but also
                                very effective for extracting descriptive and temporally stable geometric features.
                                On the upper level, PPTr extracts long-term spatial-temporal features through
                                a primitive transformer. We allow very efÏcient consideration of a long sequence
                                by fitting primitives and computing the primitive features in a pre-processing
                                stage. Through the primitive transformer, we could better associate primitives
                                from different frames and effectively integrate long-term context to the frame of
                                interest.
                                     Weevaluate our Point Primitive Transformer(PPTr) on several tasks, such
                                as 3D action recognition on MSR-Action [25] and 4D semantic segmentation on
                                Synthia4D [34] and HOI4D [29]. we demonstrate significant improvements over
                                previous method(+1.33% mIoU on synthia4D, +6.28% mIoU on HOI4D and
                                +1.39% accuracy on MSR-Action).
                                     The contributions of this paper are fourfold:
                                  – First, we leverage the primitive plane to capture the long-term spatial-
                                    temporal context in 4D point cloud videos and propose a novel backbone
                                    named Point Primitive Transformer(PPTr).
                                  – Second, we propose an intra-primitive point transformer for extracting spa-
                                    tially descriptive and temporally stable short-term geometric features.
                                  – Third, we propose a primitive transformer to capture long-term spatial-
                                    temporal features efÏciently.
                                  – Fourth, extensive experiments on three datasets show that the proposed
                                    Point Primitive Transformer is more effective and efÏcient than previous
                                    state-of-the-art 4D backbones.
                                2    Related Work
                                DeeplearningonPointCloudVideoProcessing.Differentfromgrid-based
                                RGB video, point cloud video exhibits irregularities and lacks order along the
                                spatial dimension where points emerge inconsistently across time. One approach
                                to deal with that is voxilization. For instance, [6] extends temporal dimension to
                                3Dsparse convolution [15] to extract spatial temporal features on 4D occupancy
                                grids. 3DV [41] proposes a 3D motion representation to encode 3D motion infor-
                                mation via temporal rank pooling [12]. Another approach is to perform directly
                                on point sets. MeteorNet [28] adopts PointNet++ [32] to aggregate information
                                from neighbors, while point-track is needed to merge points. PSTNet [11] firstly
                                decomposes spatial and temporal information and proposes a point-based convo-
                                lution in a hierarchical manner. Following [11] [28], P4Transformer [10] proposes
                                4D Convolution that performs spatial-temporal convolution and captures dy-
                                namics of points by self-attention. While like most point-based approaches, they
                                prolong input clip by simply feeding raw points into network, which suffers from
