                                A Hyperparameters
                                All hyperparameters for our experiments are given in Table 3.
                                  Model                               GPQA AIME24 AIME25-I AIME25-II Top-p Temp.
                                  Deepseek                              16K         32K            32K             32K           0.95       0.6
                                  R1                                    32K         32K            32K             32K           0.95       0.6
                                  QwQ                                   32K         32K            32K             32K           0.95       0.6
                                  R1-Distill-Qwen                       32K         32K            32K             32K           0.95       0.6
                                  GPT-OSS-120B                          8K           8K             8K              8K           0.95       0.6
                                  Qwen3-235B                            5K           5K             5K              5K           0.95       0.6
                                  Qwen3                                 16K         32K            32K             32K           0.95       0.6
                                  Dapo-Qwen-32B                       10.1K        20.5K          20.5K           20.5K          0.7        1.0
                                                                   Global settings (shared across all models)
                                  Beamwidth                                                                8
                                  Samples(n,MV/LFS/FFS)                                                    8
                                  Answer-reserveforBF                                                     3K
                                Table 3: Decoding hyperparameters used in all experiments across models and datasets.
                                Identical values across datasets are shown once.
                                B Results
                                  Metric      BS MV LFS FFS              Metric        BS   MV LFS FFS          Metric        BS    MV LFS FFS
                                  Seq. tokens  –  9.2k  9.2k 3.0k        Seq. tokens  6.3k 13.3k 13.3k 0.8k     Seq. tokens  4.4k  5.0k  5.0k 3.0k
                                  Total tokens – 45.4k 45.4k 4.3k        Total tokens 50.3k 31.6k 31.6k 0.9k    Total tokens 34.8k 32.0k 32.0k 5.9k
                                  GPQA         –  54.0  53.5 55.1        GPQA         56.6  58.1  53.0 48.5     GPQA          71.7  72.2  66.2 66.2
                                  AIME24       –  53.3  46.7 53.3        AIME24       26.7  43.3  36.7 26.7     AIME24        73.3  80.0  80.0 76.7
                                  AIME25-I     –  46.7  40.0 33.3        AIME25-I     26.7  46.7  33.3 33.3     AIME25-I      60.0  80.0  73.3 66.7
                                  AIME25-II    –  46.7  33.3 40.0        AIME25-II    20.0  20.0  26.7 13.3     AIME25-II     80.0  86.7  86.7 86.7
                                     (a) Dapo-Qwen-32B                        (b) Deepseek-Chat                       (c) GPT-OSS-120B
                                 Metric        BS    MV LFS FFS          Metric        BS   MV LFS FFS          Metric        BS    MV LFS FFS
                                 Seq. tokens 13.4k 18.3k 18.3k 9.6k      Seq. tokens 12.4k 19.3k 19.3k 3.5k     Seq. tokens  4.1k  5.7k  5.7k   3.8k
                                 Total tokens 107k 106k 106k 10.4k       Total tokens 98.9k 89.6k 89.6k 3.9k    Total tokens 33.1k 35.8k 35.8k 16.7k
                                 GPQA         68.2  66.7  61.6   66.2    GPQA         69.2  69.2  66.7 63.1     GPQA          66.7  70.7  64.1  71.2
                                 AIME24       76.7  83.3  76.7   80.0    AIME24       83.3  90.0  83.3 40.0     AIME24        33.3  83.3  83.3  83.3
                                 AIME25-I     60.0  73.3  53.3   60.0    AIME25-I     66.7  73.3  73.3 40.0     AIME25-I      40.0  53.3  53.3  53.3
                                 AIME25-II    60.0  86.7  66.7   80.0    AIME25-II    80.0  86.7  73.3 40.0     AIME25-II     13.3  40.0  40.0  40.0
                                          (d) QwQ-32B                           (e) Qwen3-32B                          (f) Qwen3-235B
                                                     Metric        BS    MV LFS FFS          Metric        BS   MV LFS FFS
                                                     Seq. tokens   8.7k 14.6k 14.6k 6.4k     Seq. tokens 12.2k 17.9k 17.9k 6.0k
                                                     Total tokens 69.8k 78.5k 78.5k 6.7k     Total tokens 97.7k 87.9k 87.9k 6.6k
                                                     GPQA          72.2  74.2  71.7 73.7     GPQA         59.6  64.6  60.1 52.5
                                                     AIME24        70.0  83.3  70.0 86.7     AIME24       60.0  80.0  53.3 83.3
                                                     AIME25-I      60.0  73.3  53.3 66.7     AIME25-I     46.7  60.0  46.7 53.3
                                                     AIME25-II     73.3  86.7  46.7 73.3     AIME25-II    66.7  60.0  53.3 40.0
                                                                  (g) R1                         (h) R1-Distill-Qwen
                                                                                            3
                                Table 4: Accuracy (%) and compute cost (×10 tokens) across models. For each method,
                                token counts are dataset-averaged. Bold, gray cells mark the best value per row. Meth-
                                ods shown are beam search (BS), majority voting (MV), first finish search (FFS), last finish
                                search (LFS).
                                Overall performance patterns.                The four decoding strategies exhibit complementary
                                trade-offs. MVisthemostconsistentaccuracy-orientedmethodbutincursthelargesttoken
                                costs (often an order of magnitude higher than FFS). BS occasionally attains top accuracy
                                                                                         12
