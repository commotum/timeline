                                             REALM:Retrieval-AugmentedLanguageModelPre-Training
              Table 1. Test results on Open-QA benchmarks. The number of train/test examples are shown in paretheses below each benchmark.
              Predictions are evaluated with exact match against any reference answer. Sparse retrieval denotes methods that use sparse features such
              as TF-IDFand BM25. Our model, REALM,outperforms all existing systems.
              Name                                  Architectures                 Pre-training      NQ        WQ        CT      #params
                                                                                                  (79k/4k)   (3k/2k)  (1k /1k)
              BERT-Baseline (Lee et al., 2019)      Sparse Retr.+Transformer      BERT              26.5      17.7      21.3        110m
              T5(base) (Roberts et al., 2020)       Transformer Seq2Seq           T5(Multitask)     27.0      29.1       -          223m
              T5(large) (Roberts et al., 2020)      Transformer Seq2Seq           T5(Multitask)     29.8      32.2       -          738m
              T5(11b) (Roberts et al., 2020)        Transformer Seq2Seq           T5(Multitask)     34.5      37.4       -        11318m
              DrQA(Chenetal.,2017)                  Sparse Retr.+DocReader        N/A                -        20.7      25.7         34m
              HardEM(Minetal., 2019a)               Sparse Retr.+Transformer      BERT              28.1       -         -          110m
              GraphRetriever (Min et al., 2019b)    GraphRetriever+Transformer    BERT              31.8      31.6       -          110m
              PathRetriever (Asai et al., 2019)     PathRetriever+Transformer     MLM               32.6       -         -          110m
              ORQA(Leeetal.,2019)                   Dense Retr.+Transformer       ICT+BERT          33.3      36.4      30.1        330m
              Ours (X = Wikipedia, Z = Wikipedia)   Dense Retr.+Transformer       REALM             39.2      40.2      46.8        330m
              Ours (X = CC-News, Z = Wikipedia)     Dense Retr.+Transformer       REALM             40.4      40.7      42.9        330m
                                                                             AsreportedintheconcurrentworkofRobertset al.(2020),
                  Table 2. Ablation experiments on NQ’s development set.     the generative Open-QA systems based on T5 are surpris-
                                                              Zero-shot      ingly powerful, with the largest T5-11B model outperform-
                Ablation                             Exact    Retrieval      ing the previous best Open-QA system. Increasing the size
                                                    Match     Recall@5       of T5 yields consistent improvement, but comes at signif-
                REALM                                 38.2      38.5         icant computational cost (from Base to 11B, the model is
                REALMretriever+Baseline encoder       37.4      38.5         50timeslarger,and gains roughly5 points in accuracy). In
                Baseline retriever+REALMencoder       35.3      13.9         contrast, REALM outperforms the largest T5-11B model
                Baseline (ORQA)                       31.3      13.9         while being 30 times smaller. It is also important to note
                REALMwithrandomuniformmasks           32.3      24.2         that T5 accesses additional reading comprehension data
                REALMwithrandomspanmasks              35.3      26.1         from SQuAD during its pre-training (100,000+ examples).
                30×stale MIPS                         28.7      15.1         AccesstosuchdatacouldalsobeneﬁtREALM,butwasnot
                                                                             used in our experiments.
                                                                             Among all systems, the most direct comparison with
              entire model can be run on a single machine with a 12GB        REALMisORQA(Leeetal.,2019),wheretheﬁne-tuning
              GPU.                                                           setup, hyperparametersand training data are identical. The
                                                                             improvementof REALMoverORQAispurelyduetobet-
              Pre-training    We pre-train for 200k steps on 64 Google       ter pre-training methods. The results also indicate that our
              Cloud TPUs, with a batch size of 512 and a learning rate       methodofpre-trainingcanbeappliedbothon(1)thesingle-
              of 3e-5, using BERT’s default optimizer. The document          corpus setting (X = Wikipedia, Z = Wikipedia), or (2) the
              embeddingstep for the MIPS index is parallelized over 16       separate-corpus setting (X = CC-News, Z = Wikipedia).
              TPUs. For each example, we retrieve and marginalize over       Compared to other retrieval-based systems (Asai et al.,
              8candidate documents,including the null document∅.             2019; Min et al., 2019a;b) which often retrieve from 20 to
              Weexperimentwith two choices of the pre-training corpus        80documents,oursystemgetstheoverallbestperformance
              X: (1) Wikipedia, which is identical to the knowledge cor-     while only retrieving 5 documents.
              pusZ,and(2)CC-News,ourreproductionofthecorpusof
              English news proposed by Liu et al. (2019).                    4.5. Analysis
              4.4. Main results                                              In Table 2 we present results for NaturalQuestions-Open
                                                                             after ablating critical components of REALM. In addition
              Table 1 shows the accuracy of different approaches on the      to the end-to-end results, we also report how often the gold
              three Open-QA datasets. REALM outperform all previous          answer appears in the top-5 retrievals before applying any
              approachesby a signiﬁcant margin. Table 1 also shows the       ﬁne-tuning. Thelattermetricmoresigniﬁcantlyisolatesthe
              numberofparametersfor each model.                              contribution of improving the retriever during pre-training.
