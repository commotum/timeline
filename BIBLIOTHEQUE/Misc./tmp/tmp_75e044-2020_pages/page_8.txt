                                                   REALM:Retrieval-AugmentedLanguageModelPre-Training
                Table 3. An example where REALM utilizes retrieved documents to better predict masked tokens. It assigns much higher probability
                (0.129) to the correct term, “Fermat”, compared to BERT. (Note that the blank corresponds to 3 BERT wordpieces.)
                          x:    Anequilateral triangle is easily constructed using a straightedge and compass, because 3 is a     prime.
                                                                       −14
                (a)   BERT      p(y = “Fermat”|x)         = 1.1×10            (Noretrieval.)
                (b) REALM       p(y = “Fermat”|x,z) = 1.0                     (Conditional probability with document z =“257 is ... a Fermat prime.
                                                                              Thus a regular polygon with 257 sides is constructible with compass ...”)
                (c) REALM       p(y = “Fermat”|x)         = 0.129             (Marginal probability, marginalizing over top 8 retrieved documents.)
                EncoderorRetriever Weﬁrstaimtodeterminewhether                         5. Discussion and Related Work
                REALMpre-trainingimprovestheretrieveror theencoder,
                or both. To do so, we can reset the parameters of either               We previously discussed related methods for Open-QA.
                the retriever or the encoder to their baseline state before            HerewepresentseveralalternatewaysofviewingREALM
                REALMpre-training,andfeedthatintoﬁne-tuning. Reset-                    that connect it to a broader set of ideas beyond Open-QA:
                ting both the retriever and encoder reduces the system to
                our main baseline, ORQA. We ﬁnd that both the encoder                  Language modeling with corpus as context               Language
                and retriever beneﬁt from REALM training separately, but               representation models have been incorporating contexts of
                the best result requires both components acting in unison.             increasingly large scope when making predictions.             Ex-
                                                                                       amples of this progression include models that condi-
                                                                                       tion on surrounding words (Mikolov et al., 2013a;b), sen-
                                                                                       tences (Kiros et al., 2015; Peters et al., 2018), and para-
                Masking scheme We compare our salient span masking                     graphs (Radford et al., 2018; Devlin et al., 2018). We can
                scheme (Section 3.4) with (1) random token masking in-                 view REALMasageneralizationof the above work to the
                troduced in BERT (Devlin et al., 2018) and (2) random                  next level of scope: the entire text corpus.
                span masking proposed by SpanBERT (Joshi et al., 2019).
                While such salient span masking has not been shown to                  Retrieve-and-edit with learned retrieval            In order to
                be impactful in previous work with standard BERT train-                better explain the variance in the input text and en-
                ing(Joshi et al., 2019), it is crucial for REALM. Intuitively,         able controllable generation, Guu et al. (2018) proposed
                the latent variablelearning relies heavily on the utility of re-       a language model with the retrieve-and-edit frame-
                trieval and is therefore more sensitive to a consistent learn-         work (Hashimotoet al., 2018) that conditions on text with
                ing signal.                                                            high lexical overlap. REALM has a similar approach, ex-
                                                                                       cept that the model learns for itself which texts are most
                                                                                       useful for reducing perplexity. By jointly learning the re-
                MIPSindex refresh rate          During pre-training, we run a          triever, REALM has the capacity to depend on information
                parallel process to re-embed corpus documents and rebuild              beyondlexical overlap.
                the MIPS index. This results in one index refresh per ap-
                proximately 500 training steps. To demonstrate the impor-              Scalable grounded neural memory              The document in-
                tanceoffrequentindexrefreshes,wecompareagainstusing                    dex can be viewed as a memory where the keys are
                a slower refresh rate. The results in Table 2 suggests that            the document embeddings.          From this view, our work
                a stale index can hurt model training, and further reducing            share motivations with works such as product key mem-
                this staleness could offer better optimization.                        ory (Lampleet al., 2019), which enables sub-linear mem-
                                                                                       ory access in a memory network (Weston et al., 2014;
                                                                                       Graves et al., 2014; Sukhbaatar et al., 2015),          allowing
                Examples of retrieved documents              Table 3 shows an          these scalable memory layers to be integrated into large
                example of the REALM masked language model predic-                     language models. One main difference is that our memo-
                tion. In this example, “Fermat” is the correct word, and               ries are grounded—eachmemoryisassociatedwithadocu-
                REALM(row(c))givesthe word a much high probability                     mentratherthanunnamedvaluevectors. Thislevelofinter-
                compared to the BERT model (row (a)). Since REALM                      pretability is crucial for applications like Open-QA, where
                manages to retrieve some documents with a related fact                 users would require provenance for a predicted answer to
                (row (b)), the marginalized probability of the correct an-             be trustworthy.
                swer dramatically increases. This shows that REALM is
                able to retrieve document to ﬁll in the masked word even               Unsupervised       Corpus      Alignment      In    sequence-to-
                thoughit is trained with unsupervised text only.                       sequence models with attention (Bahdanau et al., 2014),
