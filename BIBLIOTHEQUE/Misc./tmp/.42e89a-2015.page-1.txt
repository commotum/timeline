                                                       Pointer Networks
                              Oriol Vinyals∗                 MeireFortunato∗                  NavdeepJaitly
                              Google Brain        Department of Mathematics, UC Berkeley       Google Brain
                                                                Abstract
                                 Weintroduce a new neural architecture to learn the conditional probability of an
                                 output sequence with elements that are discrete tokens corresponding to positions
                                 in an input sequence. Such problems cannot be trivially addressed by existent ap-
                                 proaches such as sequence-to-sequence [1] and Neural Turing Machines [2], be-
                                 cause the number of target classes in each step of the output depends on the length
                                 of the input, which is variable. Problems such as sorting variable sized sequences,
                                 and various combinatorial optimization problems belong to this class. Our model
                                 solves the problem of variable size output dictionaries using a recently proposed
                                 mechanism of neural attention. It differs from the previous attention attempts in
                                 that, instead of using attention to blend hidden units of an encoder to a context
                                 vector at each decoder step, it uses attention as a pointer to select a member of
                                 the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net).
                                 WeshowPtr-Netscanbeusedtolearnapproximatesolutionstothreechallenging
                                 geometric problems – ﬁnding planar convex hulls, computing Delaunay triangu-
                                 lations, and the planar Travelling Salesman Problem – using training examples
                                 alone. Ptr-Nets not only improve over sequence-to-sequence with input attention,
                                 but also allow us to generalize to variable size output dictionaries. We show that
                                 the learnt models generalize beyond the maximum lengths they were trained on.
                                 Wehopeourresults on these tasks will encourage a broader exploration of neural
                                 learning for discrete problems.
                         1   Introduction
                         Recurrent Neural Networks (RNNs) have been used for learning functions over sequences from
                         examples for more than three decades [3]. However, their architecture limited them to settings
                         where the inputs and outputs were available at a ﬁxed frame rate (e.g. [4]). The recently introduced
                         sequence-to-sequence paradigm [1] removed these constraints by using one RNN to map an input
        arXiv:1506.03134v2  [stat.ML]  2 Jan 2017sequencetoanembeddingandanother(possiblythesame)RNNtomaptheembeddingtoanoutput
                         sequence. Bahdanau et. al. augmented the decoder by propagating extra contextual information
                         from the input using a content-based attentional mechanism [5, 2, 6]. These developments have
                         madeit possible to apply RNNs to new domains, achieving state-of-the-art results in core problems
                         in natural language processing such as translation [1, 5] and parsing [7], image and video captioning
                         [8, 9], and even learning to execute small programs [2, 10].
                         Nonetheless,thesemethodsstillrequirethesizeoftheoutputdictionarytobeﬁxedapriori. Because
                         of this constraint we cannot directly apply this framework to combinatorial problems where the size
                         of the output dictionary depends on the length of the input sequence. In this paper, we address this
                         limitation by repurposing the attention mechanism of [5] to create pointers to input elements. We
                         show that the resulting architecture, which we name Pointer Networks (Ptr-Nets), can be trained to
                         output satisfactory solutions to three combinatorial optimization problems – computing planar con-
                         vex hulls, Delaunay triangulations and the symmetric planar Travelling Salesman Problem (TSP).
                         Theresulting models produce approximate solutions to these problems in a purely data driven fash-
                           ∗Equal contribution
                                                                    1
