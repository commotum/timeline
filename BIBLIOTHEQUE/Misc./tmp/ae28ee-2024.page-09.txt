                   Published as a conference paper at ICLR 2024
                   multiplestepstosolve(Yaoetal.,2022;Zhouetal.,2023;Dengetal.,2023;Liuetal.,2023d). There
                   are several drawbackswithsucha“potpourri”stylesetup. First, eachtasktendstonarrowlyfocuson
                   one or a few skills, resulting in challenges that are typically too simple, pigeonhole the model into a
                   reducedrole,anddonotprovidemodelswiththebandwidthtoexercisetheirversatilityorpotentially
                   demonstrate new abilities (Srivastava et al., 2023). Consequently, a model’s performance on such
                   task conglomerations may not yield actionable, deep insights regarding its capabilities and how to
                                             ´
                   improve them (Schlangen, 2019; Martınez-Plumed et al., 2021; Bowman & Dahl, 2021). SWE-
                   bench addresses these shortcomings, as our work demonstrates that it is significantly challenging,
                   presents a wide range of possibilities for improving LMs to solve this task, and is easy to refresh
                   overtimewithnewtaskinstances,eachofwhichintroducenovel,nuanced,andpracticalchallenges.
                   Code Generation Benchmarks. HumanEval (Chen et al., 2021) is the current standard in a long-
                   standing pursuit of synthesizing code from natural language descriptions (Yu et al., 2018; Austin
                   et al., 2021; Hendrycks et al., 2021; Li et al., 2022a; Zan et al., 2023). In the past year, subsequent
                   benchmarks have sought to augment HumanEval with extensions to different languages (Cassano
                   et al., 2022; Athiwaratkun et al., 2023; Orlanski et al., 2023), variations in edit scope (Yu et al.,
                   2023; Du et al., 2023), similar but novel code completion tasks (Muennighoff et al., 2023), and
                   more testing (Liu et al., 2023a). Simultaneously, separate works have sought to introduce new cod-
                   ing paradigms (Yin et al., 2022; Yang et al., 2023) or design library-specific problems (Lai et al.,
                   2022; Zan et al., 2022). Instead of partitioning problems into siloed datasets and curtailing them
                   for simplicity’s sake, SWE-bench’s collection procedure transforms the source code with minimal
                   post-processing, preserving a much broader set of challenges grounded in real-world software en-
                   gineering beyond closed form completion, such as patch generation, reasoning over long contexts,
                   navigating a codebase directory, and capturing dependency-based relationships across modules.
                   MLforSoftwareEngineering. To overcome traditional program analysis techniques that may not
                   scale or incorporate natural language, one direction of current software engineering research is to
                   use neural networks, including LMs, to automate real-world software development processes (Ma-
                   niatis et al., 2023; Zheng et al., 2023; Hou et al., 2023). Use cases include automating commit
                   generation (Jung, 2021; Liu et al., 2023c), PR review (Yang et al., 2016; Li et al., 2022b; Tufano
                   et al., 2021), bug localization Kim et al. (2019); Chakraborty et al. (2018), testing (Kang et al., 2023;
                   Xia et al., 2023; Wang et al., 2023), and program repair (Gupta et al., 2017; Allamanis et al., 2017;
                   Monperrus,2018;Jiangetal.,2018;Gouesetal.,2019;Gaoetal.,2022;Dinhetal.,2023;Motwani
                   &Brun,2023). Most relevant to SWE-bench are works that have sought to apply LMs towards au-
                   tomated program repair (Xia & Zhang, 2022; 2023; Fan et al., 2023; Sobania et al., 2023), guiding
                   code editing with commits (Chakraborty & Ray, 2021; Zhang et al., 2022; Fakhoury et al., 2023).
                   However, none of the existing datasets (Just et al., 2014; Karampatsis & Sutton, 2019) present code
                   context at the scale of SWE-bench. Moreover, SWE-bench can be easily extended to new program-
                   minglanguagesandrepositories, and it provides a significantly more realistic and challenging arena
                   to carry out experiments towards augmenting LMs with software engineering tools and practices.
                   7  DISCUSSION
                   Limitations and future directions. SWE-bench task instances are all in Python; we hope to apply
                   SWE-bench’s task instance collection procedure to expand its coverage to more programming lan-
                   guages and domains. Second, our experiments aim to establish a baseline of the simplest and most
                   straight-forward approaches for this task; we do not intend to constrain future methodologies to the
                   sametypeofapproachandencouragefutureworktoinvestigatedifferentmethods(e.g.,agent-based
                   approaches, tool augmented LMs). Lastly, while this work evaluates models using execution-based
                   codetesting, relying solely on this method is insufficient to guarantee reliable performance of model
                   generations, as we find automatedcodegenerationsfromLMscanfrequentlybelesscomprehensive,
                   efficient, or readable compared to human-written solutions.
                   Conclusion. The complexity of real-world software development processes extends far beyond
                   just code completion. By drawing on the open-source collaborative pipeline, SWE-bench creates
                   a faithful mirror of real world coding environments. This more realistic environment encourages
                   creative solutions that can have immediate applicability in open-source software development. We
                   hope that this benchmark and our other contributions can serve as valuable assets in the future
                   development of LMs that are more practical, intelligent, and autonomous.
                                                    9
