                         Preprint, Under Review.
                         Figure 3: Human-Agent collaboration in Crafter. We show an example where a human guides the
                         agent with high-level plans to clear a cave from a skeleton, and create a shelter to survive the night, a
                         complex behaviour that was not observed in any of the training runs otherwise.
                         For instance, excessive reasoning can lead to “overthinking” Sui et al. (2025) and longer contexts,
                         which in turn dilute attention to the most critical information Liu et al. (2023).
                         Finally, it is noteworthy that a dynamic planning baseline is absent from our zero-shot analysis. Our
                         initial attempts to elicit adaptive planning directly through complex prompting proved challenging and
                         unreliable. Models struggled to consistently interpret abstract instructions such as “plan only when
                         necessary,” often defaulting to fixed patterns of always planning or never planning. This difficulty
                         underscores the need for learning-based approaches, like SFT and RL, to effectively teach agents this
                         meta-cognitive skill of deciding when to allocate test-time compute for planning.
                         5.2  SFTPRIMING
                         Having established in zero-shot evaluations that prompting alone fails to induce dynamic planning,
                         we next turn to supervised fine-tuning. Recent work shows that RL tends to optimize within the
                         existing behaviors of the model rather than induce new strategies Ma et al. (2025); Gandhi et al.
                         (2025). This limitation underscores the necessity of an SFT priming stage that provides explicit
                         demonstrations over a mixture of both planning and non-planning steps within a trajectory.
                         Toevaluate the impact of SFT priming, we leverage the synthetic dataset introduced in Section 4.4.
                         During this phase the models were trained either to predict both the explicit plans and corresponding
                         actions (‘Primed-Dynamic’) or to predict only the actions (‘Primed-Naive’). Importantly, both
                         variants were derived from the same underlying action sequences, isolating the effect of explicit plans
                         onimitation learning.
                         Figure 1c–d shows that model fine-tuned on the dynamic planning dataset (“Primed-Dynamic”)
                         achieve significantly higher task progression throughout training compared to model trained without
                         plans (“Primed-Naive”). Moreover, the Primed-Dynamic agent exhibits lower KL divergence from
                         the base model, suggesting that the inclusion of plans regularizes the fine-tuning process and pre-
                         vents catastrophic forgetting. We hypothesize three complementary mechanisms underlying these
                         improvements:
                               • Explanatory Power: Plans provide semantic rationales for actions, simplifying the behav-
                                 ioral cloning objective and making future actions more predictable.
                               • LearningPlanningStructure: Exposuretoexplicitplan–actionpairsencouragesthemodel
                                 to internalize generalizable planning heuristics rather than memorizing isolated actions.
                               • Regularization and Grounding: Natural language plans act as a form of grounding signal,
                                 constraining the model’s learning trajectory and reducing deviation from base capabilities.
                         Collectively, these findings indicate that explicit plan representations not only improve downstream
                         RLsample efficiency but also enhance the SFT stage itself by stabilizing learning dynamics and
                         accelerating task progression.
                                                                    8
