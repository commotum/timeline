                                                                                                                                                                    MLP                   Class
                                                                                                                                                                    Head
                                                                                                                                                                                                                          Factorised                                           Factorised                                               Factorised
                                                                                                                                              Transformer  Encoder                                                          Encoder                                        Self-Attention                                             Dot-Product
                                                                                                                   Position + Token 
                                                                                                                       Embedding
                                                                                                                               0                                     MLP
                                                                                                                             CLS                                                                                             Temporal                                             Temporal
                                                                                                                               1                                 Layer Norm                                                                                                                                                                   Fuse
                                                                                                                                                                                                                                   ●●●                                                                                               Spatial        Temporal
                                                                                                                               2             L× Self-Attention                                                               Temporal                                               Spatial
                                                                                                                              3                                                                                                                                                        ●●●                                                      ●●●
                                                                                                  Embed to                                                        Multi-Head
                                                                                                    tokens                                                       Dot-Product                                                   Spatial                                            Temporal                                                    Fuse
                                                                                                                               …                                   Attention
                                                                                                                                                                                                                                   ●●●                                                                                               Spatial        Temporal
                                                                                                                                                                K          V         Q 
                                                                                                                                                                                                                               Spatial                                               Spatial
                                                                                                                              N                                  Layer Norm
                                                                                                                                                                                                                      1        2      ●●●      N                            1        2     ●●●      N                               1        2     ●●●      N
                                  Figure1: Weproposeapure-transformerarchitectureforvideoclassiﬁcation,inspiredbytherecentsuccessofsuchmodelsforimages[17].
                                  To effectively process a large number of spatio-temporal tokens, we develop several model variants which factorise different components
                                  of the transformer encoder over the spatial- and temporal-dimensions. As shown on the right, these factorisations correspond to different
                                  attention patterns over space and time.
                                  2. Related Work                                                                                                                                                                       Although previous works attempted to replace convolu-
                                          Architectures for video understanding have mirrored ad-                                                                                                               tions in vision architectures [48, 52, 54], it is only very re-
                                  vances in image recognition. Early video research used                                                                                                                        cently that Dosovitisky et al. [17] showed with their ViT ar-
                                  hand-crafted features to encode appearance and motion                                                                                                                         chitecture that pure-transformer networks, similar to those
                                  information [40, 68].                                            The success of AlexNet on Ima-                                                                               employed in NLP, can achieve state-of-the-art results for
                                  geNet [37, 15] initially led to the repurposing of 2D im-                                                                                                                     image classiﬁcation too.                                                  The authors showed that such
                                  age convolutional networks (CNNs) for video as “two-                                                                                                                          modelsareonlyeffectiveatlargescale,astransformerslack
                                  stream” networks [33, 55, 46]. These models processed                                                                                                                         some of inductive biases of convolutional networks (such
                                  RGBframesandoptical ﬂow images independently before                                                                                                                           as translational equivariance), and thus require datasets
                                  fusing them at the end. Availability of larger video classi-                                                                                                                  larger than the common ImageNet ILSRVC dataset [15] to
                                  ﬁcation datasets such as Kinetics [34] subsequently facili-                                                                                                                   train. ViT has inspired a large amount of follow-up work
                                  tated the training of spatio-temporal 3D CNNs [8, 21, 64]                                                                                                                     in the community, and we note that there are a number
                                  which have signiﬁcantly more parameters and thus require                                                                                                                      of concurrent approaches on extending it to other tasks in
                                  larger training datasets. As 3D convolutional networks re-                                                                                                                    computer vision [70, 73, 83, 84] and improving its data-
                                  quiresigniﬁcantlymorecomputationthantheirimagecoun-                                                                                                                           efﬁciency [63, 47]. In particular, [4, 45] have also proposed
                                  terparts, many architectures factorise convolutions across                                                                                                                    transformer-based models for video.
                                  spatial and temporal dimensions and/or use grouped convo-                                                                                                                             In this paper, we develop pure-transformer architectures
                                  lutions [58, 65, 66, 80, 19]. We also leverage factorisation                                                                                                                  for video classiﬁcation. We propose several variants of our
                                  of the spatial and temporal dimensions of videos to increase                                                                                                                  model, including those that are more efﬁcient by factoris-
                                  efﬁciency, but in the context of transformer-based models.                                                                                                                    ing the spatial and temporal dimensions of the input video.
                                                                                                                                                                                                                Wealso show how additional regularisation and pretrained
                                          Concurrently, in natural language processing (NLP),                                                                                                                   models can be used to combat the fact that video datasets
                                  Vaswani et al. [67] achieved state-of-the-art results by re-                                                                                                                  are not as large as their image counterparts that ViT was
                                  placing convolutions and recurrent networks with the trans-                                                                                                                   originally trained on. Furthermore, weoutperformthestate-
                                  former network that consisted only of self-attention, layer                                                                                                                   of-the-art across ﬁve popular datasets.
                                  normalisation and multilayer perceptron (MLP) operations.
                                  Current state-of-the-art architectures in NLP [16, 51] re-                                                                                                                    3. Video Vision Transformers
                                  maintransformer-based, and have been scaled to web-scale
                                  datasets [5]. Many variants of the transformer have also                                                                                                                              We start by summarising the recently proposed Vision
                                  been proposed to reduce the computational cost of self-                                                                                                                       Transformer [17] in Sec. 3.1, and then discuss two ap-
                                  attention when processing longer sequences [10, 11, 36,                                                                                                                       proaches for extracting tokens from video in Sec. 3.2. Fi-
                                  61, 62, 72] and to improve parameter efﬁciency [39, 14].                                                                                                                      nally, we develop several transformer-based architectures
                                  Although self-attention has been employed extensively in                                                                                                                      for video classiﬁcation in Sec. 3.3 and 3.4.
                                  computer vision, it has, in contrast, been typically incor-                                                                                                                   3.1. Overview of Vision Transformers (ViT)
                                  porated as a layer at the end or in the later stages of
                                  the network [74, 7, 31, 76, 82] or to augment residual                                                                                                                                Vision Transformer (ViT) [17] adapts the transformer
                                  blocks [29, 6, 9, 56] within a ResNet architecture [26].                                                                                                                      architecture of [67] to process 2D images with minimal
                                                                                                                                                                                                        6837
