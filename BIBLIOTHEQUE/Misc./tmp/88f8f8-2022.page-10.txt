                                    10      Wen. et al.
                                    RCk×Ci, W ∈ RCv×Ci where Ck is the key dimension and Cv is value di-
                                                 v
                                    mension. Q,K,V are queries, keys and values generated from Fi . Attention
                                                                                                             in
                                                           T
                                                         Q K                                                             i
                                    weights softmax √            is calculated in the primitive region. The output F        ∈
                                                           Ck                                                            out
                                    RCv×N′×LM is computed as a weighted sum of the values V. As shown in Fig-
                                    ure 5, we build intra-primitive transformer block with layernorm [2], GELU
                                    activation [18], one attentive layer and a following feedforward layer [39]. Feed-
                                    forward is implemented with a two-layer MLP(MultiLayer Perception).
                                    4.3    Long-Term Spatial-Temporal Feature Extraction
                                         After the short-term spatial-temporal feature extraction, primitive trans-
                                    formers are used to jointly analyze short-term features from the lower level and
                                    a memory pool containing pre-computed primitive features. This branch can not
                                    only reduce the computational cost, but also achieve long-term spatio-temporal
                                    information integration.
                                    Primitive Transformer. As demonstrated in Fig 4, two branches merge here.
                                    The output of l layer intra-primitive transformer Fl            ∈ RCl×N′×LM is then
                                                                                                out
                                    aggregated by max-pooling operator MAX{·} to obtain primitive level feature
                                              Cl×LM              l
                                    F    ∈R          , where C is feature channels, L is the clip length and M is the
                                     out
                                    primitive number. Pre-computed primitive features from memory pool F                   are
                                                                                                                     mem
                                    used to expand the spatio-temporal receptive field of the primitive transformer.
                                    Formally, the input of primitive transformer is Fprimitive = [F                ||F    ] ∈
                                                                                               in              clip   mem
                                       l    ′
                                    RC ×(L +L)M which concatenates short-term primitive features F                and primi-
                                                                                                              clip
                                    tive features from memory pool F          . Note that in the primitive attention layer,
                                                                         mem
                                    spatial-temporal attentive aggregation is performed in (L′ + L) × M primitive
                                    regions simultaneously. Identical to intra-primitive shown in Fig 5(Right), prim-
                                    itive transformer block is also composed of pre-LayerNorm, primitive attention
                                    layer, GELU, feedforward layer and residual connection. For semantic segmen-
                                    tation, we concatenate per-point features, intra-primitive point features, and
                                    primitive features to obtain point-wise features, and fuse them by a three-layer
                                    MLP. For the action recognition task, we use the primitive feature to obtain
                                    classification predictions through max-pooling and MLP.
                                    5    Experiments
                                    5.1    4D Semantic Segmentation
                                         Setup. Temporal information can help understand the dynamic objects in
                                    the scene, and improve segmentation accuracy and robustness to noise. Due to
                                    memory constraints, existing methods only process point cloud videos with a
                                    length of 3. Our method can consider a longer temporal range and achieve a
                                    more efÏcient integration of spatio-temporal information. In this task, we fit
                                    the scene point cloud into 200 primitives. We use mean IoU(mIoU) % as the
                                    evaluation metric.
