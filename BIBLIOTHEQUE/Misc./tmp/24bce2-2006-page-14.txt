              1540                                G.Hinton,S.Osindero,andY.-W.Teh
              6 PerformanceontheMNISTDatabase
                 6.1 Training the Network. The MNIST database of handwritten digits
              contains 60,000 training images and 10,000 test images. Results for many
              different pattern recognition techniquesarealreadypublishedforthispub-
              licly available database,soitisidealforevaluatingnewpatternrecognition
              methods. For the basic version of the MNIST learning task, no knowledge
              of geometry is provided, and there is no special preprocessing or enhance-
              mentofthetraining set, so an unknown but ﬁxed random permutation of
              the pixels would not affect the learning algorithm. For this “permutation-
              invariant” version of the task, the generalization performance of our net-
              work was 1.25% errors on the ofﬁcial test set. The network shown in
              Figure 1 was trained on 44,000 of the training images that were divided
              into 440 balanced mini-batches, each containing 10 examples of each digit
              class.6 The weights were updated after each mini-batch.
                 Intheinitialphaseoftraining,thegreedyalgorithmdescribedinsection4
              was used to train each layer of weights separately, starting at the bot-
              tom. Each layer was trained for 30 sweeps through the training set (called
              “epochs”). During training, the units in the “visible” layer of each RBM
              had real-valued activities between 0 and 1. These were the normalized
              pixel intensities when learning the bottom layer of weights. For training
              higherlayersofweights,thereal-valuedactivitiesofthevisibleunitsinthe
              RBMweretheactivationprobabilitiesofthehiddenunitsinthelower-level
              RBM. The hidden layer of each RBM used stochastic binary values when
              thatRBMwasbeingtrained.Thegreedytrainingtookafewhoursperlayer
              in MATLABona3GHzXeonprocessor,andwhenitwasdone,theerror
              rate on the test set was 2.49% (see below for details of how the network is
              tested).
                 Whentrainingthetoplayerofweights(theonesintheassociativemem-
              ory), the labels were provided as part of the input. The labels were repre-
              sented by turning on one unit in a “softmax” group of 10 units. When the
              activities in this group were reconstructed from the activities in the layer
              above, exactly one unit was allowed to be active, and the probability of
              picking unit i was given by
                          exp(x )
                    p =       i   ,                                           (6.1)
                     i      exp(x )
                           j     j
              where x is the total input received by unit i. Curiously, the learning rules
                      i
              are unaffected by the competition between units in a softmax group, so the
                 6 Preliminary experiments with 16×16 images of handwritten digits from the USPS
              databaseshowedthatagoodwaytomodelthejointdistributionofdigitimagesandtheir
              labels was to use an architecture of this type, but for 16 × 16 images, only three-ﬁfths as
              manyunitswereusedineachhiddenlayer.
