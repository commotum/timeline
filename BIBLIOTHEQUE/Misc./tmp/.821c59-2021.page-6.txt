                  Method                OA mAcc mIoU ceiling floor              wall   beam   column    window     door   table  chair   sofa   bookcase   board   clutter
                  PointNet [25]          –     49.0     41.1    88.8     97.3   69.8    0.1      3.9      46.3     10.8   59.0   52.6    5.9      40.3      26.4    33.2
                  SegCloud [36]          –     57.4     48.9    90.1     96.1   69.9    0.0     18.4      38.4     23.1   70.4   75.9    40.9     58.4      13.0    41.6
                  TangentConv [35]       –     62.2     52.6    90.5     97.7   74.0    0.0     20.7      39.0     31.3   77.5   69.4    57.3     38.5      48.8    39.8
                  PointCNN[20]          85.9   63.9     57.3    92.3     98.2   79.4    0.0     17.6      22.8     62.1   74.4   80.6    31.7     66.7      62.1    56.7
                  SPGraph[15]           86.4   66.5     58.0    89.4     96.9   78.1    0.0     42.8      48.9     61.6   84.7   75.4    69.8     52.6      2.1     52.2
                  PCCN[42]               –     67.0     58.3    92.3     96.2   75.9    0.3      6.0      69.5     63.5   66.9   65.6    47.3     68.9      59.1    46.2
                  PAT[50]                –     70.8     60.1    93.0     98.5   72.3    1.0     41.5      85.1     38.2   57.7   83.6    48.1     67.0      61.3    33.6
                  PointWeb [55]         87.0   66.6     60.3    92.0     98.5   79.4    0.0     21.1      59.7     34.8   76.3   88.3    46.9     69.3      64.9    52.5
                  HPEIN[13]             87.2   68.3     61.9    91.5     98.2   81.4    0.0     23.3      65.3     40.0   75.5   87.7    58.5     67.8      65.6    49.4
                  MinkowskiNet [37]      –     71.7     65.4    91.8     98.7   86.2    0.0     34.1      48.9     62.4   81.6   89.8    47.2     74.9      74.4    58.6
                  KPConv[37]             –     72.8     67.1    92.8     97.3   82.4    0.0     23.9      58.0     69.0   81.5   91.0    75.4     75.3      66.7    58.9
                  PointTransformer      90.8   76.5     70.4    94.0     98.5   86.3    0.0     38.0      63.4     74.3   89.1   82.4    74.3     80.2      76.0    59.3
                                               Table 1. Semantic segmentation results on the S3DIS dataset, evaluated on Area 5.
                          Method                     OA mAcc mIoU                                4.2. Shape Classification
                          PointNet [25]             78.5      66.2       47.6                    Data and metric. The ModelNet40 [47] dataset contains
                          RSNet[12]                   –       66.5       56.5                    12,311 CAD models with 40 object categories. They are
                          SPGraph[15]               85.5      73.0       62.1                    split into 9,843 models for training and 2,468 for testing.
                          PAT[50]                     –       76.5       64.3                    Wefollow the data preparation procedure of Qi et al. [27]
                          PointCNN[20]              88.1      75.6       65.4                    and uniformly sample the points from each CAD model
                          PointWeb [55]             87.3      76.2       66.7                    together with the normal vectors from the object meshes.
                          ShellNet [53]             87.1        –        66.8                    For evaluation metrics, we use the mean accuracy within
                          RandLA-Net[37]            88.0      82.0       70.0                    each category (mAcc) and the overall accuracy (OA) over
                          KPConv[37]                  –       79.1       70.6                    all classes.
                          PointTransformer          90.2      81.9       73.5                    Performancecomparison. TheresultsarepresentedinTa-
                Table2.SemanticsegmentationresultsontheS3DISdataset,eval-                        ble 3. The Point Transformer sets the new state of the art in
                uated with 6-fold cross-validation.                                              both metrics. The overall accuracy of Point Transformer on
                                                                                                 ModelNet40 is 93.7%. It outperforms strong graph-based
                the ground truth. Point Transformer captures detailed se-                        models such as DGCNN [44], attention-based models such
                mantic structure in complex 3D scenes, such as the legs of                       as A-SCN [48] and Point2Sequence [21], and strong point-
                chairs, the outlines of poster boards, and the trim around                       based models such as KPConv [37].
                doorways.                                                                        Visualization. To probe the representation learned by the
                                                                                                 Point Transformer, we conduct shape retrieval by retrieving
                        Method                         input     mAcc        OA                  nearest neighbors in the space of the output features pro-
                        3DShapeNets[47]               voxel       77.3       84.7                duced by the Point Transformer Some results are shown in
                        VoxNet[23]                    voxel       83.0       85.9                Figure 6. The retrieved shapes are very similar to the query,
                        Subvolume[26]                 voxel       86.0       89.2                and when they differ, they differ along aspects that we per-
                        MVCNN[34]                     image         –        90.1                ceive as less semantically salient, such as legs of desks.
                        PointNet [25]                  point      86.2       89.2
                        A-SCN[48]                      point      87.6       90.0                4.3. Object Part Segmentation
                        Set Transformer [17]           point        –        90.4                Data and metric. The ShapeNetPart dataset [52] is anno-
                        PAT[50]                        point        –        91.7                tated for 3D object part segmentation. It consists of 16,880
                        PointNet++ [27]                point        –        91.9                models from 16 shape categories, with 14,006 3D models
                        SpecGCN[40]                    point        –        92.1                for training and 2,874 for testing. The number of parts for
                        PointCNN[20]                   point      88.1       92.2                each category is between 2 and 6, with 50 different parts
                        DGCNN[44]                      point      90.2       92.2                in total. We use the sampled point sets produced by Qi et
                        PointWeb [55]                  point      89.4       92.3                al. [27] for a fair comparison with prior work. For evalua-
                        SpiderCNN[49]                  point        –        92.4                tion metrics, we report category mIoU and instance mIoU.
                        PointConv [46]                 point        –        92.5
                        Point2Sequence [21]            point      90.4       92.6                Performancecomparison. TheresultsarepresentedinTa-
                        KPConv[37]                     point        –        92.9                ble 4. The Point Transformer outperforms all prior mod-
                        InterpCNN [22]                 point        –        93.0                els as measured by instance mIoU. (Note that we did not
                        PointTransformer               point      90.6       93.7                use loss-balancing during training, which can boost cate-
                Table 3. Shape classification results on the ModelNet40 dataset.                 gory mIoU.)
                                                                                            16264
