              Applications of DEQs include optical flow [7], image seg-             found throughout the literature include Newton’s method,
              mentation [6] and inverse imaging problems [19].                      quasi-Newton methods such as Broyden’s method, and An-
                                                                                    derson’s acceleration. In these cases, one can analytically
              Recent Work Combining Diffusion and DEQs.                 In the      backpropagate through x∗ via implicit differentiation [14].
              past year, two works have merged DMs and DEQs. Differ-                However, these methods can come with significant memory
              ently from our proposal, these approaches tried to convert            and computational costs. Recently, a new iterative solving
              the entire diffusion process into a single fixed point equa-          methoddenoted Jacobian-Free Backpropagation (JFB) was
              tion. [38] considers the entire diffusion trajectory as a single      introduced to circumvent the need for complex and costly
              sample and solves for the fixed point of the trajectory, con-         implicit differentiation; we discuss and extend upon this
              verting the sequential diffusion process into a parallel one.         method in Sec. 3.4.
              [18] distills a pretrained diffusion model into a single-step         3.2. Fixed Point Denoising Networks
              DEQ. These works are exciting but come with their own
              drawbacks: the former is an inference-time technique that             Our proposed fixed-point denoising network (Fig. 2) inte-
              consumessignificantly more memory than standard ances-                grates an implicit fixed-point layer into a diffusion trans-
              tral sampling, while the latter requires a pretrained diffusion       former. The network consists of three stages: 1) explicit
              model and has not scaled to datasets larger than CIFAR-10.            timestep-independent preprocessing layers f(1:l) : X → X,
                                                                                                                                    pre
                                                                                    2) a implicit timestep-conditioned fixed-point layer f          :
              3. Methods                                                                                                                         fp
                                                                                    X×X×T →X,and3)explicittimestep-independent
                                                                                    postprocessing layers f(1:l) : X → X. The function f
              3.1. Preliminaries                                                                              post                                 fp
              Implicit Neural Networks.         The neural network layer is         takes as input both the current fixed-point solution x and
              the foundational building block of deep learning. While               a value x˜ called the input injection, which is the projected
              early neural networks used only a few layers [29, 31], mod-           output of the preceding explicit layers. One can think of ffp
                                                                                    as a map f(x,t˜ ) : X → X conditional on the input injection
              ern networks such as large-scale transformers [13, 50] often                      fp
              consist of dozens of layers connected by residual blocks.             and timestep, for which we aim to find a fixed point. The
                                                                                                                    (t)
                                                                                    network processes an input x         as follows:
              Typically, these layers share a similar internal structure and                                        input
              dimensionality, with each having distinct set of parameters.
                                                                                           (t)     (1:l) (t)
              In essence, most networks are defined explicitly: their opera-             x    =f       (x     )                                  (1)
                                                                                           pre    pre    input
              tions are precisely defined by their layer weights. Running a                (t)                 (t)
                                                                                         x˜   =projection(x      )         input injection       (2)
              forward pass always entails processing inputs with the entire                                    pre
                                                                                          ∗(t)         ∗(t)  (t)
                                                                                        x     =f (x       , x˜  , t)  via fixed point solving    (3)
              set of layers.                                                                      fp
                 Onthe other hand, implicit models define the function                    (t)      (1:l) ∗(t)
                                                                                         x    =f       (x    )                                   (4)
              or procedure to be computed by the network, rather than                     post    post
              the exact sequence of operations. This category includes              Theoutput x(t) is used to compute the loss (during training)
              models that integrate differential equation solvers (Neural                         post
                                                                                    or the input x(t−1) to the next timestep (during sampling).
              ODE/CDE/SDEs;[10,26,27]),aswellasmodelsincorporat-                                   input
              ing fixed point solvers (fixed point networks or DEQs; [5]).              Whereas explicit networks consume a fixed amount of
              Ourproposed FPDMbelongstothislatter group.                            computation, this implicit network can adapt based on the
                                                                                    desired level of accuracy or even on the difficulty of the input.
              Differentiable Fixed Point Solving.        Given a function f         In this way, it unlocks a new tradeoff between computation
              on X, a fixed point solver aims to compute x∗ ∈ X such                and accuracy. Moreover, since the implicit layer replaces a
              that f(x∗) = x∗. The computation of fixed points has been             large number of explicit layers, it significantly decreases its
              the subject of centuries of mathematical study [51], with the         size and memory consumption.
              existence and uniqueness of a system’s fixed points often                 Finally, note that our denoising network operates in latent
              proved with the Banach fixed-point theorem and its vari-              space rather than pixel space. That is, we apply a Variational
              ants [1, 20, 23].                                                     Autoencoder [28, 40] to encode the input image into latent
                 In our case, f       = f is a differentiable function              space and perform all processing in latent space.
                                            θ
              parametrized by θ, and we are interested in both solving for          3.3. Fixed Point Diffusion Models (FPDM)
              the fixed point and backpropagating through it. The simplest
              solving method is fixed point iteration, which iteratively ap-        FPDMincorporatesthefixed point denoising network pro-
              plies f until convergence to x∗. Under suitable assumptions,          posed above into a denoising diffusion process.
                     θ
              iteration converges linearly to the unique fixed point of an              Weassumethereaderisalready familiar with the basics
              equilibrium system (Thrm 2.1 in [14]). Alternative methods            of diffusion models and provide only a brief summary; if
                                                                                 3
