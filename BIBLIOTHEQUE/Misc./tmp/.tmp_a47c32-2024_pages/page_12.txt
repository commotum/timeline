                                                   Fixed Point Diffusion Models
                                                        Supplementary Material
             A. Diffusion Models                                           ward passes). For this, an online binary probing scheme can
             Asstated in Sec. 3.3, we provide an overview of diffusion     be employed: use binary search to select a Θ′, on which we
             models here in case any readers are not familiar with diffu-  perform inference for one batch of images. If the number of
                                                                                                             ′
             sion models.                                                  forward passes used to meet the Θ threshold exceeds our
                                                                                                 ′
                Diffusion denoising probabilistic models add noise to a    budget, we increase Θ in subsequent iterations; conversely,
             data sample X drawn from a target data distribution q(X ).    if the number is below our budget, we decrease Θ′. Note that
                          0                                         0      only constant time of probing is needed to find a sufficiently
             This noising process is executed in a series of steps, where  goodthresholdatthebeginningoftheinference. Thiscompu-
             each step adds a specific quantity of noise controlled by a   tational cost would be negligible, especially when sampling
                                   T
             variance schedule {βt}   . At each step, the new data sam-
                                   t=0                                     manybatches of images.
             ple X is generated from the previous one X   according to
                  t                                √ t−1
             the distribution q(X |X   ) = N(X ; 1−β X          , β I).
                                t   t−1          t        t  t−1   t
             Thereverse diffusion process, or generative process, starts
             with a noisy sample from q(X ) ∼ N(0,1) and aims to it-
                                          T
             eratively remove the noise to recover a sample from the orig-
             inal distribution q(X ). This reverse process is learned by a
                                0
             neural network, approximating the distribution q(X   |X )
             as s (X    |X ) ≈ q(X     |X ).                  t−1   t
                θ   t−1   t        t−1   t
             B. Additional Qualitative Examples
             WeprovideexamplesonCelebA-HQ,LSUNChurch,FFHQ,
             and ImageNet in Figs. 8 to 11. For each dataset, we sample
             48 images using DDPM with 560 transformer block for-
             ward passes at resolution 256px. Note that the images are
             not cherry-picked. We also provide additional qualitative
             comparisons with DiT in Fig. 12.
             C. Description of an Adaptive Allocation Algo-
                  rithm
             FPDMallowsfortheadjustment of solution accuracy at dif-
             ferent stages of the denoising process. As noted in Sec. 3.3,
             in addition to implementing straightforward heuristics such
             as “increasing” and “decreasing”, it supports using adaptive
             algorithms to allocate the forward passes across timesteps.
             Weleaveanin-depth investigation of adaptive algorithms to
             future work, but we give an example below to demonstrate
             howonesuchalgorithmcouldwork.
                Westartbyconsideringθ ,thedifferencebetweenthelast
                                       t
             two solving solutions, as a metric of solution quality at each
             step. This aligns with our observation in Fig. 6b, where θt
             decreases as more fixed point iterations (i.e. forward passes)
             are applied. Then a simple adaptive algorithm could be to
             simply set an error threshold Θ and iterate the fixed point
             iteration process at each timestep t continue until θ falls
                                                                t
             below Θ. Then the global threshold Θ controls the number
             of forward passes.
                The only question left would be how to choose Θ to
             match a given computational budget (i.e. a number of for-
                                                                        1
