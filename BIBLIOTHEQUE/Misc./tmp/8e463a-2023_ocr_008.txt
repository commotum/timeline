Context Alpaca Vicuna ChatGLM
artificial 27.63 30.1 27.47
w/ chain 29.21 33.25 44.43
w/ irrelevant — 25.38 22.98 18.56

Table 4: Results on KA questions with different knowl-
edge in context.

Alpaca Vicuna ChatGLM
JSON NL JSON NL JSON’ NL
KU =. 33.8 = 330.63. 41.22) 28.04 47.97 20.07
KD 38.97 36.00 46.76 36.12 4142 22.44
KA 27.63 26.07 30.10 25.48 2747 9.82
Avg. 35.09 32.16 42.74 31.92 42.56 20.54

Table 5: Results on ALCUNA with knowledge in JSON
and natural language format (NL).

entities in the context, which is consistent with pre-
vious work (Shi et al., 2023). More significantly,
for Vicuna and ChatGLM models, the parent entity
brings more substantial performance degradation
compared to the irrelevant entity, again confirming
the confusion problem of existing large models in
face of new knowledge.

Chain Entities are Key to Knowledge Associa-
tion To more clearly analyze why all models per-
forms poorly on the knowledge association prob-
lem, we conduct two additional sets of experiments
on the KA questions: 1) adding knowledge about
the entities involved in the reasoning chain to the
context. 2) randomly sampling the same number
of entities to the context for a fair comparison. The
final results are shown in Table 4. We can find
that the score of all models improves very much
after adding the information of the entities required
for inference, and the performance of all models
decreases after adding irrelevant entities. This also
shows that the main problem is that LLMs really
cannot make the association between new and ex-
isting knowledge well, and not just the problem of
not being able to make reasoning.

Structured Knowledge is Better Since our
knowledge is represented structurally, the input
of knowledge in our experiments is also structured,
as shown in Table 12. To explore the effect of the
form of the knowledge representation, we addition-
ally do comparison experiments with knowledge in
natural language form as input (NL). We use tem-
plates to transform each attribute into a language
description for input similar to the template-based
question generation process in Section 3.3. As can

be seen from Table 5, all models generally perform
better with the structured input setting (JSON). The
models’ understanding of this structured text may
come from the code in the training data. This indi-
cates that for this kind of high-density knowledge
input, a clear and structured representation is more
helpful for the model’s understanding.

6 Assess New Models with ALCUNA

In this section, we discuss how to apply the pro-
posed ALCUNA benchmark to other models. There
are two different application scenarios of our bench-
mark. First, if one wants to assess the knowledge
understanding performance of different LLMs in
the face of new knowledge, ALCUNA can be di-
rectly utilized for evaluation. On the other hand,
if one wants to compare the knowledge differenti-
ation and association abilities, the different back-
ground knowledge inside the different models may
lead to an unfair comparison. Therefore, we need
to conduct an additional filtering on ALCUNA to
ensure the existing entities are possessed by all
models, which will cause a shrinkage of our bench-
mark. Despite this, the current benchmark has been
filtered on models such as Alpaca. A reasonable
assumption is that the models that come after that
will be more and more powerful, so the resulting
shrinkage won’t be very severe.

7 Related Work

Large Language Models In recent years, sig-
nificant advancements in Large Language Models
(LLMs) like FLAN-T5(Chung et al., 2022), GPT-
3(Brown et al., 2020), OPT(Zhang et al., 2022),
LLama(Touvron et al., 2023) and GPT-4(OpenAI,
2023) have led to exceptional performance in nat-
ural language processing tasks. At the same time,
open-source LLMs based on LLama and other fun-
damental models for further instruction fine-tuning
have emerged recently, such as Alpaca(Taori et al.,
2023), Vicuna(Chiang et al., 2023), Koala(Geng
et al., 2023), ChatGLM(Du et al., 2022), etc.,
which have also shown strong capabilities.

These models have shown breakthroughs on a
variety of tasks, and some have been applied as a
commercial product in daily work. Since the world
is constantly changing, the ability of the models to
perform when faced with new knowledge is critical.

Existing Benchmarks Benchmarks, such as
SQuAD(Rajpurkar et al., 2016), SNLI(Bowman
