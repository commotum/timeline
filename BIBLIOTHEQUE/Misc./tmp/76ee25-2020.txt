                                     Scaling Laws for Neural Language Models
                                                  Jared Kaplan ∗                            SamMcCandlish∗
                                        Johns Hopkins University, OpenAI                          OpenAI
                                                jaredk@jhu.edu                               sam@openai.com
                              TomHenighan                TomB.Brown             BenjaminChess               RewonChild
                                  OpenAI                    OpenAI                   OpenAI                    OpenAI
                          henighan@openai.com          tom@openai.com         bchess@openai.com          rewon@openai.com
                               Scott Gray             Alec Radford              Jeffrey Wu                Dario Amodei
                                OpenAI                   OpenAI                   OpenAI                      OpenAI
                          scott@openai.com         alec@openai.com         jeffwu@openai.com          damodei@openai.com
                                                                      Abstract
                               Westudyempiricalscalinglawsforlanguagemodelperformanceonthecross-entropyloss.
                               The loss scales as a power-law with model size, dataset size, and the amount of compute
                               used for training, with some trends spanning more than seven orders of magnitude. Other
                               architectural details such as network width or depth have minimal effects within a wide
                               range. Simpleequationsgovernthedependenceofoverﬁttingonmodel/datasetsizeandthe
                               dependence of training speed on model size. These relationships allow us to determine the
                               optimalallocationofaﬁxedcomputebudget. Largermodelsaresigniﬁcantlymoresample-
                               efﬁcient, such that optimally compute-efﬁcient training involves training very large models
                               onarelatively modest amount of data and stopping signiﬁcantly before convergence.
        arXiv:2001.08361v1  [cs.LG]  23 Jan 2020
                         ∗Equal contribution.
                       Contributions: Jared Kaplan and Sam McCandlish led the research.  Tom Henighan contributed the LSTM ex-
                       periments. Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer
                       implementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided
                       guidance throughout the project.
                     Contents
                     1  Introduction                                                                                2
                     2  BackgroundandMethods                                                                        6
                     3  Empirical Results and Basic Power Laws                                                      7
                     4  Charting the Inﬁnite Data Limit and Overﬁtting                                             10
                     5  Scaling Laws with Model Size and Training Time                                             12
                     6  OptimalAllocation of the Compute Budget                                                    14
                     7  Related Work                                                                               18
                     8  Discussion                                                                                 18
                     Appendices                                                                                    20
                     A SummaryofPowerLaws                                                                          20
                     B EmpiricalModelofCompute-EfﬁcientFrontier                                                    20
                     C Caveats                                                                                     22
                     D SupplementalFigures                                                                         23
                     1   Introduction
                     Language provides a natural domain for the study of artiﬁcial intelligence, as the vast majority of reason-
                     ing tasks can be efﬁciently expressed and evaluated in language, and the world’s text provides a wealth of
                     dataforunsupervisedlearningviagenerativemodeling. Deeplearninghasrecentlyseenrapidprogressinlan-
                                                                                   +        +        +
                     guagemodeling,withstateoftheartmodels[RNSS18,DCLT18,YDY 19,LOG 19,RSR 19]approaching
                                                                       +
                     human-level performance on many speciﬁc tasks [WPN 19], including the composition of coherent multi-
                     paragraph prompted text samples [RWC+19].
                     Onemightexpectlanguagemodelingperformancetodependonmodelarchitecture,thesizeofneuralmodels,
                     the computing power used to train them, and the data available for this training process. In this work we will
                     empirically investigate the dependence of language modeling loss on all of these factors, focusing on the
                                                +        +
                     Transformer architecture [VSP 17, LSP 18]. The high ceiling and low ﬂoor for performance on language
                     tasks allows us to study trends over more than seven orders of magnitude in scale.
                     Throughout we will observe precise power-law scalings for performance as a function of training time, con-
                     text length, dataset size, model size, and compute budget.
                     1.1  Summary
                     OurkeyﬁndingsforTransformer language models are are as follows:
                       2Here we display predicted compute when using a sufﬁciently small batch size. See Figure 13 for comparison to the
                     purely empirical data.
                                                                    2
                         est Loss
                         T
                                        Compute                          Dataset Size                        Parameters 
                                  PF-days, non-embedding                     tokens                         non-embedding
                       Figure 1   Language modeling performance improves smoothly as we increase the model size, datasetset
                       size, and amount of compute2 used for training. For optimal performance all three factors must be scaled
                       up in tandem. Empirical performance has a power-law relationship with each individual factor when not
                       bottlenecked by the other two.
                       Performance depends strongly on scale, weakly on model shape:           Model performance depends most
                       strongly on scale, which consists of three factors: the number of model parameters N (excluding embed-
                       dings), the size of the dataset D, and the amount of compute C used for training. Within reasonable limits,
                       performance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section
                       3)
                       Smooth power laws: Performance has a power-law relationship with each of the three scale factors
                       N,D,C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude
                       (see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance
                       must ﬂatten out eventually before reaching zero loss. (Section 3)
                       Universality of overﬁtting:   Performanceimprovespredictably as long as we scale up N and D in tandem,
                       but enters a regime of diminishing returns if either N or D is held ﬁxed while the other increases. The
                       performance penalty depends predictably on the ratio N0.74/D, meaning that every time we increase the
                       model size 8x, we only need to increase the data by roughly 5x to avoid a penalty. (Section 4)
                       Universality of training:   Training curves follow predictable power-laws whose parameters are roughly
                       independent of the model size. By extrapolating the early part of a training curve, we can roughly predict the
                       loss that would be achieved if we trained for much longer. (Section 5)
                       Transfer improves with test performance:      Whenweevaluatemodelsontextwithadifferentdistribution
                       than they were trained on, the results are strongly correlated to those on the training validation set with
                       a roughly constant offset in the loss – in other words, transfer to a different distribution incurs a constant
                       penalty but otherwise improves roughly in line with performance on the training set. (Section 3.2.2)
                       Sample efﬁciency:    Large models are more sample-efﬁcient than small models, reaching the same level of
                       performance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4).
                       Convergenceisinefﬁcient:      WhenworkingwithinaﬁxedcomputebudgetC butwithoutanyotherrestric-
                       tions on the model size N or available data D, we attain optimal performance by training very large models
                       and stopping signiﬁcantly short of convergence (see Figure 3). Maximally compute-efﬁcient training would
                       therefore be far more sample efﬁcient than one might expect based on training small models to convergence,
                       with data requirements growing very slowly as D ∼ C0.27 with training compute. (Section 6)
                       Optimal batch size:    The ideal batch size for training these models is roughly a power of the loss only,
                       and continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million
                       tokens at convergence for the largest models we can train. (Section 5.1)
                       Taken together, these results show that language modeling performance improves smoothly and predictably
                       as we appropriately scale up model size, data, and compute. We expect that larger language models will
                       perform better and be more sample efﬁcient than current models.
                                                                           3
                                      Larger models require fewer samples        The optimal model size grows smoothly 
                                      to reach the same performance              with the loss target and compute budget
                                                                                                                           Line color indicates

                          Test Loss 10                                         10                                          number of parameters
                                                                                                                          103    106    109
                                    8                                           8
                                                                103 Params
                                    6                                           6                                         Compute-eﬃcient 
                                          109 Params                                                                      training stops far 
                                    4                                           4                                         short of convergence
                                              107         109         1011             10-9     10-6     10-3     100
                                     Tokens Processed                            Compute (PF-days)
                                                                                                                                    3      9
                         Figure 2    Weshowaseries of language model training runs, with models ranging in size from 10 to 10
                         parameters (excluding embeddings).
                                                      Minimum serial steps                             Data requirements

                                                      increases negligibly                             grow relatively slowly
                                                                            <10x Serial Steps
                                                                              100x Batch Size          Optimal model size

                                                                                                       increases very quickly
                                                                       >1,000,000x Model Size
                         Figure3 Asmorecomputebecomesavailable,wecanchoosehowmuchtoallocatetowardstraininglarger
                         models, using larger batches, and training for more steps. We illustrate this for a billion-fold increase in
                         compute. For optimally compute-efﬁcient training, most of the increase should go towards increased model
                         size. A relatively small increase in data is needed to avoid reuse. Of the increase in data, most can be used to
                         increaseparallelismthroughlargerbatchsizes,withonlyaverysmallincreaseinserialtrainingtimerequired.
                         1.2   SummaryofScalingLaws
                         ThetestlossofaTransformertrainedtoautoregressivelymodellanguagecanbepredictedusingapower-law
                         whenperformance is limited by only either the number of non-embedding parameters N, the dataset size D,
                         or the optimally allocated compute budget Cmin (see Figure 1):
                               1. For models with a limited number of parameters, trained to convergence on sufﬁciently large
                                  datasets:
                                       L(N)=(N /N)αN ; α ∼0.076, N ∼8.8×1013(non-embeddingparameters)                                  (1.1)
                                                    c            N                 c
                               2. For large models trained with a limited dataset with early stopping:
                                                    L(D)=(D /D)αD; α ∼0.095, D ∼5.4×1013(tokens)                                       (1.2)
                                                                 c             D                 c
                               3. When training with a limited amount of compute, a sufﬁciently large dataset, an optimally-sized
                                                                                                 3
                                  model, and a sufﬁciently small batch size (making optimal use of compute):
                                                                     αmin
                                                          min          C       min                 min             8
                                          L(C     ) = C       /C           ;  α     ∼0.050,      C     ∼3.1×10 (PF-days)               (1.3)
                                               min        c      min           C                  c
                            3Wealso observe an empirical power-law trend with the training compute C (Figure 1) while training at ﬁxed batch
                         size, but it is the trend with Cmin that should be used to make predictions. They are related by equation (5.5).
                                                                                  4
                                            Loss vs Model and Dataset Size                               Loss vs Model Size and Training Steps
                                   4.5                                                               4.4
                                   4.0                                                 Params        4.0                                                    108
                                                                                          708M       3.6
                                   3.5                                                    302M
                                  Loss                                                    85M       Loss3.2                                                 107
                                                                                          3M
                                   3.0                                                    25M
                                                                                          393.2K     2.8
                                                                                                                                                            106
                                   2.5                                                               2.4                                                         Parameters (non-embed)
                                       107         108         109         1010                                       104                    105
                                                     Tokens in Dataset                                                  Estimated Smin
                             Figure 4      Left: The early-stopped test loss L(N,D) varies predictably with the dataset size D and model
                             size N according to Equation (1.5). Right: After an initial transient period, learning curves for all model
                             sizes N can be ﬁt with Equation (1.6), which is parameterized in terms of Smin, the number of steps when
                             training at large batch size (details in Section 5.1).
                             These relations hold across eight orders of magnitude in Cmin, six orders of magnitude in N, and over two
                             orders of magnitude in D. They depend very weakly on model shape and other Transformer hyperparameters
                             (depth, width, number of self-attention heads), with speciﬁc numerical values associated with the Webtext2
                                                    +                                           min
                             training set [RWC 19]. The power laws α ,α ,α                            specify the degree of performance improvement
                                                                                      N D C
                             expected as we scale up N, D, or Cmin; for example, doubling the number of parameters yields a loss that
                                                           −αN                                                                   min
                             is smaller by a factor 2             = 0.95. The precise numerical values of N ,C                       , and D depend on the
                                                                                                                            c    c              c
                             vocabulary size and tokenization and hence do not have a fundamental meaning.
                             Thecritical batch size, which determines the speed/efﬁciency tradeoff for data parallelism ([MKAT18]), also
                             roughly obeys a power law in L:
                                                           B (L)= B∗ ,                      B ∼2·108tokens, α ∼0.21                                           (1.4)
                                                              crit         L1/αB              ∗                           B
                             Equation (1.1) and (1.2) together suggest that as we increase the model size, we should increase the dataset
                                                                            αN
                                                                            α          0.74
                             size sublinearly according to D ∝ N D ∼ N                      . In fact, we ﬁnd that there is a single equation combining
                             (1.1) and (1.2) that governs the simultaneous dependence on N and D and governs the degree of overﬁtting:
                                                                                          "          α           #αD
                                                                                             N
                                                                                                     α
                                                                                              N       D      D
                                                                          L(N,D)=                c        + c                                                 (1.5)
                                                                                               N              D
                             with ﬁts pictured on the left in ﬁgure 4. We conjecture that this functional form may also parameterize the
                             trained log-likelihood for other generative modeling tasks.
                             Whentraining a given model for a ﬁnite number of parameter update steps S in the inﬁnite data limit, after
                             an initial transient period, the learning curves can be accurately ﬁt by (see the right of ﬁgure 4)
                                                                                                                 
                                                                                         N αN                S         αS
                                                                       L(N,S)=             c       +           c                                              (1.6)
                                                                                         N               Smin(S)
                             where S ≈ 2.1×103 and α ≈ 0.76, and S                        (S) is the minimum possible number of optimization steps
                                       c                         S                    min
                             (parameter updates) estimated using Equation (5.4).
                             When training within a ﬁxed compute budget C, but with no other constraints, Equation (1.6) leads to the
                             prediction that the optimal model size N, optimal batch size B, optimal number of steps S, and dataset size
                             Dshouldgrowas
                                                              αmin/α                  αmin/α                 αmin/α
                                                    N∝C C N, B∝C C B, S∝C C S, D=B·S                                                                          (1.7)
                             with
                                                                          αmin = 1/(1/α +1/α +1/α )                                                           (1.8)
                                                                            C                 S          B          N
                             which closely matches the empirically optimal results N ∝ C0.73, B ∝ C0.24, and S ∝ C0.03. As the
                                                                                                             min              min                   min
                             computational budget C increases, it should be spent primarily on larger models, without dramatic increases
                             in training time or dataset size (see Figure 3). This also implies that as models grow larger, they become
                             increasingly sample efﬁcient. In practice, researchers typically train smaller models for longer than would
                                                                                                5
                       be maximally compute-efﬁcient because of hardware constraints. Optimal performance depends on total
                       compute as a power law (see Equation (1.3)).
                       Weprovide some basic theoretical motivation for Equation (1.5), an analysis of learning curve ﬁts and their
                       implications for training time, and a breakdown of our results per token. We also make some brief compar-
                       isons to LSTMs and recurrent Transformers [DGV+18].
                       1.3   Notation
                       Weusethefollowing notation:
                             • L–thecross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in
                                somecases we report the loss for speciﬁc tokens within the context.
                             • N –thenumberofmodelparameters,excluding all vocabulary and positional embeddings
                             • C ≈ 6NBS –anestimateofthetotal non-embedding training compute, where B is the batch size,
                                andSisthenumberoftrainingsteps(ieparameterupdates). WequotenumericalvaluesinPF-days,
                                                        15                           19
                                where one PF-day = 10     ×24×3600=8.64×10 ﬂoatingpointoperations.
                             • D–thedatasetsize in tokens
                             • B      – the critical batch size [MKAT18], deﬁned and discussed in Section 5.1. Training at the
                                  crit
                                critical batch size provides a roughly optimal compromise between time and compute efﬁciency.
                             • C      – an estimate of the minimum amount of non-embedding compute to reach a given value of
                                  min
                                the loss. This is the training compute that would be used if the model were trained at a batch size
                                muchless than the critical batch size.
                             • Smin –anestimateoftheminimalnumberoftrainingstepsneededtoreachagivenvalueoftheloss.
                                This is also the number of training steps that would be used if the model were trained at a batch size
                                muchgreater than the critical batch size.
                             • α –power-law exponents for the scaling of the loss as L(X) ∝ 1/XαX where X can be any of
                                  X
                                N,D,C,S,B,Cmin.
                       2   BackgroundandMethods
                       Wetrain language models on WebText2, an extended version of the WebText [RWC+19] dataset, tokenized
                       using byte-pair encoding [SHB15] with a vocabulary size nvocab = 50257. We optimize the autoregres-
                       sive log-likelihood (i.e. cross-entropy loss) averaged over a 1024-token context, which is also our principal
                       performance metric. We record the loss on the WebText2 test distribution and on a selection of other text
                                                                           +                                  +
                       distributions. We primarily train decoder-only [LSP 18, RNSS18] Transformer [VSP 17] models, though
                                                                                      +
                       wealsotrain LSTMmodelsandUniversal Transformers [DGV 18] for comparison.
                       2.1   ParameterandComputeScalingofTransformers
                       Weparameterize the Transformer architecture using hyperparameters n          (number of layers), d      (di-
                                                                                               layer                     model
                       mension of the residual stream), d  (dimension of the intermediate feed-forward layer), d     (dimension of
                                                         ﬀ                                                       attn
                       the attention output), and nheads (number of attention heads per layer). We include nctx tokens in the input
                       context, with nctx = 1024 except where otherwise noted.
                       WeuseN todenotethemodelsize,whichwedeﬁneasthenumberofnon-embeddingparameters
                                     N≈2d         n     (2d     +d )
                                             model layer    attn    ﬀ
                                                    2
                                       =12n       d         with the standard   d     =d /4=d                                 (2.1)
                                              layer model                         attn    ﬀ        model
                       where we have excluded biases and other sub-leading terms. Our models also have n         d      parameters
                                                                                                            vocab model
                       in an embedding matrix, and use nctxdmodel parameters for positional embeddings, but we do not include
                       these when discussing the ‘model size’ N; we will see that this produces signiﬁcantly cleaner scaling laws.
                       Evaluating a forward pass of the Transformer involves roughly
                                                           Cforward ≈ 2N +2nlayernctxdmodel                                   (2.2)
                       add-multiply operations, where the factor of two comes from the multiply-accumulate operation used in
                       matrix multiplication. A more detailed per-operation parameter and compute count is included in Table 1.
                                                                            6
                              Operation                      Parameters                               FLOPsperToken
                              Embed                          (n       +n )d                           4d
                                                                vocab      ctx   model                   model
                              Attention: QKV                 n      d      3d                         2n      d      3d
                                                               layer model    attn                       layer model    attn
                              Attention: Mask                —                                        2n      n d
                                                                                                         layer ctx attn
                              Attention: Project             n      d    d                            2n      d    d
                                                               layer attn model                          layer attn embd
                              Feedforward                    n      2d      d                         2n      2d       d
                                                               layer  model ﬀ                            layer   model ﬀ
                              De-embed                       —                                        2d       n
                                                                                                         model vocab
                              Total (Non-Embedding)          N=2d           n      (2d      +d ) C              =2N+2n            n d
                                                                      model layer      attn     ﬀ       forward               layer ctx attn
                          Table 1    Parameter counts and compute (forward pass) estimates for a Transformer model. Sub-leading
                          terms such as nonlinearities, biases, and layer normalization are omitted.
                          For contexts and models with dmodel > nctx/12, the context-dependent computational cost per token is a
                          relatively small fraction of the total compute. Since we primarily study models where dmodel  nctx/12,
                          wedonotinclude context-dependent terms in our training compute estimate. Accounting for the backwards
                          pass (approximately twice the compute as the forwards pass), we then deﬁne the estimated non-embedding
                          compute as C ≈ 6N ﬂoating point operators per training token.
                          2.2   Training Procedures
                          Unless otherwise noted, we train models with the Adam optimizer [KB14] for a ﬁxed 2.5 × 105 steps with
                          a batch size of 512 sequences of 1024 tokens. Due to memory constraints, our largest models (more than
                          1B parameters) were trained with Adafactor [SS18]. We experimented with a variety of learning rates and
                          schedules, as discussed in Appendix D.6. We found that results at convergence were largely independent of
                          learning rate schedule. Unless otherwise noted, all training runs included in our data used a learning rate
                          schedule with a 3000 step linear warmup followed by a cosine decay to zero.
                          2.3   Datasets
                                                                                                                            +
                          Wetrain our models on an extended version of the WebText dataset described in [RWC 19]. The original
                          WebTextdataset was a web scrape of outbound links from Reddit through December 2017 which received at
                          least 3 karma. In the second version, WebText2, we added outbound Reddit links from the period of January
                          to October 2018, also with a minimum of 3 karma. The karma threshold served as a heuristic for whether
                          people found the link interesting or useful. The text of the new links was extracted with the Newspaper3k
                                                                                                                                               10
                          python library. In total, the dataset consists of 20.3M documents containing 96 GB of text and 1.62 × 10
                                                                                                                            +
                          words (as deﬁned by wc). We then apply the reversible tokenizer described in [RWC 19], which yields
                                     10                                8
                          2.29 ×10      tokens. We reserve 6.6 × 10 of these tokens for use as a test set, and we also test on similarly-
                                                                        +
                          prepared samples of Books Corpus [ZKZ 15], Common Crawl [Fou], English Wikipedia, and a collection
                          of publicly-available Internet Books.
                          3   Empirical Results and Basic Power Laws
                          To characterize language model scaling we train a wide variety of models, varying a number of factors
                          including:
                                 • Modelsize (ranging in size from 768 to 1.5 billion non-embedding parameters)
                                 • Dataset size (ranging from 22 million to 23 billion tokens)
                                 • Shape (including depth, width, attention heads, and feed-forward dimension)
                                 • Context length (1024 for most runs, though we also experiment with shorter contexts)
                                                  19
                                 • Batch size (2     for most runs, but we also vary it to measure the critical batch size)
                                                                                     7
                            10%
                            8%
                           ease6%                                         A wide range of architectures 
                            4%                                            achieve similar performance         22% additional compute

                                                                                                              compensates for 1% loss increase
                           Loss Incr2%
                            0%
                                    Feed-Forward Ratio (d  / d   )                                       Attention Head Dimension (d     / n  ) 
                                                         ff  model          Aspect Ratio (d    / n )                                 model head
                                            50M Parameters                                model  layer               25M Parameters
                         Figure 5     Performance depends very mildly on model shape when the total number of non-embedding
                         parameters N is held ﬁxed. The loss varies only a few percent over a wide range of shapes. Small differences
                         in parameter counts are compensated for by using the ﬁt to L(N) as a baseline. Aspect ratio in particular can
                         vary by a factor of 40 while only slightly impacting performance; an (n            , d     ) = (6,4288) reaches a
                                                                                                       layer  model
                                                                                   +
                         loss within 3% of the (48,1600) model used in [RWC 19].
                                  7                                                    7
                                  6                                                    6
                                  5                                                    5
                                  4       0 Layer                                      4
                                          1 Layer                                               1 Layer
                                 Test Loss2 Layers                                    Test Loss 2 Layers
                                  3       3 Layers                                     3        3 Layers
                                          6 Layers                                              6 Layers
                                           >6 Layers                                            >6 Layers
                                  2         106        107        108        109       2 103     104    105    106    107   108    109
                                              Parameters (with embedding)                          Parameters (non-embedding)
                         Figure 6    Left: When we include embedding parameters, performance appears to depend strongly on the
                         number of layers in addition to the number of parameters. Right: When we exclude embedding parameters,
                         the performance of models with different depths converge to a single trend. Only models with fewer than 2
                         layers or with extreme depth-to-width ratios deviate signiﬁcantly from the trend.
                         In this section we will display data along with empirically-motivated ﬁts, deferring theoretical analysis to
                         later sections.
                         3.1   ApproximateTransformerShapeandHyperparameterIndependence
                         Transformer performance depends very weakly on the shape parameters n               , n      , and d  whenwehold
                                                                                                         layer  heads        ﬀ
                         the total non-embedding parameter count N ﬁxed. To establish these results we trained models with ﬁxed
                         size while varying a single hyperparameter. This was simplest for the case of n             . When varying n        ,
                                                                                                                heads                    layer
                         we simultaneously varied d           while keeping N ≈ 12n           d2      ﬁxed. Similarly, to vary d      at ﬁxed
                                                       model                              layer model                              ﬀ
                         modelsize we also simultaneously varied the dmodel parameter, as required by the parameter counts in Table
                         1. Independence of nlayers would follow if deeper Transformers effectively behave as ensembles of shallower
                         models, as has been suggested for ResNets [VWB16]. The results are shown in Figure 5.
                         3.2   PerformancewithNon-EmbeddingParameterCountN
                         In Figure 6 we display the performance of a wide variety of models, ranging from small models with shape
                         (n     , d      ) = (2,128) through billion-parameter models, ranging in shape from (6,4288) through
                            layer  model
                         (207,768). Here we have trained to near convergence on the full WebText2 dataset and observe no over-
                         ﬁtting (except possibly for the very largest models).
                         AsshowninFigure1,weﬁndasteadytrendwithnon-embeddingparametercountN,whichcanbeﬁttothe
                         ﬁrst term of Equation (1.5), so that                       
                                                                                     N αN
                                                                         L(N)≈         c                                                 (3.1)
                                                                                      N
                                                                                   8
                                    Transformers asymptotically outperform LSTMs             LSTM plateaus after <100 tokens

                                    due to improved use of long contexts                     Transformer improves through the whole context
                         Test Loss 5.4                                            Per-token 
                                                                                  Test Loss 6
                                 4.8
                                 4.2                            LSTMs                     4
                                                                                                                                       Parameters:
                                 3.6                                                                                                     400K
                                                                   1 Layer                5                                              400K
                                                                       2 Layers                                                          2M
                                 3.0       Transformers             4 Layers                                                             3M
                                                                                          3                                              200M
                                 2.4                                                                                                     300M
                                                                                          2
                                         5        6        7        8        9                           1               2              3
                                       10       10       10       10       10                          10              10             10
                                    Parameters (non-embedding)                              Token Index in Context
                                                                               Figure 7
                         To observe these trends it is crucial to study performance as a function of N; if we instead use the total
                         parameter count (including the embedding parameters) the trend is somewhat obscured (see Figure 6). This
                         suggests that the embedding matrix can be made smaller without impacting performance, as has been seen in
                         recent work [LCG+19].
                         AlthoughthesemodelshavebeentrainedontheWebText2dataset,theirtestlossonavarietyofotherdatasets
                         is also a power-law in N with nearly identical power, as shown in Figure 8.
                         3.2.1   ComparingtoLSTMsandUniversalTransformers
                         In Figure 7 we compare LSTM and Transformer performance as a function of non-embedding parameter
                         count N. The LSTMs were trained with the same dataset and context length. We see from these ﬁgures
                         that the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match
                         the Transformer performance for later tokens. We present power-law relationships between performance and
                         context position Appendix D.5, where increasingly large powers for larger models suggest improved ability
                         to quickly recognize patterns.
                         Wealso compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure
                         17intheappendix. These models re-use parameters, and so perform slightly better as a function of N, at the
                         cost of additional compute per-parameter.
                         3.2.2   Generalization Among Data Distributions
                         Wehavealso tested our models on a set of additional text data distributions. The test loss on these datasets
                         as a function of model size is shown in Figure 8; in all cases the models were trained only on the WebText2
                         dataset. We see that the loss on these other data distributions improves smoothly with model size, in direct
                         parallel with the improvement on WebText2. We ﬁnd that generalization depends almost exclusively on the
                         in-distribution validation loss, and does not depend on the duration of training or proximity to convergence.
                         Wealsoobservenodependenceonmodeldepth(seeAppendixD.8).
                         3.3   PerformancewithDatasetSizeandCompute
                         Wedisplay empirical trends for the test loss as a function of dataset size D (in tokens) and training compute
                         CinFigure1.
                         For the trend with D we trained a model with (n          , n     ) = (36,1280) on ﬁxed subsets of the WebText2
                                                                             layer   embd
                         dataset. We stopped training once the test loss ceased to decrease. We see that the resulting test losses can be
                         ﬁt with simple power-law                                       
                                                                                     D αD
                                                                         L(D)≈         c                                                 (3.2)
                                                                                      D
                         in the dataset size. The data and ﬁt appear in Figure 1.
                         The total amount of non-embedding compute used during training can be estimated as C = 6NBS, where
                         Bisthe batch size, S is the number of parameter updates, and the factor of 6 accounts for the forward and
                         backward passes. Thus for a given value of C we can scan over all models with various N to ﬁnd the model
                                                                                   9
                            7                                                          5.0
                                                                  WebText2 (Test)                                     Books during training
                            6                                     Internet Books       4.5                            Wikipedia during training
                                                                  Books                                               Books at convergence
                            5                                     Wikipedia            4.0                            Wikipedia at convergence
                                                                  Common Crawl
                            4                                                          3.5
                           Test Loss                                                   3.0
                            3
                                                                                      Loss on Other Distribution2.5
                                   104     105     106    107     108    109             5.0   4.5   4.0     3.5      3.0       2.5
                                         Parameters (non-embedding)                               Test Loss on Training Distribution
                        Figure 8    Left: Generalization performance to other data distributions improves smoothly with model size,
                        with only a small and very slowly growing offset from the WebText2 training distribution. Right: Gener-
                        alization performance depends only on training distribution performance, and not on the phase of training.
                        Wecomparegeneralization of converged models (points) to that of a single large model (dashed curves) as it
                        trains.
                        with the best performance on step S =        C . Note that in these results the batch size B remains ﬁxed for
                                                                    6BS
                        all models, which means that these empirical results are not truly optimal. We will account for this in later
                        sections using an adjusted Cmin to produce cleaner trends.
                        Theresult appears as the heavy black line on the left-hand plot in Figure 1. It can be ﬁt with
                                                                                 
                                                                                   C αC
                                                                       L(C)≈         c                                               (3.3)
                                                                                   C
                        The ﬁgure also includes images of individual learning curves to clarify when individual models are optimal.
                        Wewillstudytheoptimalallocationofcomputemorecloselylateron. Thedatastronglysuggeststhatsample
                        efﬁciency improves with model size, and we also illustrate this directly in Figure 19 in the appendix.
                        4    Charting the Inﬁnite Data Limit and Overﬁtting
                        In Section 3 we found a number of basic scaling laws for language modeling performance. Here we will
                        study the performance of a model of size N trained on a dataset with D tokens while varying N and D
                        simultaneously. We will empirically demonstrate that the optimally trained test loss accords with the scaling
                        lawofEquation(1.5). Thisprovidesguidanceonhowmuchdatawewouldneedtotrainmodelsofincreasing
                        size while keeping overﬁtting under control.
                        4.1   Proposed L(N,D)Equation
                        Wehavechosentheparameterization (1.5) (repeated here for convenience):
                                                                           "         α         #αD
                                                                               N
                                                                                     α
                                                                               N      D     D
                                                              L(N,D)=            c       + c                                         (4.1)
                                                                               N            D
                        using three principles:
                              1. Changes in vocabulary size or tokenization are expected to rescale the loss by an overall factor. The
                                  parameterization of L(N,D) (and all models of the loss) must naturally allow for such a rescaling.
                              2. Fixing D and sending N → ∞, the overall loss should approach L(D). Conversely, ﬁxing N and
                                  sending D → ∞thelossmustapproachL(N).
                              3. L(N,D)shouldbeanalyticatD = ∞,sothatithasaseriesexpansionin1/D withintegerpowers.
                                  Theoretical support for this principle is signiﬁcantly weaker than for the ﬁrst two.
                        Our choice of L(N,D) satisﬁes the ﬁrst requirement because we can rescale N ,D with changes in the
                                                                                                              c    c
                        vocabulary. This also implies that the values of N ,D have no fundamental meaning.
                                                                            c    c
                                                                                10
                                                Data Size Bottleneck                                  0.5                  Overfitting
                                 4.5
                                 4.0                                                Data Size         0.4                                                Data Size
                                                                                        21M          1                                                       21M
                                                                                        43M          )0.3                                                    43M
                                 3.5                                                    86M                                                                  86M
                                                                                        172M         =                                                       172M
                                                                                        344M         D0.2                                                    344M
                                Test Loss3.0                                            688M         (                                                       688M
                                                                                                     L
                                                                                        1.4B         /
                                                                                        22.0B        L0.1                                                    1.4B
                                                                                                                                                             22.0B
                                 2.5
                                           106         107         108         109                    0.0        10 4       10 3       10 2       10 1
                                                  Params (non-embed)                                                         N N/ D/D
                             Figure 9      Theearly-stopped test loss L(N,D) depends predictably on the dataset size D and model size N
                             according to Equation (1.5). Left: For large D, performance is a straight power law in N. For a smaller ﬁxed
                             D, performance stops improving as N increases and the model begins to overﬁt. (The reverse is also true,
                                                                                                                                       α
                                                                                                                                         N
                             see Figure 4.) Right: The extent of overﬁtting depends predominantly on the ratio N αD /D, as predicted in
                             equation (4.3). The line is our ﬁt to that equation.
                             Sincewestoptrainingearlywhenthetestlossceasestoimproveandoptimizeallmodelsinthesameway,we
                             expect that larger models should always perform better than smaller models. But with ﬁxed ﬁnite D, we also
                             donotexpectanymodeltobecapableofapproachingthebestpossibleloss(ietheentropyoftext). Similarly,
                             a model with ﬁxed size will be capacity-limited. These considerations motivate our second principle. Note
                             that knowledge of L(N) at inﬁnite D and L(D) at inﬁnite N fully determines all the parameters in L(N,D).
                             The third principle is more speculative. There is a simple and general reason one might expect overﬁtting
                             to scale ∝ 1/D at very large D. Overﬁtting should be related to the variance or the signal-to-noise ratio
                             of the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function,
                             since we expect to be able to expand the loss about the D → ∞ limit. However, this argument assumes that
                             1/Dcorrections dominate over other sources of variance, such as the ﬁnite batch size and other limits on the
                             efﬁcacy of optimization. Without empirical conﬁrmation, we would not be very conﬁdent of its applicability.
                             Our third principle explains the asymmetry between the roles of N and D in Equation (1.5). Very similar
                             symmetric expressions4 are possible, but they would not have a 1/D expansion with integer powers, and
                             would require the introduction of an additional parameter.
                             In any case, we will see that our equation for L(N,D) ﬁts the data well, which is the most important justiﬁ-
                             cation for our L(N,D) ansatz.
                             4.2    Results
                             Weregularize all our models with 10% dropout, and by tracking test loss and stopping once it is no longer
                             decreasing. The results are displayed in Figure 9, including a ﬁt to the four parameters α ,α ,N ,D in
                                                                                                                                              N D c c
                             Equation (1.5):
                                                             Parameter         α          α             N                 D
                                                                                 N          D              c                c
                                                                Value        0.076      0.103      6.4 ×1013         1.8 ×1013
                                                                                Table 2     Fits to L(N,D)
                             Weobtain an excellent ﬁt, with the exception of the runs where the dataset has been reduced by a factor of
                                                        7
                             1024, to about 2 × 10 tokens. With such a small dataset, an epoch consists of only 40 parameter updates.
                             Perhaps such a tiny dataset represents a different regime for language modeling, as overﬁtting happens very
                             early in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in
                             Section 3, as here we are ﬁtting the full L(N,D) rather than just L(N,∞) or L(∞,D).
                             To chart the borderlands of the inﬁnite data limit, we can directly study the extent of overﬁtting. For all but
                             the largest models, we see no sign of overﬁtting when training with the full 22B token WebText2 dataset,
                             so we can take it as representative of D = ∞. Thus we can compare ﬁnite D to the inﬁnite data limit by
                                                                                                             β
                                                                                              α             α
                                 4For example, one might have used L(N,D) =               Nc    N + Dc D ,butthisdoesnothavea1/Dexpansion.
                                                                                          N             D
                                                                                              11
                                                             Critical Batch Size vs. Performance
                                                   106
                                                   105
                                                   104                                   Empirical Bcrit, N=3M
                                                                                         Empirical Bcrit, N=85M
                                                                                         Bcrit =2.1×108 tokens L 4.8
                                                                                         Noise Scale Measurement
                                                  Critical Batch Size (Tokens)103
                                                             101             6×100         4×100 3×100
                                                                         WebText2 Train Loss
                        Figure 10     The critical batch size B     follows a power law in the loss as performance increase, and does
                                                                crit
                        not depend directly on the model size. We ﬁnd that the critical batch size approximately doubles for every
                        13%decrease in loss. B        is measured empirically from the data shown in Figure 18, but it is also roughly
                                                  crit
                        predicted by the gradient noise scale, as in [MKAT18].
                        deﬁning
                                                                  δL(N,D)≡ L(N,D) −1                                                 (4.2)
                                                                                 L(N,∞)
                        andstudyingitasafunctionofN,D. Infact,weseeempiricallythatδLdependsonlyaspeciﬁccombination
                        of N and D, as shown in Figure 16. This follows from the scaling law of Equation (1.5), which implies
                                                                                   α       !
                                                                             N             αD
                                                                              N αD Dc
                                                              δL≈ 1+ N                 D        −1                                   (4.3)
                                                                               c
                        Note that at large D this formula also has a series expansion in powers of 1/D.
                        Weestimate that the variation in the loss with different random seeds is roughly 0.02, which means that to
                        avoid overﬁtting when training to within that threshold of convergence we require
                                                                                   3     0.74
                                                                      D&(5×10 )N                                                     (4.4)
                                                                     9
                        With this relation, models smaller than 10 parameters can be trained with minimal overﬁtting on the 22B
                        token WebText2 dataset, but our largest models will encounter some mild overﬁtting. More generally, this
                        relation shows that dataset size may grow sub-linearly in model size while avoiding overﬁtting. Note however
                        that this does not typically represent maximally compute-efﬁcient training. We should also emphasize that
                        wehavenotoptimizedregularization (eg the dropout probability) while varying dataset and model size.
                        5    Scaling Laws with Model Size and Training Time
                        In this section we will demonstrate that a simple scaling law provides a good description for the loss as a
                        function of model size N and training time. First we will explain how to use the results of [MKAT18] to
                        deﬁne a universal training step Smin, which accounts for the fact that most of our models have not been
                        trained at an optimal batch size. Then we will demonstrate that we can ﬁt the model size and training time
                        dependence of the loss using Equation (1.6). Later we will use these results to predict the optimal allocation
                        of training compute between model size and training time, and then conﬁrm that prediction.
                        5.1   AdjustmentforTrainingatBcrit(L)
                        Asimple empirical theory for the batch size dependence of training was developed in [MKAT18] (see also
                              +          +
                        [SLA 18, ZLN 19]). It was argued that there is a critical batch size B            for training; for B up to B
                                                                                                     crit                              crit
                        the batch size can be increased with very minimal degradation in compute-efﬁciency, whereas for B > B
                                                                                                                                       crit
                        increases in B result in diminishing returns. It was also argued that the gradient noise scale provides a simple
                                                                                12
                       prediction for B   , and that neither depends directly on model size except through the value of the loss that
                                       crit
                       has been attained. These results can be used to predict how training time and compute will vary with the
                       batch size. To utilize both training time and compute as effectively as possible, it is best to train with a batch
                       size B ≈ B     . Training at B  B     minimizes the number of training steps, while B  B       minimizes
                                   crit                   crit                                                      crit
                       the use of compute.
                       Morespeciﬁcally, it was demonstrated that for a wide variety of neural network tasks, the number of training
                       steps S and the number of data examples processed E = BS satisfy the simple relation
                                                              S         E          
                                                               Smin −1       Emin −1 =1                                      (5.1)
                       whentraining to any ﬁxed value of the loss L. Here Smin is the minimum number of steps necessary to reach
                       L, while E     is the minimum number of data examples that must be processed.
                                  min
                       Wedemonstrate the relation (5.1) for Transformers in Figure 18 in the appendix. This relation deﬁnes the
                       critical batch size
                                                                    B (L)≡ Emin                                              (5.2)
                                                                      crit      S
                                                                                  min
                       which is a function of the target value of the loss. Training at the critical batch size makes a roughly optimal
                       time/compute tradeoff, requiring 2Smin training steps and processing E = 2Emin data examples.
                       In Figure 10 we have plotted the critical batch size and gradient noise scale5 as a function of training loss for
                       twodifferent models. We see that B     (L) is independent of model size, and only depends on the loss L. So
                                                           crit
                       the predictions of [MKAT18] continue to hold for Transformer language models. The critical batch size can
                       be ﬁt with a power-law in the loss
                                                                                 B
                                                                   B (L)≈          ∗                                         (5.3)
                                                                     crit      L1/αB
                       where B ≈ 2×108 andα ≈0.21.
                               ∗                  B
                       Wehavechosen this parameterization for Bcrit(L) because as the loss approaches its minimum value Lmin,
                       the gradient noise scale is expected to diverge, and we expect Bcrit to track this noise scale. We do not
                       knowL      , as we see no sign that our models are approaching it, but L   >0sincetheentropy of natural
                               min                                                            min
                       language is non-zero. Since apparently Lmin is much smaller than the values of L we have achieved, we used
                       a parameterization where B     diverges as L → 0.
                                                  crit
                       Wewill use Bcrit(L) to estimate the relation between the number of training steps S while training at batch
                                  19
                       size B = 2    tokens and the number of training steps while training at B  B    . This is simply
                                                                                                     crit
                                            S    (S) ≡         S             (minimumsteps, at B  B        )                (5.4)
                                             min        1+B (L)/B                                        crit
                                                              crit
                       for any given target value L for the loss. This also deﬁnes a critical value of the compute needed to train to L
                       with a model of size N if we were to train at B  B    (L). This is
                                                                           crit
                                          C (C)≡              C             (minimumcompute,atB  B           )              (5.5)
                                            min        1+B/B (L)                                           crit
                                                                crit
                       where C = 6NBS estimates the (non-embedding) compute used at batch size B.
                       5.2  Results for L(N,Smin) and Performance with Model Size and Compute
                       NowwewilluseSmin deﬁnedinEquation(5.4)toobtainasimpleanduniversalﬁtforthedependenceofthe
                       loss on model size and training time in the inﬁnite data limit. We will ﬁt the stable, Adam-optimized training
                       runs using Equation (1.6), repeated here for convenience:
                                                                                         
                                                                          N αN          S     αS
                                                        L(N,S      ) =      c     +       c                                  (5.6)
                                                                min       N            S
                                                                                        min
                       for the loss. We include all training steps after the warmup period of the learning rate schedule, and ﬁnd a ﬁt
                       to the data with the parameters:
                          5Although the critical batch size roughly matches the gradient noise scale, we are using a direct measurements of
                       B fromFigures18and10forallourlateranalyses.
                         crit
                                                                           13
                           8 Performance vs Compute Budget                                   Performance vs Steps
                                                                                    5.4
                           7                                        100             4.8
                           6                                                                                                  105
                                                                    10 1            4.2
                           5
                                                                    10 2            3.6
                           4                                        10 3                                                           Steps
                          Test Loss                                       PF-dayss  Test Loss3.0
                           3                                        10 4                                                      104
                                                                    10 5            2.4
                           2     104        106         108                               106       107        108       109
                                 Parameters (non-embedding)                                 Parameters (non-embedding)
                       Figure 11    When we hold either total compute or number of training steps ﬁxed, performance follows
                       L(N,S) from Equation (5.6). Each value of compute budget has an associated optimal model size that
                       maximizesperformance. MediocreﬁtsatsmallS areunsurprising,asthepower-lawequationforthelearning
                       curves breaks down very early in training.
                                                   Parameter     α        α          N            S
                                                                   N       S           c            c
                                                                                         13            3
                                                     Value      0.077    0.76    6.5 ×10       2.1 ×10
                                                                 Table 3   Fits to L(N,S)
                       Withtheseparameters, weobtainthelearningcurveﬁtsinFigure4. Thoughtheﬁtsareimperfect,webelieve
                       they are quite compelling given the simplicity of Equation (5.6).
                       The data and ﬁts can be visualized in a different and more interesting way, as shown in Figure 11. There we
                       study the test loss as a function of model size while ﬁxing either the total non-embedding compute C used
                       in training, or the number of steps S. For the ﬁts we use Equation (5.5) and (5.4) along with the parameters
                       above and Equation (5.6).
                       The power-law dependence of the loss on Smin reﬂects the interplay of optimizer dynamics and the loss
                       landscape. Since the ﬁts are best late in training, when the loss may be approximately quadratic, the power-
                       law should provide information about the spectrum of the Hessian of the loss. Its universality suggests that
                       the Hessian eigenvalue density is roughly independent of model size.
                       5.3   LowerBoundonEarlyStoppingStep
                       The results for L(N,Smin) can be used to derive a lower-bound (and rough estimate) of the step at which
                       early stopping should occur when training is data limited. It is motivated by the idea that ﬁnite and inﬁnite D
                       learning curves for a given model will be very similar until we reach Smin ≈ Sstop. Thus overﬁtting should
                       beproportionaltothecorrectionfromsimplyendingtrainingatS              . This will underestimate S    , because
                                                                                         stop                           stop
                       in reality the test loss will decrease more slowly when we have a ﬁnite D, and therefore we will require more
                       training steps to reach the optimal test loss at ﬁnite D. This line of reasoning leads to the inequality
                                                        S    (N,D)&                  Sc                                         (5.7)
                                                         stop                                  1/αS
                                                                        [L(N,D)−L(N,∞)]
                       where L(N,∞) is the converged loss, evaluated with inﬁnite available data. This inequality and its com-
                       parison to the empirical data is displayed in Figure 16 in the appendix. In that ﬁgure, the values of S
                                                                                                                                 stop
                       and L(N,D) are empirical (though S          is adjusted to mimic training at B  B       ), while L(N,∞) is
                                                              stop                                           crit
                       computed from the ﬁt to L(N,D) evaluated at D = ∞.
                       6    OptimalAllocation of the Compute Budget
                       Wedisplayed the empirical trend of performance as a function of the computation used during training in
                       the top-right of Figure 1. However, this result involved training at a ﬁxed batch size B, whereas we know
                                                                             14
                                                                                                                             Smaller models require 
                                                                                                                           more steps to train, while 
                                                                                                                          larger models require fewer
                                                Models between 0.6x and 2.2x the 
                                                optimal size can be trained with a 
                                                   20% larger compute budget
                                                                                                                                 Our framework does not 
                                                                                                                             capture early training dynamics
                              Figure 12        Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger
                              or smaller models can be trained with minimal additional compute. Right: Models larger than the compute-
                              efﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-
                              lelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the
                              power-law region of the learning curve, after initial transient effects.
                                                                       7                                   L= C /2.3 108 0.050
                                                                                                               ( min            )
                                                                       6                                   L=(C/2.0 107) 0.057
                                                                       5
                                                                       4
                                                                      Test Loss3
                                                                       2 8               6             4              2            0
                                                                      10             10            10            10             10
                                                                                   Compute (PF-days), non-embedding
                              Figure 13         When adjusting performance to simulate training far below the critical batch size, we ﬁnd a
                              somewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous
                              lump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks
                              in the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger
                              compute.
                                                                                          6
                              that in fact we could train more efﬁciently by training at the batch size B                                   discussed in Section 5.1.
                                                                                                                                       crit
                              Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,
                              and correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more
                              predictable trends.
                              In this section we will adjust for this oversight. More importantly, we will use the results of Section 5
                              to determine the optimal allocation of compute between model size N and the quantity of data processed
                              during training, namely 2B               S      . We will determine this allocation both empirically and theoretically, by
                                                                   crit  min
                              using the equation for L(N,Smin), and we will demonstrate that these methods agree.
                              6.1     OptimalPerformanceandAllocations
                              Let us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is
                              plotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the
                              newﬁtwithCmin issomewhatimproved.
                              GivenL(C            ), it is natural to ask for the optimal model size N(C                    ) that provides the minimal loss with a
                                             min                                                                       min
                              given quantity of training compute. The optimal model size is shown in Figure 14. We observe that N(Cmin)
                                   6One might ask why we did not simply train at B                 in the ﬁrst place. The reason is that it depends not only on the
                                                                                               crit
                              modelbutalso on the target value of the loss we wish to achieve, and so is a moving target.
                                                                                                     15
                                       N=(1.3 109) C0.73                                            S    (adjusted)
                                                      min                                            min
                                       N=(1.6 109) C0.88                               15000        S   =(5.4 103) C0.03
                            107                                                                      min             min
                                                                                                    S (fixed-batch)
                                                                                       10000
                            105                                                       Steps
                                                                                        5000
                            103
                           Parameters (non-embedding)                                      0
                                      10 7       10 5        10 3       10 1                      10 7       10 5       10 3       10 1
                                       Compute (PF-days), non-embedding                        Compute (PF-days), excluding embeddings
                        Figure 14    Left: Each value of the compute budget Cmin has an associated optimal model size N. Optimal
                        model size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number
                        of data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.
                        Right: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most
                        of the growth in data examples processed can be used for increased batch sizes.
                        can be ﬁt very well with a power-law
                                                                                         0.73
                                                                    N(Cmin) ∝ (Cmin)         .                                       (6.1)
                        In Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4).
                        Bydeﬁnition Cmin ≡ 6NBcritS, and so we can use N(Cmin) to extract further results. In particular, since
                        prior ﬁts show B ∝ L−4.8 and L ∝ C−0.05, we can conclude that Bcrit ∝ C0.24. This leads us to conclude
                                                                 min                                     min
                        that the optimal number of steps will only grow very slowly with compute, as
                                                                       S     ∝(C )0.03,                                              (6.2)
                                                                        min       min
                        matchingtheempiricalresultsinFigure14. Infactthemeasuredexponentissufﬁcientlysmallthatourresults
                        mayevenbeconsistent with an exponent of zero.
                        Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we
                        should predominantly increase the model size N, while simultaneously scaling up the batch size via B ∝
                        Bcrit with negligible increase in the number of serial steps. Since compute-efﬁcient training uses relatively
                        few optimization steps, additional work on speeding up early training dynamics may be warranted.
                        6.2   Predictions from L(N,Smin)
                        The results for L(C      ) and the allocations can be predicted from the L(N,S            ) equation obtained in
                                              min                                                             min
                        Section 5. Given our equation for L(N,S         ), we can substitute S      = Cmin and then ﬁnd the minimum
                                                                     min                       min     6NB
                        of the loss as a function of N, while ﬁxing the training compute. We carry out this procedure in detail in
                        Appendix B, where we also provide some additional predictions.
                        For the loss as a function of training compute, we predict that
                                                                                        min
                                                                                  Cmin αC
                                                                   L(C      ) =     c                                                (6.3)
                                                                        min       C
                                                                                    min
                        where
                                                           αmin ≡               1             ≈0.054                                 (6.4)
                                                             C      1/α +1/α +1/α
                                                                         S        B        N
                        in excellent agreement with the exponent of Figure 13. We also predict that
                                                                                αmin/α             0.71
                                                           N(C      ) ∝ (C     ) C     N ≈(C      )                                  (6.5)
                                                                min        min                 min
                        which also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive
                        framework for the performance of language modeling.
                                                                                16
                                                                                The intersection point is sensitive to 
                                                                                the precise power-law parameters
                     Figure 15  Far beyond the model sizes we study empirically, we ﬁnd a contradiction between our equations
                     for L(C   ) and L(D)duetotheslowgrowthofdataneededforcompute-efﬁcienttraining. Theintersection
                            min
                     marks the point before which we expect our predictions to break down. The location of this point is highly
                     sensitive to the precise exponents from our power-law ﬁts.
                     6.3  Contradictions and a Conjecture
                     Weobserve no signs of deviation from straight power-law trends at large values of compute, data, or model
                     size. Our trends must eventually level off, though, since natural language has non-zero entropy.
                     Indeed, the trends for compute-efﬁcient training described in this section already contain an apparent contra-
                     diction. At scales several orders of magnitude above those documented here, the performance predicted by
                     the L(Cmin)scaling law decreases below what should be possible given the slow growth in training data with
                     compute. This implies that our scaling laws must break down before this point, but we conjecture that the
                     intersection point has a deeper meaning: it provides an estimate of the point at which Transformer language
                     models reach maximal performance.
                     Since the amount of data used by compute-efﬁcient training grows slowly with the compute budget, the
                     performance predicted by L(Cmin) eventually hits a lower bound set by the L(D) power law (see Figure 15).
                     Let us work this out in more detail.
                     Tokeepoverﬁtting under control, the results of Section 4 imply that we should scale the dataset size as
                                                            D∝N0.74 ∝C0.54                                       (6.6)
                                                                           min
                     where we have used the compute-efﬁcient N(Cmin) from Figure 14.
                     Let us compare this to the data requirements of compute-efﬁcient training. If we train at the critical batch
                     size (i.e. C = 2Cmin) and never re-use data during training, we ﬁnd that data usage grows with compute as
                                                    2Cmin             10                       0.26
                                       D(Cmin) = 6N(Cmin) ≈ 4×10 tokens (Cmin/PF-Day)                            (6.7)
                     This is the maximum rate at which the dataset size can productively grow with compute, since it means that
                     we are only training for a single epoch. But it grows the dataset much more slowly than in Equation (6.6).
                     It appears to imply that compute-efﬁcient training will eventually run into a problem with overﬁtting, even if
                     the training process never re-uses any data!
                     According to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by overﬁtting), the
                     loss should scale as L(D) ∝ D−0.095. This implies that the loss would scale with compute as L(D(Cmin)) ∝
                     C−0.03 once we are data-limited. Once again, we have a contradiction, as this will eventually intersect with
                      min
                     our prediction for L(Cmin) from Figure 13, where we found a scaling L(Cmin) ∝ C−0.050.
                                                                                                min
                     Theintersection point of L(D(Cmin)) and L(Cmin) occurs at
                            ∗     4             ∗      12               ∗     12           ∗
                          C ∼10 PF-Days N ∼10 parameters, D ∼10 tokens, L ∼1.7nats/token                         (6.8)
                     though the numerical values are highly uncertain, varying by an order or magnitude in either direction de-
                     pending on the precise values of the exponents from the power-law ﬁts. The most obvious interpretation is
                     that our scaling laws break down at or before we reach this point, which is still many orders of magnitude
                     awayinbothcomputeandmodelsize.
                                                                    17
                       Onemightalsoconjecture that this intersection point has a deeper meaning. If we cannot increase the model
                       size beyond N∗ without qualitatively different data requirements, perhaps this means that once we reach
                       C∗     and N∗, we have extracted all of the reliable information available in natural language data. In this
                         min            ∗                                                              7
                       interpretation, L   would provide a rough estimate for the entropy-per-token of natural language. In this
                       scenario, we would expect the loss trend to level off at or before L∗.
                       We can guess at the functional form of L(Cmin) as it levels off by considering a version of our training
                       dataset with added noise. For example, we could append a random string of tokens to each context shown
                       to the model to artiﬁcially boost the loss by a constant additive factor. Then, the distance from the noise
                       ﬂoorL−Lnoise wouldbeamoremeaningfulperformancemetric,withevenasmalldecreaseinthisdistance
                       potentially representing a signiﬁcant boost in qualitative performance. Since the artiﬁcial noise would affect
                       all of our trends equally, the critical point of 6.8 would not change (aside from the absolute value of L∗), and
                       maybemeaningfulevenifit occurs after the leveling off.
                       7    Related Work
                       Power laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset
                       size in density estimation [Was06] and in random forest models [Bia12] may be connected with our results.
                       These models suggest that power-law exponents may have a very rough interpretation as the inverse of the
                       numberofrelevant features in the data.
                       Some early [BB01, Goo01] work found power-law scalings between performance and dataset size. More
                                           +
                       recent work [HNA 17, HAD19] also investigated scaling between model size and data size; their work is
                                                                    8                             +
                       perhaps the closest to ours in the literature . Note, however, that [HNA 17] found super-linear scaling of
                       dataset size with model size, whereas we ﬁnd a sub-linear scaling. There are some parallels between our
                       ﬁndings on optimal allocation of compute and [Kom19], including power-law learning curves. EfﬁcientNets
                       [TL19]alsoappeartoobeyanapproximatepower-lawrelationbetweenaccuracyandmodelsize. Veryrecent
                       work [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and ﬁts an
                       ansatz similar to ours.
                       EfﬁcientNet [TL19] advocates scaling depth and width exponentially (with different coefﬁcients) for optimal
                       performance of image models, resulting in a power-law scaling of width as a function of depth. We ﬁnd that
                       for language models this power should be roughly one when scaling up (as width/depth should remain ﬁxed).
                       Butmoreimportantly,weﬁndthattheprecisearchitecturalhyperparametersareunimportantcomparedtothe
                       overall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles
                       of shallower models, which could potentially explain this ﬁnding. Earlier work [ZK16] has compared width
                       and depth, and found that wide ResNets can outperform deep ResNets on image classiﬁcation. Some studies
                       ﬁx computation per data example, which tends to scale in proportion to the number of model parameters,
                       whereas we investigate scaling with both model size and the quantity of training computation.
                       Various works [AS17, BHMM18]haveinvestigatedgeneralizationinhighlyoverparameterizedmodels,ﬁnd-
                       ing a “jamming transition” [GJS+19] when the model size reaches the dataset size (this may require training
                       many orders of magnitude beyond typical practice, and in particular does not use early stopping). We do
                       not observe such a transition, and ﬁnd that the necessary training data scales sublinearly in the model size.
                                                                                               +
                       Expansionsinthemodelsize,particularlyatlargewidth[JGH18,LXS 19],mayprovideausefulframework
                       for thinking about some of our scaling relations. Our results on optimization, such as the shape of learning
                       curves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions
                             +
                       [ZLN 19] in realistic settings. Making this connection quantitative will require a characterization of the
                       Hessian spectrum [Pap18, GKX19, GARD18].
                       8    Discussion
                       Wehaveobserved consistent scalings of language model log-likelihood loss with non-embedding parameter
                       count N, dataset size D, and optimized training computation Cmin, as encapsulated in Equations (1.5) and
                       (1.6). Conversely, we ﬁnd very weak dependence on many architectural and optimization hyperparameters.
                       Since scalings with N,D,Cmin are power-laws, there are diminishing returns with increasing scale.
                           7Deﬁning words using the wc utility, the WebText2 dataset has 1.4 tokens per word and 4.3 characters per token.
                           8After this work was completed, [RRBS19a] also appeared, which makes similar predictions for the dependence of
                       loss on both model and dataset size.
                                                                             18
          WewereabletopreciselymodelthedependenceofthelossonN andD,andalternativelyonN andS,when
          these parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude
          of overﬁtting, early stopping step, and data requirements when training large language models. So our scaling
          relations go beyond mere observation to provide a predictive framework. One might interpret these relations
          as analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way,
          independent of most of the details of its microscopic consituents.
          It is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a
          maximum likelihood loss, and perhaps in other settings as well. To this purpose, it will be interesting to
          test these relations on other domains, such as images, audio, and video models, and perhaps also for random
          network distillation. At this point we do not know which of our results depend on the structure of natural
          language data, and which are universal. It would also be exciting to ﬁnd a theoretical framework from
          which the scaling relations can be derived: a ‘statistical mechanics’ underlying the ‘thermodynamics’ we
          have observed. Such a theory might make it possible to derive other more precise predictions, and provide a
          systematic understanding of the limitations of the scaling laws.
          In the domain of natural language, it will be important to investigate whether continued improvement on the
          loss translates into improvement on relevant language tasks. Smooth quantitative change can mask major
          qualitative improvements: “more is different”. For example, the smooth aggregate growth of the economy
          provides no indication of the speciﬁc technological developments that underwrite it. Similarly, the smooth
          improvements in language model loss may hide seemingly qualitative changes in capability.
          Our results strongly suggest that larger models will continue to perform better, and will also be much more
          sample efﬁcient than has been previously appreciated. Big models may be more important than big data.
          In this context, further investigation into model parallelism is warranted. Deep models can be trained using
          pipelining [HCC+18], whichsplitsparametersdepth-wisebetweendevices,buteventuallyrequiresincreased
          batch sizes as more devices are used. Wide networks on the other hand are more amenable to parallelization
          [SCP+18], since large layers can be split between multiple workers with less serial dependency. Sparsity
          [CGRS19,GRK17]orbranching(e.g. [KSH12])mayallowforevenfastertrainingoflargenetworksthrough
          increased modelparallelism. Andusingmethodslike[WRH17,WYL19],whichgrownetworksastheytrain,
          it might be possible to remain on the compute-efﬁcient frontier for an entire training run.
          Acknowledgements
          We would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner,
          DannyHernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feed-
          back on drafts of this work.
                               19
                        Appendices
                        A SummaryofPowerLaws
                        For easier reference, we provide a summary below of the key trends described throughout the paper.
                                 Parameters       Data     Compute       Batch Size     Equation
                                       N           ∞           ∞            Fixed       L(N)=(N /N)αN
                                                                                                     c
                                                                                                           αD
                                      ∞            D      Early Stop        Fixed       L(D)=(Dc/D)
                                                                                                          α
                                   Optimal         ∞           C            Fixed       L(C)=(C /C) C (naive)
                                                                                                     c
                                                                                                                   αmin
                                     N            D          C           BB            L(C      ) = Cmin/C          C
                                       opt          opt        min               crit        min      c       min     
                                                                                                             α          αD
                                                                                                        N  N       D
                                       N           D      Early Stop        Fixed       L(N,D)=           c  αD + c
                                                                                                         N        D         
                                                                                                      N αN           S       αS
                                       N           ∞        S steps           B         L(N,S)=         c     +         c
                                                                                                       N           Smin(S,B)
                                                                             Table 4
                        Theempirical ﬁtted values for these trends are:
                                                    PowerLaw          Scale (tokenization-dependent)
                                                                                      13
                                                    α =0.076          N =8.8×10 params(non-embed)
                                                     N                  c
                                                    α =0.095          D =5.4×1013tokens
                                                     D                  c
                                                                                      7
                                                    αC =0.057         Cc = 1.6×10 PF-days
                                                     min                min             8
                                                    αC =0.050 Cc            =3.1×10 PF-days
                                                                                      8
                                                    α =0.21           B =2.1×10 tokens
                                                     B                  ∗
                                                                                     3
                                                    αS =0.76          Sc = 2.1×10 steps
                                                                             Table 5
                        Theoptimal parameters for compute efﬁcient training are given by:
                                          Compute-Efﬁcient Value                PowerLaw        Scale
                                                          p
                                                           N                                                   9
                                          N =N ·C                               p   =0.73       N =1.3·10 params
                                            opt      e    min                    N                e
                                                           B             pB                                    6
                                          BB = ∗ =BC                           p =0.24         B =2.0·10 tokens
                                                  crit     1/α        e  min     B                e
                                                          L   B
                                          S     =S ·CpS (lowerbound)            p =0.03         S =5.4·103 steps
                                            min      e   min                     S                e
                                                          p
                                          D =D ·C D (1epoch)                    p =0.27         D =2·1010tokens
                                            opt      e    min                    D                e
                                                                             Table 6
                        B EmpiricalModelofCompute-EfﬁcientFrontier
                        Throughout this appendix all values of C,S, and αC are adjusted for training at the critical batch size Bcrit.
                        Wehaveleft off the ‘adj’ label to avoid cluttering the notation.
                        B.1    DeﬁningEquations
                        The power-law ﬁt to the learning curves implies a simple prescription for compute-efﬁcient training. In this
                        appendix, we will derive the optimal performance, model size, and number of training steps as a function of
                                                                                20
                         the compute budget. We start with the Equation (1.6), repeated here for convenience:
                                                                                            
                                                                                N αN           S    αS
                                                                 L(N,S)=           c      +      c      .                                 (B.1)
                                                                                 N              S
                         Here, S represents the number of parameter updates when training at the critical batch size [MKAT18],
                         which was deﬁned in Equation (5.2)9:
                                                                                        B
                                                                           B(L)=          ∗  .                                            (B.2)
                                                                                      L1/αB
                         We would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =
                         C/(6NB(L)),whereC isthenumberofFLOPsusedinthetrainingrun:
                                                                                                      
                                                                               α                           α
                                                                          N      N                 N         S
                                                         L(N,C)=            c      + 6B S                      .                          (B.3)
                                                                          N                ∗ cL1/αBC
                                             
                                             
                         Now,weset∂ L           =0toﬁndtheconditionforoptimality:
                                         N C
                                                              ∂L
                                                         0 =       
                                                              ∂N C
                                                                                                                               
                                                                             αN                               αS                 
                                                                α      N            α                N                   N ∂L 
                                                           =− N          c       + S 6B S                          1−5           
                                                                                             ∗ c                               
                                                                 N      N           N             L1/αBC                  L ∂N C
                                                                                                                            
                                                                              
                                          α      N αN                      N       αS
                                    =⇒ N           c       = 6B S                                                                         (B.4)
                                           α      N                ∗ cL1/αBC
                                            S
                         Equation (B.3) and (B.4) together determine the compute-efﬁcient frontier.
                         B.2    Efﬁcient Training
                         Nowweassembletheimplications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields
                                                            L(N (C),C)=1+αNL(N ,∞),                                                     (B.5)
                                                                 eﬀ                     α           eﬀ
                                                                                          S
                         which implies that for compute-efﬁcient training, we should train to a ﬁxed percentage αN ≈ 10% above
                                                                                                                            αS
                         the converged loss. Next, let’s determine how the optimal loss depends on the compute budget. Eliminating
                         Nyieldsapower-lawdependenceofperformanceoncompute:
                                                                          L(C)=CcαC                                                     (B.6)
                                                                                       C
                         where we deﬁned
                                                       α =1/(1/α +1/α +1/α )≈0.052                                                        (B.7)
                                                         C             S        B         N
                                                                                                       
                                                                                       1/α +1/α             1/α
                                                                                α         S      N    α        S
                                                       C =6N B S 1+ N                                   S        .                        (B.8)
                                                         c        c  ∗ c         α                    α
                                                                                  S                    N
                         Similarly, we can eliminate L to ﬁnd N (C):
                                                                                                 
                                                              N(C)          C αC/αN             α     1/αN
                                                                      =                    1+ N                                           (B.9)
                                                                N          C                    α
                                                                  c          c                    S
                         and
                                                                                                
                                                                      C            α      −1/αN     C αC/αS
                                                         S(C)=          c     1+ N                                                       (B.10)
                                                                    6N B            α               C
                                                                       c  ∗          S                c
                             9There is a slight ambiguity here: we can imagine training either at a constant batch size B (L     ), or we could
                                                                                                                            target
                                                              ˜             ˜
                         instead train at a variable batch size B (L), where B is the instantaneous critical batch size (as opposed to B, which is
                         the averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see
                         [MKAT18]).
                                                                                   21
                        B.3    ComparisontoInefﬁcient
                        Typically, researchers train models until they appear to be close to convergence. In this section, we compare
                        the efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor
                        f as the percent deviation from the converged loss:
                                                                L(N,C)=(1+f)L(N,∞).                                                (B.11)
                        For compute-efﬁcient training we have f = αN/αS 0≈ 10% from the previous section, but researchers
                        typically use a much smaller value. Here, we choose f = 2% as an estimate. For a ﬁxed value of the loss,
                        wepredict:
                                                                                 
                                                                 N         1+f 1/αN
                                                                   f =                    ≈2.7                                     (B.12)
                                                                 N 0       1+f0
                                                                   f
                                                                                  !
                                                                           1+ 1      1/αS
                                                                  Sf =          f         ≈0.13                                    (B.13)
                                                                 Sf0       1+ 1
                                                                                f0
                                                                 C       N S
                                                                   f =     f    f ≈0.35                                            (B.14)
                                                                 Cf0     Nf0 Sf0
                        So that compute-efﬁcient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less
                        compute to reach the same loss.
                        B.4    Suboptimal Model Sizes
                        Wecansolve A.1 to ﬁnd an expression for the amount of compute needed to reach a given value of the loss
                        LwithamodelofsizeN:
                                                                                         
                                                                            N                N αN −1/αS
                                                   C(N,L)= 6B S                      L−        c              .                    (B.15)
                                                                      ∗ cL1/αB               N
                        UsingA.6andA.9,wecaneliminateLinfavorofN                  (L), the model size which reaches L most efﬁciently.
                                                                               eﬀ
                        From there, we ﬁnd an expression for the excess compute needed as a consequence of using a suboptimal
                        model size:                                                            
                                                                                                   α      −1/αS
                                                  C(N,N )            N          α           N        N
                                                           eﬀ    =        1+ S 1−              eﬀ                .                 (B.16)
                                                 C(N ,N )           N          α              N
                                                      eﬀ    eﬀ        eﬀ         N
                        The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a
                        20%increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A
                        larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism
                        and faster training if sufﬁcient harware is available (see Figure Y):
                                                                                            
                                                     S(N,N )                 α           N      αN     −1/αS
                                                              eﬀ   = 1+ S 1−                eﬀ                .                    (B.17)
                                                    S(N ,N )                α              N
                                                         eﬀ    eﬀ             N
                        A2.2xlargermodelrequires45%fewerstepsatacostof20%moretrainingcompute. Notethatthisequation
                        should not be trusted for very large models, as it is only valid in the power-law region of the learning curve
                        after initial transient effects.
                        C Caveats
                        In this section we list some potential caveats to our analysis.
                               • At present we do not have a solid theoretical understanding for any of our proposed scaling laws.
                                  The scaling relations with model size and compute are especially mysterious. It may be possible to
                                  understand scaling at very large D holding model size ﬁxed [AS17], and also the shape of learning
                                  curves late in training, by modeling the loss with a noisy quadratic. But the scaling with D at very
                                  large model size still remains mysterious. Without a theory or a systematic understanding of the
                                  corrections to our scaling laws, it’s difﬁcult to determine in what circumstances they can be trusted.
                                                                                22
                                                                                                     6
                                                                                                                                     Test Loss          1010
                                                Early Stopping Step                                  5                               Train Loss
                                105                                               Data Size          4                                                  109
                                                                                       21M
                                p                                                      43M          Loss
                                o                                                      86M
                                t
                               Ss                                                      172M
                                104                                                    344M          3
                                                                                       688M                                                             108
                                                                                       1.4B                                                                    Dataset Size (Tokens)
                                103                                                                  2
                                      103                 104                105                             103            104            105
                                               S ×[L(N,D)   L(N,  )] 1/ S                                                  Step
                                                c
                            Figure 16       Left: We characterize the step on which early stopping occurs, as a function of the extent of
                            overﬁtting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right:
                            Wedisplay train and test loss for a series of 300M parameter models trained on different sized dataset sub-
                            samples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the
                            degree of overﬁtting (as compared to the inﬁnite data limit) is signiﬁcantly overestimated by L                           −L
                                                                                                                                                 test       train
                            (denoted by a black bar for each run).
                                    • We are not especially conﬁdent in the prediction of B                   (L) for values of the loss far outside the
                                                                                                           crit
                                       range we have explored. Changes in B                   could have a signiﬁcant impact on trade-offs between
                                                                                         crit
                                       data parallelism and the number of serial training steps required, which would have a major impact
                                       ontraining time.
                                    • We did not thoroughly investigate the small data regime, and our ﬁts for L(N,D) were poor for
                                       the smallest values of D (where an epoch corresponded to only 40 steps). Furthermore, we did
                                       not experiment with regularization and data augmentation. Improvements in these could alter our
                                       results, quantitatively or qualitatively.
                                    • Weused the estimated training compute C ≈ 6NBS, which did not include contributions propor-
                                       tional to nctx (see Section 2.1). So our scalings with compute may be confounded in practice in the
                                       regime of very large n         , speciﬁcally where n          &12d          .
                                                                   ctx                          ctx          model
                                    • We tuned learning rates, and we experimented with learning rate schedules. But we may have
                                       neglectedtotunesomehyperparameter(e.g. intializationscaleormomentum)thathaveanimportant
                                       effect on scaling.
                                    • Theoptimalchoiceoflearningrateissensitivetothetargetloss. Whentrainingclosetoconvergence,
                                       it may be necessary to use a smaller learning rate to avoid divergences. But when conducting a short
                                       training run (eg due to compute limitations), it may be possible to use a larger learning rate. We did
                                       not experiment with higher learning rates for training runs that did not proceed to convergence.
                            D SupplementalFigures
                            D.1     Early Stopping and Test vs Train
                            In section 5.3 we described the result shown in Figure 16, which provides a prediction for a lower bound on
                            the early stopping step. We also show the train and test loss for a given model size when training on different
                            sized datasets.
                            D.2     Universal Transformers
                                                                                                                                          +
                            Wecompare the performance of standard Transformers to recurrent Transformers [DGV 18] in Figure 17.
                            These models re-use parameters, and so perform slightly better as a function of N, but slightly worse as a
                            function of compute C. We include several different different possibilities for parameter re-use.
                            D.3     Batch Size
                            Wemeasure the critical batch size using the data displayed in ﬁgure 18. This made it possible to estimate
                            Bcrit(L) in ﬁgure 10.
                                                                                             23
                                 4.5                                                                4.5                              2x Reuse
                                                                                                                                     4x Reuse
                                 4.0                                                                4.0                              8x Reuse
                                                                                                                                     Non-recurrent Models
                                 3.5                                                                3.5
                               Test Loss3.0  2x Reuse                                              Test Loss3.0
                                             4x Reuse
                                             8x Reuse
                                 2.5         Non-recurrent Models                                   2.5
                                     105          106         107         108          109               105          106         107         108          109
                                       Parameters, including reuse (non-embedding)                                  Parameters (non-embedding)
                                                                                                 +
                            Figure 17       We compare recurrent Transformers [DGV 18], which re-use parameters, to standard Trans-
                            formers. RecurrentTransformersperformslightlybetterwhencomparingmodelswithequalparametercount,
                            but slightly worse when accounting for reuse and comparing per FLOP.
                                          Batch Size Scan - 3M Params                  10                Batch Size Scan - 85M Params                   10
                                  1011
                                  1010                                                             1010
                                                                                       8                                                                8
                                   109
                                   108                                                 6           108                                                  6
                                   107                                                      Test Loss                                                        Test Loss
                                 Tokens Processed                                      4          Tokens Processed106                                   4
                                   106
                                            102      103       104      105                            101      102     103      104     105
                                                           Step                                                            Step
                            Figure 18       These ﬁgures demonstrate ﬁts to Equation (5.1) for a large number of values of the loss L, and
                            for two different Transformer model sizes. These ﬁts were used to measure B                       (L) for Figure 10.
                                                                                                                           crit
                            D.4    SampleEfﬁciencyvsModelSize
                            It is easy to see from ﬁgure 2 that larger models train faster, and are therefore more sample efﬁcient. We
                            provide another way of looking at this phenomenon in ﬁgure 19, which shows when different models reach
                            various ﬁxed values of the loss.
                                                                                        5.5      )1011                                                   5.5
                                   5                                                             n
                                )10                                                     5.0      i                                                       5.0
                                n                                                                m
                                i                                                                E
                                m                                                                (
                                S                                                                 
                                (                                                       4.5      s   10                                                  4.5
                                                                                                 e10
                                s                                                                l
                                p                                                                p
                                e                                                       4.0      m
                                t104                                                             a                                                       4.0
                                S
                                                                                             Lossx   9                                                        Loss
                                m                                                                E 10
                                u                                                       3.5                                                              3.5
                                m                                                                m
                                i                                                                u
                                n                                                       3.0      m                                                       3.0
                                i                                                                i   8
                                M  3                                                             n 10
                                 10                                                     2.5      i                                                       2.5
                                                                                                 M
                                   106             107             108                                106            107            108
                                              Parameters (non-embedding)                                        Parameters (non-embedding)
                            Figure 19       The number of minimum serial steps needed to reach any ﬁxed value of the test loss decreases
                            precipitously with model size. Sample efﬁciency (show here for training far below the critical batch size)
                            improves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model
                            to a very large one.
                                                                                            24
                                                                                                                                      Per-token Loss (774M Params)                                 103
                                          8                                                4.0+3.2 T 0.47                       10
                                          7                                                3.4+4.0 T 0.56
                                                                                           2.9+4.5 T 0.56        108
                                                                                           2.7+4.9 T 0.60                         8                                                                102
                                          6                                                2.4+5.1 T 0.61
                                                                                           2.3+5.4 T 0.62
                                          5                                                                      107              6
                                          4                                                                                    Test Loss                                                           101 Token Index
                                         Per-Token Test Loss                                                            Model Parameters4
                                          3                                                                      106
                                                                                                                                  2                                                                100
                                             100                101                102               103                                      101                103               105
                                                                    Token Index                                                                              Step
                                    Figure 20           This ﬁgure provides information about the performance per token as a function of model size
                                    and training time. Left: Loss per token as a function of its position T in the 1024-token context. Loss scales
                                    predictably as a power-law in T. Right: Test loss per token as a function of training step.
                                                                                7.5                                                                 Token 1/1024
                                                                                                                                                    Token 2/1024
                                                                                6.0                                                                 Token 4/1024
                                                                                                                                                    Token 8/1024
                                                                                                                                                    Token 16/1024
                                                                                4.5                                                                 Token 64/1024
                                                                                                                                                    Token 256/1024
                                                                               Test Loss                                                            Token 1024/1024
                                                                                                                                                    Token 1/8
                                                                                3.0                                                                 Token 2/8
                                                                                                                                                    Token 4/8
                                                                                                                                                    Token 8/8
                                                                                       104       105        106       107       108       109
                                                                                             Parameters (excl. embedding)
                                    Figure 21           In addition to the averaged loss, individual tokens within the 1024-token context also improve
                                    smoothly as model size increases. Training runs with shorter context nctx = 8 (dashed lines) perform better
                                    onearly tokens, since they can allocate all of their capacity to them.
                                    D.5      Context Dependence
                                    The trends for loss as a function of model size are displayed for different tokens in the context in Figure 21.
                                    Wesee that models trained on nctx = 1024 show steady improvement with model size on all but the ﬁrst
                                    token.
                                    Fixing model size, it appears that the loss scales as a power-law as a function of position T in the context, see
                                    Figure 20. This may be a consequence of underlying power-law correlations in language [EP94, ACDE12,
                                    LT16], or a more general feature of the model architecture and optimization. It provides some suggestion for
                                    the potential beneﬁts (or lack thereof) from training on larger contexts. Not only do larger models converge
                                    to better performance at T = 1024, but they also improve more quickly at early tokens, suggesting that larger
                                    models are more efﬁcient at detecting patterns with less contextual information. In the right-hand plot we
                                    showhowper-tokenperformancevariesforaﬁxedmodelasafunctionofthetrainingstep. Themodelbegins
                                    bylearning short-range information, and only learns longer-range correlations later in training.
                                    We have also included models trained with a tiny context nctx = 8 in order to compare with our longer
                                    context models. Even modestly sized models trained on nctx = 8 can dominate our largest nctx = 1024
                                    models on very early tokens. This also suggests that further improvements should be possible with much
                                    larger models trained on large contexts.
                                    D.6      Learning Rate Schedules and Error Analysis
                                    We experimented with a variety of learning rates and schedules. A host of schedules and resulting test
                                    performances for a small language model are plotted in Figure 22. We conclude that the choice of learning
                                    rate schedule is mostly irrelevant, as long as the total summed learning rate is sufﬁciently large, and the
                                    schedule includes a warmup period and a ﬁnal decay to near-vanishing learning rate. Variations among
                                                                                                                       25
                                 0.0010                                                 3.90
                                 0.0008                                                 3.85
                                 0.0006                                                 3.80
                                 0.0004                                                 Loss3.75
                                Learning Rate0.0002
                                                                                        3.70
                                 0.0000                                                 3.65
                                       0     50000   100000  150000   200000  250000              50       100       150       200      250
                                                          Step                                             LR Summed Over Steps
                         Figure 22     Wetestavariety of learning rate schedules including cosine decay, linear decay, as well as other
                         faster/slower decays schedules on a 3 million parameter model, shown on the left. For these experiments we
                         do not decay to zero, since we ﬁnd that this tends to give a ﬁxed improvement close to the end of training.
                         Weﬁndthat, as long as the learning rate is not too small and does not decay too quickly, performance does
                         not depend strongly on learning rate. Run-to-run variation is at the level of 0.05 in the loss, so averaging
                         multiple runs is necessary to validate performance changes smaller than this level.
                                                                 6               L=(N/8.8 1013) 0.076
                                                                                 L= 0.25log(N/7.1 1012)
                                                                 5
                                                                 4
                                                                 3
                                                                Test Loss (at convergence)
                                                                 2   104    105   106    107    108    109
                                                                         Parameters (non-embedding)
                         Figure 23     The trend for performance as a function of parameter count, L(N), is ﬁt better by a power law
                         than by other functions such as a logarithm at a qualitative level.
                         schedules appear to be statistical noise, and provide a rough gauge for the scale of variation between different
                         training runs. Experiments on larger models suggest that the variation in the ﬁnal test loss between different
                         randomseeds is roughly constant in magnitude for different model sizes.
                         Wefoundthat larger models require a smaller learning rate to prevent divergence, while smaller models can
                         tolerate a larger learning rate. To implement this, the following rule of thumb was used for most runs:
                                                             LR(N)≈0.003239+−0.0001395log(N)                                               (D.1)
                         Weexpect that this formula could be improved. There may be a dependence on network width, likely set by
                         the initialization scale. The formula also breaks down for N > 1010 parameters. Nevertheless, we found that
                         it works sufﬁciently well for the models we considered.
                         D.7    Fit Details and Power Law Quality
                         Weexperimented with a number of functional forms for the ﬁts to L(N),L(C), and L(D); the power-law
                         ﬁts were qualitatively much more accurate than other functions such as logarithms (see Figure 23).
                         For L(C), we do not include small models with only 1 layer in the ﬁt, as the transition from 1 to 2 layers
                         causes a noticable lump in the data. For L(N) we also do not include very small models with only 1 layer in
                         the ﬁt, and we exclude the largest models that have not trained fully to convergence. Fit parameters change
                         marginally if we do include them, and the trend extrapolates well in both directions regardless.
                         D.8    Generalization and Architecture
                         Inﬁgure24weshowthatgeneralizationtootherdatadistributionsdoesnotdependonnetworkdepthwhenwe
                         hold the total parameter count ﬁxed. It seems to depend only on the performance on the training distribution.
                                                                                    26
                                                                                      2.8
                                                                                      2.7                                                         Wikipedia
                                                                                      2.6                                                         Books
                                                                                                                                                  Internet Books
                                                                                                                                                  Common Crawl
                                                                                    Test Loss2.5                                                  WebText2 (Train)
                                                                                                                                                  WebText2 (Test)
                                                                                      2.4
                                                                                      2.3
                                                                                                  101                        102
                                                                                                               Depth
                                     Figure 24           Weshowevaluations on a series of datasets for models with approximately 1.5 Billion param-
                                     eters. We observe no effect of depth on generalization; generalization performance depends primarily on
                                     training distribution performance. The 12-layer model overﬁt the Internet Books dataset and we show the
                                     early-stopped performance; we have not seen this surprising result in other experiments.
                                     List of Figures
                                           1        Summaryofsimplepowerlaws. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                   3
                                           2        Illustration of sample efﬁciency and compute efﬁciency. . . . . . . . . . . . . . . . . . . . .                                                             4
                                           3        Howtoscaleupmodelsize,batchsize, and serial steps . . . . . . . . . . . . . . . . . . . .                                                                   4
                                           4        Performance when varying model and data size, or model and training steps, simultaneously                                                                   5
                                           5        Weakdependenceofperformanceonhyperparameter tuning . . . . . . . . . . . . . . . . .                                                                        8
                                           6        Comparison of performance trend when including or excluding embeddings . . . . . . . . .                                                                    8
                                           7        LSTMandTransformerperformancecomparison . . . . . . . . . . . . . . . . . . . . . . .                                                                       9
                                           8        Generalization to other test datasets                        .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . .          10
                                           9        Universality of overﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                        11
                                           10       Critical batch size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                       12
                                           11       Performance versus compute budget or number of parameter updates . . . . . . . . . . . . .                                                                14
                                           12       Training on suboptimal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                           15
                                           13       Comparison between empirical and adjusted compute trends . . . . . . . . . . . . . . . . .                                                                15
                                           14       Optimal model size and serial number of steps versus compute budget . . . . . . . . . . . .                                                               16
                                           15       Contradiction between compute and data trends . . . . . . . . . . . . . . . . . . . . . . . .                                                             17
                                           16       Early stopping lower bound and training curves for overﬁt models . . . . . . . . . . . . . .                                                              23
                                           17       Universal transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                          24
                                           18       Batch size scans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                        24
                                           19       Another look at sample efﬁciency                          .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . .          24
                                           20       Power-law dependence of performance on position in context . . . . . . . . . . . . . . . . .                                                              25
                                           21       Performance at different context positions versus model size                                        .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . .       25
                                           22       Learning rate schedule scan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                         26
                                           23       Comparison of Power-Law and Logarithmic Fits                                      .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . .         26
                                           24       Generalization versus depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                         27
                                                                                                                         27
                        List of Tables
                            1     Parameter and compute counts for Transformer . . . . . . . . . . . . . . . . . . . . . . . .         7
                            2     Fits to L(N,D) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    11
                            3     Fits to L(N,S) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    14
                            4     Keytrend equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    20
                            5     Keyparameters to trend ﬁts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    20
                            6     Trends for compute-efﬁcient training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    20
                        References
                        [ACDE12] Eduardo G Altmann, Giampaolo Cristadoro, and Mirko Degli Esposti. On the origin of long-
                                     range correlations in texts. Proceedings of the National Academy of Sciences, 109(29):11582–
                                     11587, 2012. 25
                        [AS17]       MadhuS.AdvaniandAndrewM.Saxe. High-dimensional dynamics of generalization error in
                                     neural networks. arXiv, 2017, 1710.03667. 11, 18, 22
                        [BB01]       Michele Banko and Eric Brill. Scaling to very very large corpora for natural language disam-
                                     biguation. In Proceedings of the 39th annual meeting on association for computational linguis-
                                     tics, pages 26–33. Association for Computational Linguistics, 2001. 18
                        [BHMM18] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine
                                     learning and the bias-variance trade-off. arXiv, 2018, 1812.11118. 18
                        [Bia12]      GÃŠrard Biau. Analysis of a random forests model. Journal of Machine Learning Research,
                                     13(Apr):1063–1095, 2012. 18
                        [CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with
                                     sparse transformers. CoRR, abs/1904.10509, 2019, 1904.10509. URL http://arxiv.org/
                                     abs/1904.10509. 19
                        [DCLT18] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeep
                                     bidirectional transformers for language understanding, 2018, arXiv:1810.04805. 2
                        [DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Uni-
                                     versal transformers. CoRR, abs/1807.03819, 2018, 1807.03819. URL http://arxiv.org/
                                     abs/1807.03819. 6, 9, 23, 24
                        [EP94]       Werner Ebeling and Thorsten Pöschel. Entropy and long-range correlations in literary english.
                                     EPL(Europhysics Letters), 26(4):241, 1994. 25
                        [Fou]        TheCommonCrawlFoundation. Commoncrawl. URLhttp://commoncrawl.org. 7
                        [GARD18] GuyGur-Ari,DanielA.Roberts,andEthanDyer. Gradientdescent happens in a tiny subspace.
                                     2018, arXiv:1812.04754. 18
                        [GJS+19]     Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stéphane d’Ascoli,
                                     Giulio Biroli, Clément Hongler, and Matthieu Wyart. Scaling description of generalization with
                                     numberofparameters in deep learning. arXiv, 2019, 1901.01608. 18
                        [GKX19]      Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net op-
                                     timization via hessian eigenvalue density. CoRR, abs/1901.10159, 2019, 1901.10159. URL
                                     http://arxiv.org/abs/1901.10159. 18
                        [Goo01]      Joshua Goodman. A bit of progress in language modeling. CoRR, cs.CL/0108005, 2001. URL
                                     http://arxiv.org/abs/cs.CL/0108005. 18
                        [GRK17]      Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. ope-
                                     nai.com, 2017. 19
                        [HAD19]      Joel Hestness, Newsha Ardalani, and Gregory Diamos. Beyond human-level accuracy: Compu-
                                     tational challenges in deep learning. In Proceedings of the 24th Symposium on Principles and
                                     Practice of Parallel Programming, PPoPP ’19, pages 1–14, New York, NY, USA, 2019. ACM.
                                     doi:10.1145/3293883.3295710. 18
                                                                               28
                        [HCC+18] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,
                                     andZhifengChen. Gpipe: Efﬁcienttrainingofgiantneuralnetworksusingpipelineparallelism.
                                     CoRR,abs/1811.06965, 2018, 1811.06965. URL http://arxiv.org/abs/1811.06965. 19
                              +
                        [HNA 17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-
                                     ninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-
                                     dictable, empirically, 2017, 1712.00409. 18
                        [JGH18]      Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
                                     generalization in neural networks. In Advances in neural information processing systems, pages
                                     8571–8580, 2018. 18
                        [KB14]       Diederik P. Kingma and Jimmy Ba.         Adam: A method for stochastic optimization, 2014,
                                     1412.6980. 7
                        [Kom19]      Aran Komatsuzaki. One epoch is all you need, 2019, arXiv:1906.06669. 18
                        [KSH12]      Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep
                                     convolutional neural networks. In Proceedings of the 25th International Conference on Neural
                                     Information Processing Systems - Volume 1, NIPS’12, pages 1097–1105, USA, 2012. Curran
                                     Associates Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257. 19
                              +
                        [LCG 19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
                                     Soricut.  Albert: A lite bert for self-supervised learning of language representations, 2019,
                                     1909.11942. 9
                        [LOG+19] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,Mike
                                     Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretrain-
                                     ing approach. CoRR, abs/1907.11692, 2019, 1907.11692. URL http://arxiv.org/abs/
                                     1907.11692. 2
                        [LSP+18]     Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and
                                     NoamShazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs],
                                     2018, 1801.10198. URL http://arxiv.org/abs/1801.10198. 2, 6
                        [LT16]       Henry W Lin and Max Tegmark. Criticality in formal languages and statistical physics. arXiv
                                     preprint arXiv:1606.06737, 2016. 25
                             +
                        [LXS 19] JaehoonLee,LechaoXiao,SamuelS.Schoenholz,YasamanBahri,RomanNovak,JaschaSohl-
                                     Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
                                     under gradient descent, 2019, arXiv:1902.06720. 18
                        [MKAT18] SamMcCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model
                                     of large-batch training, 2018, arXiv:1812.06162. 3, 5, 6, 12, 13, 21
                        [Pap18]      Vardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size.
                                     CoRR,abs/1811.07062, 2018, 1811.07062. URL http://arxiv.org/abs/1811.07062. 18
                        [RNSS18]     Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
                                     understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
                                     assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018. 2, 6
                        [RRBS19a] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive
                                     prediction of the generalization error across scales, 2019, 1909.12673. 18
                        [RRBS19b] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive
                                     prediction of the generalization error across scales, 2019, arXiv:1909.12673. 18
                        [RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
                                     Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
                                     text-to-text transformer, 2019, arXiv:1910.10683. 2
                        [RWC+19] AlecRadford,JeffWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever. Language
                                     models are unsupervised multitask learners. openai.com, 2019. 2, 5, 6, 7, 8
                             +
                        [SCP 18]     NoamShazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-
                                     takool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and
                                     Blake Hechtman. Mesh-tensorﬂow: Deep learning for supercomputers, 2018, 1811.02084. 19
                        [SHB15]      Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
                                     with subword units. CoRR, 2015, 1508.07909. 6
                                                                              29
                            +
                      [SLA 18] Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
                                   George E. Dahl. Measuring the effects of data parallelism on neural network training, 2018,
                                   arXiv:1811.03600. 12
                      [SS18]       NoamShazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory
                                   cost. CoRR,abs/1804.04235,2018,1804.04235. URLhttp://arxiv.org/abs/1804.04235.
                                   7
                      [THK18]      Stefan Thurner, Rudolf Hanel, and Peter Klimek. Introduction to the theory of complex systems.
                                   Oxford University Press, 2018. 18
                      [TL19]       Mingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural
                                   networks. CoRR, abs/1905.11946, 2019, 1905.11946. URL http://arxiv.org/abs/1905.
                                   11946. 18
                      [VSP+17]     Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
                                   Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
                                   S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
                                   Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL
                                   http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf. 2,6
                      [VWB16]      Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles
                                   of relatively shallow networks, 2016, arXiv:1605.06431. 8, 18
                      [Was06]      Larry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.
                                   18
                            +
                      [WPN 19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
                                   Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose
                                   language understanding systems, 2019, 1905.00537. 2
                      [WRH17]      Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by in-
                                   creasing model capacity. 2017 IEEE Conference on Computer Vision and Pattern Recognition
                                   (CVPR), Jul 2017. doi:10.1109/cvpr.2017.323. 19
                      [WYL19]      Wei Wen, Feng Yan, and Hai Li. Autogrow: Automatic layer growing in deep convolutional
                                   networks, 2019, 1906.02909. 19
                            +
                      [YDY 19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V.
                                   Le.    Xlnet:  Generalized autoregressive pretraining for language understanding, 2019,
                                   arXiv:1906.08237. 2
                      [ZK16]       Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British
                                   Machine Vision Conference 2016, 2016. doi:10.5244/c.30.87. 18
                            +
                      [ZKZ 15] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-
                                   ralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
                                   watching movies and reading books. 2015 IEEE International Conference on Computer Vision
                                   (ICCV), Dec 2015. doi:10.1109/iccv.2015.11. 7
                            +
                      [ZLN 19] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,
                                   Christopher J. Shallue, and Roger B. Grosse. Which algorithmic choices matter at which batch
                                   sizes? insights from a noisy quadratic model. CoRR, abs/1907.04164, 2019, 1907.04164. URL
                                   http://arxiv.org/abs/1907.04164. 12,18
                                                                         30
