                 Figure 4: Sample questions from MMMU-Pro Vision. The model is required to answer a multiple-choice question
                 with up to 10 options, each embedded within a screenshot or photo. The images were manually captured by
                 annotators in diverse display environments to reflect real-world cases.
                 2024b) and GPT-4o mini (OpenAI, 2024a), Claude          (2) and (3). We include setting (1) and report the
                 3.5 Sonnet (Anthropic, 2024), and Gemini 1.5 Pro         original MMMUvalidation set performance solely
                 (0801 and 0523 versions) (Team et al., 2023; Reid        for comparison purposes, to highlight the increased
                 et al., 2024). These models represent the cutting        difficulty of MMMU-Pro.
                 edge of multimodal AI capabilities.                        We evaluate the models with both Direct and
                    Open-source models: We evaluate a range of           CoT prompts(asshowninAppendixA),andreport
                 open-source models, including InternVL2 (8B,             the higher ones in the overall results. We also
                 40B, and Llama3-76B versions) (Chen et al.,              discuss the influence of the CoT prompt in 3.3.
                 2024), LLaVA (OneVision-7B, OneVision-72B,              Approximating Human Expert Performance.
                 and various NeXT versions) (Li et al., 2024a;           While rigorous human evaluation of MMMU-Pro
                 Liu et al., 2024a), VILA-1.5-40B (Lin et al.,            provides valuable insights, conducting such an as-
                 2024), MiniCPM-V2.6 (Yao et al., 2024), Phi-             sessment is both time-consuming and costly. In-
                 3.5-Vision (Abdin et al., 2024), and Idefics3-8B-        stead, we develop an approach to approximate
                 Llama3 (Lauren√ßon et al., 2024). These models            human expert performance based on the original
                 showcase the current state of publicly available         MMMUhumanevaluationdata. Thisapproxima-
                 multimodal AI systems. We evaluate these models          tion is justified by several key factors. Firstly, the
                 across three different settings: 1) Standard setting     core content and difficulty of the questions remain
                 without augmented options (usually 4 options); 2)        unchanged in MMMU-Pro, supporting the validity
                 Standard setting with augmented options (usually         of using the original human performance data as
                 10options); 3)Vision-only input setting.                 a close approximation. Secondly, in the original
                    Theoverall performance score for MMMU-Pro             MMMUevaluation,humanexpertsarerequired to
                 is calculated as the average of scores from settings    write out their problem-solving processes, signifi-
                                                                     15137
