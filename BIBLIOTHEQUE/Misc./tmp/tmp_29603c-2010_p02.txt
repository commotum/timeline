               agent transfers from state s to s0 with transition probability                              X                     µπ(s)π(a|s)
                                                                                                 s.t. ε ≥      µπ(s)π(a|s)log                  ,  (6)
               p(s0|s,a) = Pa 0, and receives a reward r(s,a) = Ra ∈ R.                                                             q(s,a)
                              ss                                       s                                   s,a
               Asaresultfromthesestatetransfers,theagentmayconverge                     X                  X
                                                   π                                         µπ(s0)φ 0 =        µπ(s)π(a|s)Pa φ 0,                (7)
               to a stationary state distribution µ (s) for which                                    s                         ss0  s
                                                                                          0                    0
                            X                                                            s                 s,a,s
                         0           π              0           π 0                                        X
                      ∀s :      s,a µ (s)π(a|s)p(s |s,a) = µ (s )          (1)                        1 =      µπ(s)π(a|s).                       (8)
               holds under mild conditions, see (Sutton et al. 2000). The                                  s,a
               goal of the agent is to ﬁnd a policy π that maximizes the              Both µπ and π are probability distributions and the features
               expected return                                                        φ 0 of the MDP are stationary under policy π.
                                                                                       s
                            J(π) = X        µπ(s)π(a|s)r(s,a),             (2)          Without the information loss bound constraint in Eq.(6),
                                         s,a                                          there is no notion of sampled data and we obtain the stochas-
               subject to the constraints of Eq.(1) and that both µπ and π            tic control problem where differentiation of the Langrangian
                                                                                                                                    T        a
                                                                                      also yields the classical Bellman equation φ θ = R −λ+
               are probability distributions. This problem is called the op-          P a T                               T         s        s
                                                                                         0 P 0φ 0θ. In this equation, φ θ = Vθ(s) is known to-
               timal control problem; however, it does not include any no-              s    ss  s                        s
               tion of data as discussed in the previous section. In some             day as value function while the Langrangian multipliers θ
               cases, only some features of the full state s are relevant for         become parameters and λ the average return. While such
               the agent. In this case, we only require stationary feature            MDPs may be solved by linear programming (Puterman
               vectors                                                                2005), approaches that employ sampled experience cannot
                X                                       X                             be derived properly from these equations. The key differ-
                            π              0        0           π 0     0             ence to past optimal control approaches lies in the addition
                          µ (s)π(a|s)p(s |s,a)φ       =       µ (s )φ . (3)
                         0                         s         0         s
                    s,a,s                                   s                         of the constraint in Eq. (6).
               Note that when using Cartesian unit vectors u 0 of length                As discussed in the introduction, natural policy gradient
                                                                  s                   maybederivedfromasimilarproblemstatement. However,
               n as features φ 0 = u 0, Eq.(3) will become Eq.(1). Using
                               s       s                                              the natural policy gradient requires that ε is small, it can
               features instead of states relaxes the stationarity condition          only be properly derived for the path space formulation and
               considerably and often allows a signiﬁcant speed-up while              it can only be derived from a local, second order Taylor ap-
               onlyresulting in approximate solutions and being highly de-            proximation of the problem. Stepping away further from the
               pendable on the choice of the features. Good features may              sampling distribution q will violate these assumptions and,
               beRBFfeaturesandtilecodes,see(SuttonandBarto1998).                                                                               1
                                                                                      hence, natural policy gradients are inevitably on-policy .
                         Relative Entropy Policy Search                                 The ε can be chosen freely where larger values lead to
                                                                                      bigger steps while excessively large values can destroy the
               Wewill ﬁrst motivate our approach and, subsequently, give              policy. Its size depends on the problem as well as on the
               several practical implementations that will be applied in the          amountofavailable samples.
               evaluations.
               Motivation                                                             Relative Entropy Policy Search Method
                                                                                      As shown in the appendix, we can obtain a reinforcement
               Relative entropy policy search (REPS) aims at ﬁnding the               learning algorithm straightforwardly.
               optimal policy that maximizes the expected return based                Proposed Solution. The optimal policy for Problem            is
               on all observed series of states, actions and rewards. At              given by                q(s,a)exp1δ (s,a)
               the same time, we intend to bound the loss of informa-                                                       η θ
               tion measured using relative entropy between the observed                         π(a|s) = P q(s,b)exp1δ (s,b),                  (9)
                                                                    π                                          b              η θ
               data distribution q(s,a) and the data distribution p (s,a) =
               µπ(s)π(a|s) generated by the new policy π. Ideally, we                                          P
                                                                                                           a          a       0
                                                                                      whereδ (s,a) = R +           0 P 0V (s )−V (s)denotesthe
               wanttomakeuseofeverysample(s,a,s0,r)independently,                             θ            s      s   ss  θ         θ
               hence, we express the information loss bound as                        Bellman error. Here, the value function Vs(θ) = θTφs is
                                                                                      determined by minimizing
                               X                       π                                                                                 
                      π              π               µ (s)π(a|s)                                         X                       1
                  D(p ||q) =       µ (s)π(a|s)log                   ≤ε, (4)            g(θ,η) = ηlog            q(s,a)exp ε+ δ (s,a)          ,  (10)
                                                        q(s,a)                                               s,a                 η θ
                                s,a
               where D(pπ||q) denotes the Kullback-Leibler divergence,                with respect to θ and η.
                                                                                                                        T
                                                                                        The value function Vθ(s) = φ θ appears naturally in the
               q(s,a) denotes the observed state-action distribution, and ε                                             s
               is our maximal information loss.                                       derivationofthisformulation(seeAppendix). Thenewerror
               Problem Statement. The goal of relative entropy policy                    1Note that there exist sample re-use strategies for larger step
               search is to obtain policies that maximize the expected re-            away from q using importance sampling, see (Sutton and Barto
               wardJ(π)whiletheinformation loss is bounded, i.e.,                     1998; Peshkin and Shelton 2002; Hachiya, Akiyama, Sugiyama
                                    X                                                 and Peters 2008), or off-policy approaches such as Q-Learning
                                          π             a                             (which is known to have problems in approximate, feature-based
                      maxJ(π)=          µ (s)π(a|s)R ,                     (5)
                         π                              s
                      π,µ                                                             learning).
                                    s,a
                                                                               1608
