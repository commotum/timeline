                              Stabilizing Equilibrium Models by Jacobian Regularization
                                                   Shaojie Bai1 Vladlen Koltun2 J. Zico Kolter1
                                       Abstract                                equilibrium z? by the implicit function theorem (Krantz &
                    Deep equilibrium networks (DEQs) are a new                 Parks, 2012), irrespective of the method used to solve for
                    class of models that eschews traditional depth in          this equilibrium in the forward pass. Therefore, like other
                    favor of ﬁnding the ﬁxed point of a single non-            implicit-depth architectures such as Neural ODEs (Chen
                    linear layer. These models have been shown to              et al., 2018), DEQs have the notable advantages that their
                    achieve performance competitive with the state-            forward passes can rely on any black-box root solvers (e.g.,
                    of-the-art deep networks while using signiﬁcantly          Newton, quasi-Newton, simplest forward iterations), and
                    less memory. Yet they are also slower, brittle to          that their training only consumes O(1) memory. With this
                    architectural choices, and introduce potential in-         formulation, prior works have managed to extend the DEQ
                    stability to the model. In this paper, we propose          framework for multiple large-scale applications, such as
                    a regularization scheme for DEQ models that ex-            language modeling (Bai et al., 2019) and large-scale image
                    plicitly regularizes the Jacobian of the ﬁxed-point        classiﬁcation or segmentation (Bai et al., 2020).
                    update equations to stabilize the learning of equi-        However, these models suffer from a few issues. First,
                    librium models. We show that this regularization           despite their memory efﬁciency, DEQs are also slower than
                    adds only minimal computational cost, signiﬁ-              conventional deep networks that achieve the same level
                    cantly stabilizes the ﬁxed-point convergence in            of accuracy. Second, the number of iterations required to
                    both forward and backward passes, and scales               solve for the equilibrium quickly grows over the course
                    well to high-dimensional, realistic domains (e.g.,         of training, indicating a trend for approaching instability.
                    WikiText-103 language modeling and ImageNet                Third, the DEQ model is sensitive to architectural choices,
                    classiﬁcation). Using this method, we demon-               and sometimes even small modiﬁcations could break the
                    strate, for the ﬁrst time, an implicit-depth model         model’s stability of convergence. Some recent works have
                    that runs with approximately the same speed and            tackled this third issue by exploiting provably convergent
                    level of performanceaspopularconventionaldeep              layers via monotone operator splitting theories (Winston
                    networks such as ResNet-101, while still main-             &Kolter, 2020) and Lipschitz boundedness (Revay et al.,
                    taining the constant memory footprint and archi-           2020). However, these structural solutions rely extensively
                    tectural simplicity of DEQs. Code is available             on speciﬁc layer parameterizations, rendering DEQ models
                    here.                                                      unscalable and even more inﬂexible.
                                                                               In this paper, we ﬁrst summarize and provide empirical evi-
               1. Introduction                                                 dence on all of these downsides of the equilibrium networks
                                                                               that have so far thwarted many from extending DEQs to
               While conventional deep networks like ResNets (He et al.,       both broader applications and more architectural variants.
         arXiv:2106.14342v1  [cs.LG]  28 Jun 20212016) and Transformers (Vaswani et al., 2017) rely on hi-Toaddress these issues, we further propose a regularization
               erarchical layer stacking, the recently-proposed deep equi-     solution to improve on DEQ models’ stability, efﬁciency
               librium networks (DEQs) (Bai et al., 2019) directly model       and ﬂexibility. Importantly, while prior DEQs adopted regu-
               the “inﬁnite-depth” representation of a single layer fθ by      larization methods direcly borrowed from explicit deep net-
               solving for its ﬁxed point (i.e., “equilibrium”) z?:            works (e.g., recurrent dropout (Gal & Ghahramani, 2016)),
                                     z? = f (z?;x),                            weintroduce a simple and theoretically-motivated Jacobian
                                            θ                                  regularization pursuant to DEQ models’ implicitness. We
               where x is the original input. Importantly, to train these      will discuss in detail how this Jacobian regularization relates
               models, one could directly differentiate through the ﬁnal       to the contractivity of DEQ’s forward non-linear system and
                                                                               backward linear system, and is thus able to effectively stabi-
                  1Carnegie Mellon University, Pittsburgh PA, USA 2Intel Labs, lize not only forward but also backward dynamics of DEQ
               USA.Correspondenceto: Shaojie Bai <shaojieb@cs.cmu.edu>.        networks. There are two immediate beneﬁts of the resulting
               Proceedings of the 38th International Conference on Machine     stability in the dynamics. First, solving a DEQ requires
               Learning, PMLR 139, 2021. Copyright 2021 by the author(s).      far fewer iterations than before, which makes regularized
                                                  Stabilizing Equilibrium Models by Jacobian Regularization
               DEQssigniﬁcantly faster than their unregularized counter-          f (z?;x)−z? = 0; later works (Winston & Kolter, 2020;
                                                                                    θ
               parts. Second, this model class becomes much less brittle to       Revay et al., 2020) and a recent tutorial (Duvenaud et al.,
               architectural variants that would otherwise break the DEQ.         2020) have applied other algorithms, such as Peaceman-
               Wevalidate the proposed regularization by experiments on           Rachford splitting (Peaceman & Rachford, 1955) and An-
               both toy-scale synthetic tasks and large-scale real datasets       derson acceleration (Anderson, 1965).
               acrossdomains: word-levellanguagemodelingonWikiText-               Compared to Neural ODEs, deep equilibrium networks
               103(Merity et al., 2017) and high-resolutional image clas-         have been demonstrated to scale well to large and high-
               siﬁcation on the full ImageNet dataset (Deng et al., 2009).        dimensional tasks, such as language modeling, ImageNet
               Empirically, our regularized DEQs are generally 2x to 3x           classiﬁcation, and semantic segmentation (Bai et al., 2019;
               faster than prior DEQs, and can be accelerated to be as fast       2020), and are thus more applicable to domains where deep
               as explicit deep networks (e.g., ResNets and DenseNets).           learning has been traditionally successful. However, unlike
               This is the ﬁrst time that implicit models are accelerated to      ODEﬂows,DEQnetworksdonothaveauniquetrajectory,
               this level without sacriﬁcing scalability, accuracy, or struc-     and are not guaranteed to converge. Thus recent works have
               tural ﬂexibility. With their O(1) memory footprint, this           also begun to examine the stability and other theoretical
               further establishes implicit models as a strong competitor to      properties of DEQs. Winston & Kolter (2020) propose a
               explicit deep architectures.                                       monotone DEQ that has a unique ﬁxed point. Pabbaraju
                                                                                  et al. (2021); Revay et al. (2020) further study the Lipschitz
               2. Background & Related Work                                       boundedness of monotone DEQs. Kawaguchi (2021) ana-
                                                                                  lyze the gradient dynamics of a linearized version of DEQs.
               While explicit deep networks hierarchically stack layers to        Lu et al. (2021) apply an invertible equilibrium model to
               build a computation graph for their forward and backward           generative modeling via normalizing ﬂows.
               propagations, implicit models (Amos & Kolter, 2017; Chen
               et al., 2018; El Ghaoui et al., 2019; Gould et al., 2019; Bai      2.2. Regularizing Implicit Models
               et al., 2019) do not have a prescribed computation graph.          Just like explicit deep networks, implicit networks can over-
               Instead these models solve for a non-linear system. One            ﬁt to the dataset; but additionally, they can also become
               exampleistheNeuralODE(Chenetal.,2018),whichsolves                  unstable. For instance, Neural ODEs are essentially mod-
               an initial-value ODE problem that involves a residual layer.       eling inﬁntesimal steps of a residual block (He et al., 2016;
               Another example, which is the primary focus of our work,           Chang et al., 2018), and Grathwohl et al. (2019) found
               is the deep equilibrium network (DEQ) (Bai et al., 2019),          that weight decay & spectral normalization (Miyato et al.,
               which reduces the forward pass to a root-solving problem.          2018) are useful (though expensive) in reducing the rapidly
               In this section, we introduce the basics of DEQ models and         growing number of functional evaluations (NFEs) needed
               the relevant threads of work, followed by a discussion of          to solve for the ODE endpoint. On the other hand, large-
               prior approaches to regularizing implicit models.                  scale DEQ networks (Bai et al., 2019; 2020) have adopted
               2.1. Deep Equilibrium Models                                       weightnormalization(Salimans&Kingma,2016),recurrent
                                                                                  dropout (Gal & Ghahramani, 2016), and group normaliza-
               Given a layer/block f (which may contain a few shallow             tion (Wu & He, 2018) for preventing overﬁtting and diver-
                                       θ
               sublayers) and an input x, a deep equilibrium model aims           gence. Nonetheless, all these methods are borrowed from
               to approximate an “inﬁnite-level” layer stacking of the form       explicit deep networks, where they have long been known to
                 [i+1]        [i]                                                 workwell. They do not exploit the implicitness of implicit
               z      =fθ(z ;x)(wherei = 1,...,L,withL → ∞)by                     models.
               directly solving for its ﬁxed-point representation:
                                      z? = f (z?;x).                              Morerecently, a few different regularization methods have
                                             θ                                    been introduced to speciﬁcally ﬁx the numerous issues of
               Oneofthe appealing properties of this ﬁxed-point formu-            the vanilla Neural ODE and continuous normalizing ﬂow
               lation is that one can implicitly differentiate through the        models, such as augmenting the hidden state (Dupont et al.,
               equilibrium feature, without dependency on any intermedi-          2019), using hyper ODE solvers (Poli et al., 2020), and reg-
               ate activations in the forward pass. Formally, given a loss `,     ularizing higher-order time derivatives (Kelly et al., 2020).
               one can directly compute the gradient using the ﬁnal output:       These methods directly leverage the dynamical system view
                                                               ?                  of Neural ODEs. However, due to the inherent challenge of
                         ∂`       ∂`                    ∂f (z ;x)
                                                 ?   −1    θ                      solving high-dimensional ODEs, these accelerated Neural
                              =       (I −Jf (z ))                  ,
                        ∂(·)     ∂z?          θ             ∂(·)                  ODEmodelscanstilleasily take > 100 forward iterations
               where J (z?) is the Jacobian matrix at equilibrium z?.             even on MNIST classiﬁcation (Kelly et al., 2020), and even
                        fθ                                                        morefortheir backward pass. In comparison, DEQs scale
               Tosolve for the equilibrium, Bai et al. (2019) proposed to         better to high-dimensional tasks (e.g., 25-30 iterations on
               use Broyden’s method (Broyden, 1965) to ﬁnd the root of
                                                                                      Stabilizing Equilibrium Models by Jacobian Regularization
                                                                                                                                                                     2
                                                                                                                                                                                                              WikiText-103 PPL vs. Time
                                                                                                                                     CIFAR-10 DEQ ||J (z*)||  vs. Final Residual
                                                                                                                                                            f
                                                                                                                                                                     F
                                |
                                                 CIFAR-10 DEQ Convergence (T=14 steps)
                                |
                                z  |
                                   |                                                                                         |
                                                                                                                             |
                                   )
                                −  x                                                                                         z  |
                                                                                                                                |
                                )                                                                                               )
                                x  ;                                                                                         −  x
                                   z
                                                                                                                             )
                                                                                                                                ;
                                                                                                                                                                                                      70
                                   (
                                ;  θ                                                                                         x  z
                                z
                                   f                                                                                         ;  (
                                (
                                                                                                                             z  θ  −1
                                   |
                                θ                                                                                            (  f10
                                f  |                                                                                         θ  |
                                                                                                                                |
                                |                                                                                            f
                                |                                                                                            |
                                       −1                                                                                    |
                                   
                                     10                                                                                        
                                  l                                                                                           l
                                                                                                                                                                                                      60
                                  a                                                                                           a
                                                                                                                              u
                                  u                                                                                           d
                                                                     MDEQ
                                  d                                                                                           i
                                  i
                                                                                                                              s                                                                                         18-layer Transformer
                                  s
                                                                                                                              e                                       MDEQ
                                                                     MDEQ (Early stop T=5)
                                  e                                                                                           r
                                                                                                                                                                                                                        Trans. DEQ
                                                                                                                                   −2
                                                                                                                                                                                                      50
                                                                                                                                 10
                                  r
                                                                                                                                                                      MDEQ+reg. (ours)
                                   
                                       −2                                                                                     e
                                     10
                                                                     MDEQ+reg. (ours)
                                                                                                                              v                                                                                         Trans. DEQ+reg. (ours)
                                  e                                                                                           i
                                                                                                                              t
                                  v
                                  i
                                                                     MDEQ+reg. (ours) (Early stop T=5)                        a
                                  t                                                                                           l
                                                                                                                              e                                                                       40
                                  a
                                  l                                                                                           r
                                                                                                                               
                                  e                                                                                           d
                                  r                                                                                           r
                                   
                                                                                                                              a    −3
                                                                                                                                 10
                                       −3
                                  d
                                                                                                                                                                                                    Validation Perplexity (PPL)
                                     10
                                                                                                                                                                                                      30
                                  r                                                                                           w
                                                                                                                              r
                                  a                                                                                           o
                                                                                                                              F
                                  w
                                  r
                                  o
                                  F
                                                                                                                                      0.00      0.05     0.10      0.15      0.20     0.25
                                                 5        10        15        20        25       30        35        40
                                                                                                                                                                                                           0       50      100     150     200
                                                                                                                                                                        2
                                                                                                                                         Jacobian norm ||J (z*)||  (normalized)
                                                     Training Iterations (thousand steps)
                                                                                                                                                                                                         Training Wall Clock Time (Hours)
                                                                                                                                                                f
                                                                                                                                                                        F
                              (a) Without regularizations, the relative residual of a                                      (b) In the same setting as Figure 1a, (c) Perplexity (ppl) of DEQs on
                              DEQ’sﬁnaloutput gets worse over training. Both mod- DEQ’s convergence residual vs. Jacobian WikiText-103 language model-
                              els achieve roughly the same eventual level of accuracy.                                     normkJ k2 .                                                            ing as a function of time.
                                                                                                                                          f F
                                                                               Figure 1. Visualizations of DEQs’ instablity and inefﬁciency problems.
                          ImageNet) (Bai et al., 2020) and complex fθ (e.g., a Trans-                                                          gences, thereby addressing these various problems. For
                          former layer). But such extra complexities also make DEQ                                                             example, our experiments show that we can signiﬁcantly
                          models harder to regularize; e.g., simply resorting to weight                                                        stabilize DEQs with new (and more unstable) architectural
                          decay doesn’t ﬁx the instability of DEQs (see Section 5.5).                                                          variants and accelerate DEQs to be nearly as fast as cer-
                          To the best of our knowledge, there has been almost no                                                               tain explicit architectures (e.g., we only need ≤ 6 NFEs
                          exploration of directly regularizing DEQ stability and con-                                                          on CIFAR-10) on tasks across different scales and with
                          vergence.                                                                                                            comparable accuracy.
                          Ourmethodisclosely connected to the many prior works                                                                 3. DEQModelsandTheirDiscontents
                          that study Jacobian/gradient regularization (Drucker &
                          Le Cun, 1992; Novak et al., 2018; Hoffman et al., 2019;                                                              Despite the DEQ models’ success in some very challenging
                          Finlay et al., 2020; Linsley et al., 2020), though these were                                                        tasks, such as Cityscapes semantic segmentation (Cordts
                                                                                                             ´
                          also motivateddifferently. Speciﬁcally, Sokolic et al. (2017);                                                       et al., 2016; Bai et al., 2020), these models suffer from
                          Hoffmanetal.(2019)regularizedtheinput-outputJacobians                                                                multiple serious downsides. In this section, we provide a
                          of the entire (very deep) explicit classiﬁcation networks to                                                         summary of some of these problems. While these issues
                          increase the prediction margin in a robust learning setting                                                          directly lead to our subsequent discussion on the need for
                          (andarethusexpensive).Finlayetal.(2020)wasinspiredby                                                                 regularization (see Section 4), we also believe such sys-
                          a kinetic energy view and possible overﬁtting of a training-                                                         tematic discussion provides a useful overview for potential
                          time dynamical system. The method of Linsley et al. (2020)                                                           future research on further addressing these issues.
                          targeted (for a Jacobian J) a Lipschitzness level λ, used
                          max (1>J) to approximate the matrix 1-norm, and pro-
                                   i             i                                                                                             3.1. Growing Instability during Training
                                                                >               +
                          posed loss L = k(1 J − λ) k . Yet this approximation
                                                                                      2
                          is in fact problematic, as it does not provably bound the                                                            Although a DEQ network has no “depth”, a relevant mea-
                          spectral radius (i.e., stability) at all. For example, matrix                                                        sure of computational efﬁciency is the number of function
                                                                                                                                             evaluations(NFEs)ofthelayerf (z;x)usedbytheiterative
                                                                              2        −2                                                                                                                θ
                                                                 J = −2                  2                                                     root solver (e.g., Broyden’s method (Broyden, 1965)).
                          has L = 0 and yet an eigenvalue of 4 (we also empirically                                                            However, one commonphenomenontoallpriorworkson
                          verify that this method does not help DEQ models, exactly                                                            DEQsisthattheﬁxedpointsareincreasinglyhardertosolve
                          due to this issue).                                                                                                  for over the course of model training. In other words, as
                                                                                                                                               a DEQ’s performance gradually improves during training,
                          In contrast to these works, the key contributions of our paper                                                       the NFE required to converge to the same threshold ε (e.g.,
                          are that (1) we provide a thorough discussion & summary of                                                          10−3) rapidly grows. This observation has been made on
                          various issues with DEQ models, and how ill-conditioned                                                              different instantiations of equilibrium networks, and regard-
                          Jacobians are related to the forward/backward instabilities,                                                         less of whether the model is provably convergent or not
                          via the new lens of ﬁxed-point convergence; and (2) we                                                               (e.g., (Bai et al., 2019; Winston & Kolter, 2020), where a
                          demonstrate how regularizing the Jacobian of DEQs at the                                                             DEQattheendoftrainingcantake> 3×moreiterations).
                          equilibrium point (i.e., the ﬁnal output z?) can provably                                                            Intuitively, such tendency to approach “critical stability” im-
                          bound the stability of the forward and backward conver-                                                              plicitly characterizes an inclination of the model to learn
                                                                             Stabilizing Equilibrium Models by Jacobian Regularization
                                   Pre-LN                                            Post-LN                                                       WikiText-103 DEQ Forward Convergence (T=16 steps)
                                     Pre-LN                                        Post-LN
                                                                                       LaLa              La La                    |      0
                                                                                                                                  |
                      z                                      f (z;x)                                                                   10
                                                                                                                                    |
                                                               ✓        z                                      f (z;x)            z
                        z                                    f✓(z;x)                                             ✓
                                                                        z                y                  f (z;x)                 |
                                                                                       y e               y  y✓                    − )                                                                       Tran. DEQ
                                                                                   e                  e e                           x
                     xx                                                 x         r r                r r                          ) ;                                                                       Tran. DEQ w/o WN
                                                                                                                                  x
                             LaLa               LaLa                   x               NorNor            NorNor                     z
                                                                                                                                    (
                                                                                                                                  ;
                                    AA                                       A A                                                  z fθ
                                                                                                                                  (
                                                                                                                                    |
                                                                                                                                                                                                            Tran. DEQ pre-LN
                             y y    tt          yy                           t t                                                  fθ|
                             e e    tt          ee     FFNFFN                t t       m m               m  m                     |     −1
                                     e                                                                                            |
                             r r    en          rr                           e e               FFNFFN                                 10
                             NorNor nt          NorNor                       n n                                                   l
                                    ti                                       t t                                                   a
                                    ion                                      i i
                             m m    on          mm                           onon                                                  u
                                                                                                                                   d
                                                                                                                                   i
                                                                                                                                   s
                                                                                                                                        −2
                                                                                                                                   e
                                                                                                                                      10
                                                                                                                                   r
                                                                                                                                    
                       Figure 2. Pre- vs. post-LN DEQ-Transformer layer (Xiong et al.,                                             d
                                                                                                                                   r
                                                                                                                                   a
                        2020). FFN is a 2-layer feed-forward block (Vaswani et al., 2017).                                         w
                                                                                                                                   r
                                                                                                                                        −3
                                                                                                                                      10
                                                                                                                                   o
                                                                                                                                   F
                                                                                                                                                         10            20            30             40            50            60
                       “deeper” networks; so it is unsurprising that unregularized                                                                             Training Iterations (thousand steps)
                        training will keep driving it in this direction. But as a result,                                                (a) Forward ﬁnal objective of ﬁxed-point convergence
                        the dynamical system only becomes more and more brittle.
                                                                                                                                                           WikiText-103 DEQ Backward (T=16 steps)
                                                                                                                                  |
                                                                                                                                  |
                                                                                                                                  ∇
                                                                                                                                     |
                       Theexisting way of “addressing” this is to circumvent it by                                                        0
                                                                                                                                     |
                                                                                                                                       10
                                                                                                                                  +
                                                                                                                                     ∇
                                                                                                                                                                                                            Tran. DEQ
                                                                                                                                  )
                                                                                                                                  I
                                                                                                                                     +
                        setting a maximum NFE limit besides the ε-threshold; i.e.,                                                   f
                                                                                                                                  −
                                                                                                                                     J
                                                                                                                                                                                                            Tran. DEQ w/o WN
                                                                                                                                  f
                                                                                                                                    ⊤
                                                                                                                                  J
                                                                                                                                  (
                                                                                                                                     x
                                                                                                                                     |
                                                                                                                                 ⊤
                                                                                                                                     |
                                                                                                                                                                                                            Tran. DEQ pre-LN
                        the solver stops either when 1) the residual is smaller than ε,                                           x
                                                                                                                                  |
                                                                                                                                  |
                                                                                                                                    
                                                                                                                                   l
                        or 2) it has run for a max number of steps T. This could be                                                a
                                                                                                                                   u
                                                                                                                                   d
                        risky because as the convergence gets more unstable/critical,                                              i
                                                                                                                                   s
                                                                                                                                   e
                                                                                                                                   r
                                                                                                                                         −1
                                                                                                                                    
                        such a hard stop for the solver cannot guarantee that we are                                                  10
                                                                                                                                   d
                                                                                                                                   r
                        close enough to the ﬁxed point. In the backward pass, for                                                  a
                                                                                                                                   w
                                                                                                                                   k
                        instance, we may consequently be training DEQs with very                                                   c
                                                                                                                                   a
                                                                                                                                   B
                        noisy gradients. A similar issue exists for Neural ODEs,                                                                         10            20            30             40            50            60
                        though these cannot easily be hard-stopped like DEQs due                                                                               Training Iterations (thousand steps)
                        to the need to accurately trace the ﬂow to the endpoint.                                                        (b) Backward ﬁnal objective of ﬁxed-point convergence
                       Weillustrate this issue on CIFAR-10 classiﬁcation in Fig. 1a.                                            Figure 3. Comparing different architectural modiﬁcations of a
                        Onecaneasilyseethatbothforwardandbackwardestimates                                                      DEQ-Transformer (ﬁrst 60K steps). The DEQ networks are brittle:
                        of the ﬁxed points gets increasingly worse with the train-                                              even slight modiﬁcations such as changing the whereabouts of
                        ing steps (and eventually plateaus in an unstable region                                                LayerNorm(seeFigure 2) or removing weight normalization can
                       where the model keeps yielding bad gradients). Such grow-                                                cause the model to quickly diverge during training.
                        ing instability is also reﬂected empirically in the growth of
                                                                                             ?                                class of models in practice. In Figure 1c, we visualize this
                       Jacobian norm at equilibrium; i.e., ∂fθ(z ;x) (see Fig-
                                                                                   ∂z?                                        slowdownonthevalidation set of WikiText-103 language
                                                                                                        F
                        ure 1b), which we discuss in Section 4. Moreover, interest-                                             modeling(Merityetal.,2017)(withcomparablemodelsizes
                        ingly, while these plots might suggest simple regularizations                                           and number of training steps).
                        like weight decay, we show later that weight decay often
                        makes this stability issue worse for equilibrium networks,                                              3.3. Brittleness to Architectural Choices
                        and even leads to divergence.                                                                           Theneedtohavearelatively stable DEQ in order to train it
                        3.2. Inefﬁciency Compared to Explicit Networks                                                          via the implicit function theorem also calls for more care-
                                                                                                                                ful attention in designing the layer fθ. For example, the
                       Adirect ramiﬁcation of the increase in iterations required                                               largest-scale DEQs (Bai et al., 2019; 2020) all had nor-
                       (see Section 3.1) is the signiﬁcant increase in both training                                            malizations (Ba et al., 2016; Wu & He, 2018) at the end
                        and inference time for DEQ models.                                                                      of the layer to constrain the output range. How important
                        OneadvantageofDEQsnotedbyBaietal.(2019)isthatthe                                                        are these architectural choices? We demonstrate the brit-
                        forward trajectory need not strictly reach the equilibrium.                                             tleness of DEQs by ablative studies on the use of layer
                       Therefore in a certain sense, we could trade performance for                                             normalization (LN) or weight normalization (WN) in the
                        efﬁciency by stopping at a “good enough” estimate of the                                                DEQ-Transformer model on the large-scale WikiText-103
                        equilibrium. However, due to the growing instability prob-                                              language modeling task. Speciﬁcally, we compare the use
                        lem, this could still be increasingly costly. This causes the                                           of the two most popular Transformer layer designs in the
                        existing DEQs to be signiﬁcantly slower than their explicit                                             DEQframework: pre-LN andpost-LN,whichsimplyinserts
                        network counterparts of comparable size and performance.                                                the LN layers at different parts of the block (see Figure 2).
                        E.g., a DEQ-Transformer (Bai et al., 2019) is about 3×                                                  These two settings have been extensively studied, used, and
                        slower than a deep Transformer-XL (Dai et al., 2019); a                                                 compared in the literature (Liu et al., 2020; Xiong et al.,
                        multiscale DEQ (Bai et al., 2020) is over 4× slower than                                                2020; Vaswani et al., 2017; Baevski & Auli, 2019; Dosovit-
                        ResNet-101 on ImageNet. Despite their memory efﬁciency,                                                 skiy et al., 2020).
                        such slowdown is a roadblock to wider deployment of this                                                The result is shown in Figure 3. Without layer normaliza-
                                                   Stabilizing Equilibrium Models by Jacobian Regularization
                tion at the end (magenta line), the DEQ quickly diverges                              Trajectory of iteratively applying f✓
                after 25K training iterations (reﬂected in both forward and          z                             z     z            f✓(z;x)         z
                backward divergences). Similarly, without weight normal-               out                       =        out                       =
                                                                                                                t                                  t
                ization (orange line), the model becomes unstable more                                       zou                                zou
                                                                                                               f✓(z;x)              
                quickly, with ﬁxed-point solver collapse at around 18K it-                   @f✓                                @f✓ 
                                                                                                  < 1                                > 1
                erations. The original DEQ-Transformer (Bai et al., 2019)                    @z?                                @z? 
                (blue line in Figure 3), although not diverged, still suffers
                from the same increased instability problem as described
                in Section 3.1. These plots are strong indicators that while
                        Pre-LN                           Post-LN                                     ?          [0]                    ? [0]
               z                         f✓(z;x)           La            La                         z          z z                    z z            z
                equilibrium networks work on large scales, they are also
                                                 z                         f✓(z;x)
                                                           y             y
                                                         e            e
              x                                 x          r             r
                   La           La                         Nor           Nor
                relatively inﬂexible, brittle, and reliant on meticulous archi-
                        A                            A                               Figure 4. Left: when the slope is less than 1, even the simplest
                   y    t       y                    t
                   e    t       e                    t     m             m
                tectural designs.
                   r    e       r    FFN             e           FFN
                   Nor  n       Nor                  n                               iterative application of fθ converges. Right: when slope > 1, the
                        t                            t
                        i                            i
                   m    on      m                    on                              iterative approach may diverge or oscillate, but the ﬁxed point still
                3.4. The Hidden Cost of the Choice of Solver                         exists and can be solved for.
                Although DEQ models enjoy constant memory consump-                   can differentiate directly through the equilibrium z? by
                tion during training time and can use any black-box ﬁxed
                point solvers in the forward and backward passes, a com-                                                               ?
                                                                                                ∂`      ∂`                      ∂f (z ;x)
                                                                                                                         ?  −1     θ
                                                                                                    =        (I −Jf (z ))                   .      (1)
                monly neglected cost is that introduced by the choice of                       ∂(·)     ∂z?          θ              ∂(·)
                solver. For example, in Broyden’s method (Broyden, 1965)                               |          {z          }
                                                                                                                   >
                which Bai et al. (2019; 2020) used, the inverse Jacobian                                          u
                  −1                                                                 However,becausethescaleofJ            canbeprohibitively large
                J     is approximated by low-rank updates of the form                                                   fθ
                               P               >
                  −1              n     [n] [n]                 >                    andtheinverseiscostlytocompute,weusuallycomputethe
                J    ≈−I+ i=1u v                  =−I+UV . Asanother                   >
                example, Anderson mixing (Anderson, 1965) stores and                 u terminEq.1bysolvingthefollowinglinearﬁxed-point
                                                 [n−1]        [n−m]                  system that depends on the ﬁnal Jacobian:
                uses the past m iterations (z         , . . . , z   ). In most
                such cases, even storing these updates or past steps can be                              >      >        ?     ∂`
                                                                                                       u =u J (z )+                .               (2)
                                                                                                                    f
                expensive. Moreover, since we depend on the same DEQ                                                 θ            ?
                solvers also at inference time, we need to spend this same                                                     ∂z
                memorycostevenwhenwhenthetrainedmodelisserved–                       Consider the spectral radius of the Jacobian J        ∈Rd×dat
                                                                                                                                        f
                which conventional deep networks can avoid. We note that             the equilibrium:                                    θ
                this cost depends strongly on the solver; for example, the
                simplest iterative “solver” z[i+1] = fθ(z[i];x) wouldn’t                 ρ(J (z?)) = ρ(J (z?)>) = max(|λ |,...,|λ |),
                                                                                             f               f                      1         d
                have any memory cost, but suffers from bad convergence.                        θ              θ
                This issue also highlights the value of faster and stabler con-      where λis are eigenvalues. In both the forward and back-
                vergence, which entails less memory storage overall (e.g.,           ward passes, this spectral radius directly affects how stable
                                                                                                                             ?
                fewer Broyden steps).                                                the convergence to the ﬁxed point z could be in its neigh-
                                                                                     borhood. For instance, in the extreme case where we have a
                                                                                     contractive ρ(Jf ) < 1, by Lyapunov linearization theorem
                4. Regularizing the Jacobian of DEQs                                                   θ
                                                                                     even the simplest iterative calls to fθ(z) (in forward, as-
                                                                                                                                    >       ?      ∂`
                                                                                     suming good initial estimate) or g(u) = u J          (z ) +     ?
                We hypothesize that one of the fundamental factors con-                                                                f
                                                                                                                                        θ         ∂z
                tributing to some of the problems discussed in Section 3 is          (in backward) could converge uniquely, even without ad-
                that DEQ models’ conditioning is not properly regularized            vanced solvers. The linear system (2), in particular, would
                during training. Such trend for DEQ models to go unstable            enjoy global asymptotic stability. However in practice, we
                is reﬂected in Figures 1a and 1b, where increasing training          don’t always, and probably shouldn’t, require such a strong
                steps leads to monotonically growing residual difference             contractivity on the dynamical system, which might sig-
                and the Jacobian norm at the equilibrium. We now describe            niﬁcantly limit the representational capacity of the model.
                howthe Jacobian is related to the stability of equilibrium           For example, as shown in Figure 4, a ﬁxed point can exist
                                                                                     even if ρ(J    ) > 1, (the curve slope in 2D); and we are still
                networks’ forward and backward passes, and then harness                          fθ
                this relationship to stabilize and accelerate DEQs.                  able to solve for them using the much stronger root solvers
                                                                                     (e.g., Newton or quasi-Newton) than these simplest iterative
                4.1. The DEQ Jacobian                                                stackings, which could oscillate or diverge.
                We ﬁrst recall that the forward pass of a DEQ network                4.2. Jacobian Regularization
                aims to solve for the ﬁxed-point representation z? of a layer
                                ?         ?                                          These connections between J (z?) (which characterizes
                f (·;x); i.e., z  =f (z ). Then in the backward pass, one                                              fθ
                 θ                    θ                                              the shape of the transformation f around z?) and the for-
                                                                                                                           θ
                                                  Stabilizing Equilibrium Models by Jacobian Regularization
               ward/backward pass dynamics of DEQs motivate us to ap-              computation graph of this vector-Jacobian product. But at
                                                                    ?
               pend a soft and auxiliary Jacobian term ρ(J       (z )) to the      the same time, our hidden memory cost due to the solver
                                                               f
                                                                θ
               training objective in order to regularize the model’s con-          choice is smaller (e.g., Broyden’s method; see Section 3.4)
               ditioning. One way of doing this is by spectral normaliza-          as we can lower the number of iterations. As a result, em-
               tion, essentially constraining σ(J   ) = max          kJ vk .       pirically we notice a roughly 30% net growth in memory
                                                  f           kvk≤1    f     2
                                                   θ                    θ
               However, explicitly writing out the huge Jacobian and then          consumption compared to the unregularized DEQs at train-
               decomposing it (e.g., by SVD) can be computationally pro-           ing (and thus saving about 50% memory compared to ex-
               hibitive, and Miyato et al. (2018) proposes to use the power        plicit deep networks). The regularized DEQ still consumes
               method (von Mises & Pollaczek-Geiringer, 1929) to speed             O(1) memory relative to the “depth” of the model, as the
                                                                                                                         ?
               up this estimation on GANs. But in the context of DEQs,             backpropagation depends only on z .
               even power iterations are too expensive due to the succes-
               sive vector-Jacobian product computations needed. Instead,          5. Experiments
               weproposetoregularize the Jacobian through its Frobenius
               normsince                                                           We validate the proposed regularization of DEQ models
                       ρ(J ) ≤ σ(J ) ≤ qtr(J J>) = kJ k .                          on multiple fronts. First, we visualize the effect of the
                           f          f            f             f  F              proposed Jacobian regularization on a tiny DEQ trained on
                            θ          θ            θ f           θ
                                                        θ
                                                                                   a synthetic 1D dataset. Second, importantly, we focus on
               Importantly, kJ k      can be approximated via various un-
                                f   F                                              howourmethodalleviates some of the core problems with
                                 θ
               biased estimators (Hutchinson, 1989; Ubaru et al., 2017;            DEQsoutlined in Section 3. Then we show that our method
               Meyer et al., 2021). We adopt the classical Hutchinson              scales to challenging high-dimensional tasks: word-level
               estimator (Hutchinson, 1989); formally, for J       ∈Rd×d,
                                                                f                  language modeling with the WikiText-103 dataset (Merity
                                                                 θ
                                   >                     >      2                  et al., 2017) and image classiﬁcation with CIFAR-10 and
                           tr(J   J )=E               [k J k ],           (3)
                               fθ f         ∈N(0,Id)       fθ 2
                                    θ                                              ImageNet (Deng et al., 2009). We speciﬁcally compare
               which we can approximate by Monte-Carlo estimation (i.e.,           our model with both prior DEQ networks and competitive
               sampling M i.i.d.        ∈ N(0,I )). Speciﬁcally, prior            explicit models (e.g., ResNet-101, Transformers), in terms
                                       i            d                              of both efﬁciency (in space and time) and performance. We
               works (Avron & Toledo, 2011; Roosta-Khorasani & As-
               cher, 2015) have established that the relative error of this        also explore how Jacobian regularization helps stabilize
                                                 −1                                DEQsoverawiderrangeofarchitectural choices. Lastly,
               estimation diminishes with M 2; and if we compute the
               meanestimation over a mini-batch size B, the overall rel-           weperformsomeablative studies.
                                                                   >      2
               ative error with respect to E                    [k J k ] is
                                              x∼p(x),∈N(0,Id)        fθ 2         The set of tasks used in our experiment is built directly
                                                                −1
               expectedtofurtherdiminishedbyafactorofB 2 (Hoffman                  on top of Bai et al. (2019; 2020). As we found the Ja-
               et al., 2019).                                                      coabian regularization could sometimes hurt performance
               Indeed, empirically, we ﬁnd that M = 1 already works                (see Sec. 5.3), we only apply the proposed loss stochastically
               well since we use relatively large batch sizes. Since our           with a probability p, and gradually increase this p or the reg-
               backward iterations already involved computing multiple             ularization strength γ (see Eq. (4)) over training steps. We
                                             >                                     also use cosine learning rate schedule (Loshchilov & Hutter,
               vector-Jacobian products u J        (see Eq. (2)), computing
                                                fθ                                 2017) for all tasks, including the synthetic one. The mem-
               Eq. (3) only adds a cost equivalent to that of M = 1 back-          ory and speeds reported are benchmarked across different
               ward steps. The eventual training objective is thus                 models on the same setting (e.g., same batch size, sequence
                                            k>J (z?)k2                            length, number of steps, etc.) with the same GPU. We
                        ?            ?           fθ     2
                 L (z )=L (z )+γ                         ,   ∈ N(0,I ) (4)
                   total        orig              d                    d           provide more details regarding the tasks, hyperparameters,
               AsweobservedinFigure1a,withoutregularization, a DEQ                 datasets, and hardware in Appendix A, and extra experimen-
               model that stops after a ﬁxed number T of solver iterations         tal results in Appendix B. Our code and pretrained models
               exhibits increasingly poor convergence, accompanied by a            are provided here.
               growing kJ k at these ﬁxed points that empirically sig-
                            f  F
                             θ                                                     5.1. Visualization with Synthetic Data
               nals the growing instability. Therefore, by constraining the
               Jacobian’s Frobenius norm, we encourage DEQs to opti-               We start by empirically verifying the validity of the ap-
               mize for stabler and simpler dynamics whose ﬁxed points             proach and visualizing its effect on a synthetic dataset.
               are easier to solve for.                                            Wegenerated 5096 scalar data pairs (x,y) using function
                                                                                   y = h(x) = 3x3 + x2 − 5x + 2sin(x) − 3 + δ (where
               4.3. Memory Considerations                                                         2
                                                                                   δ ∈ N(0,0.05)), and split them into 4096 and 1000 training
               Although the loss objective (4) only adds minimal computa-          and validation samples, respectively. We then train a tiny
                                                               >     2
               tion cost, the need to backpropate through k J      k means
                                                                  f  2
                                                                   θ
               we also spend more memory during training to store the
                                                                                                                                                                                                                                                                                                                                                            Stabilizing Equilibrium Models by Jacobian Regularization
                                                                                                                                                                                                                                                                           =0                                                                                                                                                                                                                                            =2                                                                                                                                                                                                                                                     =4
                                                                                                                                                                                                                                                                            =0                                                                                                                                                                                                                                        =2                                                                                                                                                                                                                                                 =4
                                                                                                                                                                                                                                                                                            =0                                                                                                                                                                                                                                       =2                                                                                                                                                                                                                                               =4
                                                                                                                                                                                                                                                                                                                                                                    ?
                                                                                                                                                                                                                                                                                    =0 ? ?                                                                                                                                                                                                                               =2                                                                                                                                                                                                                                             =4
                                                                                                                                                                                                                     Learned equilibria z (x)
                                                                                                                                                                                                                       Learned equilibria z (x)
                                                                                                                                                                                                                                       Learned equilibria z (x)
                                                                                                                                                                                                                                Learned equilibria z?(x)
                                                                                                                                                    Plane z                                                                    =z
                                                                                                                                                 Plane z                                                                     =z
                                                                                                                                                                     Plane z                                                                   =z
                                                                                                                                                                                                       out
                                                                                                                                                                                                     outout
                                                                                                                                                               Plane zout = z
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          z = f (z;x)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     z = f (z;x)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    z = f (z;x)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ✓ ✓
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ✓
                                                                                                                        )  )                 )                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         z = f✓(z;x)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          f (z; 1) (i.e., x =  1)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      f (z; 1) (i.e., x =  1)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     f (z; 1) (i.e., x =  1)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ✓ ✓
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ✓
                                                                                                                        w  w                 w                                                                                                                                                                                                            f              f
                                                                                                                                                                                                                                                                                                                                                         f
                                                                                                                           e           )     e                                                                                                                                                                                                            ✓               ✓                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            f (z; 1) (i.e., x =  1)
                                                                                                                        e  i                 i                                                                                                            f f                  f                                                                         ✓ (                (                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ✓
                                                                                                                                                                                                                                                                                                                                                            (z                z                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           f (z;0.5)
                                                                                                                        i                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             f (z;0.5)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     f (z;0.5)
                                                                                                                                       w                                                                                                                  ✓                                                                                                   z f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ✓ ✓
                                                                                                                           v                 v                                                                                                               (✓ (              ✓ (                                                                              ;1             ;1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ✓
                                                                                                                                                                                                                                                                                                                                                                  ✓
                                                                                                                        v              e                                                                                                                       z z                 z                                                                            ;1(
                                                                                                                                       i                                                                                                                                f                                                                                          .9)            .
                                                                                                                                                                                                                                                                 ;  ;  ✓             ;                                                                              . z;1          9)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  f✓(z;0.5)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          f (z;1.9)
                                                                                                                                                                                                                                                                                                                                                                     9)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               f (z;1.9)
                                                                                                                                       v                                                                                                                                   (                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         f (z;1.9)
                                                                                                                                                                                                                                                                                                             f               f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ✓ ✓
                                                                                                                                                                                                                                                                        1) z            1)         f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ✓
                                                                                                                                         Surface f (z;x)                                                                                                             1)       ;                               ✓               ✓                                           .9)
                                                                                                                           3D                3D           Surface f (z;x)                                                                                                                            ✓           (               (
                                                                                                                        3D                                                                             ✓ ✓                                                                                                ( z                     z                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    f (z;1.9)
                                                                                                                                      Surface f (z;x)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ✓
                                                                                                                           (                 (
                                                                                                                                                                                                    ✓                                                                                                        z f                    ;0
                                                                                                                        (                                                                                                                                                        1)                                 ;0
                                                                                                                                                                                                                                                                                                                ;0
                                                                                                                                       3D            Surface f✓(z;x)                                                                                                                                                  ✓.(z              .
                                                                                                                                                                                                                                                                                                                         5)              5)
                                                                                                                                       (                                                                                                                                                                               .5)
                                                                                                                                                                                                                                                                                                                             ;0
                                                                                                                                                                                                                                                                                                                                .5)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ? ?
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Equilibrium z?
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Equilibrium z
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Equilibrium z
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Equilibrium z?
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      given di↵erent x
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          given di↵erent x
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     given di↵erent x
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       given di↵erent x
                                                                                                                                                                                                                                                                                                                                                                                              x=1.9
                                                                                                                                         )                 )                                                                                                                                                                                                                  x=1.9
                                                                                                                                      )  x                 x                                                                                                                                                                                                                   x=1.9
                                                                                                                                      x              )                                                                                                                                                                                                                               x=1.9
                                                                                                                    ith ith              ith         x     uts
                                                                                                                    w   w           ithutswuts
                                                                                                                                                     uts                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Convergence
                                                                                                                                    w                                                                                                                                                                                                                                                         x=0.5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Convergence
                                                                                                                        w                winp              inp                                                                                                                                                                                                                x=0.5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Convergence
                                                                                                                    w                 inp                                                                                                                                                                                                                                      x=0.5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Convergence
                                                                                                                                    w    t           inp   t                                                                                                                                                                                                                          x=0.5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Trajectory
                                                                                                                                      t  vie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Trajectory
                                                                                                                    vie vie                          t                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Trajectory
                                                                                                                        e           vie  e                                                                                                                                                                                                                                                    x= 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Trajectory
                                                                                                                    e                    eren              eren                                                                                                                                                                                                               x= 1
                                                                                                                                    e eren                 ↵                                                                                                                                                                                                                   x= 1
                                                                                                                                      ↵  (Slic↵      eren  di                                                                                                                                                                                                                        x= 1
                                                                                                                    (Slic(Slic        di di          ↵
                                                                                                                                    (Slic            di
                                                                                                                                                                                                                                                        (a)                                                                                                                                                                                                                                                                                    (b)                                                                                                                                                                                                                                  (c)
                                                                                                        Figure 5. Top: the surface of the f (z;x) layer, and the eventual learned equilibria z?(x) as a function of x. As γ grows, the surface is
                                                                                                                                                                                                                                                                                                                                                 θ
                                                                                                       “lifted up” and becomes ﬂat in the z-direction. Bottom: each unique input x deﬁnes a slice of the surface, and we perform ﬁxed-point
                                                                                                         solving on this slice; larger γ values ﬂatten the curve and signiﬁcantly accelerate the convergence to equilibrium.
                                                                                                         DEQwith200parameterswiththefollowingstructure:                                                                                                                                                                                                                                                                                                                                                                                                                                           Table 1. Evaluation on WikiText-103. PPL stands for Perplexity.
                                                                                                                                                                                                                                        >                                                                                                                                                                                                                                                                                           ?                                                             All Transformer models are trained for 250K steps. ttrain stands for
                                                                                                                             fθ(z;x) = W2 ReLU(W1z+Ux+b);                                                                                                                                                                                                                                                                                                                              y^ = z                                                                                                      relative training time. JR stands for Jacobian regularization. NFEs
                                                                                                        where we used z;x ∈ R and W ;W ;U ∈ R50×1. The vi-                                                                                                                                                                                                                                                                                                                                                                                                                                         are measured at inference time. † indicates unregularized model
                                                                                                                                                                                                                                                                                                                                                       1                                  2                                                                                                                                                                                                        hard-stopped at inference time.
                                                                                                         sualizations of the effect of the Jacobian regularization, with
                                                                                                         different weights γ, are shown in Figure 5. In particular,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Size                                              PPL                                         NFEs                                                t
                                                                                                         each input x deﬁnes a slice (i.e., cross-section) of the 3D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                train
                                                                                                         surface z                                                                                    =f (z;x); for example, layer f (z;x) when                                                                                                                                                                                                                                                                                                                                                                          AWD-QRNN(Bradburyetal.,2017)                                                                                                                                                                                                                    159M                                                 33.0                                                       -                                               -
                                                                                                                                                                              out                                                   θ                                                                                                                                                                                                                         θ                                                                                                                                                Rel. Memory Core (Santoro et al., 2018)                                                                                                                                                                                                                   195M                                                 31.6                                                       -                                               -
                                                                                                         input x = −1 is highlighted in blue. After training, all three                                                                                                                                                                                                                                                                                                                                                                                                                                             18L-Transformer-XL (Dai et al., 2019)                                                                                                                                                                                                                110M                                                 24.1                                                       -                                        1×
                                                                                                         settings succesfully learned the (almost) identical equilib-                                                                                                                                                                                                                                                                                                                                                                                                                                             DEQ-Trans. (Pre-LN) (Bai et al., 2019)                                                                                                                                                                                                                    98M                                             [div.]                                                  30                                       3:1×
                                                                                                         riumfunctionz?(x)(highlightedbythereddashedline)that                                                                                                                                                                                                                                                                                                                                                                                                                                                  DEQ-Trans. (Post-LN) (Bai et al., 2019)                                                                                                                                                                                                                      98M                                               24.0                                                  30                                       3:1×
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           †                                                                                                                                                   †
                                                                                                         perfectly ﬁts the target function h(x); but note that surfaces                                                                                                                                                                                                                                                                                                                                                                                                                                               DEQ-Trans. (Post-LN) early stopped                                                                                                                                                                                                                    98M                                               29.2                                               12                                          3:1×
                                                                                                         of fθ with γ = 2;4 are “lifted up” signiﬁcantly compared                                                                                                                                                                                                                                                                                                                                                                                                                                                           DEQ-Trans. (Pre-LN) + JR (ours)                                                                                                                                                                                                                 98M                                               24.5                                                  14                                       1:6×
                                                                                                         to the unregularized (γ = 0) DEQ, which has a steep slope                                                                                                                                                                                                                                                                                                                                                                                                                                                       DEQ-Trans. (Post-LN) + JR (ours)                                                                                                                                                                                                                   98M                                               24.9                                                  12                                       1:5×
                                                                                                        (i.e., large spectral radius in 2D). This slope slows down the                                                                                                                                                                                                                                                                                                                                                                                                                             Compared to the original DEQ models, there are two ma-
                                                                                                         ﬁxed-point convergence, as reﬂected by the zigzag patterns                                                                                                                                                                                                                                                                                                                                                                                                                                jor improvements. First, we signiﬁcantly reduce the NFEs
                                                                                                         in lower Figure 5a. In contrast, the convergences for the                                                                                                                                                                                                                                                                                                                                                                                                                                 required for DEQ-Transformer models while maintaining
                                                                                                         γ > 0 cases are much faster, and larger γ typically yields                                                                                                                                                                                                                                                                                                                                                                                                                                competitive accuracy. Using the Transformer-XL as a time
                                                                                                         ﬂatter surfaces around the equilibrium point.                                                                                                                                                                                                                                                                                                                                                                                                                                             benchmark (1×), the speed of a DEQ-Transformer is signif-
                                                                                                         5.2. WikiText-103 Language Modeling                                                                                                                                                                                                                                                                                                                                                                                                                                                       icantly accelerated: training time goes from 3:1× to 1:5×.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Second, the regularized DEQ model is more ﬂexible with
                                                                                                         Oneoftheveryﬁrstsuccesses of large-scale DEQs was its                                                                                                                                                                                                                                                                                                                                                                                                                                     architectural choices. Whereas a Pre-LN DEQ-Transformer
                                                                                                        Transformer instantiation (Bai et al., 2019), which uses a                                                                                                                                                                                                                                                                                                                                                                                                                                (see Figure 2) quickly diverges in training even in the pres-
                                                                                                         multi-head self-attention (Vaswani et al., 2017) layer as the                                                                                                                                                                                                                                                                                                                                                                                                                             ence of a large NFE threshold, the Jacobian regularization
                                                                                                         underlyingfθ(z;x)function. AlthoughaDEQ-Transformer                                                                                                                                                                                                                                                                                                                                                                                                                                       resolves this issue and stabilizes the forward/backward con-
                                                                                                         is able to perform competitively with a deep Transformer-                                                                                                                                                                                                                                                                                                                                                                                                                                vergencesconsistently (see Figure 3 and Table 1), eventually
                                                                                                        XL(Dai et al., 2019) in terms of test perplexity, and con-                                                                                                                                                                                                                                                                                                                                                                                                                                 reaching 24.5 perplexity. Moreover, while we can early-stop
                                                                                                         sumes60-70%lessmemory,itisalsomuchslower(about                                                                                                                                                                                                                                                                                                                                                                                                                                            a well-trained unregularized DEQ model at inference time,
                                                                                                        3×;seeFigure1c)andbordersoninstability. In Table 1, we                                                                                                                                                                                                                                                                                                                                                                                                                                     it hurts generalization performance signiﬁcantly (e.g., 29.2
                                                                                                         demonstrate how the Jacobian regularization alleviates this.                                                                                                                                                                                                                                                                                                                                                                                                                              ppl with 12 NFEs). Similarly, we ﬁnd training with NFEs
                                                                        Stabilizing Equilibrium Models by Jacobian Regularization
                                                                                                                                                CIFAR-10 Classification Comparison
                      Table 2. Results on CIFAR-10 and ImageNet classﬁcation. The
                      CIFAR-10accuracystandard deviation is calculated with 5 runs.                                                                                              2.8                            untime
                      JRstands for Jacobian regularization. † indicates unregularized                                                   2.3               2.5
                                                                                                                           or (%)
                      modelhard-stopped at inference time.                                                                          6.2                                 6.4                6.9                  elative R
                                                                                                                                                      5.0      1.4
                                                       CIFAR-10classiﬁcation                                                                1.0                                                1.1 1.1
                                                                             Size        Accuracy         NFEs                                                               0.7
                                 ResNet-18 (He et al., 2016)                 10M 93.0(±0.1)%                 -             Classification Err
                                ResNet-101 (He et al., 2016)                 40M 93.8(±0.3)%                 -                     ResNet-101       DenseNet-121           MDEQ         MDEQ+JR (ours)          Memory (GB) or R
                             DenseNet-121 (Huang et al., 2017)                8M      95.0 (±0:1)%           -                      Error (%)             Memory (GB)                Runtime (relative)
                         monotoneDEQ(Winston&Kolter,2020)                     1M      89.4 (± 0.2)%         24          Figure 6. With the proposed regularization, DEQ models are com-
                                   MDEQ(Baietal.,2020)                       10M 93.6(±0.2)%                17
                                    MDEQearlystopped†                        10M           89.1%            6†          petitive with popular explicit networks in accuracy, memory, and
                           MDEQ+JR(ours)(Baietal.,2020)                      10M 93.1(±0.3)%                 6          runtime. Lower bars are better.
                                                   (Full) ImageNet classiﬁcation
                                                                             Size       Top-1 Acc.        NFEs                WikiText-103 DEQ Max Eigenvalue vs Train Iters (train NFE=16)
                                                                                                                           )
                                                                                                                           f
                                                                                                                           J
                                                                                                                           (
                                                                                                                            12
                                 ResNet-18 (He et al., 2016)                 13M           70.2%             -                          Trans. DEQ
                                                                                                                           ρ
                                                                                                                            
                                                                                                                                        Trans. DEQ (train NFE=30)
                           Inception-V2 (Ioffe & Szegedy, 2015)              12M           74.8%             -             e
                                                                                                                                                                                                              2
                                                                                                                           u
                                                                                                                            10
                                                                                                                                                                                                           10
                                                                                                                           l
                                                                                                                                        Trans.DEQ+reg.(ours)
                                 ResNet-50 (He et al., 2016)                 26M           75.1%             -             a
                                                                                                                           v
                                                                                                                           n
                                ResNet-101 (He et al., 2016)                 52M           77.1%             -               8
                                                                                                                           e
                                                                                                                           g
                             DenseNet-264 (Huang et al., 2017)               74M           79.7%             -             i
                                                                                                                           e
                                                                                                                            
                                                                                                                             6
                                                                                                                           )
                               MDEQ-small(Baietal., 2020)                    18M           75.4%            27             .
                                                                                                                           s
                                                                                                                           b
                               MDEQ-large(Baietal., 2020)                    63M           77.5%            30             a
                                                                                                                             4
                                                                                                                           (
                                                                                                                            
                                 MDEQ-small+JR(ours)                         17M           74.5%            14             t
                                                                                                                           s
                                                                                                                           e
                                                                                                                             2
                                 MDEQ-large+JR(ours)                         62M           76.8%            15             g
                                                                                                                           r
                                                                                                                                                                                                                 Valid. perplexity (log-scale)
                                                                                                                           a
                                                                                                                           L
                                                                                                                                                                                                              1
                                                                                                                                                                                                           10
                                                                                                                             0
                                                                                                                                  0             20            40            60            80            100
                      <30leadstoincreasingly bad generalization performance,                                                                    Training Iterations (thousand steps)
                      and when NFEs drops below 20, model training frequently                                           Figure 7. Empiricalevidenceofhowourmethodconstrainsρ(Jf ).
                                                                                                                                                                                                                θ
                      diverge as a result of extremely noisy gradients. We provide                                      In contrast, insufﬁcient NFEs (e.g., T=16) at training time cause a
                      morecomprehensive results in Table 5 in the Appendix.                                             DEQ-Transformer model to explode early in the training phase.
                      LikeDEQs,theregularizedDEQsarememoryefﬁcient,con-
                      suming about 45% less training memory than Transformer-                                           and 1b, where we show that early stopping at threshold
                      XL.Moreover,weﬁndtheJacobian-regularized DEQsre-                                                  T =6still yields good convergence with Jacobian regular-
                      duce over 50% memory consumption of the original DEQs                                             ization. We also demonstrate a more stable backward pass
                      at inference time (when both using Broyden’s method) due                                          convergence throughout training in Appendix B. On the
                      to faster/stabler convergence, suggesting its effectiveness in                                    muchlarger-scale ImageNet, where we deal with 224×224
                      addressing the hidden solver cost issue discussed in Sec. 3.4.                                    images, the factor of reduction in NFE is not as strong (e.g.,
                                                                                                                        from 27 to 14 iterations, due to the receptive ﬁeld issue;
                      5.3. CIFAR-10 and ImageNet Classiﬁcation                                                          we’ll explain this in Section 5.5) but still yields a roughly
                      We additionally conduct experiments on vision tasks                                               2×acceleration. ThisshowsthattheJacobianregularization
                      using the recent multiscale deep equilibrium networks                                             is effective in large-scale computer vision tasks, and in the
                      (MDEQ)(Baietal.,2020), which drive multiple feature res-                                          presence of multiple equilibrium points. However, we also
                      olutions to their equilibria simultaneously. Because of the                                       note that as with DEQ-Transformers on WikiText-103, we
                      need to maintain high- and low-resolutional feature maps at                                       notice a small slip in accuracy, which may be a result of
                      all iterative steps and generally higher channel dimensions in                                    constraining model parameterizations.
                      f , MDEQsaresubstantially slower than conventional net-                                           Figure 6 provides a visual comparison of different models
                        θ                                                                                               with respect to three metrics: performance, inference speed,
                      works like ResNets (which operate on progressively down-
                      sampled feature maps). This makes acceleration vital to                                           and training memory. These are reported on the CIFAR-
                      broader adoption of multiscale implicit models.                                                   10 dataset. For the ﬁrst time, we have an implicit-depth
                      The results of applying Jacobian regularization on multi-                                         model that runs with a competitive level of speed and accu-
                      scale DEQs for image classiﬁcation are shown in Table 2.                                          racy as large explicit networks such as ResNet-101, while
                      OnCIFAR-10,whereastheunregularizedDEQmodelsused                                                   consuming much less memory.
                      17 NFEs to reach the reported competitive level of perfor-                                        5.4. Effect of Jacobian Regularization on ρ(J )
                                                                                                                                                                                                  f
                      mance, our DEQ with Jacobian regularization can converge                                                                                                                     θ
                      well even within 6 iterations (in fact, we ﬁnd smaller NFE                                        In addition to the synthetic study, we also verify that the Ja-
                      values still trains, but signiﬁcantly hurts generalization per-                                   cobian regularization is indeed effectively constraining con-
                      formance). This improvement is also obvious in Figure 1a                                          ditioning of Jf . Note that the underlying Jacobian matrices
                                                                                                                                              θ
                                                                                Stabilizing Equilibrium Models by Jacobian Regularization
                                              WikiText-103 DEQ (Post-LN) Forward (T=16 steps)
                                  0                                                                                                  does quickly converge, but the constraint imposed on the
                          |
                                10
                          |
                             |
                          z
                             |
                             )
                          −
                             x                                                                                                       model class is too strong and eventually hurts performance
                          )
                             ;
                          x
                             z
                             (
                          ;
                              θ
                          z
                             f
                                 −1
                          (
                             |
                               10
                           θ                                                                                                        (e.g., since the training loss on CIFAR-10 usually overﬁts
                             |
                          f
                          |
                          |
                             
                            l
                                                                                   Tran. DEQ
                            a                                                                                                        to almost 0 towards the end of training, which makes the
                            u
                                 −2
                                                                                   Tran. DEQ + reg. (ours)
                               10
                            d
                            i                                                                                                        Jacobian loss dominant instead).
                            s
                                                                                   Trans. DEQ + weight decay 1e-6
                            e
                            r
                             
                            d
                                 −3
                            r
                               10                                                                                                   Wealso highlight two limitations of this approach. First,
                            a
                            w
                            r                                                                                                        the addition of Jacobian regularization term does not fun-
                            o
                            F
                                 −4
                               10                                                                                                    damentally solve the growing instability problem, but only
                                    0           10           20          30           40          50          60
                                                        Training Iterations (thousand steps)                                         empirically alleviates it. This means that we have to be
                                                                                                    −6
                        Figure 8. Adding weight decay of magnitude 10                                     to the DEQ-                careful about balancing the main loss objective and this aux-
                        Transformer doesn’t help stabilize the forward convergence.                                                  iliary objective (see Table 3). Second, while Jacobian reg-
                                                                                                                                     ularization facilitates faster convergence, there are certain
                        Table 3. Controlled experiments on the strength γ of the Jacobian                                           “physical laws” that we simply cannot bypass. For example,
                        regularization. The NFE value represents the “hard stop” threshold                                           if we apply a shallow convolutional DEQ whose layer has
                        wesetfor the corresponding DEQ models at inference.                                                          receptive ﬁeld 5 × 5 on a large image (e.g., 1024 × 1024),
                                          NFE=1         NFE=2         NFE=3         NFE=4          NFE=5         NFE=6               it is hard to be able to reach the ﬁxed point with just 6 iter-
                           γ = 0:1         82.4%         89.7%         91.9%         92.3%         92.7%         92.9%               ations simply because the model’s receptive ﬁeld may not
                           γ = 0:6         85.8%         91.5%         92.7%         93.0%         93.0%         93.1%               broaden sufﬁciently to cover valuable context. Although
                           γ = 1:2         84.4%         89.6%         92.2%         92.6%         92.7%         92.7%               one can possibly still force convergence with a large γ, it
                                                                                                                                    wouldundoubtedlyhurttheperformance. Thisexplainswhy
                        are large (e.g., [(B·110K) × (B·110K)] in WikiText-103,                                                     weneedmoreNFEsonImageNetthanonCIFAR-10(see
                        and [(B·198K) × (B·198K)] in ImageNet with MDEQ-                                                            Table 2); it also indicates that while our approach alleviates
                        small) and checking their full spectrum would be infeasible.                                                 the brittleness to architectural choices, its effectiveness can
                        Therefore, we conduct a study that monitors the average                                                      still depend on the architecture. This makes global-context
                        spectral radius ρ(J                  (z?)) (i.e., the largest absolute eigen-                                alternatives to ConvNets, such as self-attention-based vision
                                                         fθ                                                                          layers (e.g.,ViT (Dosovitskiy et al., 2020)) likely more ap-
                        value) on the validation set, over the ﬁrst 100K steps of                                                    pealing in the implicit model setting, which we leave for
                        DEQtrainingonWikiText-103usingthepowermethod(von                                                             future work.
                        Mises&Pollaczek-Geiringer,1929);seeFig.7. Importantly,
                        although kJ k onlyupper-boundsthespectral radius (see
                                             f    F
                                              θ
                        Sec. 4.2), we verify that our proposed regularization does ef-                                               6. Conclusion
                        fectively constrain ρ(Jf ) (see                         /    paths in Fig. 7), thereby
                                                                 θ                                                                  Wesummarized the weaknesses of existing DEQ models,
                        making DEQs more stable. In contrast, the unregularized
                        DEQwiththesamefewNFEsexplodesinbotheigenvalue                                                                including instability & inefﬁciency, architectural brittleness,
                        and shortly after also in perplexity (see                            /    paths), and only                   and hidden memory costs. We speciﬁcally discussed the
                        works if we increase NFE to 30 (see ×/× paths). In general,                                                  relationship between the spectral radius of the Jacobian
                        weempirically observe that training an unregularized DEQ                                                     and the stability of forward non-linear and backward linear
                        with insufﬁcient NFEs generally begets extremely noisy                                                       systems of DEQ models, and provided empirical evidence
                        gradients, thus leading to faster destabilization and even                                                   of the poor conditioning of the Jacobian. This motivates our
                        divergence.                                                                                                  introduction of Jacobian regularization. Our experiments
                                                                                                                                     showthatourmethodsigniﬁcantlyalleviatestheweaknesses
                        5.5. Ablative Analysis and Limitations of the Approach                                                       of DEQs, yielding a > 2:5× acceleration. This is a major
                                                                                                                                     step towards making implicit models more practical and
                        Wecontinue our discussion with some empirical ablative                                                       suitable for large-scale real-world applications. We hope
                        studies. First, while Grathwohl et al. (2019) found weight                                                   that our work will motivate further research that advances
                        decayuseful for regularizing ODE-based models’ NFEs, we                                                      our understanding and application of this class of models.
                        found weight decay generally not effective in stabilizing
                        DEQsandsometimesevencounter-productive. This is illus-                                                       References
                        trated in Figure 8, where after 50K steps the model started
                        to diverge to > 500 perplexity and stopped improving. In                                                     Amos, B. and Kolter, J. Z. OptNet: Differentiable opti-
                        addition, we also conduct an ablative experiment on how the                                                      mization as a layer in neural networks. In International
                        Jacobian regularization strength γ affects the performance                                                       Conference on Machine Learning (ICML), 2017.
                        when we constrain NFEs to ≤ 6 at inference time, with
                        results shown in Table 3 (CIFAR-10 dataset). In general, we                                                  Anderson, D. G. Iterative procedures for nonlinear integral
                        ﬁndthatifγ istoosmall, the ﬁnal performance maybegood                                                            equations. Journal of the ACM (JACM), 12(4):547–560,
                        but entails more NFEs. When γ is too large, the accuracy                                                         1965.
                                               Stabilizing Equilibrium Models by Jacobian Regularization
               Avron, H. and Toledo, S. Randomized algorithms for esti-          M., Heigold, G., Gelly, S., et al. An image is worth
                 mating the trace of an implicit symmetric positive semi-        16x16words: Transformersforimagerecognitionatscale.
                 deﬁnite matrix. Journal of the ACM (JACM), 58(2):1–34,          arXiv:2010.11929, 2020.
                 2011.                                                        Drucker, H. and Le Cun, Y. Improving generalization perfor-
               Ba, L. J., Kiros, R., and Hinton, G. E. Layer normalization.      manceusingdoublebackpropagation. IEEETransactions
                 arXiv:1607.06450, 2016.                                         onNeuralNetworks, 3(6):991–997, 1992.
               Baevski, A. and Auli, M. Adaptive input representations for    Dupont, E., Doucet, A., and Teh, Y. W. Augmented neural
                 neural language modeling. In International Conference           ODEs. In Neural Information Processing Systems, 2019.
                 onLearning Representations (ICLR), 2019.                     Duvenaud, D., Kolter, J. Z., and Johnson, M. Deep implicit
               Bai, S., Kolter, J. Z., and Koltun, V. Deep equilibrium           layers tutorial - neural ODEs, deep equilibirum models,
                 models. In Neural Information Processing Systems, 2019.         and beyond. Neural Information Processing Systems
                                                                                 Tutorial, 2020.
               Bai, S., Koltun, V., and Kolter, J. Z. Multiscale deep equilib-
                 rium models. In Neural Information Processing Systems,       El Ghaoui, L., Gu, F., Travacca, B., and Askari, A. Implicit
                 2020.                                                           deep learning. arXiv:1908.06315, 2019.
               Bradbury, J., Merity, S., Xiong, C., and Socher, R. Quasi-     Finlay, C., Jacobsen, J.-H., Nurbekyan, L., and Oberman,
                 recurrent neural networks. In International Conference          A. M. Howtotrain your neural ODE. arXiv:2002.02798,
                 onLearning Representations (ICLR), 2017.                        2020.
               Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,        Gal, Y. and Ghahramani, Z. A theoretically grounded appli-
                 J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,       cation of dropout in recurrent neural networks. In Neural
                 Askell, A., et al. Language models are few-shot learners.       Information Processing Systems, 2016.
                 arXiv:2005.14165, 2020.                                      Gould, S., Hartley, R., and Campbell, D. Deep declarative
               Broyden, C. G. A class of methods for solving nonlinear           networks: A new hope. arXiv:1909.04866, 2019.
                 simultaneous equations. Mathematics of Computation,          Grathwohl, W., Chen, R. T., Betterncourt, J., Sutskever,
                 1965.                                                           I., and Duvenaud, D. FFJORD: Free-form continuous
               Chang, B., Meng, L., Haber, E., Tung, F., and Begert, D.          dynamics for scalable reversible generative models. In
                 Multi-level residual networks from dynamical systems            International Conference on Learning Representations
                 view. In International Conference on Learning Represen-         (ICLR), 2019.
                 tations (ICLR), 2018.                                        He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
               Chen, T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud,         learning for image recognition. In Computer Vision and
                 D. K. Neural ordinary differential equations. In Neural         Pattern Recognition (CVPR), 2016.
                 Information Processing Systems, 2018.                        Hoffman, J., Roberts, D. A., and Yaida, S. Robust learning
               Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler,         with Jacobian regularization. arXiv:1908.02729, 2019.
                 M., Benenson, R., Franke, U., Roth, S., and Schiele, B.      Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger,
                 TheCityscapes dataset for semantic urban scene under-           K. Q. Densely connected convolutional networks. In
                 standing. In Computer Vision and Pattern Recognition            Computer Vision and Pattern Recognition (CVPR), 2017.
                 (CVPR), 2016.
                                                                              Hutchinson, M. F. A stochastic estimator of the trace of the
               Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and        inﬂuence matrix for laplacian smoothing splines. Com-
                 Salakhutdinov, R. Transformer-XL: Attentive language            munications in Statistics-Simulation and Computation,
                 modelsbeyondaﬁxed-lengthcontext. InAnnualMeeting                18(3):1059–1076, 1989.
                 of the Association for Computational Linguistics (ACL),
                 2019.                                                        Ioffe, S. and Szegedy, C. Batch normalization: Accelerat-
                                                                                 ing deep network training by reducing internal covariate
               Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F.        shift. In International Conference on Machine Learning
                 ImageNet: A large-scale hierarchical image database. In         (ICML), 2015.
                 Computer Vision and Pattern Recognition (CVPR), 2009.        Kawaguchi,K. Dynamicsofdeepequilibriumlinearmodels.
               Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,          In International Conference on Learning Representations
                 D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,          (ICLR), 2021.
                                                Stabilizing Equilibrium Models by Jacobian Regularization
               Kelly, J., Bettencourt, J., Johnson, M. J., and Duvenaud, D.    Peaceman, D. W. and Rachford, Jr, H. H. The numerical
                 Learning differential equations that are easy to solve. In      solution of parabolic and elliptic differential equations.
                 Neural Information Processing Systems, 2020.                    Journal of the Society for Industrial and Applied Mathe-
               Krantz, S. G. and Parks, H. R. The implicit function theorem:     matics, 3(1):28–41, 1955.
                 History, theory, and applications. Springer, 2012.            Poli, M., Massaroli, S., Yamashita, A., Asama, H., and Park,
               Krizhevsky, A. and Hinton, G. Learning multiple layers of         J. Hypersolvers: Toward fast continuous-depth models.
                 features from tiny images. Technical report, University         arXiv:2007.09601, 2020.
                 of Toronto, 2009.                                             Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
               Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet         Sutskever, I. Languagemodelsareunsupervisedmultitask
                 classiﬁcation with deep convolutional neural networks.          learners. OpenAI blog, 1(8):9, 2019.
                 In Neural Information Processing Systems, 2012.               Revay, M., Wang, R., and Manchester, I. R. Lipschitz
               Linsley, D., Ashok, A. K., Govindarajan, L. N., Liu, R., and      bounded equilibrium networks. arXiv:2010.01732, 2020.
                 Serre, T. Stable and expressive recurrent vision models.      Roosta-Khorasani, F. and Ascher, U. Improved bounds on
                 arXiv:2005.11362, 2020.                                         sample size for implicit matrix trace estimators. Founda-
                                                                                 tions of Computational Mathematics, 15(5):1187–1212,
               Liu, L., Liu, X., Gao, J., Chen, W., and Han, J.                  2015.
                 Understanding the difﬁculty of training transformers.
                 arXiv:2004.08249, 2020.                                       Salimans, T. and Kingma, D. P. Weight normalization: A
                                                                                 simple reparameterization to accelerate training of deep
               Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient           neural networks. In Neural Information Processing Sys-
                 descent with warm restarts. In International Conference         tems, 2016.
                 onLearning Representations (ICLR), 2017.
                                                                               Santoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski,
               Lu, C., Chen, J., Li, C., Wang, Q., and Zhu, J. Implicit nor-     M., Weber, T., Wierstra, D., Vinyals, O., Pascanu, R., and
                 malizing ﬂows. In International Conference on Learning          Lillicrap, T. Relational recurrent neural networks. In
                 Representations (ICLR), 2021.                                   Neural Information Processing Systems, 2018.
               Marcus, M. P., Marcinkiewicz, M. A., and Santorini, B.          Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,
                 Building a large annotated corpus of English: The Penn          J., and Catanzaro, B.     Megatron-lm: Training multi-
                 treebank. Computational Linguistics, 19(2), 1993.               billion parameter language models using model paral-
               Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer       lelism. arXiv:1909.08053, 2019.
                 sentinel mixture models. In International Conference on              ´
                                                                               Sokolic, J., Giryes, R., Sapiro, G., and Rodrigues, M. R.
                 Learning Representations (ICLR), 2017.                          Robust large margin deep neural networks. IEEE Trans-
               Merity, S., Keskar, N. S., and Socher, R. Regularizing and        actions on Signal Processing, 65(16):4265–4280, 2017.
                 optimizing LSTM language models. In International             Ubaru, S., Chen, J., and Saad, Y. Fast estimation of tr(f(a))
                 Conference on Learning Representations (ICLR), 2018.            via stochastic lanczos quadrature. SIAM Journal on Ma-
               Meyer, R. A., Musco, C., Musco, C., and Woodruff, D. P.           trix Analysis and Applications, 38(4):1075–1099, 2017.
                 Hutch++: Optimal stochastic trace estimation. In Sympo-       Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
                 sium on Simplicity in Algorithms (SOSA), 2021.                  L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
               Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spec-        tion is all you need. In Neural Information Processing
                 tral normalization for generative adversarial networks. In      Systems, 2017.
                 International Conference on Learning Representations          vonMises, R. and Pollaczek-Geiringer, H. Praktische ver-
                 (ICLR), 2018.                                                                              ¨
                                                                                 fahren der gleichungsauﬂosung. ZAMM-Journal of Ap-
                                                                                 plied Mathematics and Mechanics/Zeitschrift fur Ange-
               Novak, R., Bahri, Y., Abolaﬁa, D. A., Pennington, J., and                                                          ¨
                 Sohl-Dickstein, J. Sensitivity and generalization in neural     wandte Mathematik und Mechanik, 9(1):58–77, 1929.
                 networks: An empirical study. arXiv:1802.08760, 2018.         Winston,E.andKolter,J.Z. Monotoneoperatorequilibrium
               Pabbaraju, C., Winston, E., and Kolter, J. Z. Estimating Lip-     networks. In Neural Information Processing Systems,
                 schitz constants of monotone deep equilibrium models.           2020.
                 In International Conference on Learning Representations       Wu, Y. and He, K. Group normalization. In European
                 (ICLR), 2021.                                                   Conference on Computer Vision (ECCV), 2018.
                     Stabilizing Equilibrium Models by Jacobian Regularization
       Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing,
        C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer
        normalization in the transformer architecture. In Interna-
        tional Conference on Machine Learning (ICML), 2020.
                                 Accelerating Equilibrium Models by Stabilizing Their Jacobians
                                                                          Supplementary Material
                   A. Dataset Information, Experimental                                             causality constraint: yt depends only on x1:t and not on the
                                                                                                    future information x              . When each x represents a word
                       Settings and Hyperparameters                                                                            t+1:T                    i
                                                                                                    (i.e., a word embedding), the task is essentially a word-level
                  We provide below a detailed description of all tasks and                          language modeling task. This is a widely-studied problem
                   settings for experiments reported in Section 5, as well as                       in the NLP community (e.g., (Merity et al., 2017; 2018; Dai
                   some training speciﬁcs of the deep equilibrium network                           et al., 2019)), and has seen practical advancement in the last
                  (DEQs)weuse.                                                                      few years with development of GPT-3 (Brown et al., 2020;
                                                                                                    Radford et al., 2019).
                   A.1. 1D Synthetic Dataset                                                        Acommonly used large-scale corpus for this task is the
                  To visualize the effect of the proposed Jacobian regular-                         WikiText-103 (Merity et al., 2017) dataset, which contains
                   ization on DEQ models (see Section 5), we generated a                            103M/217K/246K words at train/validation/test time, re-
                   synthetic dataset with 5096 pairs (x;y) from the target func-                    spectively. The entire corpus has a vocabulary size of 267K
                   tion:                                                                            (i.e., the number of rows in the word embedding). Unlike
                                          3                                                         other well-processed, much smaller datasets like Penn Tree-
                         y = h(x) =         x3 +x2 +5x+2sin(x)−3+δ                                  bank (Marcus et al., 1993), WikiText-103 is much more
                                          2                                                         challenging as it contains many rare words and retains
                  where δ ∈ N(0;0:05) are i.i.d. noise variables added to                           punctuations, numbers, upper- and lower-cases from the
                      Target function h(x)     each sample in the dataset. Speciﬁ-                  source Wikipedia articles; it has been the standard bench-
                                               cally, we split the generated data into              mark for many high-capacity language models in recent
                                  h(1.9)       4096training samples and 1000 val-                   literature (Merity et al., 2018; Bradbury et al., 2017; Dai
                                               idation samples.                                     et al., 2019). We provide a shell script in our submitted code
                                                                                                                                     1
                                                                                                    to download this dataset.
                         h( 1)                 Figure 9 shows the target function.
                                               In the context of deep equilibrium                   A.3. CIFAR-10 & ImageNet Image Classiﬁcation
                                               networks, we aim to learn a func-
                                               tion z?(x) such that z? = fθ(z?;x)                   The CIFAR-10 (Krizhevsky & Hinton, 2009) dataset con-
                                               and z?(x) ≈ h(x). At a high level,                   tains 60,000 color images of resolution 32×32 that fall into
                                 h(0.5)        weshouldexpect the intersection be-                  10 object classes (with uniformly 6,000 images per class).
                                               tweenthez          =f (z;x)surfaceand                Weusethestandard setting where 50K of these images are
                                                              out      θ
                                               the z      =zplanetobeexactly like                   used for training and the rest 10K for validation purpose.
                  Figure 9. Target                    out
                   function y = h(x).          the gray curve in Figure 9.                          TheImageNet(Krizhevskyetal.,2012)dataset,ontheother
                                               The learned DEQ equilibria z?(x)                     hand, contains over 1.28M training images and 150K test
                   are empirically demonstrated in Figure 5 in red dashed lines                     images, distributed over 1,000 classes. All images are re-
                   for different choices of γ. As expected, all γ ﬁt the tar-                       scaled to 224 × 244 resolution before they are fed into the
                   get function perfectly, but the introduction of the Jacobian                     models (as the original images are of variable resolutions
                   regularization makes the surface more ﬂat around the ﬁxed                        and scales). This is a frequently used dataset for evalu-
                   point.                                                                           ating large-scale vision networks, and has been used for
                                                                                                    also pretraining many image feature extractor for use on
                   A.2. WikiText-103 Word-level Language Modeling                                   downstream tasks.
                  Word-level language modeling tasks aim to predict the next                        ForbothCIFAR-10andImageNet,eachtrainingimagegoes
                  wordofatextual sequence by integrating the semantics and                          through a canonical data augmentation process before they
                   information of current and past tokens. Formally, given an                       are fed into the model, where we perform random cropping
                   input sequence x1:T ∈ RT×p (where xi ∈ Rp and T is                               and random horizontal ﬂipping.
                   the sequence length), an autoregressive sequence model G                              1Ofﬁcially, this dataset can be downloaded at this link.
                   produces output G(x1:T) = y1:T ∈ RT×q that satisﬁes the
                                                                Stabilizing Equilibrium Models by Jacobian Regularization
                   Table 4. Hyperparameters, optimizer choices, and model details (at training time) for all tasks reported in Section 5. The arrows in the
                    Jacobian regularization strength (e.g., A → B) mean that we dynamically increase from A to B over the course of DEQ training.
                                                                Synthetic Dataset      WikiText-103 language modeling           CIFAR-10classiﬁcation          ImageNetclassiﬁcation
                               Architecture of f              2-Layer ReLU block               Transformer layer                  Multiscale DEQ layer           Multiscale DEQ layer
                                                 θ                (see Section 5)              (Pre- and Post-LN)               (residual block + fusion)      (residual block + fusion)
                                  #ofEpochs                             50                             23                                  200                            120
                                   Batch Size                           64                             60                                   96                            112
                                   Optimizer                          Adam                           Adam                                 Adam                           SGD
                               Start Learning rate                    0.001                         0.00025                               0.001                           0.05
                             Learning rate warmup                       No                        Yes, 1 epoch                             No                             No
                             Learning rate schedule                   Cosine                         Cosine                              Cosine                         Cosine
                                 Weight Decay                           0                               0                                   0                           5·10−5
                             Hidden dimensionality                      50                    700 (embedding size)              [28,56,112,224] (4 scales)          [32,64,128,256]
                             Input Sequence Length                     N/A                             150                                 N/A                            N/A
                                Input Image Size                       N/A                            N/A                                32×32                        224×224
                                 Normalization                        None                LayerNorm(Baetal., 2016)           GroupNorm(Wu&He,2018)                    GroupNorm
                                Recurrent Droput                       N/A                            0.06                                 0.25                           0.02
                             Weight Normalization                       No                             Yes                                 Yes                            Yes
                      #ofInput Injection Downsamplings                 N/A                            N/A                                  N/A                             2
                            Forward NFEs Threshold                      6                              12                                   7                              14
                           BackwardNFEsThreshold                        6                              12                                   8                              14
                              Forward Threshold "                     10−3                            10−3                                10−3                           10−3
                             BackwardThreshold "                      10−4                            10−4                                10−4                           10−4
                            Jacobian Reg. Strength γ                {0,1,2,4}                      1.6 −2.5                                0.5                         2.0 − 3.0
                           Jacobian Reg. Frequency p                   0.4                            0.35                                 0.05                           0.1
                          MforHutchinsonEstimator                       1                             1or2                                  1                            1or2
                    A.4. Training Setting and Hardware                                                    mator and backpropagate through it; and 2) it helps reduce
                    Ourexperimental protocols are intentionally set to be max-                            the likelihood of the model overﬁtting on this auxiliary loss
                    imally consistent with prior work (Bai et al., 2019; 2020).                           term (since, as we noted in Section 5.5, the model could be
                    This includes hyperparameters (see the subsection below),                             sensitive to γ, and M is small), which we generally observe
                    other regularization methods (e.g., recurrent dropout (Gal                            to beneﬁt the performance, though only slightly. Therefore,
                   &Ghahramani, 2016) & group normalization (Wu & He,                                     during training, the model would still proceed in the actual
                    2018)), and initialization schemes (where all parameters                              stochastic gradient direction, and only use the regularized
                    are initialized at the start of training by sampling from                             direction occasionally.
                    N(0;0:01)). For the multiscale DEQs that were used in                                 Formally, the training objective we highlighted in Sec-
                    the image classiﬁcation task, we used 4 resolutions, where                            tion 4.2 should be:
                                                                                                                                               P
                    each subsequent resolution is of exactly half the height and                                                                  M      >       ?  2
                                                                                                                   ?               ?              m=1k Jf (z )k2
                                                                                                           L (z )=L (z )+τ·γ                                 θ        ;     ∈N(0;I )
                   width of the previous resolution. Although Bai et al. (2020)                              total          orig                        Md                m             d
                    highlighted the need to train a ReLU-based network with                               where τ = Bernoulli(p) is a random variable and M is the
                    softplus for stability purposes, we found it not necessary                            numberofsamplesusedforHutchinson estimator.
                    in our experiments with regularized DEQs, most likely be-                             All experiments in this paper, including the speed and mem-
                    cause of the role Jacobian regularization plays in stabilizing                        ory benchmarks we provide, were conducted on RTX 2080
                    the network convergence.                                                              Ti GPUs. WikiText-103 language modeling and ImageNet
                    Onething to note is that empirically, rather than applying                            classiﬁcation models (MDEQ-small) were trained with 4
                    the proposed Jacobian regularization on all training itera-                           GPUsinadata-parallel setting.
                    tions, we only randomly and partially apply this auxiliary
                    loss. For example, when we set the auxiliary loss frequency                           A.5. Hyperparameters
                    p to 0.5, only half of the training iterations (randomly se-
                    lected) are trained with the Jacobian regularization term (see                        We report the hyperparameters used at training time in
                    Table 4). This is motivated by the empirical observation that                         Table 4. Except for those used in the synthetic data and
                    Jacobian-related regularizations usually hurt performance,                            for Jacobian regularization, most of the other hyperpa-
                    e.g., as in its application in robust learning (Hoffman et al.,                       rameters were essentially taken from the original DEQ-
                    2019). Therefore, such partial/random supervision with                                Transformer (Bai et al., 2019) and MDEQ (Bai et al.,
                    the Jacobian regularization brings two beneﬁts: 1) the rest                           2020) without major modiﬁcations. For both Anderson
                   (1−p)-portionofthetrainingiterationscanpickupafurther                                  andBroydenﬁxed-pointsolvers,weusetherelativeresidual
                                                                                                           kf (z;x)−zk
                    speedup as we don’t need to compute the Hutchinson esti-                                  θ            as a measure of convergence quality in forward
                                                                                                             kfθ(z;x)k
                                                                                 Stabilizing Equilibrium Models by Jacobian Regularization
                        Table 5. A more complete version of Table 1 with more memory and efﬁciency comparison. Memory benchmarked on batch size 15
                         and excludes the embedding layer. † indicates unregularized model hard-stopped at inference time (while still trained with more NFEs).
                         Overall, we ﬁnd that Jacobian regularization allows us to train and predict with much fewer NFEs, at a relatively small cost in performance.
                                                                                                                        ModelSize         Perplexity      ttrain (relative)    Train NFE       Valid. NFE        Training Memory
                                                 AWD-QuasiRNN(Bradburyetal.,2017)                                          159M              33.0                 -                  -                -                 7.1GB
                                             Relational Memory Core (Santoro et al., 2018)                                 195M              31.6                 -                  -                -                    -
                                               Megatron-LM(Shoeybietal., 2019) [SOTA]                                      8300M             10.8                 -                  -                -                    -
                                               Transformer-XL (18-layer) (Dai et al., 2019)                                110M              24.1                1×                  -                -                 9.0GB
                                              DEQ-Transformer (Pre-LN) (Bai et al., 2019)                                   98M           [diverged]            N/A                 30              N/A                  N/A
                                              DEQ-Transformer (Post-LN) (Bai et al., 2019)                                  98M              24.0              3:1×                 30               30                 3.9GB
                                                DEQ-Transformer (Post-LN) early stopped                                     98M              29.2              3:1×                 30               12                 3.9GB
                                              DEQ-Transformer (Post-LN) (Bai et al., 2019)                                  98M              26.0              2:2×                 20               20                 3.6GB
                                              DEQ-Transformer (Post-LN) (Bai et al., 2019)                                  98M           [diverged]            N/A                 15              N/A                 3.6GB
                                                DEQ-Transformer(Pre-LN)+JR(ours)                                            98M              24.5              1:5×                 14               14                 4.8GB
                                               DEQ-Transformer(Post-LN)+JR(ours)                                            98M              24.9              1:4×                 13               12                 4.8GB
                                DEQ-Transformer(Post-LN)+JR(ours)(trainedonseqlen=300)                                      98M              23.8              2:2×                 13               13                 6.5GB
                         and backward passes. At inference time, we generally re-                                                     whichindicatesarelativelymoreaccurategradientproduced
                         duce the number of NFEs (e.g., cf. Table 4 and Table 1),                                                      bythe implicit function theorem.
                         while the other hyperparameters (e.g., GroupNorm group                                                       Wefurther corroborate this ﬁnding via empirical evidence
                         sizes) are kept the same.                                                                                     ontheCIFAR-10datasetwithamultiscale DEQ(MDEQ)
                         B. Additional Experimental Results                                                                            instance, shown in Figure 10a. Compared to the original
                                                                                                                                       MDEQ(blueline),theJacobian-regularized version of the
                         B.1. MemoryConsumption                                                                                        backward pass experiences much fewer ﬂuctuations (and
                                                                                                                                       thus less stochastic gradients). We also compared to an alter-
                         AswenotedinSections4and5,usingJacobianregulariza-                                                             native solution that uses the simple weight decay. Although
                         tion and thus the vector-Jacobian-product-based Hutchinson                                                    it also alleviates the ﬂuctuation problem, our empirical ob-
                         estimator introduces some extra memory cost at training                                                       servations suggest that weight decay alone almost always
                         time due to the need to differentiate w.r.t. the kJ                                    k term.                adds more difﬁculty to the ﬁxed point solving. This agrees
                                                                                                             fθ F                     with what we have observed in the forward pass in Sec-
                         Overall, with the same batch size and sequence length, we
                         observearoughly25%increaseintrainingmemoryrequired                                                            tion 5.5. Such comparison can be seen in Figure 10a in
                         (from about 3.9GB to 4.8GB, excluding embeddings). This                                                       the purple line, which converged even more poorly than the
                         is less than the memory consumption of a layer, because                                                       original baseline after 14 backward solver iterations (with
                         the reduction in NFEs needed on the other side saves the                                                      relative residual > 0:05 and increasing slowly over training).
                         memoryusedbythesolver(see Section 3.4). However, this                                                         In contrast, the regularized backward pass is more smooth
                         memoryfootprint is still much better than the conventional                                                    and stable (red line) throughout training (we used γ = 0:5).
                         explicit Transformer-XL model, which consumes about 2×
                         as much GPU memory. With the Jacobian regularization,                                                         B.3. Failure of Weight Decay to Fix the Problem
                         as we can see, the DEQ models are much more efﬁcient in                                                      This overall inability of weight decay alone to ﬁx the DEQ
                         time complexity than before, while still staying competitive                                                  stability issue (e.g., see Figures 8 and 10a), we believe,
                         onthe space complexity and the performance fronts.                                                            exactly suggests that there is a deeper implicitness property
                         B.2. DEQ’s Backward Convergence with Jacobian                                                                 of the model that should be regularized than just the value
                                 Regularization (CIFAR-10)                                                                             of individual weights. As DEQ networks typically rely
                                                                                                                                       on a single fθ block, their complex non-linear structure
                         As we discussed in Section 4, the backward dynamics of                                                        makestheir stability depend as much on the linear parts of
                         a DEQ model is a linear ﬁxed point system that depends                                                        f (which weight decay does regularize) as the non-linear
                                                                                                                                         θ
                                                                                                             ?                         parts (which weight decay does not directly regularize; e.g.,
                         directly on the Jacobian at equilibrium (i.e., Jf (z )). There-
                                                                                                       θ                               self-attention in f if we use a Transformer layer). On the
                         fore, the backward pass stability is directly inﬂuenced by                                                                                   θ
                         the conditioning of the Jacobian that we regularize. The                                                      other hand, Jacobian regularization takes into account both
                         stabilizing effect of the proposed Jacobian regularization                                                    parts as it tries to constrain the overall spectral radius of the
                         on the backward pass convergence was already shown for                                                        matrix.
                        WikiText-103 language modeling in Figure 3b, where we                                                         Wealsoprovide some additional analysis on how kJ k
                                                                                                                                                                                                                                    f     F
                         empirically observe that the Jacobian-regularized DEQ-                                                                                                                                                       θ
                         Transformer’s backward pass stays at a consistent level,                                                      evolves during training in Figure 10b and 10c. Speciﬁ-
                                                                                                                                       cally, even with weight decay, the convergence of DEQ-
                                                                                            Stabilizing Equilibrium Models by Jacobian Regularization
                                                      CIFAR-10 DEQ Backward (T=14 steps)                                             Wikitext-103 DEQ ||J (z*)||2 vs. Final Residual                                                                 2
                                                                                                                                                              f       F                                             WikiText-103 DEQ ||Jf(z*)||F (T=16 steps)
                                  0.175                                                  MDEQ                           |   2.00                                  Trans. DEQ + reg.                    )
                                                                                                                        | |                                                                            d                                       Trans. DEQ + reg.
                                                                                                                        z |                                                                            e
                              |                                                          MDEQ + weight decay              )                                       Trans. DEQ + weight decay            z
                              |   0.150                                                                                 ) x                                                                            i                                       Trans. DEQ + weight decay
                                |                                                                                       x ;                                                                            l
                                |                                                        MDEQ+reg. (ours)                 z 1.75                                                                       a
                              +                                                                                         ; (                                       Trans. DEQ                                                                   Trans. DEQ
                              )                                                                                         z f
                              I +                                                                                       ( |                                                                            m
                                 f                                                                                      f |                                                                            r
                                J                                                                                       |
                              Jf  0.125                                                                                 |   1.50                                                                       o
                              ( x                                                                                        l                                                                             n    0
                                |                                                                                        a
                              x |                                                                                                                                                                      (  10
                              |                                                                                          u                                                                              
                              |                                                                                          d  1.25                                                                      2|F
                               l  0.100                                                                                  i                                                                             |
                               a                                                                                         s                                                                             )
                               u                                                                                         e                                                                             *
                               d                                                                                         r                                                                             z
                               i                                                                                            1.00                                                                       (f
                               s  0.075                                                                                  e                                                                             J
                               e                                                                                         v                                                                             |
                               r                                                                                         i                                                                             |
                                                                                                                         t  0.75                                                                        
                               d                                                                                         a                                                                             m
                               r                                                                                         l                                                                             r
                               a  0.050                                                                                  e                                                                             o
                                                                                                                         r
                               w                                                                                                                                                                       n
                               k                                                                                         d  0.50                                                                        
                               c                                                                                         r                                                                             n
                                  0.025                                                                                  a                                                                             a
                               a                                                                                         w                                                                             i
                               B                                                                                         r  0.25                                                                       b 10 1
                                                                                                                         o                                                                             o
                                                                                                                         F                                                                             c
                                  0.000                                                                                                                                                                a
                                                                                                                            0.00                                                                       J
                                               5        10       15       20        25       30        35       40                0.0           0.2          0.4           0.6          0.8                     0        10        20       30        40       50        60
                                                       Training Iterations (thousand steps)                                               Jacobian norm ||J (z*)||2 (normalized)                                        Training Iterations (thousand steps)
                                                                                                                                                               f       F
                            (a) Jacobian regularization improves both the ﬂuc- (b) Forward relative residual on WikiText- (c) Jacobian norm grows throughout train-
                            tuation and quality of the backward convergence. 103 as a function of the Jacobian norm.                                                                                 ing, even when we regularize for it.
                                                    Figure 10. Additional analysis on DEQ models’ backward convergence (on CIFAR-10), Jacobian norm, etc.
                            Transformer models can be quite bad (see purple dots in
                            Figure 10b), with a clear correlation between the larger rela-
                                                                                       2
                            tive residual and larger kJ                              k . Indeed, with a non-linear
                                                                                 fθ F
                            structure as complex as the multi-head self-attention, sim-
                            ply constraining the weights to be small is not sufﬁcient
                            to ensure well-conditioned Jacobians. Moreover, while the
                            Jacobian regularization helps signiﬁcantly stabilize the for-
                            ward and backward convergence (see Figure 1a, 10a and 3),
                            wenotethat a regularized DEQ model still in fact gradually
                            tends to “critical stability”. This can be seen in Figure 10c,
                            where the Jacobian norm grows slowly over training itera-
                            tions (red line) for a ﬁxed γ, though at a rate much slower
                            thantheunregularizedandweight-decayedbaselines. There-
                            fore, as we indicated in Section 5.5, the proposed Jacobian
                            regularization does not fundamentally ﬁx the growing in-
                            stability problem, but only alleviates it. This also calls for
                            adaptive γ scheduling during training (which we adopt in a
                            simpleforminourimplementationandleavemoreadvanced
                            schemes for future work).
