                                                         BENGIO,DUCHARME,VINCENTANDJAUVIN
                                                                       i-th output = P(w = i | context)
                                                                                        t
                                                                       . . .              softmax    . . .
                                                                                   most  computation here
                                                                               . . .      tanh. . .
                                             C(w      )                       C(w )      C(w )
                                                 t−n+1                            t−2        t−1
                                                      . . .     . . .       . . .                 . . .
                                             Table                              Matrix C
                                              look−up                           shared parameters
                                              in C                              across words
                                                  index for w          index for w           index for w
                                                            t−n+1                 t−2                   t−1
                         Figure 1: Neural architecture: f(i,w           ,···,w        )=g(i,C(w         ),···,C(w         )) where g is the
                                                                    t−1        t−n+1                t−1             t−n+1
                                     neural network andC(i) is the i-th word feature vector.
                         parameters of the mapping C are simply the feature vectors themselves, represented by a |V|×m
                         matrixC whose row i is the feature vectorC(i) for word i. The function g may be implemented by a
                         feed-forward or recurrent neural network or another parametrized function, with parameters ω.The
                         overall parameter set is θ =(C,ω).
                             Trainingisachievedbylookingforθthatmaximizesthetrainingcorpuspenalizedlog-likelihood:
                                                       L= 1        log f(w ,w      ,···,w        ;θ)+R(θ),
                                                             T ∑            t   t−1        t−n+1
                                                                 t
                         where R(θ) is a regularization term. For example, in our experiments, R is a weight decay penalty
                         applied only to the weights of the neural network and to the C matrix, not to the biases.3
                             In the above model, the number of free parameters only scales linearly with V, the number of
                         words in the vocabulary. It also only scales linearly with the order n : the scaling factor could
                         be reduced to sub-linear if more sharing structure were introduced, e.g. using a time-delay neural
                         network or a recurrent neural network (or a combination of both).
                             In most experiments below, the neural network has one hidden layer beyond the word features
                         mapping, and optionally, direct connections from the word features to the output. Therefore there
                         are really two hidden layers: the shared word features layer C, which has no non-linearity (it would
                         not add anything useful), and the ordinary hyperbolic tangent hidden layer. More precisely, the
                         neural network computes the following function, with a softmax output layer, which guarantees
                         positive probabilities summing to 1:
                                                                                                 y
                                                                                                  w
                                                                 ˆ                             e t
                                                                P(w |w      ,···w        )=          .
                                                                     t   t−1       t−n+1           y
                                                                                              ∑ie i
                          3. The biases are the additive parameters of the neural network, such as b and d in equation 1 below.
                                                                                1142
