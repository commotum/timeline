                  P.Adriaans  and  P.  Van  Emdo  Boas,  Computation,  Information,  and  the  Arrow  of  Time,  In
                  COMPUTABILITY IN CONTEXT Computation and Logic in the Real World, edited by S Barry
                  Cooper (University of Leeds, UK) & Andrea Sorbi (Universita degli Studi di Siena, Italy) (pp 1-17),
                  http://ebooks.worldscinet.com/ISBN/9781848162778/9781848162778_0001.html
                  As I am travelling I have little time to react on the contents of this blog now, but hope to ﬁnd
                  time to do this later,
                  Cheers Pieter Adriaans
                  Dr. Ajit R. Jadhav Says:
                  Comment #100 September 28th, 2011 at 7:35 am
                  A little correction to what I said in my comment #90 above. (And isn’t there always at least
                  one?)
                  The example of the spring-mass system doesn’t ﬁt what I was trying to point out. The strain
                  energy of the system by itself produces a bowl/well (a “U”)—i.e., even without considering the
                  linear change to the PE of the system as ef㘶ected by the applied force.
                  Instead, what we really needed was two dif㘶erent factors such that each by itself produces only
                  a  monotonically increasing/decreasing ef㘶ect, i.e. not a hump/well when taken alone, even
                  though their combined ef㘶ect produces a hump/well.
                  In contrast, the model of a diatomic molecule does show the required behavior. (The spring-
                  mass system does not, even if the spring is taken to be nonlinear.)
                  I stand corrected.
                  Raoul Ohio Says:
                  Comment #101 September 28th, 2011 at 11:46 pm
                  Scott is correct that when I understand more about KC I appreciate it more. (I was afraid that
                  was going to happen!).
                  Decades of understanding things by working through the standard examples has not prepared
                  me for thinking about things where you can’t calculate anything! That reminds me of my
                  brother-in-law’s deﬁnition of an engineer: A technician who can’t ﬁx anything.
                  Terry Bollinger Says:
                  Comment #102 September 28th, 2011 at 11:49 pm
                  Scott #89:
                  You said:
                  … K(x) lets you do is give a clear, observer-independent meaning to the loose notion of there
                  “not existing any patterns” in a string… detecting the existence or nonexistence of patterns is
                  hard! … you can still use the deﬁnition of K(x) to give meaning to hundreds of intuitions that
                  otherwise would’ve remained forever at a handwaving level… [e.g.] “The overwhelming majority
                  of strings are patternless.” … “If a short computer program outputs a patternless string, then it
                  can only be doing so by generating the string randomly.”
                  Fascinating comments that I seriously am trying to resist for right now…
                  But a couple observations, what the heck, that are likely very well covered, but may be of
                  interest to some readers anyway:
                  — Proximity  regions:  Regions  “near”  extreme  compressions  (e.g.  pi  subsequences),  where
                  nearness is deﬁned by the code brevity of a transformation, are also highly compressed, if not
                  by  quite  as  much.  E.g.  pi  subsequences  are  highly  compressed,  but  for  very  long  pi
                  subsequences, the integers on quite some distance to either side are also highly compressed
                  because they can be accessed by simple addition or subtraction functions that can be very
                  short in comparison to a very long pi subsequences. This ef㘶ect applies until the added or
                  subtracted  numbers  become  so  long  that  they  also  become  comparable  in  size  to  the  pi
                  subsequence… unless they too have their own compressions. The result becomes very complex
                  and recursive, and “simple” statements about the absence of K. compressions becomes a lot
                  trickier. After going through that in my head, I’m now realizing how naive my original vision of
                  a very sparse set of ﬂagpoles with high compression ratios was. K. compressions beget huge
                  ranges of “nearby” compressions via further composition with shorter transforms. The resulting
                  landscape  is  not  just  a  few  ﬂagpoles,  but  is  likely  quite  craggy,  with  the  nature  of  the
                  “cragginess” rather open as more short “generalization” transforms are added or explored.
                  — Why do K’s exist at all? If you start with very small integers and move slowly up in digit
                  length, when and where does the concept of a K compression meaningfully begin? I would
                  assume there is an inverse relationship with abstraction. The number 1 is extraordinarily useful
                  because it can represent almost anything at all, provided only that the accessing entity is itself
                  highly complex and provides huge context (“push the red button”). [This is the “V in time”
                  model of context I talked about earlier in a CV blog entry; meaning is always provided by
                  earlier-in-time duplications and separation of context.] Some folks tend to call that kind of
                  versatility “abstraction,” though you could certainly describe it other ways.
                  –“random” just invokes external complexity, so I don’t know what that really says except that
                  your “computer” becomes very complicated indeed. Normal computers enforce “well behaved”
                  memory concepts that are… well, a bit boring if you compare them to the more heuristic thinks
                  that seem to go on in biology to achieve e㘠陦cient computation.
                  A very, very complicated computer can of course make any string it wants to (or doesn’t want
                  to, if it allows in external sources of randomness).
                  Enough, I’m just idly exploring points of curiosity without having even read your references.
                  Invariances still strike me as more critical to fundamental physics, with “interesting” complexity
                  emerging from V splits back in time. Think e.g. how little information would mean if every
                  electron had dif㘶erent physics. It is the invariance of their properties that enables higher levels
                  of interesting complexity — all of which I assume is below the level (?) that K. theory works at?
                  Cheers,
                  Terry
                  P.S. Pieter Adriaans #99: Interesting seminar title and talks (quantum & new info? hmm!), but
                  the links in the agenda did not work, at least not for me.
                  Terry Bollinger Says:
                  Comment #103 September 28th, 2011 at 11:55 pm
                  Pieter Adriaans #99: Correction, the links in the agenda are now working! Something corrected
                  itself somewhere, not sure on which end.
                  Dr. Ajit R. Jadhav Says:
                  Comment #104 September 29th, 2011 at 3:26 am
                  Dear Scott,
                  Your main idea should work.
                  Over the past 20-odd minutes (a time period much shorter than what it took me to type this
                  reply down), I just manually performed the empirical version, with a 4X4 square cup, with the
                  upper two rows black, the bottom two rows white, and a deterministic rule. At each time step,
                  the deterministic rule pushes one black square one cell down (provided certain conditions are
                  met,  of  course.)  The  rule  does  ultimately  take  the  system  “down”  to  a  chess-board
                  conﬁguration (alternate B&W) as the ﬁnal state. (The rule is such that it halts the process once it
                  reaches a chess-board state—which happens to have the maximum entropy.)
                  There are in all 32 boundaries between _adjacent_ squares. (We drop the 8 boundaries at, say
                  the right and the bottom edges, for symmetry: to ensure a possibility of getting inﬁnite lattice
                  via periodic repetition.) Deﬁne a boundary between two adjacent similar squares (BB or WW) as
                  similar (S), and those between two adjacent dissimilar squares as dissimilar (D).
                  A  graph  of  the  total  number  of  D  boundary  segments  vs.  time  does  empirically  show  an
                  inﬂection. (Rapid increase, slow increase, (no increase?—this being a manual thing, I did not
                  check  all  the  intermediate  conﬁgurations),  slow  increase,  rapid  increase.)  Hence,  (some
                  normalized) “one” minus the rate of growth of number of D segments should give you a hump.
                  (The rate of growth itself will give a bowl/well.)
                  Aside:  The  total  of  number  of  S  segments  follows  a  symmetrically  opposite  trend.  Not
                  surprising: D + S = 32, a constant.
                  Since  I  am  a  novice  in  TCS,  I  was  happy  with  imagining  how  the  RLE  would  work  as  the
                  compression method.
                  In the initial stages, moving a black square down increases the compressed size. By destroying
                  the “contiguous-ness” pattern, it should increase the upper bound of KC in any compression
                  method, because most of the stuf㘶 is contiguous. In the ﬁnal stages, moving a black square
                  down increases the “alternativity” pattern, and so it should once again decrease the upper
                  bound of KC. So, the upper bound of KC must be high somewhere in between. Can there be
                  some ﬂuctuations in the upper bound of KC in the intermediate stages? I am afraid this could
                  perhaps be the case. Manual checks require real^real hard work! In any case, “as everyone
                  knows,” the entropy only increases throughout the time.
                  Is the decrease in (the upper bound of) KC symmetrical around the half-time? I have no idea
                  how to estimate this thing.
                  Of what use is an upper bound if we don’t know the lower bounds? No idea.
                  So, I am happy to leave these two questions to the TCS proper guys.
                  That’s about the empirical part. If something strikes me on the theoretical side, will let you
                  know. (But you do know better than wanting to wait for my reply, don’t you?)
                  Just one aside before leaving. At least for this system, I have a logic whereby one could (I think)
                  show that having a probabilistic evolution rule would not lead to an overall dif㘶erent outcome,
                  though in the probabilistic case, there could be ﬂuctuations in the middle (or more of them, if
                  ﬂuctuations are present also in a bounded-KC measure for the deterministic case). This should
                  not be an issue. On the contrary, it might help make the things in the middle even more
                  interesting to people like Sean!
                  I  think I am done with this thread (though I will continue to read the comments here for a
                  while). I will have a look at your paper once it is ready.
                  Best,
                  –Ajit
                  [E&OE]
                  Juan Ramón González Álvarez Says:
                  Comment #105 September 29th, 2011 at 9:36 am
                  Thanks by this discussion. First, let me correct a mistake in my last post: “Evidently for 0 =0
                  does not hold globally and your basic assumption would be invalid.
                  If universe is isolated, as most experts believe, then the law holds globally. However, unless
                  you have a new functional expression for S, you cannot compute the rate because Universe is
                  not globally homogeneous. The standard procedure in cosmological thermodynamics is to work
                  locally using the standard functional expression for S (See Misner, Thorne, and Wheeler). But
                  locally, the law (dS/dtau)>=0 is only valid if you assume constant composition and not local
                  heat ﬂow (Misner, Thorne, and Wheeler emphasize that they are assuming *both*). Evidently
                  composition and temperature are not constants in our Universe.
                  What you call “max-entropy” correspond to a stable equilibrium, deﬁned by dS=0 (equilibrium)
                  and d^2S<0 (stability) see #78. Evidently, the latter condition implies that S is a maximum.
                  Regarding  your  deﬁnition  of  complexity,  it  cannot  dif㘶erentiate  situations  of  dif㘶erent
                  complexity. Your deﬁnition only could be valid for time scales above t_mem. That is why your
                  evolution law is so simple. When studying the ﬁne-details of the evolution, your cell approach
                  "subject to random nearest-neighbor mixing dynamics" is not valid. My question remains: why
                  would I abandon a deﬁnition of complexity in term of the degree of contraction (and the
                  characteristic time scales) by the your own?
                  There is  much more room for our disagreement than I can write here. For instance, your
                  attempt to study the mixing process of the “cof㘶ee cup” by discretizing it and describing the
                  state by the number of black and white pixels (the “cof㘶ee” and “milk”) is only valid for mixing
                  processes  involving  slow  and  small  chemical  gradients.  Your  approach  is  an  excellent
                  approximation for the calm mixing of the cof㘶ee and milk at your home, but it fails for studying
                  'violent' mixings as those what happened at very early cosmological scales.
                  The thermodynamic and hydrodynamic equations found in Misner, Thorne, and Wheeler are
                  only valid for slow and small gradients in temperature, composition, pressure… The whole
                  formalism would give wrong values for the entropy and complexity of the Universe at early
                  times. A possibility to study fast processes and strong gradients is to use the laws of *extended
                  thermodynamics* (see the celebrated monograph by Jou, Casas-Vázquez, and Lebon).
                  The main point here is that at early times complexity is larger and you need to use more
                  complex equations (plus an extended state space) for describing processes. As a consequence,
                  the red line in your above ﬁgure about complexity is not correct.
                  Regards.
                  Scott Says:
                  Comment #106 September 29th, 2011 at 10:45 am
                  Hi Juan, just a quick correction: neither of the ﬁgures is mine. As I wrote in the post, both were
                  taken directly from a talk by the physicist and cosmologist Sean Carroll, author of From Eternity
                  to Here. So, you might need to take up your physics disagreements with him…
                  FB36 Says:
                  Comment #107 October 1st, 2011 at 1:13 pm
                  I have another deﬁnition for complexity.
                  Assume  state  description  of  a  physical  system  converted  to  a  string.  Both  Kolmogorov
                  Complexity and Shannon Entropy would give max values in the equilibrium state in the end,
                  same as entropy. But notice the dif㘶erence, even though reproducing a particular random string
                  require max length UTM program, a very short program can reproduce arbitrary strings which
                  has the same SE/KC value as that random string.
                  So here is a new deﬁnition of complexity of a physical system at state t:
                  Length  of  the  minimal  UTM  program  that  can  reproduce  random  strings  (w/  uniform
                  probability) which has the same SE or KC value as the string of state t description.
                  (Assuming the UTM has access to a random number generator.)
                  With this deﬁnition the complexity value would me max around the middle of time frame of a
                  system and would be minimal at the ordered start and the max entropy end.
                  Ian Durham Says:
                  Comment #108 October 1st, 2011 at 3:35 pm
                  I have to be honest and say I just simply did not have the time to read through all 106 previous
                  comments, though many sounded interesting.
                  At any rate, one of the things that struck me at the conference (and which is what led to my
                  continued harping on the entropy thing until I was shouted down by David Albet [:)]) is that we
                  have a tendency to introduce unintentional bias even into our deﬁnitions.
                  Now, I like the idea of Kolmogorov complexity (a colleague of mine in the Math Dept. studies
                  Kolmogorov entropy  and  I  had  a  student  a  few  years  back  who  was  working  with  me  to
                  investigate how we could use it as a generalization). But I think that we really ought to go back
                  even further to deﬁne what these things mean.
                  The conference (combined with reading a bunch of E.T. Jaynes’ work) got me to start work on
                  developing a toy universe theory in which I stripped out absolutely everything including pre-
                  conceived notions (e.g. how can we say an empty universe is Minkowski when there’s no way to
                  operationally verify this for such a universe since, by deﬁnition, there’s nothing in it to do the
                  veriﬁcation? And what about an empty universe with dif㘶erent physical laws?). In essence, I’m
                  trying to think like Jaynes a bit – don’t take information for granted that really isn’t there.
                  So, in short, I think there are unintended (hidden) assumptions and biases that go into our
                  deﬁnitions and it’s something we need to be more aware of.
                  Ian Durham Says:
                  Comment #109 October 1st, 2011 at 10:02 pm
                  One more thought. I think we tend to misunderstand entropy. First, we should be more careful
                  than  to  go  on  “intuition,”  á  là  Sean’s  milk  example  above.  If  we  did  that  we’d  all  be
                  Aristotelians. Second, I think it is more instructive to think of entropy as a measure of the
                  number of possible conﬁgurations of a system. In that context it makes complete sense to see
                  the early universe as low entropy and a sputtering soup of low-energy particles in the distant
                  future as having a high entropy. It’s just like having a box of Legos: if some of the Legos are
                  assembled, it reduces the number of things you can build with what’s left. But if they’re all
                  disassembled then you can build almost anything! That being said, I think the entropy of the
                  universe at that point will have reached some kind of maximum.
                  Charles H. Bennett Says:
                  Comment #110 October 4th, 2011 at 8:24 pm
                  On logical depth, its increase at intermediate times, and its relation to sophistication and other
                  complexity measures.
                  This  is  primarily  a  comment  the  logical  depth  of  ﬁnite  strings  and  its  relation  to  Scott’s
                  discussion  (original  post  and  comment  9)  of  the  cof㘶ee  cup  experiment.  Abram  Demski
                  (comment 47) made some similar points, and Dave Doty (comment 93) nicely summarizes the
                  theory of logical depth and sophistication for inﬁnite strings.
                  “Logical  depth”  (http://www.research.ibm.com/people/b/bennetc/utmx.ps)  formalizes  the
                  complexity of an object’s plausible causal history, as the time required for a deterministic
                  universal  computer  to  generate  (a  digitized  representation  of)  the  object  from  a  nearly
                  incompressible  algorithmic  description.  Requiring  the  description  to  be  incompressible  or
                  nearly so excludes descriptions that are computable from a still simpler descriptions, which
                  would violate Occam’s razor.
                  In Scott’s cof㘶ee cup example, the intermediate state has a visibly complex pattern at the milk-
                  cof㘶ee interface. This arises from a nontrivial physical process, whose simulation would require
                  a  nontrivial  computation.  I  think  the  state  illustrated  in  the  middle  picture  is  rather  more
                  complicated than could be reproduced by a discrete dif㘶usion process of randomly swapping
                  adjacent voxels, and likely involves convective turbulence as well as dif㘶usion. The interface is
                  logically  deep  because  any  near-incompressible  program  for  generating  it  would  need  to
                  approximate this physical evolution. The program might comprise a few bits describing the
                  relevant   dynamical  laws  (thermal  expansion,  heat  conduction,  ﬂuctuations,  and
                  hydrodynamics), a few more bits describing the simple initial condition (cold milk over hot
                  cof㘶ee), and many bits representing stochastic (thermal, chaotic, or quantum) inﬂuences on the
                  evolution, leading to features such as the precise location of the tendrils that would occur
                  dif㘶erently if the experiment were repeated. By contrast, the ﬁnal equilibrium state of the cof㘶ee
                  is  logically  shallow, despite its longer temporal history, because an alternative computation
                  could  short-circuit  the  system’s  actual  evolution  and  generate  the  structure  simply  by
                  calculating the thermodynamic equilibrium state under the prescribed boundary conditions. A
                  fast-running, near-incompressible program would su㘠陦ce to do so. Informally speaking, the
                  intermediate state is deep because it contains internal evidence of a complicated causal history,
                  while the ﬁnal state is shallow because such evidence has been obliterated by the equilibration
                  process.
                  Logical depth is a fundamentally dynamic measure, having units of time or machine cycles, in
                  contrast to minimal program size (Kolmogorov
                  complexity) which is measured in informational units of bits. Program size enters into the
                  deﬁnition of logical depth, but only in a secondary role, to quantify the unlikelihood that the
                  object originated more quickly than its logical depth (an object x is deﬁned to have logical
                  depth  d  with  b  bits  conﬁdence  if㘶  all  programs  to  compute  it  in  time  less  than  t  are
                  compressible by at least b bits).  By  contrast,  some  other  proposed  measures  of  nontrivial
                  complexity, such as “sophistication” and “computational depth” are informational quantities
                  like  Kolmogorov  complexity,  but  strive  to  measure  only  the  nontrivial  part  of  an  object’s
                  information content. Thus, like logical depth, they assign low complexity to random strings.
                  Sophistication  (Koppel  88)  does  so  by  considering  near-minimal-sized  descriptions  of  x
                  consisting of two parts, a “program” part p specifying a total function and a “data” part d giving
                  an input to this function. The minimal size of p such that pd is a near-minimal description of x
                  then  deﬁnes  the  object’s  sophistication,  thereby  formalizing  the  notion  of  the  information
                  content  of  the  simplest  computable  ensemble  within  which  x  is  a  typical.  Thus  deﬁned,
                  sophistication is not very useful at sub-busy-beaver levels of run time, where it remains O(1)
                  (to  see  this,  let  p  be  a  constant  program  specifying  a  truncated  version  of  the  universal
                  function, with run time bounded by some rapidly-growing computable function of d).
                  “Computational depth” (Antunes et al ’06, ’07) refers to a family of complexity measures that
                  combine time and program size into a single quantity by taking the dif㘶erence between an
                  object’s  time-penalized  and  plain  Kolmogorov  complexities.  With  a  logarithmic  penalty
                  function—the  usual  choice—computational  depth  often  corresponds  roughly  to  subjective
                  notions  of  complexity  at  lower  levels  of  complexity,  but  underestimates  the  complexity  of
                  deeper objects, especially those with a small amount of deeply buried redundancy. Conversely,
                  with  an  inverse  busy-beaver  penalty  function,  computational  depth  approximates
                  sophistication,  reﬂecting  the  equivalence  between  truth-table  reducibility  and  Turing
                  reducibility  in  recursively-bounded  time,  and  is  similarly  insensitive  to  merely  exponential
                  levels of subjective complexity.
                  I think these infelicities of sophistication and computational depth stem from the procrustean
                  attempt  to  combine  time-complexity  and  program  size  into  a  single  scalar  measure.  The
                  proper role of program size is as a certiﬁer of plausibility or signiﬁcance. If considerations of
                  program size justify a statistically signiﬁcant inference about an object’s causal history, then
                  the dynamic complexity associated with that inference can be conﬁdently imputed to the object
                  itself. That is the approach of logical depth.
                  Ken Miller Says:
                  Comment #111 October 7th, 2011 at 11:05 pm
                  Those interested in measures of complexity should have a look at papers of Bialek, Nemenman
                  and Tishby:
                  arXiv:physics/0103076
                  arXiv:physics/0007070
                  Scott Says:
                  Comment #112 October 7th, 2011 at 11:17 pm
                  Charles Bennett: Thanks so much for your wonderfully-informative comment, and sorry for the
                  delay in responding!
                  The issue you mentioned with Koppel’s sophistication measure—that it remains O(1) if we allow
                  arbitrary programs—is something I worried about as well. But as discussed in the post, I think
                  this  problem can be solved simply by restricting the minimization to “simple” or “e㘠陦cient”
                  programs, for almost any deﬁnition of “simple” or “e㘠陦cient” one likes. And crucially, restricting
                  attention  to  e㘠陦cient  programs  is  something  that  I’d  want  to  do  anyway,  for  at  least  two
                  separate reasons:
                  1. For me, an important requirement for any “complextropy” measure is that one be able to
                  write actual computer programs to (crudely) approximate it, in at least some cases of physical
                  interest. So the simpler the class of programs we’re dealing with, the better.
                  2. While this might reﬂect nothing more than complexity-theoretic prejudice, I worry about the
                  relevance  to  physics  of  any  running  time  greater  than  exponential  (or  certainly  doubly-
                  exponential). So for example, if an n-bit string x could be generated by a log(n)-bit program
                                  2^2^n
                  that  runs  in  2       time,  but  was  indistinguishable  from  random  by  any  o(n)-bit  program
                  running  in  less  time,  I’d  be  tempted  to  call  x  “incompressible  for  all  physically-relevant
                  purposes.”
                  Now, regarding the question of whether the logical depth becomes large at intermediate times
                  in the cof㘶ee cup example. Thanks so much for explaining your intuition for why it should
                  become large! My basic worry is the following: how do we know that there isn’t some equally-
                  compact program that will output the intermediate states in a short (near-linear) amount of
                  time? Such a program could conceivably work, not by simulating the whole causal history of the
                  cof㘶ee cup, but by some more direct method that ﬁrst “sketched” the milk tendrils and then
                  ﬁlled in more details as needed. Can you give me an argument why that sort of program would
                  require more hard-coded entropy than the sort of program you described?
                  Of course, you might point out that the same objection could be raised against any other
                  “complextropy”  measure!  For  example,  how  do  I  know  that  my  resource-bounded
                  sophistication  measure,  or  Sean  Carroll’s  measure  based  on  coarse-graining,  will  actually
                  become large at intermediate times in the cof㘶ee-cup system? But for my and Sean’s measures,
                  I  know  how  to  write  practical  computer  programs  that  give,  admittedly  not  the  measures
                  themselves, but certain crude approximations to them.
                  For  Sean’s  measure,  what  you  do  is  to  ﬁrst  “blur”  or  “smear”  your  bitmap,  then  feed  the
                  resulting image to some standard compression program (like gzip or pkzip) and measure the
                  size  of  the  compressed ﬁle. For the sophistication-based measure, what you do is to ﬁrst
                  compress your bitmap using some two-part code, then measure the size of the ﬁrst part of the
                  code only.
                  Over the past few weeks, my student Lauren Oullette has actually been coding up these crude
                  approximations, and seeing how they behave in a simulated cof㘶ee cup. So far, we’ve found
                  that  the  coarse-graining-based  measure  does  indeed  increase  and  then  decrease  as  Sean
                  sketched on his slide, though there are a few surprises that we’re still trying to understand
                  better. I’m looking forward to seeing what happens with the sophistication-based measure.
                  So, let me close with the following question for you: can you give us a “crude approximation” to
                  logical  depth  that  we  can  actually  calculate  in  practice,  analogous  to  the  “crude
                  approximations”  to  coarse-grained  entropy  and  resource-bounded  sophistication  that  I
                  sketched above? If so, we’ll be thrilled to code that up as well and compare it against the other
                  two!
                  Alexei Grinbaum Says:
                  Comment #113 October 25th, 2011 at 5:32 am
                  With regard to Kolmogorov complexity and entropy, note that there’s a whole development in
                  the theory of dynamical systems showing how to connect Kolmogorov complexity with Shannon
                  entropy. This goes all the way back to the 60s, when Zvonkin and Levin were trying to apply
                  Kolmogorov’s idea to various ﬁelds and see if they yield a better understanding of what was
                  going on in those ﬁelds. Then in the 70s a formal result was proved bu Brudno. Now, I believe
                  there’s an unrealized potential for applying Kolmogorov’s approach to system identiﬁcation in
                  quantum physics: how does the observer know what the system is that he’s going to measure,
                  and this problem of observer identity vs system identity actually makes use of the Zvonkin-
                  Levin  connection.  This  is  all  highly  speculative,  of  course,  but  I  thought  it  was  worth
                  mentioning in this discussion about entropy and Kolmogorov complexity.
                  Joshua Zelinsky Says:
                  Comment #114 October 31st, 2011 at 6:15 pm
                  Scott,
                  Since  you  made  the  mistake  of  mentioning  the  faster-than-light  neutrinos,  this  lets  me
                  mention a question that is actually closer to your line of work. You had shown if one has
                  closed-time  like  curves  then  in  that  framework  BQP=P=PSPACE.  But,  if  it  turned  out  that
                  neutrinos were tachyons this would not in practice let one do this since it would be really
                  di㘠陦cult to send more than a tiny number of bits back using neutrinos. This leads to a question
                  which I’m not sure how to make more precise, but essentially is: how restrictive can one be
                  about how many bits one is allowed to send back and still be able to solve PSPACE problems in
                  polynomial time? An obvious similar question is for problems in NP.
                  How natural this question is may be a function of what happens with the neutrinos. I agree that
                  the claim is probably incorrect.
                  Okie Says:
                  Comment #115 November 12th, 2011 at 5:06 am
                  “Why does “complexity” or “interestingness” of physical systems seem to increase with time and
                  then  hit  a  maximum  and  decrease,  in  contrast  to  the  entropy,  which  of  course  increases
                  monotonically?”
                  What an neat question!
                  What  are  some  other  examples  of  closed  physical  systems  that  evolve  on  this  sort  of
                  “interesting roller coaster of complexity”? The milk is a self-obsessed coagulate trying to keep
                  its  surface in tact. When a bit of the surface gets messed up, other milk moves in right in
                  behind it for a swirly, tendril dance party. It seems there’s a lot of thermodynamic potential to
                  drive complexity in the initial conditions…like hot and cold weather fronts about to form a
                  tornado…but with a more interesting boundary. I think the prevalence of so many complex
                  interfaces in our Universe between environments on every scale is astounding, and that’s a
                  source of a lot of turbulence. I’m having trouble putting to rest the thought that the pattern of
                  the cof㘶ee in the middle of mixing tickles our brain, sort of moving through the sweet-spot of
                  how we compress the pattern, massaging our mental models. It’s okay to think of the positions
                  of  the  milk  molecules  requiring  more  bits  to  specify  once  they  start  swirling?  Were  they
                  informed that there was a cover charge at the dance party? All because we can’t compress them
                  with as simple of a model on our computer? They’re saying don’t worry, the caf㘶eine hasn’t
                  given them the nervous shakes so much to forget where they are. Aren’t they just going to
                  dance  like  any  other  simple  program  that  produces  beautiful,  complex,  nested,  repetitive
                  structures before the heat gets to them and everything turns into an orgy?
                  Have you ever left a cup of cof㘶ee with cream on your desk for a day and a half? The patterns
                  that the cream forms on the surface are great!
                  Erik Wennstrom Says:
                  Comment #116 December 7th, 2011 at 2:47 pm
                  I’m a bit late with this comment, but I just noticed something. You deﬁned K(S) as “the number
                  of bits in the shortest computer program that outputs the elements of S and then halts”, but
                  elsewhere (in particular, in the paper you linked from Gács, Tromp, and Vitányi), the equivalent
                  notion is deﬁned not in terms of a program that outputs members of S, but in terms of a
                  program that decides S. Was that intentional, and if so, why make that distinction? Could you
                  use the decision version to deﬁne a notion of complextropy along the same lines of what you
                  did, and would it dif㘶er in any meaningful way?
                  Can Neutrinos Kill Their Own Grandfathers? | Cosmic Variance « Science
                  Technology Informer Says:
                  Comment #117 December 30th, 2011 at 2:28 am
                  […] to Favorites Building in part on my talk at the time conference, Scott Aaronson has a blog
                  post about entropy and complexity that you should go read right now. It’s similar to one I’ve
                  been contemplating myself, […]
                  AMS Says:
                  Comment #118 December 30th, 2011 at 1:11 pm
                  @116:
                  It seems like he meant to say “decides”, since he later refers to an oracle for membership in S.
                  Not sure.
                  @Scott: You mention using gzip to estimate K(), but it fails badly in a speciﬁc case:
                  –Take a batch of random numbers or other high-entropy ﬁle of length N
                  –Concatenate it with itself
                  –Run through gzip
                  The result was ~2N when I tried it, rather than ~N (give or take a constant factor) as you would
                  expect for the Kolmogorov complexity. It seems that gzip doesn’t handle large-scale patterns
                  in the data well, which could be relevant to complexity / complextropy research. I can guess
                  why not (memory constraints, probably) but it still seems problematic. Does LZMA or another
                  more sophisticated algorithm handle this better?
                  What exactly [i]is[/i] information, anyway? « Quantum Moxie Says:
                  Comment #119 February 12th, 2012 at 8:12 pm
                  […]  by  Sean  Carroll's  FQXi  presentation  and  has  been  subsequently  discussed  by  Scott
                  Aaronson (see here and here) and Charlie […]
                  What increases when a self-organizing system organizes itself? Logical
                  depth to the rescue. | The Quantum Pontif㘶 Says:
                  Comment #120 February 25th, 2012 at 5:58 pm
                  […] content, as opposed to its mere randomness.  A recent example is Scott Aaronson’s notion
                  of complextropy, deﬁned roughly as the number of bits in the smallest program for a universal
                  computer to […]
                  How Good Is Your I-magination? « Gödel’s Lost Letter and P=NP Says:
                  Comment #121 March 16th, 2012 at 10:44 pm
                  […]  with  Scott,  is  there  any  relation  to  his  “complexodynamics”  framework  here?  We
                  congratulate him and Robert Wood on winning the Alan T. Waterman Award. See also this
                  recent […]
                  Theoretical  Physics:  Why  does  the  complexity  ﬁrst  increase  and  then
                  decrease when entropy increases? And at what point does the complexity
                  change from increasing to decreasing? - Quora Says:
                  Comment #122 June 19th, 2012 at 6:57 am
                  […] complexity is desirable, but what I think you have in mind is somewhat related to this blog
                  entry:http://www.scottaaronson.com/blo…Comment Loading… • Post • 4:57am  Add […]
                  Hector Zenil Says:
                  Comment #123 July 25th, 2012 at 4:25 pm
                  Scott (and Charles (Bennett)),
                  I think your concern about the applicability of complexity measures is legitimate. Myself and
                  some colleagues of mine have been trying to use and evaluate information theoretic measures
                  such as Kolmogorov Complexity and Logical Depth in real-world situations, in your words
                  “crude approximations” of at least the “simplest versions”.
                  We recently published a paper (in the journal of Complexity) showing how Logical Depth can be
                  successfully (with care) applied (the greatest challenge was the instability of timing functions in
                  modern computers).
                  You may want to have a look at the paper (Image Characterization and Classiﬁcation by Physical
                  Complexity) available online at:
                  http://arxiv.org/abs/1006.0051
                  (we used the term “physical complexity” from Charles Bennett’s own suggestion in his original
                  paper).
                  The  results  are  quite  interesting,  ﬁrst  it  is  shown  that  Logical  Depth  (LD)  does  measure
                  dif㘶erent  properties  compared  to  Kolmogorov  Complexity  (K)  alone  (that  is  LD  assigns
                  intuitively simple and random objects to the lowest complexity values, unlike K that assigns
                  random objects the greatest complexity).  Second,  we  also  show  that  LD  seems  to  classify
                  images  containing  objects  of  dif㘶erent  apparent  complexities  in  a  reasonable  way,  one  in
                  agreement with our intuition of what is complex versus random and simple.
                  Sincerely,
                  Hector Zenil
                  P.s.  We  were  also  tackling  the  question  you  made about the rate of complex behaviour in
                  increasingly  larger  rule  spaces  of  cellular  automata,  I  will  keep  you  updated  if  you  are
                  interested.
                  gwern Says:
                  Comment #124 September 18th, 2012 at 11:52 am
                  AMS:
                  > You mention using gzip to estimate K(), but it fails badly in a speciﬁc case: 1. Take a batch of
                  random numbers or other high-entropy ﬁle of length N 2. Concatenate it with itself 3. Run
                  through gzip.
                  > The result was ~2N when I tried it, rather than ~N…I can guess why not (memory constraints,
                  probably) but it still seems problematic. Does LZMA or another more sophisticated algorithm
                  handle this better?
                  Yeah, I’d guess look-ahead is the main limit. But you can easily see for yourself using 7ZIP,
                  which while not *perfect* does do a lot better than gzip apparently does. Here’s a script and
                  output, and we’ll run it on ﬁle size from hundred kb to scores of mb range:
                  $ for i in {1..5000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt > test2.txt &&
                  7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z -m0=lzma -
                  mx=9  -mfb=64  -ms=on  test2.txt.7z  test2.txt  &&  du  -ch  test.txt  test.txt.7z  test2.txt
                  test2.txt.7z; rm test*.txt*
                  …
                  224Ktest.txt
                  16Ktest.txt.7z
                  448Ktest2.txt
                  16Ktest2.txt.7z
                  704Ktotal
                  16K and 16K. Fantastic. Let’s get bigger:
                  $ for i in {1..500000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt > test2.txt &&
                  7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z -m0=lzma -
                  mx=9  -mfb=64  -ms=on  test2.txt.7z  test2.txt  &&  du  -ch  test.txt  test.txt.7z  test2.txt
                  test2.txt.7z; rm test*.txt*
                  …
                  23Mtest.txt
                  1.2M test.txt.7z
                  46Mtest2.txt
                  1.3M test2.txt.7z
                  71Mtotal
                  1.2M and 1.3M. Good, but not perfect.
                  $ for i in {1..1000000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt > test2.txt
                  && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z -m0=lzma
                  -mx=9  -mfb=64  -ms=on  test2.txt.7z  test2.txt  &&  du  -ch  test.txt  test.txt.7z  test2.txt
                  test2.txt.7z; rm test*.txt*
                  …
                  47Mtest.txt
                  2.4M test.txt.7z
                  93Mtest2.txt
                  2.4M test2.txt.7z
                  144Mtotal
                  Note that the 2 compressed ﬁles are very close – 2.4M and 2.4M. Great.
                  But  we  *can*  blow  the  lookback  window  if  we  keep  increasing  the  ﬁle  size!  Here  is  what
                  happens if we double the ﬁle sizes yet again:
                  $ for i in {1..2000000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt > test2.txt
                  && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z -m0=lzma
                  -mx=9  -mfb=64  -ms=on  test2.txt.7z  test2.txt  &&  du  -ch  test.txt  test.txt.7z  test2.txt
                  test2.txt.7z; rm test*.txt*
                  …
                  93Mtest.txt
                  4.8M test.txt.7z
                  186Mtest2.txt
                  9.5M test2.txt.7z
                  293Mtotal
                  Whups! Now test.txt.7z is 4.8M and test2.txt.7z is… 9.5M. A little less than twice as large.
                  —————————————————————————–
                  Scott:
                  > In the interest of a full disclosure, a wonderful MIT undergrad, Lauren Oullette, recently
                  started a research project with me where she’s trying to do exactly that. So hopefully, by the
                  end of the semester, we’ll be able to answer Sean’s question at least at a physics level of rigor!
                  Answering the question at a math/CS level of rigor could take a while longer.
                  I’ve been following the RSS feed for a long time, but I don’t recall any followup. How did the
                  research go?
                  Ping Conference, Play Edition, Day 1 - Marius Soutier NoDev - Not Only
                  Software Development Says:
                  Comment #125 January 20th, 2014 at 3:57 pm
                  […] an intro to entropy on Wikipedia, and also see this blog post that they referred to in their
                  […]
                  Aschendorf㘶 Verlag Mnster Jobs Says:
                  Comment #126 September 29th, 2016 at 3:27 pm
                  […] The First Law of Complexodynamics – Unfortunately, as we deﬁned it above, sophistication
                  still doesn’t do the job. For deterministic systems, the problem is the same as the one pointed
                  out earlier for Kolmogorov complexity: we can always describe the system’s state after t time …
                  […]
                  Can Neutrinos Kill Their Own Grandfathers? | Sean Carroll Says:
                  Comment #127 November 24th, 2016 at 12:47 pm
                  […] in part on my talk at the time conference, Scott Aaronson has a blog post about entropy
                  and complexity that you should go read right now. It’s similar to one I’ve been contemplating
                  myself, […]
                                             Shtetl-Optimized is proudly powered by WordPress
                                                     Entries (RSS) and Comments (RSS).
