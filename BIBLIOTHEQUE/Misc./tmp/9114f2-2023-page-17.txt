through the task. We used the results of this continuous quality control to
remove labelers whose quality slipped too far, as well as to prepare educational
material on common mistakes in order to improve labeler alignment with our
instructions.

C_ Evaluation

As we scaled up the project, we began having to collect labels on multiple
solutions for the same training problem. In order to avoid the risk of over-fitting
on the 7,500 MATH training problems, we expanded the training set to include
4,500 MATH test split problems. We therefore evaluate our models only on the
remaining 500 held-out problems. We selected these 500 test problems uniformly
at random. In Figure 5, we show that the distribution of difficulty levels and
subjects in this subset is representative of the MATH test set as a whole. The
specific test set we used can be found at https://github.com/openai/prm800k.
We leave it for future work to explore how many distinct training problems are
actually necessary, and how quickly our methods overfit to the training set.

MATH difficulty level distribution MATH subject distribution

mm our test
0.25 | mam MATH test
0.28
0.20
020
oas
os
p20 oo
. I .
oo |, \ : j j j
1 2 3 > 6 a

Level s Fy

HE our test

ws

SAE &

< &
subject

Figure 5: Two histograms comparing the distribution of problem difficulty levels
and subjects in both the original MATH test set and in our 500 problem test
subset.

18
