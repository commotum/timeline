                      SupplementaryMaterials for
               “Test-Time Learning for Large Language Models”
      In the Supplementary, we provide descriptions of more related works, details, and experimental results of the proposed TLM.
      Weorganize the supplementary into the following sections.
       • In Section A, we provide descriptions of related works regarding Large Language Models, Retrieval-Augmented
        Generation, and Test-Time Adaptation.
       • In Section B, we present a more detailed version of our AdaptEval Benchmark.
       • In Section C, we present a more detailed implementation of our experiments.
       • In Section D, we report more results of our experiments.
       • In Section E, we provide the discussion of TLM and future directions.
      A. MoreRelatedWork
      A.1. Large Language Models
      The rapid progress in natural language processing (NLP) has been marked by the emergence of large language models
      (LLMs), which have fundamentally transformed the landscape of artificial intelligence. These models, rooted in the
      Transformer architecture (Vaswani et al., 2017), leverage extensive pre-training on massive text corpora to acqire remarkable
      capabilities. They have shown impressive performance across a wide range of tasks (Wei et al., 2022a; Hu et al., 2025b),
      such as high-quality question answering (Shao et al., 2023; Peng et al., 2023), coding (Chen et al., 2021), and intermediate
      reasoning (Wei et al., 2022b). The unprecedented success of LLMs has spurred significant discussions regarding their
      application for achieving artificial general intelligence (AGI) (Zhao et al., 2023). Based on their architectural design, existing
      LLMscanbecategorizedinto three major classes: encoder-only models, decoder-only models, and encoder-decoder models.
      Encoder-only models. The encoder-only models primarily employ the Transformer encoder to encode input sequences
      into rich contextual representations. They are particularly effective in natural language understanding (NLU) tasks, where
      the focus lies in extracting semantic meaning from text. One notable example is BERT (Devlin et al., 2019), which uses
      bidirectional encoding to capture context from both preceding and succeeding tokens. Pre-trained on extensive datasets
      like BooksCorpus (Zhu et al., 2015) (800M words) and English Wikipedia (2,500M words), BERT set new benchmarks on
      datasets such as GLUE and MultiNLI. Subsequent iterations, including RoBERTa (Liu, 2019) and DeBERTa (He et al.,
      2021), introduced architectural refinements and improved pre-training strategies, further enhancing performance. Despite
      their strengths in understanding tasks, encoder-only models are inherently unsuited for tasks that require sequence generation,
      such as translation or text completion.
      Decoder-only models. This kind of models, in contrast, rely solely on the Transformer decoder and are designed to generate
      text in an auto-regressive manner, where each token is generated sequentially, conditioned on previously generated tokens.
      Obviously These models excel in natural language generation (NLG) tasks, such as summarization, content creation and QA.
      TheGenerative Pre-trained Transformer (GPT) series (Radford, 2018; Brown et al., 2020; Achiam et al., 2023) developed
      by OpenAI examines this class, with GPT-3 being a landmark model that features 175 billion parameters. Trained on a
      diverse corpus spanning Common Crawl (Raffel et al., 2020), WebText2, Books 1, Books 2, and Wikipedia datasets. GPT-3
      has demonstrated extraordinary few-shot and zero-shot learning capabilities on many language tasks. In addition to GPT
      series, many decoder-only models have been developed, such as OPT, LLaMA, Llama2, Llama3 from Meta (Zhang et al.,
      2022b; Touvron et al., 2023a;b; Dubey et al., 2024), PaLM, PaLM2 from Google (Chowdhery et al., 2023; Anil et al.,
      2023), BLOOMfromBigScience(LeScaoetal., 2023), and Qwen series from Alibaba (Bai et al., 2023; Yang et al., 2024).
                              14
