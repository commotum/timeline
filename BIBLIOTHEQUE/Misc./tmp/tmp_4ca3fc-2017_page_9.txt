                              Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
                              model. All metrics are on the English-to-German translation development set, newstest2013. Listed
                              perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to
                              per-word perplexities.
                                         N d             d       h     d       d      P              train     PPL     BLEU params
                                                model      ff            k      v      drop     ls                                      6
                                                                                                      steps    (dev)     (dev)      √ó10
                                base     6      512     2048     8     64      64      0.1     0.1    100K      4.92     25.8        65
                                                                 1     512    512                               5.29     24.9
                                 (A)                             4     128    128                               5.00     25.5
                                                                 16    32      32                               4.91     25.8
                                                                 32    16      16                               5.01     25.4
                                 (B)                                   16                                       5.16     25.1        58
                                                                       32                                       5.01     25.4        60
                                         2                                                                      6.11     23.7        36
                                         4                                                                      5.19     25.3        50
                                         8                                                                      4.88     25.5        80
                                 (C)            256                    32      32                               5.75     24.5        28
                                               1024                    128    128                               4.66     26.0        168
                                                        1024                                                    5.12     25.4        53
                                                        4096                                                    4.75     26.2        90
                                                                                       0.0                      5.77     24.6
                                 (D)                                                   0.2                      4.95     25.5
                                                                                               0.0              4.67     25.3
                                                                                               0.2              5.47     25.7
                                 (E)              positional embedding instead of sinusoids                     4.92     25.7
                                 big     6     1024     4096     16                    0.3            300K      4.33     26.4        213
                              In Table 3 rows (B), we observe that reducing the attention key size d hurts model quality. This
                                                                                                            k
                              suggests that determining compatibility is not easy and that a more sophisticated compatibility
                              function than dot product may be beneÔ¨Åcial. We further observe in rows (C) and (D) that, as expected,
                              biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-Ô¨Åtting. Inrow(E)wereplaceour
                              sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical
                              results to the base model.
                              7    Conclusion
                              In this work, we presented the Transformer, the Ô¨Årst sequence transduction model based entirely on
                              attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with
                              multi-headed self-attention.
                              For translation tasks, the Transformer can be trained signiÔ¨Åcantly faster than architectures based
                              on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014
                              English-to-French translation tasks, we achieve a new state of the art. In the former task our best
                              model outperforms even all previously reported ensembles.
                              Weareexcited about the future of attention-based models and plan to apply them to other tasks. We
                              plan to extend the Transformer to problems involving input and output modalities other than text and
                              to investigate local, restricted attention mechanisms to efÔ¨Åciently handle large inputs and outputs
                              such as images, audio and video. Making generation less sequential is another research goals of ours.
                              The code we used to train and evaluate our models is available at https://github.com/
                              tensorflow/tensor2tensor.
                              Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
                              comments, corrections and inspiration.
                                                                                    9
