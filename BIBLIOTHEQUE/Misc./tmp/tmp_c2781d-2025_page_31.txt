                      Preprint, Under Review.
                      C.3  RLCOSTPENALTYABLATION
                      Weexperimented with different penalties on the cost of planning (Ctokens) to analyze how agents
                      adapt their behavior to computational constraints. As illustrated in Figure 10, the addition of these
                      penalties led agents to reduce their planning frequency and plan length over time; however, we
                      observed that the normalized score was largely unaffected regardless of penalty level. This is in
                      contrast to our main findings, where explicit planning consistently enhances agent performance.
                      This divergence suggests that, after sufficient training, agents may increasingly internalize planning
                      behaviors; as they gain proficiency in the environment, much of the reasoning and strategy required
                      for success can become implicit within the policy, reducing reliance on overt, explicit planning actions.
                      This could help explain why further penalizing explicit plan generation produced only limited effects
                      onfinal scores, even though explicit planning is crucial during earlier stages.
                      Figure 10: Comparison of training with different planning cost penalties. We compare the (left)
                      Normalized Score, (center) Planning Frequency, and (right) Plan Length for agents trained with no
                      penalty, a small penalty (-0.001 per token), and a big penalty (-0.005 per token).
                                                             31
