                             TaskinBBEH↓/Accuracyon→ OldtaskfromBBH NewtaskinBBEH
                                       BoardgameQA                          88.0                  42.5
                                    Boolean Expressions                     97.6                  27.0
                                   Causal Understanding                     65.2                  52.0
                                    DisambiguationQA                        42.0                  48.3
                                      DyckLanguages                         65.2                  14.0
                                     Geometric Shapes                       73.6                  35.0
                                        Hyperbaton                          94.8                   4.5
                                       SARCTriples                          86.0                  37.5
                                          Linguini                          62.8                  15.5
                                  MovieRecommendation                       66.4                  59.5
                                   Multistep Arithmetic                     99.6                   9.5
                                           NYCC                             81.2                  11.0
                                     Object Properties                      96.8                   1.5
                                      Object Counting                       97.6                  11.0
                                      Shuffled Objects                     100.0                   9.0
                                     Spatial Reasoning                      97.6                  18.5
                                          SportQA                           89.6                  23.0
                                        BuggyTables                         98.6                   3.5
                                    TemporalSequences                       98.8                   0.5
                                      TimeArithmetic                        92.0                  48.0
                                        WebofLies                           94.8                  18.5
                                       WordSorting                          84.8                  26.0
                                       ZebraPuzzles                         87.6                  44.5
                                           BBEH                             85.2                  23.90
                             Table 9: Performance of Gemini 2.0 Flash on BBEH and its counterpart task from BBH.
                 is not considered. To address this limitation, sev-    E Potential Risks
                 eral benchmarks have been developed to integrate       Our benchmark contains questions that contain
                 multiple tasks into a single evaluation framework,     long contexts and require a great amount of think-
                 including (Wang, 2018; Wang et al., 2019; Weston       ing, reflected in the number of output tokens that
                 et al., 2015; Lu et al., 2023; Kazemi et al., 2024;    the models have to generate to solve them. As
                 Hendrycks et al., 2020; Wang et al., 2024; Parmar      such, evaluating models on our benchmark may
                 et al., 2024; Srivastava et al., 2022). Our work       require a higher amount of computation and energy
                 builds on this line of research, introducing a new     compared to some other benchmarks.
                 set of challenging tasks for future model evaluation
                 and performance improvement. The multi-task na-
                 ture of our benchmark with fine-grained tasks each
                 focused on some reasoning skills enables model
                 developers to discover and analyze failure modes
                 in further depth. Note that while private initiatives
                 such as ChatBot Arena (lmarena) and the SEAL
                 leaderboard (ScaleAI) conduct model evaluations
                 acrossvariousaspects,theymaysufferfromseveral
                 potential issues as pointed out in (Bansal and Maini,
                 2024). Our benchmark provides an open evalua-
                 tion frameworkwithanautomaticanddeterministic
                 scoring mechanism, ensuring full transparency and
                 reproducibility for the broader research commu-
                 nity.
                                                                   26501
