                  Jifan Zhang, Lalit Jain, Yang Guo, Jiayi Chen, KuanLok        • Movie Recommendation: We programmati-
                     Zhou, Siddharth Suresh, Andrew Wagenmaker,                   cally updated the options for each question
                     Scott Sievert, Timothy Rogers, Kevin Jamieson,               to turn the problem from a “recommendation”
                     Robert Mankoff, and Robert Nowak. 2024. Hu-                  task to a “best subset selection” task.
                     morinai: Massive scale crowd-sourced preferences
                     and benchmarks for cartoon captioning. Preprint,           • Spatial Reasoning (the SpatialLLMEval sub-
                     arXiv:2406.10522.
                                                                                  set): We programmatically sampled from the
                  A DetailedDescription of the Tasks and                          harder subtasks of the dataset.
                       Task-Specific Insights from                             Category 2: Re-running the code that gener-
                       Experiments                                           ates an existing dataset, but changing the origi-
                  weclassifythe23tasksinBBEHintothefollowing                 nal parameters: These are the tasks that already
                  four categories.                                           existed in the literature and they were generated
                     Category1: Apost-processedversionofanal-                using code. The code contained parameters that
                  ready existing dataset: These are the tasks where          could be modified to create more difficult versions
                  wetookanalreadyexisting dataset from the liter-            of them. The tasks in this category include:
                  ature and did some post-processing to make them               • BoardgameQA:Weonlyincreasedthedepth
                  conform to the format of the tasks in BBEH. The                 parameter. Zebra Puzzles: We only increased
                  tasks in this category include:                                 the puzzle size and
                     • SARCTriples: We did some length filtering                • distractor parameters.
                        andcombinedthreeproblemsintoone. Every-                Category 3: Creating new tasks through
                        thing was done programmatically.                     writing code that generates the tasks: These
                     • SportQA: We took a subset of examples from            are the tasks that did not previously exist. We
                        the most difficult subset. Everything was done       wrote code that could be executed and created in-
                        programmatically.                                    stances of them. The tasks in this category include:
                     • Linguini: The original questions asked about          Boolean Expressions, Buggy Tables, Geometric
                        multiple things; we slightly modified them           Shapes, Hyperbaton, Multi-Step Arithmetic, Ob-
                        so each question asks about a single thing.          ject Counting, Object Properties, Shuffled Objects,
                        Everything was done programmatically.                Spatial Reasoning(excepttheSpatialLLMEvalsub-
                                                                             set), Temporal Sequences, Web of lies (except the
                     • Causal Understanding: We did some manual              LiveBench subset), and Word Sorting (except the
                        cleaning for all the examples in this task as        error finding subset).
                        described in detail in the Appendix.                   Category 4: Creating new tasks manually:
                                                                             This subset only includes the DisambiguationQA
                     • NYCC:Wetookanexistingbinaryclassifica-                task, which was created manually by the authors.
                        tion dataset and turned it into multiple-choice        In what follows, we describe in detail how each
                        questions. Everything was done programmati-          of the 23 new tasks in BBEH have been created.
                        cally.                                               Moreover, we provide interesting task-specific in-
                     • Time Arithmetic: We took an existing dataset          sights from our experiments.
                        and combined multiple questions into one to          A.1   BoardgameQA
                        create a compositional version of it. Every-         BoardgameQA(Kazemietal.,2023b)isabench-
                        thing was done programmatically.                     markwheregivenadefeasibletheory(asetofinput
                     • DyckLanguagesWordSorting(theerror de-                 facts, possibly contradictory rules, and preferences
                        tection subset): We took a subset of the exist-      overtherules), and a question about that theory, the
                        ing dataset that involved more reasoning steps.      task is to do multi-hop reasoning and conflict reso-
                        Everything was done programmatically.                lution over the input theory to answer the question.
                                                                             Thefinal answer to the question is either ‘proved‘
                     • Web of lies (the LiveBench subset): Part of           (if the statement in the question derives from the
                        the data comes from LiveBench with minimal           theory), ‘disproved‘ (if the negation of the state-
                        programmatic post-processing.                        ment in the question derives from the theory), or
                                                                       26485
