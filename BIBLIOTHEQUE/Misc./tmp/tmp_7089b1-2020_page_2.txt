       Ourformulation of learning as program induction traces back to the earliest days of AI (2): We
      treat learning a new task as search for a program that solves it, or which has intended behavior. Fig. 1
      shows examples of program induction tasks in eight different domains that DreamCoder is applied to
      (Fig. 1A), along with an in-depth illustration of one task in the classic list-processing domain: learning
      a program that sorts lists of numbers (Fig. 1B), given a handful of input-output examples. Relative
      to purely statistical approaches, viewing learning as program induction brings certain advantages.
      Symbolic programs exhibit strong generalization properties–intuitively, they tend to extrapolate rather
      than merely interpolate. This also makes learning very sample-efﬁcient: Just a few examples are
      often sufﬁcient to specify any one function to be learned. By design, programs are richly human-
      interpretable: They subsume our standard modeling languages from science and engineering, and
      they expose knowledge that can be reused and composed to solve increasingly complex tasks. Finally,
      programs are universal: in principle, any Turing-complete language can represent solutions to the full
      range of computational problems solvable by intelligence.
       Yet for all these strengths, and successful applications in a number of domains (3–9), program
      inductionhashadrelativelylimitedimpactinAI.ABayesianformulationhelpstoclarifythechallenges,
      as well as a path to solving them. The programming language we search in speciﬁes the hypothesis
      space and prior for learning; the shorter a program is in that language, the higher its prior probability.
      While any general programming language can support program induction, previous systems have
      typically found it essential to start with a carefully engineered domain-speciﬁc language (DSL), which
      imparts a strong, hand-tuned inductive bias or prior. Without a DSL the programs to be discovered
      would be prohibitively long (low prior probability), and too hard to discover in reasonable search
      times. Even with a carefully tuned prior, though, search for the best program has almost always been
      intractable for general-purpose algorithms, because of the combinatorial nature of the search space.
      Hencemostpractical applications of program induction require not only a hand-designed DSL but also
      a search algorithm hand-designed to exploit that DSL for fast inference. Both these requirements limit
      the scalability and broad applicability of program induction.
       DreamCoderaddresses both of these bottlenecks by learning to compactly represent and efﬁciently
      induce programs in a given domain. The system learns to learn – to write better programs, and to
      search for them more efﬁciently – by jointly growing two distinct kinds of domain expertise: (1)
      explicit declarative knowledge, in the form of a learned domain-speciﬁc language, capturing conceptual
      abstractions common across tasks in a domain; and (2) implicit procedural knowledge, in the form of a
      neural network that guides how to use the learned language to solve new tasks, embodied by a learned
      domain-speciﬁc search strategy. In Bayesian terms, the system learns both a prior on programs, and an
      inference algorithm (parameterized by a neural network) to efﬁciently approximate the posterior on
      programs conditioned on observed task data.
       DreamCoder learns both these ingredients in a self-supervised, bootstrapping fashion, growing
      themjointly across repeated encounters with a set of training tasks. This allows learning to scale to new
      domains, and to scale within a domain provided it receives sufﬁciently varied training tasks. Typically
      only a moderate number of tasks sufﬁces to bootstrap learning in a new domain. For example, the list
      sorting function in Fig. 1B represents one of 109 tasks that the system cycles through, learning as it
      goes to construct a library of around 20 basic operations for lists of numbers which in turn become
      components for solving many new tasks it will encounter.
       DreamCoder’s learned languages take the form of multilayered hierarchies of abstraction (Fig. 1B,
      &Fig.7A,B).Thesehierarchiesarereminiscentoftheinternalrepresentationsinadeepneuralnetwork,
      but here each layer is built from symbolic code deﬁned in terms of earlier code layers, making the
      representations naturally interpretable and explainable by humans. The network of abstractions grows
                          2
