                Pan et al.                                                                                                                         9
                historical data. This approach ensures continuity and pro-          For each TP metric, we calculate the mTP metric across all
                vides a baseline for comparison in the absence of prior frame       classes, and all TP metrics are calculated using a center dis-
                information.                                                        tance of d =2 m for counting NDS scores in equation (13)
                                                                                                           mTP= 1 XTP                           ð12Þ
                Experiments                                                                                         C         c
                                                                                                                   jj
                Metrics                                                                                                c2C
                                                                                                    "#
                The nuScenes 3D detection benchmark data set comprises                   NDS= 1 5mAP+ X ðÞ1minðÞ1,mTP                          ð13Þ
                1000videosegments, each 20 secondsin length, featuring key-                      10             mTP2TP
                frames sampled at a 2-Hz rate. Each keyframe captures a 360
                panoramic view through the integration of six cameras. The
                data set is segmented into 700 videos for training, 150 for vali-          Higher values indicate better performance for NDS
                dation, and another 150 for testing, spanning 10 categories,                and mAP. NDS assesses object detection models by
                and encompassing approximately 14 million annotated 3D                      combining accuracy and recall, reflecting how well-
                detection boxes. For assessing model performance, we rely on                detected objects match with actual ones. mAP, a key
                the nuScenes data set’s supported unified metrics.                          metric in object detection, calculates the area under
                    The evaluation metric provided by the nuScenes data set                 the precision-recall curve to represent the system’s
                still uses the commonly used AP (Average Precision) from                    average precision across different recall levels.
                object detection. However, instead of using Intersection over              Conversely, lower values are preferable for metrics
                Union (IoU) for threshold matching, it uses the 2D center                   such as ATE, ASE, AOE, AVE, and AAE, which
                distance d on the ground plane. This approach decouples the                 evaluate errors in vehicle displacement, scale, orienta-
                impact of object size and orientation on the AP calculation                 tion, speed, and acceleration estimations compared to
                                             1   XX                                         actual measurements, respectively.
                                  mAP=                   AP                  ð11Þ
                                           jjC jjD           c,d
                                                 c2C d2D                                In the realm of 3D object detection, NDS and mAP serve
                    In the equation, C represents the categories of object detec-   as the primary metrics for evaluation, whereas ATE and AOE
                tion, and D denotes the difficulty weight parameters for pre-       shed light on a model’s accuracy in map segmentation.
                dicting different object categories and distances.
                    In addition to mAP, nuScenes also introduces another            Environment settings and baseline
                metric called NDS, which is calculated using the true positive
                (TP) metric. NDS is half based on detection performance             The experiments were carried out on V100 GPUs, setting the
                (mAP) and the other half on detection quality, which is mea-        default number of training epochs to 30 and a learning rate (lr)
                sured by position, size, orientation, attributes, and velocity      of 23104. Drawing from the insights of previous projects
                (ATE,ASE,AOE,AVE,AAE):                                              (Park et al., 2021; Wang et al., 2021a, 2021b), we opted for
                                                                                    ResNet101-DCN, initialized from FCOS3D checkpoints, and
                       mATE (Average Translation Error): The Average               VoVnet99, which initiated from DD3D checkpoints, as our
                        Translation Error (ATE) is the 2D Euclidean center          backbone networks. The experiments employed multi-scale fea-
                        distance measured in meters.                                tures created by Feature Pyramid Networks (FPN), which
                       mASE (Average Scale Error): The Average Scale               downscaled the backbone-derived image features to 1 , 1,and
                                                                                                                                          16  32
                        Error (ASE) is calculated as 1  IoU, where IoU is the       1 of their original sizes across various configurations. Before
                                                                                    64
                        Intersection over Union after aligning the angles in        entering TSA, the feature dimension (C) was set to 256.
                        3Dspace.                                                        OnthenuScenesdataset, BEVqueries were defaulted to a
                       mAOE (Average Orientation Error): The Average               resolution of 200 3 200. The data set defines the vehicle’s
                        Orientation Error (AOE) is the smallest yaw angle dif-      perception range from the coordinate system’s origin, span-
                        ference between the predicted and ground truth val-         ning the X and Y axes from [251.2 m to 51.2 m] and the Z-
                        ues. The angle deviations for all categories are within     axis from [25 m to 3 m]. Each BEV grid’s resolution (s)
                        360, except for the barrier category, where the angle      matches a real-world square region with sides of 0.512 m.
                        deviations are within 180.                                 Within the RSCA, every BEV query sampled N           =4anchor
                                                                                                                                      ref
                       mAVE (Average Velocity Error): The Average                  points evenly distributed in the real-world 3D space within
                        Velocity Error (AVE) is the L2 norm of the 2D velo-         [25 m, 3 m]. Each anchor point was mapped to its corre-
                        city difference, measured in meters per second (m/s).       sponding 2D image feature (Vhit) by sampling four surround-
                       mAAE (Average Attribute Error): The Average                 ing reference points.
                        Attribute Error (AAE) is defined as 1  acc, where acc
                        is the classification accuracy for different categories.
                                                                                    Benchmark comparison. To effectively assess the RetentiveBEV
                    For the metrics above, we calculate the mean true positive      neck network’s performance, we included VPN, Lift-Splat, and
                (mTP) across all categories. In equation (12), c represents the     BEVFormerasbenchmarks,applyingthe same head network for
                categories for the corresponding metrics mentioned above.           tasks of BEV detection and segmentation.
