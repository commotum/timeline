                    contains a grounded set of atoms specifying the
                    particular kinship relations that underlie a single
                    story. In other words, R contains the logical rules
                    that govern all the generated stories in CLUTRR,
                    while G contains the grounded facts that underlie a
                    speciﬁc story.
                    Graphgeneration. To generate the kinship graph
                    Gunderlying a particular story, we ﬁrst sample a
                    set of gendered2 entities and kinship relations using
                    a stochastic generation process. This generation
                    process contains a number of tunable parameters—               Figure 3: Illustration of how a set of facts can split and
                    such as the maximum number of children at each                 combined in various ways across sentences.
                    node, the probability of an entity being married to            tation to natural language through crowd-sourcing.
                    another entity, etc.—and is designed to produce a              Paraphrasing using Amazon Mechanical Turk.
                    valid, but possibly incomplete “backbone graph”.               Thebasicideabehindourapproachisthatweshow
                    For instance, this backbone graph generation pro-              AmazonMechanicalTurk(AMT)crowd-workers
                    cesswillspecify“parent”/“child”relationsbetween                the set of facts B    corresponding to a story and ask
                    entities but does not add “grandparent” relations.                                C
                    After this initial generation process, we recursively          the workers to paraphrase these facts into a narra-
                    apply the logical rules in R to the backbone graph             tive. Since workers are given a set of facts BC to
                    to produce a ﬁnal graph G that contains the full set           workfrom, they are able to combine and split mul-
                    of kinship relations between all the entities.                 tiple facts across separate sentences and construct
                    Backward chaining. The resulting graph G pro-                  diverse narratives (Figure 3). Appendix 1.6 con-
                    videsthebackgroundknowledgeforaspeciﬁcstory,                   tains further details on our AMT interface (based
                    as each edge in this graph can be treated as a                 ontheParlAIframework(Milleretal.,2017)), data
                    grounded predicate (i.e., fact) between two entities.          collection, and the quality controls we employed.
                    Fromthis graph G, we sample the facts that make                Reusability and composition. One challenge for
                    up the story, as well as the target fact that we seek          datacollection via AMTisthatthenumberofpossi-
                    to predict: First, we (uniformly) sample a target re-          ble stories generated by CLUTRR grows combina-
                    lation H , which is the fact that we want to predict           torially as the number of supporting facts increases,
                              C                                                    i.e., as k = |B | grows. This combinatorial ex-
                    from the story. Then, from this target relation H ,                              C
                                                                             C     plosion for large k—combined with the difﬁculty
                    werunasimplevariation of the backward chaining                 of maintaining the quality of the crowd-sourced
                    (Gallaire and Minker, 1978) algorithm for k itera-             paraphrasing for long stories—makes it infeasible
                    tions starting from HC, where at each iteration we             to obtain a large number of paraphrased examples
                    uniformlysampleasubgoaltoresolveandthenuni-                    for k > 3. To circumvent this issue and increase
                    formly sample a KB rule that resolves this subgoal.            the ﬂexibility of our benchmark, we reuse and com-
                    Crucially, unlike traditional backward chaining, we            pose AMTparaphrases to generate longer stories.
                    do not stop the algorithm when a proof is obtained;            In particular, we collected paraphrases for stories
                    instead, we run for a ﬁxed number of iterations k              containing k = 1,2,3 supporting facts and then
                    in order to sample a set of k facts B that imply the
                    target relation H .                      C                     replaced the entities from these collected stories
                                       C                                           with placeholders in order to re-use them to gener-
                    3.3   Addingnaturallanguage                                    ate longer semi-synthetic stories. An example of
                                                                                   a story generated by stitching together two shorter
                    So far, we have described the process of generat-              paraphrases is provided below:
                    ing a conjunctive logical clause C = (H ` B ),
                                      ∗                             C       C            [Frank] went to the park with his father, [Brett].
                    whereHC = [α ]isthetargetfact(i.e.,relation)we                       [Frank] called his brother [Boyd] on the phone.
                    seek to predict and B = [α ,...,α ] is the set of
                                             C        1       k                          Hewantedtogooutforsomebeers. [Boyd] went
                    supporting facts that imply the target relation. We                  to the baseball game with his son [Jim].
                    nowdescribe how we convert this logical represen-                    Q:Whatis[Brett] and [Jim]’s relationship?
                       2Kinship and gender roles are oversimpliﬁed in our data     Thus, instead of simply collecting paraphrases for a
                    (compared to the real world) to maintain tractability.         ﬁxednumberofstories, we instead obtain a diverse
                                                                              4509
