                                           Systematic Generalization - Trained on k=2 and k=3                              Systematic Generalization - Trained on k=2,3 and 4
                             1.0                                                     BERT                    1.0                                                    BERT
                                                                                     BERT - LSTM                                                                    BERT - LSTM
                                                                                     GAT                                                                            GAT
                             0.8                                                     MAC                     0.8                                                    MAC
                                                                                     RN                                                                             RN
                                                                                     BiLSTM Mean                                                                    BiLSTM Mean
                             0.6                                                     BiLSTM Attention        0.6                                                    BiLSTM Attention
                             Accuracy                                                                       Accuracy
                             0.4                                                                             0.4
                             0.2                                                                             0.2
                                    2       3       4       5       6       7       8       9      10              2       3       4       5       6       7       8       9      10
                                                             Relation Length                                                                Relation Length
                          Figure 5: Systematic generalization performance of different models when trained on clauses of length k = 2,3
                          (Left) and k = 2,3,4 (Right).
                              Tomakeaprediction about a target query given                                  Q1: Systematic Generalization
                          a story, we concatenate the embedding of the story                                WebeginbyusingCLUTRRtoevaluatetheabil-
                          (generated by the baseline model) with the embed-                                 ity of the baseline models to perform systematic
                          dings of the two target entities and we feed this                                 generalization (Q1). In this setting, we consider
                          concatenated embedding to a 2-layer feed-forward                                  twotraining regimes: in the ﬁrst regime, we train
                          neural network with a softmax prediction layer.                                   all models with clauses of length k = 2,3, and in
                          4.2     Experimental Setup                                                        the second regime, we train with clauses of length
                                                                                                            k = 2,3,4. Wethentestthegeneralizationofthese
                          Hyperparameters. Weselected hyperparameters                                       models on test clauses of length k = 2,...,10.
                          for all models using an initial grid search on the sys-                               Figure 5 illustrates the performance of different
                          tematic generalization task (described below). All                                modelsonthisgeneralization task. We observe that
                          models were trained for 100 epochs with Adam op-                                  the GATmodelisabletoperformnear-perfectlyon
                          timizer and a learning rate of 0.001. The Appendix                                the held-out logical clauses of length k = 3, with
                          provides details on the selected hyperparameters.                                 the BERT-LSTMbeingthetop-performer among
                          Generated datasets. For all experiments, we gen-                                  the text-based models but still signiﬁcantly be-
                          erated datasets with 10-15k training examples. In                                 low the GAT. Not surprisingly, the performance
                          manyexperiments,wereporttrainingandtestingre-                                     of all models degrades monotonically as we in-
                          sults on stories with different clause lengths k. (For                            crease the length of the test clauses, which high-
                          brevity, we use the phrase “clause length” through-                               lights the challenge of “zero-shot” systematic gen-
                          out this section to refer to the value k = |B |, i.e.,                            eralization (Lake and Baroni, 2018; Sodhani et al.,
                                                                                            C               2018). However, as expected, all models improve
                          the number of steps of reasoning that are required
                          to predict the target query.) In all cases, the training                          ontheir generalization performance when trained
                          set contains 5000 train stories per k value, and, dur-                            on k = 2,3,4 rather than just k = 2,3 (Figure 5,
                          ing testing, all experiments use 100 test stories per                             right). The GAT, in particular, achieves the biggest
                          k value. All experiments were run 10 times with                                   gain by this expanded training.
                          different randomly generated stories, and means                                   Q2: TheBeneﬁtofStructure
                          and standard errors over these 10 runs are reported.                              Theempirical results on systematic generalization
                          As discussed in Section 3.5, during training we                                   also provide insight into how the text-based NLU
                          holdout 20% of the paraphrases, as well as 10% of                                 systems compare against the graph-based GAT
                          the possible logical clauses.                                                     modelthathasfullaccesstothelogicalgraphstruc-
                          4.3     Results and Discussion                                                    ture underlying the stories (Q2). Indeed, the rela-
                          Withourexperimental setup in place, we now ad-                                    tively strong performance of the GAT model (Fig-
                          dress the three key questions (Q1-Q3) outlined at                                 ure 5) suggests that the language-based models fail
                          the beginning of Section 4.                                                       to learn a robust mapping from the natural language
                                                                                                            narratives to the underlying logical facts.
                          bedding approaches.                                                                   Tofurtherconﬁrmthistrend,weranexperiments
                                                                                                      4512
