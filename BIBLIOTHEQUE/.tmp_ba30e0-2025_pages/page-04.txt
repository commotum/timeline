                   Scale                  Translation
                                                                                                 place on canvas
                Figure 4. The raw input undergoes random scale and translation           [task]
                transformations and is placed on the “canvas” (denoted in gray).
                                                                                                                  ...                 ...
                ses data augmentations encourage the model to learn un-                                        patch embedding
                derlying mappings invariant to geometric transformations
                                                                                                                  ...                 ...
                grounded in the visual world. Formally, we perform:
                • Scale augmentation: Given a raw input, we randomly                                         Transformer block
                                                                                                                  ...
                   resize it by an integer scaling ratio s, duplicating each
                                                                                                             Transformer block
                   raw pixel into s×s (see Fig. 4, left). This is analo-
                                                                                                                  ...
                   gous to nearest-neighbor interpolation in natural im-                                                              ...
                   ages. However, note that “colors” in ARC do not cor-
                                                                                                                  predictor
                   respond to real-world colors, so it is not meaningful to
                                                                                                                                      ...
                   perform other interpolations (such as bilinear).                                               ...
                • Translation augmentation: given the scaled grid, we
                   randomly place it on the fixed-size canvas. We ensure
                   all pixels are visibile. See Fig. 4 (right).
                Weempirically show that these visual priors are important
                for generalization to unseen tasks.                                                  off canvas
                Vision Transformer. Given a canvas with an input ran-
                domly placed, we perform image-to-image translation by a            Figure 5. The ViT architecture in VARC. The input is randomly
                standard vision model. By default, we use a ViT [17].               placed on a canvas, which is then treated as a natural image and
                   The principle of ViT is Transformer on patches. For-             processed by a standard ViT, conditioned on the task token.
                mally, the input canvas is divided into non-overlapping
                patches (e.g., 2×2), projected by a linear embedding, added         vertical coordinate. This can be applied both to additive po-
                with positional embedding [52], and processed by a stack            sitional embeddings for encoding absolute positions and to
                of Transformer blocks [52]. The model has a linear projec-          the encoding of relative positions (e.g., RoPE [48]).
                tion layer as the output, which performs per-pixel classifica-      Alternative: convolutional networks. Beyond ViT, we
                tion for each patch. Note that unlike natural images where          also study the more classical vision-based architecture, i.e.,
                each raw pixel has continuous values, in our case, the raw          convolutional neural networks [30]. Specifically, we adopt
                pixels have discrete values. Therefore, before patchifica-          the U-Netmodel[46],ahierarchicalconvolutionalnetwork.
                tion, we first map each pixel’s discrete index into a learnable     The original U-Net was proposed precisely for the image-
                continuous-valued embedding.                                        to-image translation problem of segmentation [46], making
                   Conceptually, patchification can be viewed as a special          it a natural candidate for the problem we consider.
                form of convolution. Like convolution, it incorporates sev-
                eral critical inductive biases in vision: most notably, local-      3.4. Two-stage Training
                ity (i.e., grouping nearby pixels) and translation invariance
                (i.e., weight sharing across locations).                            Weadoptatwo-stagetrainingparadigmtolearntheparam-
                2Dpositional embedding. Unlike language data, which is              eters of the neural network.
                generally modeled as 1D sequences, images are inherently            Offline training. This stage is applied on the entire train-
                                                               ¨                                                         T
                2D. This 2D structure can be lost if we naıvely treat the           ing set T    . It is on all demos D       for any T ∈ T      .
                                                                                             train                       demo                 train
                embedded patches as a 1D sequence. We empirically show              Wetrain one model f jointly for all k training tasks (e.g.,
                                                                                                           θ
                that explicitly modeling positions in 2D is essential.              k=400), based on the loss in Eq. (1). All tasks share the
                   Formally, we adopt separable 2D positional embed-                same parameters, only except that each task has its own
                dings, following [11]: with D channels for positional em-           task-conditional token.    We do not use the inference set
                beddings, we use the first half of the channels to embed            DT from the training tasks (i.e., T ∈ Ttrain) to train the
                                                                                      infer
                the horizontal coordinate and the second half to embed the          model. These sets are used only for validation purposes.
                                                                                4
