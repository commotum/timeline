                                                                                                      model     width    depth    #params    Gflops    acc.
                     (a) naïve baseline
                                                 26.8
                                                                                                                 384       5          6M         10    44.4
                                                        32.8
                     (b) w/ 2D absolute pos embed                                                      ViT       512      10         18M         28    54.5
                                                                                                                 768      20         66M         99    53.0
                                                                     43.0
                     (c) w/ 2D RoPE
                                                                                                                  setting (a)         7M         18    42.8
                                                                        45.4
                     (d) 1x1 patch on 32x32  → 2x2 patch on 64x64                                     U-Net       setting (b)        17M         33    47.5
                     (e) w/ translation aug. on canvas
                                                                           48.3                                   setting (c)        55M         87    48.3
                     (f) w/ scale aug. on canvas
                                                                                   54.5       Table 1. Vision backbones. We compare variants of ViTs and U-
                                                                                    (%)
                    0          10          20          30          40          50
                                                                                              Nets of similar sizes. U-Net settings are in appendix.
                  Figure 7. Effects of visual priors in VARC. Accuracy is reported
                  on the ARC-1 evaluation set. The model used is ViT-18M. En-
                                                                                                 60                                  80
                  tries (a-c) use a patch size of 1×1 on a 32×32 canvas, whereas                        Depth = 5                            w/ offline training
                                                                                                                                     70
                                                                                                        Depth = 10                           wo/ offline training
                  entries (d-f) use a patch size of 2×2 on a 64×64 canvas. Each                  55
                                                                          ¨                                                          60                 54.5
                  entry modifies the one above it. We start from a naıve baseline
                                                                                                )
                                                                                                                         54.5        50
                                                                                                %
                                                                                                 50
                                                                                                (
                                                                                                                         18M              44.8
                  with components (b-f) removed. These vision priors cumulatively                
                                                                                                               51.6
                                                                                                y
                                                                                                c
                                                                                                a                                    40
                                                                                                               12M
                  yield 27.7 improvement (a→f), in which the canvas-based designs               r
                                                                                                u
                                                                                                c
                                                                                                c
                                                                                                 45
                                                                                                      47.0               47.0        30        29.1
                  (c→f) contribute an 11.5 gain.                                                A                                   Accuracy (%)              26.4
                                                                                                      6M
                                                                                                                         10M
                                                                                                               44.4                  20
                                                                                                                6M
                                                                                                 40
                                                                                                      41.0                           10
                                                                                                      3M
                                                                                                                                      0
                  5.1. Visual Priors                                                             35                                       TTT jointly  TTT independently
                                                                                                      256       384      512
                  Fig. 7 summarizes the effects of visual priors, starting from               Figure 8.    Scalability: ViTs       Figure 9.     TTT strategies:
                  a baseline (a) without the other components in this figure.                 with different width (x-axis)        with vs. without offline train-
                  These priors jointly have a gain of 27.7 points, where the                  and depth. The circle areas de-      ing, and joint vs. independent
                  canvas-based designs (c→f) has a gain of 11.5 points. We                    note model sizes.                    for each task.
                  discuss these components as follows.
                  2D positional embedding.             Extending from 1D posi-                Translation and scale augmentation. In image recogni-
                  tional embedding to its 2D counterpart is beneficial: see                   tion, even highly capable network architectures still benefit
                  Fig. 7(b)(c). This is observed in both (b) absolute and (c)                 greatly from translation and scale augmentations. We draw
                  relative positional embeddings.                                             similar observations in ARC. See Fig. 7(e,f).
                     To demonstrate this effect on a stronger baseline, we re-                   In Fig. 7(e), we apply fully flexible translation augmen-
                  place the 2D RoPEinFig.7(f)witha1DRoPEandobserve                            tation on the canvas. Compared with the “one-pixel” aug-
                  a degradation of 3.5 points, from 54.5 to 51.0.                             mentation in Fig. 7(d), this setting yields an additional gain
                  Patchification. A key design principle of our method is                     of 2.9 points (from 45.4 to 48.3). In Fig. 7(f), we further ap-
                  to prepare the input as a natural image. This enables the                   ply the scale augmentation enabled by the concept of can-
                  expansionofthetokensetfromaverylimitedsize(e.g.,10)                         vas.   Scale augmentation yields a substantial gain of 6.2
                  to an exponentially large number. The entries Fig. 7(d-f) all               points. Unlike translation invariance, which can be partially
                  benefit from this design.                                                   addressed by patchification (i.e., a special form of convo-
                     In Fig. 7(d), we advance from 1×1 patches on a 32×32                     lution), the ViT architecture has little to no inductive bias
                  canvas to 2×2 patches on a 64×64 canvas. Doing so does                      aboutscaleinvariance. This can explain why scale augmen-
                  not increase the computational cost of the Transformer. In                  tation yields a substantial gain.
                  this ablation (d), the scaling ratio is fixed as 2×. As such,               5.2. Other Ablation Experiments
                  if we constrain each 2×2 patch to cover only one raw pixel,                 ViT vs. U-Net. In Tab. 1, we compare ViT with U-Nets,
                  it becomes equivalent to the 1×1 patch counterpart on the                   a type of convolutional network. We evaluate three model
                  32×32 canvas. Therefore, to ensure a meaningful compar-                     sizes for each architecture. Although ViTs consistently per-
                  ison, we do not impose this constraint, allowing each 2×2                   formbetter,allU-Netvariantsachievedecentaccuracy,sug-
                  patch to cover multiple colors. This can be interpreted as
                  one-pixel translation augmentation on the canvas.                           gesting that this problem can also be effectively addressed
                     Evenso,the2×2patchificationleadstoanoticeablegain                        byclassical vision backbones.
                  of 2.4 points, improving from 43.0 to 45.4; see Fig. 7(c,d).                Scalability. In Fig. 8, we show ViTs with varying depths
                  In spite of the small one-pixel augmentation, each patch can                and widths. In this regime, our method demonstrates good
                  cover multiple colors (as in natural images), which substan-                scalability: increasing depth and/or width leads to higher
                  tially enriches the data space for learning.                                accuracy as a result of better fitting.        Going beyond this
                                                                                          6
