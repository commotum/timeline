                [33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.          [51] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet
                     Hoi. BLIP: bootstrapping language-image pre-training for             Singh, Adina Williams, Douwe Kiela, and Candace Ross.
                     unified vision-language understanding and generation.   In           Winoground: Probing vision and language models for visio-
                     ICML,2022.                                                           linguistic compositionality. In CVPR, 2022.
                [34] Wenhao Li, Yudong Xu, Scott Sanner, and Elias Boutros          [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
                     Khalil. Tackling the abstraction and reasoning corpus with           reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
                     vision transformers: the importance of 2D representation,            Polosukhin. Attention is all you need. In NeurIPS, 2017.
                     positions, and objects. arXiv:2410.06405, 2024.                [53] Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu,
                [35] Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon                Yue Wu, Meng Lu, Sen Song, and Yasin Abbasi Yadkori.
                     Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Wei-Long               Hierarchical reasoning model. arXiv:2506.21734, 2025.
                     Zheng,YewenPu,andKevinEllis. Combininginductionand             [54] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu,
                     transduction for abstract reasoning. In ICLR, 2025.                  Nick Haber, and Noah D. Goodman. Hypothesis search: In-
                [36] Isaac Liao and Albert Gu.     ARC-AGI without pretrain-              ductive reasoning with language models. In ICLR, 2024.
                     ing.   https://iliao2345.github.io/blog_                       [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
                     posts/arc_agi_without_pretraining/arc_                               Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and
                     agi_without_pretraining.html,2025.                                   Denny Zhou. Chain-of-thought prompting elicits reasoning
                [37] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.                      in large language models. In NeurIPS, 2022.
                     Visual instruction tuning. NeurIPS, 2023.                      [56] Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv
                [38] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully             Batra, and Devi Parikh. Yin and yang: Balancing and an-
                     convolutional networks for semantic segmentation.       In           swering binary visual questions. In CVPR, 2016.
                     CVPR,2015.
                [39] Laurens van der Maaten and Geoffrey Hinton. Visualizing
                     data using t-SNE. Journal of machine learning research,
                     2008.
                                                   ´
                [40] Matthew V Macfarlane and Clement Bonnet. Searching la-
                     tent program spaces. arXiv:2411.08706, 2024.
                [41] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B.
                     Tenenbaum, and Jiajun Wu. The neuro-symbolic concept
                     learner: Interpreting scenes, words, and sentences from nat-
                     ural supervision. In ICLR, 2019.
                [42] Arseny Moskvichev, Victor Vikram Odouard, and Melanie
                     Mitchell.    The ConceptARC benchmark:         Evaluating
                     understanding and generalization in the ARC domain.
                     arXiv:2305.07141, 2023.
                                                ¨    ¨
                [43] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
                     Darrell, and Alexei A. Efros.  Context encoders: Feature
                     learning by inpainting. In CVPR, 2016.
                [44] Rolf Pfister and Hansueli Jud. Understanding and bench-
                     marking artificial intelligence: OpenAI’s o3 is not AGI.
                     arXiv:2501.07458, 2025.
                [45] Jean-Francois Puget. A 2D nGPT model for ARC Prize.
                     2024.
                [46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
                     net: Convolutional networks for biomedical image segmen-
                     tation. In International Conference on Medical image com-
                     puting and computer-assisted intervention. Springer, 2015.
                [47] Yang Song and Stefano Ermon. Generative modeling by es-
                     timating gradients of the data distribution. In NeurIPS, 2019.
                [48] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen
                     Bo,andYunfengLiu. Roformer: Enhancedtransformerwith
                     rotary position embedding. Neurocomputing, 2024.
                [49] YuSun,XiaolongWang,ZhuangLiu,JohnMiller,AlexeiA.
                     Efros, and Moritz Hardt.     Test-time training with self-
                     supervision for generalization under distribution shifts. In
                     ICML,2020.
                [50] Hao Tang, Keya Hu, Jin Zhou, Sicheng Zhong, Wei-Long
                     Zheng, Xujie Si, and Kevin Ellis. Code repair with LLMs
                     gives an exploration-exploitation tradeoff. In NeurIPS, 2024.
                                                                                10
