                           offline training                                                                   U-net                        7M     17M     55M
                           epochs                                              100                            #stages                        3       3        3
                           warmupepochs                                         10                            layers per stage               1       1        2
                           optimizer                 Adam[28],betas=(0.9, 0.999)                              #channels at resolution 1     80     120     160
                           batch size                                           32                            attention at resolution 1    No      No       No
                           learning rate                                      3e-4                            #channels at resolution 2   160      240     320
                           learning rate scheduler                          cosine                            attention at resolution 2   Yes      Yes     Yes
                           weight decay                                          0                            #channels at resolution 3   160      240     320
                           dropout                                             0.1                            attention at resolution 3   Yes      Yes     Yes
                           test-time training                                                                 midblock                     No      No      Yes
                           epochs                                              100
                           warmupepochs                                         10                Table7. ConfigurationoftheU-Netarchitecture. Thedefinition
                           optimizer                 Adam[28],betas=(0.9, 0.999)                  follows standard U-Nets used in generative models [47, 15].
                           batch size                                            8
                           learning rate                                      3e-4
                           learning rate scheduler                          cosine
                           weight decay                                          0
                           dropout                                             0.1
                                         Table 4. Configurations.
                     offline training                    test-time training                       Figure 13. Shape Handling. The gray pixels denote the back-
                     GPUtype              H100           GPUtype                   H100           ground tokens [BG], which keep the canvas size fixed (64×64 by
                     GPUnumber                8          GPUnumber                     1          default). The white pixels denote the border tokens [BD], which
                     GPUtime          4.8 hours          GPUtime          0.7s per epoch          indicate the output shape. (Left): a pair (x,y) with a scaling ratio
                  Table 5. Running time of the ViT-18M model. The reported time                   of 1×. (Right): a pair (x,y) with a scaling ratio of 2×.
                  is obtained with torch.compile optimization.
                                                                                                  inal task. We train for 100 epochs on these 51 tasks, covering
                               ViT                        6M     18M      66M                     100 × 51 × 3 = 15.3k samples in total for test-time training for
                               hidden dim                 384     512     768                     one test task T (assuming 3 raw samples in this task).
                               Transformer blocks          5      10       20
                               #heads                      8       8       12                     A.3. Shape Handling
                               MLPblockhiddendim                  512
                               dropout                             0.1                            Unlikestandardsemanticsegmentation,inARC,therawinputand
                               patch size                         2×2                             outputsizesarenotalwaysidentical(e.g.,seeFig.3,TestSet,Task
                               canvas size                       64×64                            1). This issue can be addressed on the canvas in a unified frame-
                  Table 6. Configuration of the ViT architecture. The 18M model                   work. In our method, the input/output canvas always has a fixed
                  is our default setting.                                                         size and is filled with a background token [BG]. In addition, when
                                                                                                  the raw output is placed on the canvas (serving as the ground truth
                                                                                                  during training), we always use an extra border token, [BD], to
                  A. Additional Implementation Details                                            indicate the right and bottom edges. Specifically, the token [BD]
                                                                                                  is filled along the one-pixel-wide edge on the right and bottom
                  A.1. Configurations                                                             sides. During inference, we locate the rightmost and bottommost
                                                                                                  [BD]tokens and crop the output accordingly to recover the final
                  Wereport the training configurations in Tab. 4. The running time                predicted shape. This is illustrated in Fig. 13.
                  under this configuration is profiled in Tab. 5.                                     Since the number of background pixels [BG] can dominate
                      The hyperparameters for our ViT models are listed in Tab. 6,                in some examples, we apply attention masks in the self-attention
                  and those for our U-net models are shown in Tab. 7.                             blocks to encourage the model to focus on the foreground pixels.
                  A.2. Test-time Training Augmentation                                            The attention masks are applied after the query-key dot-product
                                                                                                  computation,addingalargenegativevaluetothekeyscorrespond-
                  During test-time training, we augment the single test task T into               ing to background inputs. The resulting softmax attention scores
                  multiple auxiliary tasks. We use a distinct task embedding for                  are therefore zero at those key positions. Moreover, during train-
                  each auxiliary task, as not all of these augmentations correspond               ing, the loss is computed only on locations where the inputs are not
                  to the same underlying rule (e.g., consider “gravity” under a 90◦               background pixels [BG]. These designs encourage the model to
                  rotation). We apply 2 flippings (horizontal and vertical) or 3 rota-            paymoreattentiontoforegroundsandthereforeimproveaccuracy,
                  tions (in multiples of 90◦), and 10 predefined color index permu-               although we note that even without them, our method still per-
                  tations, resulting in (2+3)×10=50 auxiliary tasks with the orig-                forms competitively, as observed in our preliminary experiments.
                                                                                             11
