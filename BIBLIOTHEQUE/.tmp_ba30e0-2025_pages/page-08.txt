                                                                                                                                                                                                                                                                                                                             model prediction
                                                                                                                                                      model prediction
                                      one pixel, different layers                                                                       -1                                                  +1
                                                                             layer 3                                             layer 4                                              layer 8
                                       one layer, different pixels
                                                                             layer 8                                              layer 8                                             layer 8
                                                                                                                                                                                                               Figure 11. Visualization of layer-wise attention maps. For each
                                                                                                                                                                                                               layer, we compute pixel-to-pixel attention and then average the
                                                                                                                                                                                                               softmax maps across all pixels to obtain a single map per layer.
                                       Figure 10. Visualization of pixel-to-pixel attention. (Top): a                                                                                                          This map reveals which pixels are most attended in this layer. We
                                       test task from ARC-1 eval: showing demo pairs, inference input,                                                                                                         showatesttaskfromARC-1eval. Inthistask,somelayersexhibit
                                       and model prediction. (Middle): attention maps for a single pixel                                                                                                       strong attention to the 3×3 neighborhood, reflecting the influence
                                       across different layers. With the highlighted pixel as query, we                                                                                                        ofthepattern’score. Incomparison,someotherlayers(e.g.,layers
                                       show pre-softmax logits. Different layers exhibit different behav-                                                                                                      7–9)focusontheoutward-radiatingrays,correspondingtotherule
                                       ior. (Bottom): attention maps in layer 8 with other query pixels.                                                                                                       that extends colored pixels along the eight directions.
                                       All of them correctly attend to their corresponding palette pixel.
                                                                                                                                                                                                                    “Use the blue layout in the         “Use the green layout in the 
                                                                                                                                                                                                                                                                                                “Copy the small colored           “Copy the small colored 
                                                                                                                                                                                                                    blue box as the blueprint,          blue box as the blueprint, 
                                       different layers exhibit different specialties: some layers at-                                                                                                                                                                                          pattern center to the gray        pattern center to the blue 
                                                                                                                                                                                                                    fill its four regions with the      fill its four regions with the 
                                                                                                                                                                                                                                                                                                pixel.”                           pixel.”
                                       tend to the pixels that are to be copied, and some layers                                                                                                                    four corner colors”                 small square of four colors.”
                                       attend to the target lines alone the eight directions.
                                       t-SNE of task embeddings. Our model is conditioned on
                                       a task token, with an embedding learned to represent each
                                       task. With 400 training tasks in ARC-1, our model learns
                                       400 distinct task embeddings in offline training. We visu-
                                       alize these 400 embeddings in the 2D space by t-SNE [39]
                                       (see Fig. 12). Each point corresponds to a task..
                                              Interestingly, we observe that nearby points in the task                                                                                                               “If a cell is black in both     “If a cell is black in both        “Expand the red pixel vertically  “Expand the gray pixel diagonally 
                                                                                                                                                                                                                     input grids, make it green.  input grids, make it green.           with blue pixels, and expand          with blue pixels, and expand 
                                       embedding space exhibit similar semantics. For example,                                                                                                                       Otherwise leave it black.”      Otherwise leave it black.”         pattern in four dimensions.”          pattern in four dimensions.” 
                                       the top-left corner in Fig. 12 shows two tasks related to                                                                                                               Figure 12. t-SNE of task embeddings, on the 400 task tokens
                                       coloring; the bottom-left corner shows two tasks related to                                                                                                             learned from the ARC-1 training set. Each point represents a sin-
                                       generalized logic operations (i.e., AND/OR/XOR). This vi-                                                                                                               gle task. To aid the reader, we provide human-written descriptions
                                       sualization suggests that our method attempts to learn the                                                                                                              for the tasks (which are not used in any form by our method).
                                       relations between different tasks, which is an essential abil-
                                       ity for abstraction and reasoning.                                                                                                                                      lem, emphasizing abstraction and reasoning emerging di-
                                                                                                                                                                                                               rectly from image pixels.
                                       7. Conclusion                                                                                                                                                                   We hope this work will encourage the community to
                                                                                                                                                                                                               leverage ARC not only as a symbolic reasoning problem,
                                       Our work explores a previously overlooked perspective in                                                                                                                but also as a testbed for promoting the generalization ca-
                                       theARCtaskbyframingitasanimage-to-imagetranslation                                                                                                                      pacity of visual methods. Future research may extend this
                                       problem. Itnaturallyenablestheadaptationofvisualframe-                                                                                                                  direction through more expressive architectures, richer vi-
                                       works and yields strong few-shot generalization competi-                                                                                                                sual priors, or larger-scale image pre-training. We envision
                                       tive with recent approaches, while remainingordersofmag-                                                                                                                that vision-centric reasoning will play a key role in build-
                                       nitudesmallerthanmostLLM-basedmodels. Thisopensup                                                                                                                       ing AI systems capable of learning and applying abstract
                                       a new possibility of treating ARC as a vision-centric prob-                                                                                                             concepts in a human-like manner.
                                                                                                                                                                                                      8
