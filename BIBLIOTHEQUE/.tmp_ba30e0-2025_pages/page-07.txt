                      single-view, pass@1  multi-view, pass@1   multi-view, pass@2                   system                  #params    ARC-1     ARC-2
                             35.9                 49.8                 54.5                          large language models (LLMs)
                                                                                                     Deepseek R1 [21]         671B         15.8       1.3
                           Table 2. Single-view vs. multi-view inference.                            Claude 3.7 8k [18]       N/A          21.2       0.9
                                                                                                     o3-mini-high [18]        N/A          34.5       3.0
                                                                                                     GPT-5[18]                N/A          44.0       1.9
                                                                                                     Grok-4-thinking [18]     1.7T         66.7      16.0
                  regime can lead to overfitting in our current setting, as                          Bespoke (Grok-4) [8]     1.7T         79.6      29.4
                  shown in Tab. 1 for the 66M ViT model. We observe that                             recurrent models
                  this larger model achieves higher training accuracy, sug-                          HRM[53]                  27M          40.3       5.0
                  gesting that future research should focus on generalization.                       TRM[27]                   7M          44.6       7.8
                  Test-timetraining(TTT)strategies. InFig.9(b),westudy                               vision models
                  TTTwithandwithout offline training, and TTT performed                              VARC                     18M          54.5       8.3
                                                                                                     VARC(ensemble)           73M          60.4      11.1
                  jointly on all test tasks vs. independently for each test task.                    humanresults
                     As expected, offline training greatly improves the per-                         avg. human [31]            -          60.2         -
                  formance of TTT, suggesting that common sense about the                            best human [18]            -          98.0     100.0
                  visual world can be learned from the training set. We also                 Table 3. System-level comparisons on the ARC-1 and ARC-2
                  note that even without offline training, our TTT strategy                  benchmarks. LLM-based results are from the ARC-AGI leader-
                  canachievenontrivialaccuracy(26.4),suggestingthatsome                      board [18]. HRM, TRM, and our VARC are trained from scratch
                  tasks in this benchmark can be solved tabula rasa. This re-                only on ARC data. Our single-model result is based on ViT, with
                  sult outperforms that in [36] under a similar setting.                     mean±std of 54.5±0.7 (ARC-1) and 8.3±0.4 (ARC-2) over four
                     Surprisingly, performing TTT independently for each                     runs. Our ensemble result aggregates an 18M ViT and a 55M
                                                                                  ∼
                  test task yields substantially better performance (by            10        U-Net, each with test-time training performed four times.
                  points) than doing so jointly across all test tasks, even
                  though the latter relies on a stronger assumption about the                orders of magnitude smaller.
                  availability of multiple test tasks at once.3 We hypothesize
                  that overtraining on the test tasks may cause the model to                    In the controlled setting of training from scratch on
                  forget the knowledge acquired during offline training.                     ARCdata, our method substantially outperforms the recur-
                                                                                             rent models: HRM [53] and TRM [27]. Our VARC with
                  Single-view vs. multi-view inference.            As discussed in                                 ∼
                                                                                             18Mparameters is 10 points better than TRM on ARC-1,
                  Sec. 3.5, we adopt multi-view inference by default. For                    a >20% relative improvement. Note that, once test-time
                  completeness, we also examine the single-view inference                    training is completed, our model performs fully feedfor-
                  accuracy. Since single-view inference cannot produce mul-                  ward inference, with no recurrence involved in reasoning.
                  tiple predictions, we compare pass@1 accuracy. See Tab. 2.                    Following the classical ensembling practice in vision
                     Single-view inference has a decent pass@1 accuracy of                   (e.g., AlexNet [29]), we ensemble one ViT and one U-Net,
                  35.9; multi-view inference further boosts to 49.8, thanks to               eachwithtest-time training run four times. Doing so boosts
                  majority voting. Unlike typical computer vision applica-                   our result to 60.4. This result closes the gap with the re-
                  tions such as semantic segmentation, in ARC, a mistake on                  ported average human performance (60.2 [31]).
                  even a single pixel renders the entire prediction incorrect.
                  This may explain the large gain seen here.                                 6. Visualization and Analysis
                  5.3. System-level Comparisons                                              Beyond numerical metrics, we provide additional qualita-
                  In Tab. 3 we compare with leading results using LLMs or                    tive results that help reveal the model’s behavior. We refer
                  recurrent models, on ARC-1 and ARC-2.4                                     readers to the appendix for more visualizations.
                     Our model compares favorably with some of the most                      Attention patterns. Fig. 10 shows the attention patterns of
                  powerful LLMs at the time their results were reported: in-                 our ViT model in a test task. These attention maps show
                  cluding Deepseek, Claude, o3, and GPT-5 (we note that                      that our model can correctly reason about the relationship
                  given the rapid progress of LLMs, these models may have                    between a source pixel and its target pixel to copy from.
                  stronger results by the time our paper is public). LLMs are                   Figure11visualizesthelayer-wiseattentionmapsforan-
                  pre-trained on internet-scale data, and some may also incor-               other test task. A layer-wise map is the softmax attention
                  porate multimodal data that include images. Our method                     mapaveragedacrossall pixels in the layer: it reveals which
                  does not rely on such data and uses a model that is several                pixels receive the most attention in that layer. In this task,
                     3In general, it cannot be assumed that multiple unseen tasks will be       4Our ARC-2 models are trained only on the ARC-1 dataset, with test-
                  presented all at once.                                                     time training and inference on the ARC-2 set.
                                                                                         7
