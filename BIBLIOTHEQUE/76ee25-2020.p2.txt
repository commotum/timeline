                     Contents
                     1  Introduction                                                                                2
                     2  BackgroundandMethods                                                                        6
                     3  Empirical Results and Basic Power Laws                                                      7
                     4  Charting the Inﬁnite Data Limit and Overﬁtting                                             10
                     5  Scaling Laws with Model Size and Training Time                                             12
                     6  OptimalAllocation of the Compute Budget                                                    14
                     7  Related Work                                                                               18
                     8  Discussion                                                                                 18
                     Appendices                                                                                    20
                     A SummaryofPowerLaws                                                                          20
                     B EmpiricalModelofCompute-EfﬁcientFrontier                                                    20
                     C Caveats                                                                                     22
                     D SupplementalFigures                                                                         23
                     1   Introduction
                     Language provides a natural domain for the study of artiﬁcial intelligence, as the vast majority of reason-
                     ing tasks can be efﬁciently expressed and evaluated in language, and the world’s text provides a wealth of
                     dataforunsupervisedlearningviagenerativemodeling. Deeplearninghasrecentlyseenrapidprogressinlan-
                                                                                   +        +        +
                     guagemodeling,withstateoftheartmodels[RNSS18,DCLT18,YDY 19,LOG 19,RSR 19]approaching
                                                                       +
                     human-level performance on many speciﬁc tasks [WPN 19], including the composition of coherent multi-
                     paragraph prompted text samples [RWC+19].
                     Onemightexpectlanguagemodelingperformancetodependonmodelarchitecture,thesizeofneuralmodels,
                     the computing power used to train them, and the data available for this training process. In this work we will
                     empirically investigate the dependence of language modeling loss on all of these factors, focusing on the
                                                +        +
                     Transformer architecture [VSP 17, LSP 18]. The high ceiling and low ﬂoor for performance on language
                     tasks allows us to study trends over more than seven orders of magnitude in scale.
                     Throughout we will observe precise power-law scalings for performance as a function of training time, con-
                     text length, dataset size, model size, and compute budget.
                     1.1  Summary
                     OurkeyﬁndingsforTransformer language models are are as follows:
                       2Here we display predicted compute when using a sufﬁciently small batch size. See Figure 13 for comparison to the
                     purely empirical data.
                                                                    2
