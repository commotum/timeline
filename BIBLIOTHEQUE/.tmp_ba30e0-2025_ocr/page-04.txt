Scale Translation

Figure 4. The raw input undergoes random scale and translation
transformations and is placed on the “canvas” (denoted in gray).

ses data augmentations encourage the model to learn un-
derlying mappings invariant to geometric transformations
grounded in the visual world. Formally, we perform:

¢ Scale augmentation: Given a raw input, we randomly
resize it by an integer scaling ratio s, duplicating each
raw pixel into sxs (see Fig. 4, left). This is analo-
gous to nearest-neighbor interpolation in natural im-
ages. However, note that “colors” in ARC do not cor-
respond to real-world colors, so it is not meaningful to
perform other interpolations (such as bilinear).

¢ Translation augmentation: given the scaled grid, we
randomly place it on the fixed-size canvas. We ensure
all pixels are visibile. See Fig. 4 (right).

We empirically show that these visual priors are important
for generalization to unseen tasks.

Vision Transformer. Given a canvas with an input ran-
domly placed, we perform image-to-image translation by a
standard vision model. By default, we use a ViT [17].

The principle of ViT is Transformer on patches. For-
mally, the input canvas is divided into non-overlapping
patches (e.g., 22), projected by a linear embedding, added
with positional embedding [52], and processed by a stack
of Transformer blocks [52]. The model has a linear projec-
tion layer as the output, which performs per-pixel classifica-
tion for each patch. Note that unlike natural images where
each raw pixel has continuous values, in our case, the raw
pixels have discrete values. Therefore, before patchifica-
tion, we first map each pixel’s discrete index into a learnable
continuous-valued embedding.

Conceptually, patchification can be viewed as a special
form of convolution. Like convolution, it incorporates sev-
eral critical inductive biases in vision: most notably, local-
ity (Ze., grouping nearby pixels) and translation invariance
(i.e., weight sharing across locations).

2D positional embedding. Unlike language data, which is
generally modeled as 1D sequences, images are inherently
2D. This 2D structure can be lost if we naively treat the
embedded patches as a 1D sequence. We empirically show
that explicitly modeling positions in 2D is essential.
Formally, we adopt separable 2D positional embed-
dings, following [11]: with D channels for positional em-
beddings, we use the first half of the channels to embed
the horizontal coordinate and the second half to embed the

place on canvas J

[task]

| | | |
a a= a2 7“ “= = z “=

patch embedding }
tt J fq Lt { J 4 |
( Transformer block }
( Transformer block }
LI I | l l | l
( predictor ]
ty ft f 4 } fy ft 4 t

. a BB ...
On Be Be ee | an ae

off canvas

Figure 5. The ViT architecture in VARC. The input is randomly
placed on a canvas, which is then treated as a natural image and
processed by a standard ViT, conditioned on the task token.

vertical coordinate. This can be applied both to additive po-
sitional embeddings for encoding absolute positions and to
the encoding of relative positions (e.g., RoPE [48]).

Alternative: convolutional networks. Beyond ViT, we
also study the more classical vision-based architecture, i.e.,
convolutional neural networks [30]. Specifically, we adopt
the U-Net model [46], a hierarchical convolutional network.
The original U-Net was proposed precisely for the image-
to-image translation problem of segmentation [46], making
it a natural candidate for the problem we consider.

3.4. Two-stage Training

We adopt a two-stage training paradigm to learn the param-
eters of the neural network.

Offline training. This stage is applied on the entire train-
ing set Tain. It is on all demos DjZ,,,, for any T € Train.
We train one model fg jointly for all & training tasks (e.g.,
k=400), based on the loss in Eq. (1). All tasks share the
same parameters, only except that each task has its own
task-conditional token. We do not use the inference set
Dz... from the training tasks (i.e., T € Train) to train the
model. These sets are used only for validation purposes.
