                            Model     Baseline  +Fine-tune     +Candidate     +AugScore + DFS
                          Llama-mix     21.0         35.5          55.5           57.5       63.5
                           Nemo-mix     26.0        40.5           57.5          69.0        72.5
                         Table 1: Performance on 100 randomly split-off examples of the ARC-AGI evaluation
                         dataset, adding parts of our pipeline. Baseline score shows performance of our network
                         after preliminary finetuning when taking two samples from generation. Fine-tune adds
                         test-time training on the examples of the puzzles. Candidate selection uses augmentation
                         and greedy sampling to generate a candidate for each of 8 different augmentation of the
                         task, using log softmax scores for selection. AugScore additionally uses the sum of the
                         log softmax probabilites from 8 different augmentations as score. Finally, DFS uses our
                         custom DFS scheme to find all candidates with a sampling probability larger than 10%,
                         at the same time increasing the augmentations during inference from 8 to 16 per task
                         (to reflect the speedups gained by the DFS sampling). Scores were measured on the 100
                         problems of the evaluation dataset that were not merged into the training set.
                         3    Methods
                         Our approach to solving ARC-AGI combines data expansion, multi-stage
                         fine-tuning of language models, and specialized solution evaluation. Below,
                         we explain how these components work together to improve the modelâ€™s
                         performance while keeping computational costs manageable.
                         3.1   Datasets
                         The Abstraction and Reasoning Corpus (ARC-AGI) introduced by Chollet
                         [1] challenges the idea that language models cannot effectively generalize
                         from a small number of examples, often referred to as few-shot prompting.
                         The original ARC-AGI dataset consists of 900 reasoning tasks, divided into
                         400 training tasks, 400 public evaluation tasks, and 100 private evaluation
                         tasks. Each task involves grids of varying sizes, ranging from 1x1 to 30x30,
                         utilizing a palette of ten distinct colors.
                         Importantly, each task contains only a hand full instances of the respective
                         problem, as illustrated in Figure 2(a). Each instance consists of two grids:
                         one representing the input of the problem and the other representing the
                         expected output. The objective is to infer the underlying mechanics from
                         a few examples and apply this understanding to a new, unseen instance as
                         illustrated in the figure.
                         Hodel [11] introduced the re-ARC dataset, which re-implements all 400 tasks
                         of the public training dataset. Their code can be used to generate an ar-
                         bitrary amount of training data for these tasks. An example of code and
                         generated data can be seen in Figure 2.
                         In addition to ARC-AGI and its re-implementation re-ARC, we make use
                         of Concept-ARC, which is a second dataset containing similar problems in-
                                                              5
