             fashion (1, 2, 4, 8, ···) for a good coverage. This approach      code, and (3) since we do not know the expression size n, we
             provides better running times for most benchmarks in our set,     run concurrent searches for different values of n, whereas the
             but it can also be more expensive in certain cases.               super-optimizer can use the size of the input program as an
             F. Learning by Stochastic Search                                  upper bound on program size.
               The stochastic learning procedure is an adaptation of the                   IV. BENCHMARKS AND EVALUATION
             algorithm recently used by Schufza et al. [11] for program           We are in the process of assembling a benchmark suite
             super-optimization. The learning algorithm of the CEGIS loop      of synthesis problems to provide a basis for side-by-side
             uses the Metropolis-Hastings procedure to sample expressions.     comparisons of different solution strategies. The current set
             Theprobability of choosing an expression e is proportional to a   of benchmarks is limited to synthesis of loop-free functions
             value Score(e), which indicates the extent to which e meets the   with no optimality criterion; nevertheless, the benchmarks
             speciﬁcation ϕ. The Metropolis-Hastings algorithm guarantees      provide an initial demonstration of the expressiveness of the
             that, in the limit, expressions e are sampled with probability    base formalism and of the relative merits of the individual
             proportional to Score(e). To complete the description of the      solution strategies presented earlier. Speciﬁcally, in this section
             search procedure, we need to deﬁne Score(e) and the Markov        we explore three key questions about the benchmarks and the
             chain used for successor sampling. We deﬁne Score(e) to be        prototype synthesizers.
             exp(−βC(e)), where β is a smoothing constant (set by default         • Complexity of the benchmarks. Our suite includes a
             to 0.5), and the cost function C(e) is the number of concrete          range of benchmarks from simple toy problems to non-
             examples on which e does not satisfy ϕ.                                trivial functions that are difﬁcult to derive by hand. Some
               We now describe the Markov chain underlying the search.              of the benchmarks can be solved in a few hundredths of
             Fix an expression size n, and consider all expressions in L with       a second, whereas others could not be solved by any of
             parse trees of size n. The initial candidate is chosen uniformly       our prototype implementations. In all cases, however, the
             at random from this set [26]. Given a candidate e, we pick a           complexity of the problems derives from the size of the
             node v in its parse tree uniformly at random. Let ev be the            space of possible functions and not from the complexity
             subexpression rooted at this node. This subtree is replaced by         of checking whether a candidate solution is correct.
             another subtree (of the same type) of size equal to |ev| chosen      • Relative merits of different solvers. The use of a
             uniformly at random. Given the original candidate e, and a             standard format allows us to perform the ﬁrst side-to-
                        0                                              0
             mutation e thus obtained, the probability of making e the              side comparison of different approaches to synthesis.
             new candidate is given by the Metropolis-Hastings acceptance           None of the implementations were engineered with high-
                        0                    0
             ratio α(e,e ) = min(1,Score(e )/Score(e)).                             performance in mind, so the exact solution times are not
               The ﬁnal step is to describe how the algorithm selects the           necessarily representative of the best that can be achieved
             expression size n. Although the solver comes with an option            by a particular approach. However, the order of magni-
             to specify n, the expression size is typically not known a priori      tude of the solution times and the relative complexity of
             given a speciﬁcation ϕ. Intuitively, we run concurrent searches        the different approaches on different benchmarks can give
             for a range of values for n. Starting with n = 1, with some            us an idea of the relative merits of each of the approaches
             probability pm (set by default to 0.01), we switch at each step        described earlier.
             to one of the searches at size n±1. If an answer e exists, then      • Effect of problem encoding. For many problems, there
             the search at size n = |e| is guaranteed to converge.                  are different natural ways to encode the space of desired
               Consider the earlier example for computing the maxi-                 functions into a grammar, so we are interested in observ-
             mum of two integers. There are 768 integer-valued expres-              ing the effect of these differences in encoding for the
             sions in the grammar of size six. Thus, the probability of             different solvers.
             choosing e = ITE(x ≤ 0,y,x) as the initial candidate                 To account for variability and for the constant factors
             is 1/768. The subexpression to mutate is chosen uniformly
             at random, and so the probability of deciding to mutate           introduced by the prototype nature of the implementations,
             the boolean condition x ≤ 0 is 1/6. Of the 48 boolean             we report only the order of magnitude of the solution times
             conditions in the grammar, y ≤ 0 may be chosen with               in ﬁve different buckets: 0.1 for solution times less than half
                                                      0                        a second, 1 for solution times between half a second and 15
             probability 1/48. Thus, the mutation e = ITE(0 ≤ y,y,x)           seconds, 100 for solution times up to two minutes, 300 for
             is considered with probability 1/288. Given a set of con-
             crete examples {(−1,−4),(−1,−3),(−1,−2),(1,1),(1,2)},             solution times of up to 5 minutes, and inﬁnity for runs that
                                                 0                         0   time out after 5 minutes.
             Score(e) = exp(−2β), and Score(e ) = exp(−3β), and so e              The benchmarks themselves are grouped into three cate-
             becomes the new candidate with probability exp(−β). If, on
             the other hand, e0 = ITE(x ≤ y,y,x) had been the mutation         gories: hacker’s delight problems, integer benchmarks, and
                                       0             0                         assorted boolean and bit-vector problems.
             considered, then Score(e ) = 1, and e would have become
             the new candidate with probability 1.                                Hacker’s delight benchmarks: This set includes 57 differ-
               Our algorithm differs from that of Schufza et al. [11] in       ent benchmarks derived from 20 different bit-manipulation
             three ways: (1) we do not attempt to optimize the size of         problems from the book Hacker’s Delight        [27]. These bit-
             the expression while the super-optimizer does so; (2) we          vector problems were among the ﬁrst to be successfully
             synthesize expression graphs rather than straight-line assembly   tackled by synthesis technology and remain an active area of
