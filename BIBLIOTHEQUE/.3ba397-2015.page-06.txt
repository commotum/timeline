                                     model                     top-1 err.       top-5 err.                                                64-d                                      256-d
                                     VGG-16[41]                 28.07             9.33                                                                                           1x1, 64
                                                                                                                                    3x3, 64
                                                                                                                                                                                      relu
                                     GoogLeNet[44]                  -             9.15                                                   relu
                                                                                                                                                                                 3x3, 64
                                                                                                                                                                                      relu
                                     PReLU-net[13]              24.27             7.38                                              3x3, 64
                                                                                                                                                                                 1x1, 256
                                     plain-34                   28.54            10.02
                                     ResNet-34 A                25.03             7.76                                                   relu                                         relu
                                     ResNet-34 B                24.52             7.46                               Figure 5. A deeper residual function F for ImageNet. Left: a
                                     ResNet-34 C                24.19             7.40                               building block (on 56×56 feature maps) as in Fig. 3 for ResNet-
                                     ResNet-50                  22.85             6.71                               34. Right: a “bottleneck” building block for ResNet-50/101/152.
                                     ResNet-101                 21.75             6.05
                                     ResNet-152                 21.43             5.71                               parameter-free, identity shortcuts help with training. Next
                   Table 3. Error rates (%, 10-crop testing) on ImageNet validation.                                 weinvestigate projection shortcuts (Eqn.(2)). In Table 3 we
                   VGG-16isbased on our test. ResNet-50/101/152 are of option B                                      comparethreeoptions: (A) zero-padding shortcuts are used
                   that only uses projections for increasing dimensions.                                             for increasing dimensions, and all shortcuts are parameter-
                          method                                          top-1 err.       top-5 err.                free (the same as Table 2 and Fig. 4 right); (B) projec-
                                                                                                  †                  tion shortcuts are used for increasing dimensions, and other
                          VGG[41](ILSVRC’14)                                   -             8.43                    shortcuts are identity; and (C) all shortcuts are projections.
                          GoogLeNet[44](ILSVRC’14)                             -             7.89
                          VGG[41](v5)                                       24.4              7.1                        Table 3 shows that all three options are considerably bet-
                          PReLU-net[13]                                    21.59             5.71                    ter than the plain counterpart. B is slightly better than A. We
                          BN-inception [16]                                21.99             5.81                    argue that this is because the zero-padded dimensions in A
                          ResNet-34 B                                      21.84             5.71                    indeedhavenoresiduallearning. Cismarginallybetterthan
                          ResNet-34 C                                      21.53             5.60                    B, and we attribute this to the extra parameters introduced
                          ResNet-50                                        20.74             5.25                    by many (thirteen) projection shortcuts. But the small dif-
                          ResNet-101                                       19.87             4.60                    ferences amongA/B/Cindicatethatprojectionshortcutsare
                          ResNet-152                                       19.38             4.49                    not essential for addressing the degradation problem. So we
                                                                                                                     donotuseoptionCintherestofthispaper,toreducemem-
                   Table 4. Error rates (%) of single-model results on the ImageNet                                  ory/time complexity and model sizes. Identity shortcuts are
                   validation set (except † reported on the test set).                                               particularly important for not increasing the complexity of
                            method                                             top-5 err. (test)                     the bottleneck architectures that are introduced below.
                            VGG[41](ILSVRC’14)                                        7.32                           Deeper Bottleneck Architectures. Next we describe our
                            GoogLeNet[44](ILSVRC’14)                                  6.66                           deepernetsforImageNet. Becauseofconcernsonthetrain-
                            VGG[41](v5)                                                6.8                           ing time that we can afford, we modify the building block
                                                                                                                                                      4
                            PReLU-net[13]                                             4.94                           as a bottleneck design . For each residual function F, we
                            BN-inception [16]                                         4.82                           use a stack of 3 layers instead of 2 (Fig. 5). The three layers
                            ResNet(ILSVRC’15)                                         3.57                           are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers
                   Table 5. Error rates (%) of ensembles. The top-5 error is on the                                  are responsible for reducing and then increasing (restoring)
                   test set of ImageNet and reported by the test server.                                             dimensions,leavingthe3×3layerabottleneckwithsmaller
                                                                                                                     input/output dimensions. Fig. 5 shows an example, where
                                                                                                                     both designs have similar time complexity.
                   ResNet reduces the top-1 error by 3.5% (Table 2), resulting                                           Theparameter-freeidentityshortcutsareparticularlyim-
                   fromthesuccessfully reduced training error (Fig. 4 right vs.                                      portantforthebottleneckarchitectures. Iftheidentityshort-
                   left). This comparison veriﬁes the effectiveness of residual                                      cut in Fig. 5 (right) is replaced with projection, one can
                   learning on extremely deep systems.                                                               show that the time complexity and model size are doubled,
                        Last, we also note that the 18-layer plain/residual nets                                     as the shortcut is connected to the two high-dimensional
                   are comparably accurate (Table 2), but the 18-layer ResNet                                        ends. So identity shortcuts lead to more efﬁcient models
                   converges faster (Fig. 4 right vs. left). When the net is “not                                    for the bottleneck designs.
                   overly deep” (18 layers here), the current SGD solver is still                                        50-layer ResNet: We replace each 2-layer block in the
                   able to ﬁnd good solutions to the plain net. In this case, the                                        4Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy
                   ResNet eases the optimization by providing faster conver-                                         from increased depth (as shown on CIFAR-10), but are not as economical
                   gence at the early stage.                                                                         asthebottleneckResNets. Sotheusageofbottleneckdesignsismainlydue
                                                                                                                     to practical considerations. We further note that the degradation problem
                   Identity vs. Projection Shortcuts. We have shown that                                             of plain nets is also witnessed for the bottleneck designs.
                                                                                                               6
