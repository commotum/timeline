           model on a test task. However, such fine-tuning is computationally expensive and highly prone to
           overfitting. This requires creating synthetic datasets on the fly, making fine-tuning a less scalable
           approach.
           Toaddress these limitations, we introduce a general and scalable algorithm, Latent Program Network
           (LPN), to perform program induction learning [Sun et al., 2018]. LPN builds a mechanism for
           test-time adaptation directly into the neural architecture, without the need for parameter updates,
           and utilizes a training objective suitable to test-time search. To perform adaptation, LPN leverages
           a continuous latent space to model a wide range of potential programs, which a neural decoder
           can then use to execute that program on a specific input. Note that our decoder directly generates
           outputs pixel by pixel instead of writing Python code that would execute the task. Contrary to
           most existing works, we train the neural architecture from scratch, as opposed to building on top
           of large-scale models such as LLMs. Instead, our goal is to generalize purely through a test-time
           adaptation mechanism, with as few priors as possible. Our key innovation is the ability to search
           through a structured latent space during both training and inference, enabling efficient adaptation
           at test time. We leverage gradient-based optimization in this latent space to find the latent program
           that best explains the given specification. This latent program can then be executed on new inputs
           to generate their corresponding outputs. Since this space is a highly compressed representation
           of input-output pairs, we can perform gradient ascent in this space without encountering potential
           overfitting that parameter-based fine-tuning methods face and therefore we do not require synthetic
           expansion. Our training objective ensures that we learn a structured latent space that is smooth and
           performs well with gradient-based optimization, allowing for more efficient program discovery. First,
           to assess the benefits of our latent-search architecture, we evaluate it on a simple subset of ARC-type
           programs. Second, we benchmark on ARC-AGI, a difficult program synthesis problem with a public
           train and evaluation set and hidden test set. In this work, we specifically choose to not enhance our
           results by using additional synthetic datasets, human or LLM generated, as we believe it is in the
           spirit of the ARC-AGI competition to only use priors from the training set. Specifically, we only
           use re-arc [Hodel, 2024] to generate input-output pairs that follow the programs of the training set.
           Bytraining only on train set problems, we ensure no possibility of data leakage from the evaluation
           dataset, which likely occurs in methods leveraging pre-trained LLMs or LLM-generated programs [Li
           et al., 2024a].
           Ourworksâ€™maincontributions are:
              1. We introduce Latent Program Network (LPN) which directly builds in test time adaption
               into the architecture by learning a latent space representing the space of possible programs,
               enabling test time adaption of an output decoder by moving in this latent space. We train this
               with a novel training objective for learning the latent space that prevents encoding the output
               directly into the latent space. Instead, we encode an input-output pair to the latent space but
               train this representation to decode the output of a different input-output pair, which prevents
               the latent space from representing the output space instead of the program space.
              2. We demonstrate that gradient-based search on the given specification in the latent space
               significantly improves the performance of LPN at test time compared to performance without
               latent search.
              3. Weshowthataddinggradient-based latent search during training enables the latent space
               itself to be trained such that gradient optimization in the space works well, resulting in
               significant improvement in sample efficiency.
              4. Wedonotmakeuseofanypre-trainedmodelsorLLM/human-generatedsyntheticdata
               when evaluating our work in the ARC domain, aside from generating input-output pairs
               using re-arc [Hodel, 2024] based on the training set. This makes our method highly
               scalable and could be quickly applied to a different domain without requiring a domain-
               specific language, synthetic data, or pre-trained model. LPN can be applied to any problem
               for which a large enough number of input-output pairs from a given set of programs is
               available. In our current setting, we do not even apply enough compute during training to
               observe convergence indicating that improved results on ARC-AGI could be found simply
               byscaling training compute resources/parameter counts.
              5. Our work directly refutes recent claims [Li et al., 2024b] that vision transformer architec-
               tures [Dosovitskiy, 2020] struggle to solve individual arc problems. We show this is not a
                               2
