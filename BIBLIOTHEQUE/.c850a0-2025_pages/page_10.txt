                    Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiad-    Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Ku-
                    bench: A challenging benchmark for promoting agi        mar. 2024. Scaling llm test-time compute optimally
                    with olympiad-level bilingual multimodal scientific     can be more effective than scaling model parameters.
                    problems. arXiv preprint arXiv:2402.14008.              arXiv preprint arXiv:2408.03314.
                 Aitor Lewkowycz, Anders Andreassen, David Dohan,         MingyangSong,ZhaochenSu,XiaoyeQu,JiaweiZhou,
                    Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,         and Yu Cheng. 2025. Prmbench: A fine-grained
                    Ambrose Slone, Cem Anil, Imanol Schlag, Theo            and challenging benchmark for process-level reward
                    Gutman-Solo, et al. 2022. Solving quantitative rea-     models. Preprint, arXiv:2501.03124.
                    soning problems with language models. Advances        Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang
                    in Neural Information Processing Systems, 35:3843–      Liu, Yiming Yang, Sean Welleck, and Chuang
                    3857.                                                   Gan. 2024. Easy-to-hard generalization: Scalable
                 Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri        alignment beyond human supervision.       Preprint,
                    Edwards, Bowen Baker, Teddy Lee, Jan Leike,             arXiv:2403.09472.
                    John Schulman, Ilya Sutskever, and Karl Cobbe.        Zhengyang Tang, Xingxing Zhang, Benyou Wang, and
                    2023.   Let’s verify step by step.  arXiv preprint      Furu Wei. 2024. Mathscale: Scaling instruction
                    arXiv:2305.20050.                                       tuning for mathematical reasoning. arXiv preprint
                 Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-          arXiv:2403.02884.
                    guang Lou, Chongyang Tao, Xiubo Geng, Qingwei         Jonathan Uesato, Nate Kushman, Ramana Kumar, Fran-
                    Lin, Shifeng Chen, Yansong Tang, and Dongmei            cis Song, Noah Siegel, Lisa Wang, Antonia Creswell,
                    Zhang. 2025. Wizardmath: Empowering mathemat-           Geoffrey Irving, and Irina Higgins. 2022. Solving
                    ical reasoning for large language models via rein-      math word problems with process- and outcome-
                    forced evol-instruct. Preprint, arXiv:2308.09583.       based feedback. Preprint, arXiv:2211.14275.
                 Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat          JunWang,MengFang,ZiyuWan,MuningWen,Jiachen
                    Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei         Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen,
                    Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav           Lionel M. Ni, Linyi Yang, Ying Wen, and Weinan
                    Rastogi. 2024a. Improve mathematical reasoning in       Zhang. 2024a. Openr: An open source framework
                    language models by automated process supervision.       for advanced reasoning with large language models.
                    Preprint, arXiv:2406.06592.                             Preprint, arXiv:2410.09671.
                 Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat          Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai
                    Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu,      Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang
                    Lei Meng, Jiao Sun, et al. 2024b. Improve mathemat-     Sui. 2024b. Math-shepherd: Verify and reinforce
                    ical reasoning in language models by automated pro-     llms step-by-step without human annotations. In
                    cess supervision. arXiv preprint arXiv:2406.06592.      Proceedings of the 62nd Annual Meeting of the
                                                                            Association for Computational Linguistics (Volume
                 OpenAI     O1.   2023.        Learning    to  reason       1: Long Papers), pages 9426–9439.
                    with   llms.        https://openai.com/index/         Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
                    learning-to-reason-with-llms/.          Accessed:       Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
                    2025-02-08.                                             Hajishirzi. 2023. Self-instruct: Aligning language
                 Skywork o1 Team. 2024. Skywork-o1 open series.             models with self-generated instructions. Preprint,
                    https://huggingface.co/Skywork.                         arXiv:2212.10560.
                 QwQ.2023. Qwq: Reflect deeply on the boundaries of       Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
                    the unknown. https://qwenlm.github.io/blog/             Bosma,BrianIchter, Fei Xia, Ed Chi, Quoc Le, and
                    qwq-32b-preview//. Accessed: 2025-02-08.                DennyZhou.2023. Chain-of-thoughtpromptingelic-
                                                                            its reasoning in large language models. Preprint,
                 Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano     arXiv:2201.11903.
                    Ermon,Christopher D. Manning, and Chelsea Finn.       Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and
                    2024. Direct preference optimization: Your lan-         Pengfei Liu. 2025. Evaluating mathematical reason-
                    guage model is secretly a reward model. Preprint,       ing beyond accuracy. Preprint, arXiv:2404.05692.
                    arXiv:2305.18290.
                                                                          An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao,
                 Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,            Bowen Yu, Chengpeng Li, Dayiheng Liu, Jian-
                    Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan          hong Tu, Jingren Zhou, Junyang Lin, Keming Lu,
                    Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024.             Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang
                    Deepseekmath: Pushing the limits of mathemati-          Ren, and Zhenru Zhang. 2024. Qwen2.5-math tech-
                    cal reasoning in open language models. Preprint,        nical report: Toward mathematical expert model via
                    arXiv:2402.03300.                                       self-improvement. Preprint, arXiv:2409.12122.
                                                                     13459
