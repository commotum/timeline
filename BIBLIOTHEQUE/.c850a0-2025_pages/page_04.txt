                 (Yw,Yl), where Yw is favored over Yl. Accord-               • PRMBench (Song et al., 2025) constitutes
                 ingly, multiple evaluation processes and their cor-           a comprehensive benchmark for evaluating
                 responding judgments are sampled and categorized              PRMs,withparticular emphasis on granular
                 into two groups depending on whether the judg-                error diagnosis. It assesses evaluation capabil-
                 ments align with the annotated labels. We encour-             ities across three error dimensions: Simplicity,
                 age our PRM to generate evaluation trajectory that            Soundness, and Sensitivity, which are further
                 can yield correct judgments; therefore, we treat              divided into nine specific aspects 2.
                 consistent trajectories as Y w and inconsistent ones       Furthermore, we validate the effectiveness
                 as Y l to construct preference pairs. We copy and        of our model by employing it to guide two
                 freeze R-PRM-SFTasthereferencepolicyπref and             distinct test-time scaling paradigm: Best-of-N
                 optimize it using the following loss function:           and Guide Search.       Performance is evaluated
                                                                          on MATH500 (Lightman et al., 2023), Min-
                 L     (π ;π ) = −E         w l
                   DPO    θ  ref        (x,Y ,Y )∼D                       erva Math (Lewkowycz et al., 2022), Olympiad-
                                πθ(Yw | x)            πθ(Yl | x)  Bench (He et al., 2024), College Math (Tang et al.,
                   logσ βlog πref(Yw | x) −βlog πref(Yl | x)              2024) 3, AIME24, and AMC23. Consistent with
                                                                          previous work (Zhang et al., 2025b), we used
                 3.3   Inference-Time Scaling Strategy                    Qwen2.5-7B-Instruct to generate eight candidate
                 Leveraging R-PRM’scapability to generate diverse         steps with temperature T=1.0.
                 evaluation trajectories, we explore the scalable in-        • Best-of-N: selects the response with the high-
                 ference strategy that enhances evaluation perfor-             est score among N candidates, as evaluated
                 mancewithout training. During inference, for each             byaPRM.
                 reasoning step s , we sample K independent ana-             • Guide Search: at each step, the model gen-
                                   i
                 lytical processes as follows:                                 erates N candidate continuations and selects
                                                                               the one with the highest reward score, as eval-
                     (A(k),J(k)) = G(Q,s ,...,s ),k ∈ [1,K]                    uated by the PRM, to extend the reasoning.
                        i     i              1      i
                                                                               This process repeats until the solution is com-
                                 (k)
                 where each A       represents a distinct analytical           plete.
                 reasoning process and J(k) is the corresponding          Baselines: We selected the following strong pro-
                 judgment. This multi-trajectory approach helps           cess reward models as baselines.
                 mitigate potential reasoning inconsistencies and            • Math-Shepherd(Wangetal., 2024b): Auto-
                 stochastic variations inherent in LLMs. To aggre-             matically obtaining the probability of reach-
                 gate multiple evaluations, we calculate the average           ing the correct solution as step labels based
                 probabilityof“Yes”judgments(usingsoftmaxwith                  onMonteCarloTreeSearch(MCTS).
                 “No”judgments) as the reward:                               • Math-PSA(Wangetal.,2024a): combining
                            K                                                  existing automatic annotation techniques (Luo
                         1 X        (k)                          (k)           et al., 2024a) and integrating data from Math-
                  R =           P(J     =“Yes”|Q,s ,...,s ,A       ).
                    i   K           i                 1      i   i             Shepherd and PRM800Kdatasets.
                           k=1
                 4 Experiment                                                • RLHFlow-DeepSeek/Mistral (Dong et al.,
                                                                               2024): Similar to Math-Shepherd, but trained
                 4.1   ExperimentSettings                                      with iterative DPO.
                 Tasks and Benchmarks: To validate the accu-                 • Skywork-PRM-7B(o1Team,2024): based
                 racy of our method in process reward modeling,                on Qwen2.5-Math-Instruct and recently re-
                 weconductevaluations on two challenging bench-                leased by Skywork.
                 marksProcessBench(Zhengetal.,2024)andPRM-                   • ReasonEval-7B (Xia et al., 2025): Evaluates
                 Bench (Song et al., 2025).                                    mathematical problem-solving step by step,
                     • ProcessBench (Zheng et al., 2024) assesses a            assessing validity and redundancy.
                       model’sability to detect the first incorrect step     • Llemma-PRM800K-7B (Sun et al., 2024):
                       in LLM-generated mathematical solutions. It             Trained exclusively on PRM800K from levels
                       consists of 3,400 problems of varying diffi-          2See Appendix C for detailed description.
                                                                             3Due to the large size of OlympiadBench and College
                       culty, each paired with a step-by-step solution    Math, we randomly select 200 samples from each for evalua-
                       and human annotation of the earliest error.        tion.
                                                                     13453
