                      Setting                       AIME24     AMC23 MATH500 Olympiad College               Minerva   Avg.
                                                                                       Bench       Math      Math
                      Pass@1                          11.2       47.8       73.0        38.0       38.6       37.2     41.0
                      Maj@8                           20.0       57.5       79.6        47.0       41.5       42.7     48.0
                      Pass@8(Oracle)                  33.3       82.5       88.8        58.5       47.5       57.7     61.4
                      Math-Shepherd-7B                13.3       52.5       74.6        38.5       36.5       41.2     42.8
                      Math-PSA-7B                     6.7        57.5       79.8        42.5       41.0       39.3     44.5
                      RLHFlow-PRM-Mistral-8B          10.0       57.5       73.4        37.5       38.0       41.2     42.9
                      RLHFlow-PRM-DeepSeek-8B         13.3       52.5       74.8        39.5       37.0       40.8     43.0
                      Llemma-PRM800K-7B               13.3       57.5       73.8        40.0       36.5       38.2     43.2
                      Skywork-PRM-7B                  10.0       57.5       77.8        41.5       39.0       43.4     44.9
                      ReasonEval-7B                   3.3        55.0       73.0        37.5       35.5       37.9     40.4
                      Qwen2.5-Math-7B-PRM800K         23.3       45.0       78.2        42.0       35.5       38.6     43.8
                      ⋆R-PRM-7B-DPO                   16.7       70.0       80.0        46.5       39.5       43.4     49.4
                  Table 3: The performance of PRM-guidedgreedysearchwiththeQwen2.5-7B-Instructpolicymodel,wheresuperior
                  performance indicates a more accurate reward from the PRM and consequently, more effective guidance.
                  tion paradigms and our training framework.               R-PRMdemonstrates superior generalization
                    Impressively, R-PRM-DPO achieves F1 score              capability. As shown in Table 1, all listed open-
                  improvements of 13.0 points over LLaMA3.3-70B-           source PRMs, except Skywork-PRM-7B for which
                  Instruct (used for generating our synthetic cold-        the training data sources is unknown, have been
                  start data) and 8.5 points over GPT-4o. Collectively,    trained exclusively on GSM8KandMATH.Among
                  these findings directly demonstrate that our method      these PRMs, only Math-PSA-7B and Qwen2.5-
                  extends beyond simple distillation and maximizes         Math-7B-PRM800KachieveF1scoresabove60on
                  the utility of human-annotated data.                     certain ProcessBenchsubsets, whileothersperform
                    Wealsoconductedadditionalexperimentstoval-             relatively poorly, particularly on out-of-domain
                  idate the effectiveness of continuous self-evolution     datasets such as OmniMATH and OlympiadBench.
                  through two rounds of iterative training. After the      Bycontrast, R-PRM not only performs well on the
                  second iteration, our model achieved an average          MATHdatasetbutalso achieves F1 scores above
                  F1 score of 74.1 on ProcessBench, these results          60onallout-of-domaindatasets. Thissuggeststhat
                  demonstrate the significant potential of our method.     R-PRMacquiresageneralizable reasoning pattern,
                  Please refer to Appendix 5.1 for comprehensive           enabling it to perform well across datasets with
                  results.                                                 varying difficulty.
                                                                           R-PRM guides policy model to reach correct
                  R-PRMprovidescomprehensiveevaluationsin                  answer effectively. As shown in Table 3 and Ta-
                  multiple dimensions.      In rigorous benchmark-         ble 4, our method achieves 8.4 and 8.6 average
                  ing with PRMBench, R-PRM-DPOdemonstrates                 accuracy improvements over the Pass@1 baseline
                  advantages over Qwen2.5-Math-7B-PRM800K,                 in the Guide Search and Best-of-N settings, re-
                  achieving improvements of 7.0, 9.0, and 4.4 points       spectively.  It also achieves state-of-the-art per-
                  across the three evaluation dimensions. Notably, it      formance by outperforming Qwen2.5-Math-7B-
                  surpasses GPT-4o in both soundness and sensitivity       PRM800Kby5.6and1.9points, and surpassing
                  metrics, establishing itself as a more comprehen-        Majority Voting in both settings. Moreover, on the
                  sive assessment paradigm.                               AIME24 benchmark, R-PRM boosts DeepSeek-
                    R-PRMespecially excels in soundness evalua-            R1-Distilled-Qwen-7B’sPass@1from54.5to60.8,
                  tion through its reasoning paradigm for empirical        demonstrating its effectiveness even for advanced
                  validity, step consistency, and domain consistency.      reasoners (see Appendix B for details). These re-
                  This structural evaluation paradigm enables supe-        sults directly demonstrate that our method’s ac-
                  rior detection of logical errors by analyzing each       curate reward evaluation at each reasoning step
                  reasoning step in context of previous ones. More-        effectively guides the policy model to arrive at cor-
                  over, R-PRM even outperforms o1-mini in prereq-          rect solutions. Furthermore, we believe our ap-
                  uisite sensitivity, effectively identifying reasoning    proach holds greater potential for integration with
                  steps that appear superficially valid but contain        backtracking-enabled strategies like Monte Carlo
                  logical flaws—precisely the type that conventional       Tree Search and multi-candidate strategies such as
                  evaluation systems frequently fail to detect.            BeamSearch,whichfurther boost the performance
                                                                      13455
