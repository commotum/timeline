                                   R-PRM:Reasoning-DrivenProcessRewardModeling
                                     1∗                  1∗                1                  1                2                      1†
                    Shuaijie She , Junxiao Liu , Yifeng Liu , Jiajun Chen , Xin Huang , Shujian Huang
                             1 National Key Laboratory for Novel Software Technology, Nanjing University
                                  2 China Mobile Communications Company Limited Research Institute
                                           {shesj,junxiaoliu,yfliu}@smail.nju.edu.cn, chenjj@nju.edu.cn,
                                                   huangxinyjy@chinamobile.com, huangsj@nju.edu.cn
                                          Abstract                              ing ability (Lightman et al., 2023). Unlike Out-
                                                                                come Reward Models (ORMs) that only focus
                       Process Reward Models (PRMs) have emerged                on the final results, PRMs evaluate each reason-
                       as a promising solution to address the reason-           ing step in a more fine-grained manner, enabling
                       ing mistakes of large language models (LLMs).            themtobetter identify and mitigate error processes,
                       However, existing PRMs typically output eval-            thereby improving both performance and general-
                       uation scores directly, limiting both learning           ization (Lightman et al., 2023; Wang et al., 2024b).
                       efficiency and evaluation accuracy. This limi-
                       tation is further compounded by the scarcity                AprimarychallengeinPRMdevelopmentarises
                       of annotated data. To address these issues,              from data scarcity. While human annotation can
                       wepropose Reasoning-Driven Process Reward                provide high-quality process-level labels (Light-
                       Modeling (R-PRM),whichactivates inherent                 man et al., 2023), it incurs substantial costs. Al-
                       reasoning to enhance process-level evaluation.           ternative automated approaches, such as Monte
                       First, we leverage stronger LLMs to generate             Carlo (MC) methods that estimate step correctness
                       seed data from limited annotations, effectively          based on the probability of reaching the correct fi-
                       activating reasoning capabilities and enabling
                       comprehensive step-by-step evaluation. Sec-              nalanswer(Wangetal.,2024b,a;Luoetal.,2024b),
                       ond, we explore self-improvement of our PRM              or methods that use stronger language models as
                       through preference optimization, without re-             judges for data filtering (Zhang et al., 2025b), have
                       quiring additional annotated data. Third, we             shown some promise. However, these methods
                       introduce inference time scaling to fully har-           either require significant computational resources
                       ness our model’s reasoning potential. Exten-             or still struggle with noise and bias, leaving the
                       sive experiments demonstrate R-PRM’s effec-              challenge of sufficient high-quality training data
                       tiveness: on ProcessBench and PRMBench, it               unresolved.
                       surpasses strong baselines by 13.9 and 8.5 F1
                       scores. When applied to guide mathematical                  Moreover, existing process reward models di-
                       reasoning, R-PRM achieves consistent accu-               rectly provide evaluations based on the given steps.
                       racy improvements of over 8.6 points across              Weargue that for challenging process-level eval-
                       six challenging datasets. Further analysis re-           uation tasks, this direct evaluation approach con-
                       veals that R-PRMexhibitsmorecomprehensive                strains the model’s learning process and reduces
                       evaluation and robust generalization, indicating         learning efficiency. Furthermore, it lacks inter-
                       its broader potential.                                   pretability, as it fails to identify why specific steps
                   1 Introduction                                               are incorrect, making it difficult to provide con-
                                                                                structive feedback for improvement.
                   Recently, large language models (LLMs) have                     Toaddresstheseissues,weproposeaReasoning-
                   demonstrated significant progress in solving chal-           Driven Process Reward Modeling (R-PRM) frame-
                   lenging mathematical problems through chain-of-              work that leverages the inherent reasoning capa-
                   thought reasoning (Wei et al., 2023; Yang et al.,            bilities of LLMs to conduct process-level eval-
                   2024; Shao et al., 2024). However, LLMs still tend           uation.    The framework consists of three key
                   to make reasoning errors, undermining the reliabil-          components: First, we construct seed data by
                   ity of their solutions and hindering their capacity          prompting stronger LLMs based on a small set
                   to generate correct solutions.                               of human-annotated process-level labels and sub-
                      Therefore, Process RewardModels(PRMs)have                 sequently fine-tune Qwen2.5-Math-7B-Instruct as
                   been proposed to further improve model reason-               a quick cold-start. Through this reasoning-centric
                                                                           13450
                         Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 13450–13463
                                             November4-9,2025©2025AssociationforComputational Linguistics
