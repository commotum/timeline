                  Shimao Zhang, Xiao Liu, Xin Zhang, Junxiao Liu,                                              74.0%    74.1%    74
                                                                                  1.0                                     1.08s
                     Zheheng Luo, Shujian Huang, and Yeyun Gong.                         Average Time (s)
                     2025a. Process-based self-rewarding language mod-                   F1 Score     72.3%
                                                                                  0.8                            0.83s           72
                     els. arXiv preprint arXiv:2503.03746.                       ime (s)
                                                                                  0.6        69.8%                               70 e (%)
                  Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen                                      0.52s                      F1 Scor
                     Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jin-              verage T0.4   0.39s
                     gren Zhou, and Junyang Lin. 2025b. The lessons of           A                                               68
                     developing process reward models in mathematical             0.2 0.18s
                     reasoning. Preprint, arXiv:2501.07301.                         65.9%                                        66
                                                                                  0.0   1        2        4        8       10
                  Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji                                  Sample Number (K)
                     Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jin-             Figure 7: R-PRM-DPO-Iter2 Inference Time vs F1 on
                     gren Zhou, and Junyang Lin. 2024. Processbench:          ProcessBench
                     Identifying process errors in mathematical reasoning.
                     Preprint, arXiv:2412.06559.                                Model                                 Avg@16(%)
                  A PracticalInference-Time Overhead                            DeepSeek-R1-Distilled-Qwen-7B             54.5
                        Analysis of R-PRM                                       w/R-PRM-DPO(GuideSearch)                  60.8
                  Toevaluatethepracticalcomputationaloverheadof               Table 5: Accuracy of DeepSeek-R1-Distilled-Qwen-7B
                  R-PRMinreal-worldusecases, we benchmarked                   onAIME24withandwithoutR-PRMguidance.
                  inference speed under realistic deployment settings.
                  Figure 7 presents the average time required to eval-        8 candidates at each step. Following this 30-step
                  uate a single ProcessBench example at different             guidedgeneration, the model completed the reason-
                  sample sizes (K) on a single NVIDIA H100 GPU                ing chain autoregressively. We sampled 16 such
                  (vLLM0.8.4).                                                completions and found that this partial guidance in-
                     Experiments show that with K=4, evaluating a             creased the average accuracy (avg@16) to 60.8%, a
                  single data point takes only 0.52 s, which balances         notableimprovementoverthe54.5%baseline. This
                  performance and computational cost. To contextu-            result demonstrates that even limited, early-stage
                  alize this for a practical application like online rein-    guidance from R-PRM can significantly enhance
                  forcement learning, consider a common setup with            the performance of long-reasoning models.
                  128promptsand8rollouts. In this scenario, com-
                  puting rewards with our R-PRMs across 8 GPUs                C DetailedDescription of PRMBench
                  requires only 66.56 seconds for K=4, and this time               Subcategories
                  reduces to just 23.4 seconds for K=1.                          • Non-Redundancy (NR): Evaluates the
                     These results indicate that R-PRM’s inference
                  overhead is acceptable for typical online training                model’s ability to identify and eliminate
                  pipelines, and even more favorable for offline or                 unnecessary steps within the reasoning pro-
                  latency-insensitive tasks such as SFT or DPO data                 cess, ensuring efficiency without sacrificing
                  synthesis. The sublinear scaling further suggests                 correctness.
                  that increasing the number of samples yields better            • Non-Circular        Logic    (NCL): Assesses
                  accuracy with moderate additional cost, making                    whether the model can detect circular reason-
                  R-PRMpractical for large-scale or production en-                  ing, where conclusions are reintroduced as
                  vironments.                                                       premises, leading to logical loops.
                  B R-PRMGuidanceforLong-Reasoning                               • Empirical Soundness (ES): Measures the
                        Model                                                       model’s capability to identify and reject rea-
                  Toassess R-PRM’s efficacy on long-form reason-                    soning steps that contradict established facts
                  ing, we evaluated it on the DeepSeek-R1-Distilled-                or real-world knowledge.
                  Qwen-7B model using the AIME24 benchmark.
                  To manage the lengthy reasoning, we restricted                 • Step Consistency (SC): Evaluates whether
                  R-PRMguidance to the initial 30 steps. Specifi-                   the reasoning steps maintain consistency with
                  cally, we constructed a guided prefix by iteratively              each other, ensuring that all steps logically
                  selecting the highest-scoring continuation out of                 flow from one to the next.
                                                                         13460
