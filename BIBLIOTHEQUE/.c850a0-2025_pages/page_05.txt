                       MODEL                            GSM8K                   MATH               OlympiadBench           OmniMATH          Avg. F1
                                                  error  correct   F1    error  correct   F1    error  correct  F1    error   correct  F1
                       LLM-as-judge, Proprietary language models
                       GPT-4o*                    70.0    91.2    79.2   54.4    76.6    63.6   45.8    58.4    51.4   45.2    65.6    53.5    61.9
                       o1-mini*                   88.9    97.9    93.2   83.5    95.1    88.9   80.2    95.6    87.2   74.8    91.7    82.4    87.9
                       LLM-as-judge, Open-source language models
                       Llama-3.3-70B-Instruct     71.0    97.9    82.3   42.8    95.3    59.0   30.7    94.1    46.3   27.4    88.8    41.9    57.4
                       Qwen2.5-Math-72B-Instruct  51.7    95.9    67.2   36.9    94.3    53.0   18.9    96.5    31.6   19.8    95.4    32.7    46.1
                       Qwen2.5-72B-Instruct       62.8    97.4    76.4   46.1    93.1    61.7   37.7    92.9    53.6   37.5    87.1    52.5    61.1
                       PRMs
                       Math-Shepherd-7B*          32.4    91.7    47.9   18.0    82.0    29.5   15.0    71.1    24.8   14.2    73.0    23.8    31.5
                       Math-PSA-7B                48.3    88.1    62.4   29.5    72.7    41.9   20.7    65.8    31.5   15.4    68.9    25.2    40.3
                       RLHFlow-Mistral-8B*        33.8    99.0    50.4   21.7    72.2    33.4   8.2     43.1    13.8   9.6     45.2    15.8    28.4
                       RLHFlow-DeepSeek-8B*       24.2    98.4    38.8   21.4    80.0    33.8   10.1    51.0    16.9   10.9    51.9    16.9    26.6
                       Llemma-PRM800K-7B          36.7    71.0    48.4   39.2    47.8    43.1   33.1    25.1    28.5   35.4    31.5    33.4    38.4
                       Skywork-PRM-7B*            61.8    82.9    70.8   43.8    62.2    53.6   17.9    31.9    22.9   14.0    41.9    21.0    42.1
                       ReasonEval-7B              26.1    95.3    41.0   35.7    77.6    48.9   27.5    55.2    36.7   27.0    60.6    37.4    41.0
                       Qwen2.5-Math-7B-PRM800K*   53.1    95.3    68.2   48.0    90.1    62.6   35.7    87.3    50.7   29.8    86.1    44.3    56.5
                       ⋆R-PRM-7B-SFT              66.2    92.7    77.2   60.3    88.2    71.6   48.6    77.3    59.6   40.1    75.5    52.3    65.2
                       ⋆R-PRM-7B-DPO              72.0    91.7    80.7   71.2    83.5    76.9   60.2    67.8    63.8   55.5    65.6    60.1    70.4
                     Table 1: Performance on ProcessBench. ⋆ indicates our models. Results marked with * are from Zhang et al.. Bold
                     indicates the best within PRMs. For LLM-as-judge baselines, we sample 10 trajectories and apply majority voting
                     to align with our method. The correct and error indicate accuracy on correct and incorrect samples, respectively.
                      ModelName                          Simplicity                   Soundness                        Sensitivity           Overall
                                                   NR.     NCL.    Avg.    ES     SC.    DC.     CI    Avg.    PS     DR.    MS.     Avg.
                      LLM-as-judge, Proprietary language models
                      GPT-4o*                      57.0    62.4    59.7   72.0    69.7   70.7   71.1   70.9    62.5   65.7   99.2    75.8     66.8
                      o1-mini*                     65.6    63.7    64.6   74.5    67.7   73.8   72.3   72.1    61.8   64.8   100.0   75.5     68.8
                      PRMs
                      Math-Shepherd-7B*            44.0    50.3    47.1   49.4    44.5   41.3   47.7   45.7    47.2   48.6   86.1    60.7     47.0
                      Math-PSA-7B                  47.6    55.1    51.3   56.5    49.4   47.1   54.2   51.8    51.7   54.1   88.9    64.9     52.3
                      RLHFlow-Mistral-8B*          46.1    47.3    46.7   56.6    55.1   54.4   63.8   57.5    51.5   56.2   97.9    68.5     54.4
                      RLHFlow-DeepSeek-8B*         46.4    48.9    47.6   55.7    55.0   53.2   66.2   57.5    49.0   55.4   99.8    68.1     54.2
                      Llemma-PRM800k-7B*           49.3    53.4    51.4   56.4    47.1   46.7   53.3   50.9    51.0   53.5   93.6    66.0     52.0
                      Skywork-PRM-7B*              35.7    41.2    38.4   36.7    29.1   30.6   34.4   32.7    36.8   37.4   88.8    54.3     36.2
                      ReasonEval-7B*               61.0    50.1    55.5   62.1    65.9   61.5   66.0   63.9    55.6   58.0   99.5    71.0     60.0
                      Qwen2.5-Math-7B-PRM800K      48.6    47.8    48.2   62.1    59.4   58.7   68.5   62.2    52.9   64.0   99.8    72.2     58.3
                      ⋆R-PRM-7B-SFT                52.7    64.7    58.7   70.1    62.7   63.4   69.5   66.4    61.4   67.4   98.3    75.7     64.9
                      ⋆R-PRM-7B-DPO                52.2    58.2    55.2   72.1    69.1   68.9   75.0   71.2    61.2   69.5   99.1    76.6     66.8
                     Table 2: Performance on PRMBench. ⋆ represents the models we trained. Results marked with * come from Zhang
                     et al. Bold text denotes the best results within PRM.
                           1 through 3.                                                4.2    ExperimentResults
                        • Qwen2.5-Math-7B-PRM800K(Zhengetal.,
                           2024): Qwen2.5-Math-7B-Instruct fine-tuned                  R-PRMachieveshighevaluationaccuracyeffi-
                           onthe PRM800Kdataset.                                       ciently. As detailed in Table 1 and Table 2, our
                                                                                       SFT approach demonstrates strong performance,
                     Implementation details: We prompt LLaMA3.3-                       achieving F1 scores of 65.2 on ProcessBench
                     70B-Instruct to generate four evaluation trajecto-                and 64.9 on PRMBench. These results signifi-
                     ries per PRM800K case, yielding approximately                     cantly outperform state-of-the-art baselines, includ-
                     289kSFTand269kDPOsamples. Qwen2.5-Math-                           ing Qwen2.5-Math-7B-PRM800K (the strongest
                     7B-Instruct is fine-tuned for one epoch with batch                PRM800K-basedmethod), by 8.7 and 6.6 points,
                     size 128 and learning rates of 5e-6 (SFT) and 5e-7                respectively. The model’s capabilities are further
                     (DPO). We reserve 20k samples for validation and                  elevated through meta-optimization, leading to re-
                     select the checkpoint with the lowest validation                  markable F1 scores of 70.4 on ProcessBench and
                     loss. Unless stated otherwise, results are reported               66.8 on PRMBench. These improvements high-
                     using ten evaluation trajectories per step.                       light the potential of our reasoning driven evalua-
                                                                                 13454
