                                  Agent57: Outperforming the Atari Human Benchmark
                            Adrià PuigdomènechBadia*1 Bilal Piot*1 Steven Kapturowski*1 Pablo Sprechmann*1
                                                   Alex Vitvitskyi1 Daniel Guo1 Charles Blundell1
                                        Abstract
                    Atari games have been a long-standing bench-
                    markinthereinforcementlearning(RL)commu-
                    nity for the past decade. This benchmark was
                    proposed to test general competency of RL al-
                    gorithms. Previous work has achieved good av-
                    erage performance by doing outstandingly well
                    on many games of the set, but very poorly in
                    several of the most challenging games. We pro-
                    pose Agent57, the ﬁrst deep RL agent that out-
                    performs the standard human benchmark on all
                    57Atari games. To achieve this result, we train a
                    neural network which parameterizes a family of
                    policies ranging from very exploratory to purely             Figure 1. Number of games where algorithms are better than the
                    exploitative. We propose an adaptive mechanism               human benchmark throughout training for Agent57 and state-of-
                    to choose which policy to prioritize throughout              the-art baselines on the 57 Atari games.
                    the training process. Additionally, we utilize a
                    novelparameterizationofthearchitecturethatal-                Deep Q-Networks (DQN ; Mnih et al., 2015) was the ﬁrst
                    lows for more consistent and stable learning.                algorithm to achieve human-level control in a large num-
                                                                                 ber of the Atari 2600 games, measured by human nor-
                                                                                 malized scores (HNS). Subsequently, using HNS to assess
               1. Introduction                                                   performance on Atari games has become one of the most
               TheArcadeLearningEnvironment(ALE; Bellemareetal.,                 widely used benchmarks in deep reinforcement learning
               2013) was proposed as a platform for empirically assess-          (RL), despite the human baseline scores potentially under-
               ing agents designed for general competency across a wide          estimating human performance relative to what is possi-
               range of games. ALE offers an interface to a diverse set          ble (Toromanoff et al., 2019). Nonetheless, human bench-
               of Atari 2600 game environments designed to be engaging           mark performance remains an oracle for “reasonable per-
               and challenging for human players. As Bellemare et al.            formance” across the 57 Atari games. Despite all efforts,
               (2013) put it, the Atari 2600 games are well suited for eval-     nosingleRLalgorithmhasbeenabletoachieveover100%
               uating general competency in AI agents for three main rea-        HNS on all 57 Atari games with one set of hyperparam-
               sons: (i) varied enough to claim generality, (ii) each inter-     eters. Indeed, state of the art algorithms in model-based
               esting enough to be representative of settings that might be      RL,MuZero(Schrittwieser et al., 2019), and in model-free
               faced in practice, and (iii) each created by an independent       RL, R2D2 (Kapturowski et al., 2018) surpass 100% HNS
               party to be free of experimenter’s bias.                          on 51 and 52 games, respectively. While these algorithms
                                                                                 achieve well above average human-level performance on
               Agents are expected to perform well in as many games as           a large fraction of the games (e.g. achieving more than
               possible making minimal assumptions about the domain              1000% HNS), in the games they fail to do so, they often
               at hand and without the use of game-speciﬁc information.          fail to learn completely. These games showcase particu-
                                                                                 larly importantissuesthatageneralRLalgorithmshouldbe
                 *Equal contribution 1DeepMind. Correspondence to: Adrià         able to tackle. Firstly, long-term credit assignment: which
               Puigdomènech Badia <adriap@google.com>.                           decisions are most deserving of credit for the positive (or
               Proceedings of the 37th International Conference on Machine       negative) outcomes that follow? This problem is particu-
               Learning, Online, PMLR 119, 2020. Copyright 2020 by the au-       larly hard when rewards are delayed and credit needs to
               thor(s).                                                          be assigned over long sequences of actions, such as in the
