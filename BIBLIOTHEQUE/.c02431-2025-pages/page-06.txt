                 APreprint.
                 3.1    Stage 1: Symbolic Object Detection
                 Thefirststageconvertstherawpixelgridintoastructuredsymbolicscenegraph. Thisprocessisentirelydeterministic,
                 ensuringthatidenticalperceptualinputsalwaysproduceidenticalsymbolicoutputs,whichiscriticalforruleinduction.
                     Wefirstprofilethegridtoidentifythebackgroundcolor,whichisdefinedasthemostfrequentpixelcolor(typically
                 ’0’ or black). Then, a Breadth-First Search (BFS) algorithm iterates over all non-background pixels, grouping them
                 into connected components based on 8-way adjacency.
                     For each detected object, we compute a comprehensive feature vector:
                       • Spatial Properties: Coordinates of the boundary box (y    , x   , y   , x    ), centroid (mean x,y) and
                                                                                min   min max max
                         pixel count (area).
                       • Geometric Properties: A pixel-geometry hash (for fast identity comparison) and a list of relative pixel
                         coordinates.
                       • Color Properties: A histogram of colors present within the object.
                       • Topological Properties: We perform cavity detection by running a secondary BFS on the background color
                         within an object’s bounding box. This allows us to distinguish solid shapes from hollow frames, a critical
                         feature in many ARC tasks.
                     TheaveragedimensionofthegridintheARC-AGI-2subsetthatweusedwas20. Thisstagetransformsa20×20
                 grid of 400 pixels into a structured list of, for example, ”3 objects,” each with a precise set of symbolic attributes.
                 This deterministic extraction, which averages ∼15ms per grid, provides a stable symbolic foundation for subsequent
                 reasoning stages, avoiding the stochasticity and brittleness of learned feature extractors.
                 3.2    Stage 2: Neural-Guided Hypothesis Generation
                 Thesecondstageidentifieswhichtransformationsmightexplainthechangesbetweenaninputandoutputpair. Instead
                 of a computationally intractable brute-force search, we use o4-mini as a fast, neurally-guided hypothesis generator.
                     Wedefinealibraryof22compositional”UnitPatterns”thatformourDomain-SpecificLanguage(DSL)forvisual
                 reasoning(seeFigure2andAppendixA).Thesepatternsaremorecomplexthansimpleatomicoperationsanddescribe
                 commonARCreasoningtasks. Examplesinclude:
                       • Filling Operations: Horizontal Fill, Vertical Fill,Diagonal Fill,Cavity Fill.
                       • Pattern/Repetition: Creating Patterns based on starting Objects,Alternating Pattern
                         Filling,Pattern Matching Fill / Remove.
                       • ObjectManipulation: Connecting Bridges,Object Translation Based on Goal,Falling
                         Down (Gravity-Effect).
                       • LogicalOperations: Find Objects...           and Color Them,Remove Objects...               in a Particular
                         Sequence,Symmetry-Based Pattern.
                     For each individual training example (or ”sub-question”) (I ,O ), we use a self-consistency (SC) mechanism to
                                                                              i  i
                 robustly identify candidate patterns. We query o4-mini N = 10 times with the same prompt, which includes the
                 symbolic object lists from Stage 1 and the full 22-pattern library (see Appendix B). This prompt specifically asks
                 the model to return a structured JSON object and a ‘reason‘ for its detection. We found this ”chain-of-thought” style
                 requirement, forcing the model to justify its choice, significantly improves accuracy. On our internal validation subset,
                 this method achieved 70% accuracy in identifying the correct primary Unit Pattern.
                     Each of the 10 queries returns a JSON detection, such as:
                 {
                       "pattern_name": "Falling Down (Gravity-Effect)",
                       "pattern_detected": true,
                       "params": {
                             "gravity_direction": "downward",
