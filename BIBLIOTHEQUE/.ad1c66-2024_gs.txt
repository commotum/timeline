                                              International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                                                                     Contents lists available at ScienceDirect
                                        InternationalJournalofAppliedEarthObservationand
                                                                           Geoinformation
                                                              journal homepage: www.elsevier.com/locate/jag
            Pointcloudsemanticsegmentationwithadaptivespatialstructuregraph
            transformer‚ú©
                                               ‚àó
            Ting Hana, Yiping Chena, , Jin Maa, Xiaoxue Liub, Wuming Zhanga, Xinchang Zhangc,d,e,
            HuajuanWangf
            a School of Geospatial Engineering and Science, Sun Yat-Sen University, Zhuhai, 519082, China
            b Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University, Xiamen, 361005, China
            c School of Geography and Remote Sensing, Guangzhou University, Guangzhou, 510006, China
            d College of Geography and Remote sensing Sciences, Xinjiang University, Urumqi, 830046, China
            e Guangdong Urban and Rural Planning and Construction Intelligent Service Engineering Technology Research Center, Guangzhou, 511300, China
            f Zhuhai Surveying and Mapping Institution, Zhuhai, 519000, China
            A R T I C L E      I N F O                       A B S T R A C T
            Keywords:                                        With the rapid development of LiDAR and artificial intelligence technologies, 3D point cloud semantic
            Graphtransformer                                 segmentation has become a highlight research topic. This technology is able to significantly enhance the
            Point cloud                                      capabilities of building information modeling, navigation and environmental perception. However, current
            LiDAR                                            deep learning-based methods primarily rely on voxelization or multi-layer convolution for feature extraction.
            Semanticsegmentation                             These methods often face challenges in effectively differentiating between homogeneous objects or structurally
            Deeplearning                                     adherent targets in complex real-world scenes. To this end, we propose a Graph Transformer point cloud
                                                             semantic segmentation network (ASGFormer) tailored for structurally adherent objects. Firstly, ASGFormer
                                                             combines Graph and Transformer to promote global correlation understanding in the graph. Secondly, spatial
                                                             index and position embedding are constructed based on distance relationships and feature differences. Through
                                                             a learnable mechanism, the structural weights between points are dynamically adjusted, achieving adaptive
                                                             spatial structure within the graph. Finally, dummy nodes are introduced to facilitate global information
                                                             storage and transmission between layers, effectively addressing the issue of information loss at the terminal
                                                             nodes of the graph. Comprehensive experiments are conducted on the various real-world 3D point cloud
                                                             datasets, analyzing the effectiveness of proposed ASGFormer through qualitative and quantitative evaluations.
                                                             ASGFormer outperforms existing approaches with of 91.3% for OA, 78.0% for mAcc, and 72.3% for mIoU on
                                                             S3DIS dataset. Moreover, ASGFormer achieves 72.8%, 45.5%, 81.6%, 70.1% mIoU on ScanNet, City-Facade,
                                                             Toronto 3D and Semantic KITTI dataset, respectively. Notably, the proposed method demonstrates effective
                                                             differentiation of homogeneous structurally adherent objects, further contributing to the intelligent perception
                                                             and modeling of complex scenes.
            1. Introduction                                                                 environmental perception (Meyer et al., 2023; Cotella, 2023; Han et al.,
               LiDAR sensor technology is advancing rapidly. The acquisition and            2023). However, point cloud semantic segmentation still suffers from a
            processing of 3D point cloud holds significant value in the fields of com-      series of complex challenges. Point clouds with similar structures tend
            puter vision (Xiao et al., 2023), geographic spatial information (Stilla        to obscure distinct features. Specifically, the adhesion and overlap of
            and Xu, 2023) and engineering (Geng et al., 2023). As a representation          objects and structures significantly increase the difficulty of point cloud
            of 3D data, point cloud accurately captures the environmental char-             segmentation. The challenges posed by adherent objects and similar
            acteristics of the real scenes, finding widespread applications such as         structures in both indoor and outdoor scenes are shown in Fig. 1.
            building information modeling (BIM) (Liu et al., 2023b), indoor posi-               In recent years, the development of deep learning has significantly
            tioning and navigation (Jiang et al., 2023b; Li et al., 2023), and interior     advanced the analysis and perception of point cloud (Xu et al., 2023b;
             ‚ú© This work was supported by the National Natural Science Foundation of China under Project 42371343, and Basic and Applied Basic Research Foundation
            of Guangdong Province, China with Grant No. 2024A1515010986.
              ‚àó Corresponding author.
                E-mail address: chenyp79@mail.sysu.edu.cn (Y. Chen).
            https://doi.org/10.1016/j.jag.2024.104105
            Received 8 April 2024; Received in revised form 27 July 2024; Accepted 16 August 2024
            Available online 7 September 2024 
            1569-8432/¬© 2024 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by- 
            nc-nd/4.0/).   
            T. Han et al.                                                                   International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
            Fig. 1. Illustration of key challenge in semantic segmentation of real-world point cloud. The proposed method demonstrates a significant advantage in addressing the segmentation
            of adherent objects.
            Zhang et al., 2023a; He et al., 2023; Jiang et al., 2023a; De G√©lis
            et al., 2023). Early methods, such as PointNet (Qi et al., 2017a)
            and PointNet++ (Qi et al., 2017b), were groundbreaking in their
            application of convolutional neural networks (CNNs) to point cloud,
            allowing for processing in an unstructured manner. However, due to
            the inherent limitations of fixed-size and fixed-pattern convolutional
            kernels, these approaches overlook structural relationships between
            points of the same object, making it challenging of segmentation to
            effectively handle adherent and occluded instances. To address these           Fig. 2. Diagram of graph structure with its complexity. (a) Fully connected graph, (b)
            limitations, researchers have developed various methods that better            3-regular graph, and (c) virtual node graph. The complexities are Ì†µÌ±Ç(Ì†µÌ±â2), Ì†µÌ±Ç(Ì†µÌ±â + Ì†µÌ∞∏),
            capture local features and structural relationships. Sparse convolution        Ì†µÌ±Ç(Ì†µÌ±â ), respectively, where Ì†µÌ±â and Ì†µÌ∞∏ denote the vertex and edge in graph.
            techniques, such as those used in MinkowskiNet (Choy et al., 2019)
            andSPVCNN(Tangetal.,2020),efficientlyprocess3Ddatabyfocusing
            onnon-emptyspaces,improvingcomputationalefficiencyandaccuracy                      The graph represents the structure and topology of points, while
            in capturing local features. Transformer-based models, such as Point           the Transformer learns the similarity of the features and attributes of
            Transformer (Zhao et al., 2021) and Stratified Transformer (Lai et al.,        points and dynamically updates the weights of points in the graph. The
            2022), leverage self-attention mechanisms to dynamically capture both          local structure information learned in Graph is incorporated into the
            local and global relationships between points. While these methods             attention mechanism to obtain the feature neighbors and the weight
            have shown superior performance in tasks like segmentation and classi-         of the edges. The graph priors can be added as inductive biases in
            fication, representing a significant advancement, they still struggle with     Transformer to effectively handle point cloud relationships. Local fea-
            capturing complex structural relationships in scenes with homogeneous          tures of graph represent the structural and topological relationships of
            structures.                                                                    nodes, while Transformer consider node feature and attribute similarity
               To provide a more detailed description of local structure and ob-           through both local and global features. Therefore, Graph Transformer
            ject relationships, point cloud is represented as the graph with sets          maintains the sparsity and locality of graphs while incorporating long-
            of vertices and edges. 3DGNN (Qi et al., 2017c) builds a k-nearest             range dependencies and overall graph characteristics, which is crucial
            neighbor graph on the point cloud, employing a graph-based strat-              for understanding complex information.
            egy for message passing in point cloud processing. DeepGCNs (Li                    The global association of the graph is closely related to computa-
            et al., 2019a) incorporates the idea of dilated convolutions into graph        tional complexity. The global message passing mechanism of proba-
            structure to construct non-local features. However, point cloud often          bilistic graph models is widely used in graph and graph Transformer,
            exhibit non-uniformity and inconsistent density distribution, making           as shown in Fig. 2. GraphGPS (Ramp√°≈°ek et al., 2022) combines local
            traditional graph representations inadequate in fully considering local        message passing and global attention mechanism. Such fully connected
            and global relationships. 3D-GCN (Lin et al., 2020) utilizes deformable        graph has poor scalability and results in computational complexity that
            convolutional kernels to learn 3D shapes and weights, extracting point         scales quadratically with the number of nodes. This cost is unacceptable
            cloud structures of arbitrary shapes and sizes. The excessive local            for point cloud. To accommodate dense point sets and improve scalabil-
            focus leads to insufficient learning of the global relationships in point      ity, Performer (Choromanski et al., 2020) allows for a sparse attention
            cloud, resulting in unclear segmentation of adherent indoor objects            mechanism in the graph. Redundant costs are reduced by avoiding the
            with similar structures. GACNet (Wang et al., 2019a) uses attention            use of full attention. K-regular graph is able to randomly select edges as
            to assign weights in the graph for adapting to the spatial information.        attention pattern for message passing, but the contributions of points
            Dynamic Graph (Wang et al., 2019b) applies convolution operations              are hard to distinguish. Meanwhile, sparse graph and attention exhibit
            on the edge set of the graph to explore geometric relationships, giving        lower adaptability in structurally compact indoor scenes. We need to
            it translational invariance and non-local characteristics. Therefore, the      find a trade-off between the efficiency of global message passing and
            representation of point cloud spatial structure must fully consider the        structural perception to establish an effective Graph Transformer for
            fusion of local and contextual features based in the distance-weighted         high performance semantic segmentation, enabling the segmentation
            relationship in the neighboring context to segment adherent instances.         of adherent point cloud objects.
                                                                                        2 
             T. Han et al.                                                                                International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                                                                                                         2018) unified graph convolution and vector learning to handle the
                                                                                                         topological structure of graph. Point-GNN (Shi and Rajkumar, 2020)
                                                                                                         simultaneously used point features and edge features to capture a com-
                                                                                                         prehensive representation. Point features represent the characteristics
                                                                                                         of each point, while edge features reflect the relationships between
                                                                                                         points. A rich feature representation is built by aggregating neighbor
                                                                                                         points at different levels. RGCNN (Te et al., 2018) utilized regularized
                                                                                                         graph network for point cloud segmentation. LocalSpecGCN (Wang
                                                                                                         et al., 2018a) used spectral graph convolution and graph pooling
                                                                                                         on local graphs to overcome the problem of neglecting point layout,
             Fig. 3. Schematic of different graph models. (a) Traditional graph convolutional net-       effectively strengthening the topological relationships between points.
             work (GCN); (b) Dynamic/Deformable GCN/graph attention network; (c) The proposed            Topology characterize the structural relationship of point cloud. How-
             graph Transformer architecture with higher scalability, stronger feature passing and        ever, it is difficult to distinguish structurally similar or homogeneous
             representation capabilities.                                                                objects solely based on topology.
                                                                                                             The weight update of each point is not only related to the fea-
                                                                                                         tures of that point, but also to the features of the nearest neighbors
                 To this end, we propose a Graph Transformer 3D point cloud                              around it. DGCNN (Wang et al., 2019b) introduced the concept of dy-
             semantic segmentation network tailored for structurally adherent ob-                        namicgraphandGeomGCNN(SrivastavaandSharma,2021)usedlocal
             jects, named Adaptive-Structure Graph Transformer (ASGFormer). The                          geometric information to augment the vertex representations. Deep-
             network addresses two key challenges: (a) the difficulty in effectively                     GCNs (Li et al., 2019a) incorporated dilated convolution into graph
             segment homogeneous adherent structures in large-scale point cloud                          to preserve structural information with dynamic manner. In traditional
             and (b) high computational overhead associated with global message                          graph strategies, convolutional feature correspondences become indis-
             passing mechanism. Firstly, spatial distance feature index is incor-                        tinguishable among point cloud, giving rise to an intrinsic limitation
             porated into the graph model, adaptively learning the neighborhood                          in poor distinctive feature learning. SphereNet (Liu et al., 2022b)
             spatial structural relationships and feature differences of the point                       conducted graph analyses in the spherical coordinate system for the
             cloud. Secondly, dynamic weights in graph attention effectively rep-                        complete identification of 3D graph structure, and used spherical mes-
             resent global features in the neighbor graph, as shown in Fig. 3. To our                    sage passing to perform large-scale graph learning. 3DGraphSeg (Geng
             knowledge,thisisthefirst instance of such an approach. Finally, virtual                     et al., 2023) constructed local embedding super-point graph, and pro-
             nodes are placed between layers to optimize graph learning, simulating                      posed a gated integration GCN to segment the graph. Fixed-shape
             global message passing, reducing information loss in sampling while                         and fixed-range convolutions cannot adapt to diverse structures, and
             maintaining efficiency. Our main contributions are summarized as                            the differences in adherent objects become smoothed within a local
             follows:                                                                                    receptive field.
                                                                                                             Inspired by the attention strategies, many efforts were made to
                  ‚Ä¢ A pyramid Graph Transformer with dynamic adaptive capabilities                       allocate appropriate weights to different points. Geometric attentional
                    is proposed to effectively address the challenges of adherent                        EdgeConv (Cui et al., 2021) incorporated extrinsic geometric topologi-
                    structures in complex scenes.                                                        cal prior into EdgeConv of DGCNN, which captures the intrinsic feature
                  ‚Ä¢ Spatial distance and feature differences are utilized to optimize                    likelihood of point cloud. PU-GACNet (Han et al., 2022) assigned
                    the structural relationships of the point cloud.                                     different attentional weights to combine spatial positions and feature
                  ‚Ä¢ The spatial relationships and global features are correlated in the                  attributes dynamically. GACNet (Wang et al., 2019a) proposed dynamic
                    graph attention to overcome the constraint of limited expression                     kernels to adapt to the structure of an object. And SGAT (Ye and Ji,
                    in local feature propagation and improve the ability to segment                      2021) learned sparse attention coefficient. 3D-GCN (Lin et al., 2020)
                    adherent objects.                                                                    developed deformable kernels, and focused on scale-invariant and shift
                  ‚Ä¢ The virtual nodes in hierarchical layers aggregate and distribute                    properties of point cloud. AGConv (Wei et al., 2023) also generated
                    global context into multi-scale features, reducing information loss                  deformable and adaptive kernels based on the dynamically learned
                    at the graph ends and promoting fine-grained understanding.                          features, outperforming in the fields of completion, denoising, and
                                                                                                         registration. Our goal is to learn relationships between objects by dy-
                                                                                                         namically updating the weights of point features. It is crucial for point
             2. Related work                                                                             cloud segmentation that taking into full consideration the differences
                                                                                                         in features and spatial distance relationships. Adherent objects are able
             2.1. Graph-based semantic segmentation                                                      to distinguished by identifying inherent differences, either in features
                                                                                                         or structure.
                 Point cloud segmentation methods typically focus on the feature                         2.2. Transformer-based semantic segmentation
             extraction of individual points and do not take into account the rela-
             tionships between adjacent points. The feature updates of each point                            As a sequence of spatial data, 3D point cloud naturally lends it-
             could not be independent of each other. Drawing the features of corre-                      self to application of Transformer. Point Transformer (Zhao et al.,
             lated or uncorrelated points closer or farther is beneficial for robustly                   2021) constructed self-attention layer by vector attention for point
             distinguishing different objects. NAS-GNNs (Xu et al., 2023a) illustrated                   cloud shape classification and semantic segmentation. PCT (Guo et al.,
             that GNNs possess expressive power without training. 3DGNN (Qi                              2021) proposed an offset-attention with implicit Laplace operators and
             et al., 2017c) built a k-nearest neighbor graph on the 3D point cloud.                      normalization refinement for Transformer blocks. PAT (Zhang et al.,
             Joint reasoning about appearance and geometric structure, every node                        2022a) computed attention map in the smaller set and used multi-
             of graph dynamically updates its current status and incomes mes-                            scale attention to build attentions among features of different scales.
             sage passing, and finally predicts the semantic class of each node.                         SPT (Robert et al., 2023) introduced hierarchical superpoint structure,
             SPG (Landrieu and Simonovsky, 2018) segmented the point cloud into                          and then a self-attention was employed to capture the relationships
             geometrically homogeneous elements and exploits the graph to encode                         between superpoints at multiple scales. However, refining the segmen-
             their relationships. Using topological graphs for feature extraction and                    tation of local region risks compromising the structural integrity of the
             processing of point clouds is a very useful idea. 3DEGCN (Cho and Choi,                     instances.
                                                                                                     3 
           T. Han et al.                                                                 International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
           Fig. 4. Overall framework diagram of Adaptive-Structure Graph Transformer (ASGFormer) for point cloud semantic segmentation; The network is designed as an end-to-end
           pyramid architecture, from Section 3.1; Multi-layer Adaptive Graph Transformer (AGT) blocks are incorporated into the architecture to dynamically learn the structural weights and
           feature relationships of points, from Section 3.2; The utilization of Virtual Nodes for Graph Optimizing learning (VNGO) are applied between network hierarchies, from Section 3.3.
               Point Transformer V2 (Wu et al., 2022) presented the more ef-
           fective group vector attention with weighted encoding and additional
           position encoding multiplier. Further, the Point Transformer V3 (Wu
           et al., 2023) has focused on overcoming the trade-off between accuracy
           and efficiency within the Transformer architectures. Considering the
           implicit relationship between semantic and instance segmentation, the
           unified Transformer-based framework (Kolodiazhnyi et al., 2023) is
           established to perform semantic and instance segmentation consistently
           with the learnable kernels. ConDaFormer (Duan et al., 2024) built the
           local window by three orthogonal 2D planes to capture local priors. The                          Fig. 5. Illustration of graph pooling.
           inter-connections between neighboring local regions remain underex-
           plored, despite their significance in Transformer-based 3D point cloud
           models.                                                                      enhance scalability of graph. AutoGT (Zhang et al., 2022b) proposes an
               LCPFormer (Huang et al., 2023) exploited the message passing in          encoding-aware estimation strategy to jointly optimized Transformer
           neighbor regions and made their representations more discriminative          and graph learning. However, these studies have not yet been ap-
           and informative. Local Transformer (Wang et al., 2022) used cross-skip       plied to point cloud analysis and understanding. Upon modeling graph
           selection of neighbors to capture similarities and geometric structure in    representation with learnable weights, Transformer is embedded to
           a larger receptive fields. Stratified Transformer (Lai et al., 2022) adopt   encode weights and point features, determining the similarity between
           contextual relative position encoding to adaptively capture position         points. Semantic segmentation is achieved by combining graph and
           information in a stratified way. SPoTr (Park et al., 2023) designed          Transformer, and the relationships between points are dynamically
           self-positioning point-based global cross-attention to adaptively locate     optimized to separate adherent objects.
           points based on the input shape. OctFormer (Wang, 2023) introduced
           the dilated octree attention to expand the receptive field for shape-        3. Method
           robust of point cloud understanding. Transformer broke the limitations
           of local receptive fields and could learn the relative positional rela-         To tackle the challenge of effectively distinguishing structurally
           tionship between points. However, the weights influenced by relative         adherent objects in existing point cloud semantic segmentation al-
           position and features remained non-learnable in previous methods.            gorithms, we introduce a Graph Transformer network named ASG-
           2.3. Incorporating graph into transformer                                    Former with dynamic adaptive capabilities. The overall framework is
                                                                                        illustrated in Fig. 4.
               In the graph, nodes and edges represent objects and relations be-        3.1. ASGFormer architecture
           tween objects, respectively. In Transformer, nodes can function as
           sequential units, and edges can serve as the links between these units.         The proposed ASGFormer is designed as an end-to-end semantic
           Applying Transformer to graph significantly enhances the representa-         segmentation architecture with pyramid structure. The backbone net-
           tional capability of the graph, making it suitable for searching relation-   workcomprisesencoderanddecodermodulesemployinguniformscale
           ships between points with larger non-local receptive fields. RT (Diao        strategy, as depicted in Fig. 4(a). The five stages in the encoder are
           and Loynd, 2022) generalizes Transformer attention to consider and           denoted as {Ì†µÌ±Ü ,Ì†µÌ±Ü ,Ì†µÌ±Ü ,Ì†µÌ±Ü ,Ì†µÌ±Ü }, where Ì†µÌ±Ü is a MLP layer, and Ì†µÌ±Ü ‚àíÌ†µÌ±Ü are
           update edge vectors in each Transformer layer, making it successful to                     1  2  3   4  5           1                      2   5
           greater expressivity of graph. GTNs (Yun et al., 2019) is Graph Trans-       composed of MLP and AGT blocks with {2,4,2,2} layers, respectively.
           former Networks, which involve identifying significant connections           We incorporated virtual nodes between each stage to facilitate global
           among unconnected nodes on the original graph, while learning node           message passing across the entire graph.
           representation. GraphGPS (Ramp√°≈°ek et al., 2022) employed position              To preserve the spatial structure of point cloud while reducing its
           embedding, local message passing and global attention mechanism to           resolution, we devised graph pooling to construct feature pyramid, as
                                                                                     4 
                 T. Han et al.                                                                                                              International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                 Fig. 6. Pipeline of weighted feature formation process. The proposed method is able to adaptively feedback the graph with structural importance according to the weighted
                 features, and adjust the graph by dynamically updating the weights.
                 shown in Fig. 5. Given the input feature Ì†µÌ∞πÌ†µÌ±† at Ì†µÌ±† stage, max pooling
                 is performed within the neighborhood of current center point. The
                 processing of graph pooling is formulated as follows:
                 Ì†µÌ∞π‚Ä≤Ì†µÌ±† = Ì†µÌ±öÌ†µÌ±éÌ†µÌ±• ‚àí Ì†µÌ±ùÌ†µÌ±úÌ†µÌ±úÌ†µÌ±ôÌ†µÌ±ñÌ†µÌ±õÌ†µÌ±î(Ì†µÌ±á Ì†µÌ±† ; Ì†µÌ±ó ‚àà Ì†µÌ±Å(Ì†µÌ±ñ))                                                         (1)
                                                    Ì†µÌ±ñÌ†µÌ±ó
                 where Ì†µÌ±Å represents the number of points in the local point set Ì†µÌ±É =
                                                              3
                 {Ì†µÌ±ÉÌ†µÌ±õ|Ì†µÌ±õ = 1,2,‚Ä¶,Ì†µÌ±Å;Ì†µÌ±ÉÌ†µÌ±õ ‚àà R }, Ì†µÌ±ó denotes the neighbors of the center
                 point Ì†µÌ±ñ, and Ì†µÌ±áÌ†µÌ±† represents the output of AGT block. As per Eq. (1),
                 the relative positional relationships within the local neighborhood can
                 embed spatial information into neighbor features in a non-learnable
                 manner. Following PointMeta (Lin et al., 2023), max pooling as a
                 special form of self-attention, exhibits sparsity and comparable feature                                                                         Fig. 7. Illustration of graph attention with details.
                 aggregation capability to learnable aggregation functions.
                       Through layer-wise graph pooling for point cloud down-sampling,
                 the output feature dimensions at each stage are respectively [Ì†µÌ±Å,32],                                                     is represented as Ì†µÌª•Ì†µÌ±ì               = Ì†µÌ∞π ‚àí Ì†µÌ∞π . Therefore, the weighted feature of
                 [Ì†µÌ±Å‚àï4,64], [Ì†µÌ±Å‚àï16,128], [Ì†µÌ±Å‚àï64,256], and [Ì†µÌ±Å‚àï256,512], where the first                                                                                     Ì†µÌ±ñÌ†µÌ±ó       Ì†µÌ±ñ      Ì†µÌ±ó
                 parameter represents the number of points, the second represents the                                                      neighboring point Ì†µÌ±ó with respect to vertex Ì†µÌ±ñ is formulated as Eq. (2):
                 feature channel dimension, and Ì†µÌ±Å denotes the number of points in the                                                     Ì†µÌª•Ì†µÌ∞π   =Ì†µÌ±ÄÌ†µÌ∞øÌ†µÌ±É(Ì†µÌª•Ì†µÌ±ì         ‚äïÌ†µÌª•Ì†µÌ±ù )                                                                         (2)
                 original input point cloud.                                                                                                   Ì†µÌ±ñÌ†µÌ±ó                 Ì†µÌ±ñÌ†µÌ±ó       Ì†µÌ±ñÌ†µÌ±ó
                       For the point-wise segmentation, we adopt the U-Net framework                                                       where ‚äï denotes the feature concatenation by channels. We implicitly
                 to design network that couples the encoder and decoder. The role of                                                       embed spatial information with relative positional relationships into
                 decoder is to interpolate the learned features with nearest neighbor                                                      the features. Later, different weights for various similarities are con-
                 interpolation to match the resolution of original point cloud. During                                                     structed based on relative position and feature differences, as shown in
                 this process, in Ì†µÌ±† stage, we search for three nearest neighbors of Ì†µÌ±† ‚àí 1                                                Eq. (3):
                 stage. Then, we calculate the weighted sum of features for these three                                                    Ì†µÌ±ä = Ì†µÌ±íÌ†µÌ±•Ì†µÌ±ù(Ì†µÌ∞πÌ†µÌ±ñÌ†µÌ±ó,Ì†µÌ±ò)                                                                                      (3)
                 nearest neighbors‚Äô distance to achieve feature mapping. The decoder                                                          Ì†µÌ±ñÌ†µÌ±ó   ‚àëÌ†µÌ±íÌ†µÌ±•Ì†µÌ±ù(Ì†µÌ∞πÌ†µÌ±ó,Ì†µÌ±ò)
                 is organized with a series of interpolation modules corresponding to                                                      where Ì†µÌ±ò represents the Ì†µÌ±òth channel to ensure the independence of
                 the encoder. These modules perform continuous interpolation to map                                                        channel features. Simultaneously, normalization is used to eliminate
                 down-sampled point set to the scale of the layer with higher reso-                                                        spatial differences caused by different scales. In contrast to traditional
                 lution. Therefore, the decoder stages are labeled to correspond with
                 the encoder as {Ì†µÌ±Ü‚Ä≤,Ì†µÌ±Ü‚Ä≤,Ì†µÌ±Ü‚Ä≤,Ì†µÌ±Ü‚Ä≤,Ì†µÌ±Ü‚Ä≤}. Additional skip connections fuse the                                                convolutional features, the obtained Ì†µÌ±äÌ†µÌ±ñÌ†µÌ±ó is a covariance matrix that
                                              1    2    3     4    5                                                                       records the relationships of point features. As shown in Fig. 6, the
                 features of corresponding scales between the encoder and decoder. In                                                      weighted feature is able to adaptively allocate weights for feature ag-
                 the final stage of the decoder, feature vector is computed for each point,                                                gregation based on the spatial position of points and feature differences,
                 and then MLP is employed to generate final segmentation results with                                                      preserving the spatial structure of objects.
                 Ì†µÌ±ÅÌ†µÌ±êÌ†µÌ±ôÌ†µÌ±† dimension.                                                                                                            Inspired by Point Transformer (Zhao et al., 2021), we design graph
                 3.2. Adaptive graph transformer block                                                                                     attention layer, as shown in Fig. 7. The input to this layer includes
                                                                                                                                           Ì†µÌ∞πÌ†µÌ±ñ, weighted feature Ì†µÌ±äÌ†µÌ±ñÌ†µÌ±ó, and the relative position Ì†µÌª•Ì†µÌ±ùÌ†µÌ±ñÌ†µÌ±ó. Attention
                       The Adaptive Graph Transformer (AGT) block is illustrated in                                                        calculation is performed using Ì†µÌ∞πÌ†µÌ±ñ as the query, Ì†µÌ±äÌ†µÌ±ñÌ†µÌ±ó as key and value,
                 Fig. 4(b). This module consists of multiple layers, including MLP,                                                        and Ì†µÌª•Ì†µÌ±ùÌ†µÌ±ñÌ†µÌ±ó as the position embedding, as description in Eq. (4):
                 graph attention layer, position embedding, graph pooling, and residual                                                    Ì†µÌ∞¥Ì†µÌ±°Ì†µÌ±°Ì†µÌ±õ = Ì†µÌ±†Ì†µÌ±úÌ†µÌ±ìÌ†µÌ±°Ì†µÌ±öÌ†µÌ±éÌ†µÌ±•((Ì†µÌºôÌ†µÌ∞πÌ†µÌ±ñ + Ì†µÌª•Ì†µÌ±ùÌ†µÌ±ñÌ†µÌ±ó) ‚ãÖ Ì†µÌºëÌ†µÌ±äÌ†µÌ±ñÌ†µÌ±ó)Ì†µÌ±äÌ†µÌ±ñÌ†µÌ±ó                                             (4)
                 connections. Given an input set of points Ì†µÌ±É = {Ì†µÌ±ÉÌ†µÌ±õ|Ì†µÌ±õ = 1,2,‚Ä¶,Ì†µÌ±Å;Ì†µÌ±ÉÌ†µÌ±õ ‚àà
                 R3}, where Ì†µÌ±Å denotes the number of points. Its corresponding features                                                    where Ì†µÌºô and Ì†µÌºë denote the MLP function. Relative position Ì†µÌª•Ì†µÌ±ùÌ†µÌ±ñÌ†µÌ±ó as
                 are formulated as Ì†µÌ∞πÌ†µÌ±É ‚àà RÌ†µÌ±ë, where Ì†µÌ±ë denotes the dimension of features.                                                 explicit position embedding is able to avoid the issue of imbalanced
                 We construct graph Ì†µÌ∞∫ = (Ì†µÌ±â,Ì†µÌ∞∏) in the point cloud, Ì†µÌ±â ‚àà Ì†µÌ±É represents                                                    neighborpoints‚Äôfeature caused by implicit position embedding. Finally,
                 the set of vertices, and Ì†µÌ∞∏ ‚äÜ |Ì†µÌ±â | √ó |Ì†µÌ±â | represents edge sets. Due to the                                              the output of graph attention is formulated as Eq. (5):
                 density-independent nature of the fixed-radius sampling strategy with                                                     Ì†µÌ±á   =Ì†µÌ±ÅÌ†µÌ±úÌ†µÌ±üÌ†µÌ±ö(Ì†µÌ∞¥Ì†µÌ±°Ì†µÌ±°Ì†µÌ±õ + Ì†µÌºôÌ†µÌ∞π )                                                                            (5)
                 point cloud, we employ the fix-radius farthest point sampling strategy                                                      Ì†µÌ±ñÌ†µÌ±ó                            Ì†µÌ±ñ
                 to select Ì†µÌ±Å(Ì†µÌ±ñ) = {Ì†µÌ±ó;(Ì†µÌ±ó,Ì†µÌ±ñ) ‚àà Ì†µÌ∞∏} ‚à™ {Ì†µÌ±ñ} neighbor points for each vertex                                                    We follow the strategy of ASSANet (Qian et al., 2021), applying
                 Ì†µÌ±ñ, determining the local geometric structure of each point set. We use                                                   an MLP before determining the neighborhood to reduce floating-point
                 Ì†µÌª•Ì†µÌ±ùÌ†µÌ±ñÌ†µÌ±ó to represent the positional offset between vertex Ì†µÌ±ñ and its neighbor                                            calculations. However, this strategy cannot set relative position as input
                 vertex Ì†µÌ±ó. Moreover, we use MLP to extract features for vertex Ì†µÌ±ñ and                                                     to the MLP, so the weighted features implicitly incorporating position
                 vertex Ì†µÌ±ó, denoted as Ì†µÌ∞πÌ†µÌ±ñ and Ì†µÌ∞πÌ†µÌ±ó, respectively. The feature difference                                                 information is able to naturally handle this. In summary, given Ì†µÌ±É and
                                                                                                                                      5 
                                  T. Han et al.                                                                                                                                                                                                                                   International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                                  Fig. 8. Different graph transformer architectures. Unlike GCN and Naive GT, AGT takes into account edge information and incorporates structural details. Furthermore, it optimizes
                                  the structure of General GT and avoids the dual-branch pathway strategy by using structural weights.
                                  Ì†µÌ∞πÌ†µÌ±É , the AGT block aggregates neighbor structural features and utilizes                                                                                                                                                                    The absolute position embedding is significantly useful at learning
                                  graph attention to generate new features for all points. During this                                                                                                                                                                         contextual representations of tokens in different positions. However,
                                  process, the relationships between points are dynamically adjusted to                                                                                                                                                                        it is not straightforward to capture vertex spatial information in the
                                  align with the optimal object structure.                                                                                                                                                                                                     graph. To this end, we consider topological priors represented by
                                            Analysis: Transformer is able to perform a weighted linear trans-                                                                                                                                                                  relative position information as relative position embedding. It contains
                                  formation on tokens based on their importance to update the current                                                                                                                                                                          more explicit knowledge between pairs of vertices. Graph Laplacian
                                  token. Graph neural networks (GNNs) update the feature of central                                                                                                                                                                            Matrix represents connectivity in terms of both adjacency and node
                                  vertex by aggregating the neighbor features on the graph. From the                                                                                                                                                                           degree of graph. It has to be said that the Laplacian matrix primarily
                                  connectivity structure, it can be observed that the Transformer and                                                                                                                                                                          focuses on local information and may have limited adaptability to dy-
                                  GNNs are coupled. Previous graph attention networks are sparse, con-                                                                                                                                                                         namic graph structures. In dynamic graphs, the relationships between
                                  sidering only neighbor vertices, while Transformer is a fully connected                                                                                                                                                                      nodes and edges undergo frequent changes. Therefore, more flexible
                                  architecture that takes into account all vertices. In contrast, Graph                                                                                                                                                                        approaches that can capture both global and local variations might be
                                  Transformer introduces the topological structural properties of graph                                                                                                                                                                        more suitable.
                                  on the basis of global context information, providing the model with                                                                                                                                                                                   Webelieve the edges connecting them should be considered in the
                                  structural spatial priors in high-dimensional space. We analyze different                                                                                                                                                                    correlation. We represent the relational information between (Ì†µÌ±ñ,Ì†µÌ±ó) as a
                                  model designs, as shown in Fig. 8.                                                                                                                                                                                                           vector Ì†µÌ±äÌ†µÌ±ñÌ†µÌ±ó. Let Ì†µÌ±äÌ†µÌ±ñÌ†µÌ±ó serve as an implicit position embedding, which is
                                            Whenconsidering the graph, the neighbor aggregation function can                                                                                                                                                                   able to complement global relationships while focusing on neighbors.
                                  be formulated to Eq. (6):                                                                                                                                                                                                                    Wemodify the attention score formula as follows:
                                  Ì†µÌ∞π‚Ä≤ = Ì†µÌºé(Ì†µÌªºÌ†µÌ∞π + ‚àë (Ì†µÌªΩÌ†µÌ∞π ))                                                                                                                                                                                         (6)                       Ì†µÌª∑         =Ì†µÌ±Ñ ‚ãÖÌ†µÌ∞æ +Ì†µÌ±ä                                                                                                                                                                                             (9)
                                      Ì†µÌ±ñ                         Ì†µÌ±ñ                                  Ì†µÌ±ó                                                                                                                                                                             Ì†µÌ±ñÌ†µÌ±ó               Ì†µÌ∞π              Ì†µÌ∞π                  Ì†µÌ±ñÌ†µÌ±ó
                                                                           Ì†µÌ±ó‚ààÌ†µÌ±Å(Ì†µÌ±ñ)                                                                                                                                                                                                                      Ì†µÌ±ñ              Ì†µÌ±ó
                                                                                                                                                                                                                                                                               Ì†µÌ∞æ            incorporates Ì†µÌ±ä , as explained in the previous context, allowing
                                  Further, the process of combining graph and Transformer (Naive Graph                                                                                                                                                                              Ì†µÌ∞πÌ†µÌ±ó                                                     Ì†µÌ±ñÌ†µÌ±ó
                                  Transformer) can be represented as updating the features of vertex Ì†µÌ±ñ                                                                                                                                                                        Eq. (9) to eliminate unnecessary addition computations. Position infor-
                                  through its neighbor vertex Ì†µÌ±ó, as described in the Eq. (7):                                                                                                                                                                                 mation is able to communicate directly with the attention mechanism
                                                     ‚àë                                                                                                                                                                                                                         in our approach. In order to explicitly capture the effect of neighbor-
                                  Ì†µÌ∞π‚Ä≤ =                            Ì†µÌ∞¥Ì†µÌ±°Ì†µÌ±°Ì†µÌ±íÌ†µÌ±õÌ†µÌ±°Ì†µÌ±ñÌ†µÌ±úÌ†µÌ±õ(Ì†µÌ±Ñ                    , Ì†µÌ∞æ          , Ì†µÌ±â         )                                                                                                             (7)                       ing points on Ì†µÌ±ñ, we still apply explicit positional embedding to Ì†µÌ±ÑÌ†µÌ∞π ,
                                      Ì†µÌ±ñ                                                              Ì†µÌ∞π            Ì†µÌ∞π           Ì†µÌ∞π                                                                                                                                                                                                                                                                                                                                                                    Ì†µÌ±ñ
                                                  Ì†µÌ±ó‚ààÌ†µÌ±Å(Ì†µÌ±ñ)                                              Ì†µÌ±ñ            Ì†µÌ±ó           Ì†µÌ±ó                                                                                                                                         preventing the imbalance of neighboring point features, as shown in
                                  This indicates that attention weight for each pair of (Ì†µÌ±ñ,Ì†µÌ±ó) is calculated                                                                                                                                                                  Eq. (4).
                                  based on the Ì†µÌ∞πÌ†µÌ±ñ and Ì†µÌ∞πÌ†µÌ±ó, and then the feature of vertex Ì†µÌ±ñ is updated                                                                                                                                                                               The third issue is the over-smoothing and over-squashing in GNNs.
                                  through the weighted accumulation of all attention weights for Ì†µÌ±ó. But                                                                                                                                                                       The receptive field of vertex becomes larger, leading to an increase in
                                  we still need to pay attention to the following aspects:                                                                                                                                                                                     the number of shared neighbors between two vertices. Therefore, the
                                            Firstly, leveraging edge features is a key factor in achieving a                                                                                                                                                                   feature embeddings of two vertices become more similar. As shown in
                                  dynamic graph. Edge features represent relationships between pairs of                                                                                                                                                                        Fig. 7, we add different MLPs as linear layers to increase the expres-
                                  vertices, reflecting implicit attention scores. While the score obtained                                                                                                                                                                     sive power of the network. These MLP layers serve as pre-processing
                                  after multiplication of Ì†µÌ±ÑÌ†µÌ∞π and Ì†µÌ∞æÌ†µÌ∞π represents the implicit information                                                                                                                                                                    step without message passing. Furthermore, we introduce residual skip
                                                                                                                  Ì†µÌ±ñ                        Ì†µÌ±ó                                                                                                                                 connections in both the overall network and AGT blocks, allowing
                                  about edge ‚ü®Ì†µÌ±ñ,Ì†µÌ±ó‚ü©, not all edges are necessarily available. An elementary                                                                                                                                                                   the updated features to reference the mappings from previous layer.
                                  attempt is to incorporate edge features into the attention calculation                                                                                                                                                                       This is a long-range residual connections from the input to last layers
                                  process as Eq. (8):                                                                                                                                                                                                                          instead of multiple short-range connections in the original Transformer.
                                  Ì†µÌ∞π‚Ä≤ = ‚àë (Ì†µÌ±†Ì†µÌ±úÌ†µÌ±ìÌ†µÌ±°Ì†µÌ±öÌ†µÌ±éÌ†µÌ±•(Ì†µÌ±Ñ                                                 ‚ãÖ Ì†µÌ∞æ           ) ‚ãÖ Ì†µÌ∞∏            Ì†µÌ±â        )                                                                                            (8)                       It alleviates the over-smoothing problems in the message passing while
                                      Ì†µÌ±ñ                                                              Ì†µÌ∞π              Ì†µÌ∞π                Ì†µÌ±ñ,Ì†µÌ±ó     Ì†µÌ∞π
                                                  Ì†µÌ±ó‚ààÌ†µÌ±Å(Ì†µÌ±ñ)                                              Ì†µÌ±ñ              Ì†µÌ±ó                          Ì†µÌ±ó                                                                                                                        alleviating vanishing gradient issues.
                                            This introduces a new issue, as the network must maintain a sep-                                                                                                                                                                             AnalyzingEq.(6),asageneralformofneighbormessagepassingop-
                                  arate pipeline to propagate edge attributes, which is redundant (See                                                                                                                                                                         eration, an assumption is proposed: the local topology of the graph may
                                  Fig. 8(c)). On the contrary, in our dynamic graph structure, the relation-                                                                                                                                                                   lead to over-squashing. If a vertex has many neighbors, it may reduce
                                  ships between edges are concealed in the weighted features Ì†µÌ±äÌ†µÌ±ñÌ†µÌ±ó, and                                                                                                                                                                       the mapping of features from distant points, leading to an information
                                  these edge relationships are learnable. Therefore, using the weighted                                                                                                                                                                        bottleneck, specific arguments can be found in Ref. Topping et al.
                                  features containing edge attributes and neighbor point features as Ì†µÌ∞æ                                                                                                                                                                        (2021). However, our approach introduces a new way of association
                                  and Ì†µÌ±â, attention scores can be computed based on similarity and                                                                                                                                                                             by incorporating edge weights while preserving the local geometric
                                  importance, see Eq. (4). Note that this edge attribute is sparse, in                                                                                                                                                                         topology. The attention weight enhances the feedback from available
                                  contrast to the fully connected Transformer.                                                                                                                                                                                                 vertices and also increases the available edges from distant vertices. In
                                            Secondly, position information is essential. In Transformer, the                                                                                                                                                                   addition, virtual nodes are also able to alleviate over-squashing to some
                                  structural bias is explicitly given as a form of position embedding.                                                                                                                                                                         extent, as will be analyzed in detail in the next section.
                                                                                                                                                                                                                                                                     6 
                          T. Han et al.                                                                                                                                                                           International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                                                                                                                                                                                                               4. Experiments
                                                                                                                                                                                                                       In the experiments section, we provide a detailed exposition of
                                                                                                                                                                                                               the experimental details and conduct thorough analysis. Firstly, the
                                                                                                                                                                                                               datasets, evaluation metrics, experimental platform, and parameter
                                                                                                                                                                                                               settings are elucidated. Subsequently, we comprehensively demonstrate
                                                                                                                                                                                                               the effectiveness and necessity of the proposed method through qual-
                                                                                                                                                                                                               itative and quantitative experiments. We compare our method with a
                                                                                                                                                                                                               series of state-of-the-art algorithms, drawing meaningful conclusions.
                                                                                                                                                                                                               Finally, controlled ablation experiments are set up to validate the
                                                                                                                                                                                                               necessity of each module.
                           Fig. 9. Diagram of information passing in fully connected manner and virtual node.                                                                                                  4.1. Datasets and evaluation metrics
                                  Graph Transformer introduces the topological structural properties                                                                                                                   S3DIS: The Stanford Large-Scale 3D Indoor Space Point Cloud
                          of graph on the basis of global context information, providing the model                                                                                                             Dataset (Armeni et al., 2016) comprises 271 rooms from six teach-
                          withstructural spatial priors in high-dimensional space. The advantages                                                                                                              ing and office areas in three different buildings (designated as Area
                          of our AGT are as follows: (1) Achieved stronger representation to cap-                                                                                                              1‚Äì6). The scenes encompass 11 different locations, including office
                          ture complex relationships between vertices. (2) Acquired the ability to                                                                                                             areas, conference rooms, stairways, educational and exhibition spaces,
                          model long-range dependency. (3) Alleviated issues of over-smoothing                                                                                                                 restrooms, open spaces, lobbies, personal offices, and hallways. The
                          and over-squashing in message passing.                                                                                                                                               semantic labels consist of 13 categories, such as table, chair, ceiling,
                                                                                                                                                                                                               floor, wall, door, window, sofa, and clutter. Out of the six regions, five
                          3.3. Virtual nodes to graph optimize                                                                                                                                                 are utilized for training, while Area 5 is reserved for validation and
                                                                                                                                                                                                               testing.
                                  In order to effectively preserve local and global information during                                                                                                                 ScanNet v2: ScanNet (Dai et al., 2017) is an RGB-D indoor envi-
                          the process of feature learning and transformation, we introduces the                                                                                                                ronments dataset that contains reconstructed indoor scenes with rich
                          specific virtual nodes connected to all vertices in the graph, as shown                                                                                                              annotations for 3D semantic labeling. It provides 1513 scenes for
                          in Fig. 4(c). The virtual node facilitates global message passing more                                                                                                               training and 100 scenes for testing. The dataset provides 40 class labels,
                          effectively without affecting the original vertices and edges attributes.                                                                                                            while only 20 of them are used for performance evaluation.
                                  Thevertex at the end of graph may be lost during the neighborhood                                                                                                                    City-Facade: City-Facade is a new dataset for real-world urban
                          update and sampling process, as shown in Fig. 9. Therefore, a mech-                                                                                                                  building facade semantic segmentation. This dataset is labeled with 8
                          anism is needed to maintain the consistency of graph structure. One                                                                                                                  semantic classes from various building styles, including wall, window,
                          strategy is to employ a fully connected approach for message passing,                                                                                                                door, roof, advertisement, air condition, rain shed, and balcony.
                          similar to conditional random fields (CRF), but it is evident that this                                                                                                                      Toronto 3D: Toronto 3D (Tan et al., 2020) is a large-scale urban
                          is not suitable for larger graphs. The virtual node is able to establish                                                                                                             outdoor point cloud dataset with 8 labeled object classes. This dataset
                          relationships with all vertices (adjacent to all vertices and connected to                                                                                                           is divided into four sections, and each section covers road about 250 m.
                          all edges), enabling the possibility of global message passing. This oper-                                                                                                           All points were preserved from real-world scenarios.
                          ates independently of AGT and serve as a complementary information                                                                                                                           Semantic KITTI: Semantic KITTI (Behley et al., 2019) is one of
                          aggregation for both global-vertex and global-edge contexts. Boosting                                                                                                                the largest urban point cloud dataset for semantic segmentation. This
                          Graph Structure Learning (Liu et al., 2022a) has demonstrated almost                                                                                                                 dataset covers 40 km with 4.5 billion points, and is labeled with 25
                          all graph models benefit from the inclusion of virtual nodes.                                                                                                                        classes. Semantic KITTI is more focused on autonomous driving tasks.
                                  For each layer, we introduce virtual node Ì†µÌ±ÉÌ†µÌ±è to facilitate message                                                                                                                 Evaluation Metrics: It includes the class-wise mean of intersection
                          passing, acting similar to a centroid in the graph to preserve its geo-                                                                                                              over union (mIoU), class-wise mean of accuracy (mAcc), and point-wise
                          metric structure. Ì†µÌ±ÉÌ†µÌ±è establishes edges Ì†µÌ∞∏Ì†µÌ±èÌ†µÌ±ó;Ì†µÌ±ó ‚àà Ì†µÌ±Å(Ì†µÌ±ñ) to all vertices in                                                                                                       overall accuracy (OA). For S3DIS, we use all three evaluation metrics.
                          the graph. The formulation of virtual node aggregation is expressed as                                                                                                               For ScanNet, mIoU is used for evaluation. And IoU is utilized for class-
                          follows:                                                                                                                                                                             level evaluation. For City-Facade, Toronto 3D, and Semantic KITTI, we
                          Ì†µÌ∞µÌ†µÌ±† =           1          ‚àë Ì†µÌ∞πÌ†µÌ±†Ì†µÌ∞∏Ì†µÌ±†                                                                                                                         (10)                  use mIoU for semantic segmentation evaluation.
                                       Ì†µÌ±Å(Ì†µÌ±ñ) Ì†µÌ±ó‚ààÌ†µÌ±Å(Ì†µÌ±ñ)             Ì†µÌ±ó     Ì†µÌ±èÌ†µÌ±ó
                          Note that we opted for summation, which is more easily normalized,                                                                                                                   4.2. Experimental setups and data augmentation
                          rather than concatenation. After aggregating the features of all vertices                                                                                                                    Experimental Setups: We trained our models using CrossEntropy
                          Ì†µÌ±ó   at the Ì†µÌ±ÉÌ†µÌ±è, the virtual node is passed to the next Ì†µÌ±† + 1 stage of the                                                                                                         loss with label smoothing, AdamW optimizer, an initial learning rate
                          network. Global features are distributed by the virtual node in the
                                                                                                                                                                                                                                  ‚àí2                                                            ‚àí4
                          graph of the next layer. The mathematical expression for distributing                                                                                                                Ì†µÌ±ôÌ†µÌ±ü   = 1Ì†µÌ±í             , and weight decay 10                                          with Cosine Decay, in a NVIDIA
                          features can be formulated as:                                                                                                                                                       80G A100 GPU. The best model on the validation set is utilized for
                          Ì†µÌ∞πÌ†µÌ±†+1 = Ì†µÌ∞µÌ†µÌ±†Ì†µÌ∞∏Ì†µÌ±†+1                                                                                                                                            (11)                  testing. For S3DIS and City-Facade segmentation, point cloud are voxel
                             Ì†µÌ±ó‚Ä≤                     Ì†µÌ±ó‚Ä≤Ì†µÌ±è                                                                                                                                                     downsampled with a voxel size of 0.04 m. For ScanNet, the voxel size
                          The virtual node is considered as a potential central node, which is                                                                                                                 is set to 0.02 m. For Toronto 3D and Semantic KITTI, the voxel size of
                          connectedtotheverticesofthegraphthroughspecialedges.Thevirtual                                                                                                                       outdoor semantic segmentation is set at 0.08 m.
                          node serves as a temporary storage space (information repository) for                                                                                                                        Data Augmentation: We follow PointNeXt (Qian et al., 2022) and
                          global context, facilitating long-distance message passing and ensuring                                                                                                              use data scaling, height appending, and color drop for data augmenta-
                          the sharing of information for each vertex.                                                                                                                                          tion.
                                                                                                                                                                                                        7 
          T. Han et al.                                                           International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                                                   Fig. 10. Visualizations results of proposed method on the S3DIS dataset.
                                             Fig. 11. Representative visualizations results of proposed method on the S3DIS dataset.
          4.3. Qualitative evaluation                                            indoor scenes, especially for some adherent structures, closely resem-
                                                                                 bling the ground truth labels. We analyzed the advantages of our
              S3DIS: The representative segmentation results of ASGFormer are    method in segmenting structural adherent targets under six different
          shown in Figs. 10 and 11. From the figure, it can be observed that     conditions. The segmentation effect of ASGFormer on door frames
                                                                                 embedded in the wall is demonstrated in Fig. 11(a). Even though wall
          ASGFormer accurately segments the boundaries of objects in complex     points occupy a significant proportion, points belonging to the door can
                                                                               8 
           T. Han et al.                                                                   International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                                           Fig. 12. The visual comparison between ASGFormer and state-of-the-art methods on the S3DIS dataset.
           still be captured by the Graph Transformer with global awareness. The          the local features in different receptive fields. However, in indoor
           most typical scenario is illustrated in Fig. 11(b), where columns are the      scenes, the excessive number of points on the walls leads to adherent
           most challenging targets to segment in indoor architectural scenes due         structures sharing the same features with the wall. One point represents
           to their strong similarity and consistency with the material, texture,         many local features, and the absence of a single point can affect the
           and structure of the walls. During the process of feature learning and         robustness of the network. Graph-based methods outperform point-
           back-propagation, the graph with dynamically adjusted weights can              based methods. Although deformable graph convolutions and graph
           bring similar or dissimilar points closer or push them apart in terms of       attention are able to alter the limitations of fixed point convolution
           features and structure. By increasing within-class similarity and inter-       to adapt to different object structures, they still cannot distinguish re-
           class dissimilarity, ASGFormer performs excellently in distinguishing          lationships between classes within the local receptive field. Obviously,
           between walls and columns. Fig. 11(c)(d) reflect two other types of            methods based on attention and Transformers enhance the integrity of
           structures embedded in the walls, bookcase and board. It can be                the structure. However, such methods lack adaptability to the structure,
           observed that the segmentation of two types has clear edges, without           resulting in a lack of clarity in local fine structures. The proposed ASG-
           noise. Moreover, the objects are separated from other instances while          Former integrates the advantages of graph structure and Transformer,
           beingsegmentedfromthewall,suchasthecluttersonthebookcaseand                    effectively combining local understanding with global perception. This
           the board attached to the bookcase. Fig. 11(e) and (f) demonstrate the         ensures the recognition of intra-class structures while meeting the
           segmentation capability in capturing fine details. The clutters on the         differentiation of inter-class structures. As shown in Fig. 12, our method
           table, and the chair against the wall and table, maintain their original       significantly enhances the segmentation of adherent structures.
           complete structures and fine contours. Additionally, the legs of the sofa         ScanNet: The representative segmentation results of ASGFormer on
           can also be accurately segmented.                                              ScanNet are shown in Figs. 13 and 14. The three representative scenes
               The proposed method is compared with the visualization results             are displayed, where ROI are in red boxes. In Fig. 14(a), fine indoor
           of five algorithms including PointNet++, PointNeXt, GACNet, 3D-                items are segmented with clear contours, such as windows, toilets and
           GCN,andPointTransformer.Thevisualizationcomparisonispresented                  washing machines. Similar to S3DIS, in ScanNet, we still maintain an
           in Fig. 12. Point based methods like PointNet++ and PointNeXt extract          advantage in segmenting building structures that are adhere to the
                                                                                       9 
           T. Han et al.                                                                  International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
           Fig. 13. Visualizations results of proposed method on the ScanNet dataset. (For interpretation of the references to color in this figure legend, the reader is referred to the web
           version of this article.)
           walls (See Fig. 14(b)). The doors adhered to the walls (even cases where      a greater exploration space for the combination of Graph and Trans-
           doors intersect with walls) and bookshelves are accurately identified.        former. The advantages of ASGFormer are more focused on addressing
           Moreover, the outlines and details of furniture such as tables, chairs,       the segmentation of structural adherent objects, which will be reflected
           and sofas are well-preserved, as shown in Fig. 14(c). City-Facade: The        in class-level metrics.
           representative semantic segmentation of our proposed method on City-             Additionally, we compared the proposed method with point cloud
           Facade are shown in Fig. 15, where ROIs in red boxes are represented          segmentation approaches based on different strategies, including voxel-
           the details. Obviously, for building facades, walls, windows, balconies,      based methods like SegCloud (addressing information loss caused by
           and advertisements share similar structural features, posing challenges       voxelization), continuous convolution-based methods like KPConv (mit-
           for semantic segmentation. The proposed method accurately identifies          igating the limited receptive field due to convolutional locality), and
           the contours of adhesive structures and segments components embed-            attention-based methods like PAT (simultaneously adjusting weights
           ded in the wall surface. Additionally, air conditioners mounted on the        for feature vectors and channel relationships to eliminate attention
           wall can also be recognized.                                                  shift under uneven density influence). Our architecture significantly
               Toronto 3D: The representative semantic segmentation of our pro-          enhances the effectiveness of graph model and Transformer framework
           posed method on Toronto 3D dataset are shown in Fig. 16. Four scenes          in indoor 3D point cloud semantic segmentation applications.
           werecombinedfromlefttorighttoformacompletearea.Theproposed                       AsshowninTable2,theclass-levelevaluation further highlights su-
           methodaccuratelysegmentsbuildingoutlinesfromdensevegetation.In                periority of proposed ASGFormer. Among thirteen classes in the S3DIS
           areas with complex feature relationships, different types of objects such     dataset, six (floor, ceiling, door, chair, board, clutter) are achieved
           as poles, trees, and cars are identified with clear structure. This part      as the top-1 by our method, while three (wall, sofa, bookcase) be-
           demonstrates the potential of the proposed ASGFormer in large-scale           come second-best. ASGFormer effectively distinguishes adherent ob-
           complex outdoor scenes.                                                       jects within homogeneous structures: (1) Completely segmented ar-
                                                                                         chitectural structures such as wall, floor, and ceiling. (2) Segmented
           4.4. Quantitative evaluation                                                  columns that are consistent with the architectural structure. (3) High-
                                                                                         precision segmentation of door, bookcase, and board that are connected
               S3DIS Area-5: Quantitative evaluation results for comprehensive           to the building structure. The unsatisfactory performance occurs in
           testing and class-level comparison are shown in Tables 1 and 2, respec-       the category of windows, where our method exhibits a significant
           tively. Following the S3DIS protocol, we validate using Area-5 of the         gap compared to the SOTA approach. This indicates that our method
           data and compare with the state-of-the-art models (See Table 1). The          has a certain degree of neglect in detecting window borders (thin
           OA, mAcc, and mIoU of our ASGFormer are 91.3%, 78.0%, and 72.3%,              and small objects), and a similar situation occurs in ScanNet as well.
           respectively. ASGFormer combines the strengths of 3D-GCN and Point            The competitive sampling strategy of the PAT adds confidence scores
           Transformer. The metrics reflect the performance improvement relative         to each sampled point, effectively avoiding the neglect of window
           to 3D-GCN (mIoU +20.4%) and Point Transformer (mIoU +1.9%).                   boundary points by the farthest point sampling. The presented method
           Compared to the previously leading graph algorithms AGConv, pro-              is compared with the Point Transformer and 3D-GCN, as shown in
           posed ASGFomer has demonstrated improvements of 1.3%, 4.8%, and               Fig. 17(a). This is because our approach is inspired by the combination
           4.4% in the three metrics. This implies that graph attention based on         of these two methods. We exhibit a significant improvement compared
           dynamic learning weights effectively combines global message passing          to 3D-GCN, and thought there are some shortcomings in certain cat-
           and local topological structure, providing more robust representation         egories compared to Point Transformer, we still maintain a dominant
           capabilities for graph learning. Although ASGFormer has a 1.1% lower          advantage.
                                                                                            S3DIS 6-fold: We also conduct a 6-fold experiment on the S3DIS
           mIoU compared to the latest SOTA Point Transformer V3, there is still         dataset. We achieved OA of 91.5% and mIoU of 77.2% ranking second
                                                                                     10 
           T. Han et al.                                                           International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
           Fig. 14. The representative visualizations results of proposed method on the ScanNet dataset. (For interpretation of the references to color in this figure legend, the reader is
           referred to the web version of this article.)
           Fig. 15. The representative visualizations results of proposed method on the City-Facade dataset. (For interpretation of the references to color in this figure legend, the reader
           is referred to the web version of this article.)
           among current methods, as shown in Table 3. The comparative results    the experiment, where wall, beam, bookcase, board, door, column are
           with 10 algorithms are shown in Fig. 18, highlighting our superiority  selected for comparison. We compared our method with the graph
           in both OA and mIoU. We also calculated the IoU for each class in      learning and Transformer algorithms, and the results indicate that our
                                                                               11 
           T. Han et al.                                                                  International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                                             Fig. 16. The representative visualizations results of proposed method on the Toronto 3D dataset.
                  Fig. 17. Per-class IoU improvements of ASGFormer over different methods on two dataset, where ASGFormer is compared with Point Transformer and 3D-GCN.
           approach is superior in addressing the issue of adherent structures.          and 2.3% over Point Transformer and Point Transformer V2, respec-
           While SPT performs well in architectural structures (such as walls            tively. The proposed method is able recognize all facade components
           and beams) using super point Transformer, it tends to systematically          and achieves the best performance across six categories. The recogni-
           aggregate points with similar structures when constructing super point        tion of structurally similar and adhesive facade elements highlights the
           graph, for instance, the boards on the wall.                                  robust semantic segmentation of the proposed graph and Transformer
               ScanNet: Quantitative evaluation results for comprehensive test-          framework. Notably, the proposed method is able to recognize compo-
           ing and class-level comparison are shown in Tables 1 and 4, respec-           nents across eight categories (with significant differences in the number
           tively. ASGFormer has an mIoU advantage of 2.2% over Point Trans-             of points in different categories). This indicates that the proposed
           former. Even more exciting is that we achieve improvements of 0.4%            method has a good ability to handle class imbalance issues caused by
           (GemoGCNN), 13.9% (SegGCN), 11.8% (SPH3D-GCN), 11.0% (HPEIN)                  the numberofpoints.However,itstillperformspoorlywhenaddressing
           over the previous series of graph-based methods. Our method achieves          class imbalance due to uneven position distribution.
           the top rank in fourteen out of twenty categories. Specifically, there is        Toronto 3D & Semantic KITTI: The overall evaluation results are
           a clear advantage in the segmentation results related to the structural       shown in Table 1. Most current methods used for indoor point cloud
           parts of buildings and furniture adhering to buildings. However, similar      semantic segmentation are difficult to transfer and apply to outdoor
           to S3DIS, our method is not well-suited for segmenting small ans              scenes. Therefore, we compared some of these methods. As shown in
           tiny structure, such as shower and sink. The comparisons of per-class         Table 1, our proposed method achieves mIoU of 81.6% and 70.1%
           IoU with 3D-GCN and Point Transformer are presented in Fig. 17(b).            for Toronto 3D and Semantic KITTI datasets, respectively. Compared
           In summary, our method achieves high-precision semantic segmenta-             against the latest method, such as Point Transformer V2 & V3, ASG-
           tion through the dynamic graph Transformer with adaptive structure            Former exhibits performance decrease of 4.1% and 2.5%, but it still
           weights. The major contribution is the ability to segment structurally        surpasses most of the previous methods. Subsequently, we compared a
           connected building components and indoor objects.                             series of methods from the Toronto 3D benchmark, most of which are
               City-Facade: Quantitative evaluation results of City-Facade are           designed for outdoor scenes. As shown in Table 6, we achieved the best
                                                                                         results in three categories, demonstrating the potential of the proposed
           shown in Tables 1 and 5. ASGFormer has an mIoU advantage of 6.2%              method in outdoor point cloud semantic segmentation.
                                                                                     12 
             T. Han et al.                                                                              International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
             Table 1
             Comprehensive evaluation results of different methods on the ScanNet, Area 5 of S3DIS dataset, City-Facade, Toronto 3D, and Semantic KITTI dataset.
               Method                                            S3DIS                                             ScanNet           City-Facade        Toronto 3D         Semantic KITTI
                                                                 OA (%)         mAcc (%)          mIoU (%)         mIoU (%)          mIoU (%)           mIoU (%)           mIoU (%)
               PointNet (Qi et al., 2017a)                       78.6           49.0              41.1             ‚Äì                 11.9               ‚Äì                  14.6
               PointNet++ (Qi et al., 2017b)                     ‚Äì              ‚Äì                 53.2             33.9              11.8               56.6               20.1
               SEGCloud (Tchapmi et al., 2017)                   ‚Äì              57.4              48.9             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               RSNet (Huang et al., 2018)                        ‚Äì              59.4              51.9             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               TangentConv (Tatarchenko et al., 2018)            82.5           62.2              52.8             43.8              ‚Äì                  ‚Äì                  40.9
               SPG (Landrieu and Simonovsky, 2018)               86.4           66.5              58.0             ‚Äì                 ‚Äì                  ‚Äì                  17.4
               PointCNN (Li et al., 2018)                        85.9           63.9              57.3             45.8              ‚Äì                  ‚Äì                  ‚Äì
               PCCN (Wang et al., 2018b)                         ‚Äì              67.0              58.3             ‚Äì                 ‚Äì                  59.0               ‚Äì
               SSP (Landrieu and Boussaha, 2019)                 87.9           68.2              61.7             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               DGCNN (Wang et al., 2019b)                        83.2           ‚Äì                 60.0             ‚Äì                 11.7               49.6               ‚Äì
               DeepGCNs (Li et al., 2019a)                       85.9           ‚Äì                 60.0             ‚Äì                 11.9               ‚Äì                  ‚Äì
               TGNet (Li et al., 2019b)                          88.5           ‚Äì                 57.8             ‚Äì                 ‚Äì                  60.9               ‚Äì
               PointWeb (Zhao et al., 2019)                      87.0           66.6              60.3             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               HPEIN (Jiang et al., 2019)                        87.2           68.3              61.9             61.8              ‚Äì                  ‚Äì                  ‚Äì
               MuGNet (Xie et al., 2020)                         88.1           ‚Äì                 63.5             ‚Äì                 ‚Äì                  ‚Äì                  50.0
               GACNet (Wang et al., 2019a)                       87.8           ‚Äì                 62.9             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               KPConv (Thomas et al., 2019)                      ‚Äì              72.8              67.1             68.4              ‚Äì                  69.1               58.8
               FusionNet (Zhang et al., 2020)                    ‚Äì              72.3              67.2             68.8              ‚Äì                  ‚Äì                  61.3
               Grid-GCN (Xu et al., 2020)                        86.9           ‚Äì                 57.8             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               SPH3D-GCN (Lei et al., 2020b)                     87.7           65.9              59.5             61.0              ‚Äì                  ‚Äì                  ‚Äì
               SegGCN (Lei et al., 2020a)                        88.2           70.4              63.6             58.9              ‚Äì                  ‚Äì                  ‚Äì
               3D-GCN (Lin et al., 2020)                         84.6           ‚Äì                 51.9             60.9              20.4               ‚Äì                  ‚Äì
               PSD (1%) (Zhang et al., 2021)                     ‚Äì              ‚Äì                 63.5             54.7              ‚Äì                  62.4               ‚Äì
               SFPN (Lin et al., 2021)                           88.3           ‚Äì                 63.8             64.1              ‚Äì                  ‚Äì                  ‚Äì
               GeomGCNN (Srivastava and Sharma, 2021)            89.4           ‚Äì                 69.4             72.4              ‚Äì                  ‚Äì                  ‚Äì
               GCN-MLP (Wang et al., 2021)                       88.5           64.1              57.3             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               ASSANet (Qian et al., 2021)                       89.7           ‚Äì                 68.0             ‚Äì                 34.5               ‚Äì                  ‚Äì
               PointTransformer (Zhao et al., 2021)              90.8           76.5              70.4             70.6              39.3               ‚Äì                  ‚Äì
               LocalTransformer (Wang et al., 2022)              87.6           71.9              64.1             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               PAT (Zhang et al., 2022a)                         ‚Äì              70.8              60.1             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               SQN (0.1%) (Hu et al., 2022)                      ‚Äì              ‚Äì                 61.41            56.9              ‚Äì                  77.7               50.8
               RepSurf-U (Ran et al., 2022)                      90.2           76.0              68.9             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               ConvNet+CBL (Tang et al., 2022)                   90.6           75.2              69.4             70.5              ‚Äì                  ‚Äì                  ‚Äì
               PointNeXt (Qian et al., 2022)                     90.6           ‚Äì                 70.5             71.5              22.2               ‚Äì                  ‚Äì
               StratifiedTransformer (Lai et al., 2022)          91.5           78.1              72.0             73.7              ‚Äì                  ‚Äì                  ‚Äì
               PointTransformerV2 (Wu et al., 2022)              91.1           77.9              71.6             75.2              43.2               ‚Äì                  72.6
               LCPFormer (Huang et al., 2023)                    90.8           76.8              70.2             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               AGConv (Wei et al., 2023)                         90.0           73.2              67.9             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               MKConv (Woo et al., 2023)                         89.6           75.1              67.7             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               SPT (Robert et al., 2023)                         ‚Äì              68.9              ‚Äì                ‚Äì                 ‚Äì                  ‚Äì                  63.5
               SPoTr (Park et al., 2023)                         90.7           76.4              70.8             ‚Äì                 ‚Äì                  ‚Äì                  ‚Äì
               OctFormer (Wang, 2023)                            ‚Äì              ‚Äì                 ‚Äì                75.7              ‚Äì                  ‚Äì                  ‚Äì
               PointTransformerV3 (Wu et al., 2023)              ‚Äì              ‚Äì                 73.4             77.5              ‚Äì                  ‚Äì                  74.2
               ASGFormer (Ours)                                  91.3           78.0              72.3             72.8              45.5               81.6               70.1
             Table 2
             Class-level evaluation (IoU) results of different methods on the Area 5 of S3DIS dataset (%).
               Method                                     Ceiling     Floor     Wall     Column       Window      Door      Table     Chair     Sofa      Bookcase      Board     Clutter
               PointNet (Qi et al., 2017a)                88.8        97.3      69.8     3.9          46.3        10.8      59.0      52.6      5.9       40.3          26.4      33.2
               PointNet++ (Qi et al., 2017b)              90.2        91.7      73.1     21.2         49.7        42.3      62.7      59.0      19.6      45.8          48.2      45.6
               SEGCloud (Tchapmi et al., 2017)            90.1        96.1      69.9     18.4         38.4        23.1      70.4      75.9      40.9      58.4          13.0      41.6
               RSNet (Huang et al., 2018)                 93.3        98.4      79.2     15.8         45.4        50.1      65.5      67.9      22.5      52.5          41.0      43.6
               TangentConv (Tatarchenko et al., 2018)     90.5        97.7      74.0     20.7         39.0        31.3      77.5      69.4      57.3      38.5          48.8      39.8
               SPG (Landrieu and Simonovsky, 2018)        89.4        96.9      78.1     42.8         48.9        61.6      84.7      75.4      69.8      52.6          2.1       52.2
               PointCNN (Li et al., 2018)                 92.3        98.2      79.4     17.6         22.8        62.1      74.4      80.6      31.7      66.7          62.1      56.7
               PCCN (Wang et al., 2018b)                  92.3        96.2      75.9     6.0          69.5        63.5      66.9      65.6      47.3      68.9          59.1      46.2
               SSP (Landrieu and Boussaha, 2019)          91.9        96.7      80.8     28.8         60.3        57.2      85.5      76.4      70.5      49.1          51.6      53.3
               DGCNN (Wang et al., 2019b)                 91.1        97.3      74.5     11.9         49.5        33.5      66.9      69.4      20.5      47.5          34.7      40.8
               DeepGCNs (Li et al., 2019a)                93.1        95.3      78.2     37.4         56.1        68.2      64.9      61.0      34.6      51.5          51.1      54.4
               PointWeb (Zhao et al., 2019)               92.0        98.5      79.4     21.1         59.7        34.8      76.3      88.3      46.9      69.3          64.9      52.5
               HPEIN (Jiang et al., 2019)                 91.5        98.2      81.4     23.3         65.3        40.0      75.5      87.7      58.5      67.8          65.6      49.4
               MuGNet (Xie et al., 2020)                  91.0        96.9      83.2     37.0         54.3        62.6      85.3      76.4      70.1      55.2          55.2      53.4
               GACNet (Wang et al., 2019a)                92.3        98.3      81.9     20.4         59.0        40.9      78.5      85.8      61.7      70.8          74.7      52.8
               KPConv (Thomas et al., 2019)               92.8        97.3      82.4     23.9         58.0        69.0      81.5      91.0      75.4      75.3          66.7      58.9
               SPH3D-GCN (Lei et al., 2020b)              93.3        97.1      81.1     33.2         45.8        43.8      79.7      86.9      33.2      71.5          54.1      53.7
               3D-GCN (Lin et al., 2020)                  91.4        97.1      75.9     22.3         43.5        30.1      71.5      79.4      21.9      53.7          42.9      44.9
               PointTransformer (Zhao et al., 2021)       90.4        98.5      86.3     38.0         63.4        74.3      89.1      82.4      74.3      80.2          76.0      59.3
               PAT (Zhang et al., 2022a)                  93.0        98.5      72.3     41.5         85.1        38.2      57.7      83.6      48.1      67.0          61.3      33.6
               MKConv (Woo et al., 2023)                  92.4        98.2      83.9     28.5         64.5        65.7      82.4      89.7      67.5      73.9          77.3      55.9
               SPT (Robert et al., 2023)                  92.6        97.7      83.5     42.0         60.6        67.1      81.0      88.8      86.0      73.2          63.1      60.0
               ASGFormer (Ours)                           93.4        98.5      84.9     41.1         61.1        82.7      83.6      92.0      80.8      77.8          78.6      63.2
                                                                                                  13 
             T. Han et al.                                                                           International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
             Table 3
             The 6-fold experiment results in S3DIS semantic segmentation.
              Method      PointNet          PointWeb           KPConv             SPG               PointNeXt          PointTransformer  PointTrans-        PointTrans-        Ours
                                                                                                                                         formerV2           formerV3
                          (Qi et al.,       (Zhao et al.,      (Thomas et al.,    (Landrieu and     (Qian et al.,      (Zhao et al.,     (Wu et al.,        (Wu et al.,
                          2017a)            2019)              2019)              Simonovsky,       2022)              2021)             2022)              2023)
                                                                                  2018)
              mIoU (%)    47.6              66.7               70.6               62.1              74.9               65.4              73.5               77.7               77.2
             Table 4
             Class-level evaluation (IoU) results of different methods on the ScanNet dataset (%).
              Method                   Bathtub Bed    Books. Cabi. Chair Counter Curt. Desk Door Floor Other Pic.           Fridge  Shower Sink Sofa Table Toilet Wall Wndw
              ScanNet (Dai et al.,     20.3     36.6  50.1    31.1  52.4   21.1      0.2   34.2  18.9   78.6  14.5    10.2  24.5    15.2    31.8  34.8   30.0   46.0   43.7  18.2
              2017)
              PointNet++ (Qi et al.,   58.4     47.8  45.8    25.6  36.0   25.0      24.7  27.8  26.1   67.7  18.3    11.7  21.2    14.5    36.4  34.6   23.2   54.8   52.3  25.2
              2017b)
              TangentConv              43.7     64.6  47.4    36.9  64.5   35.5      25.8  28.2  27.9   91.8  29.8    14.7  28.3    29.4    48.7  56.2   42.7   61.9   63.3  35.2
              (Tatarchenko et al.,
              2018)
              PointCNN (Li et al.,     57.7     61.1  35.6    32.1  71.5   29.9      37.6  32.8  31.9   94.4  28.5    16.4  21.6    22.9    48.4  54.5   45.6   75.5   70.9  47.5
              2018)
              PointConv (Wu et al.,    63.6     64.0  57.4    47.2  73.9   43.0      43.3  41.8  44.5   94.4  37.2    18.5  46.4    57.5    54.0  63.9   50.5   82.7   76.2  51.5
              2019)
              MVPNet (Luo et al.,      83.1     71.5  67.1    59.0  78.1   39.4      67.9  64.2  55.3   93.7  46.2    25.6  64.9    40.6    62.6  69.1   66.6   87.7   79.2  60.8
              2022)
              KPConv (Thomas et al.,   84.7     75.8  78.4    64.7  81.4   47.3      77.2  60.5  59.4   93.5  45.0    18.1  58.7    80.5    69.0  78.5   61.4   88.2   81.9  63.2
              2019)
              SPH3D-GCN (Lei et al.,   85.8     77.2  48.9    53.2  79.2   40.4      64.3  57.0  50.7   93.5  41.4    4.6   51.0    70.2    60.2  70.5   54.9   85.9   77.3  53.4
              2020b)
              SegGCN (Lei et al.,      83.3     73.1  53.9    51.4  78.9   44.8      46.7  57.3  48.4   93.6  39.6    6.1   50.1    50.7    59.4  70.0   56.3   87.4   77.1  49.3
              2020a)
              3D-GCN (Lin et al.,      76.0     66.7  64.9    52.1  79.3   45.7      64.8  52.8  43.4   94.7  40.1    15.3  45.4    72.1    64.8  71.7   53.6   90.4   76.5  48.5
              2020)
              PointTransformer (Zhao   83.5     74.5  79.3    67.2  81.8   49.3      80.2  62.3  61.0   94.7  47.0    24.9  59.4    83.3    70.5  77.9   64.6   89.2   82.3  61.1
              et al., 2021)
              SFPN (Lin et al., 2021)  77.1     69.2  67.2    52.4  83.7   44.0      70.6  53.8  44.6   94.4  42.1    21.9  55.2    75.1    59.1  73.7   54.3   90.1   76.8  55.7
              ConvNet+CBL (Tang        76.9     77.5  80.9    68.7  82.0   43.9      81.2  66.1  59.1   94.5  51.5    17.1  63.6    85.6    72.0 79.6 66.8      88.9   84.7  68.9
              et al., 2022)
              ASGFormer (Ours)         87.1     78.7 81.1     66.0  91.2   66.0      73.9  67.7 65.4 95.0     58.1    32.2 62.0     64.5    65.9  81.8 75.5     94.3   84.8 65.1
             Table 5
             Class-level evaluation (IoU) results of different methods on the City-Facade dataset (%).
              Method                                     Wall         Window          Door         Roof         Advertisement        Air condition        Rain shed         Balcony
              PointNet (Qi et al., 2017a)                76.0         18.9            0.0          0.0          0.0                  0.0                  0.0               0.0
              PointNet++ (Qi et al., 2017b)              75.7         18.9            0.0          0.0          0.0                  0.0                  0.0               0.0
              DGCNN (Wang et al., 2019b)                 76.4         17.2            0.0          0.0          0.0                  0.0                  0.0               0.0
              DeepGCNs (Li et al., 2019a)                76.7         18.9            0.0          0.0          0.0                  0.0                  0.0               0.0
              ASSANet (Qian et al., 2021)                81.7         50.0            33.4         6.7          28.9                 25.4                 49.5              2.4
              PointTransformer (Zhao et al., 2021)       74.8         62.6            38.1         47.7         29.8                 31.1                 44.4              41.7
              PointNeXt (Qian et al., 2022)              81.0         44.2            24.7         0.0          27.6                 0.0                  0.0               0.0
              ASGFormer (Ours)                           85.8         64.1            45.6         53.2         59.4                 29.9                 64.1              17.0
             Table 6
             Class-level evaluation (IoU) results of different methods on the Toronto 3D dataset (%).
              Method                                    Road           Rd mrk.           Natural          Building          Util. line        Pole            Car             Fence
              PointNet++ (Qi et al., 2017b)             91.44          7.59              89.80            74.00             68.60             59.53           53.97           7.54
              PCCN (Wang et al., 2018b)                 91.22          3.50              90.48            77.30             62.30             68.54           53.63           17.12
              DGCNN (Wang et al., 2019b)                90.63          0.44              81.25            63.95             47.05             56.86           49.26           7.32
              KPConv (Thomas et al., 2019)              90.20          0.00              86.79            86.83             81.08             73.06           42.85           21.57
              GACNet (Wang et al., 2019a)               92.25          26.89             90.31            79.76             66.68             54.29           69.51           6.47
              MS-TGNet (Tan et al., 2020)               90.89          18.78             92.18            80.62             69.36             71.22           51.05           13.59
              PointASNL (Yan et al., 2020)              92.20          30.49             90.24            78.56             69.80             66.03           72.38           9.12
              RandLA-Net (Hu et al., 2021)              96.69          64.21             96.92            94.24             88.06             77.84           93.37           42.86
              ResDLPS-Net (Du et al., 2021)             95.82          59.80             96.10            90.96             86.82             79.95           89.41           43.31
              GAANet (Wan et al., 2023)                 92.70          39.34             92.93            88.42             77.99             68.67           75.06           24.08
              DGFA-Net (Zhou and Ling, 2023)            97.30          69.00             97.70            93.90             88.20             82.00           93.50           41.40
              LACV-Net (Zeng et al., 2024)              97.10          66.90             97.30            93.00             87.30             83.40           93.40           43.10
              ASGFormer (Ours)                          97.26          65.41             97.76            94.13             81.28             79.07           91.11           46.57
                                                                                                14 
             T. Han et al.                                                                           International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                             Table 7
                             Control ablation study results of different modules on the Area-5 of S3DIS dataset.
                               Attention mechanism                                 Adaptive weights       Virtual node      OA (%)       mAcc (%)        mIoU (%)
                               MLP       Scalar attention     Graph attention
                               ‚úì                                                                                            87.1         68.6            61.7
                                         ‚úì                                                                                  88.4         71.9            64.6
                                                              ‚úì                                                             88.6         73.4            65.5
                                                              ‚úì                    ‚úì                                        90.1         76.1            70.4
                               ‚úì                                                                          ‚úì                 87.9         69.1            61.9
                                         ‚úì                                                                ‚úì                 88.4         72.8            65.0
                                                              ‚úì                                           ‚úì                 88.9         74.2            66.3
                                                              ‚úì                    ‚úì                      ‚úì                 91.3         78.0            72.3
             Fig. 18. Performance on S3DIS dataset with 6-fold evaluation, where the left subplot represents the overall evaluation of the 6-fold experiments, and the right subplot shows the
             class-level evaluation results (%).
             4.5. Ablation experiment                                                                  AGTcontributes to a better model. We compared the three graph
                                                                                                   Transformer architectures mentioned in the previous Fig. 8. It is ev-
                Tovalidate the necessity and effectiveness of our designed modules,                ident that AGT (70.4%) outperforms naive GT (62.6%) and general
             we conducted comprehensive ablation experiments in the Area-5 of                      GT (66.6%) in terms of mIoU. In the graph, vertex features are em-
             S3DIS dataset to demonstrate their roles and performance within the                   ployedtocapturelocalinformationbetweenpoints,whileedgefeatures
             network.                                                                              contribute to capturing relationships and global information among
                Model Analysis: The effectiveness of each module is discussed and                  vertices. The introduction of edge features strengthens the graph learn-
             the comparison of the experimental results is presented in Table 7.                   ing capabilities, while ASGFormer, in a learnable manner, utilizes the
             MLPrepresents the baseline model without incorporating any attention                  weights of edges to enhance the adaptability of the graph. Trans-
             modules, and the scalar attention follows the standard dot-product                    former allows the network to assign different weights between points,
             attention form. Scalar attention is able to improve performance but                   and embedding edge features helps strengthen the learning of struc-
             slightly falls short compared to graph attention. With the infusion of                tural information. The incorporation of edges into the attention design
             the designed adaptive weights in our model, graph attention explic-                   forms an adaptive mechanism for structural learning. Dynamically
             itly demonstrates a significant improvement both in OA (90.1% vs.                     adjusting adaptive weights aids in distinguishing different semantic
             88.6%), mAcc (76.1% vs. 73.4%), and mIoU (70.4% vs. 65.5%). AGT                       classes.
             not only focuses on the correlation between points but also considers                     Position Embedding is crucial. As mentioned earlier, position
             the similarity of structural properties. On this basis, it enhances the               embedding is also a pivotal factor influencing graph Transformer.
             correspondence between points with similar attributes in an adaptive                  Absolute PE, relative PE and proposed implicit representation method
             manner. Furthermore, aligning with probabilistic graphical models,                    are compared in Table 8, noting that we only discuss the distinct
             AGT increases the probability of assigning the same label to similar                  components across the three strategies, specifically applied to the po-
             points. The second enhancement is reflected in the introduction of                    sition embedding of Ì†µÌ∞æ and Ì†µÌ±â. By leveraging the Laplacian operator
             virtual nodes. With the incorporation of virtual nodes, the network‚Äôs                 to provide relative position information, the network is able to learn
             performance experiences a certain degree of improvement. Although                     the topological structure of the graph. However, non-learnable position
             the changes observed in scalar attention and graph attention are not                  embedding struggles to adapt to changes in the graph and differenti-
             substantial, it still indicates the effectiveness of the design of this               ate objects that share similar structures. ASGFormer exhibits greater
             module. Our ablation study demonstrates the crucial role of adap-                     scale adaptability and unbiasedness compared to absolute position
             tive weights in the network, and the auxiliary role of virtual nodes                  embedding. It incorporates relative position relations into the weight
             contributes to the performance improvement.                                           features to learn graph structure, leading to mIoU of 70.4% (+3.1%
                                                                                                15 
                T. Han et al.                                                                                                    International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                Table 8                                                                                                        normalization utilize the features of all vertices in the whole graph
                Comparison of semantic segmentation with different graph Transformer architectures,                            for normalization, reflecting the differences among vertices within the
                where position embedding is represented by PE.                                                                 graph. Therefore, in future work, a weighted combination of var-
                  Method                                 Position embedding                               mIoU (%)             ious normalization strategies should be considered to enhance the
                  Naive GT                               Absolute PE                                      62.1                 performance of graph learning.
                  Naive GT                               Relative PE                                      62.6                      There is a notable drawback, as mentioned earlier, that our method
                  General GT                             Absolute PE                                      64.8                 lacks segmentation capability for small and tiny objects. This results
                  General GT                             Relative PE                                      66.6                 in lower precision segmentation for features like window frames on
                  AGT                                    ‚Äì                                                66.9                 building facades and objects such as fences and lampposts in complex
                  AGT                                    Relative PE                                      67.3                 street scenes. The proposed method cannot address class imbalance
                  AGT                                    Implicit PE                                      70.4                 issues caused by uneven position distribution, such as the boundaries
                                                                                                                               of doors and windows. Moreover, the proposed method lacks boundary
                                                                                                                               constraints, which might also be a reason for errors in small objects and
                                                                                                                               edges. In future, we will consider fine-grained instance segmentation by
                                                                                                                               incorporating boundary-aware uncertainty estimation.
                                                                                                                                    Although our method learns topological relationships, it still does
                                                                                                                               not address the issue of class imbalance caused by the uneven dis-
                                                                                                                               tribution of positions, as shown in Fig. 19. Considering topological
                                                                                                                               boundaries as decision boundaries has led to difficulties in addressing
                                                                                                                               class imbalance issues. Perhaps, this is one of the reasons for the
                                                                                                                               inaccurate segmentation of small targets.
                                                                                                                                    A key challenge in 3D point cloud semantic segmentation lies in
                                                                                                                               the labeling efforts. Many previous methods (Huang et al., 2024; Su
                                                                                                                               et al., 2023b) have discussed the application of weakly-supervised and
                                              Fig. 19. Failed example in ScanNet.                                              semi-supervised methods (requiring only a few annotation efforts to
                                                                                                                               achieve comparable performance) in point cloud semantic segmenta-
                                                                                                                               tion, especially for ALS and photogrammetric point clouds (Lin et al.,
                over relative position embedding). Another crucial aspect is that weight                                       2022; Wang and Yao, 2022; Wang et al., 2023). Even though our
                features, serving as implicit position embedding, address the issue of                                         method has not been validated on ALS point cloud datasets, we have
                the ambiguity in structural relationships caused by the inability of                                           still considered the performance and label efforts on two large-scale
                relative coordinate relations to serve as input for MLP.                                                       point cloud datasets. As shown in Table 10, the performance gap
                                                                                                                               betweenfully supervised semantic segmentation and weakly supervised
                4.6. Efficiency evaluation                                                                                     or semi-supervised methods is narrowing. Moreover, we have conduct
                                                                                                                               the experiments on the MLS point cloud dataset Toronto 3D, as shown
                     The proposed ASGFormer has a large number of parameters, but it                                           in Table 11. Our method has demonstrated certain limitations in large-
                ensures relatively fast inference speed with lower floating-point oper-                                        scale point cloud semantic segmentation at the city scale. In future
                ations while maintaining model performance, as shown in Table 9. In                                            work, we will focus on exploring the trade-off between performance
                adaptive graph Transformer block, we follow the ASSANet by adopting                                            and labeling efforts.
                MLP before neighbor grouping to significantly reduce the FLOPs by                                              5. Conclusion
                 Ì†µÌ±ë√óÌ†µÌ±ë√óÌ†µÌ±Å√óÌ†µÌ±Å(Ì†µÌ±ñ)√óÌ†µÌ∞ø  = Ì†µÌ±Å(Ì†µÌ±ñ) times, where Ì†µÌ±Å, Ì†µÌ±Å(Ì†µÌ±ñ), and Ì†µÌ∞ø denote the point
                    Ì†µÌ±ë√óÌ†µÌ±ë√óÌ†µÌ±Å√óÌ†µÌ∞ø
                number, neighbor number, and the layer number of MLP. Moreover,                                                     In this paper, we introduce an adaptive graph Transformer 3D point
                the Ì†µÌ±äÌ†µÌ±ñÌ†µÌ±ó in adaptive graph Transformer block has Ì†µÌ±ò channels, thus,                                          cloud semantic segmentation network tailored for structurally adherent
                the learnable parameters is Ì†µÌ±ò2. Generally, Ì†µÌ±ò ‚â™ Ì†µÌ∞∂, which allows the                                          objects. The proposed ASGFormer undergoes extensive and compre-
                number of parameters is significantly smaller than that of naive graph                                         hensive experiments on five publicly available large-scale 3D point
                Transformer, where Ì†µÌ∞∂ is the channel of features. Finally, we use virtual                                      cloud datasets. Comprehensive experiments demonstrate that the ef-
                node to integrate global information, which transforms the computa-                                            fectiveness and superiority of proposed method from both quantitative
                                                                   2
                tional complexity of CRF from Ì†µÌ±Ç(Ì†µÌ±õ ) to Ì†µÌ±Ç(Ì†µÌ±õ) in a graph with diameter                                       and qualitative perspectives. Compared to state-of-the-art algorithms,
                2. Meanwhile, we use summation instead of concatenation to reduce                                              ASGFormer exhibits competitive performance. The Graph Transformer
                the redundant computation overhead introduced by multiple MLP and                                              maintains the sparsity and locality of graphs while incorporating long-
                convolutions.                                                                                                  range dependencies and overall graph characteristics. It significantly
                4.7. Limitations discussion                                                                                    improves performance on the point cloud understanding. In the future,
                                                                                                                               wewillcontinuetoexpandonthelimitationsofthisstudy,contributing
                     Virtual node introduces a new node, through which all of their new                                        to the advancement of intelligent point cloud processing in the remote
                connections pass. It serves as a global aggregation point, thus retaining                                      sensing and photogrammetry community.
                all the information flowing from the entire graph. However, if the graph                                       CRediT authorship contribution statement
                is larger or many vertices rely on virtual node to pass information,
                virtual node may lead to an information bottleneck and even reduce                                                  Ting Han: Writing ‚Äì original draft, Validation, Methodology, In-
                the performance of the model. The quantity of virtual node has not                                             vestigation. Yiping Chen: Writing ‚Äì review & editing, Validation,
                been considered in this paper and is part of our future work.                                                  Supervision, Methodology, Investigation, Funding acquisition. Jin Ma:
                     Training a multi-layer deep GCN, normalization strategies are in-                                         Writing ‚Äì original draft, Validation, Methodology. Xiaoxue Liu: Visu-
                dispensable. Node-wise normalization employs LayerNorm in Trans-                                               alization, Formal analysis, Data curation. Wuming Zhang: Writing ‚Äì
                former, calculated separately for each vertex. However, the vertices                                           review & editing. Xinchang Zhang: Formal analysis, Data curation.
                in the graph are non-sequenced, and we have not discussed better
                normalization methods. For instance, Graph normalization and Batch                                             Huajuan Wang: Validation, Data curation.
                                                                                                                          16 
             T. Han et al.                                                                               International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                               Table 9
                               Comparison of computation costs with performance.
                                Method                                          Param. (M) ‚Üì            FLOPs (G) ‚Üì            Throughput (ins./s) ‚Üë          mIoU (%) ‚Üë
                                PointNet (Qi et al., 2017a)                     3.6                     35.5                   162                            41.1
                                PointNet++ (Qi et al., 2017b)                   1.0                     7.2                    237                            53.5
                                PointCNN (Li et al., 2018)                      0.6                     ‚Äì                      ‚Äì                              57.3
                                DGCNN (Wang et al., 2019b)                      1.3                     ‚Äì                      8                              47.9
                                DeepGCN (Li et al., 2019a)                      3.6                     ‚Äì                      3                              52.5
                                3D-GCN (Lin et al., 2020)                       0.6                     ‚Äì                      ‚Äì                              51.9
                                KPConv (Thomas et al., 2019)                    15.0                    ‚Äì                      30                             67.1
                                AGConv (Wei et al., 2023)                       1.9                     3.6                    ‚Äì                              67.9
                                Point Transformer (Zhao et al., 2021)           7.8                     5.6                    34                             70.4
                                ASSANet (Qian et al., 2021)                     2.4                     2.5                    300                            68.0
                                PointNeXt (Qian et al., 2022)                   41.6                    84.8                   43                             70.5
                                PointTransformerV2 (Wu et al., 2022)            12.8                    18.2                   ‚Äì                              71.6
                                PointTransformerV3 (Wu et al., 2023)            46.2                    5.2                    ‚Äì                              73.4
                                ASGFormer (Ours)                                19.0                    7.3                    155                            72.3
             Table 10
             Class-level evaluation on the Area 5 of S3DIS dataset. We compared against unsupervised (Unsup.) and weakly supervised (*% labeling efforts) approaches (%).
               Method                           Setting      Avg.     Ceiling    Floor    Wall    Column      Window      Door     Table    Chair     Sofa    Bookcase     Board     Clutter
               K-means                          Unsup.       38.4     59.8       63.3     34.9    24.6        34.2        29.3     35.7     33.1      45.0    45.6         41.7      30.4
               Ncut                             Unsup.       40.0     63.5       63.8     37.2    24.6        35.5        29.9     38.9     34.3      47.1    46.3         44.1      31.5
               Xia et al. (2023)                Unsup.       25.7     79.1       86.6     51.8    0.3         0.5         7.6      30.6     26.4      5.6     45.5         0.0       0.7
                    3
               U3DS (Liu et al., 2024)          Unsup.       42.8     ‚Äì          ‚Äì        ‚Äì       ‚Äì           ‚Äì           ‚Äì        ‚Äì        ‚Äì         ‚Äì       ‚Äì            ‚Äì         ‚Äì
               GrowSP (Zhang et al., 2023b)     Unsup.       44.5     ‚Äì          ‚Äì        ‚Äì       ‚Äì           ‚Äì           ‚Äì        ‚Äì        ‚Äì         ‚Äì       ‚Äì            ‚Äì         ‚Äì
               Xu and Lee (2020)                10%          48.0     90.9       97.3     74.8    8.4         49.3        27.3     69.0     71.7      16.5    53.2         23.3      42.8
               PSD (Zhang et al., 2021)         1%           63.5     92.3       97.7     80.7    27.8        56.2        62.5     78.7     84.1      63.1    70.4         58.9      53.2
               HybridCR (Li et al., 2022)       1%           51.5     85.4       91.9     65.9    18.0        51.4        34.2     63.8     78.3      52.4    59.6         29.9      39.0
               GaIA (Lee et al., 2023)          1%           53.7     ‚Äì          ‚Äì        ‚Äì       ‚Äì           ‚Äì           ‚Äì        ‚Äì        ‚Äì         ‚Äì       ‚Äì            ‚Äì         ‚Äì
               Li et al. (2024)                 1%           68.2     91.7       95.5     82.5    46.6        63.3        65.4     77.0     89.0      64.7    74.5         69.2      67.2
               SQN (Hu et al., 2022)            1%           63.6     92.0       96.4     81.3    21.4        53.7        73.1     77.8     85.9      56.7    69.9         66.5      52.4
               MSC (Su et al., 2023a)           1%           65.3     93.3       97.5     82.0    29.0        56.2        64.2     75.9     87.2      70.7    71.5         66.8      54.6
               DR-Net (Zhang and Bi, 2024)      1%           64.2     93.2       98.0     81.4    34.1        53.7        60.9     79.4     86.3      61.4    70.0         62.4      52.5
               UCL (Yao et al., 2024)           1%           68.2     93.4       97.3     82.6    25.7        59.9        66.3     81.9     89.7      75.9    75.4         78.5      60.0
               SQN (Hu et al., 2022)            0.1%         61.4     91.7       95.6     78.7    24.2        55.8        63.1     70.5     83.1      60.6    67.8         56.1      50.6
               DR-Net (Zhang and Bi, 2024)      0.1%         58.7     92.1       96.6     78.0    15.6        52.3        58.4     69.2     77.1      52.8    65.2         57.8      48.5
               UCL (Yao et al., 2024)           0.1%         65.4     93.3       97.2     82.0    26.5        60.3        62.1     79.2     85.6      68.4    73.7         65.7      55.6
               VIBUS (Tian et al., 2022)        0.02%        52.0     ‚Äì          ‚Äì        ‚Äì       ‚Äì           ‚Äì           ‚Äì        ‚Äì        ‚Äì         ‚Äì       ‚Äì            ‚Äì         ‚Äì
               Xu et al. (2023c)                0.02%        55.9     ‚Äì          ‚Äì        ‚Äì       ‚Äì           ‚Äì           ‚Äì        ‚Äì        ‚Äì         ‚Äì       ‚Äì            ‚Äì         ‚Äì
               ASGFormer (Ours)                 Ful. Sup.    72.3     93.4       98.5     84.9    41.1        61.1        82.7     83.6     92.0      80.8    77.8         78.6      63.2
             Table 11
             Class-level evaluation on the Toronto 3D dataset. We compared against weakly supervised (*% labeling efforts) approaches (%).
               Method                              Setting          Avg.         Road         Rd mrk.         Natural       Building        Uti. line       Pole         Car          Fence
               MSC (Su et al., 2023a)              1%               81.20        96.60        63.60           95.70         93.50           85.40           72.20        89.30        53.30
               PSD (Zhang et al., 2021)            0.1%             73.30        95.29        60.81           94.68         82.73           84.13           72.18        82.44        14.13
               SQN (Hu et al., 2022)               0.1%             77.75        96.69        65.67           94.58         91.34           83.36           70.59        88.87        30.91
               WSPointNet (Lei et al., 2022)        0.1%            78.96        96.70        66.99           94.89         90.79           83.68           75.71        88.37        34.54
               Liu et al. (2023a)                  0.1%             67.42        70.23        9.60            94.03         91.64           83.66           62.98        84.96        42.26
               DR-Net (Zhang and Bi, 2024)         0.1%             78.10        97.00        65.80           94.70         92.20           82.10           69.60        88.80        34.50
               DAAL-WS (Lei et al., 2024)          0.01%            81.91        97.67        73.42           96.27         92.35           83.57           78.03        91.82        42.16
               ASGFormer (Ours)                    Ful. Sup.        81.60        97.26        65.41           97.76         94.13           81.28           79.07        91.11        46.57
             Declaration of competing interest                                                          Guangdong Province, China with Grant No. 2024A1515010986. We
                                                                                                        also appreciate the valuable comments and constructive suggestions
                 No conflict of interest exists in the submission of this manuscript,                   from the anonymous reviewers that helped improve the manuscript.
             and all authors approve the manuscript for publication. All the authors
             listed have approved the manuscript that is enclosed. We sincerely                         References
             appreciate your consideration of our manuscript and look forward to
             receiving comments from the reviewers.                                                     Armeni, I., Sener, O., Zamir, A.R., Jiang, H., Brilakis, I., Fischer, M., Savarese, S.,
                                                                                                            2016. 3D semantic parsing of large-scale indoor spaces. In: Proceedings of the
             Data availability                                                                              IEEE Conference on Computer Vision and Pattern Recognition. pp. 1534‚Äì1543.
                                                                                                        Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Stachniss, C., Gall, J.,
                 No data was used for the research described in the article.                                2019. Semantickitti: A dataset for semantic scene understanding of lidar sequences.
                                                                                                            In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp.
                                                                                                            9297‚Äì9307.
             Acknowledgments                                                                            Cho, H., Choi, I.S., 2018. Three-dimensionally embedded graph convolutional network
                                                                                                            (3dgcn) for molecule interpretation. arXiv preprint arXiv:1811.09794.
                 The authors would like to thank the National Natural Science Foun-                     Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,
                                                                                                            Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al., 2020. Rethinking attention
             dation of China, and Basic and Applied Basic Research Foundation of                            with performers. arXiv preprint arXiv:2009.14794.
                                                                                                    17 
              T. Han et al.                                                                                    International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
              Choy, C., Gwak, J., Savarese, S., 2019. 4D spatio-temporal convnets: Minkowski                 Lei, X., Guan, H., Ma, L., Liu, J., Yu, Y., Wang, L., Dong, Z., Ni, H., Li, J., 2024.
                  convolutional neural networks. In: Proceedings of the IEEE/CVF Conference on                   DAAL-WS: A weakly-supervised method integrated with data augmentation and
                  Computer Vision and Pattern Recognition. pp. 3075‚Äì3084.                                        active learning strategies for MLS point cloud semantic segmentation. Int. J. Appl.
              Cotella, V.A., 2023. From 3D point clouds to HBIM: Application of artificial intelligence          Earth Obs. Geoinf. 131, 103970.
                  in cultural heritage. Autom. Constr. 152, 104936.                                          Lei, X., Guan, H., Ma, L., Yu, Y., Dong, Z., Gao, K., Delavar, M.R., Li, J., 2022.
              Cui, Y., Liu, X., Liu, H., Zhang, J., Zare, A., Fan, B., 2021. Geometric atten-                    WSPointNet: A multi-branch weakly supervised learning network for semantic
                  tional dynamic graph convolutional neural networks for point cloud analysis.                   segmentation of large-scale mobile laser scanning point clouds. Int. J. Appl. Earth
                  Neurocomputing 432, 300‚Äì310.                                                                   Observ. Geoinf. 115, 103129.
              Dai, A., Chang, A.X., Savva, M., Halber, 2017. Scannet: Richly-annotated 3d reconstruc-        Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., 2018. Pointcnn: Convolution on
                  tions of indoor scenes. In: Proceedings of the IEEE Conference on Computer Vision              x-transformed points. Adv. Neural Inf. Process. Syst. 31.
                  and Pattern Recognition. pp. 5828‚Äì5839.                                                    Li, Q., Cao, R., Zhu, J., Fu, H., Zhou, B., Fang, X., Jia, S., Zhang, S., Liu, K., Li, Q.,
              De G√©lis, I., Lef√®vre, S., Corpetti, T., 2023. Siamese KPConv: 3D multiple change                  2023. Learn then match: A fast coarse-to-fine depth image-based indoor localization
                  detection from raw point clouds using deep learning. ISPRS J. Photogramm. Remote               framework for dark environments via deep learning and keypoint-based geometry
                  Sens. 197, 274‚Äì291.                                                                            alignment. ISPRS J. Photogramm. Remote Sens. 195, 169‚Äì177.
              Diao, C., Loynd, R., 2022. Relational attention: Generalizing transformers for                 Li, M., Lin, S., Wang, Z., Shen, Y., Zhang, B., Ma, L., 2024. Class-imbalanced
                  graph-structured tasks. arXiv preprint arXiv:2210.05062.                                       semi-supervised learning for large-scale point cloud semantic segmentation via
              Du, J., Cai, G., Wang, Z., Huang, S., Su, J., Junior, J.M., Smit, J., Li, J., 2021.                decoupling optimization. arXiv preprint arXiv:2401.06975.
                  ResDLPS-Net: Joint residual-dense optimization for large-scale point cloud semantic        Li, Y., Ma, L., Zhong, Z., Cao, D., Li, J., 2019b. TGNet: Geometric graph CNN on 3-D
                  segmentation. ISPRS J. Photogramm. Remote Sens. 182, 37‚Äì51.                                    point cloud segmentation. IEEE Trans. Geosci. Remote Sens. 58 (5), 3588‚Äì3600.
              Duan, L., Zhao, S., Xue, N., Gong, M., Xia, G.-S., Tao, D., 2024. ConDaFormer:                 Li, G., Muller, M., Thabet, A., Ghanem, B., 2019a. Deepgcns: Can gcns go as deep
                  Disassembled transformer with local structure enhancement for 3D point cloud                   as cnns? In: Proceedings of the IEEE/CVF International Conference on Computer
                  understanding. Adv. Neural Inf. Process. Syst. 36.                                             Vision. pp. 9267‚Äì9276.
              Geng, Y., Wang, Z., Jia, L., Qin, Y., Chai, Y., Liu, K., Tong, L., 2023. 3DGraphSeg:           Li, M., Xie, Y., Shen, Y., Ke, B., Qiao, R., Ren, B., Lin, S., Ma, L., 2022. Hybridcr:
                  A unified graph representation-based point cloud segmentation framework for                    Weakly-supervised 3d point cloud semantic segmentation via hybrid contrastive
                  full-range highspeed railway environments. IEEE Trans. Ind. Inform..                           regularization. In: Proceedings of the IEEE/CVF Conference on Computer Vision
              Guo, M.-H., Cai, J.-X., Liu, Z.-N., Mu, T.-J., Martin, R.R., Hu, S.-M., 2021. Pct: Point           and Pattern Recognition. pp. 14930‚Äì14939.
                  cloud transformer. Comput. Vis. Media 7, 187‚Äì199.                                          Lin, Z.-H., Huang, S.-Y., Wang, Y.-C.F., 2020. Convolution in the cloud: Learning
              Han, J., Liu, Y., Rong, M., Zheng, X., Shen, S., 2023. FloorUSG: Indoor floorplan                  deformable kernels in 3d graph convolution networks for point cloud analysis.
                  reconstruction by unifying 2D semantics and 3D geometry. ISPRS J. Photogramm.                  In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                  Remote Sens. 196, 490‚Äì501.                                                                     Recognition. pp. 1800‚Äì1809.
              Han, B., Zhang, X., Ren, S., 2022. PU-GACNet: Graph attention convolution network              Lin, Y., Vosselman, G., Yang, M.Y., 2022. Weakly supervised semantic segmentation
                  for point cloud upsampling. Image Vis. Comput. 118, 104371.                                    of airborne laser scanning point clouds. ISPRS J. Photogram. Remote Sens. 187,
              He, S., Jiang, X., Jiang, W., Ding, H., 2023. Prototype adaption and projection                    79‚Äì100.
                  for few-and zero-shot 3d point cloud semantic segmentation. IEEE Trans. Image              Lin, H., Wu, S., Chen, Y., Li, W., Luo, Z., Guo, Y., Wang, C., Li, J., 2021. Semantic seg-
                  Process..                                                                                      mentation of 3D indoor LiDAR point clouds through feature pyramid architecture
              Hu, Q., Yang, B., Fang, G., Guo, Y., Leonardis, A., Trigoni, N., Markham, A., 2022.                search. ISPRS J. Photogramm. Remote Sens. 177, 279‚Äì290.
                  Sqn: Weakly-supervised semantic segmentation of large-scale 3d point clouds. In:           Lin, H., Zheng, X., Li, L., Chao, F., Wang, S., Wang, Y., Tian, Y., Ji, R., 2023. Meta
                  European Conference on Computer Vision. Springer, pp. 600‚Äì619.                                 architecture for point cloud analysis. In: Proceedings of the IEEE/CVF Conference
              Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., Trigoni, N., Markham, A., 2021.            on Computer Vision and Pattern Recognition. pp. 17682‚Äì17691.
                  Learning semantic segmentation of large-scale point clouds with random sampling.           Liu, X., Cheng, J., Song, Y., Jiang, X., 2022a. Boosting graph structure learning
                  IEEE Trans. Pattern Anal. Mach. Intell. 44 (11), 8338‚Äì8354.                                    with dummy nodes. In: International Conference on Machine Learning. PMLR, pp.
              Huang, Q., Wang, W., Neumann, U., 2018. Recurrent slice networks for 3d segmentation               13704‚Äì13716.
                  of point clouds. In: Proceedings of the IEEE Conference on Computer Vision and             Liu, J., Guan, H., Lei, X., Yu, Y., 2023a. Weakly supervised semantic segmentation of
                  Pattern Recognition. pp. 2626‚Äì2635.                                                            mobile laser scanning point clouds via category balanced random annotation and
              Huang, Z., Zhao, Z., Li, B., Han, J., 2023. Lcpformer: Towards effective 3d point cloud            deep consistency-guided self-distillation mechanism. Photogramm. Rec. 38 (184),
                  analysis via local context propagation in transformers. IEEE Trans. Circuits Syst.             581‚Äì602.
                  Video Technol..                                                                            Liu, Y., Wang, L., Liu, M., Lin, Y., Zhang, X., Oztekin, B., Ji, S., 2022b. Spherical
              Huang, W., Zou, P., Xia, Y., Wen, C., Zang, Y., Wang, C., Zhou, G., 2024. OPOCA: One               message passing for 3d molecular graphs. In: International Conference on Learning
                  point one class annotation for LiDAR point cloud semantic segmentation. IEEE                   Representations. ICLR, p. 1.
                  Trans. Geosci. Remote Sens..                                                               Liu, J., Yu, Z., Breckon, T.P., Shum, H.P., 2024. U3DS3: Unsupervised 3D semantic
              Jiang, C., Huang, K., Wu, J., Wang, X., Xiao, J., Hussain, A., 2023a. PointGS: Bridging            scene segmentation. In: Proceedings of the IEEE/CVF Winter Conference on
                  and fusing geometric and semantic space for 3D point cloud analysis. Inf. Fusion               Applications of Computer Vision. pp. 3759‚Äì3768.
                  91, 316‚Äì326.                                                                               Liu, W., Zang, Y., Xiong, Z., Bian, X., Wen, C., Lu, X., Wang, C., Junior, J.M.,
              Jiang, J., Li, F., Yang, J., Kang, Z., Li, J., 2023b. Construction of indoor obstacle              Goncalves, W.N., Li, J., 2023b. 3D building model generation from MLS point
                  element map based on scene-aware priori obstacle rules. ISPRS J. Photogramm.                   cloud and 3D mesh using multi-source data fusion. Int. J. Appl. Earth Obs. Geoinf.
                  Remote Sens. 195, 43‚Äì64.                                                                       116, 103171.
              Jiang, L., Zhao, H., Liu, S., Shen, X., Fu, C.-W., Jia, J., 2019. Hierarchical point-edge      Luo, C., Li, X., Cheng, N., Li, H., Lei, S., Li, P., 2022. Mvp-net: Multiple view pointwise
                  interaction network for point cloud semantic segmentation. In: Proceedings of the              semantic segmentation of large-scale point clouds. arXiv preprint arXiv:2201.12769.
                  IEEE/CVF International Conference on Computer Vision. pp. 10433‚Äì10441.                     Meyer, T., Brunn, A., Stilla, U., 2023. Geometric BIM verification of indoor construction
              Kolodiazhnyi, M., Vorontsova, A., Konushin, A., Rukhovich, D., 2023. OneFormer3D:                  sites by photogrammetric point clouds and evidence theory. ISPRS J. Photogramm.
                  One transformer for unified point cloud segmentation. arXiv preprint arXiv:2311.               Remote Sens. 195, 432‚Äì445.
                  14405.                                                                                     Park, J., Lee, S., Kim, S., Xiong, Y., Kim, H.J., 2023. Self-positioning point-based
              Lai, X., Liu, J., Jiang, L., Wang, L., Zhao, H., Liu, S., Qi, X., Jia, J., 2022. Stratified        transformer for point cloud understanding. In: Proceedings of the IEEE/CVF
                  transformer for 3d point cloud segmentation. In: Proceedings of the IEEE/CVF                   Conference on Computer Vision and Pattern Recognition. pp. 21814‚Äì21823.
                  Conference on Computer Vision and Pattern Recognition. pp. 8500‚Äì8509.                      Qi, X., Liao, R., Jia, J., Fidler, S., Urtasun, R., 2017c. 3D graph neural networks for
              Landrieu, L., Boussaha, M., 2019. Point cloud oversegmentation with graph-structured               rgbd semantic segmentation. In: Proceedings of the IEEE International Conference
                  deep metric learning. In: Proceedings of the IEEE/CVF Conference on Computer                   on Computer Vision. pp. 5199‚Äì5208.
                  Vision and Pattern Recognition. pp. 7440‚Äì7449.                                             Qi, C.R., Su, H., Mo, K., Guibas, L.J., 2017a. Pointnet: Deep learning on point sets
              Landrieu, L., Simonovsky, M., 2018. Large-scale point cloud semantic segmentation with             for 3d classification and segmentation. In: Proceedings of the IEEE Conference on
                  superpoint graphs. In: Proceedings of the IEEE Conference on Computer Vision and               Computer Vision and Pattern Recognition. pp. 652‚Äì660.
                  Pattern Recognition. pp. 4558‚Äì4567.                                                        Qi, C.R., Yi, L., Su, H., Guibas, L.J., 2017b. Pointnet++: Deep hierarchical feature
              Lee, M.S., Yang, S.W., Han, S.W., 2023. Gaia: Graphical information gain based                     learning on point sets in a metric space. Adv. Neural Inf. Process. Syst. 30.
                  attention network for weakly supervised point cloud semantic segmentation. In:             Qian, G., Hammoud, H., Li, G., Thabet, A., Ghanem, B., 2021. Assanet: An anisotropic
                  Proceedings of the IEEE/CVF Winter Conference on Applications of Computer                      separable set abstraction for efficient point cloud representation learning. Adv.
                  Vision. pp. 582‚Äì591.                                                                           Neural Inf. Process. Syst. 34, 28119‚Äì28130.
              Lei, H., Akhtar, N., Mian, A., 2020a. Seggcn: Efficient 3d point cloud segmentation with       Qian, G., Li, Y., Peng, H., Mai, J., Hammoud, H., Elhoseiny, M., Ghanem, B., 2022.
                  fuzzy spherical kernel. In: Proceedings of the IEEE/CVF Conference on Computer                 Pointnext: Revisiting pointnet++ with improved training and scaling strategies.
                  Vision and Pattern Recognition. pp. 11611‚Äì11620.                                               Adv. Neural Inf. Process. Syst. 35, 23192‚Äì23204.
                                                                                                             Ramp√°≈°ek, L., Galkin, M., Dwivedi, V.P., Luu, A.T., Wolf, G., Beaini, D., 2022. Recipe
              Lei, H., Akhtar, N., Mian, A., 2020b. Spherical kernel for efficient graph convolution             for a general, powerful, scalable graph transformer. Adv. Neural Inf. Process. Syst.
                  on 3d point clouds. IEEE Trans. Pattern Anal. Mach. Intell. 43 (10), 3664‚Äì3680.                35, 14501‚Äì14515.
                                                                                                         18 
              T. Han et al.                                                                                     International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
              Ran, H., Liu, J., Wang, C., 2022. Surface representation for point clouds. In: Proceedings       Wu, X., Jiang, L., Wang, P.-S., Liu, Z., Liu, X., Qiao, Y., Ouyang, W., He, T.,
                  of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.                       Zhao, H., 2023. Point transformer v3: Simpler, faster, stronger. arXiv preprint
                  18942‚Äì18952.                                                                                     arXiv:2312.10035.
              Robert, D., Raguet, H., Landrieu, L., 2023. Efficient 3D semantic segmentation with              Wu, X., Lao, Y., Jiang, L., Liu, X., Zhao, H., 2022. Point transformer v2: Grouped
                  superpoint transformer. arXiv preprint arXiv:2306.08045.                                         vector attention and partition-based pooling. Adv. Neural Inf. Process. Syst. 35,
              Shi, W., Rajkumar, R., 2020. Point-gnn: Graph neural network for 3d object detection                 33330‚Äì33342.
                  in a point cloud. In: Proceedings of the IEEE/CVF Conference on Computer Vision              Wu, W., Qi, Z., Fuxin, L., 2019. Pointconv: Deep convolutional networks on 3d point
                  and Pattern Recognition. pp. 1711‚Äì1719.                                                          clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
              Srivastava, S., Sharma, G., 2021. Exploiting local geometry for feature and graph                    Pattern Recognition. pp. 9621‚Äì9630.
                  construction for better 3d point cloud processing with graph neural networks. In:            Xia, S., Yue, J., Kania, K., Fang, L., Tagliasacchi, A., Yi, K.M., Sun, W., 2023. Densify
                  2021 IEEE INternational Conference on Robotics and Automation. ICRA, IEEE, pp.                   your labels: Unsupervised clustering with bipartite matching for weakly supervised
                  12903‚Äì12909.                                                                                     point cloud segmentation. arXiv preprint arXiv:2312.06799.
              Stilla, U., Xu, Y., 2023. Change detection of urban objects using 3D point clouds: A             Xiao, A., Huang, J., Guan, D., Zhang, X., Lu, S., Shao, L., 2023. Unsupervised point
                  review. ISPRS J. Photogramm. Remote Sens. 197, 228‚Äì255.                                          cloud representation learning with deep neural networks: A survey. IEEE Trans.
              Su, Y., Cheng, M., Yuan, Z., Liu, W., Zeng, W., Wang, C., 2023a. Multi-stage scene-level             Pattern Anal. Mach. Intell..
                  constraints for large-scale point cloud weakly supervised semantic segmentation.             Xie, L., Furuhata, T., Shimada, K., 2020. Multi-resolution graph neural network for
                  IEEE Trans. Geosci. Remote Sens..                                                                large-scale pointcloud segmentation. arXiv preprint arXiv:2009.08924.
              Su, Y., Cheng, M., Yuan, Z., Liu, W., Zeng, W., Zhang, Z., Wang, C., 2023b. Spatial              Xu, X., Lee, G.H., 2020. Weakly supervised semantic point cloud segmentation: Towards
                  adaptive fusion consistency contrastive constraint: weakly supervised building                   10x fewer labels. In: Proceedings of the IEEE/CVF Conference on Computer Vision
                  facade point cloud semantic segmentation. IEEE Trans. Geosci. Remote Sens..                      and Pattern Recognition. pp. 13706‚Äì13715.
              Tan, W., Qin, N., Ma, L., Li, Y., Du, J., Cai, G., Yang, K., Li, J., 2020. Toronto-3D: A         Xu, Q., Sun, X., Wu, C.-Y., Wang, P., Neumann, U., 2020. Grid-gcn for fast and scalable
                  large-scale mobile LiDAR dataset for semantic segmentation of urban roadways.                    point cloud learning. In: Proceedings of the IEEE/CVF Conference on Computer
                  In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern                        Vision and Pattern Recognition. pp. 5661‚Äì5670.
                  Recognition Workshops. pp. 202‚Äì203.                                                          Xu, X., Wang, Z., Zhou, J., Lu, J., 2023b. Binarizing sparse convolutional networks
              Tang, H., Liu, Z., Zhao, S., Lin, Y., Lin, J., Wang, H., Han, S., 2020. Searching efficient          for efficient point cloud analysis. In: Proceedings of the IEEE/CVF Conference on
                  3d architectures with sparse point-voxel convolution. In: European Conference on                 Computer Vision and Pattern Recognition. pp. 5313‚Äì5322.
                  Computer Vision. Springer, pp. 685‚Äì702.                                                      Xu, Z., Yuan, B., Zhao, S., Zhang, Q., Gao, X., 2023c. Hierarchical point-based active
              Tang, L., Zhan, Y., Chen, Z., Yu, B., Tao, D., 2022. Contrastive boundary learning for               learning for semi-supervised point cloud semantic segmentation. In: Proceedings of
                  point cloud segmentation. In: Proceedings of the IEEE/CVF Conference on Computer                 the IEEE/CVF International Conference on Computer Vision. pp. 18098‚Äì18108.
                  Vision and Pattern Recognition. pp. 8489‚Äì8499.                                               Xu, P., Zhang, L., Liu, X., Sun, J., Zhao, Y., Yang, H., Yu, B., 2023a. Do not train
              Tatarchenko, M., Park, J., Koltun, V., Zhou, Q.-Y., 2018. Tangent convolutions for dense             it: a linear neural architecture search of graph neural networks. In: International
                  prediction in 3d. In: Proceedings of the IEEE Conference on Computer Vision and                  Conference on Machine Learning. PMLR, pp. 38826‚Äì38847.
                  Pattern Recognition. pp. 3887‚Äì3896.                                                          Yan, X., Zheng, C., Li, Z., Wang, S., Cui, S., 2020. Pointasnl: Robust point clouds
              Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese, S., 2017. Segcloud: Semantic                  processing using nonlocal neural networks with adaptive sampling. In: Proceedings
                  segmentation of 3d point clouds. In: 2017 International Conference on 3D Vision.                 of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
                  3DV, IEEE, pp. 537‚Äì547.                                                                          5589‚Äì5598.
              Te, G., Hu, W., Zheng, A., Guo, Z., 2018. Rgcnn: Regularized graph cnn for point                 Yao, B., Dong, L., Qiu, X., Song, K., Yan, D., Peng, C., 2024. Uncertainty-guided
                  cloud segmentation. In: Proceedings of the 26th ACM International Conference on                  contrastive learning for weakly supervised point cloud segmentation. IEEE Trans.
                  Multimedia. pp. 746‚Äì754.                                                                         Geosci. Remote Sens..
              Thomas, H., Qi, C.R., Deschaud, J.-E., Marcotegui, B., Goulette, F., Guibas, L.J., 2019.         Ye, Y., Ji, S., 2021. Sparse graph attention networks. IEEE Trans. Knowl. Data Eng. 35
                  Kpconv: Flexible and deformable convolution for point clouds. In: Proceedings of                 (1), 905‚Äì916.
                  the IEEE/CVF International Conference on Computer Vision. pp. 6411‚Äì6420.                     Yun, S., Jeong, M., Kim, R., Kang, J., Kim, H.J., 2019. Graph transformer networks.
              Tian, B., Luo, L., Zhao, H., Zhou, G., 2022. Vibus: Data-efficient 3d scene parsing with             Adv. Neural Inf. Process. Syst. 32.
                  viewpoint bottleneck and uncertainty-spectrum modeling. ISPRS J. Photogramm.                 Zeng, Z., Xu, Y., Xie, Z., Tang, W., Wan, J., Wu, W., 2024. Large-scale point cloud
                  Remote Sens. 194, 302‚Äì318.                                                                       semantic segmentation via local perception and global descriptor vector. Expert
              Topping, J., Di Giovanni, F., Chamberlain, B.P., Dong, X., Bronstein, M.M., 2021.                    Syst. Appl. 246, 123269.
                  Understanding over-squashing and bottlenecks on graphs via curvature. arXiv                  Zhang, L., Bi, Y., 2024. Weakly-supervised point cloud semantic segmentation based
                  preprint arXiv:2111.14522.                                                                       on dilated region. IEEE Trans. Geosci. Remote Sens..
              Wan, J., Xu, Y., Qiu, Q., Xie, Z., 2023. A geometry-aware attention network for semantic         Zhang, F., Fang, J., Wah, B., Torr, P., 2020. Deep fusionnet for point cloud semantic seg-
                  segmentation of MLS point clouds. Int. J. Geogr. Inf. Sci. 37 (1), 138‚Äì161.                      mentation. In: Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow,
              Wang, P.-S., 2023. Octformer: Octree-based transformers for 3d point clouds. ACM                     UK, August 23‚Äì28, 2020, Proceedings, Part XXIV 16. Springer, pp. 644‚Äì663.
                  Trans. Graph. 42 (4), 1‚Äì11.                                                                  Zhang, Y., Qu, Y., Xie, Y., Li, Z., Zheng, S., Li, C., 2021. Perturbed self-distillation:
              Wang, L., Huang, Y., Hou, Y., Zhang, S., Shan, J., 2019a. Graph attention convolution                Weakly supervised large-scale point cloud semantic segmentation. In: Proceedings
                  for point cloud semantic segmentation. In: Proceedings of the IEEE/CVF Conference                of the IEEE/CVF International Conference on Computer Vision. pp. 15520‚Äì15528.
                  on Computer Vision and Pattern Recognition. pp. 10296‚Äì10305.                                 Zhang, C., Wan, H., Shen, X., Wu, Z., 2022a. Patchformer: An efficient point transformer
              Wang, C., Samari, B., Siddiqi, K., 2018a. Local spectral graph convolution for point set             with patch attention. In: Proceedings of the IEEE/CVF Conference on Computer
                  feature learning. In: Proceedings of the European Conference on Computer Vision.                 Vision and Pattern Recognition. pp. 11799‚Äì11808.
                  ECCV, pp. 52‚Äì66.                                                                             Zhang, Z., Wang, X., Guan, C., Zhang, Z., Li, H., Zhu, W., 2022b. Autogt: Automated
              Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., 2019b. Dynamic              graph transformer architecture search. In: The Eleventh International Conference
                  graph cnn for learning on point clouds. ACM Trans. Graphics (TOG) 38 (5), 1‚Äì12.                  on Learning Representations. p. 1.
              Wang, S., Suo, S., Ma, W.-C., Pokrovsky, A., Urtasun, R., 2018b. Deep parametric                 Zhang, R., Wang, L., Guo, Z., Shi, J., 2023a. Nearest neighbors meet deep neural net-
                  continuous convolutional neural networks. In: Proceedings of the IEEE Conference                 works for point cloud analysis. In: Proceedings of the IEEE/CVF Winter Conference
                  on Computer Vision and Pattern Recognition. pp. 2589‚Äì2597.                                       on Applications of Computer Vision. pp. 1246‚Äì1255.
              Wang, Z., Wang, Y., An, L., Liu, J., Liu, H., 2022. Local transformer network on 3d              Zhang, Z., Yang, B., Wang, B., Li, B., 2023b. Growsp: Unsupervised semantic segmenta-
                  point cloud semantic segmentation. Information 13 (4), 198.                                      tion of 3d point clouds. In: Proceedings of the IEEE/CVF Conference on Computer
              Wang, P., Yao, W., 2022. A new weakly supervised approach for ALS point cloud                        Vision and Pattern Recognition. pp. 17619‚Äì17629.
                  semantic segmentation. ISPRS J. Photogramm. Remote Sens. 188, 237‚Äì254.                       Zhao, H., Jiang, L., Fu, C.-W., Jia, J., 2019. Pointweb: Enhancing local neighborhood
              Wang, P., Yao, W., Shao, J., 2023. One class one click: Quasi scene-level weakly                     features for point cloud processing. In: Proceedings of the IEEE/CVF Conference
                  supervised point cloud semantic segmentation with active learning. ISPRS J.                      on Computer Vision and Pattern Recognition. pp. 5565‚Äì5573.
                  Photogramm. Remote Sens. 204, 89‚Äì104.                                                        Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V., 2021. Point transformer. In:
              Wang, Y., Zhang, Z., Zhong, R., Sun, L., Leng, S., Wang, Q., 2021. Densely connected                 Proceedings of the IEEE/CVF International Conference on Computer Vision. pp.
                  graph convolutional network for joint semantic and instance segmentation of indoor               16259‚Äì16268.
                  point clouds. ISPRS J. Photogramm. Remote Sens. 182, 67‚Äì77.                                  Zhou, C., Ling, Q., 2023. GAF-net: Geometric contextual feature aggregation and
              Wei, M., Wei, Z., Zhou, H., Hu, F., Si, H., Chen, Z., Zhu, Z., Qiu, J., Yan, X., Guo, Y.,            adaptive fusion for large-scale point cloud semantic segmentation. IEEE Trans.
                  et al., 2023. Agconv: Adaptive graph convolution on 3d point clouds. IEEE Trans.                 Geosci. Remote Sens. 61, 1‚Äì15.
                  Pattern Anal. Mach. Intell..
              Woo, S., Lee, D., Hwang, S., Kim, W.J., Lee, S., 2023. MKConv: Multidimensional
                  feature representation for point cloud analysis. Pattern Recognit. 143, 109800.
                                                                                                          19 
