                         ResonanceRoPE:ImprovingContextLengthGeneralizationof
                                                     LargeLanguageModels
                                      1,2                  3           1                             3                   1,2†
                   SuyuchenWang ,IvanKobyzev ,PengLu ,MehdiRezagholizadeh and BangLiu
                     1DIRO,Université de Montréal          2Mila - Quebec AI Institute       3Huawei Noah’s Ark Lab
                                      {suyuchen.wang, peng.lu, bang.liu}@umontreal.ca
                                      {ivan.kobyzev, mehdi.rezagholizadeh}@huawei.com
                                      Abstract                           of long texts. Specifically, the train-short-test-long
                     This paper addresses the challenge of train-        (TSTL) scenario (Press et al., 2022) highlights a
                     short-test-long (TSTL) scenarios in Large Lan-      limitation where LLMs, pre-trained on shorter se-
                     guage Models (LLMs) equipped with Rotary            quences, struggle with out-of-distribution (OOD)
                     Position Embedding (RoPE), where models             token positions in longer sequences, impacting
                     pre-trained on shorter sequences face difficulty    their performance in real-world applications (Zhao
                     with out-of-distribution (OOD) token positions      et al., 2023).
                     in longer sequences.   We introduce RESO-              Recent efforts to enhance TSTL performance
                     NANCE ROPE, a novel approach designed to            have focused on LLMs equipped with Rotary Posi-
                     narrow the generalization gap in TSTL scenar-       tion Embedding (RoPE) (Su et al., 2024), such
                     ios by refining the interpolation of RoPE fea-      as LLaMA (Touvron et al., 2023a,b) and Mis-
                     tures for OOD positions, significantly improv-      tral (Jiang et al., 2023), owing to their excep-
                     ing the model performance without additional        tional capabilities and widespread adoption. These
                     online computational costs. Furthermore, we
                     present POSGEN, a new synthetic benchmark           initiatives aim to refine the test-time computa-
                     specifically designed for fine-grained behav-       tion of RoPE position embedding by introducing
                     ior analysis in TSTL scenarios, aiming to iso-      a scaling factor to either the position index of
                     late the constantly increasing difficulty of to-    each token (Chen et al., 2023) or RoPE’s base
                     ken generation on long contexts from the chal-      value (Xiong et al., 2023; Liu et al., 2024; Peng
                     lenges of recognizing new token positions. Our      et al., 2024). These methods ensure that the po-
                     experiments on synthetic tasks show that after      sition embeddings for out-of-distribution (OOD)
                     applying RESONANCE ROPE, Transformers               positions remain within the range experienced dur-
                     recognize OOD position better and more ro-
                     bustly. Our extensive LLM experiments also          ing pre-training. This minimizes the need for the
                     showsuperior performance after applying RES-        model to adapt to new position embedding value
                     ONANCEROPEtothecurrentstate-of-the-art              ranges, a task that is inherently difficult.
                     RoPEscalingmethod,YaRN,onbothupstream                  In this paper, we introduce RESONANCE ROPE,
                     language modeling tasks and a variety of down-      a novel technique designed to further narrow the
                     stream long-text applications.1
                                                                         generalization gap on position embeddings in
                 1 Introduction                                          TSTLscenarios. Recognizing that RoPE’s position
                                                                         embedding is governed by a complex, non-linear
                  Recent advancements in Large Language Models           function, we posit that minimizing extrapolation on
                 (LLMs)havedemonstrated their potential across a         OODpositions, while crucial, is insufficient. We
                 widespectrumofnaturallanguageprocessingtasks,           argue that it is equally vital to address the inter-
                 showcasing their ability to handle complex interac-     polation of RoPE features at the OOD positions.
                 tions, document analyses, professional writing, and     Byimplementing RESONANCE ROPE,weslightly
                 advanced reasoning with a unified approach (Ope-        scale each RoPE feature to correspond to an inte-
                 nAI, 2023; Touvron et al., 2023a,b; Jiang et al.,       ger wavelength. This adjustment aligns each RoPE
                 2024). As these models are increasingly adapted         feature’s wavelength with a specific token span
                 for complex applications, challenges arise in sce-      length, enabling it to "resonate" with a particular
                 narios requiring the comprehension or generation        local context length. This simple modification ef-
                    1https://github.com/sheryc/resonance_rope.           fectively reduces the generalization gap for over
                     †Canada CIFARAIChair. Corresponding author.         half of the position embedding features in LLaMA
                                                                     586
                                  Findings of the Association for Computational Linguistics: ACL 2024, pages 586–598
                                         August 11-16, 2024 ©2024 Association for Computational Linguistics
