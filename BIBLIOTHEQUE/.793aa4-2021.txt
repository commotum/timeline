                              ASimpleandEffectivePositional Encoding for Transformers
                                                                ∗                 ∗                              ∗
                                             Pu-ChinChen,HenryTsai,SrinadhBhojanapalli,
                                          HyungWonChung,Yin-WenChang,Chun-SungFerng
                                                                     Google Research
                                          Abstract                                additional segment embeddings can be added just
                        Transformer models are permutation equivari-              like the position embeddings (Devlin et al., 2018).
                        ant.  To supply the order and type informa-                  Therehavebeenmultipleworksexploringdiffer-
                        tion of the input tokens, position and seg-               ent ways to include position information in Trans-
                        ment embeddings are usually added to the in-              formers (Shaw et al., 2018; Yang et al., 2019; Raf-
                        put. Recent works proposed variations of po-              fel et al., 2020). Many of those note the advan-
                        sitional encodings with relative position en-             tages of using a relative position encoding scheme
                        codings achieving better performance.        Our          over absolute position encodings (see also Fig 1).
                        analysis shows that the gain actually comes               However what causes this difference is not clear.
                        from moving positional information to atten-              Yun et al. (2020) have shown that Transformers
                        tion layer from the input. Motivated by this,
                        we introduce Decoupled posItional attEntion               with absolute position encodings are universal ap-
                        for Transformers (DIET), a simple yet effec-              proximators of all sequence to sequence functions,
                        tive mechanism to encode position and seg-                proving that absolute position encodings can cap-
                        ment information into the Transformer mod-                ture the position information. Hence what causes
                        els. The proposed method has faster training              the superiority of relative position encodings? A
                        and inference time, while achieving compet-               systematic study and understanding of the beneﬁts
                        itive performance on GLUE, XTREME and                     anddrawbacksofdifferent position encoding meth-
                        WMTbenchmarks. We further generalize our                  ods is missing. Ke et al. (2020) hypothesised that
                        method to long-range transformers and show
                        performance gain.                                         the cross correlation between word and position
                   1    Introduction                                              embeddings while computing attention could be
                                                                                  the cause of poor performance of absolute position
                   Transformers are sequence-to-sequence models                   encodings. However such cross terms are present
                   that achieve state of the art performance in many              in some of the relative position encoding methods
                   Natural Language Processing (NLP) tasks, such as               (Shaw et al., 2018; Yang et al., 2019), and these
                   machine translation, language modeling and ques-               methods perform on par or better than the other
                   tion answering (Vaswani et al., 2017; Devlin et al.,           position encoding schemes (see §4).
                   2018; Yang et al., 2019; Liu et al., 2020). Trans-                In this paper we undertake a systematic study
                   formers have two major components: self-attention              to understand different position encoding methods.
                   and a position-wise feed forward layer. Both are               Wearguethatabsolutepositionembeddingsmainly
                   permutation equivariant and are not sensitive to               suffer from being addedattheinput. Weshow,with
                   the order of input tokens. To make these mod-                  our experiments on classiﬁcation, question answer-
                   els position-aware, the position information of the            ing and machine translation tasks, that absolute po-
                   input words is typically added as an additional em-            sition encodings added to attention matrices with
                   bedding to the input token embeddings (Vaswani                 different parameters for each head improves sig-
                   et al., 2017). For example, input embedding (W)                niﬁcantly over absolute position encodings added
                   of a sentence is added to the position embeddings              to the input. This highlights that where the posi-
                   (P), resulting in input W + P to the Transformer.              tion information is included in the Transformer is
                   These position embeddings only depend on the lo-               important, providing an explanation for the gap in
                   cation the word appears. For multi-segment tasks,              performance between absolute and relative posi-
                        ∗ The authors contribute equally to this paper. Corre-    tion encodings. We also compare different position
                   sponding author email: puchin@google.com                       encodings and the effect of sharing position encod-
                                                                             2974
                           Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2974–2988
                                                                   c
                                            November7–11,2021. 2021Association for Computational Linguistics
                           (a) English Transfer Learning on        (b) Cross-lingual Transfer on            (c) Translation on CS-EN
                                      MultiNLI                                 XNLI
                    Figure 1: Performance effect of different positional encoding methods for Transformers (see § 2) on two Natural
                    language Inference datasets from GLUE (Wang et al., 2019), XTREME (Hu et al., 2020) and one Neural Machine
                    Translation dataset WMT 18 (Bojar et al., 2018). Absolute positional encoding (DIET-ABS) can achieve better
                    performance than the relative counterpart (DIET-REL), showing the importance of designing the right position
                    encoding method.
                    ings across different heads and layers of a Trans-              2.1    Transformer
                    former. Based on these observations we propose                  ATransformerblockconsistsoftwotypesoflayers:
                    decoupled positional attention and a new segment                1) Self-attention layer and 2) Feed forward layers.
                    encoding approach (for tasks with multiple seg-
                    ments), and empirically show its superiority.                   Self-Attention Module           Given input sequence
                       Wesummarize our contributions in this paper                  length n, hidden size d, multi-head query-key
                    below.                                                          down-projection size dh, we deﬁne hidden layer
                                                                                    input to this attention head as X ∈ Rn×d, the query
                       • Wetheoretically and empirically analyze the                projection matrix as Wi ∈ Rd×dh, the key projec-
                                                                                                                Q
                          limitation of the absolute position embeddings            tion matrix as Wi ∈ Rd×dh and the value projec-
                                                                                                        K
                                                                                                         i       d×d
                          added to the input. For both absolute and                 tion matrix as W        ∈R h,i∈[h],forhheads.
                          relative information, we show that encoding                                    V
                                                                                    Usually, dh < d as we do multi-head attention
                          position to attention matrix per-head results             with a smaller representation per head (dh = d/h).
                          in superior performance.                                  With that we can write dot-product attention score:
                       • Weproposeasimpleandefﬁcientwaytoen-                                         i           i          i  >
                                                                                                  A =(XW )(XW )
                          code position and segment information. The                                             Q         K
                          proposed encoding matches the SoTA meth-                  This attention score is used to compute the output
                          ods on multiple standard NLP tasks while                  for each head, after scaling and per row normaliza-
                          having a simpler model with lower train-                  tion using softmax:
                          ing/inference costs.                                                                        √
                                                                                           headi = Softmax(Ai/ d)·(XWi )
                       • Ourproposed method can be easily applied to                                                                 V
                          long sequence models (DIET-ABSLIN) and                    Output of all attention heads in a layer are concate-
                          improve all metrics compared with Linformer               nated and passed to the next feed-forward layer
                          (Wangetal., 2020).                                        applied token-wise.
                       • Wepresent ablation studies comparing differ-               2.2    Position Aware Self Attention
                          ent position encoding methods and ways of
                          sharing position encoding parameters across               ManyNLPtasks,suchasmachinetranslation, lan-
                          heads and layers in Transformer.                          guage modeling, are sensitive to the ordering of
                                                                                    input words. Since Transformers are permutation
                    2    Position Encoding for Transformers                         equivariant, we usually additionally include the po-
                    In this section, we brieﬂy review the Transformer               sition information in the input. Below we discuss
                    models (Vaswani et al., 2017) and discuss previ-                someofthepopular position encoding methods.
                    ous improvement of position encoding and analyze                2.2.1    Absolute Position Encodings
                    the limitation of the additive position embedding               Absolute position encodings are computed in the
                    proposed in the initial and widely-adopted Trans-               input layer and are summed with the input token
                    former model.                                                   embeddings. Vaswani et al. (2017) proposed this
                                                                               2975
                 for Transformers and it has been a popular choice
                 in the followup works (Radford et al., 2018; Devlin
                 et al., 2018). There are two common variations of
                 the absolute position encodings - ﬁxed and learned.
                 2.2.2  Relative Position Encodings
                 Onedrawbackofabsolutepositionencodingisthat
                 it requires ﬁxed length of input sequence and does
                 not directly capture relative positions to each word.
                 Tosolve these problems several relative positions
                 schemes have been proposed.
                   Shawetal. (2018) proposed using relative posi-      Figure 2: Rank of attention matrices: We present a
                 tion encodinginsteadofabsolutepositionencoding,       comparison of the rank of the attention score matrices
                 andaddposition embeddings to the key and option-      of a BERTBASE model with absolute position embed-
                 ally value projections instead of the input. They     dings at input v.s. absolute position embeddings per-
                 show that this new way of encoding position in-       head (DIET-ABS (1)). With additive positional embed-
                 formation leads to better performance on machine      ding at input, the attention matrices have much lower
                                                                       rank, limiting the representative power. This is allevi-
                 translation tasks. Yang et al. (2019) simpliﬁed this  ated by DIET-ABS.
                 by removing the position embeddings in value pro-
                 jections and showed better performance on the lan-    Theorem 1. Let P ∈ Rn×d be the input position
                 guage modeling tasks. Both these approaches use                               n×d
                                                                                        ˆ         p
                 a vector representation to encode position informa-   embedding and P ∈ R          be the layer-wise po-
                                                                       sition embeddings. Let W ,W          ∈ Rd×dh be
                 tion.                                                                             Q     K
                   Raffel et al. (2020) use scalars to encode rela-    the query and key projection matrices with head
                                                                       projection size d , and d     < d ,d and n ≥
                 tive position between query and key indices and                         h        h       p
                                                                                                            >           >
                                                                       d +dp. Let Aa = (X+P)W W (X+P)
                 add directly to the attention scores matrix. They      h                               Q   K
                                                                                             > >      ˆ ˆ>
                                                                       and Ar = XW W X +PP betheatten-
                 further use logarithmic binning of position infor-                     Q    K
                 mation into a ﬁxed number of buckets. All these       tion matrices computed using input and layer-wise
                 relative position methods further share the position  position embeddings respectively. Then for any
                                                                       X,P,W ,W
                 encoding parameters across layers.                             Q     K
                   Recently Ke et al. (2020) hypothesised that the                     rank(A ) ≤ d .
                 cross correlation between position and token em-                               a      h
                 beddings can result in weaker performance of ad-                                 ˆ
                                                                       There exists a choice of X,P,W ,W        such that
                 ditive absolute position embeddings and instead                                        Q     K
                 proposed to add both absolute and relative posi-                 rank(A ) = d +d >d .
                                                                                          r      p     h    h
                 tional information based attention directly in each   Remarks. Thistheoremshowsusthattherankof
                 head. However such cross terms are present in the
                 methodproposedbyShawetal.(2018),whichdoes             attention matrices is constrained with the absolute
                 competitively with other approaches. We instead       position encodings at the input and using per-head
                 hypothesise that position encodings at input limit    position encodings by adding position information
                 the rank of the position attention matrix leading to  to attention matrix directly results in allowing for
                 its poor performance.                                 higher rank attention. See § B for the proof.
                                                                          Adding the position encodings directly to the in-
                 2.3  Limitations of the Input Additive                put further places a constraint on training dynamics
                      Position Embedding                               byforcing gradients to be same for both the input
                 In this section we discuss some limitations of the    token and position embeddings (see § B). Relative
                 de facto way of adding absolute position encodings    position encodings discussed earlier, while address-
                 to the input token embeddings.                        ing some of these concerns, suffer from slower
                   We ﬁrst compare the representation power in         training/inference times (see Table 1) with com-
                 terms of the rank of attention matrices achievable    plex implementations (Shaw et al. (2018); Ke et al.
                 with different position encodings.                    (2020)). In the next section, we present simple posi-
                                                                       tion encoding methods that avoid these limitations.
                                                                   2976
                 3   Proposed Position and Segment                      adding the position embeddings per-head removes
                     Encodings                                          this constraint and results in better performance.
                 In the previous section, we learned about the limi-       With the decoupled positional embedding, we
                                                                        can increase d to any width k to break the low-
                 tations of input additive positional embeddings and                   p
                 existing works. Based on these observations, we        rank bottleneck shown in Theorem 1. We call such
                 propose two minimal/efﬁcient ways to incorporate       model DIET-ABS-Rank-k. We also address the ef-
                 (absolute/relative) positional encodings along with    ﬁciency issue introduced by one additional matrix
                                                                                              >
                                                                        multiplication (P P ). As the positional embed-
                 a novel absolute segment encoding approach. By                           Q K
                 decoupling position and segment from token em-         dings are independent of the input, we only need
                 beddings we match the SoTA performance while           to compute the matrix multiplication once for each
                 improving training/inference time (see §3.3).          training batch, and we can cache the computed
                                                                        matrix before running inference. As a result, we
                 3.1   DecoupledAbsolutePositional Attention            observe neglectable training and inference cost in-
                 Weproposethefollowing simple absolute position         crease in this model variant.
                 encoding method that adds position information to      3.2   DecoupledRelative Positional Attention
                 the token attention matrix directly in each attention  Toincorporate relative position inductive bias, we
                 head. We further also add segment information to       consider a simpliﬁed version of the position encod-
                 the token attention instead of the input embeddings.   ing proposed in T5 (Raffel et al., 2020) without
                 This way we can set the rank of position encodings     log-binning and per-layer parameter sharing. We
                 independently resulting in higher rank attention       further also incorporate our per-head segment en-
                 matrix, addressing the limitations discussed earlier.  coding as in DIET-ABS. The model can be written
                 DIET-ABS                                               as:
                       ABS                         > √                  DIET-REL
                     A     =(Xi:W )(Xj:W ) / d
                       i,j            Q         K                (1)                                            √
                                     >                                           REL                         >
                                                                               A =(Xi:W )(Xj:W ) / d
                           +(P P ) +E (S(i),S(j)),                               i,j            Q          K
                                 Q K i,j       S                                                                        (3)
                                                                                      +Ri−j +E (S(i),S(j)).
                                     n×d                                                           S
                 whereP ,P ∈R            p are low-rank position em-
                          Q   K                                         Weshowanexampleofthismodelwithtwoseg-
                 bedding matrices and ES is the absolute segment
                 attention to model interactions between segments       ments in Figure 3.
                 deﬁned as                                              3.3   Training and Inference Costs
                             E (S(i),S(j)) = S
                               S                  ˆˆ                    We next show the proposed models introduce
                                                  i,j            (2)
                                   ˆ                         ˆ          little computational overhead compared to the
                    where S(i) = i if index i is in segment i.
                                                                        baseline model, making our model more practi-
                    Please note that we use the following notation      cal than alternatives. We consider two different
                 in the above equation. Ai,j denotes the (i,j) entry    models - BERTBASE model and a smaller model,
                 of matrix A. Xi: and X:j denote the ith row and        BERTSMALL,thathashiddensize512,4layersand
                 jth column of X respectively. We will follow this      8 attention heads.
                 notation in the remainder of the paper.                   In Table 1 we compare the training and inference
                    Bydefault, we set d same as d . This already        costs of position encoding methods of Shaw et al.
                                        p           h
                 results in potentially a rank dp+d attention matrix    (2018), Ke et al. (2020), DIET-ABS and DIET-REL.
                                                   h
                 as shown in Theorem 1. To illustrate this, we com-     Wenotice that the simplicity of the proposed meth-
                 pare the rank of the attention matrices in the ﬁrst    odsindeedtranslatestosavingsinbothtrainingand
                 layer of a baseline BERT model and a DIET-ABS          inference times compared to other position encod-
                 modelfor a sampled batch in Figure 2. The ﬁgure        ing approaches. The savings in step times are even
                 shows that attention matrices of DIET-ABS have         moresigniﬁcant for smaller models (BERT           )
                                                                                                                    SMALL
                 higher ranks than the baseline BERT. Our detailed      and during inference.
                 experiment results in § 4 also show that DIET-ABS         Note that the discrepancy between training and
                 performs noticeably better. This conﬁrms our ear-      inference speed is likely because gradient updates
                 lier observation in Theorem 1 that additive position   dominatethecostattrainingtime(Lanetal.,2020).
                 embeddings at input can constrain the model and        At inference time, we only measure the time of a
                                                                    2977
                                                                                                                                              a   a   a    a       p   p    p   p
                                             a   a    a   a        p    p
                                                                                                                                               00  01  02   03      0   1    2   3
                                              00  01   02  03       00   01
                                                                                                                                                                                          seg      seg
                                                                                                         seg      seg
                                                                                                                                                                                            00       01
                                                                                                            00       01
                                                                                                                                              a   a   a    a       p   p    p   p
                                             a   a    a   a        p    p        p    p   p   p
                                                                                                                                               10  11  12   13      -1  0    1   2
                                              10  11   12  13       10   11
                                                                                  00   01  02  03
                                                                                                                                                                +                    +
                                                               +             x                      +
                                                                                                                                              a   a   a    a       p   p    p   p
                                             a   a    a   a        p    p        p    p   p   p
                                                                                                                                                                    -2  -1   0   1
                                                                                                                                               20  21  22   23
                                              20  21   22  23       20   21       10   11  12  13
                                                                                                                                                                                          seg      seg
                                                                                                         seg      seg
                                                                                                                                                                                            10       11
                                                                                                            10       11
                                                                                                                                              a   a   a    a       p   p    p   p
                                             a   a    a   a        p    p
                                                                                                                                               30  31  32   33      -3  -2   -1  0
                                              30  31   32  33       30   31
                                                                                                                                                                    Relative Position       Segment
                                                                                                                                                   Token 
                                                  Token                    Absolute Position                Segment
                                                                        (a) DIET-ABS                                                                            (b) DIET-REL
                              Figure 3: Proposed efﬁcient approach to include position and segment encoding by adding them directly to the
                              token attention matrix per-head. Left ﬁgure shows how we encode absolute positional attention. Right ﬁgure
                              represents relative positional attention.
                                                                               Mode            Shawetal. (2018)                Keetal. (2020)              DIET-ABS             DIET-REL
                                                      BERTBASE               Training                   +13%                           +1%                     +0%                  +0%
                                                      BERTBASE              Inference                   +33%                          +19%                     +0%                  +0%
                                                      BERTSMALL              Training                   +24%                           +4%                     +0%                  +0%
                                                      BERTSMALL             Inference                   +65%                          +27%                     +1%                  +0%
                             Table 1: Pre-training and inference time of Transformers with different position encoding methods in comparison
                              to the baseline BERT model on TPU v2. We observe that simplicity of the DIET-REL and DIET-ABS result in
                              substantial gains in both training and inference time. We notice even more speedup for the smaller BERT
                                                                                                                                                                                                              SMALL
                              model compared to BERT                            .
                                                                        BASE
                              forward pass which corresponds to costs of using                                                conduct experiments in three different settings to
                              such models in real systems.                                                                    cover a wide range of use cases. First, we examine
                              3.4       Application to Long-range Transformers                                                the results of a popular transfer learning approach
                                                                                                                              from masked-LM pretraining to the end tasks in
                             Another advantage of our propose approaches is                                                   GLUE (Devlin et al., 2018). Second, we study
                              they easily extend to long range Transformer mod-                                               zero-shot cross-lingual transferability of the mul-
                              els. For long sequence inputs, Transformers suf-                                                tilingual pretrained models (Hu et al., 2020) to
                              fer from quadratic dependence of computational                                                  classiﬁcation and question answering tasks in the
                              complexity with respect to the sequence length. A                                               XTREMEbenchmark(Huetal.,2020). Lastly, we
                              class of methods reduce this complexity by using a                                              consider training Transformer models from scratch
                              low rank projection of the input sequence for atten-                                            for machine translation.
                              tion computation (Wang et al., 2020; Choromanski                                                     Wecomparethefollowingpositional encoding
                              et al., 2021; Dai et al., 2020). However, such meth-                                            approaches - absolute positional embedding (De-
                              ods use the default input position encodings, and                                               vlin et al., 2018), relative positional embedding
                              there has not been much work in incorporating po-                                               (Shaw et al., 2018), combined absolute and rela-
                              sition information per-head without introducing the                                             tive positional encoding (Ke et al., 2020), relative
                              quadratic computation complexity on the input se-                                               scalar approach (Raffel et al., 2020), our proposed
                              quencelength. Weillustrate the applicability of our                                             DIET-ABS and DIET-REL per-head positional en-
                              methods to such settings by applying DIET-ABS to                                                coding approaches. We denote the methods that
                              Linformer (Wang et al., 2020), which projects the                                               add position/segment information directly to input
                              attention key and value matrices to a lower dimen-                                              tokenembeddingswithinput,andmethodsthatadd
                              sion k during attention computation.                                                            position/segment information directly in attention
                              DIET-ABSLIN                    The proposed method can be                                       layer with per-head. For complete experimental
                             written as:                                                                                      setup, see Appendix A.
                                        LIN                                                     > √
                                     A =(X W )((EX) W ) / d
                                        i,j               i:     Q                j:       K                      (4)         4.1       English Transfer Learning Results
                                                               >
                                               +(P P ) ,
                                                         Q K i,j                                                              Datasets and Model                          For pre-training, we use
                                                      k×n                     n×d                     k×d
                             where E ∈ R                    , P       ∈R ,P ∈R .
                                                                  Q                       K                                   English Wikipedia and Books datasets (Devlin
                              4      Experiments                                                                              et al., 2018).              For Finetuning tasks we use the
                                                                                                                              datasets from the GLUE benchmark (Wang et al.,
                              In this section, we present our experimental results                                            2019). We apply sub-word tokenization on raw
                              comparing different position and segment encod-                                                 text data using WordPiece (Wu et al., 2016) with a
                              ing approaches discussed in earlier sections. We                                                30,000 token vocabulary.
                                                                                                                       2978
                       Model                         Position   Segment       MNLI        QQP QNLI SST2 CoLA STS-B Avg
                                                                               393k       364k     105k      67k      8.5k      7k
                       Devlin et al. (2018)           input       input     85.8 / 85.9   91.1     89.9     93.2      58.7     89.0     84.8
                       Shawetal. (2018)             per-head      input     86.3 / 86.0   91.2     90.5     93.2      59.8     89.3     85.2
                       Raffel et al. (2020)         per-head      input     86.4 / 86.2   91.2     90.1     93.0      59.6     90.1     85.2
                       Keetal. (2020)               per-head      input     86.1 / 86.2   91.2     90.3     93.1      59.6     89.6     85.2
                       DIET-REL                     per-head      input     86.0 / 86.1   91.0     89.8     92.8      59.6     89.0     84.9
                       DIET-REL                     per-head    per-head    86.3 / 86.3   91.0     90.5     92.9      60.3     89.3     85.2
                       DIET-ABS (dp=128, share)     per-head    per-head    86.4 / 86.4   90.8     89.5     93.0      59.8     90.2     85.2
                       Wangetal.(2020) (dp=32)        input       input     82.3 / 82.6   90.2     86.3     91.4      53.9     87.6     82.0
                       DIET-ABSLIN (d =32)          per-head      input     83.0 / 83.1   90.6     86.7     92.0      55.7     87.6     82.7
                                        p
                    Table 2: GLUE: Results on the GLUE dev set of the ﬁnetuned models based on a pre-trained model with 12-
                    layer BERT         architecture. We report the median of the maximum accuracy over all checkpoints among ﬁve
                                 BASE
                    runs. We notice that the shared DIET-ABS with rank 128 performs competitively with existing relative positional
                    embedding SoTA models without the inductive bias of the relative positions. The proposed method also improves
                    performance in the low-rank long range transformer setting of (Wang et al., 2020), where relative positional
                    embedding approaches are inefﬁcient to use.
                                                                              Classiﬁcation             Question Answering
                       Model                          Position    Segment        XNLI           XQuAD         MLQA         TyDiQA       Avg
                                                                                  393k                   88k                 3.7k
                       Devlin et al. (2018)             input       input         67.0         66.0 / 49.9  56.2 / 41.0   59.0 / 47.9   55.3
                       Shawetal. (2018)               per-head      input         67.9         69.5 / 53.9  58.2 / 43.1   64.8 / 49.9   58.2
                       Raffel et al. (2020)           per-head      input         68.5         69.9 / 53.5  59.5 / 44.3   63.8 / 50.6   58.6
                       Keetal. (2020)                 per-head      input         67.8         68.6 / 52.0  58.6 / 43.2   63.9 / 48.7   57.5
                       DIET-REL                       per-head      input         68.0         68.1 / 52.8  57.7 / 42.7   63.3 / 50.9   57.6
                       DIET-REL                       per-head    per-head        68.4         69.4 / 54.4  58.6 / 43.5   62.4 / 49.3   58.0
                       DIET-ABS (dp=128, share)       per-head    per-head        68.5         70.0 / 53.6  59.8 / 44.5   64.6 / 51.5   58.9
                       Wangetal.(2020) (dp=256)         input       input         63.6         59.1 / 43.7  48.9 / 34.0   50.5 / 37.9   48.2
                       DIET-ABSLIN(d =256)            per-head      input         64.4         61.6 / 46.0  52.2 / 37.0   53.6 / 40.9   50.8
                                        p
                    Table 3: XTREME: Fine-tune cross-lingual model on English training set (Cross-lingual Transfer). Performance
                    is measured by accuracy for classiﬁcation, and f1 score / exact match for question answering. In agreement with
                    results in Table 2 we see in this table that using per-head position encodings is strictly better than absolute position
                    encodings at the input. With layer-wise sharing, DIET-ABS with rank 128 outperforms all SoTA models.
                     Model                  EN-DE DE-EN EN-CS CS-EN                 ble 2. We ﬁrst notice that all the approaches that
                     Vaswani et al. (2017)   39.00    38.42    18.55   22.93        encodepositionfeatures explicitly at per-head level
                     Shawetal. (2018)        40.10    38.90    18.74   23.89        perform better than the baseline additive position
                      DIET-REL               39.47    38.49    18.68   23.93        encodings at the input (Devlin et al., 2018). All
                    Table 4: Machine Translation: We report results com-            modelsincorporatingrelativepositions(Shawetal.,
                    paring different position encoding methods for Trans-           2018; Raffel et al., 2020; Ke et al., 2020), despite
                    formers on machine translation tasks en-de, de-en, en-          their modeling differences, have very similar av-
                    cs and cs-en from the Newstest 2018 dataset. We no-             erage score. We show further gains (84.9 to 85.2
                    tice that all per-head position encoding schemes (all ex-       for DIET-REL) by moving segment features to per-
                    cept the ﬁrst row) do better than the absolute position         head.
                    embeddings added at the input. Further the proposed
                    simple DIET-REL approach is competitive with other                 Interestingly we notice that the proposed abso-
                    position encoding approaches.                                   lute position encoding method DIET-ABS, with
                                                                                    layer-wise sharing, is on par with all previous
                                                                                    SoTA relative positional encodings. This shows
                    Results     Weexamine how different ways of en-                 that even absolute position encodings can perform
                    coding position and segment affect the transfer                 better when included per-head instead at the input.
                    learning ability of the pre-trained English BERT                Wepresent a detailed ablation study varying the
                    models by ﬁne-tuning on the GLUE benchmark                      rank and sharing methods of absolute positional
                    (Wanget al., 2019), and present the results in Ta-              attention (DIET-ABS) in Table 8 and Tables 9 in
                                                                               2979
                 Appendix C.                                           (en-de), German-to-English (de-en), English-to-
                   For long range input, we consider Linformer          Czech (en-cs) and Czech-to-English (cs-en) (Bo-
                 (Wangetal., 2020) with a projection dimension of       jar et al., 2018). We test the corresponding mod-
                 32. Due to down-projection, we see non-trivial per-    els on Newstest 2018 datasets respectively and re-
                 formance drop, when compared to a Transformer.         port the BLEU score output by SacreBLEU (Post,
                 Evenfor this setting we see that our absolute posi-    2018) with default setting.    Our setup follows
                 tional attention DIET-ABS can be used to improve      Vaswani et al. (2017) closely and use their Ten-
                 the model’s performance.                               sor2Tensor framework (Vaswani et al., 2018). Fol-
                 4.2   Cross-lingual Model Results                      lowingVaswanietal.(2017)weusea6layerTrans-
                                                                        former with encoder-decoder architecture.      For
                 Datasets and Model       For our multilingual ex-      moredetails of our experimental setup please see
                 periments, we pre-train the models on Wikipedia       Appendix A
                 corpus in 100 languages similar to (Lample and         Results   Wereport the BLEU scores of the mod-
                 Conneau, 2019) for 125K steps with a sequence          els in Table 4. We observe that moving positional
                 length of 512, and then ﬁne-tune on downstream         information from input to per-head attention layer
                 XTREMEtasks(Huetal.,2020). Weuselanguage-              improves BLEU scores. Different variations of
                 independent tokenizer, Sentence Piece (Kudo and        per-head positional attention do not make much
                 Richardson, 2018) model, with 120,000 token vo-        difference with DIET-REL being competitive with
                 cabulary to encode input text.                         Shawetal. (2018).
                 Classiﬁcation    Weconduct5trials of ﬁne-tuning        4.4  Ablation Study
                 for each model on the MultiNLI (Williams et al.,
                 2018) training data, then perform zero-shot predic-    In this section, we share our ﬁndings of key factors
                 tions on XNLI (Conneau et al., 2018), choosing         that affect performance of decoupled positional
                 median accuracy to report.                             attention.
                 QuestionAnswering Weconduct5trialsofﬁne-               Sharing the Positional Encoding          Previous
                 tuning for each model on SQuAD V1.1 dataset,          works (Raffel et al., 2020; Ke et al., 2020; Shaw
                 following by zero-shot predictions on XQuAD (11        et al., 2018) used different sharing methods for the
                 languages), MLQA (7 languages) and TyDiQA-             positional encodings to reduce the model parame-
                 GoldP (9 languages), choosing median F1 / EM           ters. We present a detailed study on different forms
                 scores to report.                                      of sharing positional encodings and its effect on
                 Results   We present our results on the classiﬁ-       performance. In particular, we compare the fol-
                 cation and question answering ﬁnetuning tasks in       lowing variations in sharing the position encoding
                 XTREMEfordifferent position and segment en-            parameters across different heads and the layers in
                 coding methods in Table 3. Again all per-head          the Transformer.
                 position encoding methods outperform input addi-         • head-wise - Same parameters are used for all
                 tive position encodings. Interestingly, our simple          heads in a layer, with different layers using
                 DIET-ABS turns out to be the best model, better             different parameters (Shaw et al., 2018; Ke
                 than other models using relative position features.         et al., 2020).
                 Layer-wise sharing and per-head segment attention        • layer-wise - Sharing of position encoding pa-
                 allows DIET-ABS to outperform DIET-REL. We                  rameters across layers with different parame-
                 present a detailed ablation study in Table 5 to un-         ters for each head (Raffel et al., 2020).
                 derstand effect of decoupled positional attention
                 variants. Finally, we notice similar advantages in       • none - Every layer and head uses different
                 using DIET-ABS with the Linformer (Wang et al.,             position encoding parameters.
                 2020) model in the long range setting.
                 4.3   Translation Results                             We present results comparing different sharing
                                                                        methods in Table 5 for XTREME tasks. We make
                 DatasetsandModel Forthemachinetranslation              the following observations 1) head-wise sharing is
                 task we consider two language pairs (both direc-       consistently worse than layer-wise, 2) sharing hurts
                 tions) for training - WMT 2018 English-to-German       the performance of DIET-REL whereas it improves
                                                                   2980
                      Model                  Sharing    Segment    Classiﬁcation             Question Answering               Avg
                                                                      XNLI         XQuAD        MLQA       TyDiQA-GoldP
                      DIET-REL                  -        input         68.0       68.1 / 52.8  57.7 / 42.7    63.3 / 50.9    57.6
                      DIET-REL              head-wise    input         67.7       66.2 / 51.0  56.0 / 41.1    60.1 / 45.9    55.4
                      DIET-REL             layer-wise    input         68.0       68.6 / 53.3  58.1 / 43.1    61.3 / 48.2    57.2
                      DIET-REL                  -       per-head       68.4       69.4 / 54.4  58.6 / 43.5    62.4 / 49.3    58.0
                      DIET-REL              head-wise   per-head       67.8       66.0 / 50.5  55.5 / 40.4    59.2 / 44.6    54.7
                      DIET-REL             layer-wise   per-head       68.1       68.7 / 53.8  58.4 / 43.2    61.0 / 48.4    57.3
                      DIET-ABS (dp=64)          -        input         68.0       67.4 / 50.5  57.8 / 42.3    61.3 / 46.8    56.3
                      DIET-ABS (dp=64)          -       per-head       67.9       67.5 / 52.4  57.3 / 42.3    61.6 / 46.8    56.5
                      DIET-ABS (dp=128)         -       per-head       68.1       68.2 / 52.0  57.9 / 42.6    61.5 / 47.6    56.8
                      DIET-ABS (dp=512)         -       per-head       68.5       68.0 / 52.0  57.7 / 42.4    61.6 / 48.4    56.9
                      DIET-ABS (dp=64)     layer-wise    input         68.0       69.3 / 53.1  59.3 / 43.9    63.2 / 48.6    57.9
                      DIET-ABS (dp=64)     layer-wise   per-head       68.4       69.3 / 53.2  59.4 / 44.1    63.3 / 48.6    58.0
                      DIET-ABS (dp=128)    layer-wise   per-head       68.5       70.0 / 53.6  59.8 / 44.5    64.6 / 51.5    58.9
                      DIET-ABS (dp=256)    layer-wise   per-head       68.4       69.9 / 53.8  59.6 / 44.2    62.8 / 49.1    58.3
                      DIET-ABS (dp=512)    layer-wise   per-head       67.8       69.0 / 53.2  58.4 / 43.0    62.5 / 48.8    57.5
                  Table 5: Ablation study on XTREME: We run decoupled positional attention ablation study to understand the
                  effect of 1) sharing positional attention parameters across layers and heads 2) segment attention added at per-head
                  3) performance of relative and absolute 4) absolute positional attention rank dp from 64 to 512.
                                                                    English                        Multilingual
                                                         Parameters     +∆      GLUE     Parameters     +∆      XTREME
                             Devlin et al. (2018)          110.1M        -       84.8      178.9M        -         55.3
                             Shawetal. (2018)              112.9M      +2.5%     85.2      181.7M      +1.7%       57.9
                             DIET-REL                      109.9M      +0.0%     85.2      178.7M      +0.0%       58.0
                             DIET-REL (share)              109.7M      +0.0%     85.0      178.5M      +0.0%       57.3
                             DIET-ABS (d =128)             128.6M     +16.8%     85.3      197.4M     +10.0%       56.8
                                          p
                             DIET-ABS (d =128, share)      111.3M      +1.1%     85.2      180.1M      +0.6%       58.9
                                          p
                  Table 6: Model Parameters: We list the number of model parameters and performance for different position en-
                  coding approaches. We observe that sharing hurts the performance of DIET-REL with negligible beneﬁt in the
                  number of parameters. On the contrary, the regularization effect of sharing makes DIET-ABS more stable with
                  lesser parameters to achieve competitive performance.
                  the performance of DIET-ABS. We summarize the               Appendix C for segment attention visualization.
                  key settings along with the number of model pa-             RankofAbsolutePositionalAttention              Thede-
                  rameters in Table 6. For DIET-REL, sharing brings           sign of DIET-ABS allows to learn higher rank at-
                  little effect on saving parameters, and hurts the per-      tention matrices as shown in Theorem 1. To under-
                  formance. Hence, we recommend no sharing for                standtheeffectofabsolutepositionalattentionrank
                  relative positional encodings (DIET-REL). On the            (d ) in practice, we conduct experiments varying
                  other hand, it is necessary to share parameters for           p
                                                                              the rank from d = 64 to d = 512. We present
                  DIET-ABS in order to keep the number of parame-                              p             p
                  ters low. Interestingly, sharing has regularization         the results in Table 5. We notice that the perfor-
                  effect on DIET-ABS, making the model perform                manceimprovesasweincreasetherankfrom64to
                  better. We choose layer-wise sharing over head-            128. However there is a performance saturation in
                  wise sharing for its better performance.                    further increasing it to 512. We present a visualiza-
                                                                              tion of the rank of the positional attention matrix
                  Segment Encoding Our novel segment encod-                   in Appendix B.
                  ing design further improves the model perfor-               4.5   Positional Attention Pattern Visualization
                  mance showed in Table 5. Both relative and ab-
                  solute decoupled positional attention models ben-           Wenextvisualize the learned positional attention
                  eﬁt from moving the segment encoding from in-               patterns of DIET-ABS in Figure 4. We ﬁrst note
                  put to per-head: DIET-REL (+0.4%), layer-wise               that DIET-ABS has learned to capture the relative
                  shared DIET-REL (+0.1%), DIET-ABS (+0.2%),                  positional relations between inputs. Also note that,
                  layer-wise shared DIET-ABS (+0.1%). See Ap-                 for the the index zero (the [CLS] token), decoupled
                  pendix D for the results of GLUE benchmark and              absolute positional attention usually learns a spe-
                                                                         2981
                                                                              Krzysztof Choromanski, Valerii Likhosherstov, David
                                                                                Dohan, Xingyou Song, Andreea Gane, Tamas Sar-
                                                                                los, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
                                                                                Lukasz Kaiser, David Belanger, Lucy Colwell, and
                                                                                Adrian Weller. 2021. Rethinking attention with per-
                                                                                formers.
                                                                              Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-
                                                                                ina Williams, Samuel R. Bowman, Holger Schwenk,
                                                                                and Veselin Stoyanov. 2018. Xnli: Evaluating cross-
                                                                                lingual sentence representations. In Proceedings of
                                                                                the 2018 Conference on Empirical Methods in Natu-
                  Figure 4: Visualization of learned positional attention       ral Language Processing. Association for Computa-
                  patterns of DIET-ABS. Note that in addition to captur-        tional Linguistics.
                  ing the the relative positional relations, the model also
                  learn to attend to [CLS] at index 0, suggesting the ded-    Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V.
                  icated [CLS] untying design in Ke et al. (2020) is not        Le. 2020. Funnel-transformer: Filtering out sequen-
                  necessary with DIET-ABS.                                      tial redundancy for efﬁcient language processing.
                  cial pattern. This pattern cannot be solely modeled         Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
                  by existing relative positional embedding methods,            Kristina Toutanova. 2018.     BERT: Pre-training of
                  and some existing works (Ke et al., 2020) handled             deep bidirectional Transformers for language under-
                                                                                standing. arXiv preprint arXiv:1810.04805.
                  this case speciﬁcally by introducing new parame-
                  ters. This shows the beneﬁt of DIET-ABS in not              Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham
                  requiring any carefully designed inductive biases             Neubig, Orhan Firat, and Melvin Johnson. 2020.
                  as in existing approaches( Shaw et al. (2018); Raf-           Xtreme: A massively multilingual multi-task bench-
                                                                                mark for evaluating cross-lingual generalisation. In
                  fel et al. (2020)), which may not generalize across           ProceedingsofMachineLearningandSystems2020,
                  tasks.                                                        pages 7449–7459.
                  5    Conclusion                                             Guolin Ke, Di He, and Tie-Yan Liu. 2020. Rethink-
                                                                                ing the positional encoding in language pre-training.
                  In this paper we theoretically and empirically ex-            arXiv preprint arXiv:2006.15595.
                  amined the limitation of additive position embed-           TakuKudoandJohnRichardson.2018. Sentencepiece:
                  ding at input and showed that having per-head posi-           A simple and language independent subword tok-
                  tion embeddings results in better performance. We             enizer and detokenizer for neural text processing.
                  argued that the superior performance of some of             Guillaume Lample and Alexis Conneau. 2019. Cross-
                  the relative position encoding methods come from              lingual language model pretraining.
                  their per-head addition to attention matrix rather
                  than the position information being relative vs ab-         Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
                  solute. Indeed we show that using absolute position           Kevin Gimpel, Piyush Sharma, and Radu Soricut.
                  encodings per-head results in better performance.             2020. Albert: A lite bert for self-supervised learn-
                                                                                ing of language representations.
                  Motivated by this we propose a simple per-head po-
                  sition and segment attention method that achieves           Xiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng
                  the state-of-the-art performance on multiple NLP              Gao. 2020. Very deep transformers for neural ma-
                  tasks and is more computationally efﬁcient than               chine translation.
                  existing approaches.                                        Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
                                                                                dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
                                                                                Luke Zettlemoyer, and Veselin Stoyanov. 2019.
                  References                                                    RoBERTa: A robustly optimized BERT pretraining
                                                                                approach. arXiv preprint arXiv:1907.11692.
                       ˇ
                  Ondrej Bojar, Christian Federmann, Mark Fishel,
                     Yvette Graham, Barry Haddow, Philipp Koehn, and          Matt Post. 2018. A call for clarity in reporting bleu
                     Christof Monz. 2018.     Findings of the 2018 con-         scores. In WMT.
                     ference on machine translation (WMT18). In Pro-
                     ceedings of the Third Conference on Machine Trans-       AlecRadford,KarthikNarasimhan,TimSalimans,and
                     lation: Shared Task Papers, pages 272–303, Bel-            Ilya Sutskever. 2018.    Improving language under-
                     gium, Brussels. Association for Computational Lin-         standing by generative pre-training. Technical Re-
                     guistics.                                                  port, OpenAI.
                                                                         2982
                 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine     Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh
                    Lee, Sharan Narang, Michael Matena, Yanqi Zhou,        Rawat, Sashank J Reddi, and Sanjiv Kumar.
                    Wei Li, and Peter J Liu. 2020. Exploring the lim-      2020. Are Transformers universal approximators of
                    its of transfer learning with a uniﬁed text-to-text    sequence-to-sequence functions?   In International
                    transformer. JournalofMachineLearningResearch,         Conference on Learning Representations.
                    21(140):1–67.
                 Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
                    2018. Self-attention with relative position represen-
                    tations. In Proceedings of the 2018 Conference of
                    the North American Chapter of the Association for
                    Computational Linguistics: Human Language Tech-
                    nologies, Volume 2 (Short Papers), pages 464–468.
                 Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran-
                    cois Chollet, Aidan N Gomez, Stephan Gouws,
                    Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki
                    Parmar, et al. 2018. Tensor2tensor for neural ma-
                    chine translation. arXiv preprint arXiv:1803.07416.
                 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
                    Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
                    Kaiser, and Illia Polosukhin. 2017. Attention is all
                    you need. In Advances in Neural Information Pro-
                    cessing Systems, pages 5998–6008.
                 Alex Wang, Amanpreet Singh, Julian Michael, Fe-
                    lix Hill, Omer Levy, and Samuel Bowman. 2019.
                    Glue: Amulti-taskbenchmarkandanalysisplatform
                    for natural language understanding.  In 7th Inter-
                    national Conference on Learning Representations,
                    ICLR2019.
                 Sinong Wang, Belinda Z. Li, Madian Khabsa, Han
                    Fang, and Hao Ma. 2020. Linformer: Self-attention
                    with linear complexity.
                 Yu-An Wang and Yun-Nung Chen. 2020. What do
                    position embeddings learn? an empirical study of
                    pre-trained language model positional encoding. In
                    EMNLP2020.
                 Adina Williams, Nikita Nangia, and Samuel R. Bow-
                    man. 2018. A broad-coverage challenge corpus for
                    sentence understanding through inference.
                 Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
                    Le, Mohammad Norouzi, Wolfgang Macherey,
                    Maxim Krikun, Yuan Cao, Qin Gao, Klaus
                    Macherey,JeffKlingner,ApurvaShah,MelvinJohn-
                    son, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,
                    Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith
                    Stevens, George Kurian, Nishant Patil, Wei Wang,
                    Cliff Young, Jason Smith, Jason Riesa, Alex Rud-
                    nick, Oriol Vinyals, Greg Corrado, Macduff Hughes,
                    and Jeffrey Dean. 2016. Google’s neural machine
                    translation system: Bridgingthegapbetweenhuman
                    and machine translation.
                 Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G.
                    Carbonell, Ruslan Salakhutdinov, and Quoc V. Le.
                    2019. XLNet: Generalized autoregressive pretrain-
                    ing for language understanding.   arXiv preprint
                    arXiv:1906.08237.
                                                                    2983
                    A Experimentalsetup
                    In this section we present more details of our experimental setup.
                    Pre-training        Wepre-train the models using a masked LM task (Devlin et al., 2018) and do not use
                    the Next Sentence Prediction (NSP) loss as suggested in RoBERTa (Liu et al., 2019). Each input is
                    constructed with full sentences from documents, and packed up to the maximum sequence length. We use
                    the same architecture as BERTBASE (Devlin et al., 2018) (L = 12, H = 768, A = 12) for our experiments.
                    Fine-tuning        Somedownstream tasks have different groups of full sentences provided at inputs. For
                    those tasks (e.g. MNLI, CoLA, XNLI,SQuAQ),weﬁne-tunemodelswithsupplementalsegmentencoding
                    discussed in Section §3. We leave models for other tasks unchanged as their pre-training correspondences.
                    Hyper-parameters            Hyper-parameters we use are presented in Table 7.
                                                                        English                             Multilingual
                                                           Pretrain                  Finetune    Pretrain                   Finetune
                                   MaxSteps                  500K              5 or 10 epochs       125K                    3 epochs
                                   Learning Rate            0.0018    {1e-5, 2e-5, 3e-5, 4e-5}    0.0018     {1e-5, 2e-5, 3e-5, 4e-5}
                                   WarmupProportion          0.025                         0.1      0.025                         0.1
                                   Sequence Length             128                        128         512                        512
                                   Batch Size                 4096                          32      4096                          32
                                   Checkpoint Interval         20k                        3.5k        20k                       3.5k
                                                             Table 7: Hyperparameters for all models
                    Translate       For our Translate experiments we follow the setup of Vaswani et al. (2017) and use their
                    Tensor2Tensor framework (Vaswani et al., 2018). We train using WMT18 ((Europarl v7, Common Crawl
                    corpus and News Commentary v13) en-de, de-en, en-cs and cs-en datasets. We report BLUE scores
                    provided by SacreBLEU (Post, 2018) on newstest 2018 dataset. We train a 6 layer Transformer model.
                    Anychangestoposition encoding are applied to all the attention layers both in the encoder and decoder.
                    WeuseAdamoptimizerandtrainfor250ksteps. Fordecodingweusebeamsearchwithbeamsize10
                    and length penalty 0.6.
                    B Proofs
                    Proof of Theorem 1. The ﬁrst claim follows easily by observing that rank of product of an two matrices is
                    upper bounded by the minimum of the individual ranks.
                                                                                   >            >
                                     rank(Aa) = rank((X+P)W W (X+P) )
                                                                             Q     K
                                                   ≤min(rank(X+P),rank(W ),rank(X+P),rank(W ))
                                                                                           Q                                  K
                                                   ≤dh.
                                                                     >             >                                     d×dh
                                         rank((X+P)W W (X+P) )≤d ,whereW ,W ∈R
                                                               Q     K                     h              Q      K
                       Thelast inequality follows from rank(W ) ≤ d as W                        ∈Rd×dh.
                                                                           Q        h        Q
                       Toprove the second claim we follow a construction approach. Let us ﬁrst take W                        =W tobesame
                                                                                                                          Q         K
                    matrices with ﬁrst d rows being identity matrix and the remaining d − d rows being all zeros. Then
                                             h                                                                h
                                                                           I                0            
                                                                    >            d ,d         d ,d−d
                                                          W W =                   h h          h      h      .
                                                              Q     K         0            0
                                                                               d−d ,d        d−d ,d−d
                                                                                    h h          h     h
                                                                          d ×d                                                        d ,d
                    Here Id ,d denotes the identity matrix in R h               h and 0d ,d denotes the all zeros matrix in R h .
                              h h                                                         h
                       WeletXbesuchthattheﬁrstdrowsformanidentitymatrixandrestarezeros-X> = [I                                           , 0       ].
                    HenceXW W>X>becomesasimilardiagonalmatrixwith                                                                     d,d   n−d,d
                                   Q     K
                                                                              I                0             
                                                                  > >               d ,d          d ,n−d
                                                      XW W X =                       h h           h      h      .
                                                             Q    K              0             0
                                                                                   n−d ,d       n−d ,n−d
                                                                                       h h           h     h
                                                                                 2984
                                               ˆ                    ˆ
                   Choosedp = n > d andletP = I. NowchosingPwithzerosintheﬁrstn−dp columnsandidentity
                                      h
                                       ˆ
                in the last d columns (P = [0       , I    ]) gives
                           p                  d,n−d   d ,d
                                                   p   p p
                                                          0             0        
                                                    >        n−d ,n−d     n−d ,d
                                                ˆ ˆ             p    p       p p
                                                PP =         0            I         .
                                                              d ,n−d       d ,d
                                                               p    p       p p
                Combiningthese two gives us
                                                                          > > ˆˆ>
                                             rank(Ar) = rank(XW W X +PP )
                                                                     Q    K
                                                        =min(d +dp,n)>d .
                                                                 h             h
                   LetX∈Rn×dbetheinputwordembeddingsindimensiondwithsequencelengthn. Wehavetrainable
                position embeddings P ∈ Rn×d, which are added to the input sequence before feeding into the model g.
                For a given input X and label y, the objective for a loss function ` is as follows:
                                                        L=`(g(X+P),y)                                             (5)
                Theorem 2. Let X and P be trainable embedding matrices in Rn×d. Then the gradients of the loss
                function in equation (5), at any point (X,y), and for any differentiable functions ` and g, are same for X
                andP.
                Remarks. This theorem shows us that the gradients are same for the input token embeddings and
                position embeddings. While in standard NLP tasks the inputs X can be different in each step due to
                different input tokens being present in each mini batch, the result still suggests that additive position
                embedding can limit the model from learning the relative importance of position encodings with respect
                to token embeddings based on the training task at hand.
                Proof of Theorem 2. The above theorem follows by just computing the gradients and showing they are
                equal for each step.
                   Gradients of the above objective w.r.t X and P are as follows.
                                                ∇ L=∇ L·∇             g · ∇ (X+P)
                                                  X       g      X+P       X
                                                 =∇L·∇          g
                                                     g      X+P
                                                ∇ L=∇ L·∇            g · ∇ (X+P)
                                                  P       g      X+P      P
                                                 =∇L·∇          g.
                                                     g      X+P
                Theabovecomputationofgradient follows from chain rule. This shows that the gradients of L w.r.t. X
                and P are the same.
                                                                2985
               C AttentionVisualization
               In this section, we examine the model internals to understand how the proposed model works. We ﬁrst
               visualize the model internals of different modeling alternatives to argue our proposed model is sensible.
               WhyWeRemovetheInputEmbedding Tounderstandifitissensibletoremovetheinputadditive
               embedding after adding position scalars per-head, we add additive position embedding to our DIET-ABS
               model. Then, we examine the position embedding of the BERT model and our DIET-ABS variant with
               additive position embedding. Figure 5 shows that, when the model has both absolute scalar and additive
               absolute position embedding, the position embedding encodes almost no information — all position
               embeddings at input are similar.
               Figure 5: The cosine similarity distribution between all absolute position pairs of the input additive positional
               embeddingforthebaselineBERTmodelandtheproposedDIET-ABS. Weobservedthat,afterthepositionfeatures
               are added to each head as in DIET-ABS, the input position embedding contains almost no information — all input
               position pairs are similar.
               TheEffect of Segment Attention  Wealso examine the effect of adding segment attention on top of
               the position attention. Figure 6 shows some representative patterns. We observe that segment attention
               enables the model to attend more to parts of the sequence that belongs to certain segments.
                         (a) Attend to the Second Segment          (b) Down-weight Relative Position Attention
               Figure 6: We consider input of length 32 with two segments. The second segment starts at index 16. We observe
               the attention patterns in the DIET-REL model without token-to-token attention.
                                                          2986
                Shifting Pattern Learned from Absolute Positional Attention  Usingrelative position encoding gives
                generally better results despite smaller improvement scale compared to moving feature encoding per-head.
               Tounderstand this, we visualize the attention pattern of the absolute positional attention and found two
                representative patterns in DIET-ABS in Figure 7. We observe that even given absolute position features,
                the model learns a “shifting pattern” for the most part. Different from Wang and Chen (2020) which
                claimed absolute position only learns local patterns, we show the position attention can actually attend
                to longer context. However, the shifting pattern can be modeled directly by relative position. Thus,
                DIET-REL can be a better model choice with fewer parameters and more accurate inductive bias in some
                applications.
                           (a) Attend to Forward Neighbors                  (b) Attend to Previous Tokens
                Figure 7: Sampled position attention score patterns for the DIET-ABS model. We can see a clear shifting patterns
                generated by the model. Such patterns can be modeled better by relative positional scalar encodigs.
                Rank of Positional Attention Matrices   In Figure 8, we present a comparison of rank of position
                attention matrices for a BERTBASE model with absolute position embeddings at input (P W W>P>)
                                                                                                 Q   Q    K K
               v.s. absolute position embeddings per-head (DIET-ABS (1), (P P>), where P ,P     ∈Rn×dp). With
                                                                           Q K            Q   K
                additive positional embedding at input, position attention matrices have much lower rank, limiting the
                representative power. This is alleviated by DIET-ABS.
                                            Figure 8: Rank of positional attention matrices
                                                             2987
                    D AdditionalAblationStudyonGLUE
                    Earlier we present an ablation study on XTREME in Table 5 for decoupled positional attention variants.
                    Wecompare DIET-REL and DIET-ABS against the baseline (Devlin et al., 2018). We now present a
                    similar study on the GLUE benchmark in Table 8 and observe similar results.
                    Positional Encoding         In Table 8, moving positional embeddings from input to per-head improves
                    average score for both DIET-REL (+0.1%) and DIET-ABS (+0.2%).
                    Segment Encoding In Table 8, moving segment embeddings from input to per-head improves both
                    DIET-REL (+0.3%) and DIET-ABS (+0.05%).
                    Sharing Strategies        Sharing plays an important role for DIET-ABS. In Table 9, we ﬁnd that sharing will
                    degrade the performance of DIET-REL (-0.2% layer-wise, -0.3% head-wise). For DIET-ABS, sharing
                    makesthemodelmorestable, and able to compete with DIET-REL.
                       Model                         Position   Segment       MNLI        QQP QNLI SST2 CoLA STS-B Avg
                                                                               393k       364k     105k      67k      8.5k      7k
                       Devlin et al. (2018)           input       input     85.8 / 85.9   91.1     89.9     93.2      58.7     89.0     84.8
                       DIET-REL                     per-head      input     86.0 / 86.1   91.0     89.8     92.8      59.6     89.0     84.9
                       DIET-REL                     per-head    per-head    86.3 / 86.3   91.0     90.5     92.9      60.3     89.3     85.2
                       DIET-ABS (dp=64)             per-head      input     86.1 / 85.8   91.2     90.0     93.0      58.9     89.9     85.0
                       DIET-ABS (dp=64)             per-head    per-head    86.1 / 86.1   91.2     90.2     93.0      58.9     89.8     85.0
                       DIET-ABS (dp=64, share)      per-head    per-head     86/86.8      91.1     90.4     92.9      59.3     89.8     85.2
                       DIET-ABS (dp=128, share)     per-head    per-head    86.4 / 86.4   90.8     89.5     93.0      59.8     90.2     85.2
                    Table 8: Position and segment ablation study on GLUE: DIET-REL and DIET-ABS demonstrate the advantages of
                    movingbothpositional and segment embedding from input to per-head.
                               Model                    Sharing       MNLI       QQP QNLI SST2 CoLA STS-B Avg
                                                                       393k       364k     105k     67k      8.5k       7k
                               DIET-REL                    -        86.3 / 86.3   91.0     90.5     92.9     60.3      89.3     85.2
                               DIET-REL               layer-wise    86.5 / 86.3   91.1     90.0     93.0     58.8      89.6     85.0
                               DIET-REL               head-wise     85.8 / 85.7   91.2     90.2     92.8     59.8      89.1     84.9
                               DIET-ABS (dp=64)            -        86.1 / 86.1   91.2     90.2     93.0     58.9      89.8     85.0
                               DIET-ABS (dp=128)           -        86.7 / 86.5   91.2     90.6     92.8     60.1      89.4     85.3
                               DIET-ABS (dp=64)       layer-wise     86/86.8      91.1     90.4     92.9     59.3      89.8     85.2
                               DIET-ABS (dp=128)      layer-wise    86.4 / 86.4   90.8     89.5     93.0     59.8      90.2     85.2
                    Table 9: Sharing ablation study on GLUE: We run ablation study to understand the effect of sharing position
                    encoding parameters across layers and heads. We notice that sharing improves the performance of DIET-ABS, but
                    hurts the performance of DIET-REL with both layer-wise or head-wise sharing.
                                                                               2988
