                           5.6  Doesimplicit slot attention still produce intuitive masks with a different architecture?
                           Wesoughttocheckwhetherimplicit differentiation still preserves the quality of the segmentation
                           masksproducedbytheoriginalslotattention architecture by Locatello et al. [39], which uses a spatial
                           broadcast decoder [63] rather than a transformer decoder as SLATE does. It indeed does (Fig. 8),
                           suggesting that our ﬁndings are not speciﬁc to SLATE but apply to slot attention more broadly.
                           6   Discussion
                           The connection we made in this paper between slot attention and deep equilibrium models also
                           highlights various other properties about iterative reﬁnement procedures for inferring latent sets that
                           suggest connections to other areas of research that are worth theoretically developing in the future.
                           First is the connection to the literature on fast weights [6, 33, 54]: interpreting slots as parameters
                           that are modiﬁed during the inner optimization during execution time may give us novel formulation
                           for how to represent and update fast weight memories. Second is the connection to the literature on
                           meta-learning [5, 17, 53, 60]: interpreting slots as solutions to an inner optimization problem during
                           execution time may give us a novel perspective on perception as itself a learning process. Third is
                           the connection to causality [46, 47]: interpreting the independently generated and symmetrically
                           processed slots as parameterizing independent causal mechanisms [28] may give us a novel approach
                           for learning to represent causal models within a neural scaffolding. Fourth is the connection to
                           dynamical systems [44]: interpreting slots as a set of attractor basins may offer a novel theory of
                           howtheerror-correcting properties of discrete representations emerge from continuous ones. These
                           different ﬁelds have their own conceptual and implementation tools that could potentially improve
                           our understanding of how to build better iterative reﬁnement algorithms and inform how objects
                           could potentially be represented in the mind.
                           Conclusion    Wehaveproposedimplicit differentiation for training iterative reﬁnement procedures
                           for inferring representations of latent sets. Our results show clear signal that implicit differentiation
                           can offer a signiﬁcant optimization improvement over backpropagating through the unrolled iteration
                           of slot attention, and potentially any other iterative reﬁnement algorithm, with lower space and
                           time complexity and only one additional line of code. Because it is so simple to apply implicit
                           differentiation to any ﬁxed point algorithm, we hope our work inspires future work to leverage tools
                           developedforimplicitdifferentiation for improving learning representations of latent sets and iterative
                           reﬁnement methods more broadly.
                           Acknowledgements
                           This work was supported ARL, W911NF2110097, and ONR grant number N00014-18-1-2873.
                           Part of this work was completed while MC was an intern at Meta AI. Computing support came
                           from Google Cloud Platform and Meta. We would like to thank Shaojie Bai for help on implicit
                           differentiation, Gautam Singh for help on SLATE, Alban Desmaison for help on PyTorch. We would
                           also like to thank Yan Zhang, David Zhang, Thomas Kipf, and Klaus Greff for insightful discussions
                           and Jianwen Xie for pointing out previously missing references.
                           References
                            [1] David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for
                                Boltzmann machines. Cognitive science, 9(1):147–169, 1985.
                            [2] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and Zico
                                Kolter. Differentiable convex optimization layers. arXiv preprint arXiv:1910.12430, 2019.
                            [3] Luis B Almeida. A learning rule for asynchronous perceptrons with feedback in a combinatorial
                                environment. In Artiﬁcial neural networks: concept learning, pages 102–111. 1990.
                            [4] Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural
                                networks. In International Conference on Machine Learning, pages 136–145. PMLR, 2017.
                            [5] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom
                                Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by
                                                                         10
