           
         SUPPLEMENTARY INFORMATION                               In format provided by Friston (FEBRUARY 2010) 
                Supplementary information S4 (box): Value and surprise 
                Here, we compare and contrast optimal control and free-energy formulations of dynamics on 
                hidden or sensory states. To keep things simple, we will assume the hidden states are known 
                                                                            %
                (as is usually assumed in control theory) and ignore random fluctuations; i.e.,  w(t) = 0 (see 
                box S1). In optimum control, one starts with a loss or cost-function (negative reward or utility), 
                  %
                c(x) and optimises the motion of states to maximise value or expected reward over time 
                 
                      ∗          %        %
                    a =argmax f(x,a)⋅∇V(x)
                           a
                        •                                                    S4.1 
                  %          %       & %      %
                V(x(0)) = ∫−c(x(t))dt ⇒V(x(t)) = c(x)
                         0
                 
                The first equality says that motion ascends the gradients of the value-function and the second 
                just  defines value as reward that will be accumulated in the future. Note the equations of 
                      &
                motion  %  %    now include action. The value-function is the solution to the celebrated 
                      x = f (x,a)
                Hamilton-Jacobi-Bellman equation 
                 
                     & %      %
                max V(x(t))−c(x) =0⇒
                    {           }
                  a                                                          S4.2 
                       %       %     %
                max f(x,a)⋅∇V(x)−c(x) =0
                    {                 }
                  a
                 
                This solution ensures that the rate of change of value is cost, as required by the definition of 
                value. In summary, (S4.1) says that action maximises value and (S4.2) means that value is 
                the reward expected under this policy. This ensures low-cost regions attract all trajectories 
                through state-space. 
                 
                We now revisit value from the perspective of surprise and free-energy. If we put the random 
                fluctuations  back  and  assume  a  general  form  (the  Helmholtz  decomposition)  for  motion: 
                f =∇V +∇×W , it is fairly  easy  to  relate  value  and  surprise  (using  the  Fokker-Planck 
                equation, subject to ∇V ⋅(∇×W) = 0) 
         NATURE REVIEWS | NEUROSCIENCE                                      www.nature.com/reviews/neuro 
                                       © 2010 Macmillan Publishers Limited.  All rights reserved. 
