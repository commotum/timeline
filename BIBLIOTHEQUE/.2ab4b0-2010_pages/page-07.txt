                                                                                                                                                               REVIEWS
                                                However, from the point of view of the free-energy                    value or surprise is determined by the form of an agent’s  
                                                principle, perception just makes free energy a good                   generative model and its implicit priors — these specify 
                                                proxy for surprise. To actually reduce surprise we need               the value of sensory states and, crucially, are heritable 
                                                to act. In the next section, we retain a focus on cell                through genetic and epigenetic mechanisms. This means 
                                                assemblies but move to the selection and reinforcement                that prior expectations (that is, the primary repertoire) 
                                                of stimulus–response links.                                           can prescribe a small number of attractive states with 
                                                                                                                      innate value. In turn, this enables natural selection to 
                                                Neural Darwinism and value learning                                   optimize prior expectations and ensure they are con-
                                                In the theory of neuronal group selection88, the emergence            sistent with the agent’s phenotype. Put simply, valuable 
                                                of neuronal assemblies is considered in the light of selec-           states are just the states that the agent expects to fre-
                                                tive pressure. The theory has four elements: epigenetic               quent. These expectations are constrained by the form of 
                                                mechanisms create a primary repertoire of neuronal                    its generative model, which is specified genetically and 
                                                connections, which are refined by experience-dependent                fulfilled behaviourally, under active inference.
                                                plasticity to produce a secondary repertoire of neuro-                    It is important to appreciate that prior expectations 
                                                nal groups. These are selected and maintained through                 include not just what will be sampled from the world but 
                                                reentrant signalling among neuronal groups. As in cell                also how the world is sampled. This means that natural 
                                                assembly theory, plasticity rests on correlated pre- and              selection may equip agents with the prior expectation 
                                                postsynaptic activity, but here it is modulated by value.             that they will explore their environment until states 
                                                value is signalled by ascending neuromodulatory trans-                with innate value are encountered. We will look at this 
                                                mitter systems and controls which neuronal groups                     more closely in the next section, where priors on motion 
                                                are selected and which are not. The beauty of neural                  through state space are cast in terms of policies in  
                                                Darwinism is that it nests distinct selective processes               reinforcement learning.
                                                within each other. In other words, it eschews a single unit               Both neural Darwinism and the free-energy principle 
                                                of selection and exploits the notion of meta-selection                try to understand somatic changes in an individual in 
                                                (the selection of selective mechanisms; for example, see              the context of evolution: neural Darwinism appeals to 
                                                REF. 89). In this context, (neuronal) value confers evolu-            selective processes, whereas the free energy formulation 
                                                tionary value (that is, adaptive fitness) by selecting neu-           considers the optimization of ensemble or population  
                                                ronal groups that meditate adaptive stimulus–stimulus                 dynamics in terms of entropy and surprise. The key 
                                                associations and stimulus–response links. The capacity                theme that emerges here is that (heritable) prior expecta-
                                                of value to do this is assured by natural selection, in the           tions can label things as innately valuable (unsurprising); 
                                                sense that neuronal value systems are themselves subject              but how can simply labelling states engender adaptive 
                                                to selective pressure.                                                behaviour? In the next section, we return to reinforce-
                                                                                                                90
                                                    This theory, particularly value-dependent learning ,              ment learning and related formulations of action that try 
                                                has deep connections with reinforcement learning and                  to explain adaptive behaviour purely in terms of labels 
               Reentrant signalling             related approaches in engineering (see below), such as                or cost functions.
               Reciprocal message passing       dynamic programming and temporal difference mod-
                                                   91,92
               among neuronal groups.           els    . This is because neuronal value systems reinforce             Optimal control theory and game theory
                                                connections to themselves, thereby enabling the brain                 value is central to theories of brain function that are 
               Reinforcement learning           to label a sensory state as valuable if, and only if, it leads to     based on reinforcement learning and optimum con-
               An area of machine learning      another valuable state. This ensures that agents move                 trol. The basic notion that underpins these treatments 
               concerned with how an agent      through a succession of states that have acquired value to            is that the brain optimizes value, which is expected 
               maximizes long-term reward.      access states (rewards) with genetically specified innate             reward or utility (or its complement — expected loss 
               Reinforcement learning 
               algorithms attempt to find a     value. In short, the brain maximizes value, which may be              or cost). This is seen in behavioural psychology as rein-
               policy that maps states of the                                                                                                 98
                                                reflected in the discharge of value systems (for example,             forcement learning , in computational neuroscience 
               world to actions performed by    dopaminergic systems92–96). so how does this relate to                and machine learning as variants of dynamic program-
               the agent.                       the optimization of free energy?                                      ming such as temporal difference learning99–101, and in 
               Optimal control theory               The answer is simple: value is inversely proportional             economics as expected utility theory102. The notion of  
               An optimization method           to surprise, in the sense that the probability of a pheno-            an expected reward or cost is crucial here; this is the 
               (based on the calculus of        type being in a particular state increases with the value             cost expected over future states, given a particular policy 
               variations) for deriving an      of that state. Furthermore, the evolutionary value of                 that prescribes action or choices. A policy specifies the 
               optimal control law in a         a phenotype is the negative surprise averaged over all                states to which an agent will move from any given state 
               dynamical system. A control 
               problem includes a cost          the states it experiences, which is simply its negative               (‘motion through state space in continuous time’). This 
               function that is a function of   entropy. Indeed, the whole point of minimizing free                   policy has to access sparse rewarding states using a cost 
               state and control variables.     energy (and implicitly entropy) is to ensure that agents              function, which only labels states as costly or not. The 
               Bellman equation                 spend most of their time in a small number of valuable                problem of how the policy is optimized is formalized 
               (Or dynamic programming          states. This means that free energy is the complement of              in optimal control theory as the Bellman equation and its 
               equation.) Named after           value, and its long-term average is the complement of                 variants99 (see supplementary information s4 (box)), 
               Richard Bellman, it is a         adaptive fitness (also known as free fitness in evolution-            which express value as a function of the optimal policy 
               necessary condition for          ary biology97). But how do agents know what is valu-                  and a cost function. If one can solve the Bellman equa-
               optimality associated with       able? In other words, how does one generation tell the                tion, one can associate each sensory state with a value 
               dynamic programming in 
               optimal control theory.          next which states have value (that is, are unsurprising)?             and optimize the policy by ensuring that the next state 
               NATuRE REvIEWs | NeuroscieNce                                                                                                    voluME 11 | FEBRuARy 2010 | 133
                                                                        © 2010 Macmillan Publishers Limited. All rights reserved
