             
           SUPPLEMENTARY INFORMATION                                                In format provided by Friston (FEBRUARY 2010) 
                    Supplementary information S1 (box): The entropy of sensory states and their causes 
                    This  box  shows  that  the  entropy  of  hidden  states  in  the  environment  is  bounded  by  the 
                    entropy of sensory states. This means that if the entropy of sensory signals is minimised, so 
                    is the entropy of the environmental states that caused them. For any agent or model  m  the 
                    entropy of generalised sensory states  %          ′ ′′   T  is simply their average surprise 
                                                           s(t) =[s,s ,s ,K]
                            %
                     −ln p(s |m) (with a sight abuse of notion) 
                     
                                                                T
                        %             %          %      %               %                          S1.1 
                     H(s|m):=∫−p(s|m)ln p(s|m)ds = lim∫−ln p(s(t)|m)dt
                                                            T→•
                                                                0
                     
                    Under ergodic assumptions, this is just the long-term time or path-integral of surprise. We will 
                    assume sensory states are an analytic function of hidden environmental states plus some 
                    generalised random fluctuations 
                     
                     %     %      %
                     s = g(x,θ)+ z
                     &                                                                             S1.2 
                     %      %      %
                     x = f (x,θ)+w
                     
                    Here, hidden states change according to the stochastic differential equations of motion (with 
                                                      %       %
                    parameters θ ) in S1.2. Because  x and  z  are statistically independent, we have (see Eq. 
                    6.4.6 in Jones 1979, p149) 
                     
                       % %       %          %            %              %
                     I(s,z) = H(s |m)−H(x|m)− p(x|m)ln|∂%g|dx                                      S1.3 
                                                    ∫              x
                     
                             % %
                    Here,  I(s,z) ≥ 0 is the mutual information between the sensory states and noise. By Gibb’s 
                    inequality  this  cross-entropy  or  Kullback-Leibler  divergence is non-negative (Theorem 6.5; 
                    Jones 1979, p151). This means the entropy of the sensory states is greater than the entropy 
                    of the sensory mapping. Here. ∂%g  is the sensitivity or gradient of the sensory mapping with 
                                                    x
                    respect to the hidden states. The integral in S1.3 reflects the fact that entropy is not invariant 
           NATURE REVIEWS | NEUROSCIENCE                                                          www.nature.com/reviews/neuro 
                                                  © 2010 Macmillan Publishers Limited.  All rights reserved. 
