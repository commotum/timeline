                   REVIEWS
                                                                
                                                               Box 1 | The free-energy principle
                                                               Part a of the figure shows the dependencies among the                                    a
                                                               quantities that define free energy. These include the                                            Environment                                            Agent
                                                               internal states of the brain μ(t) and quantities describing its 
                                                               exchange with the environment: sensory signals (and their                                                                  Sensations
                                                                                               T                                                                                          ~     ~       ~
                                                               motion) ˜s(t) = [s,s′,s″…]  plus action a(t). The environment                                                             s = g(x, ϑ) + z
                                                               is described by equations of motion, which specify the 
                                                               trajectory of its hidden states. The causes ϑ ⊃ {x˜ , θ, γ } of                                External states                                    Internal states
                                                               sensory input comprise hidden states x˜ (t), parameters θ                                       ~     ~          ~                                                 ~
                                                                                                                                                               ˙
                                                                                                                                                               x = f(x,                                        μ = arg min F(s,
                                                               and precisions γcontrolling the amplitude of the random                                                  a, ϑ) + w                                                   μ)
                                                                                       
                                                               fluctuations  z˜ (t) and  w˜ (t). Internal brain states and action 
                                                               minimize free energy F(s˜ ,μ), which is a function of sensory                                                     Action or control signals
                                                               input and a probabilistic representation q(ϑ|μ) of its causes.                                                                           ~
                                                                                                                                                                                      a = arg min F(s,
                                                               This representation is called the recognition density and is                                                                                μ)
                                                               encoded by internal states μ.
                                                                  The free energy depends on two probability densities:                                 b
                                                               the recognition density q(ϑ|μ) and one that generates                                                         Free-energy bound on surprise
                                                               sensory samples and their causes, p(s˜ ,ϑ|m). The latter                                                                   ~
                                                                                                                                                                             F = −<ln p(s,
                                                                                                                                                                                             ϑ | m)>  + <ln q(ϑ | μ)>
                                                               represents a probabilistic generative model (denoted by                                                                              q                  q
                                                               m), the form of which is entailed by the agent or brain.                                   Action minimizes prediction errors
                                                               Part b of the figure provides alternative expressions for the                              F = D(q(ϑ                          ~
                                                                                                                                                                       | μ) || p(ϑ)) − <ln p(s(a) | ϑ, m)>q
                                                               free energy to show what its minimization entails: action                                  a = arg max Accuracy
                                                               can reduce free energy only by increasing accuracy (that is, 
                                                               selectively sampling data that are predicted). Conversely,                                                                Perception optimizes predictions
                                                               optimizing brain states makes the representation an                                                                                               ~          ~
                   Surprise                                    approximate conditional density on the causes of sensory                                                                  F = D(q(ϑ | μ) || p(ϑ | s)) − ln p(s |  m)
                   (Surprisal or self information.)            input. This enables action to avoid surprising sensory                                                                    μ = arg max Divergence
                   The negative log-probability of             encounters. A more formal description is provided below.
                   an outcome. An improbable                   optimizing the sufficient statistics (representations)
                   outcome (for example, water                                                                                                                                                   Nature Reviews | Neuroscience
                                                               Optimizing the recognition density makes it a posterior or conditional density on the causes of sensory data: this can be 
                   flowing uphill) is therefore                seen by expressing the free energy as surprise –In p(s˜ ,| m) plus a Kullback-Leibler divergence between the recognition and 
                   surprising.                                 conditional densities (encoded by the ‘internal states’ in the figure). Because this difference is always positive, minimizing 
                   Fluctuation theorem                         free energy makes the recognition density an approximate posterior probability. This means the agent implicitly infers or 
                   (A term from statistical                    represents the causes of its sensory samples in a Bayes-optimal fashion. At the same time, the free energy becomes a tight 
                   mechanics.) Deals with the                  bound on surprise, which is minimized through action.
                   probability that the entropy                optimizing action
                   of a system that is far from the            Acting on the environment by minimizing free energy enforces a sampling of sensory data that is consistent with the 
                   thermodynamic equilibrium                   current representation. This can be seen with a second rearrangement of the free energy as a mixture of accuracy and 
                   will increase or decrease over              complexity. Crucially, action can only affect accuracy (encoded by the ‘external states’ in the figure). This means that  
                   a given amount of time. It                  the brain will reconfigure its sensory epithelia to sample inputs that are predicted by the recognition density — in other 
                   states that the probability of 
                   the entropy decreasing                      words, to minimize prediction error.
                   becomes exponentially smaller 
                   with time.
                   Attractor                                      In short, the long-term (distal) imperative — of main-                                Crucially, free energy can be evaluated because it is a 
                   A set to which a dynamical                taining states within physiological bounds — translates                                    function of two things to which the agent has access: its 
                   system evolves after a long               into a short-term (proximal) avoidance of surprise.                                        sensory states and a recognition density that is encoded 
                   enough time. Points that                  surprise here relates not just to the current state, which                                 by its internal states (for example, neuronal activity 
                   get close to the attractor                cannot be changed, but also to movement from one state                                     and connection strengths). The recognition density is a 
                   remain close, even under                  to another, which can change. This motion can be com-                                      probabilistic representation of what caused a particular 
                   small perturbations.
                                                             plicated and itinerant (wandering) provided that it revis-                                 sensation.
                   Kullback-Leibler divergence               its a small set of states, called a global random attractor10,                                  This (variational) free-energy construct was  
                   (Or information divergence,               that are compatible with survival (for example, driving a                                  introduced into statistical physics to convert difficult  
                   information gain or cross                 car within a small margin of error). It is this motion that                                probability-density integration problems into eas-
                   entropy.) A non-commutative 
                                                                                                                                                                                                      11
                   measure of the non-negative               the free-energy principle optimizes.                                                       ier optimization problems . It is an information  
                   difference between two                         so far, all we have said is that biological agents must                               theoretic quantity (like surprise), as opposed to a 
                   probability distributions.                avoid surprises to ensure that their states remain within                                  thermo dynamic quantity. variational free energy has 
                   Recognition density                       physiological bounds (see supplementary information s1                                     been exploited in machine learning and statistics to 
                                                                                                                                                                                                                            12–14
                   (Or ‘approximating conditional            (box) for a more formal argument). But how do they                                         solve many inference and learning problems                                . In this 
                   density’.) An approximate                 do this? A system cannot know whether its sensations                                       setting, surprise is called the (negative) model evidence. 
                   probability distribution of the           are surprising and could not avoid them even if it did                                     This means that minimizing surprise is the same as 
                   causes of data (for example,              know. This is where free energy comes in: free energy is                                   maximizing the sensory evidence for an agent’s exist-
                   sensory input). It is the product         an upper bound on surprise, which means that if agents                                     ence, if we regard the agent as a model of its world. In 
                   of inference or inverting a 
                   generative model.                         minimize free energy, they implicitly minimize surprise.                                   the present context, free energy provides the answer to 
                   128 | FEBRuARy 2010 | voluME 11                                                                                                                                             www.nature.com/reviews/neuro
                                                                                            © 2010 Macmillan Publishers Limited. All rights reserved
