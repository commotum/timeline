                                                                                                                                                                                   REVIEWS
                                                      a fundamental question: how do self-organizing adap-                               In summary, the free energy rests on a model of how 
                                                      tive systems avoid surprising states? They can do this by                      sensory data are generated and on a recognition density 
                                                      minimizing their free energy. so what does this involve?                       on the model’s parameters (that is, sensory causes). Free 
                                                                                                                                     energy can be reduced only by changing the recognition 
                                                      Implications: action and perception. Agents can                                density to change conditional expectations about what is 
                                                      suppress free energy by changing the two things it depends                     sampled or by changing sensory samples (that is, sensory 
                Generative model                      on: they can change sensory input by acting on the world                       input) so that they conform to expectations. In what fol-
                A probabilistic model (joint          or they can change their recognition density by chang-                         lows, I consider these implications in light of some key 
                density) of the dependencies          ing their internal states. This distinction maps nicely                        theories about the brain.
                between causes and                    onto action and perception (BOX 1). one can see what this 
                consequences (data), from             means in more detail by considering three mathematically                       The Bayesian brain hypothesis
                which samples can be 
                generated. It is usually                                                                                                                                   17
                                                      equivalent formulations of free energy (see supplementary                      The Bayesian brain hypothesis  uses Bayesian probability 
                specified in terms of the             information s2 (box) for a mathematical treatment).                            theory to formulate perception as a constructive process 
                likelihood of data, given their           The first formulation expresses free energy as energy                      based on internal or generative models. The underlying 
                causes (parameters of a model)                                                                                                                                                      18–22
                and priors on the causes.             minus entropy. This formulation is important for three                         idea is that the brain has a model of the world                      that 
                                                                                                                                                                                          23–28
                                                      reasons. First, it connects the concept of free energy as                      it tries to optimize using sensory inputs                 . This idea is 
                Conditional density                                                                                                                                        20 
                                                      used in information theory with concepts used in sta-                          related to analysis by synthesis and epistemological autom-
                (Or posterior density.) The                                                                                              19
                probability distribution of           tistical thermodynamics. second, it shows that the free                        ata . In this view, the brain is an inference machine that 
                                                                                                                                                                                              18,22,25
                causes or model parameters,           energy can be evaluated by an agent because the energy                         actively predicts and explains its sensations                  . Central 
                given some data; that is, a           is the surprise about the joint occurrence of sensations                       to this hypothesis is a probabilistic model that can gener-
                probabilistic mapping from            and their perceived causes, whereas the entropy is sim-                        ate predictions, against which sensory samples are tested 
                observed data to causes.              ply that of the agent’s own recognition density. Third, it                     to update beliefs about their causes. This generative 
                Prior                                 shows that free energy rests on a generative model of the                      model is decomposed into a likelihood (the probability of 
                The probability distribution or       world, which is expressed in terms of the probability of a                     sensory data, given their causes) and a prior (the a priori 
                density of the causes of data         sensation and its causes occurring together. This means                        probability of those causes). Perception then becomes the 
                that encodes beliefs about            that an agent must have an implicit generative model of                        process of inverting the likelihood model (mapping from 
                those causes before observing         how causes conspire to produce sensory data. It is this                        causes to sensations) to access the posterior probability of 
                the data.                             model that defines both the nature of the agent and the                        the causes, given sensory data (mapping from sensations 
                Bayesian surprise                     quality of the free-energy bound on surprise.                                  to causes). This inversion is the same as minimizing the 
                A measure of salience based               The second formulation expresses free energy as                            difference between the recognition and posterior densi-
                on the Kullback-Leibler               surprise plus a divergence term. The (perceptual) diver-                       ties to suppress free energy. Indeed, the free-energy for-
                divergence between the                gence is just the difference between the recognition den-                      mulation was developed to finesse the difficult problem 
                recognition density (which            sity and the conditional density (or posterior density) of the                 of exact inference by converting it into an easier optimi-
                encodes posterior beliefs) and 
                the prior density. It                 causes of a sensation, given the sensory signals. This con-                    zation problem11–14. This has furnished some powerful 
                measures the information that         ditional density represents the best possible guess about                      approximation techniques for model identification and 
                can be recognized in the data.        the true causes. The difference between the two densities                      comparison (for example, variational Bayes or ensemble 
                Bayesian brain hypothesis             is always non-negative and free energy is therefore an                         learning29). There are many interesting issues that attend 
                The idea that the brain uses          upper bound on surprise. Thus, minimizing free energy                          the Bayesian brain hypothesis, which can be illuminated 
                internal probabilistic                by changing the recognition density (without changing                          by the free-energy principle; we will focus on two.
                (generative) models to update         sensory data) reduces the perceptual divergence, so that                           The first is the form of the generative model and 
                posterior beliefs, using sensory      the recognition density becomes the conditional density                        how it manifests in the brain. one criticism of Bayesian 
                information, in an                    and the free energy becomes surprise.                                          treatments is that they ignore the question of how prior 
                (approximately) Bayes-optimal 
                fashion.                                  The third formulation expresses free energy as com-                        beliefs, which are necessary for inference, are formed27. 
                Analysis by synthesis                 plexity minus accuracy, using terms from the model                             However, this criticism dissolves with hierarchical 
                Any strategy (in speech coding)       comparison literature. Complexity is the difference                            generative models, in which the priors themselves are 
                                                                                                                                                   26,28
                in which the parameters of a          between the recognition density and the prior density                          optimized          . In hierarchical models, causes in one 
                                                                                                                 15
                signal coder are evaluated by         on causes; it is also known as Bayesian surprise  and is the                   level generate subordinate causes in a lower level; sen-
                decoding (synthesizing) the           difference between the prior density — which encodes                           sory data per se are generated at the lowest level (BOX 2). 
                signal and comparing it with          beliefs about the state of the world before sensory data are                   Minimizing the free energy effectively optimizes empiri-
                the original input signal.            assimilated — and posterior beliefs, which are encoded                         cal priors (that is, the probability of causes at one level, 
                Epistemological automata              by the recognition density. Accuracy is simply the sur-                        given those in the level above). Crucially, because empir-
                Possibly the first theory for why     prise about sensations that are expected under the recog-                      ical priors are linked hierarchically, they are informed 
                top-down influences (mediated         nition density. This formulation shows that minimizing                         by sensory data, enabling the brain to optimize its prior 
                by backward connections in            free energy by changing sensory data (without changing                         expectations online. This optimization makes every level 
                the brain) might be important         the recognition density) must increase the accuracy of                         in the hierarchy accountable to the others, furnishing an 
                in perception and cognition.          an agent’s predictions. In short, the agent will selectively                   internally consistent representation of sensory causes at 
                Empirical prior                       sample the sensory inputs that it expects. This is known                       multiple levels of description. Not only do hierarchical 
                A prior induced by hierarchical                               16
                models; empirical priors              as active inference . An intuitive example of this process                     models have a key role in statistics (for example, ran-
                                                      (when it is raised into consciousness) would be feeling                        dom effects and parametric empirical Bayes models30,31), 
                provide constraints on the            our way in darkness: we anticipate what we might touch                         they may also be used by the brain, given the hierarchical 
                recognition density in the usual 
                                                                                                                                                                                       32–34
                way but depend on the data.           next and then try to confirm those expectations.                               arrangement of cortical sensory areas                  .
                NATuRE REvIEWs | NeuroscieNce                                                                                                                     voluME 11 | FEBRuARy 2010 | 129
                                                                                 © 2010 Macmillan Publishers Limited. All rights reserved
