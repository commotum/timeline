                REVIEWS
                Optimal decision theory             is the most valuable of the available states. In general,                    because it explains why agents must minimize expected 
                (Or game theory.) An area of        it is impossible to solve the Bellman equation exactly,                      cost. Furthermore, free energy provides a quantitative 
                applied mathematics                 but several approximations exist, ranging from simple                        and seamless connection between the cost functions 
                concerned with identifying the                                        98
                values, uncertainties and other     Rescorla–Wagner models  to more comprehensive for-                           of reinforcement learning and value in evolutionary 
                                                                                      100
                constraints that determine an       mulations like Q-learning            . Cost also has a key role in           biology. Finally, the dynamical perspective provides a 
                optimal decision.                   Bayesian decision theory, in which optimal decisions                         mechanistic insight into how policies are specified in the 
                                                                                                                                                                                          99
                                                    minimize expected cost in the context of uncertainty                         brain: according to the principle of optimality  cost is the 
                Gradient ascent                     about outcomes; this is central to optimal decision theory                   rate of change of value (see supplementary information 
                (Or method of steepest                                                                      102–104
                ascent.) A first-order              (game theory) and behavioural economics                       .              s4 (box)), which depends on changes in sensory states. 
                optimization scheme that finds          so what does free energy bring to the table? If one                      This suggests that optimal policies can be prescribed by 
                a maximum of a function by          assumes that the optimal policy performs a gradient                          prior expectations about the motion of sensory states. 
                changing its arguments in           ascent on value, then it is easy to show that value is                       Put simply, priors induce a fixed-point attractor, and 
                proportion to the gradient of       inversely proportional to surprise (see supplementary                        when the states arrive at the fixed point, value will stop 
                the function at the current         information s4 (box)). This means that free energy is                        changing and cost will be minimized. A simple exam-
                value. In short, a hill-climbing    (an upper bound on) expected cost, which makes sense                         ple is shown in FIG. 2, in which a cued arm movement 
                scheme. The opposite scheme 
                is a gradient descent.              as optimal control theory assumes that action mini-                          is simulated using only prior expectations that the arm 
                                                    mizes expected cost, whereas the free-energy principle                       will be drawn to a fixed point (the target). This figure 
                                                    states that it minimizes free energy. This is important                      illustrates how computational motor control105–109 can 
                                                                                                                                 be formulated in terms of priors and the suppression of 
                                                                                                                                 sensory prediction errors (K.J.F., J. Daunizeau, J. Kilner 
                                                                Predictions                                                      and s.J. Kiebel, unpublished observations). More gener-
                                                (2)
                                               ξ                                                                                 ally, it shows how rewards and goals can be considered 
                                      (1)       v               Prediction errors
                                                                                                                                                                                                       16
                                     ξx                                                                                          as prior expectations that an action is obliged to fulfil  
                                                 (1)                                                                             (see also REF. 110). It also suggests how natural selection 
                                               μ
                                                 v
                          (1)           (1)                                                                                      could optimize behaviour through the genetic specifi-
                                      μ
                         ξ              x
                          v                                                                                                      cation of inheritable or innate priors that constrain the 
                                                                                                           Movement              learning of empirical priors (BOX 2) and subsequent goal-
                                                          V                                                trajectory            directed action.
                                                  s     =+ w
                                                   visual  J     visual                                                              It should be noted that just expecting to be attracted 
                                                                            (0, 0)                                               to some states may not be sufficient to attain those states. 
                     Motor 
                     signals                                                    x                                                This is because one may have to approach attractors vicar-
                                                                                 1
                                                          x                                                                      iously through other states (for example, to avoid obsta-
                                                  s        1                       J                                             cles) or conform to physical constraints on action. These 
                                                       =+ w                        1
                                                   prop   x      prop
                                                           2                                                   V = (v, v , v )   are some of the more difficult problems of accessing  
                                  (1)                                                                                1  2 3
                                ξv                                                         J                                     distal rewards that reinforcement learning and opti-
                                                                                 x2        2                                     mum control contend with. In these circumstances, 
                               a                       Action                                                                    an examination of the density dynamics, on which the  
                                                                                                            J = J  + J  = ( j , j )
                                                     ˙a = −∂ εTξ                 Jointed arm                    1   2    1 2
                                                            a                                                                    free-energy principle is based, suggests that it is sufficient 
                Figure 2 | A demonstration of cued reaching movements. The lower right part of the                               to keep moving until an a priori attractor is encountered 
                figure shows a motor plant, comprising a two-jointed arm with two hidden states, each of                         (see supplementary information s5 (box)). This entails 
                which corresponds to a particular angular position of the two joints; the current position                       destroying unexpected (costly) fixed points in the envi-
                                                                                         Nature Reviews | Neuroscience
                of the finger (red circle) is the sum of the vectors describing the location of each joint.                      ronment by making them unstable (like shifting to a new 
                Here, causal states in the world are the position and brightness of the target (green                            position when sitting uncomfortably). Mathematically, 
                circle). The arm obeys Newtonian mechanics, specified in terms of angular inertia and                            this means adopting a policy that ensures a positive 
                friction. The left part of the figure illustrates that the brain senses hidden states directly                   divergence in costly states (intuitively, this is like being 
                in terms of proprioceptive input (S          ) that signals the angular positions (x ,x ) of the 
                                                          prop                                           1  2                    pushed through a liquid with negative viscosity or  
                joints and indirectly through seeing the location of the finger in space (J ,J ). In addition, 
                                                                                                       1 2                       friction). see FIG. 3 for a solution to the classical  
                through visual input (S        ) the agent senses the target location (v ,v ) and brightness (v ). 
                                           visual                                              1  2                      3       mountain car problem using a simple prior that induces 
                Sensory prediction errors are passed to higher brain levels to optimize the conditional                          this sort of policy. This prior is on motion through state 
                expectations of hidden states (that is, the angular position of the joints) and causal (that 
                is, target) states. The ensuing predictions are sent back to suppress sensory prediction                         space (that is, changes in states) and enforces exploration  
                errors. At the same time, sensory prediction errors are also trying to suppress themselves                       until an attractive state is found. Priors of this sort may 
                by changing sensory input through action. The grey and black lines denote reciprocal                             provide a principled way to understand the exploration–
                message passing among neuronal populations that encode prediction error and                                                                 111–113
                                                                                                                                 exploitation trade-off             and related issues in evolu-
                conditional expectations; this architecture is the same as that depicted in BOX 2. The                                               114
                blue lines represent descending motor control signals from sensory prediction-error                              tionary biology        . The implicit use of priors to induce 
                units. The agent’s generative model included priors on the motion of hidden states that                          dynamical instability also provides a key connection 
                effectively engage an invisible elastic band between the finger and target (when the                             to dynamical systems theory approaches to the brain 
                target is illuminated). This induces a prior expectation that the finger will be drawn to                        that emphasize the importance of itinerant dynamics, 
                the target, when cued appropriately. The insert shows the ensuing movement trajectory                            metastability, self-organized criticality and winner-
                caused by action. The red circles indicate the initial and final positions of the finger,                        less competition115–123. These dynamical phenomena 
                which reaches the target (green circle) quickly and smoothly; the blue line is the                               have a key role in synergetic and autopoietic accounts of  
                simulated trajectory.                                                                                            adaptive behaviour5,124,125.
                134 | FEBRuARy 2010 | voluME 11                                                                                                                   www.nature.com/reviews/neuro
                                                                              © 2010 Macmillan Publishers Limited. All rights reserved
