                         Under review as a conference paper at ICLR 2025
                 108                         ViT-Vanilla          ViTARC-VT (Ours)           ViTARC (Ours)
                 109
                 110
                 111                                                                                          So
                                                                                                              l
                                                                                                              v
                 112                                                                                          ed
                                                                                                               
                                                                                                              T
                 113        s                                                                                 es
                                                                                                              t
                                                                                                               
                            sk                                                                                I
                 114        a                                                                                 n
                            T                                                                                 s
                                                                                                              t
                 115        C                                                                                 an
                            R                                                                                 ces
                 116        A
                             
                            f                                                                                  
                            o                                                                                 p
                 117                                                                                          er
                            r
                            e                                                                                  
                 118        b                                                                                 A
                            m                                                                                 R
                 119                                                                                          C
                                                                                                               
                            Nu                                                                                T
                 120                                                                                          as
                                                                                                              k 
                                                                                                              (
                 121                                                                                          %)
                 122
                 123                     Solved Test Instances per ARC Task (%)
                 124
                 125
                 126     Figure3: Modelperformanceson400ARCtasks. Threemodelsareshown: ViT-Vanilla(red)rep-
                 127     resents the vanilla vision transformer setup (cf. Section 3); ViTARC-VT (light green) and ViTARC
                 128     (dark green) represent the variants of our framework introduced in Sections 4 and 5, respectively.
                 129     (Left) Distribution of Solve Rates: The horizontal axis shows the solve rate (percentage of test in-
                 130     stances that are solved correctly) on 1000 test instances per task. The vertical axis displays the
                 131     numberoftasksateachsolveratelevel. (Right) Distribution Statistics: The stars and corresponding
                 132     values are the overall solve rates across all test instances from all tasks. VITARC-VT and VITARC
                 133     showsignificant improvement in performance over the vanilla ViT.
                 134
                 135     2   RELATED WORK
                 136
                 137     Abstract Visual Reasoning (AVR) is an emerging field that seeks to measure machine “intelli-
                 138                  ´        ´
                 139     gence” (Małkinski & Mandziuk, 2023). Unlike many popular studies that focus on visual reasoning
                 140     with multi-modal input (Antol et al., 2015; Johnson et al., 2017; Zellers et al., 2019; Bakhtin et al.,
                         2019; Li et al., 2024), AVR focuses on reasoning tasks where the inputs are strictly images. The
                 141     goal of AVR tasks is to discover abstract visual concepts and apply them to new settings. While
                 142     the ARC is a generation task using abstract rules, other AVR tasks include classification tasks with
                 143     explicit rules, such as the Raven’s Progressive Matrices (Raven, 2003) and Odd-One-Out (Gard-
                 144                                                     ´         ´
                         ner & Richards, 2006). We refer the readers to Małkinski & Mandziuk (2023) for a more detailed
                 145     introduction to AVR.
                 146
                 147     Vision Transformers & Positional Encoding.  ATransformer architecture is based on the atten-
                 148     tion mechanism (Vaswani et al., 2017). Following successes in natural language processing (Brown
                 149     et al., 2020; Achiam et al., 2023; Devlin et al., 2019), recent studies have extended the Transformer
                 150     to the vision domain (Han et al., 2023). State-of-the-art approaches involve dividing the image into
                 151     rectangular “patches”(Dosovitskiy et al., 2021), where various techniques such as dynamic patch
                 152     sizes allow for more effective capture of local information (Havtorn et al., 2023; Zhou & Zhu,
                 153     2023). Vision Transformers have been successfully used to perform various image-to-image gener-
                         ation tasks such as inpainting (Li et al., 2022), image restoration (Liang et al., 2021), colorization
                 154     (Kumaretal.), and denoising (Wang et al., 2022).
                 155     Due to the set-based (permutation-invariant) nature of attention, Positional Encodings are used to
                 156     inject positional information in a Transformer (Vaswani et al., 2017). State-of-the-art Positional
                 157     Encodings include Absolute Positional Encodings (APEs) where unique encodings are added to the
                 158     inputs directly (Devlin et al., 2019), Additive Relative Positional Encodings (RPEs) (Shaw et al.,
                 159
                 160        1Task A follows a rule based on color count: if the input grid has two distinct colors, the output contains
                 161     a grey diagonal from the top-left to the bottom-right. Conversely, if the input grid has three colors, the grey
                         diagonal is from the top-right to the bottom-left.
                                                                    3
