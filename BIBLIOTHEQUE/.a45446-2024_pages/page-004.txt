                                                                                                            Positional Vectors Across Layers of TL-Window
                                                                                                      2000
                                                                                                      1750
                                                                                                      1500
                                                                                                     ectors1250
                                                                                                      1000
                                                                                                     # Distinct V750
                                                                                                      500
                                                                                                      250                          Distinct positional vectors
                                                                                                        0                          Maximum TRF
                                                                                                            1      2      3      4      5      6      7
                                                                                                                               Layer
                                 Figure 1: PCA visualization of positional vectors                Figure 2: Comparison of distinct positional
                                 from the 1-st and 7-th layers.                                   vectors and theoretical receptive field.
                                 3.2.1     FormationofPositional Vectors with Full Attention
                                 Positional Vector After the First Layer               Tostudy how positional information is distributed over
                                 different positions, we first visualize the positional vectors p1,t (Eq. 4) decomposed from the outputs
                                 of the first layer using principal component analysis (PCA). As shown in Figure 1 (left column),
                                 initial tokens (e.g., ≤ 4 tokens) exhibit significantly distinct positional vectors, while the positional
                                 vectors of subsequent tokens are similar to each other. As a comparison, we also present the PCA
                                 results of all positional vectors at the 7-th layer. Interestingly, position vectors are evenly distributed
                                 across all the positions in Figure 1 (right column). Such a finding indicates that position vectors have
                                 captured the corresponding positional information since these vectors are distinct from each other
                                 across positions. In other words, being distinct can be considered as a kind of positional evidence. By
                                 comparing the left and right columns of Figure 1, it seems that only initial tokens are different from
                                 the rest tokens after the first layer, which might suggest that after the first layer, initial tokens have
                                 already formed distinct positional information but subsequent tokens have not yet established
                                 such information. To investigate the reasons behind this phenomenon, we select the first attention
                                 head in the first layer (similar to other heads) to analyze attention scores, as detailed in Appendix B.
                                 Wecanprovethatthepositional vector p1,1 for the first token is different from the following tokens
                                 and the attention scores affect the formation of positional information. Thus, through several layers,
                                 the tokens after the first token will gradually form distinct positional vectors (Figure 1 right column).
                                 Positional Information Flow From Initial Tokens                     Byapplyingpositional vectors at top layers
                                 (PCAvisualized in Appendix G), we find that after forwarding several layers, tokens at all positions
                                 can also exhibit distinct positional vectors, and similar findings are also found in previous work [19].
                                 Totrace back to the source of positional information, a reasonable speculation is that initial tokens
                                 play a key role in the formation of positional information for the rest tokens since only initial tokens
                                 capture positional information after the first layer. To validate this, we select two groups of reference
                                 tokens: initial tokens (1∼4) and secondary tokens (4∼256), and further analyze what information
                                 of these tokens is critical for the formation of positional information in subsequent tokens (>256).
                                 Thus, based on a top-down strategy, we conduct an ablation study for each group by respectively
                                 deleting the value vs of attention module (w/o value), the semantic vector cs (w/o semantic vector),
                                                         l,t                                                                 l,t
                                 positional vector p        (w/o positional vector), and positional basis m             (w/o positional basis). Then,
                                                        l,t                                                          l,t
                                 for each variant, we average the outputs of all layers and decompose new positional vectors based on
                                 Eq. 4. Finally, we compute the average cosine similarity between the original and new positional
                                 vectors for those subsequent tokens (>256) and also report PPL on samples in RedPajama. From
                                 Table 2, we can see that removing the positional vector and basis of 1∼4 tokens largely affect the
                                 positional vectors at later positions (low similarity). Conversely, removing the semantic vector or
                                 altering secondary tokens has slight effects on both similarity and PPL. From these findings, we
                                 conclude that the positional vectors of initial tokens seem to serve as the role of anchors, largely
                                 contributing to the formation of positional information in subsequent tokens.
                                                                                             4
