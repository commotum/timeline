           [20] Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang, and Xiaol-
              ing Wang. Length generalization of causal transformers without position encoding. CoRR,
              abs/2404.12224, 2024.
           [21] Jiajun Song and Yiqiao Zhong. Uncovering hidden geometry in transformers via disentangling
              position and context. CoRR, abs/2310.04861, 2023.
           [22] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.
              Compressive transformers for long-range sequence modelling. In 8th International Confer-
              ence on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
              OpenReview.net, 2020.
           [23] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small
              language model, 2024.
           [24] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset,
              2023.
           [25] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position rep-
              resentations. In Proceedings of the 2018 Conference of the North American Chapter of the
              Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New
              Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), pages 464–468. Association
              for Computational Linguistics, 2018.
           [26] ZihangDai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhut-
              dinov. Transformer-xl: Attentive languagemodelsbeyondafixed-lengthcontext. InProceedings
              of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence,
              Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 2978–2988. Association for
              Computational Linguistics, 2019.
           [27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
              Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
              text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.
           [28] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva
              Reddy. Theimpactofpositional encoding on length generalization in transformers. In Advances
              in Neural Information Processing Systems 36: Annual Conference on Neural Information
              Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023,
              2023.
           [29] Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander Rudnicky, and Peter J. Ramadge. Latent
              positional information is in the self-attention variance of transformer language models without
              positional embeddings. In Proceedings of the 61st Annual Meeting of the Association for
              Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14,
              2023, pages 1183–1193. Association for Computational Linguistics, 2023.
           [30] Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. BAMBOO: A
              comprehensivebenchmarkforevaluatinglongtextmodelingcapacitiesoflargelanguagemodels.
              In Proceedings of the 2024 Joint International Conference on Computational Linguistics,
              Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy,
              pages 2086–2099. ELRA and ICCL, 2024.
           [31] Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, and Ji-Rong Wen. Rear: A
              relevance-aware retrieval-augmented framework for open-domain question answering. CoRR,
              abs/2402.17497, 2024.
           [32] Saurav Pawar, S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Vinija Jain, Aman Chadha,
              and Amitava Das. The what, why, and how of context length extension techniques in large
              language models - A detailed survey. CoRR, abs/2401.07872, 2024.
           [33] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR,
              abs/2307.08691, 2023.
                               12
