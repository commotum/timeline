                                          can effectively stabilize the model performance beyond the context window [6, 17, 18]. Furthermore,
                                          decoder-only Transformers without positional encodings (NoPE) have been found to be capable of
                                          learning implicit positional information [19], and their context window size can be extended via the
                                          adjustmentoftemperaturehyper-parameters[20]. However, theaboveextensionmethodssolelyfocus
                                          on adapting positional encodings or attention scores, lacking a detailed analysis of the underlying
                                          mechanisms of hidden states in LLMs.
                                          In this work, we aim to investigate the inner working mechanism of LLMs within and beyond the
                                          context window to interpret these context window extension approaches. As the basis, our work is
                                          developed by analyzing the positional information implicitly encoded in the hidden states of LLMs
                                          across various layers and positions, both within and outside the context window. Inspired by previous
                                          work[21], we use a mean-based decomposition approach to disentangle positional vectors from the
                                          hidden states, which captures the information independent of semantics but related to positions.
                                          Specifically, we first investigate how positional information is formed and examine its impact on the
                                          attention mechanism within the context window. Second, for inputs beyond the context window, we
                                          analyze the change of positional vectors in two settings, i.e., direct extrapolation and context window
                                          extension. Our key findings include: (1) After the first layer, initial tokens can form distinct positional
                                          vectors, serving as anchors for shaping positional vectors in subsequent tokens; (2) Positional vectors
                                          play a critical role in modulating the long-term decay and establishing attention sinks; (3) When
                                          exceeding the context window, OOD positional vector is the major factor contributing to performance
                                          degradation, while length extrapolation can effectively keep the consistency of positional vectors both
                                          within and beyond the context window; (4) Context window extension methods enable interpolation
                                          of positional vectors by adjusting the information flow from initial tokens to subsequent tokens.
                                          Based on the empirical findings, we further propose two training-free context window extension
                                          methods from the perspective of interpolating positional vectors: positional vector replacement
                                          and attention window extension. For LLMs with NoPE, the former method replaces the positional
                                          vectors in critical layers with interpolated ones; while for LLMs with window attention and NoPE,
                                          the latter method directly scales the window size and adjusts the temperature hyper-parameter. We
                                          evaluate the length generalization capacities of the proposed methods on PG-19 [22]. Experimental
                                          results demonstrate that our methods can effectively generalize to longer texts without fine-tuning,
                                          achieving comparable performance to previous methods.
                                          Ourmaincontributions are summarized as follows:
                                             • Weexplicitly delineate the formation process and the effect of positional vectors, highlighting
                                                the anchoring role of initial tokens in shaping different positional vectors across tokens and their
                                                importance in achieving long-term decay and attention sinks.
                                             • Wearethefirst to unify length extrapolation and context window extension from the perspective
                                                of positional vectors, identifying that preventing OOD positional vectors is crucial for avoiding
                                                performance degradation.
                                             • Weproposetwotraining-free context window extension methods via the lens of adjusting posi-
                                                tional vectors, i.e.,positional vector replacement and attention window extension. Experimental
                                                results show that our methods can effectively generalize to longer texts without fine-tuning.
                                          2      Background
                                          Transformer              Decoder-onlyTransformer [4]hasbecomethefoundationalarchitectureforLLMs[4,
                                          8, 1]. For a Transformer with L layers and a context window size C, given an input sequence s of
                                                                                                                                                             s           s             s
                                          T tokens, i.e., {x ,...,x }, we denote the output of the l-th layer l as H = {h                                                   , . . . , h     }. At
                                                                     1            T                                                                          l           l,1           l,T
                                                                               s
                                          each layer, the output H is obtained through multi-head attention (MHA) and feed-forward network
                                                                               l
                                          (FFN) with residual connections applied to both components as follows:
                                                                            es                      s              s             s                es         es
                                                                           H =MHA(H )+H , H =FFN(H )+H .                                                                                       (1)
                                                                               l                    l−1            l−1           l                   l          l
                                                                                                  s
                                          Finally, the output of the last layer H                     is then projected into the logits, which will be used to generate
                                                                                                  L
                                          the prediction probability for each token in the vocabulary.
                                          Positional Vector               Previous work has found that positional information can be learned and encoded
                                          in the hidden states of Transformers [19]. Drawing inspiration from prior work [21], we hypothesize
                                                                                                                      2
