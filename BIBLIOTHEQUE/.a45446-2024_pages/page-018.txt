                                  Figure 10: PCA visualization of positional vectors from the 1-st layer of Llama-3-8B, Qwen1.5-7B,
                                  Yi-9B, and TL-NoPE-new.
                                  training from scratch. Therefore, we selected three mainstream LLMs: Llama-3-8B [34], Yi-9B [35],
                                  and Qwen1.5-7B [36], for comparison. Additionally, we trained a new LLM, TL-NoPE-new, from
                                  scratch under the same conditions as TL-NoPE. In a similar vein, we extracted positional vectors
                                  using 32K samples from the RedPajama dataset.
                                  F.1    FormationofPositional Vectors within Context Window
                                  Through principal component analysis (PCA), we first visualize the positional vectors from the initial
                                  layer of these LLMs, as illustrated in Figure 10. Consistent with our expectations, the initial tokens
                                  exhibit distinct positional information, while the subsequent tokens display a high degree of similarity.
                                  This observation supports the conclusion that the first-layer attention mechanism makes the initial
                                  tokens form unique positional information, as discussed in Section 3.2.1.
                                  Furthermore, we remove different components of the value vectors at different positions across all
                                  attention heads after the first layer. We then evaluate the impact of these modifications on both the
                                  positional vectors and the perplexity (PPL). As shown in Table 6, removing the positional basis of the
                                  initial tokens significantly degrades the model’s performance. Conversely, removing the components
                                  from subsequent tokens has a relatively smaller effect, highlighting the pivotal role of initial token
                                  positioning in influencing later tokens. However, we observe that larger LLMs are more attuned to
                                  semantic information and less affected by the removal of positional vectors compared to smaller
                                  LLMs.
                                                        Table 6: Results of removing different components in attention.
                                                             original      w/ovalue         w/opositional vector   w/opositional basis    w/osemantic basis
                                                             -          0∼4      32-256     0∼4       32∼256       0∼4       32-256       0∼4       32-256
                                     Llama-3          simi   1          0.75     0.9995     0.75      0.9583       0.2059    0.9997       0.9259    0.8666
                                                      ppl    6.74       16.27    6.63       17.20     8.4          >1000     6.60         17.6      15.18
                                     Yi-9B            simi   1          0.98     0.9999     0.92      0.9998       0.5368    1            0.91      0.9996
                                                      ppl    7.08       8.03     6.56       37.92     6.62         >1000     6.52         42.271    7.08
                                     Qwen-1.5-7B      simi   1          0.98     0.9997     0.9847    0.9986       0.7382    0.9993       0.9998    0.9951
                                                      ppl    7.97       9.51     8.03       9.51      8.04         217.13    7.98         8.09      8.68
                                    TL-NoPE-new      simi       1        0.70     0.95       0.69       0.95        0.41       0.93       0.99        1.0
                                                      ppl     11.03    224.74     22.36    263.53       20.91      >1000       21.78      11.66     12.699
                                  F.2    Effect of Positional Vectors on Attention
                                  In line with the experiments conducted in Section 3.2.3, we extract various components from the keys
                                  and queries to assess their impact on attention scores. The attention maps for the first 50 tokens are
                                  illustrated in Figure 11. When both the positional vectors and positional basis are removed, attention
                                                                                              18
