                             that each hidden state (e.g., query, key, value, output of each layer) within Transformer can be
                             decomposed into two parts, i.e., a positional vector that captures positional information and a
                             semantic vector that captures the contextual information. Taking the output hs of the l-th layer at
                                                                                                                 l,t
                             t-th position as an example, it can be decomposed into a positional vector p       and a semantic vector
                                                                                                             l,t
                             cs :
                               l,t
                                                                          s              s
                                                                        h =pl,t+c .                                                 (2)
                                                                          l,t            l,t
                             Such a decomposition can disentangle two primary factors, namely positional and semantic vectors,
                             for interpreting the internal mechanism of LLMs. Notably, since positional vectors are globally
                             shared across different inputs, there is no superscript s for p  . Further, the positional vector p   can
                                                                                            l,t                                 l,t
                             be decomposed into a mean vector u and a positional basis m :
                                                                    l                           l,t
                                                                         p =u +m ,                                                  (3)
                                                                          l,t     l     l,t
                             where the mean vector u denotes the mean of the distribution of positional vectors and the positional
                                                       l
                             basis m     denotes the offset of t-th position from the mean vector within the context window size C.
                                      l,t
                             Following previous work [21], we adopt a mean-based decomposition method to obtain the above
                             three vectors based on N samples from the training corpus as follows:
                                                          N                             C
                                                      1 X s                         1 X        ′     s      s
                                              p =            h , m =p −                    p , c =h −p .                            (4)
                                               l,t    N        l,t     l,t    l,t   C        l,t     l,t    l,t    l,t
                                                         s=1                           t′=1
                             With this decomposition, it offers an explicit way to analyze and explore the positional information
                             encodedinthehiddenstatesofTransformermodels. Forexample,wecanusesimilaritymeasurements
                             to comparethepositionalvectorsofdifferentpositionsandalsocanvisualizetheminlow-dimensional
                             embedding space. In the following sections, we will mainly focus on studying the formation and
                             impact of the positional vector p    , and conduct the analysis experiments.
                                                                l,t
                             3    Empirical Analysis
                             3.1   Experimental Settings
                             Tobetter analyze positional information, we consider model variants with different positional encod-
                             ings (PE) and attention mechanisms: variants without positional encodings (NoPE) [19] as well as
                             variants with two different positional encodings: RoPE [7] and ALiBi [6]. We continually pre-train
                             the TinyLlama-1.1B checkpoint [23] on 50B tokens from RedPajama [24] with a context window
                             C=2048,resultinginasetofcomparisonmodelswithdifferent positional encodings and attention
                             mechanisms, as shown in the Table 1. Full attention means that each token can attend to all previous
                             tokens, while window attention restricts each token to attend only to previous tokens within a window
                             size W. The training details are described in Appendix A. We also evaluate common LLMs (e.g.,
                             Llama-3-8B) and LLMs without positional encodings trained from scratch, and the evaluated results
                             are listed in Appendix F.
                             Specifically, we subsample 32K samples with the same number of tokens from RedPajama. We
                             perform the inference on these data to obtain hidden states of LLMs. By using the mean-based
                             decomposition method (Eq. 4), we can obtain the positional vectors pl,t of tokens in these sample
                             texts.
                             Table 1: The compared model variants. Full attention is denoted as Full and window attention with a
                             windowsize of W tokens is denoted as Window (W). We abbreviate TinyLLaMA as TL.
                                   Model     TL-NoPE    TL-RoPE     TL-ALiBi     TL-Window      TL-Window-80     TL-Window-RoPE
                                    PE         NoPE       RoPE        ALiBi         NoPE            NoPE               RoPE
                                 Attention     Full        Full        Full     Window(512)      Window(80)        Window(512)
                             3.2   FormationandEffectofPositional Vectors within Context Window
                             In existing LLMs, the bottom (first) layer typically takes as input token embeddings that lack inherent
                             positional information; while interestingly, the hidden states from top layers can implicitly capture
                             positional information, even without explicit positional encodings [19, 21, 14]. In order to have a
                             deep understanding of implicit positional information, we next investigate the formation and effect of
                             positional vectors in Transformers, with both full attention and window attention.
                                                                                 3
