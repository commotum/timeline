                             Effect of Initial Tokens on Context WindowExtension          Sincetheinitial tokens serve as the anchor
                             for the formation of subsequent positional vectors, we evaluate whether changing the information
                             flow from the initial tokens to the rest tokens can achieve the interpolation effect. To avoid the effect
                             of OODpositional encodings, we follow the attention scaling method on TL-NoPE but only scale the
                             attention logits between the initial tokens and others, denoted as Initial Scaling. As shown in Table 3,
                             it can achieve comparable performance and interpolation ratios closer than scaling all attention logits
                             in Attention Scaling (e.g., 2.38 vs 2.56), further underscoring that the interpolation of positional
                             vectors is mainly achieved by adjusting the information flow of anchor tokens.
                             4    Extending Context Window via Positional Vectors
                             Inspired by our analysis of the formation of positional vectors and the interpolation of positional
                             vectors when extending the context window, we propose two training-free context window extension
                             methods, i.e., positional vector replacement and attention window extension. The pseudocode of
                             these methods can presented in Appendix E.
                             4.1   Positional Vector Replacement
                             In Section 3.2.3, we show that when exceeding the context window, the OOD positional vectors tend
                             to cause the collapse of attention distribution. Further, we observe that context window extension
                             methods can achieve length interpolation of positional vectors and the effective interpolation ratio is
                             close to the expansion factor of the context window. Thus, we propose to replace all the implicitly
                             learned positional vectors with the interpolated ones, called positional vector replacement, to avoid
                             the OODissue in LLMs without positional encodings (NoPE).
                             Specifically, we linearly interpolate the positional vectors within the context window with an in-
                             terpolation ratio r and multiply the interpolated ones with a times α (≥ 1). In practice, we find
                             that properly increasing the interpolation ratio r and times α can achieve better effectiveness of
                             interpolation (details are discussed in Appendix D). Owing to the critical role of initial tokes, the
                             positional vectors of the first four tokens remain unchanged, while those of subsequent tokens are
                                                                                            ˆ
                             replaced with the interpolated vectors. The replaced output h      for each layer can be formulated as:
                                                                                             l,t
                                                                     ˆ                          ˆ
                                                                     h     = h −p +αp ,                                           (5)
                                                                      l,t        l,t     l,t     l,t
                                                 ˆ         ˆ
                                               {p ,...,p               } = Interpolation({p ,...,p              }),               (6)
                                                  l,5       l,r(C−4)+5                             l,5       l,C
                             where C, l, and s represent the original context window size, replaced layer, and interpolation ratio.
                             Since replacing positional vectors for all layers requires heavy recalculation efforts and the positional
                             information is passed across layers, we only apply the replacement strategy to a single early layer.
                             Wefindthat the 4-th layer is the optimal layer for replacement in TL-NoPE, as shown in Figure 8.
                             4.2   Attention Window Extension
                             Asdiscussed in Section 3.2.2, the positional vectors are shaped across layers and windows by the
                             distinct positional information of initial tokens. Inspired by these observations, we propose attention
                             window extension, the first training-free length interpolation method for window attention-based
                             LLMswithoutpositional encodings. The core idea is to extend the attention window size to control
                             the formation of positional vectors. When scaling the context window by a ratio, the window size also
                             needs to be extended by the same interpolation ratio r. However, for positions in the extended first
                             window{W +1,...,rW},theirpositionvectorsareOOD.Toavoidthis,wefollowtheattention
                             scaling method[20]andscaletheattentionlogitswithascalingfactorλ,achievingbetterinterpolation
                                                                                                             D                    D
                             of positional vectors. We define the attention score a   between query q ∈ R H and key k ∈ R H
                                                                                   ij                  i                   j
                             for any heads and layers as:
                                                                                      √
                                                                          exp(λq k / D )
                                                              a =                 i j     H        .                              (7)
                                                                ij   P                      √
                                                                       i        exp(λq k / D )
                                                                       z=i−rW          i  z     H
                             4.3   Results on Language Modeling
                             Toassess the effectiveness of our proposed methods, we evaluate language modeling performance
                             onthetest set of PG-19 [22]. In line with previous work [6], we measure PPL across various input
                                                                                8
