            [7] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Ro-
              former: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063,
              2024.
            [8] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
              thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez,
              ArmandJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
              language models. CoRR, abs/2302.13971, 2023.
            [9] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
              Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
              Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,
              Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony
              Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
              Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut
              Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
              Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,
              Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-
              qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
              Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien
              Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation
              and fine-tuned chat models. CoRR, abs/2307.09288, 2023.
           [10] bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size
              without any fine-tuning and minimal perplexity degradation., 2023.
           [11] emozilla. Dynamically Scaled RoPE further increases performance of long context LLaMA
              with zero fine-tuning, 2023.
           [12] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context
              windowextension of large language models. CoRR, abs/2309.00071, 2023.
           [13] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context
              windowoflargelanguage models via positional interpolation. CoRR, abs/2306.15595, 2023.
           [14] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite:
              Simple on-the-fly length generalization for large language models. CoRR, abs/2308.16137,
              2023.
           [15] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming
              language models with attention sinks. CoRR, abs/2309.17453, 2023.
           [16] HongyeJin,XiaotianHan,JingfengYang,ZhimengJiang,ZiruiLiu,Chia-YuanChang,Huiyuan
              Chen, and Xia Hu. LLM maybe longlm: Self-extend LLM context window without tuning.
              CoRR,abs/2401.01325, 2024.
           [17] Ta-ChungChi,Ting-HanFan,PeterJ.Ramadge,andAlexanderRudnicky. KERPLE:kernelized
              relative positional embedding for length extrapolation. In Advances in Neural Information
              Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022,
              NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.
           [18] Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter J. Ramadge. Dissecting trans-
              former length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st
              AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),ACL
              2023, Toronto, Canada, July 9-14, 2023, pages 13522–13537. Association for Computational
              Linguistics, 2023.
           [19] Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models
              without positional encodings still learn positional information. In Findings of the Association
              for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December
              7-11, 2022, pages 1382–1390. Association for Computational Linguistics, 2022.
                               11
