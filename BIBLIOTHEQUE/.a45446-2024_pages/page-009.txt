                                      Table 4: Results of language modeling in PG-19. The context window size C is 2048.
                                      Model              Interpolation Method           Factor        2K       4K       6K        8K
                                                                                                                  3         3        3
                                      TL-RoPE                      -                       -         10.17    >10      >10      >10
                                                            DynamicNTK                     -         10.17    10.45    11.28     28.58
                                                                                                                  3         3        3
                                                                   -                       -         11.92    >10      >10      >10
                                                           Attention Scaling            λ=1.2        17.03    17.05    54.26    >103
                                      TL-NoPE                                           λ=1.3        32.07    43.84    51.50     46.59
                                                     Positional Vector Replacement  r = 2,α = 1.1    13.54    15.58      -         -
                                                                (ours)              r = 5,α = 1.3    28.15    47.65    49.79     73.79
                                                                   -                       -         12.86   713.51    660.30   660.51
                                      TL-Window      Attention Window Extension     r = 2,λ = 1.1    13.70    14.10      -         -
                                                                (ours)              r = 4,λ = 1.2    17.23    31.66    29.27     29.30
                               lengths (from 2K to 8K) using a sliding window approach. We apply positional vector replacement
                               to TL-NoPE and attention window extension to TL-Window. All the hyper-parameters are selected
                               according to the PPL and the change of positional vectors across layers. For compared baselines, we
                               select Dynamic-NTK [11] for TL-RoPE and Attention Scaling [20] for TL-NoPE.
                               The results are shown in Table 4. First, without interpolation, the PPL increases extremely after
                               beyond the context window (e.g., > 103). When using the positional vector replacement or attention
                               windowextension methods, we observe that PPL decreases substantially, showing the effectiveness
                               of our proposed methods. Compared to attention scaling, our attention window extension method
                               successfullyextendsthecontextwindowto8KtokenswithlowerPPL.Moreover,ourpositionalvector
                               replacement method achieves similar performance to attention scaling within 6K tokens but shows
                               increased PPL at 8K. We attribute this phenomenon to the decreasing effective interpolation ratio
                               across layers, as shown in Figure 9. Additionally, an increase in PPL with the rising interpolation ratio
                               r is also observed in both our methods, likely due to imperfect interpolation of positional vectors.
                               5    Related Work
                               Position Information in Transformers           Positional information was crucial in Transformer-based
                               LLMs, to enhance the sequence modeling abilities. The vanilla Transformer introduced absolute
                               positional encodings, using a unique embedding to each position and adding it to the corresponding
                               input embedding [4]. In contrast, relative positional encodings introduced biases based on the relative
                               distance between tokens within attention modules [25–27, 6, 7]. Besides explicit positional encodings,
                               some work investigated the implicit positional information within hidden states of Transformers.
                               Evenwithout positional encodings, positional information was found in hidden states of Transformer
                               decoders [19, 28, 29]. Besides, prior work decoupled positional basis from hidden states in Trans-
                               formers and analyzed geometric properties [21]. Our work mainly explores positional information
                               embeddedinthehiddenstates of LLMs, examining the formation and impact of positional vectors,
                               and using it to analyze the mechanism of context window for LLMs.
                               Extending Context Window LLMs were often constrained by pre-defined context windows.
                               When processing inputs that exceed these windows, models typically encountered OOD issues,
                               leading to significant performance degradation. To meet the growing demands of long context
                               tasks [30, 31], various methods were proposed to address this limitation and model longer texts,
                               which can be roughly categorized into length extrapolation and context window extension [32].
                               Length extrapolation techniques aimed to maintain stable PPL regardless of text length by designing
                               specialized positional encodings or window attention mechanisms [6, 17, 18, 15, 14]. Conversely,
                               context window extension methods focused on extending the context window of existing models
                               by adapting positional encodings or temperature hyper-parameters, thereby enlarging the context
                               windowwithminimalperformanceloss[13, 12, 16, 20, 11, 10]. This paper bridges the concepts of
                               length extrapolation and context window extension through the lens of positional vectors, enhancing
                               the interpretability of context windows in LLMs.
                                                                                     9
