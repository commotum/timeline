                                 B.2    Proof of Preference in Attention Scores
                                 Following previous work [28], we only focus on a single attention head in the first layer with specific
                                 weights. In our parameterization, we only consider the first two dimensions. We demonstrate the
                                 capacity of causal Transformers to learn this capacity with either NoPE or RoPE.
                                 In our settings, the Transformer has H attention heads in each layer and the word embedding matrix
                                 is W ∈RD×V,whereDisthedimensionofthehiddenstatesandV isthenumberofvocabulary.
                                       E
                                Weset that the first dimension in word embedding conforms to normal distribution N(0,1), (i.e.,
                                 e1,t ∼ N(0,1),∀t ∈ {1,...,V}), while the second dimension is 1. Other dimensions are arbitrary
                                 values. Thus, the word embedding matrix W               is:
                                                                                      E
                                                                        e         e       e       . . .   e    
                                                                            1,1     1,2      1,3            1,V
                                                                         1         1        1     . . .     1 
                                                                        e         e       e       . . .   e    
                                                               WE= 3,1             3,2      3,3            3,V                                  (11)
                                                                         .          .       .      .        .  
                                                                         .          .       .       ..      .  
                                                                            .        .       .               .
                                                                          e        e       e       . . .  e
                                                                            D,1     D,2     D,3             D,V D×V
                                 Next, we set the projection matrix of query, key, and value as W , W , W                     of the first layer and
                                                                                                             Q      K       V
                                 first head of Transformers.
                                                                                                                                       
                                             0   1 ...      0                       1   0 ...       0                      1   0    . . .  0
                                             0   1 ...      0                       1   0 ...       0                      1   0    . . .  0
                                                                                                                                       
                                 W =. .                     .        , W =. .                    .        , W =. .                    .        .
                                     Q     .    .    ...    .             K     .     .   ...    .             V     .     .   ...    . 
                                             .   .           .                      .    .          .                      .    .          .
                                             0   1 ...      0 D×D                   1   0 ...       0 D×D                  1   0    . . .  0 D×D
                                                                H                                      H                                      H (12)
                                WKmakesurethatonlythefirstdimension will be considered in attention scores. WQ transfers the
                                 second dimension in the input to the first dimension. For Transformers without positional vectors
                                                                                                      s
                                 (NoPE), the attention logits for each key at position i is a             =e1,s . Thus, the vectors with large
                                                                                                      i,j         i
                                 first dimensions will be assigned with large attention logits, which proves that the model without
                                 positional vectors can learn to preference some values regardless of the query.
                                 For Transformer with RoPE, we assume that the first two dimensions correspond to the basis of the
                                 rope with a value of 1/10000 and the maximum context window size is 2048. Thus, for the largest
                                 relative distance i − j, the rotation angle is smaller than π. Thus, the attention score can be represent
                                     s                                                             2
                                 as a    =e       cos((i −j)/10000). Thus, the attention logits will be larger than zero only if the first
                                             1,s
                                     i,j        i
                                 dimension of the key is larger than zero. In addition, for the same relative distance, keys with larger
                                 first dimensions have large attention logits. Thus, keys with positive values in the first dimension will
                                 be assigned greater attention weights.
                                                         Transformers with NoPE                        Transformers with RoPE
                                                1.0                                           1.0
                                               ter attention0.8                              ter attention0.8
                                                0.6                                           0.6
                                                0.4                                           0.4
                                                0.2                                           0.2
                                               irst element of each hidden state af          irst element of each hidden state af
                                               F0.0                                          F0.0
                                                    0       500      1000     1500    2000        0        500     1000     1500     2000
                                                                   position id                                    position id
                                 Figure7: Thevaluesoffirstelementsoftheoutputofsingleheadattentionduetoattentionpreferences
                                 in Transformers with NoPE and RoPE.
                                We also experimentally examine the first element of the output at different positions. With the
                                 above settings, we generate 10000 sequences with a length of 2048 from this distribution. Then,
                                                                                          15
