                                                     PPL During Direct Extrapolation                    Maximum Similarity During Direct Extrapolation
                                           10     TL-NoPE                                              1.0
                                           9      TL-RoPE                                              0.9
                                           8      TL-ALiBi                                             0.8
                                                  TL-Window
                                           7      TL-Window-RoPE                                       0.7
                                          PPL6    TL-Window-80                                         0.6
                                           5                                                           0.5     TL-NoPE
                                                                                                               TL-RoPE
                                           4                                                           0.4     TL-ALiBi
                                           3                                                           0.3     TL-Window
                                                                                                      Maximum cosine similarityTL-Window-RoPE
                                           2                                                           0.2     TL-Window-80
                                               0       2000      4000     6000      8000                   0       2000      4000      6000     8000
                                                              Position id                                                  Position id
                                  Figure 4: Left: The average PPL across positions during direct extrapolation. Right: The maximum
                                  cosine similarity between positional vectors within and beyond context window during extrapolation.
                                  Effect of Positional Vectors on Attention Sinks                 Previous work has found that the initial tokens
                                  will be assigned high attention scores, called “attention sinks” [15], which can be clearly observed
                                  in Figure 3. However, once the positional vector or positional basis is removed from the keys and
                                  queries, the attention scores between initial tokens and other tokens drop significantly for TL-NoPE
                                  and TL-RoPE. This finding suggests that the presence of attention sinks is likely attributed to the
                                  inherent positional information in the positional vectors of initial tokens.
                                  Effect of Positional Vectors on Long-term Decay                    For long texts, the attention scores of LLMs
                                  often exhibit a long-term decay pattern, which means that the score decreases as the relative distance
                                  between tokens increases [7, 6]. However, as shown in Figure 3, when removing the positional
                                  vector or positional basis, TL-NoPE fails to exhibit long-term decay. Even with explicit relative
                                  positional encoding, the distribution of attention scores in TL-RoPE tends to be smooth after removing
                                  decomposedpositional vectors. Therefore, positional vectors also play a crucial role in the long-
                                  termdecaypropertyofattention scores.
                                  3.3    Effect of Positional Vectors beyond Context Window
                                  Typically, when dealing with texts that exceed the context window, there are two lines of research,
                                  i.e., direct extrapolation and context window extension. In this section, we aim to investigate the
                                  change of positional vectors in these two methods for revealing their effectiveness.
                                  3.3.1    Direct Extrapolation
                                  Relationship Between Positional Vectors and Length Extrapolation Ability                               To examine the
                                  impact of positional vectors in direct extrapolation, we reuse the trained model variants in Table 1
                                  to perform inference on samples consisting of 8192 tokens. Further, we analyze the change in PPL
                                  score and the maximum cosine similarity between positional vectors within and beyond the context
                                  window. As shown in Figure 4 Left, only TL-Window-RoPE and TL-Window-80 demonstrate the
                                  length extrapolation ability, maintaining stable PPL across longer texts. These models can preserve
                                  the consistency of positional vectors both within and beyond the context window (high similarity in
                                  Figure 4 Right). Conversely, the rest models, including those with extrapolated positional encodings
                                  or window attention (e.g., TL-ALiBi), struggle to generalize to longer contexts. Notably, these
                                  models exhibit rapid changes in positional vectors (beyond 2048), diverging from the distributions
                                  observed within the context window. Thus, our findings underscore the critical role of the stability
                                  of positional vectors in enhancing the capability for length extrapolation.
                                  EffectofOODPositionalVectors                 Beyondthecontextwindow,positionvectorsarenotencountered
                                  during training and are out-of-distribution from those vectors within the context window. To explore
                                  whether OOD positional vector is a key factor in performance degradation, we select TL-NoPE
                                  for evaluation, which does not use explicit positional encodings. First, we compare the attention
                                  distribution within and beyond the context window. Figure 5 shows the attention map and scores
                                  between initial and rest tokens by averaging all heads of the 5-th layer (similar results in other layers).
                                  Onceexceeding the context window (T = 2048), the attention distribution in these positions changes
                                  sharply, losing the characteristics of attention sinks and long-term decay. Since these properties
                                                                                              6
