                         bottleneck in making progress on ARC-AGI using purely neural approaches that predict
                         outputs pixel by pixel.
                   2  Related Work
                   Early approaches to program synthesis focused on fully symbolic methods for LISP program con-
                   struction, such as Summers [1977], Biermann [1978], which aimed to infer LISP programs from
                   examples using symbolic reasoning. Such approaches were extended to domains such as string
                   programming[Gulwani, 2011] with applications to Microsoft Excel spreadsheets. However, these
                   methods were limited in their scalability and adaptability to more complex domains, often relying on
                   enumerative search with domain-specific heuristics to reduce the search space [Albarghouthi et al.,
                   2013, Osera and Zdancewic, 2015]. With the advent and successes of Deep Learning [LeCun et al.,
                   2015], there has been a trend to either train fully end-to-end neural networks for program synthesis
                   or combine neural networks with symbolic methods. Neural programmers-interpreters [Reed and
                   DeFreitas, 2015] proposed a full end-to-end recurrent neural network approach. They found that
                   such architecture could learn 21 different programs using the same core inference module, inferring
                   programs to execute at test time. Our model extends such approaches by learning a latent program
                   representation instead of learning only a fixed discrete set of programs. We also do not require
                   program IDs during training, which are inferred by our encoder during training. A popular neuro-
                   symbolic approach called DreamCoder [Ellis et al., 2021] tackles the limitations of a domain-specific
                   language (DSL) by building up a library of useful programs that can be reused progressively making
                   search more efficient by being able to search over higher-level programs. LPN works by predicting
                   outputs from specifications via the latent space, and is therefore distinct from methods leveraging
                   DSLstosynthesize programs that map inputs to outputs. By directly predicting outputs, LPN does
                   not restrict the types of programs we can execute, which is a consequence of using a DSL.
                   Recently, much of the work in neural program synthesis has shifted toward leveraging pre-trained
                   large language models (LLM). These approaches are significantly different from ours in terms of
                   what priors come into the model since these LLMs are pre-trained on data that likely have non-zero
                   mutual information with program synthesis problems, allowing them to tighten any generalization
                   gap. This branch of approaches aims to improve generalization by reducing the required gap between
                   train and test, by expanding the dataset directly or via a pre-trained model. CodeIt [Butt et al.,
                   2024] iteratively samples programs from a pre-trained CodeT5 [Wang et al., 2023] model, performs
                   hindsight relabeling, and then fine-tunes the model, helping tackle the sparsity of rewards in program
                   synthesis. Kim et al. [2024] create ARCLE an environment consisting of fixed actions according to
                   core knowledgepriorsinwhichReinforcementLearningcanbeperformed. Thisisnotasscalableand
                   relies on human-extracted primitives for training and restricts the space of possible programs that can
                   beexecutedtosolveARCtasks. Gendronetal.[2023]investigatethezero-shotperformanceofLLMs
                   on various reasoning tasks, including ARC-AGI, showing limited but non-zero performance without
                   fine-tuning, which suggests some degree of generalization. Mitchell et al. [2023] report comparable
                   findings on Concept-ARC [Moskvichev et al., 2023]. Li et al. [2024a] investigate distinct induction
                   and transduction methods using LLM-based synthetic datasets for training. However, following a
                   trend with ARC-based research, they evaluate on the public evaluation dataset instead of the hidden
                   test dataset. Since they leverage synthetic LLM-based programs and human-crafted prompts, there is
                   a strong possibility for data leakage. LPN is classified as an induction-based approach since it tries to
                   find a latent program that explains each of the input-output pairs in the specification to then predict
                   the test input.
                   Atraining approach that is most similar to LPN is Kolev et al. [2020], one of the few attempts to learn
                   ARCprogramsynthesisend-to-end using a neural approach without a DSL or pre-trained language
                   models. They use an autoencoder to learn grid embeddings, embedding the entire instruction set in
                   onesteptopredict the output grid directly using a transformer [Vaswani et al., 2017] architecture, also
                   makinguseofspectral norm regularization [Yoshida and Miyato, 2017]. There are still significant
                   differences in architecture as they do not learn a latent space of programs that can then be used for
                   search, instead opting to directly predict the output from the specification. Such approaches do not
                   leverage test-time adaptation, likely struggle to generalize across varying specification sizes, and
                   require careful architectures to be permutation invariant neither of which are problems present in the
                   LPNarchitecture.
                                                    3
