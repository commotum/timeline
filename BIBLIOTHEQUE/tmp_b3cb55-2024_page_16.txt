                               References
                                 [1]   FrançoisChollet.“OntheMeasureofIntelligence”.In:CoRR abs/1911.01547
                                       (2019). arXiv: 1911.01547. url: http://arxiv.org/abs/1911.
                                       01547.
                                 [2]   Icecuber / top-quarks. Code for 1st place solution to Kaggle’s Ab-
                                       straction and Reasoning Challenge. Accessed: 2024-11-11. 2024. url:
                                       https://github.com/top-quarks/ARC-solution.
                                 [3]   Wenhao Li et al. Tackling the Abstraction and Reasoning Corpus with
                                       Vision Transformers: the Importance of 2D Representation, Positions,
                                       and Objects. 2024. arXiv: 2410.06405 [cs.CV]. url: https://arxiv.
                                       org/abs/2410.06405.
                                 [4]   Abhimanyu Dubey et al. “The Llama 3 Herd of Models”. In: CoRR
                                       abs/2407.21783 (2024). doi: 10.48550/ARXIV.2407.21783. arXiv:
                                       2407.21783. url: https://doi.org/10.48550/arXiv.2407.21783.
                                 [5]   Sharath Turuvekere Sreenivas et al. LLM Pruning and Distillation in
                                       Practice: The Minitron Approach. 2024. arXiv: 2408.11796 [cs.CL].
                                       url: https://arxiv.org/abs/2408.11796.
                                 [6]   Zeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part
                                       3.2, Knowledge Manipulation. 2024. arXiv: 2309.14402 [cs.CL]. url:
                                       https://arxiv.org/abs/2309.14402.
                                 [7]   Zeyuan Allen-Zhu and Yuanzhi Li. “Physics of Language Models: Part
                                       3.1, KnowledgeStorageandExtraction”.In:ArXiv e-prints abs/2309.14316
                                       (Sept. 2023). Full version available at http://arxiv.org/abs/2309.
                                       14316.
                                 [8]   Aaditya K. Singh and DJ Strouse. Tokenization counts: the impact of
                                       tokenization on arithmetic in frontier LLMs. 2024. arXiv: 2402.14903
                                       [cs.CL]. url: https://arxiv.org/abs/2402.14903.
                                 [9]   Kaj Bostrom and Greg Durrett. Byte Pair Encoding is Suboptimal for
                                       Language Model Pretraining. 2020. arXiv: 2004.03720 [cs.CL]. url:
                                       https://arxiv.org/abs/2004.03720.
                               [10]    Kaiser Sun et al. Tokenization Consistency Matters for Generative
                                       Models on Extractive NLP Tasks. 2023. arXiv: 2212.09912 [cs.CL].
                                       url: https://arxiv.org/abs/2212.09912.
                               [11]    Michael Hodel. Addressing the Abstraction and Reasoning Corpus via
                                       Procedural Example Generation. 2024. arXiv: 2404.07353 [cs.LG].
                                       url: https://arxiv.org/abs/2404.07353.
                               [12]    Arsenii Moskvichev, Victor Vikram Odouard, and Melanie Mitchell.
                                       “The ConceptARC Benchmark: Evaluating Understanding and Gen-
                                       eralization in the ARC Domain”. In: Trans. Mach. Learn. Res. 2023
                                       (2023). url: https://openreview.net/forum?id=8ykyGbtt2q.
                               [13]    Wen-DingLietal.Combining Induction and Transduction for Abstract
                                       Reasoning. 2024. arXiv: 2411.02272 [cs.LG]. url: https://arxiv.
                                       org/abs/2411.02272.
                                                                             16
