# Generative Pretraining from Pixels (2020)
Source: 70912d-2020.pdf

## Core reasons
- Applies a sequence Transformer to model images by predicting pixels, extending Transformer use from language tokens to image data.
- Reshapes 2D images into a 1D sequence for Transformer processing, which is the key adaptation to a higher-dimensional domain.

## Evidence extracts
- "Figure 1. An overview of our approach. First, we pre-process raw images by resizing to a low resolution and reshaping into a 1D sequence." (p. 2)
- "We also apply the sequence Transformer architecture to predict pixels instead of language tokens." (p. 2)

## Classification
Class name: Increasing Transformer's Dimensions
Class code: 2

$$
\boxed{2}
$$
