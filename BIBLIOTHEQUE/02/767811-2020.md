# SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks (2020)
Source: 767811-2020.pdf

## Core reasons
- The paper introduces an attention-based Transformer variant designed for 3D point cloud and graph data, which is an adaptation of Transformers to a higher-dimensional domain.
- It targets robustness/equivariance to 3D rotations and translations of point clouds, reflecting a structural change for 3D spatial inputs rather than a positional encoding tweak.

## Evidence extracts
- "a self-attention mechanism speciﬁcally for 3D point cloud and graph data" (Section 1 Introduction)
- "Wehavepresented an attention-based neural architecture designed speciﬁcally for point cloud data." (Section 5 Conclusion)

## Classification
Class name: Increasing Transformer's Dimensions
Class code: 2

$$
\boxed{2}
$$
