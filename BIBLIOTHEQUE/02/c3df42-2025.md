# LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias (2025)
Source: c3df42-2025.pdf

## Core reasons
- Proposes a transformer-based model for novel view synthesis, targeting 3D/vision data rather than 1D sequences.
- Describes an architecture that converts 2D image inputs into 1D latent tokens representing a 3D scene, indicating a transformer adaptation for higher-dimensional inputs.

## Evidence extracts
- "In this work, we presented the Large View Synthesis Model (LVSM), a transformer-based approach designed to minimize 3D inductive biases for scalable and generalizable novel view synthesis." (p. 10)
- "Theencoder-decoder model transforms 2D image inputs into a fixed-length set of 1D latent tokens, which serve as a compressed representation of the 3D scene. This approach simplifies the decoder," (p. 10)

## Classification
Class name: Increasing Transformer's Dimensions
Class code: 2

$$
\boxed{2}
$$
