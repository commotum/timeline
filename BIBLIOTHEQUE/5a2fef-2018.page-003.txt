                                                                                          Output                    MULTI-HEAD DOT PRODUCT ATTENTION
                                                     CORE                                                                                            query
                                           Prev.            A       +               +                     Memory                                      key                                 Updated
                                           Memory                         MLP                                                                                                             Memory
                                                                Residual         Residual                                                            value
                                                                                          *        Next
                                                                                   Apply gating    Memory Input
                                                          Input        ***computation of gates not depicted
                                                                       (a)                                                                          (b)
                                                  Compute attention weights         Normalize weights with row-wise softmax Compute weighted average of values Return updated memory
                                                  Queries               Keys                 Weights     Normalized Weights           Weights                Values           Updated Memory
                                                               .
                                                                                                                                                     .
                                                                                                                 (c)
                                           Figure 1: Relational Memory Core. (a) The RMC receives a previous memory matrix and input
                                          vector as inputs, which are passed to the MHDPA module labeled with an “A”. (b). Linear projections
                                           are computed for each memory slot, and input vector, using row-wise shared weights Wq for the
                                           queries, Wk for the keys, and Wv for the values. (c) The queries, keys, and values are then compiled
                                           into matrices and softmax(QKT)V is computed. The output of this computation is a new memory
                                          where information is blended across memories based on their attention weights. An MLP is applied
                                           row-wise to the output of the MHDPA module (a), and the resultant memory matrix is gated, and
                                           passed on as the core output or next memory state.
                                           self-attention. Using MHDPA, each memory will attend over all of the other memories, and will
                                           update its content based on the attended information.
                                           First, a simple linear projection is used to construct queries (Q = MWq), keys (K = MWk), and
                                          values (V = MWv) for each memory (i.e. row m ) in matrix M. Next, we use the queries, Q, to
                                                                                                                          i
                                           perform a scaled dot-product attention over the keys, K. The returned scalars can be put through a
                                           softmax-function to produce a set of weights, which can then be used to return a weighted average
                                                                                                                QKT
                                           of values from V as A(Q,K,V) = softmax                                   √dk       V, where dk is the dimensionality of the key
                                          vectors used as a scaling factor. Equivalently:
                                                                                           MWq(MWk)T                              v                           q      k       v
                                                            Aθ(M)=softmax                              √                    MW , whereθ =(W ,W ,W )                                             (1)
                                                                                                          dk
                                                                                                                         f
                                          The output of Aθ(M), which we will denote as M, is a matrix with the same dimensionality as
                                                  f
                                           M. M canbeinterpreted as a proposed update to M, with each me comprising information from
                                                                                                                                                    i
                                           memories m . Thus, in one step of attention each memory is updated with information originating
                                                              j
                                           from other memories, and it is up to the model to learn (via parameters Wq, Wk, and Wv) how to
                                           shuttle information from memory to memory.
                                          As implied by the name, MHDPA uses multiple heads. We implement this producing h sets of
                                           queries, keys, and values, using unique parameters to compute a linear projection from the original
                                           memoryfor each head h. We then independently apply an attention operation for each head. For
                                           example, if M is an N ×F dimensional matrix and we employ two attention heads, then we compute
                                           g                             g                                  g g
                                           M1=Aθ(M)andM2=Aφ(M),whereM1andM2areN×F/2matrices,θandφdenoteunique
                                                                                                                                                                                    g g
                                                                                                                                                                          f             1        2
                                           parameters for the linear projections to produce the queries, keys, and values, and M = [M                                                      : M ],
                                          where [:] denotes column-wise concatenation. Intuitively, heads could be useful for letting a memory
                                           share different information, to different targets, using each head.
                                                                                                                       3
