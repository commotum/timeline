                                                                                                                                              a   a   a    a       p   p    p   p
                                             a   a    a   a        p    p
                                                                                                                                               00  01  02   03      0   1    2   3
                                              00  01   02  03       00   01
                                                                                                                                                                                          seg      seg
                                                                                                         seg      seg
                                                                                                                                                                                            00       01
                                                                                                            00       01
                                                                                                                                              a   a   a    a       p   p    p   p
                                             a   a    a   a        p    p        p    p   p   p
                                                                                                                                               10  11  12   13      -1  0    1   2
                                              10  11   12  13       10   11
                                                                                  00   01  02  03
                                                                                                                                                                +                    +
                                                               +             x                      +
                                                                                                                                              a   a   a    a       p   p    p   p
                                             a   a    a   a        p    p        p    p   p   p
                                                                                                                                                                    -2  -1   0   1
                                                                                                                                               20  21  22   23
                                              20  21   22  23       20   21       10   11  12  13
                                                                                                                                                                                          seg      seg
                                                                                                         seg      seg
                                                                                                                                                                                            10       11
                                                                                                            10       11
                                                                                                                                              a   a   a    a       p   p    p   p
                                             a   a    a   a        p    p
                                                                                                                                               30  31  32   33      -3  -2   -1  0
                                              30  31   32  33       30   31
                                                                                                                                                                    Relative Position       Segment
                                                                                                                                                   Token 
                                                  Token                    Absolute Position                Segment
                                                                        (a) DIET-ABS                                                                            (b) DIET-REL
                              Figure 3: Proposed efﬁcient approach to include position and segment encoding by adding them directly to the
                              token attention matrix per-head. Left ﬁgure shows how we encode absolute positional attention. Right ﬁgure
                              represents relative positional attention.
                                                                               Mode            Shawetal. (2018)                Keetal. (2020)              DIET-ABS             DIET-REL
                                                      BERTBASE               Training                   +13%                           +1%                     +0%                  +0%
                                                      BERTBASE              Inference                   +33%                          +19%                     +0%                  +0%
                                                      BERTSMALL              Training                   +24%                           +4%                     +0%                  +0%
                                                      BERTSMALL             Inference                   +65%                          +27%                     +1%                  +0%
                             Table 1: Pre-training and inference time of Transformers with different position encoding methods in comparison
                              to the baseline BERT model on TPU v2. We observe that simplicity of the DIET-REL and DIET-ABS result in
                              substantial gains in both training and inference time. We notice even more speedup for the smaller BERT
                                                                                                                                                                                                              SMALL
                              model compared to BERT                            .
                                                                        BASE
                              forward pass which corresponds to costs of using                                                conduct experiments in three different settings to
                              such models in real systems.                                                                    cover a wide range of use cases. First, we examine
                              3.4       Application to Long-range Transformers                                                the results of a popular transfer learning approach
                                                                                                                              from masked-LM pretraining to the end tasks in
                             Another advantage of our propose approaches is                                                   GLUE (Devlin et al., 2018). Second, we study
                              they easily extend to long range Transformer mod-                                               zero-shot cross-lingual transferability of the mul-
                              els. For long sequence inputs, Transformers suf-                                                tilingual pretrained models (Hu et al., 2020) to
                              fer from quadratic dependence of computational                                                  classiﬁcation and question answering tasks in the
                              complexity with respect to the sequence length. A                                               XTREMEbenchmark(Huetal.,2020). Lastly, we
                              class of methods reduce this complexity by using a                                              consider training Transformer models from scratch
                              low rank projection of the input sequence for atten-                                            for machine translation.
                              tion computation (Wang et al., 2020; Choromanski                                                     Wecomparethefollowingpositional encoding
                              et al., 2021; Dai et al., 2020). However, such meth-                                            approaches - absolute positional embedding (De-
                              ods use the default input position encodings, and                                               vlin et al., 2018), relative positional embedding
                              there has not been much work in incorporating po-                                               (Shaw et al., 2018), combined absolute and rela-
                              sition information per-head without introducing the                                             tive positional encoding (Ke et al., 2020), relative
                              quadratic computation complexity on the input se-                                               scalar approach (Raffel et al., 2020), our proposed
                              quencelength. Weillustrate the applicability of our                                             DIET-ABS and DIET-REL per-head positional en-
                              methods to such settings by applying DIET-ABS to                                                coding approaches. We denote the methods that
                              Linformer (Wang et al., 2020), which projects the                                               add position/segment information directly to input
                              attention key and value matrices to a lower dimen-                                              tokenembeddingswithinput,andmethodsthatadd
                              sion k during attention computation.                                                            position/segment information directly in attention
                              DIET-ABSLIN                    The proposed method can be                                       layer with per-head. For complete experimental
                             written as:                                                                                      setup, see Appendix A.
                                        LIN                                                     > √
                                     A =(X W )((EX) W ) / d
                                        i,j               i:     Q                j:       K                      (4)         4.1       English Transfer Learning Results
                                                               >
                                               +(P P ) ,
                                                         Q K i,j                                                              Datasets and Model                          For pre-training, we use
                                                      k×n                     n×d                     k×d
                             where E ∈ R                    , P       ∈R ,P ∈R .
                                                                  Q                       K                                   English Wikipedia and Books datasets (Devlin
                              4      Experiments                                                                              et al., 2018).              For Finetuning tasks we use the
                                                                                                                              datasets from the GLUE benchmark (Wang et al.,
                              In this section, we present our experimental results                                            2019). We apply sub-word tokenization on raw
                              comparing different position and segment encod-                                                 text data using WordPiece (Wu et al., 2016) with a
                              ing approaches discussed in earlier sections. We                                                30,000 token vocabulary.
                                                                                                                       2978
