                Published as a conference paper at ICLR 2021
                    Cifar10
                   Cifar100
                   Imagenet
                  Finetuning
                    SVHN 
                   F-MNIST
                  Noisy Cifar
                          0    20   40
                           Error reduction (%)
                Figure 1: (left) Error rate reduction obtained by switching to SAM. Each point is a different dataset
                / model / data augmentation. (middle) A sharp minimum to which a ResNet trained with SGD
                converged. (right) A wide minimum to which the same ResNet trained with SAM converged.
                batch normalization (Ioffe & Szegedy, 2015), stochastic depth (Huang et al., 2016), data augmenta-
                tion (Cubuk et al., 2018), and mixed sample augmentations (Zhang et al., 2017; Harris et al., 2020).
                Theconnectionbetweenthegeometryofthelosslandscape—inparticular,theﬂatnessofminima—
                and generalization has been studied extensively from both theoretical and empirical perspectives
                (Shirish Keskar et al., 2016; Dziugaite & Roy, 2017; Jiang et al., 2019). While this connection
                has held the promise of enabling new approaches to model training that yield better generalization,
                practical efﬁcient algorithms that speciﬁcally seek out ﬂatter minima and furthermore effectively
                improve generalization on a range of state-of-the-art models have thus far been elusive (e.g., see
                (Chaudhari et al., 2016; Izmailov et al., 2018); we include a more detailed discussion of prior work
                in Section 5).
                Wepresenthereanewefﬁcient,scalable, and effective approach to improving model generalization
                ability that directly leverages the geometry of the loss landscape and its connection to generaliza-
                tion, and is powerfully complementary to existing techniques. In particular, we make the following
                contributions:
                    • We introduce Sharpness-Aware Minimization (SAM), a novel procedure that improves
                      model generalization by simultaneously minimizing loss value and loss sharpness. SAM
                      functions by seeking parameters that lie in neighborhoods having uniformly low loss value
                      (rather than parametersthatonlythemselveshavelowlossvalue,asillustratedinthemiddle
                      and righthand images of Figure 1), and can be implemented efﬁciently and easily.
                    • We show via a rigorous empirical study that using SAM improves model generalization
                      ability across a range of widely studied computer vision tasks (e.g., CIFAR-{10, 100},
                      ImageNet,ﬁnetuningtasks)andmodels,assummarizedinthelefthandplotofFigure1. For
                      example, applying SAMyieldsnovelstate-of-the-art performance for a number of already-
                      intensely-studied tasks, such as ImageNet, CIFAR-{10, 100}, SVHN, Fashion-MNIST,
                      and the standard set of image classiﬁcation ﬁnetuning tasks (e.g., Flowers, Stanford Cars,
                      Oxford Pets, etc).
                    • WeshowthatSAMfurthermoreprovidesrobustnesstolabelnoiseonparwiththatprovided
                      bystate-of-the-art procedures that speciﬁcally target learning with noisy labels.
                    • Through the lens provided by SAM, we further elucidate the connection between loss
                      sharpness and generalization by surfacing a promising new notion of sharpness, which
                      wetermm-sharpness.
                Section 2 below derives the SAM procedure and presents the resulting algorithm in full detail. Sec-
                tion 3 evaluates SAM empirically, and Section 4 further analyzes the connection between loss sharp-
                ness and generalization through the lens of SAM. Finally, we conclude with an overview of related
                workandadiscussion of conclusions and future work in Sections 5 and 6, respectively.
                                             2
