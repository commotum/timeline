                                      Effect of Image Fusion: Row 1 is a LiDAR-
                                      only model which does not utilize image input                         #       PF   CAF  DE   PC      PAT LSTQ            PTQ PQ
                                      in any way. This achieves 59.7 PAT which is                           1.               ✓ ✓           59.7      64.3      60.8     63.6
                                      significantly worse than the final model’s 66.1.                      2.    ✓          ✓ ✓           61.8      65.2      64.3     67.6
                                      This shows that using image information yields                        3.         ✓ ✓ ✓               63.2      66.1      63.1     66.3
                                      significant performance improvements. Row 2                           4.    ✓ ✓             ✓        64.1      66.4      65.7     69.1
                                      utilizes point-level fusion (Sec. 3.1), but does                      5.    ✓ ✓ ✓                    64.6      66.7      66.0     69.4
                                      not apply cross-attention to image features in                        6.    ✓ ✓ ✓ ✓                  66.1      67.4      66.2     69.9
                                      the decoder (Sec. 3.2). This setting achieves                       Table4: AblationresultsonnuScenesvalset. PF:
                                      61.8 PAT which is better than the LiDAR-only                        Point Fusion, CAF: Cross-attention Fusion, DE:
                                      setting (59.7), but still much worse than the fi-                   Depthencodings,PC:Pseudo-camerafeatureloss.
                                      nal model (66.1). Row 3 tests the opposite con-
                                      figuration: the decoder includes cross-attention
                                      to image features, but no point-level fusion is applied. This yields 63.2 PAT which is slightly higher
                                      than row 2 (61.8) but worse than the final setting (66.1). We conclude that while both types of fusion
                                      are beneficial in a standalone setting, combining them yields a larger improvement.
                                      Effect of Depth Encodings: As discussed in Sec. 3.2, our positional encodings contain a depth
                                      component which is calculated by applying sine/cosine activations with multiple frequencies to the
                                      depth value of each voxel feature. Row 4 omits this component and instead only uses Fourier
                                      encodings based on the xyz coordinates. This setting yields 64.1 PAT which is lower than the full
                                      model (66.1), thus showing that explicitly encoding depth is beneficial.
                                      Effect of Pseudo-camera Feature Loss: Recall from Sec. 3.4 that we supervise pseudo-camera
                                      features for point fusion with an L2 regression loss. Row 5 shows that without this loss the PAT
                                      reduces from 66.1 to 64.6. Other metrics also reduce, though to a lesser extent.
                                      5     Limitations
                                      Our method performs less effectively on SemanticKITTI compared to nuScenes, particularly in
                                      crowded scenes with several objects. In addition to lower camera image coverage, this is due to the
                                      limited numberofmovingactorsintheSemanticKITTItrainingsetwhich,onaverage,containsonly
                                      0.63 pedestrians and 0.18 riders per frame. Existing LiDAR-only methods [2, 20, 55] overcome this
                                      by using instance cutmix augmentation which involves randomly inserting LiDAR scan cutouts of
                                      actors into training scenes. Doing the same in a multimodal setting is, however, non-trivial since
                                      it would require the camera images to also be augmented accordingly. Consequently, a promising
                                      future direction is to develop more effective augmentation techniques for multimodal training.
                                      Our tracking quality is generally good for vehicles, but is comparatively worse for smaller object
                                      classes e.g. bicycle, pedestrian (see class-wise results in the supplementary material), and although
                                      the TAM is more effective than mask IoU, the improvement plateaus at Thist = 4 (i.e. 2s into the
                                      past). Anotherareaforfutureworkthusinvolvesimprovingthetrackingmechanismtohandlelonger
                                      time horizons and challenging object classes.
                                      6     Conclusion
                                      Weproposed a novel, online approach for 4D panoptic segmentation which leverages both LiDAR
                                      scansandRGBimages. Weemployatransformer-basedPanopticDecoderwhichsegmentssemantic
                                      classes and object tracklets by attending to scenes features from both modalities. Furthermore, our
                                      Tracklet Association Module (TAM) accurately associates tracklets over time in a learned fashion
                                      by reasoning over spatial and appearance cues. 4D-Former achieves state-of-the-art results on the
                                      nuScenesandSemanticKITTIbenchmarks,thusdemonstratingitsefficacyonlarge-scale,real-world
                                      data. We hope our work will spur advancement in SDV perception systems, and encourage other
                                      researchers to develop multi-sensor methods for further improvement.
                                                                                                         8
