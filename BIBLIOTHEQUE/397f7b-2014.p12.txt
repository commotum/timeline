                             Published as a conference paper at ICLR 2015
                             A MODELARCHITECTURE
                             A.1    ARCHITECTURAL CHOICES
                             TheproposedschemeinSection3isageneralframeworkwhereonecanfreelydeﬁne,forinstance,
                             the activation functions f of recurrent neural networks (RNN) and the alignment model a. Here, we
                             describe the choices we made for the experiments in this paper.
                             A.1.1    RECURRENT NEURAL NETWORK
                             For the activation function f of an RNN, we use the gated hidden unit recently proposed by Cho
                             et al. (2014a). The gated hidden unit is an alternative to the conventional simple units such as an
                             element-wise tanh. This gated unit is similar to a long short-term memory (LSTM) unit proposed
                             earlier by Hochreiter and Schmidhuber (1997), sharing with it the ability to better model and learn
                             long-term dependencies. This is made possible by having computation paths in the unfolded RNN
                             for which the product of derivatives is close to 1. These paths allow gradients to ﬂow backward
                             easily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al., 1994;
                             Pascanu et al., 2013a). It is therefore possible to use LSTM units instead of the gated hidden unit
                             described here, as was done in a similar context by Sutskever et al. (2014).
                             Thenewstates oftheRNNemployingngatedhiddenunits8 iscomputedby
                                             i
                                                      s =f(s       , y   , c ) = (1 − z ) ◦ s     +z ◦s˜,
                                                        i       i−1   i−1   i           i    i−1     i    i
                             where ◦ is an element-wise multiplication, and z is the output of the update gates (see below). The
                                                                                i
                             proposed updated state s˜ is computed by
                                                       i
                                                        s˜ = tanh(We(y         ) +U[r ◦s       ] + Cc ),
                                                          i                i−1         i    i−1        i
                             where e(y     ) ∈ Rm is an m-dimensional embedding of a word y             , and r is the output of the
                                        i−1                                                         i−1        i
                             reset gates (see below). When y is represented as a 1-of-K vector, e(y ) is simply a column of an
                                                               i                                        i
                             embedding matrix E ∈ Rm×K. Whenever possible, we omit bias terms to make the equations less
                             cluttered.
                             Theupdategates z allow each hidden unit to maintain its previous activation, and the reset gates r
                                                 i                                                                                  i
                             control how much and what information from the previous state should be reset. We compute them
                             by
                                                            z =σ(W e(y         ) +U s       +C c ),
                                                             i         z    i−1      z i−1       z i
                                                            r =σ(W e(y         ) +U s       +C c ),
                                                             i         r    i−1      r i−1      r i
                             where σ(·) is a logistic sigmoid function.
                             At each step of the decoder, we compute the output probability (Eq. (4)) as a multi-layered func-
                             tion (Pascanu et al., 2014). We use a single hidden layer of maxout units (Goodfellow et al., 2013)
                             and normalize the output probabilities (one for each word) with a softmax function (see Eq. (6)).
                             A.1.2    ALIGNMENT MODEL
                             Thealignment model should be designed considering that the model needs to be evaluated T × T
                                                                                                                              x     y
                             times for each sentence pair of lengths T and T . In order to reduce computation, we use a single-
                                                                        x       y
                             layer multilayer perceptron such that
                                                          a(s     , h ) = v> tanh(W s        +U h ),
                                                              i−1   j      a           a i−1      a j
                             where W ∈ Rn×n,U ∈ Rn×2n and v ∈ Rn are the weight matrices. Since U h does not
                                       a             a                   a                                             a j
                             depend on i, we can pre-compute it in advance to minimize the computational cost.
                                8 Here, we show the formula of the decoder. The same formula can be used in the encoder by simply
                             ignoring the context vector ci and the related terms.
                                                                                12
