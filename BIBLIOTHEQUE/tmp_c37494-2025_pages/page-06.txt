Method ABF ABF HARPE
Single-Stage Multi-Stage (Ours)
MMLU 40.87 41.10 40.74
Hellaswag 77.33 77.83 77.99
ARC-c 52.82 52.65 52.73
PIQA 78.56 78.56 78.56
TriviaQA 62.39 63.29 63.72
Avg. 62.39 62.69 62.75

Table 5: Short-Context Benchmark Results: Evalu-
ation Results of the Top 3 Long-Context-Performance
Models on 5 Short-Context Datasets.

Short-Context Benchmarks assess whether
short-context capabilities are preserved during
long-context training. We include five widely used
short-context evaluation datasets: 5-shot MMLU
(Hendrycks et al., 2020), 10-shot Hellaswag
(Zellers et al., 2019), 25-shot ARC-Challenge
(Clark et al., 2018), 0-shot PiQA (Bisk et al., 2019),
and 5-shot TriviaQA (Joshi et al., 2017).

4.4 Training Configuration

For continual pretraining, we follow the configu-
rations outlined in (Fu et al., 2024b), utilizing the
upsampling dataset from (Yaofu, 2023b). We em-
ploy the Llama2-7B-Base model as the pre-trained
backbone, with a learning rate of 2e—° and AdamW
optimizer settings of 3; = 0.9 and 82 = 0.95. All
models were continually pre-trained with 6B to-
kens using these consistent settings.

5 Experimental Results

5.1 HARPE vs. Baseline Systems

We utilize HARPE to conduct a comparative evalu-
ation with the five long-context methods outlined
in Sec. 4.1, employing three evaluation metrics as
detailed in Sec. 4.3.

First, to evaluate the long context modeling ca-
pability of HARPE, we evaluate HARPE and the
competing methods with the PPL metric. As shown
in Tab. 3, on the tested Proof-pile and GovReport
datasets, our HARPE achieves comparable or even
better results compared to the state-of-the-art multi-
stage methods and various single-stage methods.
This indicates that the proposed HARPE has the
capability to handle long text sequences.

Furthermore, we employ the upgraded Needle-
in-a-Haystack test, as defined in the RULER
benchmark (Hsieh et al., 2024), to evaluate the
long-context relationship capturing performance of

HARPE and its competitors. As shown in Tab. 4,
HARPE significantly outperforms all listed meth-
ods. Notably, HARPE proves more effective than
multi-stage approaches, surpassing the multi-stage
ABF by 5.5%. While typical single-stage methods,
such as YARN and PI, fail as the context length
increases, HARPE successfully extends the effec-
tive context length to 128K tokens. More details on
the NiaH results, including scores for each of the
8 NiaH tasks (e.g., multi-key and multi-value), are
provided in the Appendix Tab. 8. Simultaneously,
we evaluate traditional NiaH tasks at a finer granu-
larity, following the code in (Liu et al., 2024b). As
shown in Fig. 2, HARPE achieves a 100% accuracy
rate across various lengths within 128k tokens.

We also evaluate HARPE on the short-context
benchmarks. Results in Tab. 5 show that HARPE
also yields comparable or even slightly better per-
formance than competing methods in terms of av-
erage accuracy across 5 short-context tasks.

5.2 Study of Various Base Schemes

In this section, we evaluate the performance of two
base selection methods for the head-specific RoPE
bases in HARPE: uniform distribution and peak-
valley opposition. For the uniform distribution
method, we conduct two experiments with uniform
ascending and descending intervals to analyze
the impact of different base orders on model perfor-
mance. For the peak-valley opposition method, as
described in algorithm 1 and Eq. (10), we test five
variations with different base strides (10k, 20k,
30k, 40k, 50k) to further explore their effects.

The results of various HARPE configurations,
along with the original LLaMA2 model, on the
upgraded Needle-in-a-Haystack test are presented
in Tab. 6. Under different RoPE base settings,
our HARPE consistently outperforms the original
LLaMA2-7B-Base model. Among the two meth-
ods evaluated, the peak-valley opposition approach
with stride = 30k demonstrates the best perfor-
mance, surpassing the next closest competitor by
1.25%. As a result, we adopt the peak-valley ap-
proach with a stride of 30k for HARPE.

5.3 Comparative Results on RULER
Evaluation

In this section, we evaluate HARPE against vari-
ous open-source pre-trained models on a range of
long-context tasks using the RULER benchmark.
RULER is a comprehensive and widely recognized
standard for long-context evaluation, comprising

4903
