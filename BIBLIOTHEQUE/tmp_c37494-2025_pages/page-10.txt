Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
rico Shippole. 2023. Yarn: Efficient context window
extension of large language models. arXiv preprint
arXiv:2309.00071.

Ofir Press, Noah A Smith, and Mike Lewis. 2021.
Train short, test long: Attention with linear biases
enables input length extrapolation. arXiv preprint
arXiv:2108.12409.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the lim-
its of transfer learning with a unified text-to-text
transformer. Journal of machine learning research,

21(140):1-67.

Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,
Wen Bo, and Yunfeng Liu. 2024a. Roformer: En-
hanced transformer with rotary position embedding.
Neurocomputing, 568:127063.

Zhenpeng Su, Zijia Lin, Xue Bai, Xing Wu,
Yizhe Xiong, Haoran Lian, Guangyuan Ma, Hui
Chen, Guiguang Ding, Wei Zhou, et al. 2024b.
Maskmoe: Boosting token-level learning via rout-
ing mask in mixture-of-experts. arXiv preprint
arXiv:2407.09816.

Together.AI. 2023. Preparing for the era of 32k con-
text: Early learnings and explorations. https: //www.
together.ai/blog/llama-2-7b-32k. Accessed:
2024-08-31.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023a. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need.

Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu,
Xifeng Yan, Jianfeng Gao, and Furu Wei. 2024. Aug-
menting language models with long-term memory.
Advances in Neural Information Processing Systems,

36.

Kaiwen Wei, Xian Sun, Zequn Zhang, Li Jin, Jingyuan
Zhang, Jianwei Lv, and Zhi Guo. 2023a. Implicit
event argument extraction with argument-argument
relational knowledge. IEEE Trans. Knowl. Data
Eng., 35(9):8865-8879.

Kaiwen Wei, Xian Sun, Zequn Zhang, Jingyuan Zhang,
Zhi Guo, and Li Jin. 2021. Trigger is not sufficient:
Exploiting frame-aware knowledge for implicit event
argument extraction. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing, ACL/IJCNLP
2021, (Volume 1: Long Papers), Virtual Event, Au-
gust 1-6, 2021, pages 4672-4682. Association for
Computational Linguistics.

Kaiwen Wei, Yiran Yang, Li Jin, Xian Sun, Zequn
Zhang, Jingyuan Zhang, Xiao Li, Linhao Zhang, Jin-
tao Liu, and Zhi Guo. 2023b. Guide the many-to-
one assignment: Open information extraction via
iou-aware optimal transport. In Proceedings of the
61st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), ACL
2023, Toronto, Canada, July 9-14, 2023, pages 497 1-
4984. Association for Computational Linguistics.

Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang,
and Xing Xie. 2021. Fastformer: Additive at-
tention can be all you need. arXiv preprint
arXiv:2108.09084.

Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,
Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
et al. 2023. Effective long-context scaling of founda-
tion models. arXiv preprint arXiv:2309.16039.

Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zi-
jia Lin, Haoran Lian, Jianwei Niu, and Guiguang
Ding. 2024. Temporal scaling law for large language
models. arXiv preprint arXiv:2404.17785.

Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang,
Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,
Dong Yan, et al. 2023. Baichuan 2: Open large-scale
language models. arXiv preprint arXiv:2309.10305.

Yaofu. 2023a. Llama-2 7b 80k model. https:
//huggingface.co/yaofu/1lama-2-7b-80k/
tree/main. Accessed: 2024-08-31.

Yaofu. 2023b. Slimpajama-per-source-length-upsample.
https: //huggingface.co/datasets/yaofu/
slimpajama-per-source-length-upsample.
Accessed: 2024-09-03.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, et al. 2020. Big bird: Transformers for
longer sequences. Advances in neural information
processing systems, 33:17283-17297.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a

4907
