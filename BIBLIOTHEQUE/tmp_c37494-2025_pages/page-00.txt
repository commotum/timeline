Breaking the Stage Barrier: A Novel Single-Stage Approach to
Long Context Extension for Large Language Models

Haoran Lian'*, Junmin Chen**, Wei Huang“, Yizhe Xiong**, Wenping Hu",
Guiguang Ding’, Hui Chen’, Jianwei Niu '5"', Zijia Lin?’ ,
Fuzheng Zhang’, Di Zhang”

'Beihang University, >Kuaishou Technology, *Tsinghua University, “BUPT,
>Zhongguancun Laboratory, “Zhengzhou University

Correspondence’

Abstract

Recently, Large language models (LLMs) have
revolutionized Natural Language Processing
(NLP). Pretrained LLMs, due to limited train-
ing context size, struggle with handling long
token sequences, limiting their performance on
various downstream tasks. Current solutions
toward long context modeling often employ
multi-stage continual pertaining, which pro-
gressively increases the effective context length
through several continual pretraining stages.
However, those approaches require extensive
manual tuning and human expertise. In this
paper, we introduce a novel single-stage contin-
ual pretraining method, Head-Adaptive Rotary
Position Encoding (HARPE), to equip LLMs
with long context modeling capabilities while
simplifying the training process. Our HARPE
leverages different Rotary Position Encoding
(RoPE) base frequency values across different
attention heads and directly trains LLMs on the
target context length. Extensive experiments
on 4 language modeling benchmarks, including
the latest RULER benchmark, demonstrate that
HARPE excels in understanding and integrat-
ing long-context tasks with single-stage train-
ing, matching and even outperforming existing
multi-stage methods. Our results highlight that
HARPE successfully breaks the stage barrier
for training LLMs with long context modeling
capabilities.

1 Introduction

In recent years, generative Large Language Models
(LLMs) (Brown, 2020; Raffel et al., 2020; Tou-
vron et al., 2023a; Fu et al., 2023; Su et al., 2024b;
Xiong et al., 2024; Lian et al., 2024b,a) have dom-
inated the field of Natural Language Processing,
outperforming traditional task-specific methods on
many tasks, like text summarization (Liu and La-
pata, 2019; Zaheer et al., 2020; Wu et al., 2021),
information extraction (Wei et al., 2021, 2023b,a)

*These authors contributed equally to this work.

: niujianwei @buaa.edu.cn, dinggg @tsinghua.edu.cn, linzijia07 @tsinghua.org.cn

and question answering (Brown, 2020; Raffel et al.,
2020). In the process of utilizing LLMs for down-
stream tasks, it is often necessary for the LLM to
handle long token sequences. For example, when
conducting text summarization with an LLM, the
input sequence may include an entire book (Zhang
et al., 2024a; Karpinska et al., 2024), which con-
tains millions of words. To equip LLMs with the
capability to handle long texts, current methods
typically continually pretrain LLMs on a larger
context window (Xiong et al., 2023; Peng et al.,
2023; Fu et al., 2024a) compared to that in LLM
pretraining. Given that Rotary Position Encoding
(RoPE) (Su et al., 2024a) is the prevailing position
encoding in most LLMs, among those methods, the
mainstream approach is to increase the RoPE base
frequency in the positional encoding during con-
tinual pretraining (Xiong et al., 2023), as studies
have demonstrated that a larger base frequency is
the prerequisite for handling longer text sequences
(Liu et al., 2023; Men et al., 2024).

To achieve a large effective context size, existing
works commonly employ a multi-stage approach,
progressively increasing the context length through
a series of continued pretraining steps. For instance,
Large World Model (Liu et al., 2024c), GLM4-
Chat-1M (ChatGLM, 2024), MiniCPM-2.4B-128K
(Hu et al., 2024) and Llama 3.1 (Dubey et al., 2024)
utilize multi-stage pipelines to reach context win-
dows of 1M and 128k, respectively. This approach
has become the dominant method in the community
for equipping LLMs with long context capabilities.

Our single-stage experiments show that directly
scaling a larger RoPE base in a single stage is less
effective than using multi-stage approaches. This
likely explains why most publicly available mod-
els employ multi-stage ABF (Adjusted Base Fre-
quency) training. We hypothesize that direct scal-
ing to the final training length without intermediate
stages struggles to adapt to increased complexity,
which is better managed through gradual, multi-

4897

Proceedings of the 31st International Conference on Computational Linguistics, pages 4897-4909
January 19-24, 2025. ©2025 Association for Computational Linguistics
