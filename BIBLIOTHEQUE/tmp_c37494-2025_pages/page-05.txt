Depth Percent

Se
- ss
Oe 8
Swe

P Sf oe oo
SPP gO Kos FP KES PS
ws PP Os SF of SS

Pressure Testing "HARPE"
Fact Retrieval Across Context Lengths ("Needle In A HayStack")

44.
100.0

2 °
wd O° a Pay
POSS PS ns FS OO

10

06

4

oo
2 2 SP
wv ~
oe is
SL
ees oe

Token Limit

Figure 2: Traditional Single-Key Needle-in-a-Haystack: the x-axis represents the number of tokens in the test
sample, ranging up to 128k tokens with finer granularity. The y-axis shows the depth of the needle’s position within

the current test sample.

Method 4k 8k 16k 32k 64k =: 128k_—s Avg.
Llama2-7B-Base 90.90 - - - - -
PI 77.56 26.59 16.50 0.00 0.00 0.00 20.11
ABF Single-Stage 92.44 88.78 84.16 78.03 70.81 62.72 79.49ra)
ABF Multi-Stage 95.19 91.72 87.53 78.84 72.78 62.13 81.36,2na)
YaRN 83.88 73.66 64.84 46.53 12.69 0.00 46.93
Self-Extend* 76.47 66.25 58.84 52.16 1.38 0.00 42.52
HARPE(ours) 97.03 96.88 93.72 86.66 79.41 67.19 86.82(1.)

Table 4: Upgraded Needle-in-a- Haystack Tests: Average scores for 8 NiaH tasks at various lengths. Asterisks (*)

denote training-free methods.

4.3, Evaluation Metric

Perplexity (PPL) is evaluated on the Proof-pile
(Zhangir Azerbayev, 2022) and GovReport (Huang
et al., 2021) datasets. Following the setup in Yarn,
for the Proof-pile dataset, we selected samples with
a minimum of 128k tokens and measured perplexity
for token lengths ranging from 2k to 128k in incre-
ments of 2k, averaging the scores for each length.
For the GovReport dataset, we reported the average
PPL scores for samples with a context window of
32k tokens. Evaluations are conducted using the
sliding window method proposed by Press (Press
et al., 2021), with a window size of 256 tokens.

Needle-in-a-Haystack is a task that assesses a

model’s ability to accurately locate and recite a spe-
cific sentence, referred to as the "needle", within
a lengthy document, known as the "haystack". To
provide a more comprehensive evaluation of a
model’s long-context capabilities, we extend this
method, inspired by RULER (Hsieh et al., 2024),
to include multi-key, multi-value and multi-query
scenarios, as well as diverse types of needles and
background documents in each scenario. A multi-
key task involves multiple keys, similar to ’the nee-
dle’, in the background, where the model must
find the target needle among the distractions. In a
multi-value task, multiple needles are inserted in
haystack, and the model earns one point for each
correct needle found.

4902

Score
