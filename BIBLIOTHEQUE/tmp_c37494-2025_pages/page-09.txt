Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Han-
naneh Hajishirzi, Yoon Kim, and Hao Peng. 2024b.
Data engineering for scaling language models to 128k
context.

Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chen-
hui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Han-
lin Zhao, Hanyu Lai, et al. 2024. Chatglm: A family
of large language models from glm-130b to glm-4 all
tools. arXiv preprint arXiv:2406.12793.

Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang
Song, LivYiBo Liu YiBo, Qianguosun Qianguosun,
Yuxin Liang, Hao Wang, Enming Zhang, and Jiax-
ing Zhang. 2024. Never lost in the middle: Master-
ing long-context question answering with position-
agnostic decompositional training. In Proceedings
of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 13628-13642.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2020. Measuring massive multitask language under-
standing.

Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shan-
tanu Acharya, Dima Rekesh, Fei Jia, and Boris Gins-
burg. 2024. Ruler: What’s the real context size of
your long-context language models? arXiv preprint
arXiv:2404.06654.

Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu
Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang
Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng
Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao,
Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai,
Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li,
Zhiyuan Liu, and Maosong Sun. 2024. Minicpm:
Unveiling the potential of small language models
with scalable training strategies.

Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng
Ji, and Lu Wang. 2021. Efficient attentions for
long document summarization. arXiv preprint
arXiv:2104.02112.

Gautier Izacard and Edouard Grave. 2020. Leverag-
ing passage retrieval with generative models for
open domain question answering. arXiv preprint
arXiv:2007.01282.

Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng
Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen,
and Xia Hu. 2024. Llm maybe longlm: Self-extend
lim context window without tuning. arXiv preprint
arXiv:2401.01325.

Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion.

Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya
Goyal, and Mohit Iyyer. 2024. One thousand and one
pairs: A" novel" challenge for long-context language
models. arXiv preprint arXiv:2406.16264.

Shanda Li, Chong You, Guru Guruganesh, Joshua
Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit
Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh
Bhojanapalli. 2023. Functional interpolation for rel-
ative positions improves long context transformers.
arXiv preprint arXiv:2310.04418.

Haoran Lian, Yizhe Xiong, Zijia Lin, Jianwei Niu,
Shasha Mo, Hui Chen, Peng Liu, and Guiguang
Ding. 2024a. Lbpe: Long-token-first tokenization
to improve large language models. arXiv preprint
arXiv:2411.05504.

Haoran Lian, Yizhe Xiong, Jianwei Niu, Shasha Mo,
Zhenpeng Su, Zijia Lin, Peng Liu, Hui Chen, and
Guiguang Ding. 2024b. Scaffold-bpe: Enhancing
byte pair encoding with simple and effective scaffold
token removal. arXiv preprint arXiv:2404.17808.

Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang,
Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong
Ruan, Damai Dai, Daya Guo, et al. 2024a.
Deepseek-v2: A strong, economical, and efficient
mixture-of-experts language model. arXiv preprint
arXiv:2405.04434.

Hao Liu, Wilson Yan, Matei Zaharia, and Pieter
Abbeel. 2024b. Lwm. https://github.
com/zhangir-azerbayev/proof-pile. Accessed:
2024-09-13.

Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.
2024c. World model on million-length video and lan-
guage with blockwise ringattention. arXiv preprint
arXiv:2402.08268.

Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An,
Xipeng Qiu, and Dahua Lin. 2023. Scaling
laws of rope-based extrapolation. arXiv preprint
arXiv:2310.05209.

Yang Liu and Mirella Lapata. 2019. Text summa-
rization with pretrained encoders. arXiv preprint
arXiv:1908.08345.

Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou,
Jonathan May, Hao Ma, and Luke Zettlemoyer. 2021.
Luna: Linear unified nested attention. Advances
in Neural Information Processing Systems, 34:2441-
2453.

André Martins, Antdénio Farinhas, Marcos Treviso, Vlad
Niculae, Pedro Aguiar, and Mario Figueiredo. 2020.
Sparse and continuous attention mechanisms. Ad-
vances in Neural Information Processing Systems,

33:20989-21001.

Xin Men, Mingyu Xu, Bingning Wang, Qingyu Zhang,
Hongyu Lin, Xianpei Han, and Weipeng Chen. 2024.
Base of rope bounds context length. arXiv preprint
arXiv:2405.14591.

Mistral.AI. 2023. La plateforme. https://mistral.
ai/news/la-plateforme/. Accessed: 2024-08-31.

4906
