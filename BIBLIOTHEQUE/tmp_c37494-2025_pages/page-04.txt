Algorithm 1 The searching algorithm of B,

Require: A candidate base set B,
1: Define a function f,(b) ++ peak positions in
attention waveforms corresponding to base b
2: Define a function f,,(b) +> valley positions in
attention waveforms corresponding to base b

3: Initialize the searched base set Bs + {bmin}

5: while |B,| < N do

6: for b; in B. do

7: Be Sp(bj)s Vi — fo(b5)

8 dT ye P, (Pit — Usui
Us, E Vs

dF HD yey, Irie Pail
Psi € Ps

. ; a
10: dj dj +d;
11: end for

122: B,+ BU {bi with the minimum d;}
13; P,<P,U Fp(0)); Vs +VzU fu(0})
14: end while

15: return By

Head _ Base | Head _ Base | Head Base | Head _ Base

1 1.00 9 2.50 17 3.01 25 3.61
1.15 10 2.65 18 3.04 26 3.88
1.30 11 2.68 19 3.10 27 4.09
1.45 12 2.71 20 3.13 28 4.15
2.17 13 2.74 21 3.16 29 4.39
6 2.20 14 2.80 22 3.22 30 4.45
2.23 15 2.83 23 3.43 31 4.51
2.47 16 2.92 24 3.46 32 4.54

URwWN

on

Table 2: RoPE base frequency settings for each head
in HARPE, with each base value expressed in millions
(x 10°), and stride of 30k.

Subsequently, the final searched base set B, is de-
termined by iteratively complementing the valleys
and peaks of attention waveforms of different bases
within B,, as shown in Algorithm 1.

4 Experimental Setup

We select LLama2-7B-Base (Touvron et al., 2023b)
as our base model, which is configured with a ROPE
base frequency of 10k and a context length of 4k.

4.1 Baseline Systems

We compare HARPE with 4 continual pretraining
methods and one training-free method.

PI (Chen et al., 2023) employs a linear down-
scaling of the input position indices to match the
original context window size, thereby avoiding

Method Proof-pile GovReport

Llama2-7B-Base 4336.96 7289.38
PI 20.73 11.47
ABF Single-Stage 3.06 3.58
ABF Multi-Stage 3.03 3.57
YaRN 4.53 4.52
Self-Extend* 5.45 3.76
HARPE(ours) 3.02 3.54

Table 3: Sliding window perplexity (S = 256) for Proof-
pile and GovReport documents. Asterisks (**) denote
the training-free method. Lower perplexity values indi-
cate better model performance.

extrapolation beyond the trained context length,
which can lead to catastrophically high attention
scores that compromise the self-attention mech-
anism. The interpolation scale is set to 32 =
128k/4k.

ABF Single-Stage (Xiong et al., 2023) imple-
ments a minimal yet necessary modification to the
RoPE positional encoding for long-context mod-
eling: increasing the hyperparameter "base fre-
quency" b to 5m (i.e., decreasing the rotation an-
gle), which mitigates the decaying effect of RoPE
for distant tokens. Concurrently, the input sequence
length is increased to 128k.

ABF Multi-Stage also increases the base and
input sequence length, but with a key difference: it
does so in a gradual, multi-stage manner. Specif-
ically, we divide the process into three stages:
(1)b = 1m;l = 32k, (2)b = 2m;1 = 64k, and
(3)b = 5m; 1 = 128k.

YaRN (Peng et al., 2023) utilizes the RoPE for-
mula Eq. (5) to distinguish between high-frequency
and low-frequency positional components. It ad-
justs the base within a 64-dimensional space ac-
cording to these frequency components, applying a
scale factor of 32.

Self-Extend (Jin et al., 2024) is a training-free
method. We apply it with window_size = 1024
and group_size = 32.

4.2 HARPE Base Setting

We adopt the second strategy (i.e.,the peak-valley
search method) mentioned in Sec. 3.2. We set
bmin = 1M; bmax = 5M; 8 = 30k. The final bases
are shown in Tab. 2. And we will discuss other
base settings in Sec. 5.2.

4901
