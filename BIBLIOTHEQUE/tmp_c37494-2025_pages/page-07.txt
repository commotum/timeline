Method Model 4k 8k 16k 32k 8 664k_~—s «128k = Avg.
RoPE Llama2-7B-Base 90.9 - - - - - -
stride=10k 96.38 96.38 88.97 84.09 76.00 65.22 84.51
complementarity  stride=20k 96.97 96.78 91.69 85.97 74.44 67.59 85.57 (na)
peak&valley stride = 30k 97.03 96.88 93.72 86.66 79.41 67.19 86.82(st)
stride=40k 96.84 96.09 91.13 83.13 75.94 65.00 84.69
stride=50k 92.75 924 87.31 83.97 75.31 70.34 83.68
same stride ascending order 96.53 95.13 89.22 84.72 74.31 65.88 84.30
: : descending order 96.50 95.22 91.91 87.16 76.31 64.69 85.30,3ra)

Table 6: Upgraded Needle-in-a-Haystack Results of HARPE:

Comparison of different base selection schemes in

HARPE models.
Model Size 4k 8k 16k = 332k 64k 128k Avg.
Jamba (AI21, 2024) 52B 81.20 75.4 688 65.3 61.00 514 67.18
Mixtral (Mistral.AI, 2023) 7B 91.60 89.80 86.30 77.20 52.30 8.00 67.50
Llama2-7B-Base 7B 79.4 - - - - - -

Together (Together.AI, 2023) 7B 846 78.7 68.3 57.9 0.0 0.0 48.25

Yarn (Peng et al., 2023) 7B 77.30 67.50 59.00 47.30 38.60 13.90 50.60

LongLoRA (Chen et al.,2024c) 7B 81.90 804 75.6 65.1 60.80 0.0 60.63
LWM (Liu et al., 2024c) 7B 77.50 74.00 69.60 64.60 61.30 59.00 67.67(3:)
lama-2-7b-80k (Yaofu, 2023a) 7B 87.95 80.68 72.70 63.47 54.62 47.65  67.85(2na)
HARPE (ours) 7B 88.48 83.44 74.87 68.10 55.64 51.88 70.40 st)

Table 7: RULER Benchmark Results: Comparison of HARPE and Open-Source Base Models Across All Lengths

for 13 RULER Tasks.

13 tasks that include "needle in a haystack" as well
as additional tasks such as Variable Tracing, Ag-
gregation Ability, and Question Answering.

As shown in Tab. 7, our comparison in 10 base
models primarily involves 7B models, along with
model using other architecture such as Jamba.
HARPE surpasses all LLaMA2-based models and
ranks Ist overall, surpassing the 2nd by 2.55%.
Notably, HARPE demonstrates a significant ad-
vantage in shorter context performance compared
to the multi-stage ABF-trained LWM with a 1M
fine-tuning length. Furthermore, HARPE consis-
tently outperforms the YaRN model with a 128k
fine-tuning length, achieving an average improve-
ment of nearly 20 points across various lengths.
Additionally, when compared to the Hama-2-7b-
80k model, which has the same training parameters
and dataset but a shorter fine-tuning length of 80k,
HARPE still shows superior performance in shorter
context tasks with lengths less than 32k.

6 Conclusion

In this paper, we present a novel single-stage con-
tinual pretraining method, HARPE, to enhance
the long-context modeling capabilities of LLMs.
Specifically, our HARPE distributes the different
training stages across different attention heads, and
assigns different base values in the RoPE for dif-
ferent attention heads during continual pretraining
stage. Experimental results across 4 benchmarks
demonstrate that HARPE outperforming or match-
ing existing multi-stage methods in long-context
modeling tasks, while maintaining comparable per-
formance on short-context tasks. In practical ap-
plications, our HARPE breaks the stage barrier,
offering a simplified pipeline with minimal man-
ual tuning and expertise, thereby streamlining the
process of equipping LLMs with long-context ca-
pabilities.

7 Limitations

Despite that HARPE demonstrates promising re-
sults on benchmarks with long context lengths,

4904
