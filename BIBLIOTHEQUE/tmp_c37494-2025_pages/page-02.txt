multi-stage methods across all benchmarks.
2 Related Works

Large Language Models (LLMs). Language
models are a type of statistical model that aims
to maximize the likelihood token sequences (Tou-
vron et al., 2023a). The Transformer architecture
(Vaswani et al., 2017) marked a turning point in the
evolution of language models, accelerating their de-
velopment. Transformer-based models, like BERT
(Devlin, 2018), GPT-2 (Radford et al., 2019), and
T5 (Raffel et al., 2020), have achieved ground-
breaking results across numerous natural language
processing tasks. More recently, the release of
GPT-4 (Achiam et al., 2023) has further pushed
the boundaries of LLMs performance, showcas-
ing exceptional capabilities. As these models con-
tinue to scale and evolve architecturally, they have
become the driving force behind cutting-edge re-
search in natural language processing, exhibiting
notable adaptability and versatility across a wide
range of applications (Liu et al., 2024a; Cai et al.,
2024; ?). Consequently, LLMs have profoundly
transformed human-computer interaction.

Long Context Modeling. Trained on relatively
short context sequences (i.e., generally <10K to-
kens), open-source LLMs show dramatic perfor-
mance drops on long context modeling (Touvron
et al., 2023a; Bai et al., 2023; Liu et al., 2024a).
Methods to improve the ability of LLMs to han-
dle long context can be mainly divided into the
following categories: attention mechanism opti-
mizing, long-term memory caching, contexual pro-
cessing, and positional encoding optimizing. At-
tention mechanism optimizing methods (Beltagy
et al., 2020; Ma et al., 2021; Dao et al., 2022) re-
duce the computational and memory bottlenecks
of the Transformer, thereby enabling the model to
process longer text sequences. Long-term mem-
ory caching methods (Wang et al., 2024; Bulatov
et al., 2022; Martins et al., 2020; Dai et al., 2019)
utilize internal or external memory caches to fetch
information in long context. Contextual process-
ing methods (Ding et al., 2020; Izacard and Grave,
2020) process long context inputs by calling the
model multiple times to process different parts of
the long text sequence.

Apart from those methods, the most common
approach is to improve the RoPE (Su et al., 2024a)
while conducting continual pretraining with a
longer context window. Specifically, Position In-
terpolation (PI) (Chen et al., 2023) reduces the

input position index to match the original context
window size. ABF (Xiong et al., 2023) adjusts
the RoPE base (i.e., @) to scale the low-frequency
part more significantly, thereby dispersing the in-
terpolation pressure to multiple dimensions. NTK-
by-parts interpolation (bloc97, 2023) interpolates
RoPE bases according to the wavelength of dif-
ferent dimensions in RoPE relative to the context
size: high-frequency dimensions are not interpo-
lated, low-frequency dimensions are fully interpo-
lated, and intermediate frequency dimensions are
partially interpolated using a ramp function. YARN
(Peng et al., 2023) combines the NTK-by-parts
interpolation with the attention scaling technique
to achieve an even longer effective context length.
Self-Extend (Jin et al., 2024) constructs a two-layer
attention mechanism, consisting of group attention
and neighbor attention, to successfully expand the
context window without additional training. Stud-
ies have also explored the construction (Chen et al.,
2024a) and training strategies (Bai et al., 2024) of
long context data.

While methods based on improving positional
encoding have achieved promising experimental
results (Liu et al., 2024c; He et al., 2024; Zhang
et al., 2024b), they typically rely on complicated
multi-stage training pipelines to gradually increase
the effective context length(e.g., 8k + 16k >
32k... â€”+ 128k). In contrast, our proposed
HARPE offers a simplified single-stage continual
pretraining approach. Experimental results demon-
strate that our HARPE achieves comparable perfor-
mance to existing multi-stage methods.

3  Head-Adaptive Rotary Position
Encoding based Approach

In this section, we first revist the formulation of
Rotary Position Encoding (RoPE) in Sec. 3.1. We
then present the proposed HARPE in Sec. 3.2, de-
tailing its multi-head RoPE base mechanism and
base selection strategies.

3.1 Preliminaries

RoPE (Su et al., 2024a) is a widely adopted tech-
nique for position encoding in LLMs built on the
Transformer architecture (GLM et al., 2024; Liu
et al., 2024a; Yang et al., 2023; Dubey et al., 2024).
The primary objective of RoPE is to encode posi-
tional information in a way that the inner product
of the query and key embeddings inherently cap-
tures the relative position information, which can

4899
