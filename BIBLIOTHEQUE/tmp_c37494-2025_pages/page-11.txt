machine really finish your sentence? arXiv preprint
arXiv:1905.07830.

Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang
Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai,
Shuo Wang, Zhiyuan Liu, et al. 2024a. 00 bench: Ex-
tending long context evaluation beyond 100k tokens.
In Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 15262-15277.

Yiyuan Zhang, Handong Li, Jing Liu, and Xiangyu Yue.
2024b. Explore the limits of omni-modal pretraining
at scale. arXiv preprint arXiv:2406.09412.

Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao,
Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, and
Zhangyang Wang. 2024c. Found in the middle:
How language models use long contexts better via
plug-and-play positional encoding. arXiv preprint
arXiv:2403.04797.

Bartosz Piotrowski Zhangir Azerbayev, Edward Ay-
ers. 2022. proof-pile. https://github.

com/zhangir-azerbayev/proof-pile. Accessed:
2022.

A _ Detail Scores

4908
