limitation still remains. Our research is primar-
ily concentrated on the continual pretraining stage,
leaving its applicability to other stages, such as su-
pervised fine-tuning, unexplored. We will address
those limitations in our future research.

Acknowledgment

This work was supported in part by National
Key R&D Program of China under Grant No.
2023 YFB4503700, National Natural Science Foun-
dation of China under Grant No. 62372027,
U23B2025, Beijing Natural Science Foundation
(L247026).

References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774.

AI21. 2024. Introducing jamba: Ai21’s groundbreaking
ssm-transformer model. https://www.ai21.com/
blog/announcing-jamba. Accessed: 2024-08-31.

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. 2023. Qwen technical report. arXiv
preprint arXiv:2309. 16609.

Yushi Bai, Xin Ly, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou,
Jie Tang, Yuxiao Dong, and Juanzi Li. 2024. Lon-
galign: A recipe for long context alignment of large
language models. arXiv preprint arXiv:2401.18058.

Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150.

Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng
Gao, and Yejin Choi. 2019. Piqa: Reasoning about
physical commonsense in natural language.

bloc97. 2023. Add ntk-aware interpolation "by parts"
correction. Accessed: 2023.

Tom B Brown. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165.

Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev.
2022. Recurrent memory transformer. Advances
in Neural Information Processing Systems, 35:11079-
11091.

Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen,
Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi
Chen, Pei Chu, et al. 2024. Internlm2 technical re-
port. arXiv preprint arXiv:2403.17297.

ChatGLM. 2024. Glm: Long scaling pre-trained model
contexts to millions. Accessed: 2024-09-14.

Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li,
Run Luo, and Min Yang. 2024a. Long context is
not long at all: A prospector of long-dependency
data for large language models. arXiv preprint
arXiv:2405.17915.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.
arXiv preprint arXiv:2306.15595.

Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen,
Yuchuan Wu, Fei Huang, Yongbin Li, and Rui Yan.
2024b. Fortify the shortest stave in attention: En-
hancing context awareness of large language models
for effective tool use. In Proceedings of the 62nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 11160-
11174.

Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
Zhijian Liu, Song Han, and Jiaya Jia. 2024c. Lon-
glora: Efficient fine-tuning of long-context large lan-
guage models. In Proceedings of the International
Conference on Learning Representations (ICLR).

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc V Le, and Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language mod-
els beyond a fixed-length context. arXiv preprint
arXiv:1901.02860.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
Advances in Neural Information Processing Systems,

35:16344-16359.

Jacob Devlin. 2018. Bert: Pre-training of deep bidi-
rectional transformers for language understanding.
arXiv preprint arXiv:1810.04805.

Ming Ding, Chang Zhou, Hongxia Yang, and Jie Tang.
2020. Cogltx: Applying bert to long texts. Advances
in Neural Information Processing Systems, 33:12792-
12804.

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela
Fan, et al. 2024. The llama 3 herd of models. arXiv
preprint arXiv:2407.21783.

Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong
Chen, Zhirui Yang, Shengnan Zhang, Xue Zheng,
Yan Li, Yuliang Liu, et al. 2023. Kwaiyiimath: Tech-
nical report. arXiv preprint arXiv:2310.07488.

Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Han-
naneh Hajishirzi, Yoon Kim, and Hao Peng. 2024a.
Data engineering for scaling language models to 128k
context. arXiv preprint arXiv:2402.10171.

4905
