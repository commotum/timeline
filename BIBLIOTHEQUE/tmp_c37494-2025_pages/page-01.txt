Three-Stage ABF on LLaMA

Metric Uniform 2B Tokens Carefully Selected
NiaH 67.83 81.36
Benchmark 62.59 63.10

Table 1: Performance comparison of different contin-
ual pertaining pipelines when conducting three-stage
ABF (Adjusted Base Frequency) (Xiong et al., 2023).
For both pipelines, we report the average score of the
Needle-in-a-Haystack task (6 lengths, 8 tasks) and on
Short-Context Benchmarks (5 tasks). For more details
on the experiment settings, please refer to Sec. 4.

stage adjustments.

Although multi-stage approaches have shown
promising results, our experiments reveal that they
require substantial manual tuning and human exper-
tise to achieve good performance on long context
modeling benchmarks. For instance, our results
in Tab. 1 demonstrate that a carefully scheduled
three-stage pipeline outperforms a naive approach
by 13.5% on the NiaH benchmark, highlighting the
limited generalizability of hyperparameters across
different LLM sizes and architectures. Moreover,
multi-stage training poses practical challenges due
to varying data and resource requirements. This
motivates the need for single-stage continual pre-
training approaches. However, single-stage train-
ing also presents challenges, such as the risk of
suboptimal outcomes when training with a much
longer context window and larger RoPE base fre-
quency, as shown in our experiments.

In this paper, we introduce a novel single-stage
approach, termed Head-Adaptive Rotary Position
Encoding (HARPE), designed to address the long-
context problem. Our goal is to achieve an effective
context length comparable to that of multi-stage
methods. Drawing inspiration from the finding that
different attention heads can acquire distinct knowl-
edge during training (Li et al., 2023), we propose
to distribute the training of different stages across
multiple attention heads concurrently. Specifically,
we leverage RoPE (Su et al., 2024a) with varying
base values to represent different effective context
lengths, thereby simulating multiple training stages.
By assigning different base values to different at-
tention heads, we enable the LLMs to be trained in
a single stage.

To determine the RoPE base values for each
attention head, we employ a complementary ap-
proach, carefully selecting values that fill in the
peaks and valleys of the sine and cosine waves

in RoPE, thereby optimizing the experimental re-
sults. In contrast to existing methods, our proposed
HARPE offers a significant advantage in terms of
simplicity and efficiency. By pretraining LLMs in
a single stage, we substantially streamline the pro-
cess of data preparation and pipeline adjustment,
eliminating the need for multiple stages and associ-
ated complexities.

We conduct a comprehensive evaluation of
HARPE on 4 benchmarks, including the recently
introduced RULER benchmark (Hsieh et al., 2024),
to assess its effectiveness on both long-context
and short-context tasks. The experimental results
demonstrate that HARPE consistently matches or
surpasses the performance of its multi-stage coun-
terparts across all evaluated benchmarks. No-
tably, on the challenging Needle-in-a-haystack task,
HARPE achieves a significant improvement of
5.46% over the multi-stage Adjused Base Fre-
quency (ABF) (Xiong et al., 2023) approach, under-
scoring its exceptional capabilities in long-context
modeling.

Unlike inference methods that employ multiple
RoPE bases simultaneously to support long con-
texts (Zhang et al., 2024c; Chen et al., 2024b), our
HARPE approach fundamentally alters the learning
dynamics of LLMs during continual pretraining,
enabling a straightforward and streamlined train-
ing pipeline. In summary, our contributions are
threefold:

¢ We introduce a novel single-stage continual
pretraining approach, termed Head-Adaptive
Rotary Position Encoding (HARPE), to ad-
dress the long context problem in LLMs. By
doing so, we significantly simplify the process
of data preparation and pipeline adjustment.

¢ To overcome the limitations of traditional
multi-stage approaches, we propose a novel
training strategy that distributes the training
of different stages across multiple attention
heads. We utilize different RoPE base values
to represent distinct training stages and care-
fully select these values to complement the
attention scores.

¢ We conduct a comprehensive evaluation of
HARPE on 4 long context benchmarks,
including the recently introduced RULER
benchmark. Our experimental results demon-
strate that HARPE consistently yields compa-
rable or even better performance than existing

4898
