                                                                                                                                                                                     Intelligent Systems with Applications 22 (2024) 200361
                                                                                                                                                                                                       Contents lists available at                                                     ScienceDirect
                                                                                                                                                                Intelligent                                             Systems                                       with                        Applications
                                                                                                                          journal homepage: www.journals.elsevier.com/intelligent-systems-with-applications
                                 DeLiVoTr:                                             Deep                         and                    light-weight                                                      voxel                          transformer                                                     for               3D                object                            detection
                                 Gopi Krishna Erabati                                                                  ‚àó,  Helder Araujo
                                 Institute           of  Systems              and        Robotics,             University               of  Coimbra,                 Rua        Silvio        Lima          -Pool          II,   Coimbra,               3030-290,                  Portugal
                                 A  R  T  I C  L  E                                             I   N   F   O                                                                   A  B  S  T  R  A  C  T
                                 Keywords:                                                                                                                                      The        image-based                         backbone                    (feature                extraction)                     networks                   downsample                          the        feature              maps            not        only          to  increase                   the        
                                 3D      object           detection                                                                                                             receptive                 Ô¨Åeld           but        also         to  eÔ¨Éciently                         detect             objects              of  various                   scales.             The          existing                feature              extraction                    networks                    
                                 Transformer                                                                                                                                    in    LiDAR-based                           3D       object             detection                   tasks          follow              the       feature               map           downsampling                              similar              to  image-based                             feature              
                                 Voxel                                                                                                                                          extraction                  networks                   to  increase                   the        receptive                  Ô¨Åeld.          But,         such          downsampling                              of  LiDAR                 feature              maps            in  large-scale                       
                                 LiDAR                                                                                                                                          autonomous                        driving               scenarios                 hinder              the       detection                  of     small           size        objects,              such          as     pedestrians                  .  To       solve          this        issue          we       
                                 Autonomous                      driving                                                                                                        design            an      architecture                       that        not        only         maintains                    the       same           scale          of     the       feature             maps            but        also        the       receptive                  Ô¨Åeld         in     the       
                                 Computer                 vision                                                                                                                feature             extraction                   network                  to  aid          for      eÔ¨Écient                 detection                  of  small              size        objects.              We         resort           to  attention                    mechanism                        to  
                                                                                                                                                                                build          suÔ¨Écient                   receptive                  Ô¨Åeld          and         we        propose                a     De      ep     and          Li    ght-weight                     Vo      xel       Tr    ansformer                    (DeLiVoTr)                       network                 
                                                                                                                                                                                with         voxel           intra- and                   inter-region                      transformer                       modules                 to  extract                 voxel           local          and         global            features                respectively.                       We        
                                                                                                                                                                                introduce                  DeLiVoTr                    block           that         uses         transformations                             with          expand                and         reduce             strategy                to     vary          the       width            and         depth            
                                                                                                                                                                                of    the        network                 eÔ¨Éciently.                     This         facilitates                  to  learn             wider             and         deeper              voxel           representations                             and         enables               to  use           not        
                                                                                                                                                                                only         smaller               dimension                      for       attention                  mechanism                        but         also         a  light-weight                          feed-forward                          network,                  facilitating                    the        
                                                                                                                                                                                reduction                  of  parameters                          and         operations.                      In  addition                    to  model                 scaling,              we        employ                layer-level                   scaling             of  DeLiVoTr                       
                                                                                                                                                                                encoder                layers            for      eÔ¨Écient                  parameter                     allocation                   in  each             encoder                 layer           instead              of  Ô¨Åxed               number                 of  parameters                          as  
                                                                                                                                                                                in    existing                approaches.                        Leveraging                      layer-level                  depth             and        width             scaling             we  formulate                          three            variants                of  DeLiVoTr                        
                                                                                                                                                                                network.                 We         conduct                extensive                  experiments                        and         analysis                on      large-scale                    Waymo                 and         KITTI            datasets.                 Our         network                 
                                                                                                                                                                                surpasses                 state-of-the-art                          methods                  for       detection                  of  small              objects              ( pedestrians                  )  with           an      inference                  speed            of  20.5             FPS.
                                 1.     Introduction                                                                                                                                                                                                                      centrated                     in  the               3D  space                       (Fan             et  al.,              2022              ).   But           many                existing                   3D  
                                                                                                                                                                                                                                                                          object              detection                     approaches                           (Fan            et  al.,           2021             ,  Lang             et  al.,           2019             ,  Yan           et  
                                            3D       object               detection                      is  one             of  the              fundamental                             tasks             in  autonomous                                                al.,       2018             ,  Yin         et  al.,            2021             )  inherited                      the        concept                   of  multi-scale                             feature                 
                                 driving                and           robotics.                    3D  object                      detection                      from             LiDAR                 point             cloud               is  a                      maps             from             their            2D         counterparts,                             which                is  not            useful               for        3D         object              de-
                                 challenging                         task            due           to  sparse                     and            unordered                         nature                 of  point                   cloud.                              tection               in  large-scale                            outdoor                   environments.                                Moreover,                        the         relative                 size         
                                 Existing                 approaches                           either              directly                 use         the        point             cloud              (Shi          et  al.,           2019            ,                of     objects                 in  3D              space              compared                         to  perception                              range              is  quite                 low          (for          
                                 Chen             et  al.,          2019            ,  Qi       et  al.,          2018             )  or      quantize                    the        point             cloud             into          voxels                             instance,                   perception                          range              for         autonomous                              driving                 is  150 m                     √ó 150 m                       
                                 (Zhou              &  Tuzel,                    2018             ,  Yan           et  al.,            2018             ,  Yin         et  al.,            2021             )  to  extract                      the                       but         a  pedestrian                           occupies                     1m.)                With             objects                 of  relatively                           small              scale            
                                 LiDAR                features                  for        eÔ¨Écient                    3D        object               detection.                      Many               approaches                           (Yin                         compared                        to  the              large             scale            perception                          range               in  autonomous                                   driving,                  
                                 et     al.,       2021             ,  Yan          et  al.,           2018            )  transform                        the        quantized                       voxels               into          Bird‚Äôs                           downsampling                                  the         LiDAR                feature                maps              for        multi-scale                        features                  will         hin-
                                 Eye         View               (BEV)               to  leverage                          the          techniques                         from              2D  object                       detection                                    der        the         detection                     of  small                  size         objects.
                                 (Redmon                      &  Farhadi,                       2018             ,  Liu         et  al.,           2016             ,  Li     et  al.,            2022            ,  Wang                et  al.,                                    To       solve             these              issues,               inspired                    by  (Fan                   et  al.,             2022             )  we           intend                to  
                                 2024            ,  Carion                et  al.,           2020             ).                                                                                                                                                          design               an       architecture                           to  maintain                         the         same             scale            of  feature                    maps              with-
                                            The         2D         object              detection                      (Redmon                       &  Farhadi,                       2018            ,  Liu         et  al.,           2016             ,                out        downsampling                                    in  our              proposed                      LiDAR                 feature                 extraction                        network.                     
                                 Li     et  al.,             2022             ,  Wang                et  al.,             2024             )  approaches                            are         designed                      to  tackle                                  However,                       it  brings                 a  drawback                           of  decrease                         in  receptive                         Ô¨Åeld            compared                        
                                 the        objects                of  varying                       scales              with            the         help           of  multi-scale                             feature                 maps.                             to      the         network                     with            feature                 map             downsampling.                                    In  order                   to  maintain                          
                                 However,                       the        scales              of  objects                     in  the              3D         object              detection                      task           are         con-                         the        same              scale            of  feature                    maps               along              with           the         receptive                     Ô¨Åeld,             we         think             
                                     *    Corresponding                              author.
                                            E-mail           addresses:                   gopi.erabati@isr.uc.pt                                         (G.K. Erabati),                          helder@isr.uc.pt                                (H. Araujo).
                                 https://doi.org/10.1016/j.iswa.2024.200361
                                 Received                  23       January                 2024;            Received                   in  revised                 form           3  March                 2024;            Accepted                   13       March              2024
                                 Available online 19 March 2024
                                 2667-3053/¬©2024TheAuthors. PublishedbyElsevierLtd. ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-
                                 nc-nd/4.0/).
                       G.K. Erabati and H. Araujo                                                                                                                                                                                                  Intelligent Systems with Applications 22 (2024) 200361
                       of   attention           mechanism                (Vaswani             et  al.,      2017       )  as   a  better         option         to  ob-                LiVoTr_small,                DeLiVoTr_large)                     (Table         4 )  leveraging             the      layer-level          depth
                       tain     a  good          representation                  of  voxels           in  a  latent            space.        This       is  due        to              and      width       scaling.
                       the     fact     that      the     attention           mechanism                provides            long      range         dependency                                 We      conduct           extensive             experiments                and       analysis          on  the         large-scale             
                       and      semantically                obtains          large       context           which         maintains              the     receptive                      Waymo             Open         Dataset            (WOD)            (Sun        et   al.,      2020       )   and        KITTI         dataset         
                       Ô¨Åeld      of  same          scale       feature          maps.         However,              applying            attention           mecha-                     (Geiger         et  al.,      2012        ).  Our      DeLiVoTr              network            achieves            72.8       APH,        70.2       
                       nism       on  all  the          sparse         voxels        in  a  large-scale                outdoor           scenario           such       as              APH       and      66.8       APH        for    LEVEL_1             vehicle      ,  pedestrian           and      cyclist       categories            
                       autonomous                 driving         is  computationally                     very       expensive.                                                        respectively,              and       64.5      APH,         63.4      APH        and       64.2      APH        for     LEVEL_2            vehi-
                              To    tackle        the     computational                   complexity,               motivated             from       (Fan       et  al.,               cle  ,  pedestrian            and     cyclist        categories            respectively.               Our      method           surpasses            
                       2022       ),  we     divide         the     entire        perception              range         into      non-overlapping                     re-              the     existing          approaches                on  small            size      pedestrian             class      (by      leveraging              
                       gions       and      compute             the     attention            mechanism                within         each       region         in  our                 intra-  and           inter-region               transformer                powered             by  our          DeLiVoTr              block)         
                       voxel       intra-region               transformer               to  obtain            voxel        local      features.           Inspired                     and      achieves           competitive               performance                 to  other         methods             on    vehicle         and     
                       by    the      concept           of  shifted           window             (Liu      et  al.,       2021        ),  SST      (Fan        et  al.,                cyclist       categories.             The      three        variants          of  our        model          not      only       show eÔ¨É-
                       2022       )  uses     the     concept          of    shifted        regional          grouping            to    model         the     objects                  cient      parameter              allocation            with       lesser       GPU        memory             allocation            but      also     
                       truncated            by    windowing.                Instead          of  shifting          the     windows,             we      formulate                      improved             performance                 with       an  inference               speed        of  20.5         FPS       (LiDAR           in  
                       voxel       inter-region               transformer               with        voxel        feature          region         aggregation,                          the     Waymo            Open        Dataset          (Sun       et  al.,     2020        )  operates          at  10      Hz).
                       inter-region              attention           and      voxel        feature         propagation                modules            to  obtain                           We      summarize               our      main       contributions                 as  follows:
                       voxel       global        features.           In  the       voxel       inter-region              transformer               module,            we     
                       aggregate            the      features           in  each         region          and      model          the      interactions               be-                     ‚Ä¢We        introduce             a  deep         and       light-weight                voxel        transformer               (DeLiV-
                       tween         the     regions         to  increase            the     receptive            Ô¨Åeld       size.     The       voxel       feature                            oTr)      network             that      can      be  employed                 as  an  eÔ¨Écient                 backbone             net-
                       region        aggregation               not     only      helps       to    reduce         the     computational                  complex-                               work         for      LiDAR           point        cloud         feature          extraction.              Unlike           existing         
                       ity   of  our       approach             but     also      achieves           the     global        context          of  the      scene.        As                       methods,             we     maintain            not     only       the     same        scale       of  feature           maps        but     
                       we     intend         to  maintain              the      same        scale       of  feature           maps         in  our        encoder,                              also      the    receptive            Ô¨Åeld      by     region        attention           mechanism                to   eÔ¨Éciently             
                       we     propagate             the      voxel       global         features          back       to  the       corresponding                   vox-                         detect        the     small        size     objects.
                       els    of  each         corresponding                   region          and      fuse       the     voxel         local       and      global                         ‚Ä¢We        propose           DeLiVoTr              block        with       DeLighT             transformation                   (Mehta          
                       features          to  obtain          a  large       context          and      better        semantic            voxel       features          for                       et   al.,    2021       )  for   eÔ¨Écient            parameter             allocation            in   the     encoder           layers,       
                       eÔ¨Écient           3D     object        detection.                                                                                                                        which         powers           both       the     voxel       intra- and            inter-region              transformer.
                              The       standard             transformer                layer         (Vaswani              et   al.,       2017        )   consists                         ‚Ä¢We        introduce             three        variants           of  our        network            (DeLiVoTr_base,                     De-
                       of   multi-head               self-attention               (MHSA)             and       feed-forward                 network            (FFN)                            LiVoTr_small,                 DeLiVoTr_large)                     (Table          4 )  leveraging              the     layer-level           
                       which         contextualizes                 the     relationships                between            voxels        and       learn       wider                           depth        and     width        scaling.
                       representations                  respectively.               The       number            of  operations                in  MHSA              and                      ‚Ä¢We        conduct            experiments                 and       analysis           on  Waymo                 Open         Dataset           
                                                       2                         2
                       FFN      are      Óàª(Ì†µÌ±ëÌ†µÌ±öÌ†µÌ±õ ) and               Óàª(8Ì†µÌ±ëÌ†µÌ±ö) respectively,                       where          Ì†µÌ±ëÌ†µÌ±ö is  the         model          di-                       (Sun       et  al.,       2020        )   and      KITTI         dataset          (Geiger           et  al.,       2012        ).  Our       
                       mension            and      Ì†µÌ±õ is  number              of  input          tokens.          The      performance                 of  vanilla                              network            surpasses            the     existing          approaches               on  small          size      pedestrian
                       attention           (Vaswani             et  al.,     2017        )  based        transformer               models           is  often        im-                        class      with       an    inference            speed        of  20.5        FPS      (LiDAR           operates           at  10     Hz     
                       proved         by     model         ( width       and     depth      1 )  scaling.          However,             the     model         scaling                           in    WOD).
                       not     only       increases            the     model          operations              and      parameters                but     also       em-                      ‚Ä¢We         release          our       source          codes         to   facilitate            further           research.            The      
                       ploys       constant           number            of  parameters                 in  each         transformer                layer       which                            source         codes        are     at:    https://github                 .com      /gopi       -erabati        /DeLiVoTr            .
                       may       lead      to  sub       optimal           solution.
                              To    tackle         the      issues        with        standard            transformer                layer,        we  propose                         2.   Related             work
                       De    ep    and       Li   ght-weight              Vo    xel     Tr   ansformer             (DeLiVoTr)                that      inputs         Li-
                       DAR       point        cloud         and       predicts          3D  bounding                  boxes         in  an  autonomous                                 2.1.     LiDAR-based                 3D     object       detection
                       driving         scenario.            Here,        deep       refers       to  increasing               the      number            of  layers          
                       in   the     voxel        transformer               without           increasing             the     parameters               and      opera-                          The      LiDAR          based         approaches               are     classiÔ¨Åed            into      four      types        depend-
                       tions,      hence         light-weight           .  The     DeLiVoTr              network           consists          of  voxelization                          ing     upon        the     representation                  of  the        LiDAR          point        cloud.        Point-based               ap-
                       module           (Sec.      3.2    ),  encoder           layers        (Sec.       3.3   )  with       voxel        intra- and           inter-                 proaches            (Shi      et  al.,      2019        ,  Chen        et  al.,       2019       ,  Qi     et  al.,      2018        ,  Wu      et  
                       region        transformer                (both         powered             by  our         proposed              DeLiVoTr              block)                   al.,   2019        )  directly         operate          on  the        point       clouds         to  extract           point       features          
                       and      decoder           (Sec.       3.4    ).  Inspired           by  (Mehta             et  al.,      2021       ),  we      introduce                      from       local      neighborhoods leveraging                               the     PointNet            models          (Qi,     Su,     et  al.,    
                       DeLiVoTr             block        (Sec.      3.3.1      )  with       DeLighT           transformation                   (Mehta          et  al.,               2017       ,  Qi,   Yi,    et   al.,    2017      ).  The      point-based               approaches               are    computation-
                       2021       )  that     uses       group         linear        transformations                    (GLTs)          with       expand           and                ally     very      expensive              for    large-scale             point       clouds         such       as  in  autonomous                     
                       reduce         strategy          to  vary         the      width        and      depth        of   the     network            eÔ¨Éciently.                        driving.         Due       to  the        unordered              nature         of  point          clouds,          few      approaches               
                       The      expand           and      reduce          layers         of  DeLighT              transformation                   facilitate          to              converts           point       clouds         to  3D        voxels.         The       voxel-based              approaches               (Zhou         
                       learn       wider         and     deeper          voxel        representations                   and      therefore             it  enables                     &  Tuzel,          2018       ,  Maturana              &  Scherer,            2015        )  apply        3D  CNNs             on  the         3D  
                       to   use     smaller          dimensions               for     attention           and      also      to  replace           MHSA           with                 voxels        to  extract          the     voxel        features.          However,              the     3D     CNNs         are     compu-
                                                                                                             2
                       single-head              self-attention               (SHSA)           (Óàª(Ì†µÌ±ë Ì†µÌ±õ ), where                  Ì†µÌ±ë    =Ì†µÌ±ë ‚àï2) and  FFN                      
                                                                             2                           Ì†µÌ±ú                         Ì†µÌ±ú         Ì†µÌ±ö                                      tationally           expensive,             therefore            (Yan       et  al.,     2018        ,  Yin    et  al.,      2021       ,  Choy       
                       with      light-weight                FFN       (Ì†µÌ±ëÌ†µÌ±ö‚àï2), thus               reducing            the      total      number             of  pa-                 et   al.,    2019       )  apply       3D     sparse         convolutions                (Graham            et  al.,     2018       )  to   mit-
                       rameters.            Instead         of  Ô¨Åxed          number            of  parameters                in  each         encoder           layer                 igate      the      memory             and       eÔ¨Éciency              problems             with       3D  CNNs.              Range-view
                       as   in  standard               transformer                layer        (Vaswani             et  al.,       2017        ),  we  employ                          (Fan      et  al.,      2021       ,  Meyer         et  al.,     2019       )  based        methods            project         the     LiDAR          
                       layer-level           scaling         of  each         encoder            layer        that      facilitates           variable-sized                           point       cloud         into      range         view        for     computational                   eÔ¨Éciency.               Hybrid           ap-
                       encoder           layers        for     eÔ¨Écient            parameter               allocation.             With        the      layer-level                     proaches            (Shi      et  al.,      2020       ,  2023       ,  Sun      et  al.,      2021       ,  Wang          et  al.,      2020       ,  
                       scaling        we  can           employ            shallower              and       narrower             encoder            layers         near                 Tang       et  al.,      2020       )  use      diÔ¨Äerent            types       (Point/Voxel)                  of  representations                    
                       the     input        and      deeper           and      wider         encoder            layers         near       the      output         sim-                 to   extract         the     LiDAR          features          for    3D      object        detection.
                       ilar    to  Convolutional                     Neural          Networks              (CNNs)           (He       et  al.,       2016       ).  We                        Frustum-PointNet                      (Qi      et  al.,      2018        )  uses      2D  detection                 approach             to  
                       introduce            three       variants           of  our       DeLiVoTr             network            (DeLiVoTr_base,                     De-               generate           object        proposals             and      lift   them         to  3D       frustum           to  detect          the     3D     
                                                                                                                                                                                       objects.         Depending               on  the         usage         of  anchors              the     methods             are      divided          
                          1   The     depth       of  neural        network          refers      to  number           of  layers       in  the      network          and               into     anchor-based                 (Zhou         &  Tuzel,           2018        ,  Yan      et  al.,      2018        ,  Lang       et  al.,      
                       width      of   neural       network          refers      to  maximum              dimension            of  latent       space.                                 2019       ,  Yang       et  al.,      2020       )  and       anchor-free              approaches              (Yin       et  al.,      2021       ,  
                                                                                                                                                                                 2
                       G.K. Erabati and H. Araujo                                                                                                                                                                                                  Intelligent Systems with Applications 22 (2024) 200361
                       Wang         &  Solomon,              2021       ,  Erabati         &  Araujo,           2023       ,  Fan      et  al.,     2021       ,  Chen                to-one         mapping            between            points        and      their      corresponding                  voxels.        Given         a  
                       et   al.,    2020       ,  Misra       et  al.,     2021       ).                                                                                                                                          Ì†µÌ±Å                                  3
                                                                                                                                                                                      point       cloud        Óàº ={Ì†µÌ±ùÌ†µÌ±ñ}                ,  where        Ì†µÌ±ùÌ†µÌ±ñ ‚àà ‚Ñù          and      Ì†µÌ±Å is  the         number           of  points,          
                                                                                                                                                                                                                                  Ì†µÌ±ñ=1
                                                                                                                                                                                      and      the      voxel        size      (Ì†µÌ±£ , Ì†µÌ±£ , Ì†µÌ±£ ), the             dynamic             voxelization                (Zhou         et  al.,      
                                                                                                                                                                                                                                    Ì†µÌ±•     Ì†µÌ±¶    Ì†µÌ±ß
                       2.2.     Transformer-based                      3D     object       detection                                                                                  2020       )  quantizes             the     point        cloud        into       a  set      of  voxels           ÓâÇ ={(Ì†µÌ±ê , Ì†µÌ±ù )}Ì†µÌ±Å , 
                                                                                                                                                                                                                  3                                                                                                      Ì†µÌ±ñ    Ì†µÌ±ñ   Ì†µÌ±ñ=1
                                                                                                                                                                                      where         Ì†µÌ±ê    ‚àà‚Ñù is  the  voxel                     coordinate               for    the      corresponding                  point        Ì†µÌ±ù .  
                              Inspired          by    the     seminal           work       on     transformers                (Vaswani            et  al.,     2017       )           The      point   Ì†µÌ±ñ  cloud        is  discretized             into     a  sparse         grid      of   shape        (Ì†µÌ±† , Ì†µÌ±† , Ì†µÌ±† ).  Sim- Ì†µÌ±ñ
                       in   language            modeling,             researchers              started        to  adapt          transformers               for    var-                                                                                                                                        Ì†µÌ±•    Ì†µÌ±¶     Ì†µÌ±ß
                       ious     vision        tasks       such       as  classiÔ¨Åcation,                 detection,            segmentation                 etc.     ViT               ilar     to  (Park         et  al.,      2022       )  we      employ           centroid-aware                   voxelization               pro-
                       (Dosovitskiy               et  al.,     2021       )  is  the      Ô¨Årst      work        which         demonstrated                  that      an              cess      that     considers            the     distance           between            a  point        and       it‚Äôs   corresponding                  
                       image        can       be  divided              into      patches           and       fed     to  the        transformer                as  in-                voxel        centroid          to  avoid          loss     due      to  quantization.                  The      hard       voxelization               
                       put     tokens.         Vision         transformers               are     used       in  feature           extraction            networks                      (Zhou         &  Tuzel,         2018       )  process          assigns         Ô¨Åxed       number            of  voxels         and      points        
                       (backbone)              (Wang          et  al.,     2021       ,  Wang,         Xie,     et  al.,     2022       ,  Liu     et  al.,     2021      ,           per     voxel.        The      drawbacks               of  this      approach             are    certain         voxels/points                 are    
                       Yuan        et  al.,    2021       ,  Han      et  al.,     2021       ),  detection            tasks      (Carion          et  al.,     2020       ,          dropped            due      to  the        Ô¨Åxed        buÔ¨Äer          size     which          leads        to  information                 loss,      
                       Zhu      et  al.,     2020       ,  Wang,          Guizilini,          et  al.,      2022       ,  Erabati         &  Araujo,           2023        ,          the     stochastic             drop       of  voxels/points                    leads        to  non-deterministic                        voxel        
                       Wang         &  Solomon,               2021        )  and      segmentation                  tasks       (Zheng           et  al.,      2021       ,           features          and      padded           voxels         results        in  redundant                computation                 load.       To  
                       Xie     et  al.,     2021       ,  Cheng         et  al.,     2022       ,  2021      ).                                                                       overcome              these       limitations,              we     employ           dynamic             voxelization               (Zhou         et  
                              Point       cloud         transformer               (Guo         et  al.,      2021        )  employs            self-attention                         al.,    2020        )   which         completely               maps         the      points         to  their         corresponding                   
                       mechanism               globally           on  the        entire        point       cloud.         The      complexity               of  such                  voxels        with        dynamic             number            of  voxels           and       points.         There          is  minimal             
                       approach            increases            quadratically                 with       number            of  points.           Point        Trans-                  information               loss     in  dynamic              voxelization               and      yields       deterministic                voxel       
                       former         (Zhao         et  al.,     2021        ),  Fast      Point       Transformer                 (Park        et  al.,     2022        ),           features.          The      features          of   voxels        and      their      corresponding                  points        are    trans-
                       VoTr       (Mao         et  al.,      2021        ),  PointFormer                 (Pan        et  al.,      2021        )  applies         self-               formed          into      a  latent         space         using        voxel        feature         encoding             (Wang          et  al.,      
                                                                                                                                                                                      2020       ,  Yin     et  al.,      2021       )  to  obtain          set     of  voxel         features          ÓâÇ ={(Ì†µÌ±ê , Ì†µÌ±ì )}Ì†µÌ±Ä , 
                       attention           on  local          neighborhood of                      points.         The       receptive            Ô¨Åeld       size      of                                                                                                                                               Ì†µÌ±ñ     Ì†µÌ±ñ   Ì†µÌ±ñ=1
                                                                                                                                                                                                                  Ì†µÌ±ë
                                                                                                                                                                                      where         Ì†µÌ±ì ‚àà ‚Ñù Ì†µÌ±ö is  the                voxel       feature         with       model         dimension              Ì†µÌ±ë     and      Ì†µÌ±Ä is  
                       such      local       neighbor approaches                           is  limited          by  the        neighborhood query.                                    the     number Ì†µÌ±ñ         of  voxels.                                                                                         Ì†µÌ±ö
                       SST      (Fan      et   al.,    2022      )  is  a  window-based                    sparse        voxel       transformer               where         
                       the    voxels         are     divided          into     non-overlapping                     windows             and      self-attention                        3.3.     DeLiVoTr              -encoder
                       is  calculated             among          the     sparse        voxels         within        the     window.             The      windows             
                       are    shifted          to  calculate              the     attention            across         windows.              This       approach                               Each       DeLiVoTr              encoder           layer        consists          of  voxel          intra-region               trans-
                       lacks      a  global          receptive            Ô¨Åeld       as  the        self-attention               is  calculated              within                   former,           voxel         inter-region               transformer                (both         powered              by      DeLiVoTr             
                       local      regions.          In  our      approach,             we      divide        the     voxels        into      regions         similar                  block        (Fig.        2 ))  to  obtain             voxel        local       features           and       global         features           re-
                       to   SST       and      employ           voxel        intra-region               transformer               to  extract            voxel       lo-              spectively            and       voxel        intra- and             inter-region               feature          fusion         module           to  
                       cal    features          and      further         formulate             the     interactions               among          the     diÔ¨Äerent                     eÔ¨Äectively             fuse      voxel        local       and      global         features.          The       voxel        features          are     
                       regions         using        inter-region              attention           to  extract          the     voxel       global         features.                   Ô¨Ånally         rasterized           to    a  BEV       feature         map       and      passed         to   the    detection            head.
                       3.   Methodology                                                                                                                                               3.3.1.       DeLiVoTr              block
                       3.1.     Overview                                                                                                                                                      The     vanilla          transformer               layer        (Vaswani             et  al.,      2017        )  consists          of  a  
                                                                                                                                                                                      MHSA           block        which         contextualizes                  the     relationships                between            input        to-
                              An     overview             of    De    ep    and       Li  ght-weight              Vo    xel     Tr   ansformer             (DeLiV-                    kens       and      a  FFN        block       to  learn         wider        representations.                   The      depth         (layers        
                       oTr)      is  presented               in  Fig.         1 .  The       DeLiVoTr              inputs         LiDAR           point        cloud                  with       trainable            parameters)                of  standard              transformer                layer       is  4,  which             
                       and      predicts          3D  bounding                  boxes         in  an  autonomous                       driving          scenario.                     consists          of  a  three           parallel          linear         layers        to  project            input        tokens          into      
                       It  consists          of  three         main         modules:            voxelization,                encoder           and       decoder.                     queries,          keys       and      values,         a  linear         layer       which         fuses       the     output          of  mul-
                       The      voxelization               (Zhou         et  al.,     2020        )  module           quantizes the                point        cloud                 tiple      heads        and      two       linear       layers        in  the      FFN.        The      number            of  operations              
                                                                                                                                                                                                                                                     2                         2
                       into     evenly          spaced          sparse        voxels         and       generates             many-to-one                 mapping                      in    MHSA           and      FFN        are     Óàª(Ì†µÌ±ëÌ†µÌ±öÌ†µÌ±õ ) and               Óàª(8Ì†µÌ±ëÌ†µÌ±ö) respectively,                       where          Ì†µÌ±ëÌ†µÌ±ö is  
                       between           the     points         and      their      corresponding                   voxels.        In  addition             we      em-               the     model          dimension               and       Ì†µÌ±õ is  number                 of  input          tokens.           The       vanilla         
                       ploy      dynamic            voxel        feature          encoding            (VFE)         (Wang          et  al.,      2020       ,  Yin     et             attention           (Vaswani              et  al.,       2017       )  based         transformer                models           are      often       
                       al.,   2021       )  to    transform            the     sparse         voxels        into      a  latent        space        representa-                       scaled        to  be  wider             (by      increasing             the     hidden           dimension              of  the       model)          
                       tion.     The       quantized            voxels         (tokens)          are     passed         to  the       DeLiVoTr             encoder                    or    deeper         (by      stacking           more        number            of  transformer                layers)         to  improve             
                       to   extract        the     voxel        features         by     the    encoder           layers.        As    the     relative         size     of            their      performance.                  However,             the     model         scaling        ( width        or   depth        or    both)       
                       objects        compared               to  large-scale              outdoor           environments                  is  small,        instead                   not     only       increases           the     model          parameters              and       operations             in  both         MHSA          
                       of   a  multi-scale               feature         map        representation,                  we  adopt             the     single-scale                       and      FFN       (and       complicates                learning)           but      also      employs            same        number            of  
                       feature         map       design         motivated              by  (Fan          et  al.,     2022        ),  maintaining                suÔ¨É-                 parameters               inside        each       transformer               layer        which         may        lead      to  sub        opti-
                       cient      receptive            Ô¨Åeld      by  the        attention           mechanism                (Vaswani             et  al.,     2017        ,          mal      solution.
                       Mehta         et  al.,       2021        ).  Each        DeLiVoTr               encoder            layer       consists           of  voxel                            Motivated             by  (Mehta              et  al.,      2021        ),  we  introduce                 Deep        and       Light-
                       intra-region              transformer               to  capture           the      voxel       local       features          (within          the              weight          Voxel         Transformer                (DeLiVoTr)                block        as  shown             in  Fig.         2    with      
                       region),         voxel        inter-region              transformer               to  capture            the     voxel        global        fea-               DeLighT            transformation                   (Mehta            et  al.,       2021        )   that     uses       group          linear        
                       tures      (among            the     regions)           and      the     intra- and             inter-region              voxel        fusion                  transformations                   (GLTs)          (Mehta           et  al.,       2018       )  with        expand           and      reduce          
                       module          to  eÔ¨Äectively                fuse      the     voxel        local       and      global        features.           The       en-              layers        to  vary        the     width        and      depth        of  the       network           eÔ¨Éciently.              The      width
                       coder       layer       is  repeated             Ì†µÌ∞ø number              of  times         with       model         ( width       and      depth                and      depth       scaling         with      DeLighT            transformation                  facilitates          learning          wider        
                       scaling)         and      layer-level           scaling         to  obtain          deep        voxel       features.           The      voxel                 representations                  eÔ¨Éciently              and      thus      enables          to  use       lesser      feature         dimen-
                       features         are      rasterized            to  a  BEV         feature          map       according             to  their         respec-                  sion      for     computing                of  attention              and       FFN.        Therefore,              we  can          replace          
                       tive     spatial        locations           and      the    BEV       feature         map        is  passed         to  the      detection                     the     MHSA           with       single-head               self-attention               (SHSA)           and       FFN       with       light-
                       head       for    3D      object        predictions.                                                                                                           weight          FFN,       reducing            total      number           of  parameters.                 In  addition            to  model
                                                                                                                                                                                      scaling,         similar         to  (Mehta            et  al.,      2021       ),  we     introduce             layer-level           scaling        
                       3.2.     Voxelization                                                                                                                                          of    DeLiVoTr             blocks        in  the       encoder           layers        that     allows         variable-sized                lay-
                                                                                                                                                                                      ers    instead          of  uniform            stacking           of  encoder            layers.        The      layer-level           scaling        
                              Voxelization               process          quantizes             the      point       cloud         into      evenly          spaced                   facilitates           eÔ¨Écient           allocation            of  parameters                with       shallower             and      narrow          
                       sparse        voxels        and      generates            voxel        coordinates              for    each       point       as  a  many-                     encoder           layers        near      the     input       and      wider        and      deeper          encoder          layers        near      
                                                                                                                                                                                3
                                  G.K. Erabati and H. Araujo                                                                                                                                                                                                                                                                                                           Intelligent Systems with Applications 22 (2024) 200361
                                  Fig. 1.               An      overview                    of  DeLiVoTr                          architecture.                        The          input             point            cloud             is  quantized                        into         voxels              (Sec.             3.2      ),   grouped                  into          diÔ¨Äerent                  regions                and          contextualized                             the         
                                  relationships                       among                voxel           features                in  intra-region                          transformer                        (Sec.           3.3.2         )  and         inter-region                       transformer                       (Sec.           3.3.3         )  (both             powered                  by       DeLiVoTr                    block            (Fig.           2 )).      The         
                                  intra- and                  inter-region                       voxel           features                are        fused           together                 and          rasterized                   to  a  BEV                feature              map           and         passed              to  the           detection                   head           (Sec.           3.4      )  to      predict              the        3D        objects.
                                                                                                                                                                                                                                                                              using            ‚åàÌ†µÌ±ÅÌ†µÌ±îÌ†µÌ±ôÌ†µÌ±° ‚åâ layers,                         where                 Ì†µÌ±Å              is    the         number                    of  GLT                layers.                2)  Reduc-
                                                                                                                                                                                                                                                                                                         2                                                              Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°
                                                                                                                                                                                                                                                                              tion         phase:                In  the             reduction                       phase               the         Ì†µÌ±ë             -dimensional                             voxel              features                   
                                                                                                                                                                                                                                                                                                                                                                                                          Ì†µÌ±öÌ†µÌ±éÌ†µÌ±•
                                                                                                                                                                                                                                                                              are       projected                       to  Ì†µÌ±ë -dimensional                                       space,               where                 Ì†µÌ±ë        =Ì†µÌ±ë ‚àï2, using                                the         re-
                                                                                                                                                                                                                                                                                                                                      Ì†µÌ±ú                                                                                          Ì†µÌ±ú              Ì†µÌ±ö
                                                                                                                                                                                                                                                                              maining                   Ì†µÌ±Å             ‚àí‚åàÌ†µÌ±ÅÌ†µÌ±îÌ†µÌ±ôÌ†µÌ±° ‚åâ layers.                             The           Ì†µÌ±ë -dimensional                                  voxel             features                   are        fed         
                                                                                                                                                                                                                                                                                                               Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                 2                                                    Ì†µÌ±ú
                                                                                                                                                                                                                                                                              to     SHSA               and          light-weight                           FFN           to      model                their           relationships.                              Ì†µÌ±Å             and          Ì†µÌ±§
                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                     Ì†µÌ±ö
                                                                                                                                                                                                                                                                              are       two           signiÔ¨Åcant                         parameters                           of  DeLighT                          transformation                                   to  scale                 the          
                                                                                                                                                                                                                                                                              depth             and          width               of     the          model.                 The            wider               representation                                 learning                    abil-
                                                                                                                                                                                                                                                                              ity      of  DeLighT                         transformation                                  allows                to  replace                      MHSA                  with            SHSA.                The           
                                                                                                                                                                                                                                                                              output               of  DeLighT                          transformation                                   with             group               linear               transformation                                   Óà≥
                                                                                                                                                                                                                                                                              for      a  given                 input             Ì†µÌ±ã is  Ì†µÌ±å = Óà≥(Ì†µÌ±ã).
                                                                                                                                                                                                                                                                              SHSA.                 The         Ì†µÌ±ëÌ†µÌ±ú-dimensional                                  voxel             features                   are         projected                       to  queries                      (Ì†µÌ±Ñ), 
                                                                                                                                                                                                                                                                              keys          (Ì†µÌ∞æ) and                     values                (Ì†µÌ±â ) using                      three              parallel                  linear               layers               and           scaled                
                                                                                                                                                                                                                                                                              dot-product                          attention                     is  calculated                           to  model                     the         relationships                             between                      
                                                                                                                                                                                                                                                                              diÔ¨Äerent                   voxel              features                  as  in  Eq. (                    1 ).  In  order                     to  facilitate                        the         skip          con-
                                                                                                                                                                                                                                                                              nections,                   the          Ì†µÌ±ëÌ†µÌ±ú-dimensional                                  output                 of  attention                           is  projected                           back              to  
                                                                                                                                                                                                                                                                              Ì†µÌ±ëÌ†µÌ±ö-dimensional                                 space              by       a  linear                 layer.
                                                                                                                                                                                                                                                                              SHSA             (Ì†µÌ±ã)=Attention                                  (Ì†µÌ±Ñ,Ì†µÌ∞æ,Ì†µÌ±â )=softmax                                          (Ì†µÌ±ÑÌ†µÌ∞æÌ†µÌ±á )Ì†µÌ±â                                                                          (1)
                                                                                                                                                                                                                                                                                                                                                                                                                 ‚àö
                                                                                                                                                                                                                                                                                                                                                                                                                        Ì†µÌ±ëÌ†µÌ±ú
                                                                                                                                                                                                                                                                                        The          computational                                   cost            of  attention                            in  vanilla                       transformer                             layer              
                                                                                                                                                                                                                                                                                                                                                                              2                                       2
                                                                                                                                                                                                                                                                              and         DeLiVoTr                        block              are        Óàª(Ì†µÌ±ëÌ†µÌ±öÌ†µÌ±õ ) and                            Óàª(Ì†µÌ±ëÌ†µÌ±úÌ†µÌ±õ ) respectively,                                            where                Ì†µÌ±ëÌ†µÌ±ú =
                                                                                                                                                                                                                                                                              Ì†µÌ±ëÌ†µÌ±ö‚àï2. Thus,                         attention                      in  DeLiVoTr                             block              requires                    2 √ó fewer                       computa-
                                                                                                                                                                                                                                                                              tions           than           vanilla                 transformer                           layer.
                                                                                                                                                                                                                                                                              Light-weight                               FFN             (L-FFN).                         The          dimension                          of  tokens                       in  the                vanilla                  
                                                                                                                                                                                                                                                                              transformer                          is  expanded                           by  a  factor                      of  4  in  FFN                        layers.               However,                        since             
                                  Fig. 2.              An       overview                    of  DeLiVoTr                         block             in  each               encoder                  layer            for       both           voxel                            the       DeLighT                      transformation                                  has         already                   facilitated                      the         wider               represen-
                                  intra-region                      attention                  and          voxel             inter-region                       attention.                    Layers              with           learnable                                   tation             of  voxel                    features,                    we  can                   employ                     a  light-weight                                FFN             layer              in  
                                  parameters                     ( Linear                and        DeLighT                  )  are        shown                in  color.                Ì†µÌ±ë         is   model               dimension,                                      the       DeLiVoTr                         block              by  reducing                            the          dimensionality                                  of  voxel                    features.                    
                                        Ì†µÌ±ô                    Ì†µÌ±ô                                                                                                                               Ì†µÌ±ö
                                  Ì†µÌ±Å         and         Ì†µÌ±§         are      given            in  Eq. (             2 ).  Adapted                    from           (Mehta                et  al.,          2021           ).                                                 SpeciÔ¨Åcally,                           the          Ô¨Årst            linear                layer              reduces                    the           Ì†µÌ±ë        -dimensional                              input              
                                       Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                 Ì†µÌ±ö                                                                                                                                                                                                                                                                                                                                                                         Ì†µÌ±ö
                                                                                                                                                                                                                                                                              to     Ì†µÌ±ë ‚àï4 dimension,                                        while               the         second                  linear               layer              expands                     the          Ì†µÌ±ë ‚àï4-
                                                                                                                                                                                                                                                                                           Ì†µÌ±ö                                                                                                                                                                                              Ì†µÌ±ö
                                                                                                                                                                                                                                                                              dimensional                           input              back            to  Ì†µÌ±ë              dimension.                         The          light-weight                            FFN           reduces                   
                                                                                                                                                                                                                                                                                                                                                                    Ì†µÌ±ö
                                  the       output.                  This           is  analogous                            to  CNNs                    (He          et  al.,            2016             )  with             shallower                                      the       number                    of      parameters                          by       a  factor                of      16      √ócompared                             to      standard                     FFN.
                                  and         narrower                      layers              near           the         input            and          deeper                 and          wider              layers              near           the                                  In     addition                    to  model                     scaling,                 similar                to  (Mehta                       et  al.,            2021             ),    we         in-
                                  output.                Each             DeLiVoTr                        block              consists                  of  DeLighT                          transformation                                  mod-                              corporate                     layer-level                        scaling               in  our               DeLiVoTr                         encoder                    layers.                The           two            
                                  ule,        SHSA               module                    and           light-weight                           FFN            module                    with            skip           connections                                           main             parameters                            of   DeLighT                            transformation:                                    number                      of   GLT                  layers               
                                  as     shown                 in  Fig.               2 .                                                                                                                                                                                     (Ì†µÌ±Å            ) and            the         width                multiplier                      (Ì†µÌ±§ ) facilitates                                 to  learn                 deep             and           wide             
                                                                                                                                                                                                                                                                                      Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                                                                                             Ì†µÌ±ö
                                  DeLighT                     transformation.                                          The          DeLighT                     transformation                                   (Mehta                   et  al.,                            voxel            feature                 representations.                                    Therefore,                        we         can          change                  these             parame-
                                  2021           )  basically                      helps              to  learn                  wide              and           deep              semantically                             rich           voxel                              ters        for        each             DeLiVoTr                        encoder                    layers               to  increase                        the         representational                                     
                                  features                 and            thus            enables                    to  use               lesser               dimension                          for         voxel               features                                   ability             of      voxel             features                  independent                             of      input             model                dimension                         (Ì†µÌ±ë        ) and            
                                  to     compute                      the          computationally                                      expensive                         attention                     mechanism.                              The                           output               dimension                         (Ì†µÌ±ë ) of  DeLighT                                  transformation.                                   Such             layer-level                Ì†µÌ±ö scal-
                                                                                                                                                                                                                                                                                                                                             Ì†µÌ±ú
                                  DeLighT                    transformation                                   (Mehta                   et  al.,             2021              )  uses             GLTs               with             feature                                 ing       is  not            possible                   in  standard                         transformer                            layers              because                   the         represen-
                                  shuÔ¨Ñing                     to  share                   information                             between                     diÔ¨Äerent                      groups                 and            it  is  for-                                tation             ability               of  MHSA                       and           FFN            formulated                          in  the              vanilla                 transformer                            
                                  mulated                   in  two               phases:                 1)  Expansion                             phase:                In  the             expansion                        phase               the                        layers             is  dependent                             on  input                    dimension                          (Ì†µÌ±ë ). The                      number                    of  GLT                 lay-
                                                                                                                                                                                                                                                                                                  Ì†µÌ±ô                                                                                   Ì†µÌ±ô                          Ì†µÌ±ö
                                  input            voxel             features                   with           dimension                         Ì†µÌ±ë          are       linearly                  projected                     to  a  high-                                   ers      (Ì†µÌ±Å              ) and            width               multiplier                       (Ì†µÌ±§ ) for  each                              DeLiVoTr                       encoder                    layer            Ì†µÌ±ô
                                                                                                                                                      Ì†µÌ±ö                                                                                                                                         Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                                                                                Ì†µÌ±ö
                                  dimensional                           space               Ì†µÌ±ëÌ†µÌ±öÌ†µÌ±éÌ†µÌ±• = Ì†µÌ±§Ì†µÌ±öÌ†µÌ±ëÌ†µÌ±ö, where                                            Ì†µÌ±§Ì†µÌ±ö is  the                       width                 multiplier,                                        is   given              as:
                                                                                                                                                                                                                                                                    4
                      G.K. Erabati and H. Araujo                                                                                                                                                                                               Intelligent Systems with Applications 22 (2024) 200361
                                                                                                                                Fig. 3.       An illustration of DeLiVoTr encoder layer.
                                                     (Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±éÌ†µÌ±• ‚àí Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±ñÌ†µÌ±õ)Ì†µÌ±ô
                      Ì†µÌ±ÅÌ†µÌ±ô      =Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±ñÌ†µÌ±õ +                Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°           Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                                                                                                 3.3.3.       Voxel        inter-region            transformer
                           Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°        Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                 Ì†µÌ∞ø ‚àí1                                                                                              (2)                    The       voxel         intra-region               transformer                obtains           voxel         local        features          
                                                 (Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±éÌ†µÌ±• ‚àí Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±ñÌ†µÌ±õ)Ì†µÌ±ô                                                                                                           within          each        region.          The       voxel        local        features          have         a   limited           recep-
                         Ì†µÌ±§Ì†µÌ±ô   =Ì†µÌ±§ +                  Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°           Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°    , 0 ‚â§ Ì†µÌ±ô ‚â§ Ì†µÌ∞ø ‚àí1                                                                                 tive     Ô¨Åeld       which          hinders          the      detection            of  large          size      objects.         In  order           
                            Ì†µÌ±ö           Ì†µÌ±ö          Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±ñÌ†µÌ±õ(Ì†µÌ∞ø ‚àí1)
                                                         Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                                                                                                                    to   increase           the      receptive            Ô¨Åeld       size      of  the        voxel        features,          we  employ                
                      where,         Ì†µÌ∞ø is  the         number            of  DeLiVoTr               encoder           layers,         Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±ñÌ†µÌ±õ and          Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±éÌ†µÌ±•              voxel       inter-region              transformer              to   obtain        voxel       global        features.          The      voxel       
                                                                                                                                            Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°           global        features          are     obtained           by  the        interaction             of  region          level       features          
                      are     the     minimum               and      maximum               number            of  GLTs         in  DeLighT              transfor-                   which         are     obtained           by  leveraging                the     voxel       feature         region         aggregation               
                      mation.          After       incorporating                layer-level           scaling        in  DeLiVoTr               encoder           the     
                                                                                                                                              ‚àëÌ†µÌ∞ø‚àí1             Ì†µÌ±ô                 module.
                      depth        of  DeLiVoTr                encoder           with        Ì†µÌ∞ø encoder              layers        is  2(         Ì†µÌ±ô=0 (Ì†µÌ±Å           +
                                                                                                                                                               Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°              Voxel         feature          region          aggregation.                  For     every        region         Ì†µÌ±ü,  we      aggregate             
                      4)) + Ì†µÌ∞ø (here,                2  is  because               each       DeLiVoTr              encoder           layer        consists          of             the     voxel        features          spatially           located          in  the        corresponding                  region.          For      
                      intra- and            inter-region             transformer               and      L  is  added           to  include           the     linear                a  given         region         Ì†µÌ±ü,  the     intra-region              voxel        features          are     given        by  Óà≤intra          =
                      layer       for     fusion        of  intra- and               inter-region              voxel        features),           whereas            in                   intra                                                                                                                           Ì†µÌ±ü
                                                                                                                                            Ì†µÌ±ô                Ì†µÌ±ô                   {Ì†µÌ±ì           }         .  We     aggregate             the    voxel        features         in  a  given          region         Ì†µÌ±ü as:
                      case      of  a  standard             transformer               the     depth        is  2(4Ì†µÌ∞ø) + Ì†µÌ∞ø. Ì†µÌ±Å                   and      Ì†µÌ±§      are                   Ì†µÌ±ñ         Ì†µÌ±ñ‚ààÓâÇÌ†µÌ±ü
                                                                                                                                           Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°             Ì†µÌ±ö                                              ‚àë
                      the     two      key      parameters              for    layer-level           scaling        facilitating            shallower            and               Ì†µÌ±ìÌ†µÌ±éÌ†µÌ±îÌ†µÌ±îÌ†µÌ±ü =        1              Ì†µÌ±ìintra                                                                                                   (4)
                      narrower            encoder           layers        near       the     input       and      deeper          and      wider        encoder                       Ì†µÌ±ü            |Ì†µÌ±âÌ†µÌ±ü| Ì†µÌ±ñ‚ààÓâÇ          Ì†µÌ±ñ
                      layers        near      the     output         thus      enabling           eÔ¨Écient           allocation            of  parameters.                                                           Ì†µÌ±ü
                                                                                                                                                                                          The      total      aggregated               voxel       features           for    a  given         input        LiDAR          point        
                      3.3.2.       Voxel        intra-region            transformer                                                                                                cloud        are     Óà≤ ={Ì†µÌ±ìÌ†µÌ±éÌ†µÌ±îÌ†µÌ±îÌ†µÌ±ü}                                .   The     indexes          of  the        regions          are      ÓàØ =
                      Voxel         region          grouping.              The      global        attention           among           all   the     voxel        fea-              [1, 2, ‚Ä¶ , Ì†µÌ±Ö].                      Ì†µÌ±ü         Ì†µÌ±ü‚àà[1,2,‚Ä¶,Ì†µÌ±Ö]
                      tures       is  computationally                     very      expensive             due      to  large        number            of  voxels                   Voxel         inter-region                attention.              The     aggregated               features         in  each         region         
                      in   an     outdoor          autonomous                 driving         environment.                 Instead,          we     divide        the              potentially            represent             the     semantics              of  the       corresponding                   region         in  a  
                      voxels        (ÓâÇ) into           non-overlapping                    regions          with       size     (Ì†µÌ±ü , Ì†µÌ±ü , Ì†µÌ±ü ) similar              to             latent       space.        We      can      formulate            voxel       inter-region              attention           mechanism                
                                                                                                                                   Ì†µÌ±•    Ì†µÌ±¶    Ì†µÌ±ß                                  for    global        interaction             of  voxel        features          among          all   the     regions.          Given        the     
                      (Fan       et  al.,     2022       ),  so  that       the     voxels        within         a  region         interact         with       each                aggregated              voxel       features          Óà≤ and         their       corresponding                 region         indexes          ÓàØ, 
                      other       to  maintain              the     required            receptive           Ô¨Åeld.       The       voxels         are     divided                   the     inter-region             attention           module           is  similar         to  Eq. (       3 )as:
                      into      regions         depending               upon        their      spatial         voxel        coordinates              (Ì†µÌ±ê ) as  il-
                      lustrated           in  Fig.        3 .  The      voxel        region         grouping            divides          the     voxels Ì†µÌ±ñ      into      
                                                                                    Ì†µÌ±†        Ì†µÌ±†Ì†µÌ±¶      Ì†µÌ±†                                                                               Óà≤‚Ä≤ =LN (SHSA                  (Óà≥(Óà≤,PE (ÓàØ)))+Óà≤)
                      total      number           of  regions            Ì†µÌ±Ö = Ì†µÌ±• √ó                 √ó Ì†µÌ±ß and            the     voxels         in  a  region           Ì†µÌ±ü                                                                                                                                                        (5)
                                                                                    Ì†µÌ±üÌ†µÌ±•      Ì†µÌ±üÌ†µÌ±¶      Ì†µÌ±üÌ†µÌ±ß                                                                       Óà≤inter       =LN (L-FFN              (Óà≤‚Ä≤)+Óà≤‚Ä≤)
                      are     given       by     ÓâÇ ={(Ì†µÌ±ê , Ì†µÌ±ì )}                  ,  where        Ì†µÌ±ü ‚àà[1, 2, ‚Ä¶ , Ì†µÌ±Ö], interact                       with      each       
                                                    Ì†µÌ±ü           Ì†µÌ±ñ    Ì†µÌ±ñ    Ì†µÌ±ñ‚ààÌ†µÌ±ü
                      other       in  the       intra-region             transformer.                                                                                                                 inter            Ì†µÌ±Ö√óÌ†µÌ±ë
                             As    the     LiDAR          point       clouds        are     sparse        in  nature          the    number            of  voxels                  where,         Óà≤            ‚àà‚Ñù Ì†µÌ±ö.
                      in   each       region        varies.        Similar         to  (Fan        et  al.,    2022       ),  we     group        the     regions                  Voxel         feature          propagation.                   The      voxel        inter-region             attention            features          
                      into      batches         with       similar        number           of  voxels         to  leverage            the     parallel         com-                (Óà≤inter      ) are      a  set      of  voxel          inter-region              attention           features          for     every        re-
                                                                                                                                                                                                                inter              inter                                             inter            1√óÌ†µÌ±ë
                                                                                                                                                                                   gion      Ì†µÌ±ü,  i.e.,     Óà≤            ={Ì†µÌ±ì             }                      ,  where        Ì†µÌ±ì           ‚àà‚Ñù Ì†µÌ±ö. However,                          
                      putation           capability.                                                                                                                                                                              Ì†µÌ±ü         Ì†µÌ±ü‚àà[1,2,‚Ä¶,Ì†µÌ±Ö]                           Ì†µÌ±ü         intra            |ÓâÇ |√óÌ†µÌ±ë
                                                                                                                                                                                   the     voxel        intra-region              features          for     a  region          Ì†µÌ±ü are      Óà≤             ‚àà‚Ñù Ì†µÌ±ü              Ì†µÌ±ö .  In   
                      Voxel         intra-region                attention.              As    the      voxel       features          are     grouped            into               order       to  fuse       the     intra- and           inter-region             voxel        features,     Ì†µÌ±ü  we     need       to  prop-
                      diÔ¨Äerent           regions          based        on  their         corresponding                  spatial        locations           by  the                                                                                                   inter            1√óÌ†µÌ±ë
                                                                                                                                                                                   agate       the     inter-region              voxel        features          (Ì†µÌ±ì           ‚àà‚Ñù Ì†µÌ±ö) to  all  |ÓâÇ | voxels                              
                      voxel       region         grouping            module,           we     can     employ           voxel       intra-region              atten-                                                                                                  Ì†µÌ±ü                                             Ì†µÌ±ü
                      tion      for    local       interaction              of  input          tokens         within         each        region.         Let     the               present         in  the        corresponding                  region         to  match           the      dimension              of  intra-
                                                                                                                                                                                                                                       intra            |ÓâÇ |√óÌ†µÌ±ë
                                                                                                                                                                                   region        voxel        features           (Óà≤             ‚àà‚Ñù Ì†µÌ±ü              Ì†µÌ±ö ).  In  order          to  achieve            this,      for     
                      voxel       features           in  a  region            Ì†µÌ±ü be  Óà≤ ={Ì†µÌ±ì }                          and     their       corresponding                                                                              Ì†µÌ±ü                                                                inter
                                                                                            Ì†µÌ±ü           Ì†µÌ±ñ  Ì†µÌ±ñ‚ààÓâÇÌ†µÌ±ü                                                                every        region        we      assign        the     voxel        inter-region             feature         (Ì†µÌ±ì           ) to  all      the     
                      voxel       coordinates              be    ÓàØ ={Ì†µÌ±ê }                   ,  the   voxel       intra-region              attention           pow-                                                                                                                                    Ì†µÌ±ü
                                                                    Ì†µÌ±ü          Ì†µÌ±ñ  Ì†µÌ±ñ‚ààÓâÇÌ†µÌ±ü                                                                                         |ÓâÇ | voxels             present           in  the       corresponding                   region         as  their         voxel        inter-
                      ered      by     DeLiVoTr             block       follows         the     pipeline          as:                                                                   Ì†µÌ±ü
                                                                                                                                                                                   region        features,          as:
                             Óà≤‚Ä≤ =LN (SHSA                  (Óà≥(Óà≤ ,PE (ÓàØ )))+Óà≤ )
                                Ì†µÌ±ü                                  Ì†µÌ±ü            Ì†µÌ±ü             Ì†µÌ±ü                                                                (3)             Óà≤inter       ={Ì†µÌ±ìinter         ‚àÄÌ†µÌ±ñ ‚àà ÓâÇ }                                                                                                     (6)
                      Óà≤intra       =LN (L-FFN               (Óà≤‚Ä≤)+Óà≤‚Ä≤)                                                                                                                   Ì†µÌ±ü                Ì†µÌ±ü                     Ì†µÌ±ü
                          Ì†µÌ±ü                                     Ì†µÌ±ü          Ì†µÌ±ü
                      where,          LN       is   Layer          Normalization,                   PE      is   Positional             Encoding              (Car-                3.3.4.       Intra- and            inter-region           voxel       feature        fusion
                      ion     et  al.,      2020        ),  SHSA         is  Single-Head                 Self-Attention,                 Óà≥ is  group             lin-                     The      voxel        local      (intra-region)               and      global         (inter-region)               features          are     
                      ear     transformations,                   L-FFN         is  Light-weight                Feed        Forward            Network            and               fused       in  the       feature         fusion         module           to  obtain          Ô¨Ånal       voxel        features          for     a  
                          intra            |ÓâÇ |√óÌ†µÌ±ë
                      Óà≤            ‚àà‚Ñù Ì†µÌ±ü              Ì†µÌ±ö is  intra-region               voxel       local      features.                                                           region        Ì†µÌ±ü as:
                          Ì†µÌ±ü
                                                                                                                                                                             5
                              G.K. Erabati and H. Araujo                                                                                                                                                                                                                                                                          Intelligent Systems with Applications 22 (2024) 200361
                              Óà≤ =MLP (Óà≤inter                                ‚äïÓà≤intra               )                                                                                                                        (7)                    Table 1
                                  Ì†µÌ±ü                            Ì†µÌ±ü                     Ì†µÌ±ü                                                                                                                                                         Model            details.
                              where,              ‚äïis  the                concatenation                            operator                 and         MLP           is  Multi-Layer                         Percep-                                                                                                      Waymo                                                   KITTI
                              tron.         The          Ô¨Ånal          voxel            features                obtained                  in  each              DeLiVoTr                     encoder                 layer           
                              are       passed              to  the           consecutive                        encoder                 layer           and         the        encoder                 layers            are                          Point cloud range - X-axis                [-74.88 m, +74.88 m]                     [0 m, 80.64 m]
                              repeated                 Ì†µÌ∞ø number                      of  times                with          layer-level                    scaling             to  obtain                 deep            se-                         Point cloud range - Y-axis                [-74.88 m, +74.88 m]                     [-40.32 m, +40.32 m]
                              mantic              voxel            features.                                                                                                                                                                           Point cloud range - Z-axis                [-2.0 m, +4.0 m]                               [-3.0 m, +1.0 m]
                                                                                                                                                                                                                                                       Voxel resolution                                  [0.32 m, 0.32 m, 6.0 m]                  [0.16 m, 0.16 m, 4.0 m]
                              3.4.        Decoder                                                                                                                                                                                                      Region shape                                          [12, 12, 1]                                           [24, 24, 1]
                                        The        sparse              semantically                          rich         voxel             features                 from            the        encoder                  are                      on      two         RTX           4090           GPUs.              We         apply            fade          strategy               by       disabling                  copy-and-
                              rasterized                  to  a  BEV                   feature               map           according                     to  their               respective                     spatial                           paste          augmentation                             (Yan           et  al.,          2018           )  for       the        last        quarter                epochs.                The         
                              locations.                  The          spatial              locations                   with           no  voxels                    are         Ô¨Ålled            with            zeros.                          model             is  trained                  with           AdamW                    (Loshchilov                       &  Hutter,                   2017            )  optimizer                    
                                                                                                                                                                                                                                                                                                                                                ‚àí5
                              We        employ                 3  √ó 3  convolutions                                    on  the             BEV           feature              map            to  Ô¨Åll          out         the                     with         initial           learning                 rate         of  1 √ó10                        and        weight               decay             of  0.05.
                              holes.           For        the        detection                  head           in  the          decoder                 we        employ                the       CenterPoint                                     4.2.       Results               and        discussion
                              (Yin         et  al.,         2021           )  head           which              predicts                class-speciÔ¨Åc                        heatmap,                    sub-voxel                   
                              location               reÔ¨Ånement                       (Ì†µÌ±ú ‚àà ‚Ñù2), height                             above             ground                (‚Ñé ‚àà‚Ñù), object                               size         
                                                3                                                                                                                                 Ì†µÌ±î    2                                                         4.2.1.          Quantitative                        results
                              (Ì†µÌ±† ‚àà ‚Ñù ) and                        yaw           rotation                 angle            (sinÌ†µÌªº, cosÌ†µÌªº) ‚àà[‚àí1, 1] , each                                                    with           its                   Waymo                  dataset.                   The         comparison                        of  our            DeLiVoTr                     approach                    with           the        
                              own head.                     We         use        the        Gaussian                   Focal            Loss          (Law            &  Deng,                2018            )  as      the                     state-of-the-art                          approaches                       on  the             Waymo                   (Sun           et  al.,          2020           )  val         set     in  
                              classiÔ¨Åcation                        loss         (Óà∏             ) and           L1  loss             as  the             regression                    loss         (Óà∏              ).  The           
                              Ô¨Ånal          loss        is  the          weighted Ì†µÌ±êÌ†µÌ±ôÌ†µÌ±†            sum          of  both              classiÔ¨Åcation                        and          regression        Ì†µÌ±üÌ†µÌ±íÌ†µÌ±î      loss:                      terms           of  LEVEL_1                       and          LEVEL_2                    AP/APH                    is  as  shown                      in  Table                   2 .  Our           
                                                                                                                                                                                                                                                  method                achieves                  72.8           APH,            70.2           APH            and          66.8           APH            for       LEVEL_1                   ve-
                              Óà∏=Ì†µÌªº Óà∏ +Ì†µÌªº Óà∏                                                                                                                                                                                 (8)                    hicle       ,  pedestrian                   and        cyclist             categories                   respectively,                        and         64.5          APH,             63.4          
                                              Ì†µÌ±êÌ†µÌ±ôÌ†µÌ±†     Ì†µÌ±êÌ†µÌ±ôÌ†µÌ±†           Ì†µÌ±üÌ†µÌ±íÌ†µÌ±î      Ì†µÌ±üÌ†µÌ±íÌ†µÌ±î                                                                                                                                                      APH          and         64.2          APH            for      LEVEL_2                   vehicle           ,  pedestrian                   and        cyclist            categories                   
                              where,              Ì†µÌªº           and        Ì†µÌªº            are      balancing                    coeÔ¨Écients.
                                                      Ì†µÌ±êÌ†µÌ±ôÌ†µÌ±†                  Ì†µÌ±üÌ†µÌ±íÌ†µÌ±î                                                                                                                                                              respectively.                       It  surpasses                     the        well         established                      approaches                        such           as  SEC-
                              4.     Experiments                                                                                                                                                                                                  OND           (Yan           et  al.,           2018           ),   PointPillars                      (Lang             et  al.,          2019           )  and          VoTr-SSD                     
                                                                                                                                                                                                                                                  (Mao           et  al.,         2021           )  by       good            margin               for       vehicle            ,  pedestrian                   and        cyclist            cat-
                                        We       evaluate                  our         DeLiVoTr                     model              on  large-scale                          publicly                 available                                egories.              Pedestrian                    detection                 in  an  autonomous                                  driving              scenario                 is  more              
                              autonomous                         driving               datasets:                  Waymo                  Open             Dataset                (Sun           et  al.,          2020           )                challenging                      due         to  small                size        and          non-rigid                  nature               of  pedestrians.                          Our          
                              and        KITTI             dataset               (Geiger               et  al.,          2012           ).   In  this            section,                we        provide                the                     method                surpasses                   CenterPoint                         (Yin          et  al.,           2021            )  and          SST          (Fan           et  al.,           
                              implementation                               details,              present               the        quantitative                       and         qualitative                     results                          2022          )  on       small            object             category                  like        pedestrian                    and        achieves                  a  competi-
                              and        ablation                 studies.                                                                                                                                                                        tive       performance                          on       vehicle             and        cyclist            categories.
                                                                                                                                                                                                                                                           The         intra-region                       attention                   mechanism                         in  our             model               obtains                voxel            
                              4.1.        Implementation                             details                                                                                                                                                      local         features                which             hugely               increases                  the        performance                         of  small               objects.               
                                                                                                                                                                                                                                                  As     our         network                 maintains                    the        same           scale           of  voxel              feature               maps            without                
                                                                                                                                                                                                                                                  any        downsampling,                                there            is  no  semantic                            information                        loss         which               also         
                              Waymo                  dataset.                   The         Waymo                  Open            Dataset                (Sun           et  al.,          2020           )  consists                             helps          in  the             increase                 of  performance                               of  small                 objects,                such           as  pedes-
                              of     798,          202          and         150          scenes             for       training,                 validation                    and         testing              respec-                            trians        .  Although                    the        scale           of  the            voxel            feature               maps             remains                 constant,                  
                              tively.           Each            consists                of  more                than          200 K              LiDAR               point            clouds              collected                               the      receptive                   Ô¨Åeld          is  increased                      by  leveraging                          the       inter-region                       attention                  
                              by      LiDAR                at  10  frames                         per         second               (FPS).              Each            point            cloud             covers               a                  mechanism                       which               helps            to  maintain                       the         performance                           of  vehicle                   cate-
                              scene           of  150 m                  √ó 150 m                   area.          vehicles             ,  pedestrians                    and         cyclist            categories                                gory.
                              are       used          for       the        evaluation.                                                                                                                                                            KITTI            dataset.                    We        compare                   our         DeLiVoTr                      method                 with            the        state-of-
                              KITTI             dataset.                 The         KITTI            dataset               (Geiger               et  al.,         2012           )  consists               of  3,712                             the-art            approaches                       on       the        KITTI            (Geiger                et  al.,         2012           )  val        set     in  terms                of  
                              and        3,769             samples                 for       training                and         validation.                     Each           sample                consists               of                   Ì†µÌ∞¥Ì†µÌ±É                  for      small            size        object             categories                    such           as  pedestrian                       and        cyclist            as     
                                                                                                                                                                                                                                                          Ì†µÌ∞µÌ†µÌ∞∏Ì†µÌ±â
                              a  LiDAR                point            cloud             captured                  by  a  64-beam                            LiDAR               sensor.              Small             size                      shown              in  Table                3 .  Our           method                 outperforms                         Complex-YOLO                                (Simony                  et  
                              objects             like        pedestrian                    and        cyclist            are       used          for       the        evaluation.                                                                al.,     2018           ),   Li3DeTr                 (Erabati               &  Araujo,                  2023           ),   SECOND                   (Yan           et    al.,      2018           )  
                              Evaluation                        metrics.                      We         follow               the         oÔ¨Écial                 evaluation                       protocol                  of                    and        VoxelNet                   (Zhou             &  Tuzel,                2018          )  for       easy        ,  moderate                   and        hard          samples                
                              Waymo                 Open            Dataset               to     calculate                 3D        AP       and         3D       APH           (average                 precision                               for      small           size        pedestrian                    class.         Our          method                surpasses                   on       easy          samples                of  
                              weighted                   by  heading)                        for       three            categories.                     0.7        is  set          as  IoU             threshold                                 cyclist           class         and         obtains               competitive                       performance                          on       moderate                   and        hard
                              for      vehicle              class         and         0.5       is  set         as  IoU             threshold                   for       pedestrian                   and         cyclist                        samples                of  cyclist               category.
                              classes.             There              are         two          diÔ¨Äerent                  levels             of  evaluation                          depending                       upon                          Variants                  of  DeLiVoTr.                             As      discussed                   in  Sec.               3.3.1         ,  we         can         assign             dif-
                              number                 of  LiDAR                  points             in  the           object.             The          objects              with           more            than          Ô¨Åve                       ferent           values              to  two              signiÔ¨Åcant                     parameters                        (width               multiplier                    Ì†µÌ±§           and        
                                                                                                                                                                                                                                                                                                                                                                                                                                      Ì†µÌ±ö
                              LiDAR              point           are        considered                      as  LEVEL_1                       and         objects              with           less        than          Ô¨Åve                       number                of  GLT              layers             Ì†µÌ±Å           )  of  our             DeLiVoTr                    block             to  model                  the        depth
                                                                                                                                                                                                                                                                                                                       Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°
                              LiDAR              points            are        considered                     as     LEVEL_2.                   For        KITTI            dataset,               we       compute                                and        width             scaling              of  our            model.                The          increase                 of  width                 by  width                   mul-
                              mAP           with           IoU        threshold                   0.5        for       pedestrian                    and         cyclist             categories.                                                  tiplier          does            not        have            eÔ¨Äect             on  the              number                  of  attention                      operations                      in  
                              Model               details.                  The        input            data           processing                     such            as  point                cloud             range,                           our       model               (thanks                to  DeLighT                        transformation),                                whereas                  the         increase                 
                              voxel           resolution                    of  LiDAR                    point            clouds              for        diÔ¨Äerent                  datasets                 is  given                             of    width              of  the             model               increases                    the        number                   of  attention                       operations                      
                              in     Table             1 .  The           DeLiVoTr                    block            is  set         with           model              dimension                      (Ì†µÌ±ë        ) 128,                         in    a  standard                     transformer                        network                  (Vaswani                    et  al.,          2017            ).   Along             with           
                                                                                                                    Ì†µÌ±öÌ†µÌ±ñÌ†µÌ±õ                       Ì†µÌ±öÌ†µÌ±éÌ†µÌ±•                                                        Ì†µÌ±ö                                 model             scaling             the        layer-level                    linear           scaling              with           Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±ñÌ†µÌ±õ for               Ô¨Årst          encoder                 
                              width            multiplier                    (Ì†µÌ±§ ) 2  and                    Ì†µÌ±Å               and         Ì†µÌ±Å                varying               from           2  to  8  to  ob-                                                                                                                                                                            Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°
                                                                                     Ì†µÌ±ö                             Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                      Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                                                                                            layer          and          Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±éÌ†µÌ±• for                 last        encoder                  layer            obtains                eÔ¨Écient                   allocation                    of  
                              tain        models               with          diÔ¨Äerent                  complexity                      as  shown                  in  Table                4 .  The          number                                                                  Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°
                              of     DeLiVoTr                    encoder                 layers            (Ì†µÌ∞ø) is  set               to  6.  The                detection                   head           parame-                               parameters                      with           shallower                    and          narrow                encoder                  layers             near           the        input            
                              ters       follow              CenterPoint                       (Yin          et  al.,         2021           ).                                                                                                   and        deeper                and          wider              encoder                  layers             near           the         output.                The           parame-
                              Training                   details.                  Our        DeLiVoTr                     model               is  implemented                               based              on  the                           ters       (Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±ñÌ†µÌ±õ, Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±éÌ†µÌ±•and                          Ì†µÌ±§Ì†µÌ±ö) of  DeLiVoTr                                block            allow us                   to  increase                      the        
                              MMDetection3D                                 (MMDetection3D                                    contributors,                         2020           )  codebase                    in  Py-                                             Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°           Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°
                                                                                                                                                                                                                                                  learnable                 parameters                        independently                             of  model                   dimension                      Ì†µÌ±ë          and        thus          
                              Torch            (Paszke                et  al.,           2019           ).   We         train           the        model              for       24  and               40  epochs                                                                                                                                                                                                        Ì†µÌ±ö
                                                                                                                                                                                                                                                  we       deÔ¨Åne             three           variants                of  our           network                 DeLiVoTr_base,                              DeLiVoTr_small                               
                              with          a  batch              size        of  2  and              4  for         Waymo                  and         KITTI             datasets                respectively                                    and        DeLiVoTr_large                              as  shown                  in  Table                 4 .
                                                                                                                                                                                                                                         6
                              G.K. Erabati and H. Araujo                                                                                                                                                                                                                                                                          Intelligent Systems with Applications 22 (2024) 200361
                                                                            Table 2
                                                                            Comparison                     of  recent              works           on      Waymo                val      dataset            (Sun         et  al.,        2020         ).
                                                                                                                                                                                             Vehicle                                                             Pedestrian                                                              Cyclist
                                                                                 Method                                                                               LEVEL_1                       LEVEL_2                       LEVEL_1                       LEVEL_2                       LEVEL_1                       LEVEL_2
                                                                                                                                                                      AP/APH                        AP/APH                        AP/APH                         AP/APH                        AP/APH                        AP/APH
                                                                                 SECOND (Yan et al.,                            2018       )                         72.8/71.7                   63.9/63.3                    68.7/58.2                   60.7/51.3                    60.6/59.3                   58.3/57.0
                                                                                 PointPillars (Lang et al.,                             2019       )                 72.1/71.5                   63.6/63.1                    70.6/56.7                   62.8/50.3                    64.4/62.3                   61.9/59.9
                                                                                 VoTr-SSD (Mao et al.,                             2021       )                      69.0/68.4                   60.2/59.7                    -                                      -                                      -                                      -
                                                                                 LaserNet (Meyer et al.,                             2019       )                    56.1/-                          -/48.4                           62.9/-                           -/45.4                           -                                      -
                                                                                 Pillar-OD (Wang et al.,                             2020       )                    69.8/-                          -                                       72.51/-                        -                                      -                                      -
                                                                                 CenterPoint (Yin et al.,                            2021        )                    74.8      / 74.2                  66.7            / 66.2                       75.8/69.7                    68.3/62.6                                          71.3       / 70.2                  68.7           / 67.6
                                                                                 SST (Fan et al.,                   2022       )                                     74.2/73.8                   65.5/65.1                    78.7/69.6                   70.0/61.7                    -                                      -
                                                                                 DeLiVoTr (Ours)                                                    73.4/72.8                   65.0/64.5                                                            79.2       / 70.2                  71.7            / 63.4                       68.5/66.8                    65.9/64.2
                                                                            Table 3
                                                                            Comparison                     of  recent              works           in  terms             of  Ì†µÌ∞¥Ì†µÌ±É                   detection                on     KITTI           val       set    (Geiger             et  al.,        2012         ).
                                                                                                                                                                                         Ì†µÌ∞µÌ†µÌ∞∏Ì†µÌ±â
                                                                                 Method                                                                                                                     Pedestrian (IoU=0.5)                                                                        Cyclist (IoU=0.5)
                                                                                                                                                                                   Easy                 Mod.                  Hard                 Overall                 Easy                 Mod.                 Hard                 Overall
                                                                                 Complex-YOLO (Simony et al.,                                          2018       )                46.0                 45.9                  44.2                  45.4                       72.3                 63.3                  60.2                  65.3
                                                                                 Li3DeTr (Erabati & Araujo,                                   2023        )                        56.5                 50.0                  45.2                  50.6                       80.5                 64.0                  60.3                  68.3
                                                                                 SECOND (Yan et al.,                            2018       )                                       56.5                 52.9                  47.7                  52.4                       80.5                                           67.1                  63.1                            70.2
                                                                                 VoxelNet (Zhou & Tuzel,                                 2018        )                             65.9                 61.0                  56.9                  61.3                       74.4                 52.1                  50.4                  59.0
                                                                                 DeLiVoTr (Ours)                                                                                   75.2                 69.6                  64.4                  69.7                      87.6                                            64.6                  61.3                            71.2
                                                                            Table 4
                                                                            Comparison                     of  performance                         (LEVEL_1                  AP)        and        inference                speed           (FPS)          of  diÔ¨Äerent                  variants              of  our          DeLiVoTr                  network               on  a  RTX                
                                                                            4090          GPU          for      diÔ¨Äerent               categories.                 Using           20%          of  training               data         on      Waymo               dataset             (Sun         et  al.,        2020         ).
                                                                                 Model                                                      Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±ñÌ†µÌ±õ              Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±éÌ†µÌ±•               Ì†µÌ±§Ì†µÌ±ö              #param.             Memory (GB)                Vehicle            Pedestrian            Cyclist           Speed (FPS)
                                                                                                                                                 Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°                Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°
                                                                                 SST (Fan et al.,                   2022       )           -                    -                     -                 1.60M                  4.0                                  67.9                  70.9                          -                       17.0
                                                                                 DeLiVoTr_small                           2                    3                    1                                      1.56M                  3.7                                             68.3                   74.8                         62.8                                20.5
                                                                                 DeLiVoTr_base                             2                    4                    2                1.76M                  3.8                                  69.1                   75.7                         63.6                 19.5
                                                                                 DeLiVoTr_large                            4                   8                     2               2.47M                  3.9                                                                   70.0                  76.5                         65.1                                 17.0
                                        The         DeLiVoTr_small                                 model                 eÔ¨Éciently                       allocates                   the          parameters                                      the      receptive                  Ô¨Åeld          by       the       attention                  modules.                  The         high          performance                          gain         
                              among               the         diÔ¨Äerent                   encoder                  layers             with            1.56M                parameters                        (occupy                               for      pedestrians                      far      away             from            ego         vehicle                is  possible                    due          to  the            long           
                              3.7       GB  of  GPU                      memory),                      not        only           achieves                 improved                     performance                           in                   range           context                information                       provided                   by  our             inter-region                       attention                  mod-
                              terms            of  LEVEL_1                      AP  compared                            to  SST              but        also          achieves                 20.5           FPS          in-                    ule.
                              ference              speed              (20%             more             than           SST).            To  improve                        the         performance                           of                   4.2.2.          Ablation                  studies
                              our        network,                   we  have                  the        possibility                   to  adapt                 the         width             scaling               (Ì†µÌ±§ )
                              and        layer-level                   depth             scaling             (Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±ñÌ†µÌ±õand                Ì†µÌ±ÅÌ†µÌ±öÌ†µÌ±éÌ†µÌ±•) of  our                     network                   to  obtain Ì†µÌ±ö                            Voxel             resolution.                          We        compare                    the         performance                           of  our               network                   in  
                              DeLiVoTr_base                             and          DeLiVoTr_large                   Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°     variants Ì†µÌ±îÌ†µÌ±ôÌ†µÌ±°          as  shown                   in  Table                  4 .  The                         terms           of  LEVEL_1                      AP        for       diÔ¨Äerent                  voxel            resolutions                     on  Waymo                       dataset               
                              three          variants                 of  our            deep            and         light-weight                        voxel             transformer                       operate                              (Sun         et  al.,          2020          )  as      shown               in  Table                7 .  The          decrease                  in  voxel               resolution                   
                              above             17      FPS         at  real-time                     (Remember,                         the        LiDAR              in  the           Waymo                 dataset                            has       slight           impact               in  performance                              of  our            network.                   The         optimized                     voxel            
                              (Sun          et  al.,         2020           )  operates                  at  10          Hz).                                                                                                                     resolution                  is  based               on       the       accuracy-speed                              trade-oÔ¨Ä                 requirement                         for       spe-
                              Performance                            by       object              distance.                    We        compare                   the       performance                          of  our                         ciÔ¨Åc        application.                       0.32 m               √ó 0.32 m                   voxel            resolution                   works             better            for       our        
                              DeLiVoTr                    model              by  object                  distance                 for       vehicle              category                on  the             Waymo                                method                as  a  trade-oÔ¨Ä                       between                  performance                          and         eÔ¨Éciency.
                              dataset              (Sun           et  al.,           2020            )  in  terms                 of  LEVEL_1                        APH            as  shown                    in  Ta-                          Region                shape.                We        compare                   the        performance                          of  our             network                  in  terms                
                              ble       5 .  The           vehicles                are       split         into         three           categories                   basing              on       the       distance                              of    LEVEL_1                    AP  for  diÔ¨Äerent                              region               shapes               as  shown                    in  Table                   8 .  The           
                              between                 object              center              and         ego          vehicle:                [0~m,30~m], [30~m,50~m]                                                                            increase               in  region                   size          increases                   the        receptive                    Ô¨Åeld           but         at  the             same             
                              and        [50~m,+ ‚àû ]. Our  model                                                         show             improvements                              of  3.1             APH,             4.2                      time         the        computational                             complexity                       of  the           model              increases                  as  the            num-
                              APH           and         6.2        APH           for       0-30 m,                 30-50 m                   and         50 m-Inf                  respectively.                        The                       ber       of  voxels                  in  a  region                     increases.                   12  √ó 12  region                               shape             (3.84 m                  √ó
                              inter-region                      attention                   mechanism                        which               contextualize                          long           range             de-                      3.84 m)               works             better            for      our        approach                    for      performance-eÔ¨Éciency                                             trade-
                              pendency                    between                  diÔ¨Äerent                  regions               achieves                 high          gain          in  performance                                           oÔ¨Ä.       Large            local           regions               help to              obtain              large           context               information                        which              
                              (6.2        APH)             for       vehicles              which             are        far      away             from           ego        vehicle.                                                              aid      for        the        performance                           of  small                 objects               like         pedestrians                   .  The          perfor-
                                        We       compare                    the        performance                           of  our            model               by  object                   distance                 for                     mance              of     large          size        objects              like       vehicles              is    maintained                      by       the       inter-region                      
                              pedestrian                  category                 on       the        Waymo                 dataset               (Sun          et  al.,          2020           )  in     terms            of                   attention                 mechanism                        of  our           model.
                              LEVEL_1                  AP  as  shown                         in  Table                  6 .  The           pedestrians                       are        split          into         three                         Attention                    mechanism.                              We        compare                   the        performance                          of  our             network                  
                              categories                   basing               on  the              distance                 from            ego         vehicle,                similar               to  vehicle                               in    terms             of  LEVEL_1                      APH            for       diÔ¨Äerent                 attention                  mechanisms                          as  shown                   
                              category.                  Our         model              achieves                  an  improvement                                 of  0.9           AP        and          5.7       mAP                          in    Table               9 .   The           number                  of  operations                           in  vanilla                   MHSA                (Vaswani                     et  
                                                                                                                                                                                                                                                                                                                                         2                                   2
                              for      30-50 m                    and         50 m-Inf                    respectively.                        The          improvement                             in  the            per-                       al.,     2017            )  and           FFN           are         Óàª(Ì†µÌ±ëÌ†µÌ±öÌ†µÌ±õ ) and                         Óàª(8Ì†µÌ±ëÌ†µÌ±ö) respectively,                                       where               Ì†µÌ±ëÌ†µÌ±ö
                              formance                   of  small                objects               ( pedestrians                    )  is  attributed                       to  two              important                                   is   the        model               dimension                      and          Ì†µÌ±õ is  number                       of  input                tokens.               To  improve                        
                              characteristics                          of  our           network:                   maintaining                        the       same            scale           of  voxel              fea-                      the      performance                          of  vanilla                 transformer                       (Vaswani                    et  al.,         2017           ),   we        need           
                              ture        maps            without                 any        downsampling                              and         at  the          same            time          maintaining                                     to    either             increase                 the         model              dimension                       ( width              scaling)              or  stack                more             
                                                                                                                                                                                                                                         7
                      G.K. Erabati and H. Araujo                                                                                                                                                                                               Intelligent Systems with Applications 22 (2024) 200361
                                                                                           Table 5
                                                                                           Performance              of  our      model        by  object        distance         for    vehicle      category         on  the       Waymo          dataset        
                                                                                           (Sun      et  al.,    2020      )  in  terms       of  LEVEL_1           APH.       The     scores       in  green        indicate        the    increase         in  
                                                                                           performance              with     respect        to  scores       in  underline          .
                                                                                               Method                                                             [0m,30m]                           [30m,50m]                            [50m,+‚àû]
                                                                                               LaserNet (Meyer et al.,                2019    )                   68.7                           51.4                             28.6
                                                                                               PointPillars (Lang et al.,               2019    )                 84.4                           58.6                             35.2
                                                                                               StarNet (Ngiam et al.,               2019    )                     82.4                           53.2                             25.7
                                                                                               SECOND (Yan et al.,                2018    )                       87.5                           64.6                             39.6
                                                                                               VoTr-SSD (Mao et al.,                2021    )                     87.6                               66.1                                 41.4
                                                                                               DeLiVoTr (Ours)                                       90.7                  ‚Üë 3.1                     70.3    ‚Üë 4.2                        47.6    ‚Üë 6.2
                                                                                           Table 6
                                                                                           Performance              of  our     model       by    object      distance        for   pedestrian         category         on    the    Waymo          dataset       
                                                                                           (Sun      et  al.,    2020      )  in  terms        of  LEVEL_1            AP.     The     scores       in  green         indicate       the     increase         in  
                                                                                           performance              with     respect        to  scores       in  underline          .
                                                                                               Method                                                             [0m,30m]                           [30m,50m]                            [50m,+‚àû]
                                                                                               RSN (Sun et al.,           2021    )                               83.9                           74.1                             62.1
                                                                                               PointPillars (Lang et al.,               2019    )                 72.5                           71.9                             63.8
                                                                                               SST (Fan et al.,          2022    )                                85.3                           77.0                             63.4
                                                                                               DeLiVoTr (Ours)                                       83.9                           77.9                     ‚Üë 0.9                        69.1    ‚Üë 5.7
                                          Table 7                                                                                                                                  scaling         in  the      DeLiVoTr            block        learns       the     wider        and      deeper         voxel       feature         
                                          Performance             of  our       network         in  terms         of  LEVEL_1            AP  for                                   representation                 which         improves             the     performance                 as  shown            in  Table           9 .  
                                          diÔ¨Äerent         voxel      resolutions.          Using       20%       of  training        data.                                        In   addition            to  model           scaling        we  also          employ           layer-level            scaling        of  De-
                                             Voxel resolution                                            LEVEL_1 AP                                                                LiVoTr         encoder           layers        for    eÔ¨Écient           parameter             allocation            with       shallower            
                                                                                   Vehicle          Pedestrian          Cyclist                                                    and      narrower            encoder           layers        near       the     input       and       deeper         and      wider         en-
                                             0.36                               68.0               74.5                     62.5                                                   coder        layers       near      the     output.         Such       layer-level          scaling        is  not      possible         with       
                                             0.32                                  69.1                    75.7                    63.6                                            vanilla        transformer               layers       (Vaswani             et  al.,     2017       )  because          the     represen-
                                             0.28                                  69.2                    75.6                    63.5                                            tation       ability        of  MHSA             and      FFN       formulated              in  the       vanilla         transformer               
                                                                                                                                                                                   layers       is  a  function               of  model            dimension.              As  our          model         learns         wider         
                                                                                                                                                                                   representation                 in  DeLighT               transformation,                   we  can          replace           FFN       with        
                                          Table 8                                                                                                                                  light-weight              FFN        and       the     depth         (learnable             layers)         of  each          DeLiVoTr              
                                          Performance             of  our       network         in  terms         of  LEVEL_1            AP  for                                   layer       is  more         than       the     depth        of  vanilla           transformer              layer       by  the        num-
                                          diÔ¨Äerent         region       shapes.       Using       20%       of  training        data.                                              ber     of  GLT         layers.        Thus,        our      deep        and      light-weight               voxel        transformer               
                                                                                                       LEVEL_1 AP                                                                  achieves           better        performance                 as  shown             in  Table           9 ,  in  terms           of  lesser          
                                             Region shape                                                                                                                          number           of  parameters,                lesser       GPU        memory            allocation            and      higher        infer-
                                                                                Vehicle            Pedestrian           Cyclist                                                    ence       speed.
                                             9   √ó9                            68.0                 74.5                      62.4
                                             12    √ó12                        69.1                       75.7                          63.6                                        4.2.3.       Qualitative             results
                                             15    √ó15                        69.0                       75.8                          63.5                                               The      qualitative             results        of  our        DeLiVoTr              network            are     illustrated            in  
                                                                                                                                                                                   Fig.      4 .  While        maintaining                the     same        scale       of  voxel         feature         maps         to  aid       
                      number           of  transformer                 layers         ( depth        scaling).         However,              this     not      only                the     detection            of  small          size      objects,          such       as  pedestrians              ,   the    attention            
                      increases           the     number           of  parameters                of  network             but     also      allocates          same                 mechanism                (DeLiVoTr             block        (Sec.       3.3.1     ))   in  our      intra- and            inter-region              
                      number           of  parameters                in  each        transformer               layer      leading          to  sub       optimal                   transformer              module            helps        to  maintain               the     receptive            Ô¨Åeld.        This       is  il-
                      solution.                                                                                                                                                    lustrated          by  detection               of  not        only       small        size      objects         ( pedestrians           )  but      
                             An     eÔ¨Écient           attention           mechanism               (FlashAttention                  (Dao       et  al.,     2022      ))            also     large       size     objects         ( vehicles       )  as   shown          in  Fig.       4 .  A  short        demo         video        
                      is   proposed             to  tackle           the      quadratic             complexity               of  the        self-attention                         demonstrating                  the      predictions              of  our         DeLiVoTr              network            on  Waymo                 
                      mechanism                by  reducing               the      memory             read/write              cycles         of  GPU          high                 Open        Dataset          is  presented             at  https://youtu                 .be   /jnYDfZbVsjw                 .
                      bandwidth               memory            (HBM).          FlashAttention                  is  developed              mainly         on     two               5.    Conclusion
                      ideas:       tiling      and     recomputation               ,  to   not     only      speed        up    the     attention           mech-
                      anism         but      also      to  reduce            the      memory             complexity               from        quadratic             to                    In   this     paper,         we     develop          a  LiDAR-based                  feature        extraction            network            
                      linear.        In  our       case,       we  replace             the      standard           attention           (Vaswani             et  al.,               which         not     only      maintains             the     same       scale       of  voxel        feature         maps        but     also      
                      2017       )  with      the     FlashAttention                 in  the       standard           transformer              encoder           lay-              the     receptive            Ô¨Åeld       of  voxel         features          to  aid       the      detection            of  small         size      
                      ers    and       we  observe              that      the     performance                 of  FlashAttention                    is  slightly                   objects         in  an  autonomous                    driving         scenario,           such      as  pedestrians             .  We     pro-
                      similar         to  MHSA            except         for    the      lower        GPU        memory            allocation.             This      is            pose       deep       and      light-weight              voxel       transformer               (DeLiVoTr)              network           with       
                      may       be  due          to  lesser         number            of  voxels           (~150)            (sequence             length)          in             voxel       intra- and            inter-region              transformer              modules            to  obtain          voxel        local      
                      each       local      region         and       FlashAttention                 shows          improved             inference            speed                 and      global        features         respectively.              We      introduce            DeLiVoTr             block       which         is  
                      for    larger        sequence            lengths         ( > 512).                                                                                           the     core      module           of  our       network           with       transformations                   to  vary        the     width
                             In    our     DeLiVoTr              attention           mechanism                we  employ               width         and     depth                 and      depth       of   network           eÔ¨Éciently.             This      enables          to  learn        wider        and      deeper         
                      scaling         in  the        DeLighT            transformation                   so  that         the      SHSA         mechanism                 
                                    2                                                                                                                                              voxel       representations                  and       therefore           we     can      use     smaller          feature         dimen-
                      (Óàª(Ì†µÌ±ëÌ†µÌ±úÌ†µÌ±õ ), where                Ì†µÌ±ëÌ†µÌ±ú = Ì†µÌ±ëÌ†µÌ±ö‚àï2) operates                    at  a  reduced               factor        of  model           di-
                      mension,            thus      reducing            the    number            of  parameters.                The      width       and      depth                sion      for    attention            mechanism                and      replace          FFN       with       light-weight              FFN,
                                                                                                                                                                             8
                         G.K. Erabati and H. Araujo                                                                                                                                                                                                                      Intelligent Systems with Applications 22 (2024) 200361
                                                                              Table 9
                                                                              Performance                of  our      network            in  terms         of  LEVEL_1             APH        for    diÔ¨Äerent           attention           mechanisms.
                                                                                 Attention type                                            #param.          Memory (GB)           Vehicle        Pedestrian         Cyclist        Speed (FPS)
                                                                                 MHSA (Vaswani et al.,                      2017     )                 2.40M              4.1                            72.3               69.8                    66.2             19.0
                                                                                 FalshAttentoin(Daoeta,.l                           2022     )         1.60M              3.8                            72.2               69.3                    65.9             19.2
                                                                                 DeLiVoTr (Ours)                                                        1.56M              3.7                           72.8               70.2                   66.8             20.5
                         Fig.  4.        Visualization               of  results           of  our        DeLiVoTr              network            on  Waymo               (Sun        et  al.,      2020        )  dataset.         Blue         and     green          boxes        represent            predictions              and      ground           truth       
                         respectively.              Points        inside       the     bounding             boxes        are     shown         in  red      .  Best     viewed          in  color        and      zoom-in.
                         facilitating              the      reduction               of  model              parameters.                  We  employ                  layer-level                        LiVoTr_large)                   (Table           4 )  leveraging                the       layer-level            depth          and      width          scal-
                         scaling          for     eÔ¨Écient             parameter                allocation               with        shallower              and        narrower                         ing.      Our       network             surpasses             existing            state-of-the-art                   methods             on      Waymo             
                         encoder            layers          near        the       input          and        deeper           and        wider           encoder             layers                     Open         Dataset            and       KITTI         dataset           for      detection              of  small           objects,           such        as  
                         near       the      output,           similar          to  CNNs             (He       et  al.,      2016         ).  We       formulate               three                   pedestrians             ,  with       an  inference                 speed          of  20.5          FPS       (LiDAR            operates             at  10  
                         variants           of  our        DeLiVoTr               network             (DeLiVoTr_base,                        DeLiVoTr_small,                       De-                 Hz     in  WOD).
                                                                                                                                                                                                9
                                    G.K. Erabati and H. Araujo                                                                                                                                                                                                                                                                                                                                  Intelligent Systems with Applications 22 (2024) 200361
                                    CRediT                    authorship                                contribution                                   statement                                                                                                                                 Law,         H.,       &  Deng,               J.  (2018).                 Cornernet:                    Detecting                  objects             as  paired                keypoints. In                        Proceed-
                                                                                                                                                                                                                                                                                                            ings      of  the         European                  conference                  on  computer                     vision          (ECCV)                (pp. 734‚Äì750).
                                               Gopi Krishna                                    Erabati:                       Conceptualization,                                             Investigation,                                  Methodol-                                           Li,    C.,      Li,      L.,      Jiang,            H.,       Weng,              K.,      Geng,             Y.,      Li,      L.,     Ke,        Z.,      Li,      Q.,       Cheng,              M.,        Nie,         W.,        Li,      
                                    ogy,           Software,                        Writing                     ‚Äì  original                       draft.               Helder                   Araujo:                       Funding                     acqui-                                            Y.,     Zhang,              B.,      Liang,            Y.,      Zhou,            L.,     Xu,        X.,      Chu,          X.,      Wei,          X.,      &  Wei,             X.  (2022).                 Yolov6:               
                                    sition,              Resources,                           Supervision,                               Writing                     ‚Äì  review                     &  editing.                                                                                              A  single-stage                      object            detection                  framework                     for      industrial                  applications.                      Retrieved                  from           
                                                                                                                                                                                                                                                                                                            arXiv          :2209          .02976.
                                                                                                                                                                                                                                                                                                 Liu,       W.,       Anguelov,                   D.,      Erhan,             D.,      Szegedy,                 C.,     Reed,           S.,     Fu,       C.     Y.,      &  Berg,            A.     C.     (2016).              Ssd:        
                                    Declaration                               of  competing                                    interest                                                                                                                                                                     Single           shot         multibox                 detector. In                      European                conference                   on  computer                     vision           (pp. 21‚Äì37).                     
                                                                                                                                                                                                                                                                                                            Springer.
                                               The           authors                    declare                    that            they             have               no  known                           competing                             Ô¨Ånancial                                        Liu,       Z.,     Lin,        Y.,      Cao,          Y.,      Hu,        H.,       Wei,          Y.,      Zhang,              Z.,     Lin,        S.,      &  Guo,             B.  (2021).                  Swin           trans-
                                    interests                    or      personal                      relationships                                that          could               have              appeared                        to       inÔ¨Çuence                                                   former:             Hierarchical                       vision            transformer                      using            shifted             windows. In                         Proceedings                   of  the         
                                    the        work               reported                       in  this               paper.                                                                                                                                                                              IEEE/CVF                  international                     conference                  on  computer                     vision           (pp. 10012‚Äì10022).
                                                                                                                                                                                                                                                                                                 Loshchilov,                    I.,   &  Hutter,                F.  (2017).                 Decoupled                    weight              decay           regularization.                         arXiv           preprint.                
                                                                                                                                                                                                                                                                                                            Retrieved                 from           arXiv           :1711          .05101.
                                    Data             availability                                                                                                                                                                                                                                Mao,          J.,     Xue,         Y.,      Niu,         M.,       Bai,        H.,      Feng,           J.,     Liang,            X.,      Xu,       H.,      &  Xu,           C.  (2021).                 Voxel            trans-
                                                                                                                                                                                                                                                                                                            former             for      3d      object            detection. In                       Proceedings                   of  the         IEEE/CVF                    international                    conference                   
                                               The           link          to  code                   is  shared                     in  the               paper.                                                                                                                                           on     computer                 vision           (pp. 3164‚Äì3173).
                                                                                                                                                                                                                                                                                                 Maturana,                   D.,      &  Scherer,                   S.  (2015).                 Voxnet:                A  3d  convolutional                                neural             network                 for      real-
                                                                                                                                                                                                                                                                                                            time         object            recognition. In                          2015          IEEE/RSJ                   international                    conference                   on  intelligent                  robots           
                                    Acknowledgements                                                                                                                                                                                                                                                        and       systems              (IROS),              IEEE           (pp. 922‚Äì928).
                                                                                                                                                                                                                                                                                                 Mehta,             S.,      Ghazvininejad,                           M.,        Iyer,         S.,      Zettlemoyer,                        L.,      &  Hajishirzi,                     H.  (2021).                   Delight:                
                                               This           work                has          been               supported                           by  European                                 Union‚Äôs                    H2020                    MSCA-                                                Deep          and         light-weight                      transformer. In                             International                    conference                   on  learning                  representa-
                                    ITN-ACHIEVE                                    [Grant                   No.           765866],                          Funda√ß√£o                           para             a  Ci√™ncia                        e  a  Tec-                                                tions       .  Retrieved                 from           https://openreview                                   .net      /forum             ?id     =ujmgfuxSLrO                            .
                                    nologia                  (FCT)                 for         ISR           -Comibra                            [Ref.              No.            UIDB/00048/2020                                                  https://                                     Mehta,             S.,     Koncel-Kedziorski,                                 R.,      Rastegari,                  M.,        &  Hajishirzi,                     H.  (2018).                  Pyramidal                    recur-
                                    doi        .org         /10         .54499                  /UIDB                /00048                   /2020              ]  and             FCT             Portugal                      PhD              research                                                 rent        unit        for      language                  modeling.                   arXiv           preprint.                Retrieved                  from           arXiv          :1808           .09029.
                                                                                                                                                                                                                                                                                                 Meyer,             G.  P.,  Laddha,                          A.,       Kee,           E.,      Vallespi-Gonzalez,                                  C.,       &  Wellington,                          C.  K.  (2019).                         
                                    grant             [Ref.              No.          2021.06219.BD].                                                                                                                                                                                                       Lasernet:                An  eÔ¨Écient                       probabilistic                       3d  object                   detector                for        autonomous                         driving. In
                                                                                                                                                                                                                                                                                                            Proceedings                   of  the            IEEE/CVF                     conference                    on  computer                        vision           and          pattern              recognition
                                    References                                                                                                                                                                                                                                                              (pp. 12677‚Äì12686).
                                                                                                                                                                                                                                                                                                 Misra,           I.,    Girdhar,                R.,      &  Joulin,                A.  (2021).                 An       end-to-end                    transformer                      model             for      3d       object            
                                    Carion,             N.,      Massa,              F.,     Synnaeve,                   G.,      Usunier,                N.,      Kirillov,              A.,      &  Zagoruyko,                        S.  (2020).                End-                                     detection. In                       Proceedings                   of  the           IEEE/CVF                    international                     conference                   on  computer                      vision
                                               to-end            object            detection                  with          transformers. In                              European                conference                   on  computer                     vision                                      (pp. 2906‚Äì2917).
                                               (pp. 213‚Äì229).                           Springer.                                                                                                                                                                                                MMDetection3D                                contributors                        (2020).                 MMDetection3D:                                   OpenMMLab                             next-generation                              
                                    Chen,           Q.,       Sun,          L.,     Wang,              Z.,      Jia,        K.,      &  Yuille,                A.  (2020).                  Object              as  hotspots:                    An  anchor-                                                platform                for      general               3D  object                   detection.                  Retrieved                   from           https://github                          .com         /open           -
                                               free       3d  object                  detection                  approach                   via        Ô¨Åring           of  hotspots. In                            European                conference                   on                                  mmlab              /mmdetection3d                              .
                                               computer                vision           (pp. 68‚Äì84).                      Springer.                                                                                                                                                              Ngiam,              J.,     Caine,             B.,      Han,           W.,        Yang,            B.,      Chai,           Y.,       Sun,         P.,      Zhou,             Y.,      Yi,       X.,      Alsharif,                O.,       
                                    Chen,           Y.,       Liu,         S.,      Shen,             X.,       &  Jia,            J.  (2019).                   Fast          point           R-CNN. In                        Proceedings                    of  the                                      Nguyen,                P.,     et  al.        (2019).              Starnet:               Targeted                 computation                        for      object            detection                 in  point             
                                               IEEE/CVF                   international                    conference                   on  computer                    vision           (pp. 9775‚Äì9784).                                                                                                   clouds.            arXiv           preprint.                Retrieved                   from          arXiv           :1908          .11069.
                                    Cheng,             B.,      Misra,             I.,    Schwing,                  A.  G.,          Kirillov,              A.,       &  Girdhar,                   R.  (2022).                  Masked-attention                                                Pan,        X.,      Xia,         Z.,      Song,            S.,     Li,      L.  E.,         &  Huang,                   G.  (2021).                   3d  object                 detection                 with           point-
                                               mask           transformer                      for       universal                 image              segmentation. In                                Proceedings                   of  the          IEEE/CVF                                               former. In                  Proceedings                   of  the         IEEE/CVF                   conference                   on      computer                vision          and        pattern             recog-
                                               conference                  on  computer                    vision          and         pattern            recognition                   (pp. 1290‚Äì1299).                                                                                                    nition          (pp. 7463‚Äì7472).
                                    Cheng,             B.,      Schwing,                  A.,       &  Kirillov,                 A.  (2021).                   Per-pixel                classiÔ¨Åcation                        is  not          all     you         need                           Park,         C.,      Jeong,              Y.,      Cho,          M.,        &  Park,              J.  (2022).                 Fast         point           transformer. In                            Proceedings                   of  
                                               for      semantic                  segmentation.                           Advances                   in  Neural                 Information                      Processing                  Systems               ,  34      ,                             the     IEEE/CVF                   conference                   on     computer                 vision          and        pattern            recognition                   (pp. 16949‚Äì16958).
                                               17864‚Äì17875.                                                                                                                                                                                                                                      Paszke,             A.,       Gross,             S.,       Massa,              F.,       Lerer,            A.,        Bradbury,                    J.,      Chanan,                 G.,        Killeen,               T.,       Lin,         
                                    Choy,           C.,      Gwak,             J.,     &  Savarese,                   S.  (2019).                 4d      spatio-temporal                            convnets:                 Minkowski                      convo-                                        Z.,     Gimelshein,                       N.,       Antiga,               L.,      Desmaison,                       A.,       Kopf,            A.,       Yang,             E.,      DeVito,                Z.,      Rai-
                                               lutional              neural            networks. In                        Proceedings                   of  the         IEEE/CVF                   conference                  on      computer                 vision                                     son,        M.,       Tejani,              A.,       Chilamkurthy,                           S.,      Steiner,               B.,      Fang,            L.,      . . .    Chintala,                  S.  (2019).                  
                                               and        pattern            recognition                  (CVPR)               .                                                                                                                                                                            Pytorch:               An  imperative                           style,           high-performance                                  deep            learning                library.      In    H.                 Wal-
                                    Dao,         T.,      Fu,       D.,      Ermon,                S.,     Rudra,              A.,      &  R√©,          C.  (2022).                  Flashattention:                           Fast        and         memory-                                              lach,         H.  Larochelle,                          A.  Beygelzimer,                              F.  d‚ÄôAlch√©-Buc,                            E.  Fox,              &  R.  Garnett                        (Eds.),              
                                               eÔ¨Écient                exact          attention                 with          IO-awareness.                          Advances                 in  Neural               Information                    Processing                                             Advances                in  neural                  information                     processing                   systems:               Vol.       32             (pp.     8024‚Äì8035).                             Cur-
                                               Systems             ,  35     ,  16344‚Äì16359.                                                                                                                                                                                                                ran       Associates,                   Inc.       Retrieved                  from           http://papers                        .neurips             .cc     /paper            /9015           -pytorch              -an      -
                                    Dosovitskiy,                     A.,       Beyer,              L.,      Kolesnikov,                       A.,       Weissenborn,                           D.,       Zhai,           X.,       Unterthiner,                        T.,                                  imperative                   -style         -high        -performance                        -deep         -learning               -library            .pdf     .
                                               Dehghani,                   M.,         Minderer,                    M.,        Heigold,                  G.,       Gelly,             S.,      Uszkoreit,                    J.,      &  Houlsby,                       N.                       Qi,     C.  R.,          Liu,        W.,       Wu,          C.,      Su,       H.,      &  Guibas,                  L.  J.  (2018).                    Frustum                pointnets                  for      3d       object            
                                               (2021).              An  image                    is  worth                 16x16               words:               Transformers                          for       image               recognition                      at                                 detection                from           RGB-D               data. In                 Proceedings                   of  the          IEEE          conference                   on  computer                      vision           
                                               scale.    In             International                    conference                   on  learning                   representations                        .  Retrieved                  from            https://                                          and       pattern             recognition                   (pp. 918‚Äì927).
                                               openreview                      .net      /forum             ?id     =YicbFdNTTy                           .                                                                                                                                      Qi,     C.  R.,          Su,       H.,      Mo,         K.,      &  Guibas,                  L.  J.  (2017).                   Pointnet:                 Deep           learning                on      point           sets       for      
                                    Erabati,              G.  K.,  &  Araujo,                             H.  (2023).                    Li3detr:               A  lidar              based             3d  detection                        transformer.                                                   3d      classiÔ¨Åcation                       and         segmentation. In                               Proceedings                   of  the          IEEE          conference                  on  computer                      
                                               In      Proceedings                    of  the           IEEE/CVF                    winter             conference                   on  applications                          of  computer                     vision                                       vision         and        pattern             recognition                   (pp. 652‚Äì660).
                                               (pp. 4250‚Äì4259).                                                                                                                                                                                                                                  Qi,     C.  R.,           Yi,       L.,      Su,       H.,       &  Guibas,                   L.  J.  (2017).                      Pointnet+                  +:      Deep           hierarchical                       feature              
                                    Fan,        L.,     Pang,           Z.,     Zhang,              T.,     Wang,             Y.     X.,      Zhao,           H.,      Wang,             F.,     Wang,             N.,      &  Zhang,                 Z.    (2022).                                         learning               on  point                sets        in  a  metric                    space.             Advances                  in  Neural                 Information                     Processing                  
                                               Embracing                     single           stride           3d      object            detector                with          sparse            transformer. In                            Proceedings                   of                                Systems            ,  30     .
                                               the      IEEE/CVF                   conference                  on  computer                     vision          and        pattern             recognition                   (pp. 8458‚Äì8468).                                                    Redmon,                 J.,     &  Farhadi,                   A.  (2018).                  Yolov3:                An       incremental                      improvement.                           arXiv           preprint.                 
                                    Fan,        L.,     Xiong,             X.,      Wang,              F.,     Wang,             N.,       &  Zhang,                Z.  (2021).                  Rangedet:                   In  defense                 of  range                                          Retrieved                 from           arXiv           :1804          .02767.
                                               view          for      lidar-based                    3d  object                 detection. In                        Proceedings                   of  the         IEEE/CVF                    international                                     Shi,       S.,     Guo,           C.,       Jiang,            L.,      Wang,              Z.,      Shi,        J.,      Wang,              X.,      &  Li,  H.  (2020).                           Pv-rcnn:                Point-
                                               conference                  on  computer                    vision           (pp. 2918‚Äì2927).                                                                                                                                                                voxel          feature              set      abstraction                     for       3d  object                 detection. In                        Proceedings                   of  the          IEEE/CVF                    
                                    Geiger,             A.,       Lenz,           P.,      &  Urtasun,                     R.  (2012).                   Are        we  ready                  for       autonomous                         driving?                The                                     conference                 on  computer                     vision          and        pattern             recognition                   (pp. 10529‚Äì10538).
                                               KITTI           vision            benchmark                      suite. In               Conference                  on  computer                     vision          and        pattern             recognition                                  Shi,       S.,      Jiang,            L.,      Deng,             J.,      Wang,               Z.,      Guo,           C.,       Shi,        J.,      Wang,               X.,      &  Li,  H.  (2023).                            Pv-
                                               (CVPR)              .                                                                                                                                                                                                                                        rcnn+          +:      Point-voxel                     feature              set       abstraction                     with          local          vector             representation                           for      3d  
                                    Graham,                 B.,      Engelcke,                  M.,       &  Van             Der        Maaten,                L.  (2018).                  3d  semantic                      segmentation                         with                                     object           detection.                  International                     Journal              of  Computer                    Vision           ,  131       ,  531‚Äì551.
                                               submanifold                       sparse            convolutional                          networks. In                         Proceedings                   of  the          IEEE          conference                  on                       Shi,       S.,     Wang,             X.,      &  Li,        H.      (2019).              Pointrcnn:                   3d       object            proposal                generation                    and        detection                  
                                               computer                vision          and        pattern             recognition                   (pp. 9224‚Äì9232).                                                                                                                                        from         point            cloud. In                   Proceedings                   of  the           IEEE/CVF                    conference                   on  computer                      vision           and         
                                    Guo,         M.  H.,            Cai,        J.  X.,         Liu,        Z.  N.,         Mu,          T.  J.,        Martin,               R.  R.,         &  Hu,            S.  M.         (2021).               Pct:        Point                                      pattern           recognition                   (pp. 770‚Äì779).
                                               cloud           transformer.                      Computational                          Visual           Media           ,  7 ,  187‚Äì199.                                                                                                        Simony,               M.,        Milzy,             S.,       Amendey,                     K.,       &  Gross,                H.  M.  (2018).                          Complex-yolo:                            An  Euler-
                                    Han,         K.,      Xiao,          A.,      Wu,          E.,     Guo,          J.,     Xu,        C.,     &  Wang,                 Y.  (2021).                 Transformer                       in  transformer.                                                     region-proposal                           for       real-time                 3d  object                  detection                  on  point                clouds. In                    Proceedings                   of  
                                               Advances                 in  Neural               Information                    Processing                  Systems             ,  34     ,  15908‚Äì15919.                                                                                                   the     European                  conference                  on  computer                     vision          (ECCV)               workshops                  .
                                    He,       K.,      Zhang,               X.,      Ren,           S.,     &  Sun,              J.  (2016).                  Deep           residual                learning                for       image             recogni-                                Sun,        P.,      Kretzschmar,                        H.,       Dotiwalla,                   X.,      Chouard,                  A.,       Patnaik,               V.,      Tsui,          P.,      Guo,          J.,     Zhou,             
                                               tion.   In            Proceedings                    of  the          IEEE          conference                   on  computer                      vision           and         pattern             recognition                                              Y.,     Chai,          Y.,      Caine,            B.,      et  al.       (2020).              Scalability                   in  perception                      for      autonomous                        driving:              
                                               (pp. 770‚Äì778).                                                                                                                                                                                                                                               Waymo                open          dataset. In                   Proceedings                   of  the         IEEE/CVF                   conference                  on      computer                 vision          and        
                                    Lang,          A.  H.,           Vora,           S.,      Caesar,              H.,       Zhou,             L.,     Yang,            J.,      &  Beijbom,                     O.  (2019).                   Pointpillars:                                                pattern           recognition                   (pp. 2446‚Äì2454).
                                               Fast        encoders                 for       object            detection                  from          point           clouds. In                   Proceedings                    of  the         IEEE/CVF                                    Sun,        P.,       Wang,               W.,        Chai,            Y.,       Elsayed,                 G.,       Bewley,                 A.,       Zhang,               X.,       Sminchisescu,                           C.,       &  
                                               conference                  on  computer                    vision          and         pattern            recognition                   (pp. 12697‚Äì12705).                                                                                                  Anguelov,                   D.  (2021).                   Rsn:          Range              sparse             net        for        eÔ¨Écient,                  accurate                 lidar          3d  object                  
                                                                                                                                                                                                                                                                                     10
                            G.K. Erabati and H. Araujo                                                                                                                                                                                                                                                      Intelligent Systems with Applications 22 (2024) 200361
                                     detection.  In               Proceedings             of  the      IEEE/CVF              conference             on  computer               vision       and       pattern                     Yan,      Y.,    Mao,         Y.,    &  Li,  B.  (2018).                Second:           Sparsely           embedded                convolutional                 detection.            
                                     recognition            (pp. 5725‚Äì5734).                                                                                                                                                              Sensors        ,  18   ,  3337.
                            Tang,        H.,    et  al.     (2020).          Searching             eÔ¨Écient           3D      architectures               with       sparse        point-voxel              con-                   Yang,       Z.,    Sun,      Y.,    Liu,     S.,   &  Jia,      J.  (2020).          3DSSD:          Point-based               3d    single       stage       object        detec-
                                     volution. In A.                 Vedaldi,           H.  Bischof,             T.  Brox,          &  J.  M.  Frahm                 (Eds.),        Lecture         notes       in                        tor. In       Proceedings             of  the     IEEE/CVF             conference            on    computer            vision      and     pattern        recognition
                                     computer           science:        Vol. 12373            .  Computer            vision      ‚Äì  ECCV         2020       .  Cham:         Springer.           Retrieved                                (pp. 11040‚Äì11048).
                                     from       https://doi             .org    /10     .1007       /978      -3  -030     -58604         -1  _41   .                                                                             Yin,     T.,    Zhou,        X.,   &  Krahenbuhl,                  P.  (2021).          Center-based                3d     object       detection            and      tracking.
                            Vaswani,            A.,    Shazeer,            N.,    Parmar,            N.,    Uszkoreit,             J.,    Jones,         L.,   Gomez,            A.  N.,       Kaiser,         ≈Å.,                        In     Proceedings            of  the       IEEE/CVF              conference             on  computer              vision        and      pattern         recognition
                                     &  Polosukhin,                 I.  (2017).           Attention            is  all      you       need.        Advances            in  Neural           Information                                   (pp. 11784‚Äì11793).
                                     Processing           Systems         ,  30   .                                                                                                                                               Yuan,        L.,    Chen,         Y.,     Wang,           T.,     Yu,      W.,       Shi,      Y.,     Jiang,         Z.  H.,        Tay,       F.  E.,  Feng,              J.,    &  
                            Wang,         C.  Y.,      Yeh,      I.  H.,     &  Liao,        H.    Y.  M.      (2024).          Yolov9:          Learning           what        you      want       to  learn                             Yan,       S.  (2021).            Tokens-to-token                     vit:    Training            vision         transformers                from        scratch          on  
                                     using       programmable                    gradient          information.               Retrieved             from       arXiv       :2402       .13616.                                            imagenet. In                 Proceedings             of  the      IEEE/CVF              international              conference            on  computer               vision
                            Wang,         W.,      Xie,     E.,    Li,   X.,    Fan,      D.  P.,      Song,       K.,    Liang,        D.,    Lu,     T.,    Luo,      P.,    &  Shao,         L.  (2021).                               (pp. 558‚Äì567).
                                     Pyramid           vision        transformer:                A  versatile           backbone              for    dense         prediction            without           con-                   Zhao,       H.,     Jiang,       L.,    Jia,     J.,   Torr,      P.  H.,      &  Koltun,           V.  (2021).           Point        transformer. In                 Proceed-
                                     volutions. In               Proceedings             of  the       IEEE/CVF             international              conference             on  computer              vision                            ings     of  the      IEEE/CVF             international             conference            on  computer              vision       (pp. 16259‚Äì16268).
                                     (pp. 568‚Äì578).                                                                                                                                                                               Zheng,         S.,    Lu,     J.,   Zhao,         H.,     Zhu,       X.,    Luo,       Z.,    Wang,          Y.,    Fu,     Y.,    Feng,        J.,    Xiang,         T.,    Torr,       
                            Wang,         W.,      Xie,     E.,    Li,   X.,    Fan,      D.  P.,      Song,       K.,    Liang,        D.,    Lu,     T.,    Luo,      P.,    &  Shao,         L.  (2022).                               P.    H.,    et  al.     (2021).          Rethinking              semantic            segmentation                 from        a  sequence-to-sequence                           
                                     Pvt     v2:     Improved              baselines            with       pyramid             vision        transformer.                Computational                 Visual                             perspective              with       transformers. In                   Proceedings             of  the      IEEE/CVF             conference            on  computer              
                                     Media       ,  8 ,  415‚Äì424.                                                                                                                                                                         vision       and     pattern         recognition            (pp. 6881‚Äì6890).
                            Wang,         Y.,    Fathi,        A.,    Kundu,          A.,     Ross,       D.  A.,      Pantofaru,             C.,    Funkhouser,                T.,    &  Solomon,               J.               Zhou,        Y.,    Sun,       P.,    Zhang,           Y.,    Anguelov,              D.,     Gao,       J.,    Ouyang,            T.,     Guo,       J.,    Ngiam,           J.,    &  
                                     (2020).         Pillar-based              object        detection            for    autonomous                 driving. In             European           conference                                 Vasudevan,               V.  (2020).            End-to-end               multi-view              fusion        for    3d  object           detection            in  lidar        
                                     on    computer           vision       (pp. 18‚Äì34).               Springer.                                                                                                                           point       clouds. In            Conference             on  robot         learning,         PMLR          (pp. 923‚Äì932).
                            Wang,         Y.,    Guizilini,           V.  C.,      Zhang,          T.,    Wang,          Y.,    Zhao,        H.,     &  Solomon,              J.  (2022).           Detr3d:                       Zhou,        Y.,    &  Tuzel,           O.  (2018).             Voxelnet:             End-to-end               learning           for     point        cloud        based         3d  
                                     3d    object         detection            from        multi-view              images          via     3d-to-2d            queries. In              Conference             on                         object        detection. In                Proceedings             of  the      IEEE       conference            on  computer               vision       and      pattern        
                                     robot      learning,         PMLR          (pp. 180‚Äì191).                                                                                                                                            recognition            (pp. 4490‚Äì4499).
                            Wang,         Y.,     &  Solomon,               J.  M.  (2021).                Object         dgcnn:          3d  object            detection            using        dynamic                         Zhu,      X.,    Su,     W.,      Lu,     L.,    Li,    B.,    Wang,          X.,    &  Dai,        J.  (2020).           Deformable                detr:      Deformable                
                                     graphs.         Advances            in  Neural          Information             Processing            Systems        ,  34   .                                                                       transformers                for    end-to-end               object        detection.            arXiv        preprint.           Retrieved             from        arXiv        :
                            Wu,      W.,      Qi,     Z.,   &  Fuxin,           L.  (2019).           Pointconv:             Deep        convolutional                 networks             on  3d  point                                 2010        .04159.
                                     clouds. In           Proceedings             of  the      IEEE/CVF             conference            on  computer              vision       and     pattern        recog-
                                     nition       (pp. 9621‚Äì9630).
                            Xie,     E.,    Wang,          W.,     Yu,      Z.,    Anandkumar,                  A.,     Alvarez,          J.  M.,       &  Luo,        P.  (2021).           Segformer:               
                                     Simple         and      eÔ¨Écient           design         for    semantic           segmentation                 with       transformers.                Advances            in  
                                     Neural        Information              Processing           Systems         ,  34   ,  12077‚Äì12090.
                                                                                                                                                                                                                         11
