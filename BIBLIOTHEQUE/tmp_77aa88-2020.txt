                        Published as a conference paper at ICLR 2021
                         SHARPNESS-AWAREMINIMIZATIONFOREFFICIENTLY
                         IMPROVING GENERALIZATION
                          Pierre Foret ‚àó                    Ariel Kleiner              Hossein Mobahi
                          Google Research                   Google Research            Google Research
                          pierre.pforet@gmail.com           akleiner@gmail.com         hmobahi@google.com
                          BehnamNeyshabur
                          Blueshift, Alphabet
                          neyshabur@google.com
                                                              ABSTRACT
                                In today‚Äôs heavily overparameterized models, the value of the training loss pro-
                                vides few guarantees on model generalization ability. Indeed, optimizing only
                                the training loss value, as is commonly done, can easily lead to suboptimal
                                model quality. Motivated by prior work connecting the geometry of the loss
                                landscape and generalization, we introduce a novel, effective procedure for in-
                                stead simultaneously minimizing loss value and loss sharpness. In particular,
                                our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie
                                in neighborhoods having uniformly low loss; this formulation results in a min-
                                max optimization problem on which gradient descent can be performed efÔ¨Å-
                                ciently. We present empirical results showing that SAM improves model gen-
                                eralization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, Ima-
                                geNet, Ô¨Ånetuning tasks) and models, yielding novel state-of-the-art performance
                                for several. Additionally, we Ô¨Ånd that SAM natively provides robustness to la-
                                bel noise on par with that provided by state-of-the-art procedures that speciÔ¨Å-
                                cally target learning with noisy labels. We open source our code at https:
                                //github.com/google-research/sam.
                         1   INTRODUCTION
                        Modern machine learning‚Äôs success in achieving ever better performance on a wide range of tasks
                        has relied in signiÔ¨Åcant part on ever heavier overparameterization, in conjunction with developing
                        ever more effective training algorithms that are able to Ô¨Ånd parameters that generalize well. Indeed,
                        manymodernneuralnetworkscaneasilymemorizethetrainingdataandhavethecapacitytoreadily
        arXiv:2010.01412v3  [cs.LG]  29 Apr 2021overÔ¨Åt (Zhang et al., 2016). Such heavy overparameterization is currently required to achieve state-
                        of-the-art results in a variety of domains (Tan & Le, 2019; Kolesnikov et al., 2020; Huang et al.,
                        2018). In turn, it is essential that such models be trained using procedures that ensure that the
                        parameters actually selected do in fact generalize beyond the training set.
                        Unfortunately, simply minimizing commonly used loss functions (e.g., cross-entropy) on the train-
                        ing set is typically not sufÔ¨Åcient to achieve satisfactory generalization. The training loss landscapes
                        of today‚Äôs models are commonly complex and non-convex, with a multiplicity of local and global
                        minima, and with different global minima yielding models with different generalization abilities
                        (Shirish Keskar et al., 2016). As a result, the choice of optimizer (and associated optimizer settings)
                        from among the many available (e.g., stochastic gradient descent (Nesterov, 1983), Adam (Kingma
                        &Ba, 2014), RMSProp (Hinton et al.), and others (Duchi et al., 2011; Dozat, 2016; Martens &
                        Grosse, 2015)) has become an important design choice, though understanding of its relationship
                        to model generalization remains nascent (Shirish Keskar et al., 2016; Wilson et al., 2017; Shirish
                        Keskar & Socher, 2017; Agarwal et al., 2020; Jacot et al., 2018). Relatedly, a panoply of methods
                        for modifying the training process have been proposed, including dropout (Srivastava et al., 2014),
                           ‚àóWorkdoneaspartoftheGoogleAIResidencyprogram.
                                                                    1
                Published as a conference paper at ICLR 2021
                    Cifar10
                   Cifar100
                   Imagenet
                  Finetuning
                    SVHN 
                   F-MNIST
                  Noisy Cifar
                          0    20   40
                           Error reduction (%)
                Figure 1: (left) Error rate reduction obtained by switching to SAM. Each point is a different dataset
                / model / data augmentation. (middle) A sharp minimum to which a ResNet trained with SGD
                converged. (right) A wide minimum to which the same ResNet trained with SAM converged.
                batch normalization (Ioffe & Szegedy, 2015), stochastic depth (Huang et al., 2016), data augmenta-
                tion (Cubuk et al., 2018), and mixed sample augmentations (Zhang et al., 2017; Harris et al., 2020).
                Theconnectionbetweenthegeometryofthelosslandscape‚Äîinparticular,theÔ¨Çatnessofminima‚Äî
                and generalization has been studied extensively from both theoretical and empirical perspectives
                (Shirish Keskar et al., 2016; Dziugaite & Roy, 2017; Jiang et al., 2019). While this connection
                has held the promise of enabling new approaches to model training that yield better generalization,
                practical efÔ¨Åcient algorithms that speciÔ¨Åcally seek out Ô¨Çatter minima and furthermore effectively
                improve generalization on a range of state-of-the-art models have thus far been elusive (e.g., see
                (Chaudhari et al., 2016; Izmailov et al., 2018); we include a more detailed discussion of prior work
                in Section 5).
                WepresenthereanewefÔ¨Åcient,scalable, and effective approach to improving model generalization
                ability that directly leverages the geometry of the loss landscape and its connection to generaliza-
                tion, and is powerfully complementary to existing techniques. In particular, we make the following
                contributions:
                    ‚Ä¢ We introduce Sharpness-Aware Minimization (SAM), a novel procedure that improves
                      model generalization by simultaneously minimizing loss value and loss sharpness. SAM
                      functions by seeking parameters that lie in neighborhoods having uniformly low loss value
                      (rather than parametersthatonlythemselveshavelowlossvalue,asillustratedinthemiddle
                      and righthand images of Figure 1), and can be implemented efÔ¨Åciently and easily.
                    ‚Ä¢ We show via a rigorous empirical study that using SAM improves model generalization
                      ability across a range of widely studied computer vision tasks (e.g., CIFAR-{10, 100},
                      ImageNet,Ô¨Ånetuningtasks)andmodels,assummarizedinthelefthandplotofFigure1. For
                      example, applying SAMyieldsnovelstate-of-the-art performance for a number of already-
                      intensely-studied tasks, such as ImageNet, CIFAR-{10, 100}, SVHN, Fashion-MNIST,
                      and the standard set of image classiÔ¨Åcation Ô¨Ånetuning tasks (e.g., Flowers, Stanford Cars,
                      Oxford Pets, etc).
                    ‚Ä¢ WeshowthatSAMfurthermoreprovidesrobustnesstolabelnoiseonparwiththatprovided
                      bystate-of-the-art procedures that speciÔ¨Åcally target learning with noisy labels.
                    ‚Ä¢ Through the lens provided by SAM, we further elucidate the connection between loss
                      sharpness and generalization by surfacing a promising new notion of sharpness, which
                      wetermm-sharpness.
                Section 2 below derives the SAM procedure and presents the resulting algorithm in full detail. Sec-
                tion 3 evaluates SAM empirically, and Section 4 further analyzes the connection between loss sharp-
                ness and generalization through the lens of SAM. Finally, we conclude with an overview of related
                workandadiscussion of conclusions and future work in Sections 5 and 6, respectively.
                                             2
                             Published as a conference paper at ICLR 2021
                             2    SHARPNESS-AWARE MINIMIZATION (SAM)
                             Throughoutthepaper,wedenotescalarsasa,vectorsasa,matricesasA,setsasA,andequalityby
                             deÔ¨Ånition as ,. Given a training dataset S , ‚à™n       {(x ,y )} drawn i.i.d. from distribution D, we
                                                                               i=1     i  i
                             seek to learn a model that generalizes well. In particular, consider a family of models parameterized
                             byw‚ààW‚äÜRd;givenaper-data-pointlossfunctionl : W√óX √óY ‚Üí R ,wedeÔ¨Ånethetraining
                                                    P                                                      +
                             set loss L (w) , 1       n   l(w,x ,y ) and the population loss L (w) , E                 [l(w,x,y)].
                                       S          n   i=1        i   i                             D           (x,y)‚àºD
                             Having observed only S, the goal of model training is to select model parameters w having low
                             population loss LD(w).
                             Utilizing L (w) as an estimate of L (w) motivates the standard approach of selecting parameters
                                        S                          D
                             wbysolvingminwLS(w)(possiblyinconjunctionwitharegularizeronw)usinganoptimization
                             procedure such as SGD or Adam. Unfortunately, however, for modern overparameterized mod-
                             els such as deep neural networks, typical optimization approaches can easily result in suboptimal
                             performance at test time. In particular, for modern models, LS(w) is typically non-convex in w,
                             with multiple local and even global minima that may yield similar values of LS(w) while having
                             signiÔ¨Åcantly different generalization performance (i.e., signiÔ¨Åcantly different values of LD(w)).
                             Motivatedbytheconnectionbetweensharpnessofthelosslandscapeandgeneralization,wepropose
                             a different approach: rather than seeking out parameter values w that simply have low training loss
                             valueL (w),weseekoutparametervalueswhoseentireneighborhoodshaveuniformlylowtraining
                                     S
                             loss value (equivalently, neighborhoods having both low loss and low curvature). The following
                             theorem illustrates the motivation for this approach by bounding generalization ability in terms of
                             neighborhood-wise training loss (full theorem statement and proof in Appendix A):
                             Theorem(statedinformally)1. ForanyœÅ > 0,withhighprobabilityovertrainingsetS generated
                             from distribution D,
                                                                                                  2   2
                                                         LD(w)‚â§ max LS(w+)+h(kwk /œÅ ),
                                                                    kk ‚â§œÅ                        2
                                                                       2
                             where h : R    ‚ÜíR isastrictlyincreasing function (under some technical conditions on L (w)).
                                         +       +                                                                           D
                             Tomakeexplicit our sharpness term, we can rewrite the right hand side of the inequality above as
                                                   [ max L (w+)‚àíL (w)]+L (w)+h(kwk2/œÅ2).
                                                   kk2‚â§œÅ S                 S          S                2
                             TheterminsquarebracketscapturesthesharpnessofLS atwbymeasuringhowquicklythetraining
                             loss can be increased by moving from w to a nearby parameter value; this sharpness term is then
                             summedwith the training loss value itself and a regularizer on the magnitude of w. Given that the
                             speciÔ¨Åc function h is heavily inÔ¨Çuenced by the details of the proof, we substitute the second term
                                        2
                             with Œª||w|| for a hyperparameter Œª, yielding a standard L2 regularization term. Thus, inspired by
                                        2
                             thetermsfromthebound,weproposetoselectparametervaluesbysolvingthefollowingSharpness-
                             AwareMinimization (SAM) problem:
                                                 SAM                2                 SAM
                                          minL        (w)+Œª||w||         where      L      (w) , max LS(w+),                    (1)
                                            w    S                  2                 S            |||| ‚â§œÅ
                                                                                                      p
                             where œÅ ‚â• 0 is a hyperparameter and p ‚àà [1,‚àû] (we have generalized slightly from an L2-norm
                             to a p-norm in the maximization over , though we show empirically in appendix C.5 that p = 2 is
                                                                 1
                             typically optimal). Figure 1 shows the loss landscape for a model that converged to minima found
                             by minimizing either L (w) or LSAM(w), illustrating that the sharpness-aware loss prevents the
                                                     S           S
                             model from converging to a sharp minimum.
                                                         SAM
                             In order to minimize L           (w), we derive an efÔ¨Åcient and effective approximation to
                                   SAM                   S
                             ‚àá L        (w)bydifferentiating through the inner maximization, which in turn enables us to apply
                               w S
                             stochastic gradient descent directly to the SAM objective. Proceeding down this path, we Ô¨Årst ap-
                             proximate the inner maximization problem via a Ô¨Årst-order Taylor expansion of LS(w + ) w.r.t. 
                             around 0, obtaining
                                 ‚àó                                                      T                          T
                                 (w) , argmaxL (w+)‚âàargmaxL (w)+ ‚àá L (w)=argmax ‚àá L (w).
                                           kk ‚â§œÅ    S              kk ‚â§œÅ    S             w S           kk ‚â§œÅ       w S
                                              p                        p                                     p
                                1Figure 1 was generated following Li et al. (2017) with the provided ResNet56 (no residual connections)
                             checkpoint, and training the same model with SAM.
                                                                                3
                                        Published as a conference paper at ICLR 2021
                                                                   ÀÜ
                                        In turn, the value (w) that solves this approximation is given by the solution to a classical dual
                                        normproblem(|¬∑|q‚àí1 denotes elementwise absolute value and power)2:
                                                                                                                                                         
                                                                                                                                                            1/p
                                                              ÀÜ                                                           q‚àí1                           q
                                                              (w) = œÅ sign(‚àá L (w))|‚àá L (w)|                                   / k‚àá L (w)k                                        (2)
                                                                                        w S                 w S                          w S            q
                                        where 1/p+1/q = 1. Substituting back into equation (1) and differentiating, we then have
                                                                                                                                ÀÜ
                                                                  SAM                                  ÀÜ             d(w+(w))
                                                          ‚àá L            (w) ‚âà ‚àá L (w+(w))=                                              ‚àá L (w)|
                                                             w                         w S                                                   w S                ÀÜ
                                                                  S                                                          dw                            w+(w)
                                                                                                                       ÀÜ
                                                                                =‚àá L (w)|                       +d(w)‚àá L (w)|                             .
                                                                                       w S               ÀÜ                       w S                ÀÜ
                                                                                                     w+(w)            dw                       w+(w)
                                                                                 SAM
                                        This approximation to ‚àá L                       (w)canbestraightforwardly computed via automatic differentia-
                                                                            w S
                                        tion, as implemented in commonlibrariessuchasJAX,TensorFlow,andPyTorch. Thoughthiscom-
                                                                                                                                ÀÜ
                                        putationimplicitlydependsontheHessianofL (w)because(w)isitselfafunctionof‚àá L (w),
                                                                                                           S                                                             w S
                                        the Hessian enters only via Hessian-vector products, which can be computed tractably without ma-
                                        terializing the Hessian matrix. Nonetheless, to further accelerate the computation, we drop the
                                        second-order terms. obtaining our Ô¨Ånal gradient approximation:
                                                                                    ‚àá LSAM(w)‚âà‚àá L (w)|                                    .                                        (3)
                                                                                       w                         w S                ÀÜ
                                                                                            S                                  w+(w)
                                        AsshownbytheresultsinSection3, this approximation (without the second-order terms) yields an
                                        effective algorithm. In Appendix C.4, we additionally investigate the effect of instead including the
                                        second-order terms; in that initial experiment, including them surprisingly degrades performance,
                                        and further investigating these terms‚Äô effect should be a priority in future work.
                                        Weobtain the Ô¨Ånal SAM algorithm by applying a standard numerical optimizer such as stochastic
                                        gradient descent (SGD) to the SAM objective LSAM(w), using equation 3 to compute the requisite
                                                                                                             S
                                        objective function gradients. Algorithm 1 gives pseudo-code for the full SAM algorithm, using SGD
                                        as the base optimizer, and Figure 2 schematically illustrates a single SAM parameter update.
                                                                           n
                                        Input: Training set S , ‚à™              {(xi,yi)}, Loss function
                                                                           i=1
                                                  l : W √óX √óY ‚ÜíR+,Batchsizeb,StepsizeŒ∑ > 0,
                                                  Neighborhood size œÅ > 0.
                                        Output: Model trained with SAM                                                                                             w
                                                                                                                                                         L(w )       t+1
                                        Initialize weights w , t = 0;                                                                                        t
                                                                  0
                                        while not converged do                                                                                        wt                    wSAM
                                                                                                                                           L(w )                              t+1
                                                                                                                                 || L(w )||     t
                                             Samplebatch B = {(x1,y1),...(xb,yb)};                                                    t 2
                                             Computegradient ‚àá L (w)ofthebatch‚Äôstraining loss;
                                                          ÀÜ             w B                                                      wadv                            L(wadv)
                                             Compute(w)perequation2;
                                             Computegradient approximation for the SAM objective
                                               (equation 3): g = ‚àá L (w)|                        ;
                                                                          w B              ÀÜ
                                                                                       w+(w)
                                             Update weights: w              =w ‚àíŒ∑g;
                                                                      t+1         t
                                             t = t + 1;                                                                       Figure2: SchematicoftheSAMparam-
                                        end                                                                                   eter update.
                                        return wt         Algorithm 1: SAM algorithm
                                        3      EMPIRICAL EVALUATION
                                        In order to assess SAM‚Äôs efÔ¨Åcacy, we apply it to a range of different tasks, including image clas-
                                        siÔ¨Åcation from scratch (including on CIFAR-10, CIFAR-100, and ImageNet), Ô¨Ånetuning pretrained
                                        models, and learning with noisy labels. In all cases, we measure the beneÔ¨Åt of using SAM by simply
                                        replacing the optimization procedure used to train existing models with SAM, and computing the
                                        resulting effect on model generalization. As seen below, SAM materially improves generalization
                                        performance in the vast majority of these cases.
                                            2In the case of interest p = 2, this boils down to simply rescaling the gradient such that its norm is œÅ.
                                                                                                               4
                         Published as a conference paper at ICLR 2021
                         3.1  IMAGE CLASSIFICATION FROM SCRATCH
                         WeÔ¨Årst evaluate SAM‚Äôs impact on generalization for today‚Äôs state-of-the-art models on CIFAR-10
                         and CIFAR-100 (without pretraining): WideResNets with ShakeShake regularization (Zagoruyko
                         &Komodakis, 2016; Gastaldi, 2017) and PyramidNet with ShakeDrop regularization (Han et al.,
                         2016; Yamada et al., 2018). Note that some of these models have already been heavily tuned in
                         prior work and include carefully chosen regularization schemes to prevent overÔ¨Åtting; therefore,
                         signiÔ¨Åcantly improving their generalization is quite non-trivial. We have ensured that our imple-
                         mentations‚Äô generalization performance in the absence of SAM matches or exceeds that reported in
                         prior work (Cubuk et al., 2018; Lim et al., 2019)
                         All results use basic data augmentations (horizontal Ô¨Çip, padding by four pixels, and random crop).
                         Wealso evaluate in the setting of more advanced data augmentation methods such as cutout regu-
                         larization (Devries & Taylor, 2017) and AutoAugment (Cubuk et al., 2018), which are utilized by
                         prior work to achieve state-of-the-art results.
                         SAMhasasingle hyperparameter œÅ (the neighborhood size), which we tune via a grid search over
                                                                                                 3
                         {0.01,0.02,0.05,0.1,0.2,0.5} using 10% of the training set as a validation set . Please see ap-
                         pendix C.1 for the values of all hyperparameters and additional training details. As each SAM
                                                                                          ÀÜ
                         weight update requires two backpropagation operations (one to compute (w) and another to com-
                         pute the Ô¨Ånal gradient), we allow each non-SAM training run to execute twice as many epochs as
                         each SAMtraining run, and we report the best score achieved by each non-SAM training run across
                         either the standard epoch count or the doubled epoch count4. We run Ô¨Åve independent replicas of
                         eachexperimentalconditionforwhichwereportresults(eachwithindependentweightinitialization
                         and data shufÔ¨Çing), reporting the resulting mean error (or accuracy) on the test set, and the associ-
                         ated 95% conÔ¨Ådence interval. Our implementations utilize JAX (Bradbury et al., 2018), and we
                                                                                5
                         train all models on a single host having 8 NVidia V100 GPUs . To compute the SAM update when
                         parallelizing across multiple accelerators, we divide each data batch evenly among the accelerators,
                         independently compute the SAM gradient on each accelerator, and average the resulting sub-batch
                         SAMgradientstoobtain the Ô¨Ånal SAM update.
                         As seen in Table 1, SAM improves generalization across all settings evaluated for CIFAR-10 and
                         CIFAR-100. For example, SAM enables a simple WideResNet to attain 1.6% test error, versus
                         2.2% error without SAM. Such gains have previously been attainable only by using more complex
                         modelarchitectures (e.g., PyramidNet) and regularization schemes (e.g., Shake-Shake, ShakeDrop);
                         SAMprovides an easily-implemented, model-independent alternative. Furthermore, SAM delivers
                         improvements even when applied atop complex architectures that already use sophisticated regular-
                         ization: for instance, applying SAM to a PyramidNet with ShakeDrop regularization yields 10.3%
                         error on CIFAR-100, which is, to our knowledge, a new state-of-the-art on this dataset without the
                         use of additional data.
                         Beyond CIFAR-{10, 100}, we have also evaluated SAM on the SVHN (Netzer et al., 2011) and
                         Fashion-MNIST datasets (Xiao et al., 2017). Once again, SAM enables a simple WideResNet to
                         achieve accuracy at or above the state-of-the-art for these datasets: 0.99% error for SVHN, and
                         3.59%forFashion-MNIST.Details are available in appendix B.1.
                         To assess SAM‚Äôs performance at larger scale, we apply it to ResNets (He et al., 2015) of different
                         depths (50, 101, 152) trained on ImageNet (Deng et al., 2009). In this setting, following prior work
                         (Heetal., 2015; Szegedy et al., 2015), we resize and crop images to 224-pixel resolution, normalize
                         them,andusebatchsize4096,initiallearningrate1.0,cosinelearningrateschedule,SGDoptimizer
                         withmomentum0.9,labelsmoothingof0.1,andweightdecay0.0001. WhenapplyingSAM,weuse
                         œÅ = 0.05 (determined via a grid search on ResNet-50 trained for 100 epochs). We train all models
                         on ImageNet for up to 400 epochs using a Google Cloud TPUv3 and report top-1 and top-5 test
                         error rates for each experimental condition (mean and 95% conÔ¨Ådence interval across 5 independent
                         runs).
                            3We found œÅ = 0.05 to be a solid default value, and we report in appendix C.3 the scores for all our
                         experiments, obtained with œÅ = 0.05 without further tuning.
                            4Training for longer generally did not improve accuracy signiÔ¨Åcantly, except for the models previously
                         trained for only 200 epochs and for the largest, most regularized model (PyramidNet + ShakeDrop).
                            5Because SAM‚Äôs performance is ampliÔ¨Åed by not syncing the perturbations, data parallelism is highly
                         recommendedtoleverage SAM‚Äôs full potential (see Section 4 for more details).
                                                                    5
                        Published as a conference paper at ICLR 2021
                                                                       CIFAR-10           CIFAR-100
                              Model                  Augmentation   SAM       SGD      SAM       SGD
                              WRN-28-10(200epochs)   Basic         2.7       3.5      16.5     18.8
                                                                      ¬±0.1      ¬±0.1     ¬±0.2      ¬±0.2
                              WRN-28-10(200epochs)   Cutout        2.3       2.6      14.9     16.9
                                                                      ¬±0.1      ¬±0.1     ¬±0.2      ¬±0.1
                              WRN-28-10(200epochs)   AA            2.1       2.3      13.6     15.8
                                                                     ¬±<0.1      ¬±0.1     ¬±0.2      ¬±0.2
                              WRN-28-10(1800epochs)  Basic         2.4       3.5      16.3     19.1
                                                                      ¬±0.1      ¬±0.1     ¬±0.2      ¬±0.1
                              WRN-28-10(1800epochs)  Cutout        2.1       2.7      14.0     17.4
                                                                      ¬±0.1      ¬±0.1     ¬±0.1      ¬±0.1
                              WRN-28-10(1800epochs)  AA            1.6      2.2       12.8     16.1
                                                                      ¬±0.1     ¬±<0.1     ¬±0.2      ¬±0.2
                              Shake-Shake (26 2x96d) Basic         2.3       2.7      15.1     17.0
                                                                     ¬±<0.1      ¬±0.1     ¬±0.1      ¬±0.1
                              Shake-Shake (26 2x96d) Cutout        2.0       2.3      14.2     15.7
                                                                     ¬±<0.1      ¬±0.1     ¬±0.2      ¬±0.2
                              Shake-Shake (26 2x96d) AA            1.6       1.9      12.8     14.1
                                                                     ¬±<0.1      ¬±0.1     ¬±0.1      ¬±0.2
                              PyramidNet             Basic         2.7       4.0      14.6     19.7
                                                                      ¬±0.1      ¬±0.1     ¬±0.4      ¬±0.3
                              PyramidNet             Cutout        1.9       2.5      12.6     16.4
                                                                      ¬±0.1      ¬±0.1     ¬±0.2      ¬±0.1
                              PyramidNet             AA            1.6       1.9      11.6     14.6
                                                                      ¬±0.1      ¬±0.1     ¬±0.1      ¬±0.1
                              PyramidNet+ShakeDrop   Basic         2.1       2.5      13.3     14.5
                                                                      ¬±0.1      ¬±0.1     ¬±0.2      ¬±0.1
                              PyramidNet+ShakeDrop   Cutout        1.6       1.9      11.3     11.8
                                                                     ¬±<0.1      ¬±0.1     ¬±0.1      ¬±0.2
                              PyramidNet+ShakeDrop   AA            1.4      1.6       10.3     10.6
                                                                     ¬±<0.1     ¬±<0.1     ¬±0.1      ¬±0.1
                        Table 1: Results for SAM on state-of-the-art models on CIFAR-{10, 100} (WRN = WideResNet;
                        AA=AutoAugment;SGDisthestandardnon-SAMprocedureusedtotrainthesemodels).
                        As seen in Table 2, SAM again consistently improves performance, for example improving the
                        ImageNettop-1errorrateofResNet-152from20.3%to18.4%. Furthermore,notethatSAMenables
                        increasing the number of training epochs while continuing to improve accuracy without overÔ¨Åtting.
                        Incontrast, thestandardtrainingprocedure(withoutSAM)generallysigniÔ¨ÅcantlyoverÔ¨Åtsastraining
                        extends from 200 to 400 epochs.
                                    Model     Epoch          SAM           Standard Training (No SAM)
                                                        Top-1     Top-5      Top-1       Top-5
                                   ResNet-50   100     22.5      6.28       22.9        6.62
                                                          ¬±0.1      ¬±0.08      ¬±0.1        ¬±0.11
                                               200     21.4      5.82       22.3        6.37
                                                          ¬±0.1      ¬±0.03      ¬±0.1        ¬±0.04
                                               400     20.9      5.51       22.3        6.40
                                                          ¬±0.1      ¬±0.03      ¬±0.1        ¬±0.06
                                  ResNet-101   100     20.2      5.12       21.2        5.66
                                                          ¬±0.1      ¬±0.03      ¬±0.1        ¬±0.05
                                               200     19.4      4.76       20.9        5.66
                                                          ¬±0.1      ¬±0.03      ¬±0.1        ¬±0.04
                                               400    19.0       4.65       22.3        6.41
                                                         ¬±<0.01     ¬±0.05      ¬±0.1        ¬±0.06
                                  ResNet-152   100    19.2       4.69      20.4         5.39
                                                         ¬±<0.01     ¬±0.04      ¬±<0.0       ¬±0.06
                                               200     18.5      4.37       20.3        5.39
                                                          ¬±0.1      ¬±0.03      ¬±0.2        ¬±0.07
                                               400    18.4       4.35      20.9         5.84
                                                         ¬±<0.01     ¬±0.04      ¬±<0.0       ¬±0.07
                               Table 2: Test error rates for ResNets trained on ImageNet, with and without SAM.
                        3.2  FINETUNING
                        Transfer learning by pretraining a model on a large related dataset and then Ô¨Ånetuning on a smaller
                        target dataset of interest has emerged as a powerful and widely used technique for producing high-
                        quality models for a variety of different tasks. We show here that SAM once again offers con-
                        siderable beneÔ¨Åts in this setting, even when Ô¨Ånetuning extremely large, state-of-the-art, already
                        high-performing models.
                        In particular, we apply SAM to Ô¨Ånetuning EfÔ¨ÅcentNet-b7 (pretrained on ImageNet) and
                        EfÔ¨ÅcientNet-L2(pretrainedonImageNetplusunlabeledJFT;inputresolution475)(Tan&Le,2019;
                        Kornblith et al., 2018; Huang et al., 2018). We initialize these models to publicly available check-
                        points6 trained with RandAugment (84.7% accuracy on ImageNet) and NoisyStudent (88.2% ac-
                        curacy on ImageNet), respectively. We Ô¨Ånetune these models on each of several target datasets by
                        training each model starting from the aforementioned checkpoint; please see the appendix for details
                        of the hyperparameters used. We report the mean and 95% conÔ¨Ådence interval of top-1 test error
                        over 5 independent runs for each dataset.
                          6https://github.com/tensorflow/tpu/tree/master/models/official/
                        efficientnet
                                                                 6
                               Published as a conference paper at ICLR 2021
                               As seen in Table 3, SAM uniformly improves performance relative to Ô¨Ånetuning without SAM.
                               Furthermore, in many cases, SAM yields novel state-of-the-art performance, including 0.30% error
                               onCIFAR-10,3.92%erroronCIFAR-100,and11.39%erroronImageNet.
                                 Dataset            EffNet-b7    EffNet-b7      Prev. SOTA        EffNet-L2    EffNet-L2    Prev. SOTA
                                                      +SAM                    (ImageNet only)      +SAM
                                 FGVC Aircraft       6.80         8.15        5.3 (TBMSL-Net)     4.82          5.80        5.3 (TBMSL-Net)
                                                         ¬±0.06        ¬±0.08                           ¬±0.08         ¬±0.1
                                 Flowers             0.63         1.16        0.7 (BiT-M)         0.35         0.40         0.37 (EffNet)
                                                         ¬±0.02        ¬±0.05                           ¬±0.01        ¬±0.02
                                 Oxford IIIT Pets    3.97         4.24        4.1 (Gpipe)         2.90         3.08         4.1 (Gpipe)
                                                         ¬±0.04        ¬±0.09                           ¬±0.04        ¬±0.04
                                 Stanford Cars       5.18         5.94        5.0 (TBMSL-Net)     4.04         4.93         3.8 (DAT)
                                 CIFAR-10            0.88¬±0.02    0.95¬±0.06   1 (Gpipe)           0.30¬±0.03    0.34¬±0.04    0.63 (BiT-L)
                                                         ¬±0.02        ¬±0.03                           ¬±0.01        ¬±0.02
                                 CIFAR-100           7.44         7.68        7.83 (BiT-M)        3.92         4.07         6.49 (BiT-L)
                                                         ¬±0.06        ¬±0.06                           ¬±0.06        ¬±0.08
                                 Birdsnap           13.64        14.30        15.7 (EffNet)       9.93         10.31        14.5 (DAT)
                                                         ¬±0.15        ¬±0.18                           ¬±0.15         ¬±0.15
                                 Food101             7.02         7.17        7.0 (Gpipe)         3.82         3.97         4.7 (DAT)
                                                         ¬±0.02        ¬±0.03                           ¬±0.01        ¬±0.03
                                 ImageNet           15.14¬±0.03      15.3      14.2 (KDforAA)      11.39¬±0.02      11.8      11.45 (ViT)
                               Table 3: Top-1 error rates for Ô¨Ånetuning EfÔ¨ÅcientNet-b7 (left; ImageNet pretraining only) and
                               EfÔ¨ÅcientNet-L2 (right; pretraining on ImageNet plus additional data, such as JFT) on various down-
                               stream tasks. Previous state-of-the-art (SOTA) includes EfÔ¨ÅcientNet (EffNet) (Tan & Le, 2019),
                               Gpipe (Huang et al., 2018), DAT (Ngiam et al., 2018), BiT-M/L (Kolesnikov et al., 2020), KD-
                               forAA(Weietal., 2020), TBMSL-Net (Zhang et al., 2020), and ViT (Dosovitskiy et al., 2020).
                               3.3    ROBUSTNESS TO LABEL NOISE
                               ThefactthatSAMseeksoutmodelparametersthat
                               are robust to perturbations suggests SAM‚Äôs poten-                    Method                 Noise rate (%)
                               tial to provide robustness to noise in the training set                                20    40     60     80
                               (which would perturb the training loss landscape).             Sanchez et al. (2019)   94.0  92.8   90.3   74.1
                               Thus, we assess here the degree of robustness that           Zhang&Sabuncu(2018)       89.7  87.6   82.7   67.9
                               SAMprovidestolabelnoise.                                         Lee et al. (2019)     87.1  81.8   75.4    -
                                                                                                Chenetal. (2019)      89.7    -      -    52.3
                               In particular, we measure the effect of apply-                  Huangetal. (2019)      92.6  90.3   43.4    -
                                                                                               MentorNet (2017)       92.0  91.2   74.2   60.0
                               ing SAM in the classical noisy-label setting for                  Mixup(2017)          94.0  91.5   86.8   76.9
                               CIFAR-10, in which a fraction of the training set‚Äôs             MentorMix(2019)        95.6  94.2   91.3   81.0
                               labels are randomly Ô¨Çipped; the test set remains                      SGD              84.8  68.8   48.2   26.2
                               unmodiÔ¨Åed (i.e., clean). To ensure valid compar-                      Mixup            93.0  90.0   83.8   70.2
                               ison to prior work, which often utilizes architec-              Bootstrap + Mixup      93.3  92.0   87.6   72.0
                                                                                                     SAM              95.1  93.4   90.5   77.9
                               tures specialized to the noisy-label setting, we train           Bootstrap + SAM       95.4  94.2   91.8   79.9
                               a simple model of similar size (ResNet-32) for 200         Table 4: Test accuracy on the clean test set
                               epochs, following Jiang et al. (2019). We evalu-           for modelstrainedonCIFAR-10withnoisyla-
                               ate Ô¨Åve variants of model training: standard SGD,          bels. Lower block is our implementation, up-
                               SGDwith Mixup (Zhang et al., 2017), SAM, and               per block gives scores from the literature, per
                               ‚Äùbootstrapped‚Äù variants of SGD with Mixup and              Jiang et al. (2019).
                               SAM(wherein the model is Ô¨Årst trained as usual
                               and then retrained from scratch on the labels pre-
                               dicted by the initially trained model). When apply-
                               ing SAM,weuseœÅ = 0.1forallnoiselevelsexcept80%,forwhichweuseœÅ = 0.05formorestable
                               convergence. For the Mixup baselines, we tried all values of Œ± ‚àà {1,8,16,32} and conservatively
                               report the best score for each noise level.
                               As seen in Table 4, SAM provides a high degree of robustness to label noise, on par with that
                               provided by state-of-the art procedures that speciÔ¨Åcally target learning with noisy labels. Indeed,
                               simply training a model with SAM outperforms all prior methods speciÔ¨Åcally targeting label noise
                               robustness, with the exception of MentorMix (Jiang et al., 2019). However, simply bootstrapping
                               SAMyieldsperformancecomparabletothat of MentorMix (which is substantially more complex).
                                                                                      7
                                  Published as a conference paper at ICLR 2021
                                            SGD                     SAM
                                             max=62.9                max=18.6
                                             max/ 5=2.5              max/ 5=3.6       0.08        m                        0.050
                                  Epoch: 1                                                                                                 Task 1
                                   0     20     40    60   0     20     40    60      0.07        1                        0.045           Task 2  0.17
                                                                                                  4
                                             max=12.5                max=8.9                      16                       0.040                   0.16
                                             max/ 5=1.7              max/ 5=1.9       0.06        64
                                  Epoch: 50                                                       256                      0.035                   0.15
                                    0       5      10       0       5      10        Error rate (%)0.05
                                                =24.2                   =1.0                                              Mutual information0.030         Mutual information
                                             max/ =11.4              max/ =2.6                                                                     0.14
                                             max 5                   max 5            0.04
                                  Epoch: 300010    20      0       10      20            0.00    0.05    0.10    0.15            1   4 16 64 256
                                            p( )                    p( )                                                                 m
                                  Figure 3: (left) Evolution of the spectrum of the Hessian during training of a model with standard
                                  SGD(lefthand column) or SAM (righthand column). (middle) Test error as a function of œÅ for dif-
                                  ferent values of m. (right) Predictive power of m-sharpness for the generalization gap, for different
                                  values of m (higher means the sharpness measure is more correlated with actual generalization gap).
                                  4     SHARPNESS AND GENERALIZATION THROUGH THE LENS OF SAM
                                  4.1    m-SHARPNESS
                                  ThoughourderivationofSAMdeÔ¨ÅnestheSAMobjectiveovertheentiretrainingset,whenutilizing
                                  SAMinpractice, we compute the SAM update per-batch (as described in Algorithm 1) or even by
                                  averaging SAMupdatescomputedindependentlyper-accelerator (where each accelerator receives a
                                  subset of size m of a batch, as described in Section 3). This latter setting is equivalent to modifying
                                  the SAM objective (equation 1) to sum over a set of independent  maximizations, each performed
                                  on a sum of per-data-point losses on a disjoint subset of m data points, rather than performing the
                                   maximization over a global sum over the training set (which would be equivalent to setting m
                                  to the total training set size). We term the associated measure of sharpness of the loss landscape
                                  m-sharpness.
                                  To better understand the effect of m on SAM, we train a small ResNet on CIFAR-10 using SAM
                                  with a range of values of m. As seen in Figure 3 (middle), smaller values of m tend to yield models
                                  having better generalization ability. This relationship fortuitously aligns with the need to parallelize
                                  across multiple accelerators in order to scale training for many of today‚Äôs models.
                                  Intriguingly, the m-sharpness measure described above furthermore exhibits better correlation with
                                                                                                                                              7
                                  models‚Äô actual generalization gaps as m decreases, as demonstrated by Figure 3 (right) . In partic-
                                  ular, this implies that m-sharpness with m < n yields a better predictor of generalization than the
                                  full-training-set measure suggested by Theorem 1 in Section 2 above, suggesting an interesting new
                                  avenue of future work for understanding generalization.
                                  4.2     HESSIAN SPECTRA
                                  Motivated by the connection between geometry of the loss landscape and generalization, we con-
                                  structed SAM to seek out minima of the training loss landscape having both low loss value and low
                                  curvature (i.e., low sharpness). To further conÔ¨Årm that SAM does in fact Ô¨Ånd minima having low
                                  curvature, we compute the spectrum of the Hessian for a WideResNet40-10 trained on CIFAR-10
                                  for 300 steps both with and without SAM(withoutbatchnorm,whichtendstoobscureinterpretation
                                  of the Hessian), at different epochs during training. Due to the parameter space‚Äôs dimensionality, we
                                  approximate the Hessian spectrum using the Lanczos algorithm of Ghorbani et al. (2019).
                                  Figure 3 (left) reports the resulting Hessian spectra. As expected, the models trained with SAM
                                  converge to minima having lower curvature, as seen in the overall distribution of eigenvalues, the
                                      7We follow the rigorous framework of Jiang et al. (2019), reporting the mutual information between
                                  the m-sharpness measure and generalization on the two publicly available tasks from the Predicting gen-
                                  eralization in deep learning NeurIPS2020 competition.               https://competitions.codalab.org/
                                  competitions/25301
                                                                                               8
                       Published as a conference paper at ICLR 2021
                       maximumeigenvalue (Œª    ) at convergence (approximately 24 without SAM, 1.0 with SAM), and
                                            max
                       the bulk of the spectrum (the ratio Œªmax/Œª5, commonly used as a proxy for sharpness (Jastrzebski
                       et al., 2020); up to 11.4 without SAM, and 2.6 with SAM).
                       5   RELATED WORK
                       Theideaofsearchingfor‚ÄúÔ¨Çat‚ÄùminimacanbetracedbacktoHochreiter&Schmidhuber(1995),and
                       its connection to generalization has seen signiÔ¨Åcant study (Shirish Keskar et al., 2016; Dziugaite &
                       Roy, 2017; Neyshabur et al., 2017; Dinh et al., 2017). In a recent large scale empirical study, Jiang
                       et al. (2019) studied 40 complexity measures and showedthatasharpness-basedmeasurehashighest
                       correlation with generalization, which motivates penalizing sharpness. Hochreiter & Schmidhuber
                       (1997) was perhaps the Ô¨Årst paper on penalizing the sharpness, regularizing a notion related to Min-
                       imumDescription Length (MDL). Other ideas which also penalize sharp minima include operating
                       on diffused loss landscape (Mobahi, 2016) and regularizing local entropy (Chaudhari et al., 2016).
                       Anotherdirection is to not penalize the sharpness explicitly, but rather average weights during train-
                       ing; Izmailov et al. (2018) showed that doing so can yield Ô¨Çatter minima that can also generalize
                       better. However, the measures of sharpness proposed previously are difÔ¨Åcult to compute and differ-
                       entiate through. In contrast, SAM is highly scalable as it only needs two gradient computations per
                       iteration. The concurrent work of Sun et al. (2020) focuses on resilience to random and adversarial
                       corruption to expose a model‚Äôs vulnerabilities; this work is perhaps closest to ours. Our work has a
                       different basis: we develop SAM motivated by a principled starting point in generalization, clearly
                       demonstrate SAM‚Äôs efÔ¨Åcacy via rigorous large-scale empirical evaluation, and surface important
                       practical and theoretical facets of the procedure (e.g., m-sharpness). The notion of all-layer margin
                       introduced by Wei & Ma (2020) is closely related to this work; one is adversarial perturbation over
                       the activations of a network and the other over its weights, and there is some coupling between these
                       two quantities.
                       6   DISCUSSION AND FUTURE WORK
                       In this work, we have introduced SAM, a novel algorithm that improves generalization by simulta-
                       neously minimizing loss value and loss sharpness; we have demonstrated SAM‚Äôs efÔ¨Åcacy through a
                       rigorous large-scale empirical evaluation. We have surfaced a number of interesting avenues for fu-
                       ture work. Onthetheoretical side, the notion of per-data-point sharpness yielded by m-sharpness (in
                       contrast to global sharpness computed over the entire training set, as has typically been studied in the
                       past) suggests an interesting new lens through which to study generalization. Methodologically, our
                       results suggest that SAM could potentially be used in place of Mixup in robust or semi-supervised
                       methods that currently rely on Mixup (giving, for instance, MentorSAM). We leave to future work
                       a more in-depth investigation of these possibilities.
                       7   ACKNOWLEDGMENTS
                       Wethank our colleagues at Google ‚Äî Atish Agarwala, Xavier Garcia, Dustin Tran, Yiding Jiang,
                       BasilMustafa,SamyBengio‚Äîfortheirfeedbackandinsightfuldiscussions. WealsothanktheJAX
                       andFLAXteamsforgoingaboveandbeyondtosupportourimplementation. WearegratefultoSven
                       Gowal for his help in replicating EfÔ¨ÅcientNet using JAX, and Justin Gilmer for his implementation
                       of the Lanczos algorithm8 used to generate the Hessian spectra. We thank Niru Maheswaranathan
                       for his matplotlib mastery. We also thank David Samuel for providing a PyTorch implementation of
                       SAM9.
                       REFERENCES
                       Naman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang. Revisiting the gener-
                         alization of adaptive gradient methods, 2020. URL https://openreview.net/forum?
                         id=BJl6t64tvr.
                          8https://github.com/google/spectral-density
                          9https://github.com/davda54/sam
                                                                9
                          Published as a conference paper at ICLR 2021
                          James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
                            Maclaurin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy
                            programs, 2018. URL http://github.com/google/jax.
                          Niladri S Chatterji, Behnam Neyshabur, and Hanie Sedghi. The intriguing role of module criticality
                            in the generalization of deep networks. In International Conference on Learning Representations,
                            2020.
                          Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
                            Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing Gradi-
                            ent Descent Into Wide Valleys. arXiv e-prints, art. arXiv:1611.01838, November 2016.
                          Pengfei Chen, Benben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing
                            deep neural networks trained with noisy labels. CoRR, abs/1905.05040, 2019. URL http:
                            //arxiv.org/abs/1905.05040.
                                                                            ¬¥
                          Ekin Dogus Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le.           Au-
                            toaugment: Learning augmentation policies from data. CoRR, abs/1805.09501, 2018. URL
                            http://arxiv.org/abs/1805.09501.
                          J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
                            Image Database. In CVPR09, 2009.
                          Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks
                            with cutout. CoRR, abs/1708.04552, 2017. URL http://arxiv.org/abs/1708.04552.
                          Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
                            for deep nets. arXiv preprint arXiv:1703.04933, 2017.
                          AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
                            Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
                            reit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at
                            Scale. arXiv e-prints, art. arXiv:2010.11929, October 2020.
                          Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
                          John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
                            stochastic optimization. Journal of machine learning research, 12(7), 2011.
                          Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
                            deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
                            arXiv:1703.11008, 2017.
                          Xavier Gastaldi.   Shake-shake regularization.  CoRR, abs/1705.07485, 2017.      URL http:
                            //arxiv.org/abs/1705.07485.
                          Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An Investigation into Neural Net Optimiza-
                            tion via Hessian Eigenvalue Density. arXiv e-prints, art. arXiv:1901.10159, January 2019.
                          Dongyoon Han, Jiwhan Kim, and Junmo Kim.          Deep pyramidal residual networks.     CoRR,
                            abs/1610.02915, 2016. URL http://arxiv.org/abs/1610.02915.
                                                                                                       ¬®
                          Ethan Harris, Antonia Marcu, Matthew Painter, Mahesan Niranjan, Adam Prugel-Bennett, and
                            Jonathon Hare.    FMix: Enhancing Mixed Sample Data Augmentation.        arXiv e-prints, art.
                            arXiv:2002.12047, February 2020.
                          KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
                            nition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.
                          Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
                            lecture 6a overview of mini-batch gradient descent.
                                               ¬®
                          Sepp Hochreiter and Jurgen Schmidhuber. Simplifying neural nets by discovering Ô¨Çat minima. In
                            Advances in neural information processing systems, pp. 529‚Äì536, 1995.
                                                                        10
                         Published as a conference paper at ICLR 2021
                                              ¬®
                         SeppHochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):1‚Äì42, 1997.
                         Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep Networks with
                            Stochastic Depth. arXiv e-prints, art. arXiv:1603.09382, March 2016.
                         J. Huang, L. Qu, R. Jia, and B. Zhao. O2u-net: A simple noisy label detection approach for deep
                            neural networks. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp.
                            3325‚Äì3333, 2019.
                         Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, Hy-
                            oukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen.         GPipe: Ef-
                            Ô¨Åcient Training of Giant Neural Networks using Pipeline Parallelism.  arXiv e-prints, art.
                            arXiv:1811.06965, November 2018.
                         Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
                            reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
                         Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wil-
                            son. Averaging Weights Leads to Wider Optima and Better Generalization. arXiv e-prints, art.
                            arXiv:1803.05407, March 2018.
                                                           ¬¥
                         Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gener-
                            alization in neural networks. CoRR, abs/1806.07572, 2018. URL http://arxiv.org/abs/
                            1806.07572.
                         Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
                            Cho, and Krzysztof Geras. The Break-Even Point on Optimization Trajectories of Deep Neural
                            Networks. arXiv e-prints, art. arXiv:2002.09572, February 2020.
                         Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Regularizing
                            very deep neural networks on corrupted labels. CoRR, abs/1712.05055, 2017. URL http:
                            //arxiv.org/abs/1712.05055.
                         Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond Synthetic Noise: Deep Learning on
                            Controlled Noisy Labels. arXiv e-prints, art. arXiv:1911.09781, November 2019.
                         Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
                            generalization measures and where to Ô¨Ånd them. arXiv preprint arXiv:1912.02178, 2019.
                         Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv e-prints,
                            art. arXiv:1412.6980, December 2014.
                         Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
                            and Neil Houlsby. Big transfer (bit): General visual representation learning, 2020.
                         Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do Better ImageNet Models Transfer Better?
                            arXiv e-prints, art. arXiv:1805.08974, May 2018.
                         John Langford and Rich Caruana. (not) bounding the true error. In Advances in Neural Information
                            Processing Systems, pp. 809‚Äì816, 2002.
                         Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-
                            tion. Annals of Statistics, pp. 1302‚Äì1338, 2000.
                         Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo Li, and Jinwoo Shin. Robust inference via
                            generative classiÔ¨Åers for handling noisy labels, 2019.
                         HaoLi,ZhengXu,GavinTaylor,andTomGoldstein. Visualizing the loss landscape of neural nets.
                            CoRR,abs/1712.09913, 2017. URL http://arxiv.org/abs/1712.09913.
                         Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment.
                            CoRR,abs/1905.00397, 2019. URL http://arxiv.org/abs/1905.00397.
                         James Martens and Roger Grosse. Optimizing Neural Networks with Kronecker-factored Approxi-
                            mate Curvature. arXiv e-prints, art. arXiv:1503.05671, March 2015.
                                                                      11
                          Published as a conference paper at ICLR 2021
                          David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual confer-
                            ence on Computational learning theory, pp. 164‚Äì170, 1999.
                          Hossein Mobahi. Training recurrent neural networks by diffusion. CoRR, abs/1601.04114, 2016.
                            URLhttp://arxiv.org/abs/1601.04114.
                          Y. E. Nesterov. A method for solving the convex programming problem with convergence rate
                            o(1/k2). Dokl. Akad. Nauk SSSR, 269:543‚Äì547, 1983. URL https://ci.nii.ac.jp/
                            naid/10029946121/en/.
                          Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
                            digits in natural images with unsupervised feature learning. 2011.
                          BehnamNeyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
                            ization in deep learning. In Advances in neural information processing systems, pp. 5947‚Äì5956,
                            2017.
                          Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc V. Le, and Ruoming Pang.
                            Domain adaptive transfer learning with specialist models. CoRR, abs/1811.07056, 2018. URL
                            http://arxiv.org/abs/1811.07056.
                          Eric Arazo Sanchez, Diego Ortego, Paul Albert, Noel E. O‚ÄôConnor, and Kevin McGuinness. Un-
                            supervised label noise modeling and loss correction.   CoRR, abs/1904.11238, 2019.    URL
                            http://arxiv.org/abs/1904.11238.
                          Nitish Shirish Keskar and Richard Socher. Improving Generalization Performance by Switching
                            from AdamtoSGD. arXive-prints, art. arXiv:1712.07628, December 2017.
                          Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
                            ter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.
                            arXiv e-prints, art. arXiv:1609.04836, September 2016.
                          Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
                            Dropout: a simple way to prevent neural networks from overÔ¨Åtting. The journal of machine
                            learning research, 15(1):1929‚Äì1958, 2014.
                          Xu Sun, Zhiyuan Zhang, Xuancheng Ren, Ruixuan Luo, and Liangyou Li. Exploring the Vul-
                            nerability of Deep Neural Networks: A Study of Parameter Corruption. arXiv e-prints, art.
                            arXiv:2006.05620, June 2020.
                          Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-
                            thinking the inception architecture for computer vision, 2015.
                          Mingxing Tan and Quoc V. Le. EfÔ¨ÅcientNet: Rethinking Model Scaling for Convolutional Neural
                            Networks. arXiv e-prints, art. arXiv:1905.11946, May 2019.
                          Colin Wei and Tengyu Ma. Improved sample complexities for deep neural networks and robust
                            classiÔ¨Åcation via an all-layer margin. In International Conference on Learning Representations,
                            2020.
                          Longhui Wei, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang, and Qi Tian. Circumventing
                            Outliers of AutoAugment with Knowledge Distillation. arXiv e-prints, art. arXiv:2003.11342,
                            March2020.
                          Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
                            value of adaptive gradient methods in machine learning. In Advances in neural information pro-
                            cessing systems, pp. 4148‚Äì4158, 2017.
                          HanXiao,KashifRasul,andRolandVollgraf. Fashion-mnist: anovelimagedatasetforbenchmark-
                            ing machine learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/
                            abs/1708.07747.
                          Yoshihiro Yamada, Masakazu Iwamura, and Koichi Kise.        Shakedrop regularization.  CoRR,
                            abs/1802.02375, 2018. URL http://arxiv.org/abs/1802.02375.
                                                                       12
           Published as a conference paper at ICLR 2021
           Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016.
            URLhttp://arxiv.org/abs/1605.07146.
           Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
            deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016. URL http:
            //arxiv.org/abs/1611.03530.
           FanZhang,MengLi,GuishengZhai,andYizhaoLiu. Multi-branchandmulti-scaleattentionlearn-
            ing for Ô¨Åne-grained visual categorization, 2020.
           HongyiZhang,MoustaphaCisse,YannNDauphin,andDavidLopez-Paz.mixup: Beyondempirical
            risk minimization. arXiv preprint arXiv:1710.09412, 2017.
           Zhilu Zhang and Mert R. Sabuncu. Generalized cross entropy loss for training deep neural networks
            with noisy labels. CoRR, abs/1805.07836, 2018. URL http://arxiv.org/abs/1805.
            07836.
                               13
                             Published as a conference paper at ICLR 2021
                             A APPENDIX
                             A.1   PACBAYESIANGENERALIZATION BOUND
                             Below, we state a generalization bound based on sharpness.
                             Theorem 2. For any œÅ > 0 and any distribution D, with probability 1 ‚àí Œ¥ over the choice of the
                             training set S ‚àº D,                 v
                                                                 u                         q        2!
                                                                 u              kwk2           log(n)              n
                                                                 u                  2                                    Àú
                                                                 uklog 1+ 2             1+                 +4log +O(1)
                                                                 t                œÅ               k                Œ¥
                               LD(w)‚â§ max LS(w+)+                                            n‚àí1                               (4)
                                           kk2‚â§œÅ
                             where n = |S|, k is the number of parameters and we assumed L (w) ‚â§ E                   [L (w+)].
                                                                                                D          i‚àºN(0,œÅ)    D
                             The condition LD(w) ‚â§ E               [LD(w+)]meansthatadding Gaussian perturbation should
                                                         i‚àºN(0,œÅ)
                             not decrease the test error. This is expected to hold in practice for the Ô¨Ånal solution but does not
                             necessarily hold for any w.
                             Proof. First, note that the right hand side of the bound in the theorem statement is lower bounded
                                p                 2   2                                             2      2
                             by    klog(1+kwk /œÅ )/(4n) which is greater than 1 when kwk > œÅ (exp(4n/k) ‚àí 1). In
                                                  2                                                 2
                             that case, the right hand side becomes greater than 1 in which case the inequality holds trivially.
                                                                                                       2     2
                             Therefore, in the rest of the proof, we only consider the case when kwk ‚â§ œÅ (exp(4n/k) ‚àí 1).
                                                                                                       2
                             The proof technique we use here is inspired from Chatterji et al. (2020). Using PAC-Bayesian
                             generalization bound McAllester (1999) and following Dziugaite & Roy (2017), the following gen-
                             eralization bound holds for any prior P over parameters with probability 1 ‚àí Œ¥ over the choice of
                             the training set S, for any posterior Q over parameters: s
                                                                                          KL(Q||P)+logn
                                                Ew‚àºQ[LD(w)]‚â§Ew‚àºQ[LS(w)]+                                      Œ¥                 (5)
                                                                                                2(n‚àí1)
                             Moreover, if P = N(¬µ ,œÉ2I) and Q = N(¬µ ,œÉ2I), then the KL divergence can be written as
                                                      P   P                    Q Q
                             follows:                                                                   !
                                                                      2                 2                  2
                                                                1 kœÉ +k¬µ ‚àí¬µ k                            œÉ
                                               KL(P||Q)=              Q       P      Q 2 ‚àík+klog           P                    (6)
                                                                2            œÉ2                          œÉ2
                                                                              P                            Q
                             GivenaposteriorstandarddeviationœÉ ,onecouldchooseapriorstandarddeviationœÉ tominimize
                                                                    Q                                                P
                                                                                                                    10
                             the above KL divergence and hence the generalization bound by taking the derivative       of the above
                                                                                                    ‚àó 2      2                 2
                             KLwith respect to œÉ     and setting it to zero. We would then have œÉ       =œÉ +k¬µ ‚àí¬µ k /k.
                                                   P                                                P        Q       P      Q 2
                             However, since œÉ    should be chosen before observing the training data S and ¬µ ,œÉ       could depend
                                              P                                                                Q Q
                             on S, we are not allowed to optimize œÉ       in this way. Instead, one can have a set of predeÔ¨Åned
                                                                       P
                             values for œÉ   and pick the best one in that set. See Langford & Caruana (2002) for the discussion
                                         P
                             around this technique. Given Ô¨Åxed a,b > 0, let T = {cexp((1 ‚àí j)/k)|j ‚àà N} be that predeÔ¨Åned
                             set of values for œÉ2 . If for any j ‚àà N, the above PAC-Bayesian bound holds for œÉ2 = cexp((1 ‚àí
                                                P                                                                  P
                             j)/k) with probability 1 ‚àí Œ¥ with Œ¥      = 6Œ¥ , then by the union bound, all above bounds hold
                                                           j        j     œÄ2j2
                                                                           P
                             simultaneously with probability at least 1 ‚àí    ‚àû 6Œ¥ =1‚àíŒ¥.
                                                                             j=1 œÄ2j2
                             Let œÉ   =œÅ,¬µ =wand¬µ =0.Therefore,wehave:
                                  Q         Q             P
                                                 2                 2       2         2       2
                                                œÉ +k¬µ ‚àí¬µ k /k‚â§œÅ +kwk /k‚â§œÅ (1+exp(4n/k))                                         (7)
                                                 Q       P      Q 2                  2
                                                                                                 2        2
                             Wenowconsidertheboundthatcorrespondstoj = b1‚àíklog((œÅ +kwk /k)/c)c. Wecanensure
                                                                                             2            2
                             that j ‚àà N using inequality equation 7 and by setting c = œÅ (1 + exp(4n/k)). Furthermore, for
                             œÉ2 =cexp((1‚àíj)/k),wehave:
                              P
                                                        2        2        2                2         2   
                                                      œÅ +kwk /k ‚â§œÉ ‚â§exp(1/k) œÅ +kwk /k                                          (8)
                                                                 2       P                           2
                               10Despite the nonconvexity of the function here in œÉ2 , it has a unique stationary point which happens to be
                             its minimizer.                                     P
                                                                               14
                             Published as a conference paper at ICLR 2021
                             Therefore, using the above value for œÉP, KL divergence can be bounded as follows:
                                                           2                 2                2 !
                                                      1 kœÉ +k¬µ ‚àí¬µ k                            œÉ
                                     KL(P||Q)=              Q       P      Q 2 ‚àík+klog           P                              (9)
                                                      2            œÉ2                          œÉ2
                                                                    P                            Q                     !
                                                            2        2                                 2        2      
                                                      1 k(œÅ +kwk /k)                       exp(1/k) œÅ +kwk /k
                                                   ‚â§                  2     ‚àík+klog                              2             (10)
                                                            2         2                                  2
                                                      2    œÅ +kwk /k                                    œÅ
                                                                      2                     !
                                                                            2         2      
                                                      1          exp(1/k) œÅ +kwk /k
                                                   = klog                              2                                       (11)
                                                      2                      œÅ2
                                                      1                  kwk2!
                                                   = 1+klog 1+                 2                                               (12)
                                                      2                    kœÉ2
                                                                             Q
                             Given the bound that corresponds to j holds with probability 1 ‚àí Œ¥ for Œ¥ =        6Œ¥ , the log term in
                                                                                                  j      j    œÄ2j2
                             the bound can be written as:
                                                         n         n        œÄ2j2
                                                     log Œ¥  =log Œ¥ +log 6
                                                          j
                                                                              2 2    2      2        2
                                                                   n        œÄ k log (c/(œÅ +kwk /k))
                                                            ‚â§log     +log                            2
                                                                   Œ¥                       6
                                                                   n        œÄ2k2log2(c/œÅ2)
                                                            ‚â§log Œ¥ +log             6
                                                                   n        œÄ2k2log2(1+exp(4n/k))
                                                            ‚â§log Œ¥ +log                  6
                                                                   n        œÄ2k2(2+4n/k)2
                                                            ‚â§log Œ¥ +log              6
                                                            ‚â§log n +2log(6n+3k)
                                                                   Œ¥
                             Therefore, the generalization bound can be written as follows:
                                                                                  v                    
                                                                                  u1               kwk2      1        n
                                                                                  u klog 1+            2  + +log +2log(6n+3k)
                             E          [L (w+)] ‚â§ E               [L (w+)]+t4                   kœÉ2       4        Œ¥
                              i‚àºN(0,œÉ)    D              i‚àºN(0,œÉ)   S                                      n‚àí1
                                                                                                                               (13)
                                                                                             2
                             In the above bound, we have  ‚àº N(0,œÉ). Therefore, kk has chi-square distribution and by
                                                              i                              2
                             Lemma1inLaurent&Massart(2000),wehavethatforanypositivet:
                                                              2       2      2‚àö           2
                                                        P(kk2 ‚àíkœÉ ‚â•2œÉ           kt+2tœÉ ) ‚â§ exp(‚àít)                            (14)
                                                                ‚àö
                             Therefore, with probability 1 ‚àí 1/    nwehavethat:
                                                                                                    r        !
                                                           ‚àö            q ‚àö                                    2
                                             2      2                                      2           ln(n)         2
                                          kk ‚â§œÉ (2ln( n)+k+2 kln( n))‚â§œÉ k 1+                                    ‚â§œÅ
                                             2                                                           k
                                                                               15
                             Published as a conference paper at ICLR 2021
                             Substituting the above value for œÉ back to the inequality and using theorem‚Äôs assumption gives us
                             following inequality:          ‚àö                              ‚àö
                                        LD(w)‚â§(1‚àí1/ n) max LS(w+)+1/ n
                                                                 kk ‚â§œÅ
                                                   v                2
                                                   u                            q         !
                                                   u                                        2
                                                   u1                kwk2           log(n)             n
                                                        klog 1+          2   1+                 +log +2log(6n+3k)
                                                   u                   2
                                                   t4                 œÅ                k               Œ¥
                                                 +                                    n‚àí1
                                                 ‚â§ max LS(w+)+
                                                    kk2‚â§œÅ
                                                   v                                        !
                                                   u                           q        2
                                                   u               kwk2            log(n)              n
                                                   uklog 1+ 22 1+                              +4log +8log(6n+3k)
                                                   u                 œÅ               k                 Œ¥
                                                 +t                                   n‚àí1
                              B ADDITIONALEXPERIMENTAL RESULTS
                              B.1   SVHNANDFASHION-MNIST
                             We report in table 5 results obtained on SVHN and Fashion-MNIST datasets. On these datasets,
                             SAMallows a simple WideResNet to reach or push state-of-the-art accuracy (0.99% error rate for
                             SVHN,3.59%forFashion-MNIST).
                             ForSVHN,weusedalltheavailabledata(73257digitsfortrainingset+531131additionalsamples).
                             For auto-augment, we use the best policy found on this dataset as described in (Cubuk et al., 2018)
                             plus cutout (Devries & Taylor, 2017). For Fashion-MNIST, the auto-augmentation line correspond
                             to cutout only.
                                                         Table 5: Results on SVHN and Fashion-MNIST.
                                                                                         SVHN                  Fashion-MNIST
                                 Model                        Augmentation         SAM         Baseline       SAM         Baseline
                                 Wide-ResNet-28-10            Basic              1.42          1.58         3.98          4.57
                                                                                      ¬±0.02        ¬±0.03         ¬±0.05        ¬±0.07
                                 Wide-ResNet-28-10            Autoaugment        0.99          1.14         3.61          3.86
                                                                                      ¬±0.01        ¬±0.04         ¬±0.06        ¬±0.14
                                 Shake-Shake (26 2x96d)       Basic              1.44          1.58         3.97          4.37
                                                                                      ¬±0.02        ¬±0.05         ¬±0.09        ¬±0.06
                                 Shake-Shake (26 2x96d)       Autoaugment        1.07          1.03         3.59          3.76
                                                                                      ¬±0.02        ¬±0.02         ¬±0.01        ¬±0.07
                              C EXPERIMENTDETAILS
                              C.1   HYPERPARAMETERS FOR EXPERIMENTS
                             Wereportintable6thehyper-parametersselectedbygridsearchfortheCIFARexperiments,andthe
                             onesforSVHNandFashion-MNISTin7. ForCIFAR-10,CIFAR-100,SVHNandFashion-MNIST,
                             weuseabatchsizeof256anddeterminethelearningrateandweightdecayusedtotraineachmodel
                             via a joint grid search prior to applying SAM; all other model hyperparameter values are identical
                             to those used in prior work.
                             For the Imagenet results (ResNet models), the models are trained for 100, 200 or 400 epochs on
                             Google Cloud TPUv3 32 cores with a batch size of 4096. The initial learning rate is set to 1.0 and
                             decayedusingacosineschedule. Weightdecayissetto0.0001withSGDoptimizerandmomentum
                             =0.9.
                             Finally, for the noisy label experiments, we also found œÅ by gridsearch, computing the accuracy
                             on a (non-noisy) validation set composed of a random subset of 10% of the usual CIFAR training
                             samples. We report the validation accuracy of the bootstrapped version of SAM for different levels
                             of noise and different œÅ in table 8.
                                                                                 16
                           Published as a conference paper at ICLR 2021
                                         Table 6: Hyper-parameter used to produce the CIFAR-{10,100} results
                                          CIFARDataset                LR      WD      œÅ(CIFAR-10)      œÅ(CIFAR-100)
                                 WRN28-10(200epochs)                  0.1    0.0005        0.05              0.1
                                 WRN28-10(1800epochs)                0.05    0.001         0.05              0.1
                                 WRN26-2x6ShakeShake                 0.02    0.0010        0.02             0.05
                                 Pyramid vanilla                     0.05    0.0005        0.05              0.2
                                 Pyramid ShakeDrop (CIFAR-10)        0.02    0.0005        0.05               -
                                 Pyramid ShakeDrop (CIFAR-100)       0.05    0.0005          -              0.05
                                   Table 7: Hyper-parameter used to produce the SVHN and Fashion-MNIST results
                                                                              LR      WD        œÅ
                                                    SVHN       WRN            0.01   0.0005    0.01
                                                               ShakeShake     0.01   0.0005    0.01
                                                    Fashion    WRN            0.1    0.0005    0.05
                                                               ShakeShake     0.1    0.0005    0.02
                           C.2   FINETUNING DETAILS
                           Weights are initialized to the values provided by the publicly available checkpoints, except the last
                           dense layer, which change size to accomodate the new number of classes, that is randomly initial-
                                                                         ‚àí5
                           ized. We train all models with weight decay 1e   as suggested in (Tan & Le, 2019), but we reduce
                           the learning rate to 0.016 as the models tend to diverge for higher values. We use a batch size of
                           1024 on Google Cloud TPUv3 64 cores and cosine learning rate decay. Because other works train
                           with batch size of 256, we train for 5k steps instead of 20k. We freeze the batch norm statistics and
                           use them for normalization, effectively using the batch norm as we would at test time 11. We train
                           the models using SGD with momentum 0.9 and cosine learning rate decay. For EfÔ¨Åcientnet-L2, we
                           use this time a batch size 512 to save memory and adjusted the number of training steps accord-
                           ingly. For CIFAR, we use the same autoaugment policy as in the previous experiments. We do not
                           use data augmentation for the other datasets, applying the same preprocessing as for the Imagenet
                           experiments. We also scale down the learning rate to 0.008 as the batch size is now twice as small.
                           Weused Google Cloud TPUv3 128 cores. All other parameters stay the same. For Imagenet, we
                           trained both models from checkpoint for 10 epochs using a learning rate of 0.1 and œÅ = 0.05. We
                           do not randomly initialize the last layer as we did for the other datasets, but instead use the weights
                           included in the checkpoint.
                           C.3   EXPERIMENTAL RESULTS WITH œÅ = 0.05
                           A big sensitivity to the choice of hyper-parameters would make a method less easy to use. To
                           demonstrate that SAM performs even when œÅ is not Ô¨Ånely tuned, we compiled the table for the
                           CIFARandtheÔ¨Ånetuning experiments using œÅ = 0.05. Please note that we already used œÅ = 0.05
                           for all Imagenet experiments. We report those scores in table 9 and 10.
                           C.4   ABLATION OF THE SECOND ORDER TERMS
                           As described in section 2, computing the gradient of the sharpness aware objective yield some
                           second order terms that are more expensive to compute. To analyze this ablation more in depth,
                           we trained a WideResNet-40x2 on CIFAR-10 using SAM with and without discarding the second
                           order terms during training. We report the cosine similarity of the two updates in Ô¨Ågure 5, along the
                                                                                                                      ÀÜ
                           training trajectory of both experiments. We also report the training error rate (evaluated at w+(w))
                           and the test error rate (evaluated at w).
                           Weobserve that during the Ô¨Årst half of the training, discarding the second order terms does not im-
                           pact the general direction of the training, as the cosine similarity between the Ô¨Årst and second order
                           updates are very close to 1. However, when the model nears convergence, the similarity between
                             11Wefoundanecdotal evidence that this makes the Ô¨Ånetuning more robust to overtraining.
                                                                          17
                             Published as a conference paper at ICLR 2021
                                                                    20%       40%      60%       80%
                                                             0     15.0%     31.2%    52.3%     73.5%
                                                           0.01    13.7%     28.7%    50.1%     72.9%
                                                           0.02    12.8%     27.8%    48.9%     73.1%
                                                           0.05    11.6%     25.6%    47.1%     21.0%
                                                            0.1    4.6%      6.0%      8.7%     56.1%
                                                            0.2    5.3%      7.4%     23.3%     77.1%
                                                            0.5    17.6%     40.9%    80.1%     89.9%
                              Table 8: Validation accuracy of the bootstrapped-SAM for different levels of noise and different œÅ
                                                                                          CIFAR-10            CIFAR-100
                                     Model                          Augmentation      œÅ = 0.05     SGD rho=0.05         SGD
                                     WRN-28-10(200epochs)           Basic                 2.7       3.5       16.5       18.8
                                     WRN-28-10(200epochs)           Cutout                2.3       2.6       14.9       16.9
                                     WRN-28-10(200epochs)           AA                    2.1       2.3       13.6       15.8
                                     WRN-28-10(1800epochs)          Basic                 2.4       3.5       16.3       19.1
                                     WRN-28-10(1800epochs)          Cutout                2.1       2.7       14.0       17.4
                                     WRN-28-10(1800epochs)          AA                    1.6       2.2       12.8       16.1
                                     WRN26-2x6ss                    Basic                 2.4       2.7       15.1       17.0
                                     WRN26-2x6ss                    Cutout                2.0       2.3       14.2       15.7
                                     WRN26-2x6ss                    AA                    1.7       1.9       12.8       14.1
                                     PyramidNet                     Basic                 2.1       4.0       15.4       19.7
                                     PyramidNet                     Cutout                1.6       2.5       13.1       16.4
                                     PyramidNet                     AA                    1.4       1.9       12.1       14.6
                                     PyramidNet+ShakeDrop           Basic                 2.1       2.5       13.3       14.5
                                     PyramidNet+ShakeDrop           Cutout                1.6       1.9       11.3       11.8
                                     PyramidNet+ShakeDrop           AA                    1.4       1.6       10.3       10.6
                             Table 9: Results for the CIFAR-10/CIFAR-100 experiments, using œÅ = 0.05 for all mod-
                             els/datasets/augmentations
                             both types of updates becomes weaker. Fortunately, the model trained without the second order
                             terms reaches a lower test error, showing that the most efÔ¨Åcient method is also the one providing the
                             best generalization on this example. The reason for this is quite unclear and should be analyzed in
                             follow up work.
                             C.5    CHOICE OF P-NORM
                             Our theorem is derived for p = 2, although generalizations can be considered for p ‚àà [1,+‚àû] (the
                             expression of the bound becoming way more involved). Empirically, we validate that the choice
                             p = 2 is optimal by training a wide ResNet on CIFAR-10 with SAM for p = ‚àû (in which case we
                                   ÀÜ                                                   ÀÜ              œÅ
                             have (w) = œÅsign(‚àá L (w))) and p = 2 (giving (w) =                           2(‚àá L (w))). We do
                                                      w S                                       ||‚àá L (w)||      w S
                                                                                                   w S      2
                             not consider the case p = 1 which would give us a perturbation on a single weight. As an additional
                                                                                                                     ÀÜ          œÅ
                             ablation study, we also use random weight perturbations of a Ô¨Åxed Euclidean norm: (w) =             2 z
                                                                                                                              ||z||
                             with z ‚àº N(0,I ). We report the test accuracy of the model in Ô¨Ågure 6.                               2
                                               d
                             Weobserve that adversarial perturbations outperform random perturbations, and that using p = 2
                             yield superior accuracy on this example.
                             C.6    SEVERAL ITERATIONS IN THE INNER MAXIMIZATION
                             Toempiricallyverifythatthelinearizationoftheinnerproblemissensible,wetrainedaWideResNet
                             ontheCIFARdatasetsusingavariant of SAM that performs several iterations of projected gradient
                             ascent to estimate max L(w + ). We report the evolution of max L(w + ) ‚àí L(w) during
                                                                                                      
                             training (where L stands for the training error rate computed on the current batch) in Figure 7, along
                                                                                18
                             Published as a conference paper at ICLR 2021
                                              Dataset          EfÔ¨Åcientnet-b7        EfÔ¨Åcientnet-b7       EfÔ¨Åcientnet-b7
                                                              +SAM(optimal)        +SAM(œÅ=0.05)
                                         FGVC Aircraft              6.80                   7.06                8.15
                                             Flowers                0.63                   0.81                1.16
                                        Oxford IIIT Pets            3.97                   4.15                4.24
                                          Stanford Cars             5.18                   5.57                5.94
                                            CIFAR-10                0.88                   0.88                0.95
                                           CIFAR-100                7.44                   7.56                7.68
                                             Birdsnap               13.64                 13.64                14.30
                                             Food101                7.02                   7.06                7.17
                                       Table 10: Results for the Ô¨Ånetuning experiments, using œÅ = 0.05 for all datasets.
                                 0.10      order                                     )1.0
                                           second                                     t
                                 0.08                                               d ,0.8
                                           first                                    n v
                                                                                    2 d
                                 0.06      metric                                    wa0.6
                                           train_error_rate                          ,t
                                           test_error_rate                            ,
                                                                                    h v
                                                                                    t d0.4
                                 0.04                                               1wa
                                                                                     (0.2       order
                                                                                     s
                                Train error rate0.02                                 o          second
                                                                                     c0.0       first
                                 0.00    10000      20000     30000      40000             0      10000    20000    30000    40000
                                                       step                                                step
                             Figure 4: Training and test error for the Ô¨Årst      Figure 5: Cosine similarity between the Ô¨Årst and
                             and second order version of the algorithm.          second order updates.
                             with the test accuracy and the estimated sharpness (max L(w + ) ‚àí L(w)) at the end of training
                             in Table 11; we report means and standard deviations across 20 runs.
                             For most of the training, one projected gradient step (as used in standard SAM) is sufÔ¨Åcient to
                             obtain a good approximation of the  found with multiple inner maximization steps. We however
                             observe that this approximation becomes weaker near convergence, where doing several iterations
                             of projected gradient ascent yields a better  (for example, on CIFAR-10, the maximum loss found
                             oneachbatchisabout3%morewhendoing5stepsofinnermaximization,comparedtowhendoing
                             a single step). That said, as seen in Table 11, the test accuracy is not strongly affected by the number
                             of inner maximization iterations, though on CIFAR-100 it does seem that several steps outperform
                             a single step in a statistically signiÔ¨Åcant way.
                              Numberofprojected                    CIFAR-10                              CIFAR-100
                                  gradient steps        Test error   Estimated sharpness      Test error    Estimated sharpness
                                         1             2.77               0.17               16.72               0.82
                                                            ¬±0.03              ¬±0.03               ¬±0.08              ¬±0.05
                                         2             2.76               0.82               16.59               1.83
                                                            ¬±0.03              ¬±0.03               ¬±0.08              ¬±0.05
                                         3             2.73               1.49               16.62               2.36
                                                            ¬±0.04              ¬±0.05               ¬±0.09              ¬±0.03
                                         5             2.77               2.26               16.60               2.82
                                                            ¬±0.03              ¬±0.05               ¬±0.06              ¬±0.04
                             Table 11: Test error rate and estimated sharpness (max L(w+)‚àíL(w)) at the end of the training.
                                                                                19
                                                                                         Published as a conference paper at ICLR 2021
                                                                                                                                                                                   0.050
                                                                                                                                                                                   0.048
                                                                                                                                                                                   0.046
                                                                                                                                                                                   0.044
                                                                                                                                                                                   0.042
                                                                                                                                                                                error_rate0.040
                                                                                                                                                                                   0.038                                                                                                                        Constraint
                                                                                                                                                                                                                                                                                                                ||   ||2 =
                                                                                                                                                                                   0.036                                                                                                                        ||   ||  =
                                                                                                                                                                                                                                                                                                                w N(0,1)
                                                                                                                                                                                   0.034                                                                                                                        ||w||2 =
                                                                                                                                                                                            10 6           10 5           10 4           10 3          10 2           10 1            100           101            102            103
                                                                                         Figure6: TestaccuracyforaWideResNettrainedonCIFAR-10withSAM,fordifferentperturbation
                                                                                         norms.
                                                                                                     0.05                                                                  Cifar10                                                                                                                                                          Cifar100
                                                                                                 )                                                                                                                                                                 )   0.08                                                                                                                         1 steps
                                                                                                 w   0.04                                                                                                                                                          w                                                                                                                                2 steps
                                                                                                 (                                                                                                                                                                 (
                                                                                                 L                                                                                                                                                                 L   0.06                                                                                                                         3 steps
                                                                                                 )   0.03                                                                                                                                                          )                                                                                                                                5 steps
                                                                                                 +                                                                                                                                                                 +
                                                                                                 w                                    1 steps                                                                                                                      w   0.04
                                                                                                 (   0.02                                                                                                                                                          (
                                                                                                xL                                    2 steps                                                                                                                     xL
                                                                                                a    0.01                             3 steps                                                                                                                     a
                                                                                                m                                     5 steps                                                                                                                     m    0.02
                                                                                                                                             10000                            20000                             30000                                                                                          10000                            20000                             30000
                                                                                                                                                                          Updates                                                                                                                                                           Updates
                                                                                         Figure 7: Evolution of maxL(w + ) ‚àí L(w) vs. training step, for different numbers of inner
                                                                                         projected gradient steps.
                                                                                                                                                                                                                                                       20
