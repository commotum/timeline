arXiv:2310.14820v1 [cs.CL] 23 Oct 2023

ALCUNA: Large Language Models Meet New Knowledge

Xunjian Yin* and Baizhou Huang* and Xiaojun Wan
Wangxuan Institute of Computer Technology, Peking University
Center for Data Science, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{xjyin, hbz19,wanxiaojun}@pku. edu.cn

Abstract

With the rapid development of NLP, large-scale
language models (LLMs) excel in various tasks
across multiple domains now. However, exist-
ing benchmarks may not adequately measure
these models’ capabilities, especially when
faced with new knowledge. In this paper, we ad-
dress the lack of benchmarks to evaluate LLMs’
ability to handle new knowledge, an important
and challenging aspect in the rapidly evolving
world. We propose an approach called Know-
Gen that generates new knowledge by altering
existing entity attributes and relationships, re-
sulting in artificial entities that are distinct from
real-world entities. With KnowGen, we intro-
duce a benchmark named ALCUNA to assess
LLMs’ abilities in knowledge understanding,
differentiation, and association. We benchmark
several LLMs, reveals that their performance
in face of new knowledge is not satisfactory,
particularly in reasoning between new and in-
ternal knowledge. We also explore the impact
of entity similarity on the model’s understand-
ing of entity knowledge and the influence of
contextual entities. We appeal to the need for
caution when using LLMs in new scenarios or
with new knowledge, and hope that our bench-
marks can help drive the development of LLMs
in face of new knowledge.

1 Introduction

Large-scale language models (LLMs) have made
impressive progress in the last few years (Brown
et al., 2020; Ouyang et al., 2022; Touvron et al.,
2023; OpenAI, 2023), which perform surprisingly
well on various tasks on various domains, to the
extent that many traditional benchmarks (Thorne
et al., 2018; Wang et al., 2018) are no longer suffi-
cient to measure the capabilities of LLMs. There-
fore, some new benchmarks have been proposed
to evaluate the ability of the model to solve more
complex tasks such as college entrance exams, law

*These authors contributed equally to this work.

school admission tests, math competitions and so
on (Hendrycks et al., 2021; Guo et al., 2023; Zhong
et al., 2023). LLMs also achieve promising results
on these benchmarks.

However, it is surprising that there is not yet a
benchmark to evaluate the ability of large models
in face of new knowledge, which is very important
and challenging. Why is this evaluation important?
Firstly, we are in a changing world, where models
encounter new knowledge frequently in practice.
And some work (Peng et al., 2023) is exploring
retrieval methods to augment large models, which
will also cause the models to meet new knowledge
frequently. So of course we expect the model to
perform well in such situations, because re-training
the model every time is very expensive and unre-
alistic. Secondly, as Elangovan et al. (2021) men-
tioned, the presence of overlap between the train-
ing and test data can lead to a misestimation of the
model’s memory ability as generalization ability,
especially nowadays when LLMs are trained on an
enormous amount of data. Whereas, evaluation on
new knowledge does not need to worry about such
data leakage, as new knowledge can usually lead
to new data and thus more reliable and valuable
assessment of the model’s ability.

While such evaluation is important, it is chal-
lenging to construct the benchmark. The reason
is that it is difficult to ensure that the knowledge
contained in the benchmark is new for LLMs, since
training data for some models are large and non-
publicly available. Furthermore, it is also difficult
to ensure that the knowledge used for benchmark-
ing will be not outdated and inefficient, as there
are many LLMs that may soon include data from
the benchmark in their training. In summary, such
benchmark for new knowledge needs to exhibit
three basic characteristics: it contains enough new
knowledge for sufficient evaluation (sufficient), the
knowledge is new to all models (model-agnostic)
and the knowledge can remain new for a long time
