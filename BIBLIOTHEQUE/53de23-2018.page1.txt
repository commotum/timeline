                          GPipe: Easy Scaling with Micro-Batch Pipeline
                                                   Parallelism
                              YanpingHuang           YoulongCheng            AnkurBapna
                           huangyp@google.com       ylc@google.com       ankurbpn@google.com
                               OrhanFirat             MiaXuChen                DehaoChen
                           orhanf@google.com       miachen@google.com       dehao@google.com
                              HyoukJoongLee             Jiquan Ngiam            QuocV.Le
                           hyouklee@google.com       jngiam@google.com        qvl@google.com
                                      YonghuiWu                       Zhifeng Chen
                                  yonghui@google.com              zhifengc@google.com
                                                       Abstract
                            Scaling up deep neural network capacity has been known as an effective approach
                            to improving model quality for several different machine learning tasks. In many
                            cases, increasing model capacity beyond the memory limit of a single accelera-
                            tor has required developing special algorithms or infrastructure. These solutions
                            are often architecture-speciﬁc and do not transfer to other tasks. To address the
                            need for efﬁcient and task-independent model parallelism, we introduce GPipe, a
                            pipeline parallelism library that allows scaling any network that can be expressed
                            as a sequence of layers. By pipelining different sub-sequences of layers on sep-
                            arate accelerators, GPipe provides the ﬂexibility of scaling a variety of different
                            networks to gigantic sizes efﬁciently. Moreover, GPipe utilizes a novel batch-
                            splitting pipelining algorithm, resulting in almost linear speedup when a model
                            is partitioned across multiple accelerators. We demonstrate the advantages of
                            GPipebytraining large-scale neural networks on two different tasks with distinct
                            network architectures: (i) Image Classiﬁcation: We train a 557-million-parameter
                            AmoebaNetmodelandattainatop-1accuracyof84.4%onImageNet-2012,(ii)
                            Multilingual Neural Machine Translation: We train a single 6-billion-parameter,
       arXiv:1811.06965v5  [cs.CV]  25 Jul 2019128-layer Transformer model on a corpus spanning over 100 languages and achieve
                            better quality than all bilingual models.
                     1   Introduction
                     Deeplearning has seen great progress over the last decade, partially thanks to the development of
                     methods that have facilitated scaling the effective capacity of neural networks. This trend has been
                     most visible for image classiﬁcation, as demonstrated by the accuracy improvements on ImageNet
                     with the increase in model capacity (Figure 1a). A similar phenomenon can also be observed in
                     the context of natural language processing (Figure 1b) where simple shallow models of sentence
                     representations [1, 2] are outperformed by their deeper and larger counterparts [3, 4].
                     While larger models have brought remarkable quality improvements to several ﬁelds, scaling neural
                     networks introduces signiﬁcant practical challenges. Hardware constraints, including memory
                     limitations and communication bandwidths on accelerators (GPU or TPU), force users to divide larger
                     Preprint. Under review.
