ChatGPT Alpaca Vicuna ChatGLM ChatGPT Alpaca Vicuna ChatGLM

Zero-Shot- Vanilla Zero-Shot-CoT

KU 50.19 31.02 34.12 34.64 68.75 39.81 39.61 24.85

KD 58.70 15.35 38.65 32.29 61.78 14.29 38.53 22.84

KA 28.44 24.60 29.71 10.29 35.36 19.66 29.55 4.97

Avg. 52.85 25.12 35.98 31.26 63.34 29.44 38.00 22.04
Few-Shot-Vanilla Few-Shot-CoT

KU 75.44 33.80 41.22 47.97 82.18 40.77 43.67 40.91

KD 64.20 38.97 46.76 41.42 74.99 36.24 55.81 37.19

KA 41.52 27.63 30.10 27.47 37.88 25.73 25.07 26.93

Avg. 64.37 35.09 42.74 42.56 74.11 38.02 47.73 37.64

Table 1: Performance of LLMs at our benchmark under different settings

0.6

= alpaca 7
a chain. 6b
Tae chatgst

Pe vicuna_138

0.2 03 04 05 06 0.7 08 0.9
Property Similarity

0.0 0.1

Figure 3: Relationship between model performance on
KD questions and the property similarity between the
artificial entity and its parental entity.

Alpaca Vicuna ChatGLM
similar 38.74 47.23 41.61
random — 39.52 47.64 43.20

Table 2: Results on KD questions of the entity with the
same property and different name.

we divide questions according to the property sim-
ilarity between the parental entities and the artifi-
cial entities, and calculate the scores of models in
different similarity ranges on the KD problem re-
spectively. We can find that the performance of the
robust ChatGPT to differentiate entities is almost
independent of the similarity. But other than it, all
other models are affected by entity similarity. The
more similar the new entities are to the existing
entities, the harder they are to differentiate them,
which illustrates the flaws of LLMs. The further
analysis is shown in Appendix ??

The Name also Playsa Role Since each artificial
entity is identified by its name, we think that the
name might have some effect on the model’s abil-
ity to distinguish new knowledge, so we conducted
experiments on KD questions for comparison. We

Context Alpaca Vicuna ChatGLM
artificial 38.97 46.76 41.42
w/ parent 37.09 35.04 24.71
w/irrelevant 37.12 37.17 35.00

Table 3: Results on KD questions with different knowl-
edge in context.

assign two different names to artificial entities with
the same properties: one is a randomly generated
sequence of characters (random), and the other is a
random substitution of one character (similar) for
the name of its parent entity. The results of the
experiments on the KD questions are shown in Ta-
ble 2. We can find that the influence of names on
model cognition does exist, and similar names are
more likely to cause confusion in the model. How-
ever, the effect is not very large and both results
are lower, which shows that the modeling of new
entities by LLM is both derived from the names
and influenced by the properties.

5.3. Impact of Provided Knowledge

To further explore the performance of LLMs when
new knowledge is provided in the context, we vary
the knowledge content provided for the analysis
experiments.

Parent Entity Aggravates Confusion Accord-
ing to Section 5.2, the models suffer from a certain
degree of confusion when faced with new knowl-
edge that overlaps in name or properties with inter-
nal knowledge. To further verify this, we explicitly
introduce parent entities in the context. Specifi-
cally, we conducted two comparison experiments:
1) adding parent entity knowledge to the context;
2) adding a random existing entity knowledge as
control variables. As shown in Table 3, the per-
formance is affected by both parent and irrelevant
