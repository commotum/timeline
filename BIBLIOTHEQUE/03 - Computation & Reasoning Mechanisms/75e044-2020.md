# REALM: Retrieval-Augmented Language Model Pre-Training (2020)
Source: 75e044-2020.pdf

## Core reasons
- The paper argues that standard language models store world knowledge implicitly in parameters, which is limiting and motivates adding an external knowledge access mechanism.
- The core contribution is a retrieval-augmented computation where the model retrieves and attends over documents from a large corpus to inform predictions.

## Evidence extracts
- "However, this knowledge is stored implic-
itly in the parameters of a neural network, requir-
ing ever-larger networks to cover more facts." (p. 1)
- "we augment language model pre-
training with a latent knowledge retriever, which
allows the model to retrieve and attend over doc-
uments from a large corpus such as Wikipedia," (p. 1)

## Classification
Class name: Computation & Reasoning Mechanism Proposal
Class code: 3

$$
\boxed{3}
$$
