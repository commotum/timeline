# COMBINING INDUCTION AND TRANSDUCTION FOR ABSTRACT REASONING (2024)
Source: a11fa2-2024.pdf

## Core reasons
- The paper directly studies whether to infer a latent function (induction) or directly predict test outputs (transduction) for few-shot ARC tasks, framing the comparison as a question about reasoning mechanisms. 
- It defines neural models for both paradigms, trains them on the same meta-learning data, and ensembles them so that symbolic program synthesis is tried first with transductive fallback, demonstrating an explicit computation strategy change.

## Evidence extracts
- "When learning an input-output mapping from very few examples, is it better to first infer a latent function that explains the examples, or is it better to directly predict new test outputs, e.g. using a neural network? We study this question on ARC by training neural models for induction (inferring latent functions) and transduction (directly predicting the test output for a given test input)." (Section ABSTRACT)
- "Combining induction and transduction. Induction allows checking candidate hypotheses against the training examples. Therefore, we know when induction has found a plausible solution–but sometimes it fails to find any solution. Transduction has the opposite property: We can’t check if its predictions match the training examples, but it always offers a candidate answer. Therefore we ensemble by attempting induction first, then transduction if none of the candidate hypotheses explained the examples." (Section 2 NEURAL MODELS FOR INDUCTION AND TRANSDUCTION)

## Classification
Class name: Computation & Reasoning Mechanism Proposal
Class code: 3

$$
\boxed{3}
$$
