These questions are entangled and require a comprehen-
sive solution. We start by defining a promptable segmenta-
tion task that is general enough to provide a powerful pre-
training objective and to enable a wide range of downstream
applications. This task requires a model that supports flex-
ible prompting and can output segmentation masks in real-
time when prompted to allow for interactive use. To train
our model, we need a diverse, large-scale source of data.
Unfortunately, there is no web-scale data source for seg-
mentation; to address this, we build a “data engine”, i.e.,
we iterate between using our efficient model to assist in data
collection and using the newly collected data to improve the
model. We introduce each interconnected component next,
followed by the dataset we created and the experiments that
demonstrate the effectiveness of our approach.

Task (§2). In NLP and more recently computer vision,
foundation models are a promising development that can
perform zero-shot and few-shot learning for new datasets
and tasks often by using “prompting” techniques. Inspired
by this line of work, we propose the promptable segmen-
tation task, where the goal is to return a valid segmenta-
tion mask given any segmentation prompt (see Fig. la). A
prompt simply specifies what to segment in an image, e.g.,
a prompt can include spatial or text information identifying
an object. The requirement of a valid output mask means
that even when a prompt is ambiguous and could refer to
multiple objects (for example, a point on a shirt may in-
dicate either the shirt or the person wearing it), the output
should be a reasonable mask for at least one of those ob-
jects. We use the promptable segmentation task as both a
pre-training objective and to solve general downstream seg-
mentation tasks via prompt engineering.

Model (§3). The promptable segmentation task and the goal
of real-world use impose constraints on the model architec-
ture. In particular, the model must support flexible prompts,
needs to compute masks in amortized real-time to allow in-
teractive use, and must be ambiguity-aware. Surprisingly,
we find that a simple design satisfies all three constraints:
a powerful image encoder computes an image embedding,
a prompt encoder embeds prompts, and then the two infor-
mation sources are combined in a lightweight mask decoder
that predicts segmentation masks. We refer to this model as
the Segment Anything Model, or SAM (see Fig. 1b). By
separating SAM into an image encoder and a fast prompt
encoder / mask decoder, the same image embedding can
be reused (and its cost amortized) with different prompts.
Given an image embedding, the prompt encoder and mask
decoder predict a mask from a prompt in ~50ms in a web
browser. We focus on point, box, and mask prompts, and
also present initial results with free-form text prompts. To
make SAM ambiguity-aware, we design it to predict mul-
tiple masks for a single prompt allowing SAM to naturally
handle ambiguity, such as the shirt vs. person example.

Data engine (§4). To achieve strong generalization to new
data distributions, we found it necessary to train SAM on
a large and diverse set of masks, beyond any segmenta-
tion dataset that already exists. While a typical approach
for foundation models is to obtain data online [80], masks
are not naturally abundant and thus we need an alternative
strategy. Our solution is to build a “data engine”, ie., we
co-develop our model with model-in-the-loop dataset an-
notation (see Fig. lc). Our data engine has three stages:
assisted-manual, semi-automatic, and fully automatic. In
the first stage, SAM assists annotators in annotating masks,
similar to a classic interactive segmentation setup. In the
second stage, SAM can automatically generate masks for
a subset of objects by prompting it with likely object lo-
cations and annotators focus on annotating the remaining
objects, helping increase mask diversity. In the final stage,
we prompt SAM with a regular grid of foreground points,
yielding on average ~100 high-quality masks per image.

Dataset (§5). Our final dataset, SA-1B, includes more than
1B masks from 1M licensed and privacy-preserving im-
ages (see Fig. 2). SA-1B, collected fully automatically us-
ing the final stage of our data engine, has 400 x more masks
than any existing segmentation dataset [64, 43, 115, 58],
and as we verify extensively, the masks are of high quality
and diversity. Beyond its use in training SAM to be robust
and general, we hope SA-1B becomes a valuable resource
for research aiming to build new foundation models.

Experiments (§6). We extensively evaluate SAM. First, us-
ing a diverse new suite of 23 segmentation datasets, we find
that SAM produces high-quality masks from a single fore-
ground point, often only slightly below that of the manu-
ally annotated ground truth. Second, we find consistently
strong quantitative and qualitative results on a variety of
downstream tasks under a zero-shot transfer protocol using
prompt engineering, including edge detection, object pro-
posal generation, instance segmentation, and a preliminary
exploration of text-to-mask prediction. These results sug-
gest that SAM can be used out-of-the-box with prompt en-
gineering to solve a variety of tasks involving object and
image distributions beyond SAM’s training data. Neverthe-
less, room for improvement remains, as we discuss in §7.

Responsible AI. We provide model/dataset cards and report
on potential fairness concerns and biases when using SA-1B
and SAM in the supplement. Images in SA-1B span a geo-
graphically and economically diverse set of regions and we
found that SAM performs similarly across different groups
of people. Together, we hope this will make our work more
equitable for real-world use cases.

Release. We are releasing the SA-1B dataset for research
purposes and making SAM available under a permissive
open license (Apache 2.0) at hitps://segment-anything.com,
‘We also showcase SAM’s capabilities with an online demo.

4016
