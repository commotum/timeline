Figure 8: Zero-shot text-to-mask. SAM can work with sim-
ple and nuanced text prompts. When SAM fails to make a
correct prediction, an additional point prompt can help.

6.2. Zero-Shot Text-to-Mask

Approach. This experiment is a proof-of-concept of
SAM’s ability to segment objects from free-form text
prompts. While we used the exact same SAM in all prior
experiments, for this one SAM’s training procedure is mod-
ified to make it text-aware, but in a way that does not require
new text annotations. Specifically, for each manually col-
lected mask with area larger than 100° we extract the CLIP
image embedding. Then, during training, we prompt SAM
with the extracted CLIP image embeddings as its first in-
teraction. The key observation here is that because CLIP’s
image embeddings are trained to align with its text embed-
dings, we can train with image embeddings, but use text
embeddings for inference. That is, at inference time we run
text through CLIP’s text encoder and then give the resulting
text embedding as a prompt to SAM (see §E.5 for details).

Results. We show qualitative results in Fig. 8. SAM
can segment objects based on simple text prompts like “a
wheel” as well as phrases like “beaver tooth grille”. When
SAM fails to pick the right object from a text prompt only,
an additional point often fixes the prediction, similar to [30].

7. Discussion

Foundation models. Pre-trained models have been adapted
to downstream tasks since the early days of machine learn-
ing [97]. This paradigm has become increasingly impor-
tant in recent years with a growing emphasis on scale, and
such models have recently been (re-)branded as “founda-
tion models”: i.e. models that are “trained on broad data
at scale and are adaptable to a wide range of downstream
tasks” [8]. Our work correlates well with this definition,
though we note that a foundation model for image segmen-
tation is an inherently limited scope, since it represents an
important, yet fractional, subset of computer vision. We

also contrast one aspect of our approach with [8], which
emphasizes the role of se/f-supervised learning in founda-
tion models. While our model is initialized with a self-
supervised technique (MAE [46]), the vast majority of its
capabilities come from large-scale supervised training. In
cases where data engines can scale available annotations,
like ours, supervised training provides an effective solution.

Compositionality. Pre-trained models can power new ca-
pabilities even beyond ones imagined at the moment of
training. One prominent example is how CLIP [80] is used
as a component in larger systems, such as DALL-E [81].
Our goal is to make this kind of composition straightfor-
ward with SAM. We aim to achieve this by requiring SAM
to predict a valid mask for a wide range of segmentation
prompts. The effect is to create a reliable interface between
SAM and other components. For example, MCC [104] can
easily use SAM to segment an object of interest and achieve
strong generalization to unseen objects for 3D reconstruc-
tion from a single RGB-D image. In another example, SAM
can be prompted with gaze points detected by a wearable
device, enabling new applications. Thanks to SAM’s abil-
ity to generalize to new domains like ego-centric images,
such systems work without need for additional training.

Limitations. While SAM performs well in general, it is
not perfect. It can miss fine structures, hallucinates small
disconnected components at times, and does not produce
boundaries as crisply as more computationally intensive
methods that “zoom-in”, e.g. [17]. In general, we expect
dedicated interactive segmentation methods to outperform
SAM when many points are provided, e.g. [65]. Unlike
these methods, SAM is designed for generality and breadth
of use rather than high IoU interactive segmentation. More-
over, SAM can process prompts in real-time, but neverthe-
less SAM’s overall performance is not real-time when using
a heavy image encoder. Our foray into the text-to-mask task
is exploratory and not entirely robust, although we believe
it can be improved with more effort. While SAM can per-
form many tasks, it is unclear how to design simple prompts
that implement semantic and panoptic segmentation. Fi-
nally, there are domain-specific tools, such as [7], that we
expect to outperform SAM in their respective domains.

Conclusion. The Segment Anything project is an attempt to
lift image segmentation into the era of foundation models.
Our principal contributions are a new task (promptable seg-
mentation), model (SAM), and dataset (SA-1B) that make
this leap possible. Whether SAM achieves the status of a
foundation model remains to be seen by how it is used in
the community, but regardless we expect the perspective of
this work, the release of over 1B masks, and our promptable
segmentation model will help pave the path ahead.

4023
