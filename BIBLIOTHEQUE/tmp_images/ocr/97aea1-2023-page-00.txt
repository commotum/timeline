This ICCV paper is the Open Access version, provided by the Computer Vision Foundation
Except for this watermark, it is identical to the accepted version;

the final published version of the proceedings is available on IEEE Xplore.

Segment Anything

Alexander Kirillov'? Eric Mintun?Nikhila Ravi? Hanzi Mao” Chloe Rolland* Laura Gustafson*
Tete Xiao® Spencer Whitehead = AlexanderC. Berg | Wan-YenLo Piotr Dollért. Ross Girshick*
Iproject lead joint firstauthor equal contribution —_— ‘directional lead
Meta AI Research, FAIR

valid mask valid mask [ante —
model data
t— vm —
a Segment Anything 1B (SA-1B):
Pore ences + 1+ billion masks:
ick eam
t t + licensed images
segmentation prompt image prompt image

(a) Task: promptable segmentation

(b) Model: Segment Anything Model (SAM)

(c) Data: data engine (top) & dataset (bottom)

Figure 1: We aim to build a foundation model for segmentation by introducing three interconnected components: a prompt-
able segmentation task, a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range
of tasks via prompt engineering, and a data engine for collecting SA-1B, our dataset of over | billion masks.

Abstract
We introduce the Segment Anything (SA) project: a new

task, model, and dataset for image segmentation. Using our
efficient model in a data collection loop, we built the largest
segmentation dataset to date (by far), with over 1 billion
masks on 11M licensed and privacy respecting images. The
model is designed and trained to be promptable, so it can
transfer zero-shot to new image distributions and tasks. We
evaluate its capabilities on numerous tasks and find that
its zero-shot performance is impressive — often competitive
with or even superior to prior fully supervised results. We
are releasing the Segment Anything Model (SAM) and cor-
responding dataset (SA-1B) of 1B masks and 11M images
at segment-anything.com to foster research into foundation
models for computer vision. We recommend reading the
full paper at: arxiy.org/abs/2304.02643.

1. Introduction

Large language models pre-trained on web-scale datasets
are revolutionizing NLP with strong zero-shot and few-shot
generalization [10]. These “foundation models” [8] can
generalize to tasks and data distributions beyond those seen
during training. This capability is often implemented with
prompt engineering in which hand-crafted text is used to
prompt the language model to generate a valid textual re-
sponse for the task at hand. When scaled and trained with
abundant text corpora from the web, these models’ zero and
few-shot performance compares surprisingly well to (even

matching in some cases) fine-tuned models [10, 20]. Empir-
ical trends show this behavior improving with model scale,
dataset size, and total training compute [54, 10, 20, 49].

Foundation models have also been explored in computer
vision, albeit to a lesser extent. Perhaps the most promi-
nent illustration aligns paired text and images from the web.
For example, CLIP [80] and ALIGN [53] use contrastive
learning to train text and image encoders that align the two
modalities. Once trained, engineered text prompts enable
zero-shot generalization to novel visual concepts and data
distributions. Such encoders also compose effectively with
other modules to enable downstream tasks, such as image
generation (e.g., DALL-E [81]). While much progress has
been made on vision and language encoders, computer vi-
sion includes a wide range of problems beyond this scope,
and for many of these, abundant training data does not exist.

In this work, our goal is to build a foundation model for
image segmentation. That is, we seek to develop a prompt-
able model and pre-train it on a broad dataset using a task
that enables powerful generalization. With this model, we
aim to solve a range of downstream segmentation problems
on new data distributions using prompt engineering.

The success of this plan hinges on three components:
task, model, and data. To develop them, we address the
following questions about image segmentation:

1, What task will enable zero-shot generalization?
2. What is the corresponding model architecture?
3. What data can power this task and model?

4015
