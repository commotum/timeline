                  lucinations induced by MLLMs. (2) Limited Hal-             struct the underlying MLLM to judge whether the
                  lucination Categories: Prior studies have focused          claim hallucinatory with rationals for explanation.
                  onidentifying hallucinations at the object level, yet        We have conducted a thorough evaluation of
                  they fail to consider the prevalence of scene-text or      the UNIHD framework, utilizing the underlying
                  factual inconsistencies that also frequently occur         MLLMagainsttheMHaluBenchbenchmark. Our
                  in MLLMs. (3) Incomplete Granularity: It would             findings underscore the effectiveness of our ap-
                  be more valuable to assess hallucinations at a fine-       proach and confirm that multimodal hallucination
                  grained level, examining individual claims within a        detection remains a formidable challenge. In a
                  response, rather than evaluating the entire response       nutshell, We conclude our contributions as:
                  holistically. Considering these constraints hinder         • We propose a more unified problem setting for
                  rapid progress in practical hallucination detection,         hallucination detection in MLLMs, encompass-
                  it raises the question: Can we develop a unified per-        ing a broad spectrum of multimodal tasks and
                  spective for detecting hallucinations from MLLMs?            hallucination categories, thus enriching the uni-
                     To further investigate this problem, we have              fied understanding of hallucination in MLLMs.
                  broadened the concept of multimodal hallucination          • We unveil MHaluBench, a meta-evaluation
                  within MLLMs to a holistic framework, integrat-              benchmark that encompasses various halluci-
                  ing both image-to-text generation such as Image              nation categories and multimodal tasks. This
                  Captioning (IC) and Visual Question Answering                benchmark is equipped with fine-grained analyt-
                  (VQA), as well as text-to-image-synthesis (T2I)              ical features, gauging the progress of hallucina-
                  – to align with MLLMs’ capabilities of perform-              tion detectors.
                  ing varied multimodal tasks. We are committed to           • We introduce UNIHD, a task-agnostic, tool-
                  exploring a broad spectrum of hallucinatory cat-             enhanced framework for the detection of hal-
                  egories and the intricate nuances of claim-level             lucinations in content produced by MLLMs. Our
                  hallucination through a lens that integrates both            extensive experiments demonstrate the efficacy
                  modality-conflicting and fact-conflicting halluci-           of this method, underscoring that MHaluBench
                  nations. Based on the outlined perspectives, We              continues to be a challenging yet vital task.
                  have developed the MultiModal Hallucination De-
                  tection Benchmark (MHaluBench) to assess the
                                                                             Image-to-Text 
                  progress of unified multimodal hallucination detec-                                         1. whether the output text
                                                                                                              contradicts the information
                                                                                                              presented in the input image.
                  tors for MLLMs and embodied the data framework                                 ...
                                                                                                              2. whether the output text
                  depicted in Figure 1.                                                                       conflicts with world knowledge.
                     At its core, leveraging MLLMs’ inherent self-                                             1. whether the output image
                                                                             Text-to-Image 
                                                                                                               contradicts the information
                  detection mechanisms to pinpoint diverse hal-                                                presented in the input text.
                                                                                                               2. whether the output image
                  lucinations encounters significant hurdles.        We                                        conflicts with the world 
                  further develop a tool-augmented framework for                                               knowledge underlying the text.
                  unified hallucination detection, named UNIHD,              Figure 2: Unified multimodal hallucination detection.
                  which integrates evidence from multiple auxiliary
                  tools through the following procedure: (1) Essen-          2 Preliminaries
                  tial Claim Extraction involves extracting the core
                  claims within the generated response for image-to-         Weexploreaunifiedperspectiveonhallucinationin
                  text generation or user queries in text-to-image gen-      MLLMs(illustratedinFigure2)withtheaspiration
                  eration; (2) Autonomous Tool Selection via Query           of developing a unified detection framework.
                  Formulation prompts MLLMs (GPT-4/Gemini) to
                  autonomouslygeneratepertinentquestionsforeach              Unified View of Multimodal Hallucination Tax-
                  claim. These questions are crafted to determine the        onomy. Aprerequisiteforunifieddetectionisthe
                  specific type of tool required for each claim and to       coherent categorization of the principal categories
                  establish the input for the tool’s operation; (3) Par-     of hallucinations within MLLMs. Our paper su-
                  allel Tool Execution deploys a suite of specialized        perficially examines the following Hallucination
                  tools to operate concurrently, providing evidence          Taxonomyfromaunifiedperspective:
                  from their outputs to reliably validate potential hal-     • Modality-Conflicting Hallucination. MLLMs
                  lucinations; (4) Hallucination Verification with Ra-         sometimes generate outputs that conflict with
                  tionales aggregates the collected evidence to in-            inputs from other modalities, leading to issues
                                                                        3236
