                             Under review as a conference paper at ICLR 2025
                    432      2D Relative Positional Encoding (2D-RPE).           Motivated by the example in Figure 6, we aim to
                    433      enablethemodeltodistinguishbetweenpixelsindifferentspatialregions,suchasthecolor-3(green)
                    434      pixel in the cyan box versus the one in the yellow box. In this example, the positional difference
                    435      betweenthetwopixelsisjust1alongthey-coordinate. APEencodesthisdifferenceasasmallshift;
                    436      while the transformer is theoretically capable of capturing these spatial relationships, in practice
                    437      often requires many training epochs (Hahn, 2020).
                    438      Tobetteraccountforspatialrelationships in two-dimensional grids, we adapt the Relative Positional
                    439      Encoding (RPE) approach from ALiBi (Press et al., 2021) and extend it to 2D. ALiBi introduces
                    440      additive positional biases to the attention scores based on the relative positions of tokens. In its
                    441      original 1D form, ALiBi defines the positional bias as the following:
                    442                                          qn · kn
                    443                                    n      i√ j                                                            (11)
                                                         A =             +B , B =r·|i−j|,
                                                           i,j       d        Pi,j      Pi,j
                    444
                    445      where Pi,j represents the relative positional offset between tokens i and j, and r is a predefined
                    446      slope that penalizes tokens based on their distance.
                    447      Extending to 2D, we introduce distinct slopes for the “left” and “right” directions, efficiently cap-
                    448      turing directional biases along the x and y axes. This design leverages the inherent 2D structure of
                    449      the data while aligning with the sequential raster order of the generation process. Specifically:
                    450
                    451             – Pixels located above or to the left of the current pixel in 2D space are assigned a bias rleft.
                    452             – Pixels located below or to the right are assigned a bias r     .
                    453                                                                           right
                    454      Hence, the 2D-RPE bias is computed as:
                    455                                          r    · d((x ,y ),(x ,y )),       if j ≤ i,
                    456                                B      = left          i  i     j   j                                      (12)
                                                          P
                                                           i,j     r    · d((x ,y ),(x ,y )),      if j > i,
                    457                                             right      i  i     j   j
                    458      where d((x ,y ),(x ,y )) represents the 2D Manhattan distance between coordinates (x ,y ) and
                    459                   i  i     j  j                                                                     i  i
                             (x ,y ). The slope values r     andr      are derived following the ALiBi setup, forming a geometric
                    460         j  j                      left     right
                                                      −8/n                                 1                          0.5
                             sequence of the form 2         for n heads. r    starts at 1/2 , while r     starts at 1/2  , both using
                    461                                                    left                      right
                             the same ratio.
                    462      In this work, we leverage both 2D-RPE and 2D sinusoidal APE within our model. In contrast
                    463      to observations made in Swin (Liu et al., 2021), where a degradation in performance was noted
                    464      when combining RPE with APE, our results demonstrate a marked improvement. The inclusion
                    465      of 2D-RPE allows for more precise modeling of relative spatial relationships, complementing the
                    466      global positional information provided by APE. This synergy proves particularly effective for tasks
                    467      demanding fine-grained spatial reasoning.
                    468
                    469      Object-based Positional Encoding (OPE).           For tasks involving multi-colored objects, or more
                    470      generally, tasks that require objectness priors (Chollet, 2019), external sources of knowledge about
                    471      object abstractions can be integrated into the model. We inject this information through a novel
                    472      object-based positional encoding. We extend the 2D sinusoidal APE defined in Equation (9) by
                    473      introducing the object index o as an additional component to the pixel coordinates (x,y). This
                    474      results in a modified positional encoding:
                    475                           E          =concat(sinusoid(o),sinusoid(x),sinusoid(y)).                        (13)
                    476                             pos(o,x,y)
                    477      In object detection models, two primary segmentation methods are bounding box segmentation and
                    478      instance segmentation, the latter of which captures precise object boundaries. For simplicity, we
                    479      adopt bounding box segmentation to derive the object index o, as fine-grained distinctions at the
                    480      instance level can already be addressed by the model’s attention mechanism, as illustrated in Fig-
                    481      ure 6. Figure 1 demonstrates how bounding box information is obtained and incorporated into the
                    482      positional encoding.
                    483      This design integrates seamlessly with the PEmixer introduced earlier, as it enables the model to
                    484      dynamically adjust its reliance on the object index o based on the task’s needs. In scenarios where
                    485      the object index provides valuable abstraction, the model can prioritize it, while in cases where the
                             object-based method is less effective, the model can fall back on the (x,y) positional information.
                                                                                 9
