                                                                                               Validation                                      Test
                                      Method
                                                                                PAT LSTQ PTQ           PQ    TQ     S         PAT LSTQ PTQ           PQ    TQ     S
                                                                                                                     assoc                                         assoc
                                      PanopticTrackNet [53]                     44.0   43.4  50.9      51.6  38.5   32.3      45.7  44.8   51.6      51.7  40.9   36.7
                                      4D-PLS[9]                                 59.2   56.1  55.5      56.3  62.3   51.4      60.5  57.8   55.6      56.6  64.9   53.6
                                      Cylinder3D++ [1] + OGR3MOT[54]             -     -      -         -     -      -        62.7  61.7   61.3      61.6  63.8   59.4
                                      (AF)2-S3Net [3] + OGR3MOT [54]             -     -      -         -     -      -        62.9  62.4   60.9      61.3  64.5   59.9
                                      EfficientLPS [22] + KF [30]               64.6   62.0  60.6      62.0  67.6   58.6      67.1  63.7   62.3      63.6  71.2   60.2
                                      EfficientLPT [33]                          -     -      -         -     -      -        70.4  66.0   67.0      67.9  71.2   -
                                      4D-Former                                 78.3   76.4  75.2      77.3  79.4   73.9      79.4  78.2   75.5      78.0  75.5   76.1
                                                          Table 1: Benchmark results for nuScenes validation and test set.
                                    manner such that the lower dimension is 480px. For training time data augmentation, we randomly
                                    subsample the LiDAR pointcloud to 105 points, and also apply random rotation and point jitter.
                                    TheimagesundergoSSD-basedcoloraugmentation[52],andarerandomlycroppedto70%oftheir
                                    original size. In the first stage, we train for 80 epochs with AdamW optimizer with batch size 8
                                                                                                                                                         −3
                                    across 8x Nvidia T4 GPUs (14GB of usable VRAM). The learning rate is set to 3 × 10                                       for the
                                                                               −4
                                    LiDARfeature extractor, and 10                 for the rest of the network. The rate is decayed in steps of 0.1
                                    after 30 and 60 epochs. For the second stage, we train the TAM for 2 epochs on a single GPU with
                                                         −4
                                    learning rate 10        . During inference, we associate tracklets over temporal history T                        =4.
                                                                                                                                                  hist
                                    Datasets:        To verify the efficacy of our approach, we apply it to two popular benchmarks:
                                    nuScenes[12]andSemanticKITTI[13]. nuScenes[12]contains1000sequences,each20slongand
                                    annotated at 2Hz. The scenes are captured with a 32-beam LiDAR sensor and 6 cameras mounted at
                                    different angles around the ego vehicle. The training set contains 600 sequences, whereas validation
                                    and test each contain 150. The primary evaluation metric is Panoptic Tracking (PAT). Compared to
                                    nuScenes, SemanticKITTI [9] contains fewer but longer sequences, and uses LiDAR Segmentation
                                    and Tracking Quality (LSTQ) as the primary evaluation metric. One caveat is that image input is
                                    only available from a single, forward-facing camera. As a result, only a small fraction (∼ 15%)
                                    of LiDAR points are visible in the camera image. For this reason, following existing multimodal
                                    methods [43], we evaluate only those points which have valid camera image projections.
                                    Comparison to state-of-the-art:                Results on
                                    nuScenes are shown in Tab. 1 and visualized                        Method                  LSTQ S         S          IoUst IoUth
                                    in Fig. 5. We see that 4D-Former outperforms                                                        assoc   cls
                                    existing methods across all metrics. In terms                      4D-PLS[9]               65.4 72.3 59.1            62.6 61.8
                                    PAT, 4D-Former achieves 78.3 and 79.4 on the                       4D-StOP[11]             71.0 82.5 61.0            63.0 66.0
                                    val and test sets, respectively. This is signifi-                  Ours                    73.9 80.9 67.6            64.9 71.3
                                    cantly better than the 70.4 (+9.0) achieved by                      Table 2: SemanticKITTI validation results.
                                    EfficientLPT [33] on the test set and the 64.6
                                    (+13.7) achieved by EfficientLPS [22]+KF on val. We attribute this to 4D-Former’s ability to reason
                                    overmultimodalinputsandsegmentbothsemanticclassesandobjecttracksinanend-to-endlearned
                                    fashion. The results on SemanticKITTI validation set are reported in Tab. 2. For a fair compari-
                                    son, we also evaluated existing top-performing methods on the same sub-set of camera-projectable
                                    points. We see that 4D-Former achieves 73.9 LSTQ which is higher than the 71.0 (+2.8) achieved
                                    by4D-StOPandalsothe65.4(+8.4)achievedby4D-PLS[9]. AsidefromSassoc,4D-Formerisalso
                                    better for other metrics.
                                    Effect of Tracklet Association Module: The effec-
                                    tiveness of the TAM is evident from Tab. 3 where we                         Setting              PAT LSTQ           PTQ PQ
                                    compareittoabaselinewhichusesonlyusemaskIoU                                 MaskIoU              76.3     74.6      73.9    77.3
                                    in the overlapping frame for association. This results                      TAM                  78.3     76.4      75.2    77.3
                                    in the PAT dropping from 78.3 to 76.3. This highlights
                                    the importance of using a learned temporal association                    Table 3: Ablation results for temporal as-
                                    mechanismwithbothspatial and appearance cues.                             sociation on nuScenes validation set.
                                    Next, we ablate other aspects of our method in Tab. 4.
                                    For these experiments, we subsample the training set by using only every fourth frame to save time
                                    and resources. The final model is also re-trained with this setting for a fair comparison (row 6).
                                                                                                    7
