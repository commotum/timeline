                        ARC-AGIevaluation dataset and subsequently the test set. Lastly, we show that our LPN architecture
                        for ARC-AGI outperforms previous vision transformer methods Li et al. [2024b] that struggled
                        to overfit to single ARC tasks, conclusively showing that such grid-based program tasks are not
                        a challenge for existing architectures. In particular, we train a network from scratch capable of
                        executing over 180 programs on the arc training dataset. Therefore, future work should focus on
                        other challenges that limit performance.
                        Limitations and Future Work  Despite its strengths, LPN faces limitations. An initial limitation
                        of our work is that, despite access to TPUs for training, our main ARC-AGI training run has not
                        been trained to convergence yet. Training networks from scratch has the downside of long training
                        times, so future work would need to scale compute to understand the convergence properties of
                        LPNwhentraining on RE-ARC, both in training time and parameter counts. Secondly, while we
                        showthat gradient ascent can be used to boost test time performance to a significant extent, gradient
                        ascent as an optimization method may encounter local optima, which could restrict LPN’s capacity to
                        find the best latent solution. Future work could investigate different optimization procedures in the
                        latent space to overcome this challenge, such as alternatives to initializing search and procedures for
                        updating latents according to the gradient. Hybrid approaches combining evolution strategies (e.g.,
                        COMPASS[Chalumeauetal.,2023])withgradient-based methods might improve search efficacy
                        in future iterations. Another limitation is the challenge of representing complex, discrete programs
                        within a continuous latent space, which may restrict expressivity for certain tasks or compositional
                        generalization Shi et al. [2023]. Future work could explore discrete program representations, though
                        this would require addressing the complexities of discrete space search.
                        In summary, LPN represents a step forward in adaptive program synthesis, demonstrating effective
                        test-time adaptation, scalability, and potential for generalization. This work underscores the value of
                        structured search and adaptive latent representations in advancing program synthesis capabilities.
                        Acknowledgments
                        WethankGoogle’sTPUResearchCloud(TRC)forsupportingthisresearch. We are also grateful to
                        Nathan Grinsztajn, Natasha Butt, Levi Lelis, and Jessica Hu for their feedback on the early versions
                        of the paper.
                        References
                        AwsAlbarghouthi, Sumit Gulwani, and Zachary Kincaid. Recursive program synthesis. In Computer
                          Aided Verification: 25th International Conference, CAV 2013, Saint Petersburg, Russia, July 13-19,
                          2013. Proceedings 25, pages 934–950. Springer, 2013.
                        Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
                          Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
                          models. arXiv preprint arXiv:2108.07732, 2021.
                        Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling,
                          2019. URLhttps://arxiv.org/abs/1809.10853.
                        Sergey Bartunov, Vinod Nair, Peter Battaglia, and Tim Lillicrap. Continuous latent search for
                          combinatorial optimization. In Learning Meets Combinatorial Algorithms at NeurIPS2020, 2020.
                        Alan WBiermann. The inference of regular lisp programs from examples. IEEE transactions on
                          Systems, Man, and Cybernetics, 8(8):585–600, 1978.
                        MatkoBošnjak,TimRocktäschel, Jason Naradowsky, and Sebastian Riedel. Programming with a
                          differentiable forth interpreter. In International conference on machine learning, pages 547–556.
                          PMLR,2017.
                        James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
                          Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
                          Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
                          http://github.com/jax-ml/jax.
                                                                 16
