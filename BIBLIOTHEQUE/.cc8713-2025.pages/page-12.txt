                                 Learning program synthesis with self-improving language models: A case study on ARC-AGI
               Mirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D.,     Xu, Y., Li, W., Vaezipoor, P., Sanner, S., and Khalil, E. B.
                  Arenas, M. G., Rao, K., Sadigh, D., and Zeng, A. Large              Llms and the abstraction and reasoning corpus: Suc-
                  language models as general pattern machines. arXiv                  cesses, failures, and the importance of object-based rep-
                  preprint arXiv:2307.04721, 2023.                                    resentations. arXiv preprint arXiv:2305.18354, 2023.
               Olausson, T. X., Inala, J. P., Wang, C., Gao, J., and Solar-        Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and
                  Lezama, A. Is self-repair a silver bullet for code genera-          Huang,G. Doesreinforcementlearningreally incentivize
                  tion? arXiv preprint arXiv:2306.09896, 2023.                        reasoning capacity in llms beyond the base model? arXiv
                                                                                      preprint arXiv:2504.13837, 2025.
               Pourcel, J., Colas, C., Molinaro, G., Oudeyer, P.-Y., and           Zelikman,E.,Wu,Y.,andGoodman,N.D. Star: Self-taught
                  Teodorescu, L. Aces: generating diverse programming                 reasoner. arXiv preprint arXiv:2203.14465, 2022.
                  puzzles with autotelic language models and semantic
                  descriptors. Neurips, 2024.                                      Zelikman, E., Harik, G., Shao, Y., Jayasiri, V., Haber, N.,
               Romera-Paredes, B., Barekatain, M., Novikov, A., Balog,                and Goodman, N. D. Quiet-star: Language models can
                  M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S.,        teach themselves to think before speaking. arXiv preprint
                  Wang,P., Fawzi, O., et al. Mathematical discoveries from            arXiv:2403.09629, 2024.
                  program search with large language models. Nature, 625           Zhang, Y., Diddee, H., Holm, S., Liu, H., Liu, X., Samuel,
                  (7995):468–475, 2024.                                              V., Wang, B., and Ippolito, D. Noveltybench: Evaluating
               Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,           language models for humanlike diversity. arXiv preprint
                  Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., et al.      arXiv:2504.05228, 2025.
                  Code llama: Open foundation models for code. arXiv               Zheng, L., Yin, L., Xie, Z., Sun, C. L., Huang, J., Yu, C. H.,
                  preprint arXiv:2308.12950, 2023.                                    Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., et al.
                                                                                      Sglang: Efficient execution of structured language model
               Saad, F. A., Cusumano-Towner, M. F., Schaechtle, U., Ri-               programs. Advances in Neural Information Processing
                  nard, M. C., and Mansinghka, V. K. Bayesian synthesis              Systems, 37:62557–62583, 2024.
                  of probabilistic programs for automatic data modeling.
                  Proceedings of the ACM on Programming Languages, 3
                  (POPL):1–32, 2019.
               Shi, K., Bieber, D., and Singh, R. Tf-coder: Program
                  synthesis for tensor manipulations. ACM Transactions on
                  ProgrammingLanguagesandSystems(TOPLAS),44(2):
                  1–36, 2022.
               Tang, H., Hu, K., Zhou, J. P., Zhong, S. C., Zheng, W.-L.,
                  Si, X., and Ellis, K. Code repair with LLMs gives an
                  exploration-exploitation tradeoff. In The Thirty-eighth
                  Annual Conference on Neural Information Processing
                  Systems, 2024. URL https://openreview.net
                  /forum?id=o863gX6DxA.
               Tang, Y., Zheng, K., Synnaeve, G., and Munos, R. Optimiz-
                  ing language models for inference time objectives using
                  reinforcementlearning. arXiv preprint arXiv:2503.19595,
                  2025.
               Wang,R.,Zelikman, E., Poesia, G., Pu, Y., Haber, N., and
                  Goodman,N.D. Hypothesis search: Inductive reasoning
                  with language models. arXiv preprint arXiv:2309.05660,
                  2023.
               Wind, J. S. 1st place 2020 arc kaggle. https://gi
                  thub.com/top-quarks/ARC-solution, 2020.
                  [Online GitHub repository].
                                                                               12
