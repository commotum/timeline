                                                Learning program synthesis with self-improving language models: A case study on ARC-AGI
                      by training on successful reasoning traces—either self-                                            3.1. Problem definition
                      generated (Zelikman et al., 2022; 2024; Guo et al., 2025),                                         The ARC benchmark is a canonical example of program-
                      or produced by classical search algorithms (Gandhi et al.,                                         ming by example (PBE) (Menon et al., 2013), where the
                      2024). SOAR also internalizes search traces into the model,                                        goal is to synthesize a program that satisfies a specification
                      but can do so leveraging both successful and failed attempts,                                      defined through input-output examples. Formally, in PBE,
                      using hindsight learning. Moreover, SOAR also learns to                                            we aim to find a program f within a given programming
                      refine solutions from past experience, and leverages these                                         language such that for every provided example pair (x,y),
                      improved capabilities in a test-time evolutionary search.                                          the program correctly maps inputs to outputs: y = f(x).
                      TheAbstraction and Reasoning Corpus (ARC) represents                                               This paradigm enables users to specify desired behavior im-
                      a particularly challenging synthesis benchmark that has at-                                        plicitly through examples rather than writing explicit code.
                      tracted significant attention over the past years (Chollet,                                        In the specific case of ARC, each task consists of:
                      2019). ARC tasks can be solved in two ways: (1) direct                                             1. a set of 2–10 training examples {(x                              , y      )} where
                      prediction of output grids (transductive approach), or (2) pre-                                                                                                   train    train
                      diction of a transformation program used to generate the                                                x       and y          are colored grid pairs;
                                                                                                                                train          train
                      output grids (inductive approach) (Li et al., 2024). First
                      attempts at leveraging LLMs for transduction achieved poor                                         2. a set of test inputs {xtest} for evaluation.
                      results (Xu et al., 2023; Gendron et al., 2023; Mirchandani                                        Thegoalis to find a Python function f such that f(xtrain) =
                      et al., 2023) but finetuning models on synthetic data brought                                      y        for all training examples, and f(x                         ) produces the
                      significant performance gains (Li et al., 2024; Akyürek et al.,                                      train                                                         test
                                                                                                                         correct (hidden) y              .
                      2024). Early inductive approaches leveraged heavily human-                                                                     test
                      engineereddomain-specificlanguages(DSL)andexhaustive                                               Each grid is a 2D array of size h × w with (h,w) ∈ [1..30]
                      search (Hodel, 2023; Wind, 2020). More recent approaches                                           where each cell contains an integer from 0 to 9 representing
                      used closed-source LLM for Python synthesis: sampling                                              a color. The challenge lies in discovering the underlying
                      andrefining populations of candidate programs (Wang et al.,                                        transformation pattern from just a few examples. ARC is
                      2023; Li et al., 2024). Two recent projects trained LLMs                                           composed of 400 train tasks to use for algorithm develop-
                      to sample programs in ARC: Butt et al. (2024) used on-                                             ment(ARC-train), and 400 test tasks to use for evaluation
                      line RL and hindsight learning in a custom DSL, while                                              (ARC-test), with each task containing both training input-
                      Li et al. (2024) used human-generated Python solutions.                                            output examples to guide the inference and test input-output
                      With SOAR,wecontinuously refine search operators (sam-                                             grids to test it. Each ARC task encodes a new implicit
                      pling and refinement) solely from past synthesis attempts,                                         transformation that may involve fundamental concepts like
                      achieving competitive performance while eliminating any                                            counting, arithmetic, pattern completion, or spatial reason-
                      dependence on human engineering and datasets.                                                      ing. This makes them relatively easy for humans, yet sur-
                                                                                                                         prisingly difficult for AI systems (LeGris et al., 2024).
                      3. Method                                                                                          3.2. Program synthesis as evolutionary search with
                      SOAR(Self-improving Operators for Automated program                                                       LLM-basedsamplingandrefinement
                      Refinements) is a framework that enables program synthesis                                         ARCtasksaretoochallenging for current language models
                      systems to learn and improve from their own search experi-                                         to solve directly (see proof in Section 4.1). SOAR combines
                      ences. Ratherthanrelyingonafixedlanguagemodeltosam-                                                the generative capabilities of LLMs with an evolutionary
                      ple or refine programs, SOAR implements a self-improving                                           search process that iteratively improves candidate solutions.
                      loop where the system’s capabilities grow through iterative                                        At a high level, our Sample&Refine search first samples
                      search and learning phases (Fig. 1). During the evolutionary                                       an initial pool of candidate solutions (sampling step), then
                      search phase, SOAR uses an LLM to both sample candidate                                            iteratively refines the most promising ones using execution
                      programs and refine them through targeted modifications,                                           feedback (refinement step). In the end, we use majority
                      producing a diverse set of solution attempts (Section 3.2).                                        voting to select the most likely test output grids to submit
                      In the learning phase, these search traces are used to fine-                                       for evaluation (see Appendix D.1). Appendix G provides
                      tune the underlying LLM, enhancing its ability to sample                                           the prompts used for sampling and refinement.
                      and refine programs for future tasks (Section 4.2). SOAR
                      iterates this process to create a virtuous cycle: better models
                      enable more effective search, which in turn provides richer                                        Programsampling. GivenabaseLLMparameterizedby
                      training data for further model improvements (Section 3.4).                                        θ, we sample a set of Python programs f without constrain-
                                                                                                                         ing ourselves to a hand-coded domain-specific language:
                                                                                                                                                 f ∼ P (· | x            , x      , y      ).
                                                                                                                                                           θ         test    train   train
                                                                                                                     3
