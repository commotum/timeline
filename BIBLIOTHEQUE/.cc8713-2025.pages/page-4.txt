                                                Learning program synthesis with self-improving language models: A case study on ARC-AGI
                      Each candidate program is executed through a Python in-                                            through finetuning. Specifically, each search attempt pro-
                      terpreter to produce output grids: ytest = f(xtest). This                                          duces a rich set of program candidates, including both suc-
                      inductive approach allows us to implement a sample-and-                                            cessful and failed attempts, that can serve as training data
                      test strategy—scaling the number of candidate solutions                                            for enhancing both program sampling and refinement.
                      increases our chances of discovering a transformation that
                      satisfies all input-output examples, a requirement for our                                         Finetuning sampling capabilities.                            Weaimto improve
                      program to truly capture the implicit transformation of the                                        our model’s ability to sample correct programs by learning
                      task. AlphaCode scaled this approach to millions of at-                                            from its past synthesis attempts. For each task in the ARC-
                      tempts per task to achieve human-level performance on                                              train set, we have access to ground truth test outputs ytest,
                      coding challenges (Li et al., 2022).                                                               allowing us to identify correct sampled programs fcorrect.
                                                                                                                         This gives us a dataset Dgen                    of (task, solution) pairs.
                      Programrefinement.                     Whenacandidateprogramf pro-                                                                        correct
                      duces incorrect outputs (y                      =f(x            ) ̸= y        ), we can            However,thisapproachfacesasignificant limitation: search
                                                               synth             train        train                      fails to sample any correct solution in most tasks, severely
                      use this execution feedback to guide the LLM in refining its                                       limiting the size of Dgen               . To address this, we augment our
                      solution f → f+:                                                                                                                    correct
                                      f+ ∼P (· | f,x                , x      , y      , y       ),                       training data through hindsight relabeling (Andrychowicz
                                                  θ             test    train    train    synth                          et al., 2017). The key insight is that any program f0 sampled
                      clearly labeling both successful (y                           =y ),andfailed                       during search, while possibly incorrect for its intended task,
                                                                            synth         train
                      (y         ̸=y )transformationsintherefinement prompt.                                             is by definition correct for the task of mapping inputs to the
                         synth         train
                                                                                                                         outputs it produces. Formally, given a program:
                      Sample&Refinesearchalgorithm. Oursearchprocess                                                                           f ∼P (f | x ,x                      , y      ),
                      consists of two steps: (1) an initial sampling step that in-                                                               0         θ          test    train    train
                                                                                                                         wecancreate a new synthetic task for which f is a correct
                      dependently samples 3k candidate solutions, and (2) a re-                                                                                                                  0
                                                                                                                         solution by executing f on all inputs:
                      finement step with a budget of 3k refinements. The second                                                                              0
                                                                                                                                                  ∀x∈x , y                    =f (x).
                      step frames refinement as a generative multi-armed bandit:                                                                             train     synth        0
                      each refinement creates a new arm that can further be re-                                          This gives us a new valid (task, solution) pair:
                      fined. We tackle this problem with REX, a combination                                              {(x        , y       , x     ), f } where f is guaranteed to be cor-
                                                                                                                               train    synth     test     0                 0
                      of Thompson sampling based on the accuracy of training                                             rect by construction. This approach allows us to leverage all
                      input-output examples with an additional exploration bonus                                         programs sampled during search for training, not just those
                      (Tang et al., 2024). This efficiently balances our search                                          that happened to solve their intended tasks.
                      budget between the exploration of new program variations                                           Theresulting synthetic dataset Dgen contains 6k datapoints
                      and the exploitation of known successful paths.                                                                                                       synth
                                                                                                                         collected for each of the 400 tasks in ARC-train—a total
                      Ensembling with weighted majority voting.                                      Westart             of 2.4M datapoints. Given our limited computational re-
                      with 6k candidate programs (3k from sampling, 3k from                                              sources, we sub-sample this dataset to ≤ 50 examples per
                      refinement). Each program is evaluated on the ARC task’s                                           task. This is done by ranking solutions according to their
                      input-output examples to compute its example accuracy, and                                         accuracy on input-output examples and test pairs, then sam-
                      is also run on the test input to produce an output grid. We                                        pling 25 top performing solutions (greedy approach), then
                      then group programs by their test output grid and assign                                           sampling 25 bottom performing solutions to introduce some
                      each unique grid a score: the sum of example accuracies of                                         diversity in the set of relabelled problem-solution pairs. Sec-
                                                                                                                         tion 4.2 compares this solution to alternatives. With the
                      all programs that produced it. This gives us a weighted vote                                       resulting dataset D′gen , we finetune our model to sample
                      over test outputs, favoring grids produced by more accurate                                                                      synth
                      programs (see Appendix D.1 for more details). This en-                                             better programs by minimizing:
                                                                                                                                    L=E ′gen [−logP (f | x                           , y      , x     )].
                      sembling approach helps mitigate individual program errors                                                               D                    θ           train   train     test
                      while capturing common patterns across successful solu-                                                                    correct
                      tions. We eventually return two candidate solutions, as per                                        Finetuning refinement capabilities.                           Beyondimproving
                      the benchmark rules (Chollet, 2024).                                                               initial program sampling, we aim to enhance our model’s
                                                                                                                         ability to refine incorrect programs using execution feed-
                      3.3. Learning to search via self-improved sampling and                                             back. For tasks in ARC-train where we have access to
                             refinement                                                                                  ground truth outputs, we can identify correct refinements:
                      The search process described above relies entirely on the                                          cases where an incorrect program f was successfully re-
                                                                                                                         fined into a correct program f+. We collect these successful
                      base LLM’s ability to sample and refine programs. We                                               refinements into a dataset Drefine .
                      propose to leverage the data generated during the Sam-                                                                                         correct
                      ple&Refine search phase to improve these capabilities                                              Here again, we subsample this dataset to ≤ 50 examples
                                                                                                                    4
