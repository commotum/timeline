                                                       Learning program synthesis with self-improving language models: A case study on ARC-AGI
                                                                                                                                                                                                   iterate
                                                                                          Program solution                                          Evolutionary  Search                                              Learning
                                        ARC task
                                                                                                                                                             ARC task
                                                                                                                                                                                                           ﬁnetuned LLM (init. base LLM)
                                                                                                                                                                           Learned program 
                                                                                                                                                                                                                 Sampling & reﬁnement
                                                                                                                                                                                sampling
                                                                                                                                                                                                                     LLM ﬁne-tuning
                                                                                                                                                   solution
                                                                                                                                                                                       3k prog.
                                                                                                                                                                           Learned program 
                                                                                                                                                                                                                       Data selection
                                                                                                                                                                               reﬁnement
                                                                                                                                                       Majority 
                                                                                                                                                                                       6k prog.
                                                            ? 
                                                                                                                                                          vote
                                                                                                                                                                                                             search traces
                          Figure 1. Overview of the SOAR architecture solving a task from the Abstract Reasoning Corpus. Each ARC task implicitly encodes
                                                                  ˆ                                                                                      ˆ
                          for grid transformation f demonstrated via examples {x                                            , y       }suchthat f(x                 ) = y          . To solve a task, one must find the output
                                                                                                                       train    train                          train          train
                          grids ytest corresponding to test input grids xtest. SOAR learns to synthesize transformation programs f in Python by alternating between
                          an evolutionary search phase (sampling and refining candidate programs with an LLM) and a learning phase (finetuning the LLM on
                          previous synthesis attempts)—eventually solving 52% of ARC-AGI public test set.
                          soning capabilities (Chollet, 2019). Each ARC task requires                                                        their own improvement, our work opens new possibilities
                          inferring from just a few examples an implicit transforma-                                                         for creating increasingly capable AI systems that can tackle
                          tion mapping input colored grids to output grids. These                                                            complex reasoning tasks through a combination of search
                          transformations often involve fundamental concepts like ob-                                                        and learned program refinement.
                          ject permanence, arithmetic, physics, or geometric relations.
                          Current language models struggle with program synthesis                                                            2. Related Work
                          on ARC: even the state-of-the-art GPT-4.1, or Claude-4-
                          Sonnet could only solve 8.00% and 20.75% of the test tasks                                                         Traditional program synthesis algorithms rely on iterated
                          respectively. While search considerably improves the perfor-                                                       search algorithms like genetic programming or sequential
                          manceofouropen-source models (from 1-2% to 14-26%),                                                                MonteCarlo(Goldberg; Holland; Koza, 1994; Langdon &
                          fixed model capabilities create a performance ceiling.                                                             Poli, 2013; Liang et al., 2010; Saad et al., 2019), where their
                          Through iterative self-improvement, SOAR breaks through                                                            effectiveness heavily depends on well-engineered program
                          this ceiling. After four training iterations, SOAR solves                                                          priors and mutation operators. With the emergence of deep
                          an extra 10-19% tasks across model sizes. Given access to                                                          learning, recent work has explored learning these compo-
                          target tasks, but not to their ground truth solutions, SOAR                                                        nents from data: either training neural networks to sam-
                          learns to solve an extra 3-5% tasks across model sizes with                                                        ple programs conditioned on input-output examples (Balog
                          test-time training. With a final test performance of 52%,                                                          et al., 2016; Ellis et al., 2021), or training decision mech-
                          our best model outperforms previous program synthesis                                                              anisms to speed up search (Shi et al., 2022). While these
                          methods based on open weight LLM and without using any                                                             approaches rely on exhaustive search in constrained hand-
                          hand-crafteddata. Importantly, these gains arise purely from                                                       defined programming languages, SOARlearns to synthesize
                          the model learning to sample and refine better programs                                                            programs in Python from past synthesis attempts, eliminat-
                          through its own search experience, without requiring task                                                          ing the need for offline datasets or manual engineering.
                          specific human-engineering or training data.                                                                       Large language models have emerged as powerful tools for
                          Our work demonstrates how program synthesis systems                                                                program synthesis (Roziere et al., 2023; Guo et al., 2024),
                          can transcend the limitations of their base models through                                                         both as direct solution generators (Li & Ellis, 2024) and as
                          self-improvement. We present:                                                                                      mutation operators for evolutionary search (Lehman et al.,
                                                                                                                                             2023; Olausson et al., 2023; Meyerson et al., 2024). These
                          1. a framework that iteratively improves its evolutionary                                                          capabilities have enabled sophisticated search methods that
                               search capabilities by learning from its own search expe-                                                     can both refine and diversify solutions: e.g.generating di-
                               rience without human-engineered data,                                                                         verse difficult programming problems (Pourcel et al., 2024),
                          2. a test-time training mechanism enabling continuous im-                                                          high-quality poems (Bradley et al., 2023) or interesting pat-
                               provement on target problems,                                                                                 terns in cellular automata (Kumar et al., 2024). Whether
                          3. empirical evidence that iterative model improvement                                                             used for convergent or divergent search, these methods treat
                               can help overcome the performance plateaus inherent                                                           the LLM as a fixed component, preventing them from im-
                               to search methods,                                                                                            proving through experience. SOAR overcomes this lim-
                          4. state-of-the-art results for program synthesis leveraging                                                       itation by continuously adapting its underlying language
                               open-source LLMs on ARC-AGI’s public test set.                                                                model through self-improvement.
                          Byshowinghowprogramsynthesissystemscanbootstrap                                                                    Recent work has shown that LLMs can learn to reason
                                                                                                                                        2
