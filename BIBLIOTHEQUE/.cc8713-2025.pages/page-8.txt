                                               Learning program synthesis with self-improving language models: A case study on ARC-AGI
                                            QC-7B                                                                                                                                         SOAR (all models)
                                  55        QC-14B                                                                                 50
                                            QC-32B
                                            Q-72B
                                            Mistral-Large                                                                          40                                                     Gemini-2.5-Pro
                                  50        all models
                                            oracle (all models)                                                                                                                           o3-mini
                                                                                                                                   30
                                  45
                                                                                                                                                                                          Claude-4-Sonnet
                                                                                                                                  -test score (%)20                                  One-shot
                                 -test score (%)                                                                                                                                     SOAR (base model)
                                  40                                                                                                                                                 SOAR (1 train)
                                                                                                                                  ARC                                                SOAR (2 train)
                                 ARC                                                                                               10     GPT-4.1                                    SOAR (3 train)
                                                                                                                                                                                     SOAR (all train + 0 test)
                                  35                                                                                                                                                 SOAR (all train + 1 test)
                                                                                                                                                                                     SOAR (all train + 2 test)
                                                                                                                                     0                                               SOAR (all models)
                               SOAR(all-train)                       1                            2                                      7             14            32             72            123
                                                   Test time training iteration                                                                               Model size (B)
                      Figure 3. Iterated self-improvement on test problems. ARC-test                                   Figure 4. Performance plateaus with increasing model size when
                      performance across test-time training iterations. Iteration 0: search                            using fixed sampling and refinement capabilities (Sample-6k and
                      with the models finetuned in training iteration 4 (right-most points                             Sample&Refine-6k). In contrast, SOAR progressively lifts the
                      in Figure 2). All: score achieved by applying majority voting on                                 scaling curves across iterations, enabling smaller models to match
                      the combined search data of the five models.                                                     or outperform much larger ones. Note that only the 7B, 14B, and
                                                                                                                       32Bmodelsarefromthesamefamily(Qwen-2.5-Coder),72Bis
                                                                                                                       from the Qwen-2.5 family, and 123B is Mistral-large-2407.
                      Self-improvementonARC-testproblems(test-timetrain-
                      ing).      The results from Section 4.3 demonstrated signifi-                                    Self-improvement enables each model size to reach perfor-
                      cant performance gains through iterative self-improvement                                        mancelevels that previously required much larger models.
                      on training tasks. However, practical applications require                                       Figure 5 reveals a similar ceiling when scaling search bud-
                      systemsthatcanimproveonnewproblemswithoutaccessto                                                get. With the 7B base model, performance saturates after
                      ground-truth solutions. This raises a key question: Can our                                      roughly 5k search attempts. In contrast, SOAR-iteration 1
                      self-improvement framework continue to raise performance                                         nearly doubles its ARC-test accuracy, which remains true
                      whenadaptedtotarget test problems?                                                               whencontrolling for FLOP budget (see Appendix B). No-
                      Starting from models fine-tuned on all data collected                                            tably, a significant fraction of this gain appears during the
                      through 4 iterations on ARC-train problems (previous sec-                                        refinement phase. These results show that search alone is
                      tion), we perform two additional iterations of test-time train-                                  insufficient—learning to refine is essential.
                      ing on ARC-test problems, leading to an extra 5% perfor-
                      manceonARC-test(Figure2). Takentogether,thecombina-                                                        35
                      tion of train-time and test-time improvements dramatically                                                 30
                      raised performance across all model scales. Our 7B model                                                   25
                      improved from its initial 14.25% to 36.25% accuracy on                                                     20
                      ARC-test, a 2.5 fold increase. Similarly, the 14B model
                      rose from 19.87% to 42.75%, the 32B model improved from                                                    15
                                                                                                                                -test score (%)                                   SOAR (base-model)
                      25.25% to 44.37%, 72B from 25.62% to 44.87%, while                                                         10                                               SOAR (1 train)
                                                                                                                                ARC                                               SOAR (2 train)
                      Mistral-Large-2 accuracy improved from 26.25% to 45.5%.                                                      5                                              SOAR (3 train)
                                                                                                                                                                                  SOAR (all train + 0 test)
                      Bycombiningsolutions across all model sizes through ma-                                                                                        SampleRefine SOAR (all train + 1 test)
                      jority voting, we achieved our peak performance of 52.00%                                                    0                                              SOAR (all train + 2 test)
                                                                                                                                       0       1000 2000 3000 4000 5000 6000
                      onARC-testandanoracleperformance of 57.25%.                                                                                         Number of samples
                                                                                                                       Figure 5. Search alone hits diminishing returns with increased bud-
                      4.4. Escaping scaling plateaus through                                                           get: the base 7B model plateaus after about 5k search attempts.
                             self-improvement                                                                          SOARoutperformsthis baseline by a wide margin, with improve-
                                                                                                                       ments compounding across iterations.
                      Figure 4 shows that simply running search with increasing                                        Together, these results show that SOAR breaks through both
                      model size eventually yields diminishing returns. While                                          model-size and search-budget plateaus by improving the
                      larger models perform better in early iterations, Sample-6k                                      modelitself. Rather than pushing harder against fixed limits
                      and Sample&Refine-6k curves flatten beyond 32B, suggest-                                         (scaling model sizes or search budgets), SOAR lifts them—
                      ing a model-size scaling plateau: more parameters alone                                          transforming flat scaling curves into steps of improvement.
                      do not suffice when the model’s sampling and refinement                                          This effect is especially striking for smaller models: Qwen-
                      behaviors remain fixed. In contrast, the SOAR curves re-                                         2.5-7B reaches 36.25% on ARC-test after SOAR’s itera-
                      veal a different pattern. While each SOAR iteration also                                         tions, outperforming much larger systems like o3-mini and
                      plateaus, subsequent iterations consistently lift the perfor-                                    Claude-4-Sonnet. These results establish SOAR as a sig-
                      mance ceiling—establishing new, higher scaling curves.                                           nificant advance in program synthesis approaches to ARC.
                                                                                                                   8
