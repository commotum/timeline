                                  Learning program synthesis with self-improving language models: A case study on ARC-AGI
                Ourapproach outperforms prior methods that relied on ex-             self-improvement alone can bootstrap strong reasoning ca-
                tensive human-generated training data (Li et al., 2024) and          pabilities in general-purpose language models.
                and expensive search methods built on much larger closed-            Ourexperiments highlight two key findings. First, SOAR
                source models (Greenblatt, 2024) (see further comparisons            overcomes the performance plateaus typically observed
                in Appendix Table 6). By enabling models to improve them-            whenscalingmodelsizeorsearchbudget. Byimprovingthe
                selves from scratch, SOAR eliminates the need for hand-              underlying model itself, it establishes new, higher scaling
                engineered programs, DSLs, or external datasets—marking              baselines. Second, we observe complementary problem-
                a step toward more autonomous, scalable synthesis systems.           solving strategies across model sizes: smaller models often
                4.5. Solution diversity across iterations                            learn faster and sometimes solve tasks that larger ones miss.
                                                                                     Training base models on aggregated solutions from multi-
                Tokeeponimprovingandsolvingharder problems, SOAR                     ple models and iterations yields the strongest improvements
                must keep on exploring the space of possible solutions. Fig-         (see Table 5), while ensembling solutions across model sizes
                ure 6 shows a steady decrease in solution diversity across           leads to our best ARC-test performance (52%). These re-
                self-improvement iterations for the problems SOAR man-               sults suggest that cross-model diversity is a key driver of
                aged to solve. For unsolved problems, diversity initially            performance gains in self-improving program synthesis.
                drops after the first iteration but then plateaus, suggest-          Crucially, SOAR offers a substantial advantage over ap-
                ing that our relabeling method may help maintain some                proaches that rely on fixed models within static search loops.
                exploratory capacity. While this helps preserve diversity            State-of-the-art systems like FunSearch and AlphaEvolve
                on unsolved tasks, integrating explicit diversity-enhancing          (Romera-Paredesetal., 2024; Liu et al., 2024; AlphaEvolve-
                strategies could further extend SOAR’s ability to explore            team, 2025) use program synthesis without adapting the
                solution spaces and sustain continual improvement.                   model. SOAR could serve as a drop-in upgrade, enabling
                          0.6                                                        these systems to continually learn from their own search
                                    Solved                                           traces. The framework could also be extended with richer
                          0.5       Unsolved                                         operators, such as crossover (Meyerson et al., 2024).
                          0.4                                                        While SOAR is domain-agnostic, we only evaluate it on
                          0.3                                                        ARC.Futureworkshouldtestitsapplicability to domains
                        Diversity                                                    like software engineering or mathematical discovery (Dong
                          0.2                                                        &Ma,2025; Jain et al.). Computational efficiency is an-
                          (avg pairwise distance)0.1                                 other limitation: SOAR currently requires 6,000 synthesis
                          0.0                                                        attempts per task per iteration. Although we observe steady
                               0            1            2             3             gains, these diminish over time, hinting at potential lim-
                                              Generation                             its. Whether these are intrinsic or methodological remains
                Figure 6. Solution diversity across generations of SOAR. Thin        open, but several strategies could help, such as adaptively
                lines indicate solution diversity across for each of the 400 ARC-    rerouting the search budget from solved tasks to harder ones,
                train problems, colored in green when solved, in red when un-        or improving optimization methods (e.g., see Chow et al.
                solved. Thick lines indicate averages across solved and unsolved     (2024); Tang et al. (2025); Gehring et al. (2024)).
                problems respectively. SOAR maintains solution diversity for
                unsolved problems but converges on lower solution diversity for      Onelikely bottleneck is low solution diversity. While prior
                solved problems. Diversity is measured as the average pairwise       workfoundthat RLandfinetuning often reduce output di-
                cosine distance in embedding space (CodeRankEmbed)                   versity (Zhang et al., 2025; Yue et al., 2025), we find that
                5. Discussion                                                        SOARpreservesdiversity on unsolved tasks, likely due to
                                                                                     hindsight relabeling, which retroactively creates new prob-
                Our work shows that program synthesis systems can tran-              lems from failed programs. Still, this maintained diversity
                scend their initial capabilities by iteratively improving both       appears insufficient to sustain continual progress. Future
                sampling and refinement through a cycle of evolutionary              workcould enhance it by explicitly optimizing for diversity
                search and learning. Here we reflect on the broader implica-         during finetuning, introducing quality-diversity methods, or
                tions and challenges ahead.                                          generating new problems to expand solution diversity (Co-
                ARC was explicitly designed to resist pattern matching               las et al., 2022; Pourcel et al., 2024). These directions may
                and require core reasoning, making it a strong testbed               help maintain a virtuous cycle of improvement and push
                for program synthesis (Chollet, 2024). Most models fail              program synthesis closer to open-ended discovery.
                without human-written examples or human-encoded priors.
                SOAR’sability to improve purely from its own search ex-
                perience—without demonstrations or DSLs—show that
                                                                                  9
