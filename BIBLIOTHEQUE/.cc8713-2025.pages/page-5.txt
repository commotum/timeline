                                                Learning program synthesis with self-improving language models: A case study on ARC-AGI
                      per task. This is achieved by balancing the sampling over                                          large-2407 to study larger models, including one trained on
                      bins of the input-output accuracy of the parent program:                                           different data, by a different company. We finetune models
                      0%, 1-34%, 34-98%, and 100% to ensure diversity. Sec-                                              on a single H100 using the RS-LoRA (7B and 14B models)
                      tion 4.2 compares this strategy to alternatives. With the                                          and RS-QLoRAalgorithms(Huetal.,2021;Dettmersetal.,
                      resulting dataset D′refine, we finetune our model to better                                        2024; Kalajdzievski, 2023) (larger models) with the Unsloth
                                                     correct
                      refine programs by minimizing:                                                                     library (Daniel et al., 2023). We use LoRA rank 256 with
                        L = E refine -logP (f+ | f,x                           , x      , y      , y       ).          α=32,andtrainfor3epochswithalearningrateof5e-5
                                    D                    θ                  test    train    train   synth
                                      correct                                                                            (see Section D.4 for details).
                      3.4. Closing the loop: iterative self-improvement on
                             training and testing tasks                                                                  4. Experiments
                      Self-improvement on training tasks.                                The search and                  Our experiments explore how program synthesis systems
                      learning phases described above form the building blocks                                           can grow beyond their initial capabilities through self-
                      of SOAR’s self-improvement loop. At each iteration i, we                                           improvement. We begin by showing that even the strongest
                      alternate between: (1) Sample&Refine search phase: Using                                           languagemodelsstruggletosolveARCtaskswithoutsearch,
                      modelθi to sample and refine programs through evolution-                                           establishing the need for iterative exploration (Sec. 4.1).
                      ary search and (2) Learning phase: Using the search traces                                         Fromthere, we demonstrate how models can learn to search
                      to train an improved model θ                           by finetuning the base                      more effectively by fine-tuning on their own synthesis at-
                                                                       i+1
                      model(see Figure 1). Each iteration builds upon previous                                           tempts—improving both their ability to sample and re-
                      improvements—themodelfinetunediniteration i’s learn-                                               fine programs (Sec. 4.2). These improvements accumulate
                      ing phase powers the search in iteration i + 1, generating                                         across iterations, creating a virtuous cycle of increasingly ef-
                      richer training data to train the model i + 1. This creates a                                      fective search (Sec. 4.3). Crucially, this cycle allows SOAR
                      virtuous cycle where better models enable more effective                                           to break through the performance ceilings encountered by
                      search which, in turn, yields better training data.                                                scaling model size or compute budget alone (Section 4.4).
                      After this training phase, we collect and deduplicate all so-                                      We conclude by analyzing the diversity of generated so-
                      lutions generated by the models using an embedding model                                           lutions and find that while SOAR tends to converge on
                      with a cosine similarity threshold of 0.9 (CodeRankEmbed).                                         consistent programs after success, it preserves diversity on
                      Wethensubsamplethisdataset as described in Section 3.3                                             unsolved tasks (Sec. 4.5). Together, these results establish
                      (50 examples per ARC-train problem) and use the resulting                                          SOARasasignificantadvanceinprogramsynthesis,demon-
                      dataset to finetune a base model that will serve as the basis                                      strating how systems can bootstrap their own improvement
                      of test-time training iterations.                                                                  through iterative search and learning. Appendix H provides
                                                                                                                         examples of sampled programs and refinement examples.
                      Test-time training.                 Wecanadapttheself-improvement                                  4.1. Program synthesis methods must leverage search
                      loop to let the agent learn from target problems where the
                      ground truth is not accessible. This is achieved by focusing                                       Canstate-of-the-art language models solve ARC tasks in a
                      on finetuning sampling capabilities by selecting solution ex-                                      single attempt? To find out, we evaluated several models in
                      amples according to their training accuracy on input–output                                        a one-shot setting, where each model tries to generate a cor-
                      examples only (instead of ground truth accuracy), before                                           rect program in just one try. As shown in Table 1, even the
                      applying hindsight relabeling. Refinement finetuning could                                         strongest models achieve modest success rates on ARC-test:
                      potentially be adapted to work without ground truth (at test                                       e.g.Claude-4-Sonnet (20.75%), GPT-4.1 (8.00%). Smaller
                      time) with hindsight relabeling. However, we reserve this                                          open-source models perform even worse, with Qwen-2.5-
                      approach for future work, as our current test-time improve-                                        Coder models achieving 1.00-2.25% success rates across
                      ment method focuses solely on refining explicit sampling                                           modelsizes. Direct program synthesis remains too challeng-
                      capabilities. This enables a powerful test-time training loop:                                     ing for current language models.
                      after running several iterations of full self-improvement on                                       Two approaches can potentially improve performance:
                      the training set, we can further adapt our model through                                           (1) transduction, where models directly predict output grids
                      additional iterations focused specifically on test tasks.                                          without generating programs, or (2) inductive program
                                                                                                                         synthesis combined with search. While both approaches
                      Implementation details.                    Weevaluate SOARincombina-                               currently yield comparable results (Li et al., 2024), pro-
                      tion with LLMs from the Qwen-2.5-Coder series (7B, 14B,                                            gram synthesis offers a key advantage: it enables sys-
                      32B), known for their strong coding capabilities while re-                                         tematic exploration of the solution space, allowing perfor-
                      maining small enough to allow compute-efficient finetuning                                         mancetoscale with additional compute through search. We
                      (Hui et al., 2024). We also used Qwen-2.5-72B and Mistral-                                         compare three settings using a series of Qwen-2.5-Coder
                                                                                                                     5
