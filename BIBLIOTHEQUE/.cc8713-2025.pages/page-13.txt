                                Learning program synthesis with self-improving language models: A case study on ARC-AGI
               A. Comparison with prior work
                                                    Method %ARC-testsolved #attempt/task Faircomparison?
                  One-shot LLMsampling(1attempt/task)
                         Claude-3.5-Sonnet / Claude-4-Sonnet      11.25 / 20.75           1         Yes (closed-source LLM)
                                           GPT-4.1 / o3-mini      8.00 / 33.00            1         Yes (closed-source LLM)
                                              Mistral-large-2         2.50                1         Yes
                                               Qwen2.5-72B            4.00                1         Yes
                                  Qwen2.5-Coder-(7/14/32B)      1.00 / 1.00 / 1.75        1         Yes
                                            Sample&Refine
                                              Mistral-large-2        26.25              6000        Yes
                                               Qwen2.5-72B           25.62              6000        Yes
                         Qwen2.5-Coder-(7/14/32B) (QC-nB) 14.25 / 19.87 / 25.25         6000        Yes
                Ours: iterated self-improved search (SOAR)
                                              SOAR-Mistral           45.50              6000        ours
                                               SOAR-Q-72B            44.87              6000        ours
                                       SOAR-QC-(7/14/32B) 36.25/42.75/44.37             6000        ours
                                               SOAR-QC-all           52.00             6000×5       ours
                                      SOAR-QC-all(Oracle)            57.25             6000×5       ours, but using oracle eval (skip maj. vote)
                                 Prior inductive approaches
                                     CodeIt (Butt et al., 2024)      15.00              2500        Yes
                    BARC-induction (Heavy) (Li et al., 2024)         30.50              10000       Yes, but heavy use of human data and closed LLM
                 BARC-induction (Potpourri) (Li et al., 2024)        38.00              20000       Yes, but heavy use of human data and closed LLM
                                       Icecuber (Wind, 2020)         39.00            Unknown       No, looking at val set, human DSL
                                           (Greenblatt, 2024)        42.00              8160        Yes, but closed-source LLM
               Table 6. Comparison of inductive methods on the ARC benchmark. Our approach SOAR outperforms previous induction performance.
               B. Scaling laws
               Finetuning costs: Finetuning is inexpensive compared to the search phase. FLOPs per iteration is 6N ×(100·T), where N
               is LLMparametersandT istokenspercompletion. With≤ 100datapointspertask, sampling FLOPs is 2N ×(6000·T ·n),
               makingfinetuning ∼ 5% of total FLOPs—nearly negligible. Additionally, autoregressive generation is slower (token-by-
               token forward passes), while finetuning processes sequences in one forward and backward pass (see Austin et al. (2025) for
               moredetails). Figure 8 plots the performance of the different generations of SOAR with a search budget of 6k, against the
               performance of Sample&Refine using the base model with a search budget of 12.6k matching the FLOPs used by SOAR at
               generation 1 (6k search by the base model, 6k search by the finetuned model, and 5% extra to cover for finetuning costs).
               SOARatgeneration1reachesfar superior performance with the same computational budget.
                                                                             13
