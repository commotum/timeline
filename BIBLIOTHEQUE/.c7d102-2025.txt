                OPTOELECTRONICS LETTERS                                                     Vol.21 No.9, 15 September 2025 
                Point-voxel dual transformer for LiDAR 3D object detec-
                        *
                tion  
                   
                                      1                   1             1                     2
                       TONG Jigang , YANG Fanhang , YANG Sen , and DU Shengzhi ** 
                       1. Tianjin Key Laboratory for Control Theory & Applications in Complicated Systems and Intelligent Robot Labora-
                         tory, Tianjin University of Technology, Tianjin 300384, China 
                                                                                                                            1
                       2. Department of Electrical Engineering, Tshwane University of Technology, Pretoria 0001, South Africa  
                        
                       (Received 17 July 2023; Revised 2 March 2025) 
                       ©Tianjin University of Technology 2025 
                        
                       In this paper, a two-stage light detection and ranging (LiDAR) three-dimensional (3D) object detection framework is 
                       presented, namely point-voxel dual transformer (PV-DT3D), which is a transformer-based method. In the proposed 
                       PV-DT3D, point-voxel fusion features are used for proposal refinement. Specifically, keypoints are sampled from en-
                       tire point cloud scene and used to encode representative scene features via a proposal-aware voxel set abstraction 
                       module. Subsequently, following the generation of proposals by the region proposal networks (RPN), the internal en-
                       coded keypoints are fed into the dual transformer encoder-decoder architecture. In 3D object detection, the proposed 
                       PV-DT3D takes advantage of both point-wise transformer and channel-wise architecture to capture contextual infor-
                       mation from the spatial and channel dimensions. Experiments conducted on the highly competitive KITTI 3D car de-
                       tection  leaderboard  show  that  the  PV-DT3D  achieves  superior  detection  accuracy  among  state-of-the-art 
                       point-voxel-based methods. 
                       Document code: A Article ID: 1673-1905(2025)09-0547-8 
                       DOI    https://doi.org/10.1007/s11801-025-3134-9 
                        
                        
                1. Introduction 
                                                                                   grades localization accuracy.   
                                                                                      To address the issue of repetitive calculations in tradi-
                Three-dimensional  (3D)  object  detection  from  point 
                                                                                   tional  point-wise  methods  and  the  loss  of  localization 
                clouds for autonomous driving attracts increasing interest 
                                                                                   information  in  voxel-based  approaches,  we  turned  our 
                                                                                                                              [9]
                in the field of deep learning. Traditional neural networks 
                                                                                   attention  to  transformers.  Transformer   was  recently 
                                                                 [1,2]
                are  successfully  applied  in  computer  vision     ,  which 
                                                                                   considered as an ideal model for point cloud processing, 
                paved the way for light detection and ranging (LiDAR) 
                                                                                   rooting from the advantage of self-attention. As the core 
                3D  object  detection.  However,  unlike  regular  images 
                                                                                   component in transformer, self-attention is able to cap-
                where convolutional neural network (CNN)-like opera-
                                                                                   ture relationships among points in large scale. Besides, 
                tors can be directly applied, point clouds are commonly 
                                                                                   its inherent permutation invariance is well suited for un-
                unordered and sparse. To tackle these challenges, several 
                                                                                   ordered points. According to the operating space, trans-
                                                                    [3,4]
                                                                                                                                         [10]
                methods  project  raw  point  clouds  into  voxels     ,  and 
                                                                                   formers are divided into point-wise transformers           and 
                                                                                                                [11]
                then use 3D CNNs to extract features. But these methods 
                                                                                   channel-wise transformers      .  The former highlights the 
                suffer from a growing memory requirement, and inevita-
                                                                                   spatial relationships among input points, while the latter 
                bly sacrifice fine-grained position details which are im-
                                                                                   focuses on the interactions among different channels. 
                portant for accurate localization. On the other hand, fol-
                                                                                      This  paper  proposes  a  point-voxel  dual  transformer 
                                                [5]                   [6]
                lowing  the  pioneering  work   and  its  variants ,  the 
                                                                                   (PV-DT3D) by combine the advantages of the point-wise 
                point-wise methods directly extract features from the raw 
                                                                                   and  channel-wise  methods.  The  PV-DT3D  includes  a 
                                                              [7]
                points. Generally, the point-wise methods  retain more 
                                                                                   proposal-aware voxel set abstraction (VSA) module for 
                accurate  position  information  for  fine-grained  refine-
                                                                                   aggregating  information  of  point-level  positions, 
                ment, but they are usually time-consuming due to repeti-
                                                                                   multi-scale voxel features and the generated proposals. 
                                                                  [4,8]
                                                                                                                             [12]
                tive  calculation.  The  voxel-based  approaches        effec-
                                                                                   Comparing to the vanilla VSA module          , the PV-DT3D 
                tively  generate  high  quality  proposals,  but  suffer  from 
                                                                                   leverages  better  information  of  high-quality  proposals 
                high memory demands and information loss which de-
                                                                                   and adds local correlations to keypoints to stabilize the 
                                                                             
                *      This work has been supported by the Natural Science Foundation of China (No.62103298), and the South African National Research Foundation 
                    (Nos.132797 and 137951).   
                **    DU Shengzhi is a professor in Tshwane University of Technology, South Africa. He received his Ph.D. degree from Nankai University in 2005. 
                    His research interests include intelligent human-machine systems, computer vision, image analysis and understanding, artificial intelligence. 
                    E-mail: dushengzhi@gmail.com. 
                ·0548·                                                                                                                                             Optoelectron. Lett. Vol.21 No.9 
                training process. 
                                                                                    An  overview  of  the  proposed  PV-DT3D  is  shown  in 
                   Experiments  on  KITTI  dataset  show  that  the 
                                                                                    Fig.1. The raw points are firstly voxelized in the form of 
                PV-DT3D achieves superior detection accuracy among 
                                                                                    region proposal networks (RPN) for high quality propos-
                point-voxel-based methods. Ablation evaluations confirm 
                                                                                    als. And the furtherest point sampling (FPS) algorithm is 
                the  effectiveness  of  the  proposed  proposal-aware  VSA 
                                                                                    adopted to select representative points to encode scene 
                module and dual transformer for 3D object detection. 
                                                                                    features. In order to take advantages of both points and 
                                                                                    voxels,  the  proposed  proposal-aware  VSA  module  en-
                2. Related work 
                                                                                    codes  multi-scale  voxel  features,  point  features  with 
                                                                                    fine-grained localization, and the information of propos-
                2.1 Point-based LiDAR 3D object detection 
                                                  [5,6]
                                                                                    als into keypoint features. Then the dual transformer in-
                Following the pioneering work         , point-based detection 
                                                                       [7]
                                                                                    vestigates  spatial  correlations  and  channel  contextual 
                methods  are  in  rapid  development.  PointRCNN   pro-
                                                                                    interactions among the large-scale points for confidence 
                posed a two-stage point-based framework for 3D object 
                                                                                    prediction and bounding-box regression. In Fig.1, BEV 
                detection,  which  generated  proposals  from  segmented 
                                                     [6]
                                                                                    represents the bird-eye view. 
                foreground points by PointNet++ . The 3D single stage 
                                            [13]
                                                                                    3.1 3D proposal generation and keypoints sampling 
                object detector (3DSSD)        presented a single-stage ap-
                                                                                    Due to the computational efficiency and high recall, the 
                proach by a novel point-sampling strategy. Based on the 
                                                                                               [8]
                                                                            [14]
                                                                                    SECOND  is adopted as the 3D backbone network and 
                3DSSD, semantics-augmented set abstraction (SASA)               
                                                                                                                     [23]
                                                                                    RPN. For the KITTI dataset          ,  given  an  N-points  3D 
                exploited  a  semantics-guided  point  sampling  algorithm 
                                                                                    scene with position coordinates and reflectance, the pro-
                for detection. 
                                                                                    posals generated by RPN include the following informa-
                2.2 Voxel-based LiDAR 3D object detection 
                                                                                                                                        prop
                                                                                             prop  prop  prop   prop  prop  prop
                                                                                                            
                                                                                                                                      
                                                                                    tion:                                       and         ,  rep-
                                                                                           x    , y   , z     ,l   , w    ,h
                Voxel-based methods aim to transform the unstructured 
                                                                                                            
                points into regular voxels, over which 3D CNNs are able 
                                                                                    resenting the center coordinate, length, width, height, and 
                to be directly applied. Generally, voxel-based approaches 
                                                                                    orientation of the proposal, respectively. The FPS strat-
                can  generate  high  quality  proposals.  The  sparsely  em-
                                                                                    egy is applied to sample n-keypoints, in such a manner 
                                                                [8]
                bedded convolutional detection (SECOND)  is an effec-
                                                                                    that  keypoints  are  uniformly  distributed  in  the  overall 
                tive  3D  sparse  convolution network to extract features 
                                                                                    scene. 
                                              [4]
                from voxels. Voxel R-CNN  used voxel region of inter-
                                                                                    3.2 Proposal-aware VSA module  
                est (RoI) pooling to extract neighboring voxel-wise fea-
                                                                                    3.2.1 Vanilla VSA module 
                                     [15]
                tures. Focals Conv        proposed a focal sparse convolu-
                                                                                                                                    [12]
                                                                                    The VSA is firstly exploited in PV-RCNN            , which en-
                tion module to enhance the capabilities of sparse CNNs. 
                                                                                    codes the multi-scale voxel features into keypoints. Spe-
                2.3 Point-voxel-based LiDAR 3D object detection 
                                                                                    cifically,  l   and  n   represent  the  k-th  level  during  3D 
                                                                                                k        k
                            [12]
                PV-RCNN  proposed VSA module aggregating voxel 
                                                                                    sparse convolution and the number of non-empty voxels 
                features and raw point features for refinement. The point 
                                                                                                                                        l      l
                                                                                                                                 l
                                                                                                                                        k       k
                                                                                                                                  k
                                                         [16]
                                                                                    at the k-th level, respectively. Denote                         
                                                                                                                               V  v ,,v
                                                                                                                                                
                density-aware        voxels       (PDV)          investigated 
                                                                                                                                        1
                                                                                                                                               n
                                                                                                                                                k
                point-density  and  proposed  point  density-aware  voxel 
                                                                                    as    the   set    of   voxel    spatial   coordinates     and 
                network to improve the multi-class accuracy. 
                                                                                               l       l
                                                                                       l
                                                                                               k        k
                                                                                        k
                                                                                                            as the set of voxel-wise features at 
                                                                                     F  f ,,f
                                                                                                        
                                                                                              1
                                                                                                       n
                2.4 Transformer in point clouds 
                                                                                                       k
                Due to the inherent permutation invariance and strong 
                                                                                    the k-th level. For a keypoint p, identify its non-empty 
                                                                                                                        i
                capacity  of  global  modeling,  transformers  have  been 
                                                                                    neighboring  voxel-wise  features  within  radius  r   and 
                                                                                                                                             k
                applied  to  point  cloud  classification  and  segmenta-
                                                                                                                                        l
                                                                                                                                         k
                                                                                                                                       v p
                                                                                    concatenate  the  local  relative  coordinates                to 
                    [17,18]                  [19-21]
                                                                                                                                         j    i
                tion     ,  object  detection      ,  and  so  on.  For  object 
                                                        [20]
                                                                                    indicate the corresponding voxel-wise features, as shown 
                detection, voxel transformer (VoTr)          presented an ef-
                                                                                    in   
                fective voxel transformer, where the sparse voxel module 
                                                                                                                  l
                                                                                                                         2
                                                                                                                   k
                and  submanifold  voxel  module  operate  on  empty  and 
                                                                                                                              
                                                                                                                 v p r
                                                                                                                   j    i     k
                                                                                                                              
                non-empty voxels, respectively. The channel-wise trans-
                                                                                                               T
                                                                                                                              
                                                                                          l        l   l             l
                                                                                                                            l
                                                                                          k        k   k              k
                                                                                                                            k
                                                                                                            
                                                                                                                                           (1) 
                                                                                        S       f  ;v    p    | v V          .
                                                                                                                              
                former architecture to constitute a two-stage 3D object 
                                                                                         i        j    j    i         j
                                                                                                            
                                                [11]
                                                                                                                              
                detection framework (CT3D)          exploited a channel-wise                                         l
                                                                                                                            l
                                                                                                                      k
                                                                                                                             k
                                                                                                                  f    F
                                                                                                                              
                                                                                                                     j
                                                                                                                              
                re-weighting  strategy  for  refinement.  The  single-stage 
                                                                                                                                     v
                                                                                                                                      l
                3D  object  detector  with  point-voxel  transformer 
                                                                                                                                      k
                                                                                       Then, the summarized voxel features  f           within the 
                                                                                                                                    i
                             [22]
                (PVT-SSD)       leveraged the strengths of both point and 
                                                                                                                             l
                                                                                                                              k
                                                                                                                            S
                                                                                    k-th  level  neighboring  voxel  set       of  p   are  used  to 
                                                                                                                                    i
                voxel features.   
                                                                                                                             i
                                                                                                                      [5]
                   However, the above methods have not fully leveraged 
                                                                                    generate features by PointNet . And the voxel features 
                                                                                       voxel
                the  potential  of  simultaneously  incorporating  both 
                                                                                     f        will  be  aggregated  at  four  convolution  levels. 
                                                                                      i
                point-wise and channel-wise transformers. Consequently, 
                                                                                    Besides  of  the  above  voxel-wise  operation,  in 
                we propose the PV-DT3D. 
                                                                                                                                   BEV
                                                                                                [12]
                                                                                                                                 f
                                                                                    PV-RCNN ,  BEV-based  features                       and  raw 
                                                                                                                                  i
                3. Methodology 
                                                                                                                     raw
                                                                                    PointNet-based    features  f        are  encoded  into  key-
                                                                                                                     i
               TONG et al.                                                                                                                                Optoelectron. Lett. Vol.21 No.9·0549· 
               point features for making up the quantization loss of               voxelization and having larger receptive fields. 
                                                                                 
                                                                                 
                                                                                    
                                                                                                                                     
                                                            Fig.1 The overview of PV-DT3D 
                                                                               
               3.2.2 Proposal-aware VSA module                                  points. Thus, the point-wise transformer highlights spa-
               To better exploit the information of high-quality propos-        tial  relationships.  While  the  channel-wise  transformer 
               als for refinement and stable training process, inspired by      investigates interactions among different channels due to 
                                                              [11]
               proposal-to-point  (P.T.P.)  strategy  in  CT3D   ,  we  pre-    the attention weights distributed along channels. Gener-
               sent an improved proposal-aware VSA module. For sam-             ally,  the  two  kinds  of  attention  operations  can  be  ex-
               pled keypoints in the subsequent processing, we calcu-           pressed as 
               late  the  3D  relative  spatial  coordinates  between  key-
                                                                                                                          T
                                                                                   
                                                                                                                           
                                                                                                                    QK
                                                                                      Point-wise Attn  softmax              V
                                                                                   
               points and corresponding proposal points as shown in                                                        
                                                                                                                           
                                                                                                                        d
                                                                                   
                      j         j
                                                                                                                           
                  p  p  p , j 1,2,,9,
                                                                  (2) 
                                                                                                                                          (5)   
                                                                                                                                   .
                     i     i
                                                                                   
                                                                                                                       T
                                                                                                                             
                                                                                                                     Q K
                                                                                   
               where p denotes the spatial coordinates of a keypoint in 
                        i
                                                                                     Channel-wise Attn softmax                V
                                                                                                                             
                                                                                   
                                                 j
                                                                                                                             
               corresponding proposals, and p denotes the 3D coordi-
                                                                                                                         d
                                                                                                                             
                                                                                   
               nate  of  the  corresponding proposal center (or a corner 
                                                                                   From Eq.(5), one finds an interesting fact on the dif-
               point).  By  this  strategy,  the  information  of  proposal 
                                                                                ference of computation loads of these two transformers. 
                  prop  prop  prop  prop  prop  prop  prop
                 x    , y   , z   ,l   , w    , h   ,      will  be  en-
                                                         
                                                                                The size of Point-wise Attn is N×N, while the size of 
                                                                                Channel-wise  Attn  is  d×d.  Generally,  for  point  cloud 
               coded into keypoint features as shown in   
                                                                                scene,  N»d. Thus, the point-wise transformers are suf-
                    prop       1    2       9   r      128
                                                
                                                                     (3) 
                   f      p ,p ,,p ,p              ,
                    i          i   i        i   i
                                                
                                                                                fering from the computation complexity growing quad-
                        r
                                                                                ratically with respect to the size of input point clouds. 
               where p   denotes  the  reflectance  of  the  i-th  sampled 
                        i
                                                                                3.3.2 Dual transformer for proposal refinement 
               keypoint for KITTI dataset. Then, the output of the pro-
                                                                                Inspired by Refs.[11] and [24], the dual transformer en-
               posal-aware VSA module is 
                    pvsa      prop   voxel  raw   BEV      1D
                                                     
                                                                                coder-decoder architecture is proposed for bounding-box 
                                                                            (4) 
                   f      f     , f     , f   , f      .
                    i        i      i      i     i
                                                     
                                                                                refinement,  taking  advantages  of  both  point-wise  and 
                  The  above  scheme  enhances  the  local  correlations 
                                                                                channel-wise  transformers.  Thus,  the  dual  transformer 
               among input points within the same proposal, thus stabi-
                                                                                has the ability to capture the long-range spatial informa-
               lizes the training towards a higher detection accuracy. 
                                                                                tion  and  channel  contextual  dependencies  among  en-
               3.3 Dual transformer for proposal refinement 
                                                                                coded  proposal-aware  keypoint  features  for  higher 
               3.3.1 Transformers for point cloud processing 
                                                                                bounding-box refinement.     
               Due to the inherent permutation  invariant,  transformer 
                                                                                   Specifically,  N  keypoints  within  proposals  are  ran-
               has  been  an  ideal  model  to  process  point  clouds. 
                                                                                domly  sampled  for  refinement. The  keypoint  features 
               Self-attention, as the core component in transformer en-
                                                                                                                                         [10]
                                                                                       Nd
                                                                                 F        are formed by aggregating 64-dimensional         
               coders, has the ability of capturing long-range interac-
                                                                                                                               pvsa
                                                          Nd
                                                                                embeddings  obtained  by  mapping  each  f         with  an 
               tions.  Given  a  point  cloud  scene          of  N  points 
                                                    X
                                                                                                                               i
                                                                                MLP. Then, the encoded keypoints features are fed into 
               and d dimensional features. According to the operating 
                                                                                two parallel branches of dual transformer. As shown in 
               space in point cloud tasks, transformers can be divided 
                                                                                Fig.2,  one  branch  is  the  point-wise  multi-head 
               into  point-wise  transformers  and  channel-wise  trans-
                                                                                               [10]
                                                                                cosh-attention      encoder-decoder  architecture,  which
               formers. The former measures the similarity among input 
                ·0550·                                                                                                                                             Optoelectron. Lett. Vol.21 No.9 
                encodes position information among point-wise features               Due to the linearized operation of cosh-attention, the 
                and decodes the all extracted features into a point-wise          constructed  point-wise  transformer  not  only  captures 
                global proposal representation. The other branch is the           spatial contextual information but also achieves satisfac-
                channel-wise multi-head transformer, which aggregates             tory inference speed. According to the left part of Fig.2, 
                local  and  detailed  channel-wise  contextual  correlations      the point-wise multi-head cosh-attention encoding mod-
                for generating a channel-wise proposal.                           ule  also  includes  layer  normalization  operation  and  a 
                                                                                  feedforward network (FFN) with two linear layers and 
                  In  order  to  capture  spatial  context-dependencies 
                                                                                  one  ReLU  activation  layer.  A  stack  of  3  identical 
                among encoded keypoints for a point-wise proposal rep-
                                                                                  multi-head cosh-self-attention encoding modules is used 
                resentation,  a  point-wise  transformer  is  constructed  as 
                                                                                  in the point-wise transformer. 
                illustrated  in  the  upper-right  part  of  Fig.2.  The 
                               [10]
                                                                                     In  the  point-wise  decoding  module,  cosh-cross- 
                cosh-attention     is  used to replace vanilla attention for 
                                                                                  attention  is  used  to  decode  all  the  point-wise  features. 
                lower  spatial  and  temporal  complexity.  Generally,  the 
                                                                                  Specifically, only one zero-initialized queryembedding z 
                point-wise  transformer  includes  a  cosh-self-attention 
                                                                                  is   used  to  calculate  with  key-value  embeddings 
                encoding  module  and  a  cosh-cross-attention  decoding 
                                                                                     '   '
                                                                                   K ,V
                                                                                            from encoding module for obtaining point-wise 
                                                                                     h   h
                module. 
                                                                                  global  proposal  representation.  Different  from  three 
                  The         formulated         point-wise        encoding 
                                                                                  identical  multi-head  cosh-self-attentions  stacked  in  en-
                cosh-self-attention for processing the input features F is 
                                                                                  coding module, just one multi-head cosh-cross-attention 
                     P
                   F  F F
                                
                           pe
                                                                                  is used in decoding module. As shown in the upper-right 
                                                                              (6) 
                                                                                                                  pd
                    Concat PEA ,PEA ,,PEA            F,
                                                                                  part  of  Fig.2,  the  output  z   of  point-wise  decoding 
                                                        
                                      1      2         H
                          P
                                                                                  cross-cosh-attention can be calculated as 
                where  F   denotes  the  output  feature  map  of  the 
                                                                                       pd             pe
                                                                                                                
                                                                                      z    z,F         z
                                                                                                     
                point-wise  encoding  cosh-self-attention,             is  the 
                                                                                              pd
                                                                   
                                                               pe
                                                                                                Concat PDA ,PDA ,,PDA         z,          (10) 
                                                                                                                             
                multi-head    cosh-self-attention    function,    Concat   
                                                                           
                                                                                                          1      2          H
                                                                                  where                   represents      the      point-wise 
                stands for a concatenation operation, and H is the num-
                                                                                                 pd
                ber of attention heads. Denote h as the index of the atten-
                                                                                  cosh-cross-attention  operation  in  decoding  module,  z 
                tion head, and the PEA  is defined as 
                                                                                                                                 pe
                                        h
                                                                                  denotes  the  zero-initialized  vector,  and     denotes  the 
                                                                                                                              F
                                    
                   PEA F s V 
                                         
                           
                                                                                  output  of  the  point-wise  encoding  module.  The  PDA  
                       h         h  h
                                                                                                                                               h
                                                                                  stands  for  the  h-head  point-wise  cosh-cross-attention, 
                                             
                                                                              (7) 
                                s Q ,K      V ,h1,2,,H,
                                           
                                    h    h    h
                                 
                                                                                  which is calculated by the similar formulation as Eq.(9). 
                                                                                          pd
                                                                                  Next, z  proceeds through FFN and layer normalizations 
                                              
                                          Nd
                                   
                where  Q ,K ,V                  are  obtained  by  using 
                           h   h    h
                                                                                  to obtain the final point-wise global proposal representa-
                learnable matrices and rectified linear units (ReLU) ac-
                                                                                  tion. 
                                                                  d
                                                                                     Besides, in order to investigate the contextual correla-
                                                               
                tivation  to  process  input  features.  And         ,  s  de-
                                                             d 
                                                                                  tions  among  feature  channels  to  obtain  a  high-quality 
                                                                  H
                                                                                  channel-wise  global  proposal  representation,  we  intro-
                notes the similarity between Q and K of the vanilla atten-
                                                                                                                        [11]
                                                                                  duce the channel-wise architecture       ,  which achieves a 
                tion. In the cosh-attention operation, the similarity func-
                                                                                  superior detection accuracy on the widely used KITTI 
                tion s(Q, K) is decomposable with re-weighting mecha-
                nism  replacing  the  traditional  softmax  to  achieve  lin-
                                                                                  dataset. Specifically, as shown in the bottom-right part of 
                earized complexity, as shown in   
                                                                                  Fig.2, the channel-wise transformer adopts self-attention 
                                                                                  encoding scheme, which shares almost the same archi-
                                                                
                                                          i  j
                                                              
                                         T
                                      
                                                                           (8) 
                    Q ,K       Q K         2cosh a             ,
                                                                                  tecture  as  the  original  softmax-based  transformer  en-
                             
                     hi    hj      hi  hj
                                                                
                   
                                                              
                                                            B
                                                              
                                                                
                                                                                  coder. 
                                                                                     In  the  decoding  module,  to  emphasize  the  chan-
                where a is a hyper-parameter, i, j=1,…,N denote the row 
                              
                                                                                  nel-wise local information aggregation, the channel-wise 
                       Q ,K
                of the          , respectively, and                    Thus, 
                                                      Bmax i, j .
                                                                  
                         h    h
                                                                                                
                                                                                  transformer uses  Eq.(11)  to  calculate  the  new  chan-
                PEA  can be shown as   
                    h
                                                                                  nel-wise re-weighting for decoding weight vector based 
                                           T
                                            
                                                    
                   PEA F {2Q K V 
                                                                                                                            
                                               
                       h            h    h   h
                                      
                                                                                  on all the channels of key embedding  K . 
                                                                                                                              h
                                                                                                     
                                                                                                       
                                                                                                      
                                                      T
                                                            
                                                                                                   K
                                                                                       c
                                                          
                                                                                                     h
                                                                 
                                cosh Q      cosh K       V 
                                                       
                                        h            h     h
                                                 
                                                                                      w                , h 1,,H,                            (11) 
                                                                                                       
                                                                                                      
                                                                                       h
                                                            
                                                            
                                                                                                      
                                                                                                      
                                                                                                    d
                                                                                                       
                                                                                                      
                                                                                                       
                                                     T
                                                           
                                                         
                                                                                                               
                                                                                             ce     cd     Nd
                                                                 
                                sinh Q      sinh K      V }/
                                        h          h    h
                                                
                                                                                      K F W                 ,                                       (12) 
                                                           
                                                                                        h           K
                                                           
                                                                                                      h
                                                                                           c
                                         T                        T
                                                                                         w
                                                                                  where       is  the  proposed channel-wise re-weighting for 
                                                               
                                                                         
                                {2Q K      cosh Q       cosh K                           h
                                                                   
                                    h   h            h           h
                                                             
                                                                                  decoding weight vector in Ref.[11], ρ(∙) refers to a linear 
                                                    T
                                                   
                                                                                  projection, which calculates d' number of decoding values
                                                                                 (9) 
                                sinh Q     sinh K      }.
                                                     
                                        h          h
                                               
                TONG et al.                                                                                                                                Optoelectron. Lett. Vol.21 No.9·0551· 
                to  generate  a  re-weighting  scalar,  σ(∙)  is  the  softmax 
                                                                                                                              
                                                                                                           z     T       T
                                                                                                                             
                                                                                                                            
                                                                                                       r Q K        K
                                                                        ce
                                                                                                                   
                                                                                                           h     h       h
                                                                                                                             
                                                                                                                              
                                                                                                                                    
                function calculating along the d' dimension, and F  is 
                                                                                                                            
                                                                                      CDA                                     V ,  (14) 
                                                                                                                             
                                                                                          h                                         h
                                                                                                                              
                                                                                                                            
                the feature map processed by the previous self-attention 
                                                                                                                   
                                                                                                                 d
                                                                                                                             
                                                                                                                              
                                                                                                                            
                                                                                                                            
                                                                                                                             
                                                                                                                              
                encoding module in channel-wise transformer. The above 
                                                                                                            
                                                                                        z    S     cd    1d
                   cd            cd     cd
                                                                                      Q  z W        ,                     (15) 
                W and the  W        ,W  below are learnable matrices in 
                                                                                       h          Q
                  K              Q     V
                                                                                                   h
                    h             h     h
                                                                                                               
                                                                                             ce    cd     Nd
                channel-wise decoding module. 
                                                                                     V F W                  ,                                       (16) 
                                                                                       h            V
                                                                                                     h
                  As  shown  in  Fig.2,  the  extended  channel-wise 
                                                                                           cd
                                                                                  where z  is the output of channel-wise decoding atten-
                re-weighting strategy is proposed to simultaneously fo-
                                                                                         S     1d
                                                                                  tion, z       is  the  output  of  a  zero-initialized  vector 
                cus  on  the  global  aggregation  and  channel-wise  local 
                                                                                  processed by a simple self-attention, r(∙) denotes a du-
                aggregation as 
                                                                                                                                      
                                                                                                                           1N       d N
                                                                                  plicating  operator,  which  makes                      ,  and 
                                                                                                                          
                    cd         S    ce    S
                                               
                   z    z ,F         z 
                                     
                           cd                                                      
                                                                                        represents  the  Hadamard  product  operation.  Then, 
                                                                                   cd
                                                               S
                                                                                  z  will be processed with an FFN and layer normaliza-
                         Concat CDA,CDA ,,CDA              z ,
                                                                          (13) 
                                                          
                                       1      2          H
                                                                                  tions to generate a channel-wise global proposal repre-
                where 
                                                                                  sentation. 
                                                                                      
                                                                                                                                           
                                  Fig.2 Dual transformer encoder-decoder architecture for the proposal refinement 
                 
                                                                                       o                         prop
                  Finally, in order to aggregate the spatial information 
                                                                                                                          
                                                                                                                                                (17) 
                                                                                     c min 1,max 0,2IoU             0.5 ,
                                                                                                                         
                                                                                                                          
                and channel contextual dependencies, the point-wise and 
                                                                                  where the parameters with superscripts o and prop de-
                channel-wise proposal representations are combined by 
                                                                                  note regression objectives and proposals, respectively. 
                element-wise  addition  to  generate  the  global  proposal 
                                                                                     For the box refinement branch, the box regression ob-
                representation. It is noteworthy that following the princi-
                                                                                                     [8,11,12]
                                                                                  jectives are set as       
                ple  of  position  embedding  added  to  the  vanilla  trans-
                       [9]
                                                                                              g    prop
                former , the position embedding is not used in the dual 
                                                                                      
                                                                                             x x
                                                                                         t
                                                                                        x 
                                                                                      
                transformer because the features already contain spatial 
                                                                                               d
                                                                                                 diag
                                                                                      
                position information. 
                                                                                      
                                                                                              g     prop
                                                                                             y y
                3.4 Detect head and training objectives 
                                                                                         t
                                                                                      
                                                                                       y 
                The global proposal representation is fed into two sepa-
                                                                                      
                                                                                                d
                                                                                                 diag
                                                                                      
                rate  FFNs for confidence prediction and bounding box 
                                                                                      
                                                                                              g    prop
                                                                                                                                                     (18)
                                                                                                        ,b l,w,h ,
                                                                                                                   
                                                                                      
                                                                                             z z
                                                                                         t
                refinement,  respectively.  For  the  former,  as  done  in 
                                                                                        z 
                                                                                      
                                                                                                 prop
                                                                                               h
                Refs.[11, 12, 25], the 3D intersection-over-union (IoU) 
                                                                                      
                                                                                                    g
                between  the  proposals  and  their  corresponding 
                                                                                      
                                                                                                      
                                                                                                  b
                                                                                        t
                                                                                      
                                                                                       b log
                                                                                                      
                ground-truth (GT) boxes is adopted as the training objec-
                                                                                                   prop
                                                                                                      
                                                                                                 b
                                                                         o
                                                                                      
                                                                                                      
                tives. Given the proposals, the confidence objective c  is 
                                                                                      
                                                                                         t    g    prop
                normalized within [0, 1], shown as   
                                                                                         
                                                                                      
                                                                                      
                ·0552·                                                                                                                                             Optoelectron. Lett. Vol.21 No.9 
                where  the  parameters  with  superscript  g  denotes  GT         and others are treated as negative proposals. At the in-
                boxes, and d     is the diagonal length of proposal base.         ference stage, top-100 proposals are selected for the final 
                             diag
                3.5 Training losses                                               prediction. 
                                                                                  4.3 Detection performance of the KITTI dataset 
                The proposed PV-DT3D is trained end-to-end against the 
                                                                                  The commonly used “car” category of KITTI dataset is 
                first-stage  proposal  generation  loss       and  the  sec-
                                                          RPN
                                                                        [8]
                                                                                  used  for  experiments.  Tab.1  shows  the  performance 
                                             
                ond-stage refinement loss           . As the SECOND  is 
                                               RCNN
                                                                                  comparison between the proposed PV-DT3D and other 
                utilized as the 3D backbone and RPN, we adopt the same 
                                                                                  state-of-the-art methods on the official KITTI test server. 
                region proposal loss      . 
                                       RPN
                                                                                  The average precision (AP) is used for all test results, 
                                                             
                  Besides,  the  proposal  refinement  loss         is  com-
                                                                                  where the 0.7 threshold and 40 recall positions are ap-
                                                               RCNN
                                                                                  plied.  The  best  results  are  bolded  and  the  second  best 
                                                                       
                posed  of  IoU-guided  confidence  prediction  loss           
                                                                         IoU
                                                                                  ones are underlined. 
                and  box  residual  regression              .  The  binary 
                                                      
                                                       reg
                                                                                  Tab.1  Performance  comparison  with  other  Li-
                cross-entropy loss is exploited for the predicted confi-
                                                                                  DAR-based approaches on the official KITTI test set 
                                                       
                dence c to calculate confidence loss           as   
                                                         IoU
                             o               o
                                                                                                                         Car AP 3D (%) 
                       c log c  1c log 1c .         (19) 
                                                      
                                              
                    IoU
                                                                                       Type          Method 
                                                                                                                  Easy    Mod.    Hard   Mean 
                  Moreover, the box regression loss            is the same as 
                                                        
                                                          reg
                                                                                                           [3]
                                                                                                   VoxelNet       77.82   64.17  57.51   66.50 
                anchor regression loss as 
                                                                                                            [8]
                                                                                                   SECOND         84.65   75.96  68.71   76.44 
                                                                   o
                                                                                    Single-stage 
                                                                
                                                                       (20) 
                     IoU                                 r ,r   ,
                                                                                                          [13]
                                      
                                                                    
                    reg              R               smoothL1
                                                                                                   3DSSD         88.36   79.57  74.55   80.83 
                                                                                                           [22]
                                        rx,y,z,l,w,h,
                                                                                                   PVT-SSD        90.65   82.29  76.85   83.26 
                                                                                                              [4]
                where  IoU            means     that   proposals     with 
                                     
                                    R                                                            Voxel R-CNN      90.90   81.62  77.06   83.19 
                                                                                                            [20]
                IoU 
                             are used to contribute to the regression loss, 
                         R
                                                                                                   VoTr-TSD       89.90   82.09  79.14   83.71 
                                                                                                             [15]
                and r' is the predicted box residual. 
                                                                                                  Focals Conv     90.20   82.12  77.50   83.27 
                                                                                                             [7]
                                                                                                  PointRCNN       86.96   75.64  70.70   77.77 
                4. Experiments 
                                                                                                          [14]
                                                                                                    SASA          88.76   82.16  77.16   82.69 
                                                                                                            [12]
                                                                                                   PV-RCNN        90.25   81.43  76.82   82.84 
                4.1 KITTI dataset 
                                    [23]                                                                    [28]
                                                                                     Two-stage 
                                                                                                  Pyramid-PV      88.39   82.08  77.49   82.65 
                The KITTI dataset        is  utilized  for subsequent experi-
                                                                                                          [29]
                                                                                                    BADet         89.28   81.61  76.58   82.49 
                ments, which includes 7 481 training samples and 7 518 
                                                                                                           [19]
                                                                                                   M3DETR         90.28   81.73  76.96   82.99 
                test  samples.  For  ablation  experiments,  as  done  in 
                                                                                                          [30]
                                                                                                    SIENet        88.22   81.71  77.22   82.38 
                Refs.[10,  12,  26],  the  labeled  training  samples  are  di-
                                                                                                         [16]
                                                                                                     PDV          90.43   81.86  77.32   83.20 
                vided into training set with 3 712 samples and validation 
                                                                                                          [11]
                                                                                                     CT3D         87.83   81.77  77.16   82.25 
                set with 3 769 samples. 
                                                                                                      Ours        90.07   82.09  77.51   83.22 
                4.2 Implementation of experiments 
                4.2.1 RPN                                                          
                                          [8]
                The  effective  SECOND   is  considered  as  the  default            According  to  these  detection  results,  the  proposed 
                voxel-based network and RPN to generate high quality              PV-DT3D  achieves  the  best  detection  accuracy  on  the 
                proposal.  All  the  hyperparameters  of  SEOCND follow           moderate level among two-stage methods. Calculating the 
                           [27]
                PV-RCNN  for convenient comparison. For more de-                  average accuracy across all difficulty levels, it is shown 
                                                               [27]
                tails, please refer to the OpenPCDet toolbox     .                that  PV-DT3D  outperforms  other  point-voxel-based  ap-
                4.2.2 Training and inference details                              proaches,  showcasing  its  superior  performance.  When 
                                                                                                                                [11,12,16]
                A single NVIDIA 1080Ti graph processing unit (GPU) is             comparing  PV-DT3D  with  other  methods             ,  all  of 
                used  for  end-to-end  training  of  the  PV-DT3D  for  100       which  utilize  SECOND  as  the  3D  backbone,  and  the 
                epochs with Adam optimizer. The batch size and initial            PV-DT3D  consistently  demonstrates  the  best  detection 
                learning rate are set to 2 and 0.001, respectively. Cosine        results. 
                annealing  strategy  is  utilized  to  update  learning  rate.       Besides,  detection  results  of  the  PV-DT3D  and 
                Firstly, 3 072 raw points are randomly sampled by FPS.            PV-RCNN on the KITTI test set are visualized in Fig.3. 
                Then in the dual transformer, 256 internal keypoints are          Compared  with  PV-RCNN,  the  proposed  PV-DT3D 
                randomly  selected  for  subsequent  processing.  If  the         gives fewer but more reasonable detection boxes, where 
                number of internal keypoints is less than 256, dummy              fake objects are avoided. The advantages are relevant to 
                points are padded to ensure 256 points for achieving par-         the  proposal-aware  strategy  and  dual  transformer  for 
                allel  running  of  the  dual  transformer.  The  foreground      confidence prediction and accurate box refinement. 
                threshold α  and background threshold α  are set to 0.75          4.4 Ablation studies 
                            F                              B
                and  0.25.  Besides,  for  refinement,  128  proposals  are       A series of ablation studies are conducted for verifying the 
                randomly sampled into positive and negative with 1: 1             effectiveness of the point-voxel fusion features, proposal- 
                ratio, where the proposals with 3D IoU≥0.55 (i.e., α ) are        aware VSA module, and the proposed dual transformer 
                                                                       R
                considered as positive samples for subsequent regression,         aggregating point-wise and channel-wise information. For 
                  TONG et al.                                                                                                                                Optoelectron. Lett. Vol.21 No.9·0553· 
                  the reliability, the average AP of the last 10 training ep-                   Tab.2 Ablation studies of point-voxel feature and P.T.P. 
                  ochs with 0.7 threshold and 40 recall positions are taken                     strategy on the KITTI validation set   
                  as the ablation results for the “car” category on KITTI 
                                                                                                                                       Car AP 3D (%) 
                  validation set.   
                                                                                                                   Feature 
                                                                                                                                    Easy    Mod.      Hard 
                      
                                                                                                                 Point+P.T.P.      92.70    83.93    82.96 
                                                                                                                 Point-voxel       89.18    72.84    67.91 
                                                                                                              Point-voxel+P.T.P.   92.86    85.39    83.19 
                                                                                                Tab.3  Ablation  studies  of  point-wise,  channel-wise 
                                                                                                transformer and dual transformer for refinement 
                                                                                                                                                   Car AP 3D (%) 
                                                                                                             Operating space 
                                                                                                                                             Easy       Mod.       Hard 
                                                                                                                               [11]
                                                                                                         Channel-wise (CT3D)                92.14       85.37      82.94 
                                                                                                                                 [10]
                                                                                                      Point-wise (Cosh-attention)           91.67       83.62      82.60 
                                                                                                            Dual transformer                92.86       85.39      83.19 
                                                                                                5. Conclusion 
                                                                                                In our future research, we are committed to enhancing 
                                                                                                the accuracy of small target detection. We are actively 
                                                                                                exploring  strategies,  including  point  cloud  completion 
                                                                                                and  making  modifications  to  the  transformer  architec-
                                                                                                ture. These initiatives aim to make transformer-based 3D 
                                                                                                object  detection  systems  more  effective  and  robust  in 
                                                                                                addressing small targets, ultimately improving safety and 
                                                                                                reliability in 3D object detection. 
                                                                                                Ethics declarations 
                                                                                                Conflicts of interest 
                                                                                        
                                                                                                The authors declare no conflict of interests. 
                  Fig.3 Visualization of the PV-DT3D and PV-RCNN de-
                  tection results on the KITTI test set 
                                                                                                References 
                                                                                                [1]       YU J H, GAO H W, ZHOU D L, et al. Deep temporal 
                     As shown in Tab.2, the results of 3D detection demon-
                                                                                                         model-based  identity-aware  hand  detection  for  space 
                  strate  the  effectiveness  of  point-voxel  features  and  the 
                                     [11]
                                                                                                         human-robot  interaction[J].  IEEE  transactions  on  cy-
                  P.T.P.  strategy      .  A  careful  analysis  reveals  that  the 
                                                                                                         bernetics, 2021, 52(12): 13738-13751. 
                  point-voxel  fusion  features  outperform  pure  point  fea-
                                                                                                [2]       YU J H, XU Y K, CHEN H, et al. Versatile graph neural 
                  tures on all levels. In the context of the dual transformer 
                                                                                                         networks  toward  intuitive  human  activity  understand-
                  used for refinement, the inclusion of the P.T.P. strategy 
                                                                                                         ing[J]. IEEE transactions on neural networks and learn-
                  within  the  VSA  module  proves  to  be  beneficial.  This 
                                                                                                         ing systems, 2022. 
                  strategy  effectively  enhances  the  local  correlations  be-
                                                                                                [3]       ZHOU Y, TUZEL O. Voxelnet: end-to-end learning for 
                  tween  proposals  and  points,  contributing  to  improved 
                                                                                                         point cloud based 3D object detection[C]//Proceedings 
                  stability  during  training.  Thus,  the  point-voxel+P.T.P. 
                                                                                                         of the IEEE Conference on Computer Vision and Pat-
                  strategy showcases clear advantages, and it demonstrates 
                                                                                                         tern  Recognition,  June  18-23,  2018,  Salt  Lake  City, 
                  superior detection accuracy across multiple levels of dif-
                                                                                                         USA. New York: IEEE, 2018: 4490-4499. 
                  ficulty. 
                                                                                                [4]       DENG J J, SHI S S, LI P W, et al. Voxel R-CNN: to-
                     We design ablation studies of point-wise, channel-wise 
                                                                                                         wards high performance voxel-based 3D object detec-
                  transformers  and  dual  transformer  to  refine  proposals, 
                                                                                                         tion[C]//Proceedings of the AAAI Conference on Arti-
                  respectively.  As  shown  in  Tab.3,  it  is  evident  that  the 
                                                                                                         ficial  Intelligence,  February  2-9,  2021,  Vancouver, 
                  dual  transformer  outperforms  both  the  channel-wise 
                                                                                                         Canada. Washington: AAAI, 2021, 35(2): 1201-1209. 
                  transformer  and  the  cosh-attention-based  point-wise 
                                                                                                [5]       QI C R, SU H, MO K C, et al. Pointnet: deep learning 
                  transformer  on  all  difficulty  levels.  So  the  dual  trans-
                                                                                                         on  point  sets  for  3D  classification  and  segmenta-
                  former leverages the strengths of each transformer type, 
                                                                                                         tion[C]//Proceedings of the IEEE Conference on Com-
                  resulting in improved detection accuracy across all diffi-
                                                                                                         puter Vision and Pattern Recognition, July 21-26, 2017, 
                  culty levels. 
                                                                                                         Honolulu, HI, USA. New York: IEEE, 2017: 652-660.
                  ·0554·                                                                                                                                             Optoelectron. Lett. Vol.21 No.9 
                  [6]       QI C R, YI L, SU H, et al. Pointnet++: deep hierarchical            [18]     GUO M H, CAI J X, LIU Z N, et al. PCT: point cloud 
                          feature learning on point sets in a metric space[J]. Ad-                      transformer[J]. Computational visual media, 2021, 7(2): 
                          vances in neural information processing systems, 2017.                        187-199. 
                  [7]       SHI  S,  WANG X G, LI H S. PointRCNN: 3D object                     [19]     GUAN  T  R,  WANG  J,  LAN  S  Y,  et  al.  M3DETR: 
                          proposal     generation     and    detection     from     point               multi-representation,  multi-scale,  mutual-relation  3D 
                          cloud[C]//Proceedings of the IEEE/CVF Conference on                           object  detection  with  transformers[C]//Proceedings  of 
                          Computer Vision and Pattern Recognition, June 16-20,                          the  IEEE/CVF Winter Conference on Applications of 
                          2019, Long Beach, CA, USA. New York: IEEE, 2019:                              Computer  Vision,  January  3-8,  2022,  Waikoloa,  HI, 
                          770-779.                                                                      USA. New York: IEEE, 2022. 
                  [8]       YAN Y, MAO Y X, LI B. SECOND: sparsely embedded                     [20]     MAO J G, XUE Y J, NIU M Z, et al. Voxel transformer 
                          convolutional detection[J]. Sensors, 2018, 18(10): 3337.                      for   3D  object  detection[C]//Proceedings  of  the 
                  [9]       VASWANI A, SHAZEER N, PARMAR N, et al. Atten-                               IEEE/CVF International Conference on Computer Vi-
                          tion is all you need[J]. Advances in neural information                       sion,  October  10-17,  2021,  Montreal,  Canada.  New 
                          processing systems, 2017, 30.                                                 York: IEEE, 2021: 3164-3173. 
                  [10]    TONG J G, YANG F H, YANG S, et al. Hyperbolic co-                     [21]     XIE E, ZHANG Z Y, ZHANG G D, et al. Light bottle 
                          sine    transformer  for  LiDAR  3D  object  detec-                           transformer  based  large  scale  point  cloud  classifica-
                          tion[EB/OL].             (2022-11-05)            [2023-9-18].                 tion[J]. Optoelectronics letters, 2023, 19(6): 377-384. 
                          https://arxiv.org/abs/2211.05580.                                     [22]     YANG  H  H,  WANG  W  X,  CHEN  M  H,  et  al. 
                  [11]     SHENG H L, CAI S J, LIU Y, et al. Improving 3D ob-                           PVT-SSD:  single-stage  3D  object  detector  with 
                          ject      detection       with       channel-wise        trans-               point-voxel      transformer[C]//Proceedings         of     the 
                          former[C]//Proceedings of the IEEE/CVF International                          IEEE/CVF Conference on Computer Vision and Pattern 
                          Conference on Computer Vision, October 10-17, 2021,                           Recognition,  June  18-22,  2023,  Vancouver,  Canada. 
                          Montreal, Canada. New York: IEEE, 2021: 2743-2752.                            New York: IEEE, 2023: 13476-13487.   
                  [12]     SHI  S  S,  GUO  C  X,  JIANG  L,  et  al.  PV-RCNN:                 [23]     GEIGER A, LENZ P, URTASUN R. Are we ready for 
                          point-voxel feature set abstraction for 3D object detec-                      autonomous  driving?  The  KITTI  vision  benchmark 
                          tion[C]//Proceedings of the IEEE/CVF Conference on                            suite[C]//2012 IEEE Conference on Computer Vision and 
                          Computer Vision and Pattern Recognition, June 13-19,                          Pattern  Recognition,  June  16-21,  2012,  Providence, 
                          2020,  Seattle,  WA,  USA.  New  York:  IEEE,  2020:                          Rhode Island, USA. New York: IEEE, 2012: 3354-3361. 
                          10529-10538.                                                          [24]    CARION  N,  MASSA  F,  SYNNAEVE  G,  et  al. 
                  [13]     YANG Z T, SUN Y N, LIU S, et al. 3DSSD: point-based                          End-to-end       object     detection     with     transform-
                          3D  single  stage  object  detector[C]//Proceedings  of  the                  ers[C]//European Conference on Computer Vision, Au-
                          IEEE/CVF Conference on Computer Vision and Pattern                            gust  23-28,  2020,  Cham,  Glasgow,  UK.  Heidelberg: 
                          Recognition, June 13-19, 2020, Seattle, WA, USA. New                          Springer, 2020: 213-229. 
                          York: IEEE, 2020: 11040-11048.                                        [25]     JIANG B R, LUO R X, MAO J Y, et al. Acquisition of 
                  [14]     CHEN C, CHEN Z, ZHANG J, et al. SASA: seman-                                 localization  confidence  for  accurate  object  detec-
                          tics-augmented set abstraction for point-based 3D ob-                         tion[C]//Proceedings  of  the  European  Conference  on 
                          ject detection[C]//Proceedings of the AAAI Conference                         Computer Vision (ECCV), September 8-14, 2018, Mu-
                          on Artificial Intelligence, February 22-March 1, 2022,                        nich, Germany. Heidelberg: Springer, 2018: 784-799.   
                          Vancouver,  Canada.  Washington:  AAAI,  2022,  36(1):                [26]     CHEN X Z, KUNDU K, ZHU Y K, et al. 3D object pro-
                          221-229.                                                                      posals for accurate object class detection[J]. Advances in 
                  [15]    CHEN Y K, LI Y W, ZHANG X Y, et al. Focal sparse                              neural information processing systems, 2015, 28. 
                          convolutional      networks      for   3D  object       detec-        [27]     OpenPCDET  development  team.  OpenPCDET:  an 
                          tion[C]//Proceedings of the IEEE/CVF Conference on                            opensource toolbox for 3D object detection from point 
                          Computer Vision and Pattern Recognition, June19-24,                           clouds[EB/OL].           (2020-01-01)           [2023-11-25]. 
                          2022, New Orleans, Louisiana, USA. New York: IEEE,                            https://github.com/openmmlab/OpenPCDet. 
                          2022: 5428-5437.                                                      [28]     MAO J G, NIU M Z, BAI H Y, et al. Pyramid R-CNN: 
                  [16]     HU  J  S  K,  KUAI  T,  WASLANDER S L. Point den-                            towards better performance and adaptability for 3D ob-
                          sity-aware     voxels    for    lidar   3D  object  detec-                    ject detection[C]//Proceedings of the IEEE/CVF Inter-
                          tion[C]//Proceedings of the IEEE/CVF Conference on                            national  Conference  on  Computer  Vision,  October 
                          Computer Vision and Pattern Recognition, June19-24,                           10-17, 2021, Montreal, Canada. New York: IEEE, 2021: 
                          2022, New Orleans, Louisiana, USA. New York: IEEE,                            2723-2732. 
                          2022: 8469-8478.                                                      [29]     QIAN R, LAI X, LI X R. BADet: boundary-aware 3D 
                  [17]     ZHAO  H  S,  JIANG  L,  JIA  J  Y,  et  al.  Point  trans-                   object detection from point clouds[J]. Pattern recogni-
                          former[C]//Proceedings of the IEEE/CVF International                          tion, 2022, 125: 108524. 
                          Conference on Computer Vision, October 10-17, 2021,                   [30]     LI Z Y, YAO Y C, QUAN Z B, et al. Spatial information 
                          Montreal,      Canada.      New  York:         IEEE,     2021:                enhancement  network  for  3D  object  detection  from 
                          16259-16268.                                                                  point cloud[J]. Pattern recognition, 2022, 128: 108684. 
