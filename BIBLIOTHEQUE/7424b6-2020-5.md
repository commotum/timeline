# Language Models are Few-Shot Learners (2020)
Source: 7424b6-2020.pdf

## Core reasons
- The paper's main claim is that scaling up language models improves task-agnostic few-shot performance without fine-tuning, focusing on training and evaluation rather than positional encoding or domain lifting.
- It centers on training and evaluating GPT-3 at 175 billion parameters, a large-scale model development and analysis contribution.

## Evidence extracts
- "Here we show that scaling up language models greatly improves task-agnostic,
few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-
tuning approaches." (p. 1)
- "Specifically, we train GPT-3, an autoregressive language model with 175 billion
parameters, 10x more than any previous non-sparse language model, and test its performance in
the few-shot setting." (p. 1)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
