                           picked up. We also point the reader to the appendix for a description and results of another RL task
                           called BoxWorld, which demands relational reasoning in memory space.
                           4.3  LanguageModeling
                           Finally, we investigate the task of word-based language modeling. We model the conditional prob-
                           ability p(wt|w<t) of a word wt given a sequence of observed words w<t = (wt−1,wt−2,...,w1).
                           Language models can be directly applied to predictive keyboard and search-phrase completion,
                           or they can be used as components within larger systems, e.g. machine translation [27], speech
                           recognition [28], and information retrieval [29]. RNNs, and most notably LSTMs, have proven to be
                           state-of-the-art on many competitive language modeling benchmarks such as Penn Treebank [30, 31],
                           WikiText-103 [32, 33], and the One Billion Word Benchmark [34, 35]. As a sequential reasoning
                           task, language modeling allows us to assess the RMC’s ability to process information over time on a
                           large quantity of natural data, and compare it to well-tuned models.
                           Wefocusondatasetswithcontiguoussentencesandamoderatelylargeamountofdata. WikiText-103
                           satisﬁes this set of requirements as it consists of Wikipedia articles shufﬂed at the article level with
                           roughly 100M training tokens, as do two stylistically different sources of text data: books from
                                            3
                           Project Gutenberg and news articles from GigaWord v5 [36]. Using the same processing from [32]
                           these datasets consist of 180M training tokens and 4B training tokens respectively, thus they cover
                           a range of styles and corpus sizes. We choose a similar vocabulary size for all three datasets of
                           approximately 250,000, which is large enough to include rare words and numeric values.
                           5   Results
                           5.1  Nth Farthest
                           This task revealed a stark difference between our LSTM and DNC baselines and RMC when training
                           on 16-dimensional vector inputs. Both LSTM and DNC models failing to surpass 30% best batch
                           accuracy and the RMC consistently achieving 91% at the end of training (see ﬁgure 5 in the appendix
                           for training curves). The RMC achieved similar performance when the difﬁculty of the task was
                           increased by using 32-dimensional vectors, placing a greater demand on high-ﬁdelity memory storage.
                           However, this performance was less robust with only a small number of seeds/model conﬁgurations
                           demonstrating this performance, in contrast to the 16-dimensional vector case where most model
                           conﬁgurations succeeded.
                           Anattention analysis revealed some notable features of the RMC’s internal functions. Figure 3 shows
                           attention weights in the RMC’s memory throughout a sequence: the ﬁrst row contains a sequence
                           where the reference vector m was observed last; in the second row it was observed ﬁrst; and in the
                           last row it was observed in the middle of the sequence. Before m is seen the model seems to shuttle
                           input information into one or two memory slots, as shown by the high attention weights from these
                           slots’ queries to the input key. After m is seen, most evident in row three of the ﬁgure, the model
                           tends to change its attention behaviour, with all the memory slots preferentially focusing attention
                           on those particular memories to which the m was written. Although this attention analysis provides
                           some useful insights, the conclusions we can make are limited since even after a single round of
                           attention the memory can become highly distributed, making any interpretations about information
                           compartmentalisation potentially inaccurate.
                           5.2  ProgramEvaluation
                           Program evaluation performance was assessed via the Learning to Execute tasks [25]. We evaluated
                           a number of baselines alongside the RMC including an LSTM [3, 37], DNC [5], and a bank of
                           LSTMsresemblingRecurrent Entity Networks [38] (EntNet) - the conﬁgurations for each of these is
                           described in the appendix. Best test batch accuracy results are shown in Table 1. The RMC performs
                           at least as well as all of the baselines on each task. It is marginally surpassed by a small fraction of
                           performance on the double memorization task, but both models effectively solve this task. Further,
                           the results of the RMC outperform all equivalent tasks from [25] which use teacher forcing even when
                           evaluating model performance. It’s worth noting that we observed better results when we trained in a
                              3Project Gutenberg. (n.d.). Retrieved January 2, 2018, from www.gutenberg.org
                                                                          6
