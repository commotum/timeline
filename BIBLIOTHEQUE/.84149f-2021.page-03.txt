              !           "
                                                                                                                                   !
                                                                          #
              Figure 2: Uniform frame sampling: We simply sample nt frames,                                                  "
              and embed each 2D frame independently following ViT [17].                             #
              changes. In particular, ViT extracts N non-overlapping im-           Figure 3: Tubelet embedding. We extract and linearly embed non-
              age patches, xi ∈ Rh×w, performs a linear projection and             overlapping tubelets that span the spatio-temporal input volume.
                                                            d
              then rasterises them into 1D tokens z ∈ R . The sequence
                                                      i                            as ViT [17], and concatenate all these tokens together. Con-
              of tokens input to the following transformer encoder is
                                                                                   cretely, if nh · nw non-overlapping image patches are ex-
                          z = [z    , Ex ,Ex ,...,Ex ]+p,                 (1)      tracted from eachframe, asin[17], thenatotalofnt·nh·nw
                                 cls    1     2          N                         tokens will be forwarded through the transformer encoder.
              wheretheprojectionbyEisequivalenttoa2Dconvolution.                   Intuitively, this process may be seen as simply constructing
              AsshowninFig.1,anoptional learned classiﬁcation token                a large 2D image to be tokenised following ViT. We note
              z    is prepended to this sequence, and its representation at        that this is the input embedding method employed by the
               cls
              the ﬁnal layer of the encoder serves as the ﬁnal represen-           concurrent work of [4].
              tation used by the classiﬁcation layer [16]. In addition, a          Tubelet embedding        An alternate method, as shown in
              learned positional embedding, p ∈ RN×d, is added to the              Fig. 3, is to extract non-overlapping, spatio-temporal
              tokens to retain positional information, as the subsequent           “tubes”fromtheinputvolume,andtolinearlyprojectthisto
              self-attention operations in the transformer are permutation         Rd. This method is an extension of ViT’s embedding to 3D,
              invariant. The tokens are then passed through an encoder             and corresponds to a 3D convolution. For a tubelet of di-
              consistingofasequenceofLtransformerlayers. Eachlayer                 mension t×h×w,n =bTc,n =bHcandn =bWc,
              ` comprises of Multi-Headed Self-Attention [67], layer nor-                                  t     t     h     h         w      w
              malisation (LN) [2], and MLP blocks as follows:                      tokens are extracted from the temporal, height, and width
                                                                                   dimensions respectively. Smaller tubelet dimensions thus
                                  `                `       `                       result in more tokens which increases the computation.
                                y =MSA(LN(z ))+z                          (2)      Intuitively, this method fuses spatio-temporal information
                               `+1                 `       `
                              z     =MLP(LN(y ))+y .                      (3)      during tokenisation, in contrast to “Uniform frame sam-
              The MLP consists of two linear projections separated by a            pling” where temporal information from different frames is
              GELU non-linearity [27] and the token-dimensionality, d,             fused by the transformer.
              remains ﬁxed throughout all layers. Finally, a linear classi-        3.3. Transformer Models for Video
              ﬁerisusedtoclassifytheencodedinputbasedonzL ∈ Rd,
                                                                   cls                AsillustratedinFig.1,weproposemultipletransformer-
              if it was prepended to the input, or a global average pooling        based architectures. We begin with a straightforward ex-
                                  L
              of all the tokens, z , otherwise.                                    tension of ViT [17] that models pairwise interactions be-
                 As the transformer [67], which forms the basis of                 tween all spatio-temporal tokens, and then develop more
              ViT [17], is a ﬂexible architecture that can operate on any          efﬁcient variants which factorise the spatial and temporal
              sequence of input tokens z ∈ RN×d, we describe strategies            dimensions of the input video at various levels of the trans-
              for tokenising videos next.                                          former architecture.
              3.2. Embedding video clips                                           Model 1: Spatio-temporal attention           This model sim-
                 We consider two simple methods for mapping a video                ply forwards all spatio-temporal tokens extracted from the
                          T×H×W×C                                                           0
              V ∈ R                     to a sequence of tokens ˜z         ∈       video, z , through the transformer encoder. We note that [4]
                n ×n ×n ×d
              R t    h   w    . We then add the positional embedding and           also explored this concurrently in their “Joint Space-Time”
              reshape into RN×d to obtain z, the input to the transformer.         model. In contrast to CNN architectures, where the recep-
                                                                                   tive ﬁeld grows linearly with the number of layers, each
              Uniform frame sampling As illustrated in Fig. 2, a                   transformer layer models all pairwise interactions between
              straightforward method of tokenising the input video is to           all spatio-temporal tokens, and it thus models long-range in-
              uniformly sample nt frames from the input video clip, em-            teractions across the video from the ﬁrst layer. However, as
              bed each 2D frame independently using the same method                it models all pairwise interactions, Multi-Headed Self At-
                                                                                6838
