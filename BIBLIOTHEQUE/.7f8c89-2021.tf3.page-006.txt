               192                                                                                   M.-H. Guo, J.-X. Cai, Z.-N. Liu, et al.
               linear transformation, and d is the dimension of the            is modiﬁed to
                                                a
               query and key vectors. Note that d may not be
                                                           a                        F     =OA(F )=LBR(F −F )+F                          (7)
               equal to d . In this work, we set d to be d /4 for                     out           in            in     sa      in
                           e                             a          e          F −F isanalogoustoadiscreteLaplacianoperator,
               computational eﬃciency.                                           in     sa
                  First, we can use the query and key matrices to              as we now show. First, from Eqs. (2) and (5), the
               calculate the attention weights via the matrix dot-             following holds:
               product:                                                                    Fin −Fsa = Fin −AV
                                                                                                       =F −AF W
                                  ˜                     T                                                  in        in   v
                                 A=(α˜)       =Q·K                      (3)
                                           i,j                                                         ≈F −AF
               These weights are then normalized (denoted SS in                                            in        in
                                                                                                       =(I−A)F ≈LF                      (8)
               Fig. 3) to give A = (α)       :                                                                       in       in
                                          i,j                                  Here, W is ignored since it is a weight matrix of
                                  α˜                                                      v
                                   i,j
                          α¯i,j = √                                            the Linear layer. I is an identity matrix comparable
                                    da                                  (4)
                                                      exp(α¯i,j)               to the diagonal degree matrix D of the Laplacian
                          α =softmax(α¯ )= P
                            i,j              i,j        exp(α¯  )
                                                              i,k              matrix and A is the attention matrix comparable to
                                                      k                        the adjacency matrix E.
               The self-attention output features Fsa are the                     In our enhanced version of PCT, we also reﬁne the
               weighted sums of the value vector using the                     normalization by modifying Eq. (4) as follows:
               corresponding attention weights:
                                                                                                                      exp(α˜i,j)
                                                                                          α¯   =softmax(α˜ ) = P
                                                                                            i,j              i,j        exp(α˜  )
                                      Fsa = A·V                         (5)                                                   k,j
                                                                                                   α¯                 k                 (9)
                  As the query, key, and value matrices are                               α =Pi,j
                                                                                            i,j      α¯
               determined by the shared corresponding linear                                          i,k
                                                                                                   k
               transformation matrices and the input feature Fin,              Here, weusethesoftmaxoperatoronthefirstdimension
               they are all order independent. Moreover, softmax               and an l -norm for the second dimension to normalize
                                                                                         1
               and weighted sum are both permutation-independent               the attention map. The traditional Transformer scales
                                                                                                                √
               operators. Therefore, the whole self-attention process          the ﬁrst dimension by 1/ d             and uses softmax
                                                                                                                   a
               is permutation-invariant, making it well-suited to              to normalize the second dimension. However, our
               the disordered, irregular domain presented by point             oﬀset-attention sharpens the attention weights and
               clouds.                                                         reduces the inﬂuence of noise, which is beneﬁcial for
                  Finally, the self-attention feature F     and the input      downstream tasks. Figure 1 shows example oﬀset
                                                         sa                    attention maps. It can be seen that the attention
               feature Fin, are further used to provide the output
               feature F      for the whole SA layer through an LBR            maps for diﬀerent query points vary considerably, but
                          out                                                  are generally semantically meaningful. We refer to
               network:
                         F     =SA(F )=LBR(F )+F                        (6)    this reﬁned PCT, i.e., with point embedding and OA
                           out          in             sa       in             layer, as simple PCT (SPCT) in the experiments.
               3.3    Oﬀset-Attention                                          3.4    Neighbor embedding for augmented local
               Graph convolution networks [9] show the beneﬁts                        feature representation
               of using a Laplacian matrix L = D −E to replace                 PCT with point embedding is an eﬀective network
               the adjacency matrix E, where D is the diagonal                 for extracting global features. However, it ignores the
               degree matrix.        Similarly, we ﬁnd that we can             local neighborhood information which is also essential
               obtain better network performance if, when applying             in point cloud learning. We draw upon the ideas of
               Transformer to point clouds, we replace the original            PointNet++ [21] and DGCNN [26] to design a local
               self-attention (SA) module with an oﬀset-attention              neighbor aggregation strategy, neighbor embedding,
               (OA) module to enhance our PCT. As shown in                     to optimize the point embedding to augment PCT’s
               Fig. 3, the oﬀset-attention layer calculates the oﬀset          ability of local feature extraction.         As shown in
               (diﬀerence) between the self-attention (SA) features            Fig. 4, neighbor embedding module comprises two
               and the input features by element-wise subtraction.             LBR layers and two SG (sampling and grouping)
               This oﬀset feeds the LBR network in place of the SA             layers.    The LBR layers act as the basis point
               feature used in the naive version. Speciﬁcally, Eq. (5)         embedding in Section 3.2. We use two cascaded SG
