                                                                                                                      DeepResidualLearningforImageRecognition
                                                                                                         Kaiming He                                                       Xiangyu Zhang                                                                 Shaoqing Ren                                                            Jian Sun
                                                                                                                                                                                               Microsoft Research
                                                                                                                                                      {kahe, v-xiangz, v-shren, jiansun}@microsoft.com
                                                                                                                Abstract                                                                                                                          20                                                                                            20                                                                               
                                                                                                                                                                                                                                                 )
                                                                                                                                                                                                                                                 %
                                                                                                                                                                                                                                                  (                                                                                                                                                            56-layer
                                                                                                                                                                                                                                                 r
                                                Deeper neural networks are more difﬁcult to train. We                                                                                                                                            o
                                                                                                                                                                                                                                                 r                                                                                            r (%)
                                       present a residual learning framework to ease the training                                                                                                                                                 10                                                                                          rro10                                                            20-layer
                                                                                                                                                                                                                                                 ng er                                                             56-layer                    e
                                                                                                                                                                                                                                                 ni                                                                                           t
                                       of networks that are substantially deeper than those used                                                                                                                                                 ai                                                                                           tes
                                                                                                                                                                                                                                                 r
                                                                                                                                                                                                                                                 t
                                       previously. We explicitly reformulate the layers as learn-                                                                                                                                                                                                                  20-layer
                                                                                                                                                                                                                                                   0                                                                                             0 
                                                                                                                                                                                                                                                     0           1           2           3           4            5           6                   0           1           2           3            4           5           6
                                       ing residual functions with reference to the layer inputs, in-                                                                                                                                                                             iter. (1e4)                                                                                  iter. (1e4)
                                       stead of learning unreferenced functions. We provide com-                                                                                                                                             Figure 1. Training error (left) and test error (right) on CIFAR-10
                                       prehensive empirical evidence showing that these residual                                                                                                                                             with 20-layer and 56-layer “plain” networks. The deeper network
                                       networksareeasiertooptimize,andcangainaccuracyfrom                                                                                                                                                    has higher training error, and thus test error. Similar phenomena
                                       considerably increased depth. On the ImageNet dataset we                                                                                                                                              onImageNetispresented in Fig. 4.
                                       evaluate residual nets with a depth of up to 152 layers—8×                                                                                                                                            greatly beneﬁted from very deep models.
                                       deeper than VGG nets [41] but still having lower complex-
                                       ity. An ensemble of these residual nets achieves 3.57% error                                                                                                                                                   Driven by the signiﬁcance of depth, a question arises: Is
                                       ontheImageNettestset. Thisresultwonthe1stplaceonthe                                                                                                                                                   learning better networks as easy as stacking more layers?
                                       ILSVRC 2015 classiﬁcation task. We also present analysis                                                                                                                                              An obstacle to answering this question was the notorious
                                       onCIFAR-10with100and1000layers.                                                                                                                                                                       problem of vanishing/exploding gradients [1, 9], which
                                                The depth of representations is of central importance                                                                                                                                        hamper convergence from the beginning. This problem,
                                       for many visual recognition tasks. Solely due to our ex-                                                                                                                                              however, has been largely addressed by normalized initial-
                                       tremely deep representations, we obtain a 28% relative im-                                                                                                                                            ization [23, 9, 37, 13] and intermediate normalization layers
                                       provement on the COCO object detection dataset. Deep                                                                                                                                                  [16], which enable networks with tens of layers to start con-
                                       residual nets are foundations of our submissions to ILSVRC                                                                                                                                            verging for stochastic gradient descent (SGD) with back-
                                                                                                                                1                                                                                                            propagation [22].
                                       &COCO2015 competitions , where we also won the 1st
                                       places on the tasks of ImageNet detection, ImageNet local-                                                                                                                                                     When deeper networks are able to start converging, a
                                       ization, COCO detection, and COCO segmentation.                                                                                                                                                       degradation problem has been exposed: with the network
                                                                                                                                                                                                                                             depth increasing, accuracy gets saturated (which might be
                                       1. Introduction                                                                                                                                                                                       unsurprising) and then degrades rapidly.                                                                                                 Unexpectedly,
                         arXiv:1512.03385v1  [cs.CV]  10 Dec 2015                                                                                                                                                                            such degradation is not caused by overﬁtting, and adding
                                                Deep convolutional neural networks [22, 21] have led                                                                                                                                         more layers to a suitably deep model leads to higher train-
                                       to a series of breakthroughs for image classiﬁcation [21,                                                                                                                                             ing error, as reported in [11, 42] and thoroughly veriﬁed by
                                       50, 40]. Deep networks naturally integrate low/mid/high-                                                                                                                                              our experiments. Fig. 1 shows a typical example.
                                       level features [50] and classiﬁers in an end-to-end multi-                                                                                                                                                     Thedegradation (of training accuracy) indicates that not
                                       layer fashion, and the “levels” of features can be enriched                                                                                                                                           all systems are similarly easy to optimize. Let us consider a
                                       by the number of stacked layers (depth). Recent evidence                                                                                                                                              shallower architecture and its deeper counterpart that adds
                                       [41, 44] reveals that network depth is of crucial importance,                                                                                                                                         more layers onto it. There exists a solution by construction
                                       and the leading results [41, 44, 13, 16] on the challenging                                                                                                                                           to the deeper model: the added layers are identity mapping,
                                       ImageNet dataset [36] all exploit “very deep” [41] models,                                                                                                                                            and the other layers are copied from the learned shallower
                                       with a depth of sixteen [41] to thirty [16]. Many other non-                                                                                                                                          model. The existence of this constructed solution indicates
                                       trivial visual recognition tasks [8, 12, 7, 32, 27] have also                                                                                                                                         that a deeper model should produce no higher training error
                                               1http://image-net.org/challenges/LSVRC/2015/                                                                                                                        and                       than its shallower counterpart. But experiments show that
                                       http://mscoco.org/dataset/#detections-challenge2015.                                                                                                                                                  our current solvers on hand are unable to ﬁnd solutions that
                                                                                                                                                                                                                                  1
