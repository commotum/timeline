# Length Generalization of Causal Transformers without Position Encoding (2024)
Source: a5942e-2024.pdf

## Core reasons
- Studies length generalization behavior of causal Transformers without explicit positional encoding (NoPE), focusing on model behavior rather than a new positional encoding or dataset resource.
- Proposes attention temperature/softmax scaling as a parameter-efficient method to extend context length, which is a training/optimization contribution.

## Evidence extracts
- "WestudiedthelengthgeneralizationofCasual
Transformerwithoutexplicitpositionencoding." (Section 6 Discussion)
- "(Left,NoPE;Right,RoPE).NoPEcangeneralizetolongercontextbymerelyscalingthesoftmaxscores.However," (Figure 2)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
