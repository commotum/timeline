                    Training data-efﬁcient image transformers & distillation through attention
                                                   HugoTouvron12 MatthieuCord12 MatthijsDouze1
                                                                   1                              1       ´   ´    1
                                               Francisco Massa        Alexandre Sablayrolles        HerveJegou
                                          Abstract                                                                  ⚗↑
                     Recently, neural networks purely based on atten-                                                           ⚗
                     tion were shown to address image understanding
                     tasks such as image classiﬁcation. These high-
                     performing vision transformers are pre-trained                                                                        ⚗
                     with hundreds of millions of images using a large
                     infrastructure, thereby limiting their adoption.
                     In this work, we produce competitive convolution-                                    ⚗
                     free transformers trained on ImageNet only us-
                     ing a single computer in less than 3 days. Our
                     reference vision transformer (86M parameters)
                     achieves top-1 accuracy of 83.1% (single-crop)
                     onImageNetwithnoexternal data.
                     Wealsointroduce a teacher-student strategy spe-
                     ciﬁc to transformers. It relies on a distillation
                     token ensuring that the student learns from the
                     teacher through attention, typically from a con-                Figure 1. Throughput and accuracy on Imagenet of our method
                     vnet teacher. The learned transformers are com-                 (no external training data). The throughput is measured as the
                     petitive (85.2% top-1 acc.) with the state of the art           numberofimagesprocessed per second on a V100 GPU. DeiT-B
                     on ImageNet, and similarly when transferred to                  is identical to ViT-B, but with training adapted to a data-starving
                     other tasks. We will share our code and models.                 regime. It is learned in a few days on one machine. The symbol ⚗
                                                                                     refers to models trained with our transformer-speciﬁc distillation.
                                                                                     See Table 5 for details and more models.
                1. Introduction                                                      to image classiﬁcation with raw image patches as input.
                Convolutional neural networks have been the main design              Their paper presented excellent results with transformers
                paradigm for image understanding tasks, as initially demon-          trained with a large private labelled image dataset contain-
                strated on image classiﬁcation tasks. One of the ingredient          ing 300 millions images. The paper concluded that vision
                to their success was the availability of a large training set,       transformers “do not generalize well when trained on in-
                namely Imagenet. Motivated by the success of attention-              sufﬁcient amounts of data”. The training of these models
                based models in Natural Language Processing, there has               involved extensive computing resources.
                been an increasing interest in architectures leveraging atten-       In our paper, we train a vision transformer on a single 8-
                tion mechanisms within convnets. More recently several               GPUnodein two to three days (53 hours of pre-training,
                researchers have proposed hybrid architecture transplanting          and optionally 20 hours of ﬁne-tuning) that is competitive
                transformer ingredients to convnets to solve vision tasks.           with convnets having a similar number of parameters and
                The vision transformer (ViT) introduced by Dosovitskiy               efﬁciency. It uses Imagenet as the sole training set. We
                et al. (2020) is an architecture directly inherited from Natu-       build upon the visual transformer architecture from Doso-
                ral Language Processing (Vaswani et al., 2017), but applied          vitskiy et al. (2020) and improvements included in the timm
                                                                                     library (Wightman, 2019). With our Data-efﬁcient image
                   1FacebookAI2SorbonneUniversity. Correspondence to: Hugo
                Touvron <htouvron@fb.com>.                                           Transformers (DeiT), we report large improvements over
                                                                                     previous results, see Figure 1. Our ablation study details
                Proceedings of the 38th International Conference on Machine          the hyper-parameters and key ingredients for a successful
                Learning, PMLR 139, 2021. Copyright 2021 by the author(s).           training, such as repeated augmentation.
                                         Training data-efﬁcient image transformers & distillation through attention
               Weaddressanother question: how to distill these models?          without using any convolution. This performance is remark-
               Weintroduce a token-based strategy, DeiT , that advanta-         able since convnet methods for image classiﬁcation have
                                                            ⚗
               geously replaces the usual distillation for transformers.        beneﬁted from years of tuning and optimization (He et al.,
               In summary, our work makes the following contributions:          2019; Wightman, 2019). Nevertheless, according to Doso-
                                                                                vitskiy et al. (2020), a pre-training phase on a large volume
               • We show that our neural networks that contain no con-          of curated data is required for the learned transformer to be
                 volutional layer can achieve competitive results against       effective. In our paper we achieve a strong performance with
                 the state of the art on ImageNet with no external data.        ImageNet-1k and report decent results even on CIFAR-10.
                 Theyare learned on a single node with 4 GPUs in three
                      1
                 days . Our two new models DeiT-S and DeiT-Ti have              TheTransformer architecture,         introduced by Vaswani
                 fewer parameters and can be seen as the counterpart of         et al. (Vaswani et al., 2017) for machine translation is cur-
                 ResNet-50 and ResNet-18.                                       rently the reference model for all natural language process-
               • Weintroduce a new distillation procedure based on a dis-       ing (NLP) tasks. Many improvements of convnets for image
                 tillation token, which plays the same role as the class to-    classiﬁcation are inspired by transformers. For example,
                                                                                Squeeze and Excitation (
                 ken, except that it aims at reproducing the label estimated                               Hu et al., 2017), Selective Ker-
                 by the teacher. Both tokens interact in the transformer        nel (Li et al., 2019b), Split-Attention Networks (Zhang et al.,
                 through attention. This transformer-speciﬁc strategy out-      2020)andStand-AloneSelf-Attention(Ramachandranetal.,
                 performs vanilla distillation by a signiﬁcant margin.          2019) exploit mechanism akin to transformers self-attention
               • Our models pre-learned on Imagenet are competitive             (SA) mechanism. Moreover, Cordonnier et al. (Cordonnier
                 when transferred to different downstream tasks such            et al., 2020) study the link between SA and convolution.
                 as ﬁne-grained classiﬁcation, on several popular public        KnowledgeDistillation       (Hinton et al., 2015) refers to the
                 benchmarks: CIFAR-10, CIFAR-100, Oxford-102 ﬂow-
                 ers, Stanford Cars and iNaturalist-18/19.                      training paradigm in which a student model leverages “soft”
                                                                                labels coming from a strong teacher network. This is the
                                                                                output vector of the teacher’s softmax function rather than
               2. Related work                                                  just the maximum of scores, wich gives a “hard” label. Such
               Image Classiﬁcation      is so core to computer vision that      a training improves the performance of the student model
               it is often used as a benchmark to measure progress in           (alternatively, it can be regarded as a form of compression of
               image understanding. Any progress usually translates to          the teacher model into a smaller one – the student). On the
               improvement in other related tasks such as detection or          one hand the teacher’s soft labels will have a similar effect
               segmentation. Since 2012’s AlexNet (Krizhevsky et al.,           to labels smoothing (Yuan et al., 2020). On the other hand as
               2012), convnets have dominated this benchmark and have           shownbyWeietal.(2020)theteacher’s supervision takes
               becomethedefactostandard. The evolution of the state of          into account the effects of the data augmentation, which
               the art on the ImageNet dataset (Russakovsky et al., 2015)       sometimes causes a misalignment between the real label
               reﬂects the progress with convolutional architectures and        and the image. For example, let us consider image with a
               optimization methods (Simonyan & Zisserman, 2015; Tan            “cat” label that represents a large landscape and a small cat
               &Le,2019;Touvronetal., 2019).                                    in a corner. If the cat is no longer on the crop of the data
                                                                                augmentation it implicitly changes the label of the image.
               Despite several attempts to use transformers for image clas-     Knowledgedistillation can transfer inductive biases (Abnar
               siﬁcation (Chen et al., 2020a), until now their performance      et al., 2020) in a soft way in a student model using a teacher
               has been inferior to that of convnets. Nevertheless hybrid       modelwheretheywouldbeincorporated in a hard way. In
               architectures that combine convnets and transformers, in-        our paper we study the distillation of a transformer student
               cluding the self-attention mechanism, have exhibited com-        byeither a convnet or a transformer teacher, motivated by
               petitive results in image classiﬁcation (Bello et al., 2019;     inducing convolutional bias into transformers.
               Bello, 2021; Wu et al., 2020), detection (Carion et al., 2020;
               Huetal., 2018), video processing (Sun et al., 2019; Wang         3. Vision transformer: overview
               et al., 2018), unsupervised object discovery (Locatello et al.,
               2020), and text-vision tasks (Chen et al., 2020b; Li et al.,     In this section, we brieﬂy recall preliminaries associated
               2019a; Lu et al., 2019).                                         with the vision transformer (Dosovitskiy et al., 2020;
               Recently Vision transformers (ViT) (Dosovitskiy et al.,          Vaswani et al., 2017), denoted by ViT. We further discuss
               2020) closed the gap with the state of the art on ImageNet,      positional encoding and resolution.
                  1Wecanaccelerate the learning of the larger model DeiT-B by   Multi-head Self Attention layers (MSA).        Theattention
               training it on 8 GPUs in two days.                               mechanism is based on a trainable associative memory with
                                        Training data-efﬁcient image transformers & distillation through attention
              (key, value) vector pairs. A query vector q ∈ Rd is matched     NLP (Devlin et al., 2018), and departs from the typical
               against a set of k key vectors (packed together into a matrix  pooling layers used in computer vision to predict the class.
               K ∈ Rk×d) using inner products. These inner products           The transformer thus process batches of (N + 1) tokens
               are then scaled and normalized with a softmax function to      of dimension D, of which only the class vector is used to
               obtain k weights. The output of the attention is the weighted  predict the output. This architecture forces the self-attention
               sumofasetof                                      k×d           to spread information between the patch tokens and the class
                             kvaluevectors(packedintoV ∈ R          ). For
               a sequence of N query vectors (packed into Q ∈ RN×d), it       token: at training time the supervision signal comes only
               produces an output matrix (of size N × d):                     from the class embedding, while the patch tokens are the
                                                            √                 model’s only variable input.
                  Attention(Q,K,V) = Softmax(QK⊤/ d)V,                 (1)
              where the Softmax function is applied on each row of the        Fixing the positional encoding across resolutions.     Tou-
               input matrix. The √d term provides proper normalization.       vron et al. (2019) show that it is desirable to use a lower
              Vaswani et al. (2017) propose a self-attention layer. Query,    training resolution and ﬁne-tune the network at the larger
               key and values matrices are themselves computed from a         resolution. This speeds up the full training and improves
               sequence of N input vectors (packed into X ∈ RN×D):            the accuracy under prevailing data augmentation schemes.
               Q=XW ,K=XW ,V =XW ,usinglineartransfor-                        Whenincreasing the resolution of an input image, we keep
                         Q            K            V                          the patch size the same, therefore the number N of input
               mations WQ,WK,WV withtheconstraintk = N,meaning                patches does change. Due to the architecture of transformer
               that the attention is in between all the input vectors.        blocks and the class token, the model and classiﬁer do not
               Finally, Multi-head self-attention layer (MSA) is deﬁned by    needtobemodiﬁedtoprocessmoretokens. Incontrast, one
               consideringhattention“heads”, iehself-attentionfunctions       needs to adapt the positional embeddings, because there are
               applied to the input. Each head provides a sequence of size    N of them, one for each patch. Dosovitskiy et al. (2020)
               N×d. Thesehsequences are rearranged into a N × dh              interpolate the positional encoding when changing the res-
               sequence that is reprojected by a linear layer into N × D.     olution and demonstrate that this method works with the
                                                                              subsequent ﬁne-tuning stage.
               Transformerblockforimages.         Togetafull transformer
               block as in (Vaswani et al., 2017), we add a Feed-Forward
               Network (FFN) on top of the MSA layer. This FFN is             4. Distillation through attention
               composed of two linear layers separated by a GeLu acti-        In this section, we assume we have access to a strong image
              vation (Hendrycks & Gimpel, 2016). The ﬁrst linear layer        classiﬁer as a teacher model. It could be a convnet, or a
               expands the dimension from D to 4D, and the second layer       mixture of classiﬁers. We address the question of how to
               reduces it back from 4D back to D. Both MSA and FFN are        learn a transformer by exploiting this teacher. As we will
               operating as residual operators thank to skip-connections,     see in Section 5 by comparing the trade-off between accu-
               and with a layer normalization (Ba et al., 2016).              racy and image throughput, it can be beneﬁcial to replace a
               In order to get a transformer to process images, our work      convolutional neural network by a transformer. This section
               builds upon the ViT model (Dosovitskiy et al., 2020). It       covers two axes of distillation: hard versus soft distillation,
               is a simple and elegant architecture that processes an input   and classical distillation vs distillation token.
               imageasifit was a sequence of input tokens. The ﬁxed-size
               input RGB image is decomposed into a batch of N patches        Soft distillation   (Hinton et al., 2015; Wei et al., 2020)
               of a ﬁxed size of 16×16 pixels (N = 14×14). Each patch         minimizes the Kullback-Leibler divergence between the
               is projected with a linear layer that conserves its overall    softmax of the teacher and the softmax of the student model.
               dimension 3×16×16 = 768.                                       Let Zt be the logits of the teacher model, Zs the logits of the
              Thetransformer block described above is invariant to the or-    student model. Wedenotebyτ thetemperatureforthedistil-
               derofthepatchembeddings,andthusignorestheirpositions.          lation, λ the coefﬁcient balancing the Kullback–Leibler di-
              Thepositionalinformationisincorporatedasﬁxed(Vaswani            vergence loss (KL) and the cross-entropy (L     ) on ground
               et al., 2017) or trainable (Gehring et al., 2017) positional                                                CE
               embeddings. They are added before the ﬁrst transformer         truth labels y, and ψ the softmax function. The distillation
               block to the patch tokens, which are then fed to the stack of  objective is
               transformer blocks.                                                      L       =(1−λ)L (ψ(Z ),y)
                                                                                          global            CE       s
                                                                                                +λτ2KL(ψ(Z /τ),ψ(Z /τ)).               (2)
               The class token    is a trainable vector, appended to the                                       s          t
               patch tokens before the ﬁrst layer, that goes through the
               transformer layers, and is then projected with a linear layer  Hard-label distillation.   Weintroduce a variant of distil-
               to predict the class. This class token is inherited from       lation where we take the hard decision of the teacher as a
                                              Training data-efﬁcient image transformers & distillation through attention
                                                                                           the output of the teacher, as in a regular distillation, while
                                                                                           remaining complementary to the class embedding.
                                                                                           Fine-tuning with distillation.         Weuseboththetruelabel
                                                                                           and teacher prediction during the ﬁne-tuning stage at higher
                                                                                           resolution. We use a teacher with the same target resolution,
                                                                                           typically obtained from the lower-resolution teacher by the
                                                                                           methodofTouvronetal.(2019). We have also tested with
                                                                                           true labels only but this reduces the beneﬁt of the teacher
                                                                                           and leads to a lower performance.
                                          FFN:	residual+MLP                                Classiﬁcation with our approach: joint classiﬁers.                 At
                                            self-attention
                                                                                           test time, both the class or the distillation embeddings pro-
                                                                                           duced by the transformer are associated with linear classi-
                                                                                           ﬁers and able to infer the image label. Our referent method
                                                                                           is the late fusion of these two separate heads, for which we
                             class              patch            distillation
                             token             tokens              token                   add the softmax output by the two classiﬁers to make the
                                                                                           prediction. We evaluate these three options in Section 5.
                 Figure 2. Our distillation procedure: we simply include a new dis-
                 tillation token. It interacts with the class and patch tokens through     5. Experiments
                 the self-attention layers. This distillation token is employed in
                 a similar fashion as the class token, except that on output of the        This section presents a few analytical experiments and re-
                 network its objective is to reproduce the (hard) label predicted by       sults. We ﬁrst discuss our distillation strategy. Then we
                 the teacher, instead of true label. Both the class and distillation
                 tokens input to the transformers are learned by back-propagation.         comparatively analyze the efﬁciency and accuracy of con-
                                                                                           vnets and vision transformers.
                 true label. Let yt = argmaxcZt(c) be the hard decision                    5.1. Transformer models
                 of the teacher, the objective associated with this hard-label
                 distillation is:                                                          Asmentionedearlier, our architecture design is identical to
                                    1                      1                               the one proposed by Dosovitskiy et al. (2020) with no con-
                  LhardDistill =      L (ψ(Z ),y)+ L (ψ(Z ),y ). (3)                       volutions. Our only differences are the training strategies,
                    global          2 CE         s         2 CE          s    t
                                                                                           and the distillation token. Also we do not use a MLP head
                 For a given image, the hard label associated with the teacher             for the pre-training but only a linear classiﬁer. To avoid
                 maychangedepending on the speciﬁc data augmentation.                      any confusion, we refer to the results obtained in the prior
                 Wewillseethat this choice is better than the traditional one,             workbyViT,andpreﬁxoursbyDeiT.Ifnotspeciﬁed,DeiT
                 while being parameter-free and conceptually simpler: The                  refers to our referent model DeiT-B, which has the same
                 teacher prediction yt plays the same role as the true label y.            architecture as ViT-B. When we ﬁne-tune DeiT at a larger
                                                                                           resolution, we append the resulting operating resolution at
                 Label smoothing.         Hard labels can also be converted into           the end, e.g, DeiT-B↑384. Last, when using our distillation
                                                                                           procedure, we identify it with an alembic sign as DeiT .
                 soft labels with label smoothing (Szegedy et al., 2016),                                                                                   ⚗
                 where the true label is considered to have a probability                  TheparametersofViT-B(andthereforeofDeiT-B)areﬁxed
                 of 1−ε,andtheremainingεissharedacross the remaining                       as D = 768, h = 12 and d = D/h = 64. We introduce
                 classes. We ﬁx ε = 0.1 in our all experiments that use true               two smaller models, namely DeiT-S and DeiT-Ti, for which
                 labels. Note that we do not smooth pseudo-labels provided                 wechange the number of heads, keeping d ﬁxed. Table 1
                 bythe teacher (e.g., in hard distillation).                               summarizes the models that we consider in our paper.
                 Distillation token.      Wenowfocusonourproposal,which                    5.2. Distillation
                 is illustrated in Figure 2. We add a new token, the distillation
                 token, to the initial embeddings (patches and class token).               Ourdistillation method produces a vision transformer that
                 Our distillation token is used similarly as the class token:              becomes on par with the best convnets in terms of the trade-
                 it interacts with other embeddings through self-attention,                off between accuracy and throughput, see Table 5. Interest-
                 and is output by the network after the last layer. Its target             ingly, the distilled model outperforms its teacher in terms
                 objective is given by the distillation component of the loss.             of the trade-off between accuracy and throughput. Our
                 Thedistillation embedding allows our model to learn from                  best model on ImageNet-1k is 85.2% top-1 accuracy out-
                                             Training data-efﬁcient image transformers & distillation through attention
                Table 1. Variants of our DeiT architecture. The larger model, DeiT-     Table 3. Distillation experiments on ImageNet-1k with DeiT, 300
                B,hasthesamearchitectureastheViT-B(Dosovitskiyetal.,2020).              epochs of pre-training. We report the results for the architecture
                Theonlyparameters that vary across models are the embedding di-         augmented with an additional token/embedding in the last three
                mension and the number of heads, and we keep the dimension per          rows. We separately report the performance when classifying
                head constant (equal to 64). Smaller models have a lower parame-        with only one of the class or distillation embedding, and then
                ter count, and a faster throughput. The throughput is measured for      with a classiﬁer taking both of them as input. In the last row
                images at resolution 224×224.                                           (class+distillation), the result correspond to the late fusion of the
                 Model     embedding #heads #layers #params    training throughput      class and distillation classiﬁers.
                           dimension                          resolution (im/sec)                               supervision       ImageNet top-1 (%)
                 DeiT-Ti       192       3      12       5M      224       2536          DeiT: method ↓       label teacher   Ti224 S224 B224 B↑384
                 DeiT-S        384       6      12     22M       224        940          no distillation        ✓      ✗       72.2   79.8   81.8    83.1
                 DeiT-B        768      12      12     86M       224        292          usual distillation     ✗     soft     72.2   79.8   81.8    83.2
                                                                                         hard distillation      ✗    hard      74.3   80.9   83.0    84.0
                Table 2. ImageNet-1ktop-1accuracyofthestudentasafunctionof               class embedding        ✓    hard      73.9   80.9   83.0    84.2
                the teacher model used for distillation. The convolutional Regnet        distil. embedding      ✓    hard      74.6   81.1   83.1    84.4
                                                                                         DeiT : class+distil.   ✓    hard      74.5   81.2   83.4    84.5
                by Radosavovic et al. (2020) have been trained with a similar                 ⚗
                training as our transformers, except that we used SGD. We provide
                more details about their performance and efﬁciency in Table 5.
                Interestingly, image transformers learn more from a convnet than        strategy from Section 4 further improves the performance,
                from another transformer with comparable performance.                   showing that the two tokens provide complementary infor-
                                    Teacher           Student: DeiT-B                   mation useful for classiﬁcation: the classiﬁer on the two
                            Models             acc.   pretrain    ↑384                  tokens is signiﬁcantly better than the independent class and
                                                                                        distillation classiﬁers, which by themselves already outper-
                            DeiT-B             81.8     81.9      83.1                  form the distillation baseline.
                            RegNetY-4GF        80.0     82.7      83.6
                            RegNetY-8GF        81.7     82.7      83.8                  Theembeddingassociated with the distillation token gives
                            RegNetY-12GF       82.4     83.0      83.9                  slightly better results than the class token. It is also more
                            RegNetY-16GF       82.9     83.0      84.0                  correlated to the convnets prediction. In all cases, including
                                                                                        it improves the performance of the different classiﬁers. We
                                                                                        give more details and an analysis in the next paragraph.
                performs the best Vit-B model pre-trained on JFT-300M
                and ﬁne-tuned on ImageNet-1k at resolution 384 (84.15%).                Agreement with the teacher & inductive bias?               Asdis-
                Note, the current state of the art of 88.55% achieved with              cussed above, the architecture of the teacher has an im-
                extra training data is the ViT-H model (632M parameters)                portant impact. Does it inherit existing inductive bias that
                trained on JFT-300M and ﬁne-tuned at resolution 512. Here-              would facilitate the training? While we believe it difﬁcult
                after we provide several analysis and observations.                     to formally answer this question, we analyze in Table 4 the
                                                                                        decision agreement between the convnet teacher, our image
                Convnets teachers.        Wehaveobserved that using a con-              transformer DeiT learned from labels only, and our trans-
                vnet teacher gives better performance than using a trans-               former DeiT . Our distilled model is more correlated to the
                                                                                                      ⚗
                former. Table 2 compares distillation results with different            convnet than with a transformer learned from scratch. As
                teacher architectures. The fact that the convnet is a better            to be expected, the classiﬁer associated with the distillation
                teacher is probably due to the inductive bias inherited by              embeddingis closer to the convnet that the one associated
                the transformers through distillation, as explained in Abnar            with the class embedding, and conversely the one associated
                et al. (2020). In all of our subsequent distillation experi-            with the class embedding is more similar to DeiT learned
                ments the default teacher is a RegNetY-16GF (Radosavovic                without distillation. Unsurprisingly, the joint class+distil
                et al., 2020) with 84M parameters, that we trained with                 classiﬁer offers a middle ground.
                the same data and same data-augmentation as DeiT. This
                teacher reaches 82.9% top-1 accuracy on ImageNet.                       Analysis of the tokens.       Weobservethat the learned class
                                                                                        and distillation tokens converge towards different vectors:
                Comparison of distillation methods.            We compare the           the average cosine similarity (cos) between these tokens
                performance of different distillation strategies in Table 3.            equal to 0.06. The class and distillation embeddings com-
                Harddistillation signiﬁcantly outperforms soft distillation             puted at each layer gradually become more similar through
                for transformers, even when using only a class token: hard              the network, all the way through the last layer at which their
                distillation reaches 83.0% at resolution 224×224, compared              similarity is high (cos=0.93), but still lower than 1. This
                to the soft distillation accuracy of 81.8%. Our distillation            is expected since as they aim at producing targets that are
                                                  Training data-efﬁcient image transformers & distillation through attention
                  Table 4. Disagreement analysis between convnet, image transform-
                  ers and distillated transformers: We report the fraction of sample
                  classiﬁed differently for all classiﬁer pairs, i.e., the rate of different
                  decisions. We include two models without distillation (a RegNetY
                  and DeiT-B), so that we can compare how our distilled models and
                  classiﬁcation heads are correlated to the RegNetY teacher.
                                              no distillation          DeiT student
                                                                            ⚗
                                             convnet     DeiT     class    distil.  DeiT
                                                                                          ⚗                                                                         ⚗
                   groundtruth                0.171     0.182     0.170    0.169     0.166                                                                          ⚗↑
                   convnet (RegNetY)          0.000     0.133     0.112    0.100     0.102
                   DeiT                       0.133     0.000     0.109    0.110     0.107
                   DeiT –class only           0.112     0.109     0.000    0.050     0.033
                         ⚗
                   DeiT –distil. only         0.100     0.110     0.050    0.000     0.019
                         ⚗
                   DeiT –class+distil.        0.102     0.107     0.033    0.019     0.000
                         ⚗
                                                                                                  Figure 3. Distillation on ImageNet1k with DeiT-B: top-1 accuracy
                                                                                                  as a function of the training epochs. The performance without
                                                                                                  distillation (horizontal dotted line) saturates after 400 epochs.
                  similar but not identical.
                  Weveriﬁed that our distillation token adds something to
                  the model, compared to simply adding an additional class                        Our method DeiT is slightly below EfﬁcientNet, which
                  token associated with the same target label: instead of a                       shows that we have almost closed the gap between vision
                  teacher pseudo-label, we experimented with a transformer                        transformers and convnets when training with Imagenet
                  with two class tokens. Even if we initialize them randomly                      only. These results are a major improvement (+6.3% top-1
                  and independently, during training they converge towards                        in a comparable setting) over previous ViT models trained
                  the same vector (cos=0.999), and the output embedding are                       onImagenet1konly(Dosovitskiyetal.,2020). Furthermore,
                  also quasi-identical. In contrast to our distillation strategy,                 when DeiT beneﬁts from the distillation from a relatively
                                                                                                  weaker RegNetY to produce DeiT , it outperforms Efﬁ-
                  an additional class token does not bring anything to the                                                                     ⚗
                  classiﬁcation performance.                                                      cientNet. It also outperforms by 1% (top-1 acc.) the Vit-B
                                                                                                  model pre-trained on JFT300M at resolution 384 (85.2% vs
                  Numberofepochs. Increasingthenumberofepochssig-                                 84.15%), while being signiﬁcantly faster to train.
                  niﬁcantly improves the performance of training with distilla-                   Table 5 reports the numerical results in more details and
                                                               2                                  additional evaluations on ImageNet V2 and ImageNet Real,
                  tion, see Figure 3. With 300 epochs , our distilled network
                  DeiT-B       is already better than DeiT-B. But while for the                   that have a test set distinct from the ImageNet validation,
                            ⚗
                  latter the performance saturates with longer schedules, the                     which reduces overﬁtting on the validation set. Our results
                                                                                                  showthat DeiT-B          and DeiT-B ↑384outperform, by some
                  distilled network beneﬁts from a longer training time.                                                ⚗                ⚗
                                                                                                  margin, the state of the art on the trade-off between accuracy
                  5.3. Efﬁciency vs accuracy: a comparison to convnets                            and inference time on GPU.
                  In the literature, image classiﬁcaton methods are often com-                    5.4. Transfer learning to downstream tasks
                  pared as a compromise between accuracy and another cri-
                  terion, such as FLOPs, number of parameters, size of the                        Although DeiT perform very well on ImageNet it is impor-
                  network, etc. We focus in Figure 1 on the tradeoff between                      tant to evaluate them on other datasets with transfer learning
                  the throughput (images per second) and the top-1 classiﬁ-                       in order to measure the power of generalization of DeiT.
                  cation accuracy on ImageNet. The throughput is measured                         Weevaluated this on transfer learning tasks by ﬁne-tuning
                  as the number of images that we can process per second on                       onthe datasets in Table 8. Table 6 compares DeiT transfer
                  one 16GB V100 GPU: we take the largest possible batch                           learning results to those of ViT and EfﬁcientNet. DeiT is on
                  size and average the processing time over 30 runs. We focus                     par with competitive convnet models, which is in line with
                  on the popular EfﬁcientNet convnet, which has beneﬁted                          our previous conclusion on ImageNet1k.
                  from years of research on convnets and was optimized by
                  architecture search on the ImageNet validation set.                             Comparison vs training from scratch.                   We investigate
                      2Formally we have 100 epochs, but each is 3x longer because                 the performance when training from scratch on a small
                  of the repeated augmentations. We prefer to refer to this as 300                dataset, without Imagenet pre-training. We get the following
                  epochsinordertohaveadirectcomparisonontheeffectivetraining                      results on the small CIFAR-10, which is small both w.r.t.
                  time with and without repeated augmentation.                                    the number of images and labels:
                                               Training data-efﬁcient image transformers & distillation through attention
                 Table 5. Throughput (images/s) vs accuracy on Imagenet (Rus-               Table 7. Ablation study on training methods on ImageNet (top-1
                 sakovsky et al., 2015), Imagenet Real (Beyer et al., 2020) and             acc.). The top row (”none”) corresponds to our default conﬁgura-
                 Imagenet V2 matched frequency (Recht et al., 2019) of models               tion employed for DeiT. The symbols ✓ and ✗ indicate that we
                 trained without external data. We compare DeiT and Vit-B (Doso-            use and do not use the corresponding method, respectively. We
                 vitskiy et al., 2020) to several state-of-the-art convnets: ResNet (He     report the accuracy scores (%) after the initial training at resolution
                 et al., 2016), Regnet (Radosavovic et al., 2020), EfﬁcientNet (Tan         224×224,andafter ﬁne-tuning at resolution 384×384. The hyper-
                 &Le, 2019; Cubuk et al., 2019; Wei et al., 2020). We use for               parameters are ﬁxed according to Table 9, and may be suboptimal.
                 each model the deﬁnition in the same GitHub (Wightman, 2019)               * indicates that the model did not train well, possibly because
                 repository. The reported results are from corresponding papers.            hyper-parameters are not adapted.
                                         nb of  image            ImNet    Real    V2                                                               vg.
                   Network             param.    size    im/s     top-1   top-1  top-1                                                             A
                                                                                                                                                        224     384
                   ResNet-18             12M     224    4458.4    69.8    77.3    57.1                                                     Aug.    ving
                   ResNet-50             25M     224    1226.1    76.2    82.5    63.3                                                Depth
                   ResNet-101            45M     224     753.6    77.4    83.7    65.7                                                             Mo
                   ResNet-152            60M     224     526.4    78.3    84.1    67.0
                   RegNetY-4GF⋆          21M     224    1156.7    80.0    86.4    69.4           Pre-trainingFine-tuningRand-AugmentAutoAugMixupCutMixErasingStoch.RepeatedDropoutExp.pre-trainedﬁne-tuned
                   RegNetY-8GF⋆          39M     224     591.6    81.7    87.4    70.8       adamw adamw       ✓ ✗ ✓ ✓ ✓             ✓ ✓ ✗ ✗ 81.8±0.2 83.1±0.1
                   RegNetY-16GF⋆         84M     224     334.7    82.9    88.1    72.4
                   EfﬁcientNet-B0         5M     224    2694.3    77.1    83.5    64.3        SGD adamw        ✓ ✗ ✓ ✓ ✓             ✓ ✓ ✗ ✗ 74.5 77.3
                   EfﬁcientNet-B1         8M     240    1662.5    79.1    84.9    66.9       adamw SGD         ✓ ✗ ✓ ✓ ✓             ✓ ✓ ✗ ✗ 81.8 83.1
                   EfﬁcientNet-B2         9M     260    1255.7    80.1    85.9    68.8       adamw adamw       ✗ ✗ ✓ ✓ ✓             ✓ ✓ ✗ ✗ 79.6 80.4
                   EfﬁcientNet-B3        12M     300     732.1    81.6    86.8    70.6       adamw adamw       ✗ ✓ ✓ ✓ ✓             ✓ ✓ ✗ ✗ 81.2 81.9
                   EfﬁcientNet-B4        19M     380     349.4    82.9    88.0    72.3       adamw adamw       ✓ ✗ ✗ ✓ ✓             ✓ ✓ ✗ ✗ 78.7 79.8
                   EfﬁcientNet-B5        30M     456     169.1    83.6    88.3    73.6
                   EfﬁcientNet-B6        43M     528       96.9   84.0    88.8    73.9       adamw adamw       ✓ ✗ ✓ ✗ ✓             ✓ ✓ ✗ ✗ 80.0 80.6
                   EfﬁcientNet-B7        66M     600       55.1   84.3                       adamw adamw       ✓ ✗ ✗ ✗ ✓             ✓ ✓ ✗ ✗ 75.8 76.7
                   EfﬁcientNet-B5 RA     30M     456       96.9   83.7                       adamw adamw       ✓ ✗ ✓ ✓ ✗             ✓ ✓ ✗ ✗            4.3*    0.1
                   EfﬁcientNet-B7 RA     66M     600       55.1   84.7                       adamw adamw       ✓ ✗ ✓ ✓ ✓             ✗ ✓ ✗ ✗            3.4*    0.1
                   KDforAA-B8            87M     800       25.2   85.8                       adamw adamw       ✓ ✗ ✓ ✓ ✓             ✓ ✗ ✗ ✗ 76.5 77.4
                                     Transformers: training 300 epochs                       adamw adamw       ✓ ✗ ✓ ✓ ✓             ✓ ✓ ✓ ✗ 81.3 83.1
                   ViT-B/16              86M     384       85.9   77.9    83.6               adamw adamw       ✓ ✗ ✓ ✓ ✓             ✓ ✓ ✗ ✓ 81.9 83.1
                   ViT-L/16             307M     384       27.3   76.5    82.2
                   DeiT-Ti                5M     224    2536.5    72.2    80.1    60.4
                   DeiT-S                22M     224     940.4    79.8    85.7    68.5
                   DeiT-B                86M     224     292.3    81.8    86.7    71.5
                   DeiT-B↑384            86M     384       85.9   83.1    87.7    72.4              Method      RegNetY-16GF         DeiT-B     DeiT-B
                                                                                                                                                        ⚗
                   DeiT-Ti                6M     224    2529.5    74.5    82.1    62.9               Top-1            98.0            97.5         98.5
                         ⚗
                   DeiT-S                22M     224     936.2    81.2    86.8    70.0
                         ⚗
                   DeiT-B                87M     224     290.9    83.4    88.3    73.2
                         ⚗                                                                  For this experiment, we tried we get as close as possible
                   DeiT-B ↑384           87M     384       85.8   84.5    89.0    74.8
                         ⚗                                                                  to the Imagenet pre-training counterpart, meaning that (1)
                                     Transformers: training 1000 epochs
                                                                                            weconsider longer training schedules (up to 7200 epochs,
                   DeiT-Ti                6M     224    2529.5    76.6    83.9    65.4
                         ⚗                                                                  which corresponds to 300 Imagenet epochs) so that the
                   DeiT-S                22M     224     936.2    82.6    87.8    71.7
                         ⚗
                   DeiT-B                87M     224     290.9    84.2    88.7    73.9
                         ⚗                                                                  network has been fed a comparable number of images in
                   DeiT-B ↑384           87M     384       85.8   85.2    89.3    75.2
                         ⚗                                                                  total; (2) we re-scale images to 224 × 224 to ensure that we
                 ⋆: our trained teachers with SGD, whose optimization procedure is closer to DeiT
                                                                                            have the same augmentation. The results are not as good
                                                                                            as with Imagenet pre-training (98.5% vs 99.1%), which is
                 Table 6. WecompareTransformersbasedmodelsondifferenttrans-                 expected since the network has seen a much lower diversity.
                 fer learning task with ImageNet pre-training. We also give results         Howevertheyshowthatitis possible to learn a reasonable
                 obtained with Efﬁcient-B7 for reference (Tan & Le, 2019).                  transformer on CIFAR-10 only.
                                             AR-10 AR-100 wers                              6. Training details & ablation
                   Model              ImageNetCIF  CIF    Flo   Cars   iNat-18iNat-19im/sec
                   EfﬁcientNet-B7   84.3  98.9   91.7  98.8   94.7                 55.1     This section discusses the DeiT training strategy to learn vi-
                   ViT-B/32         73.4  97.8   86.3  85.4                       394.5     sion transformers in a data-efﬁcient manner. We build upon
                                                                                                                                               3
                   ViT-B/16         77.9  98.1   87.1  89.5                        85.9     PyTorch (Paszke et al., 2019) and the timm library (Wight-
                   ViT-L/32         71.2  97.9   87.1  86.4                       124.1     man, 2019). We provide hyper-parameters and an ablation
                   ViT-L/16         76.5  97.9   86.4  89.7                        27.3     study in which we analyze the impact of each choice.
                   DeiT-B           81.8  99.1   90.8  98.4   92.1  73.2   77.7   292.3
                   DeiT-B↑384       83.1  99.1   90.8  98.5   93.3  79.5   81.4    85.9         3The timm implementation includes a training procedure that
                   DeiT-B           83.4  99.1   91.3  98.8   92.9  73.7   78.4   290.9
                         ⚗                                                                  improved the accuracy of ViT-B from 77.91% to 79.35% top-1,
                   DeiT-B ↑384      84.4  99.2   91.4  98.9   93.9  80.1   83.0    85.9
                         ⚗                                                                  and trained on Imagenet-1k with a 8xV100 GPU machine.
                                         Training data-efﬁcient image transformers & distillation through attention
               Initialization and hyper-parameters.       Transformers are       Fine-tuning at different resolution.      Weadopt the ﬁne-
               relatively sensitive to initialization. After testing several     tuning procedure from Touvron et al. (2020): our schedule,
               options, some of them not converging, we follow Hanin &           regularization and optimization procedure are identical to
               Rolnick (2018) and initialize the weights with a truncated        that of FixEfﬁcientNet but we keep the training-time data
               normal distribution. Table 9 indicates the hyper-parameters       augmentation, unlike the dampened data augmentation of
               that we use by default at training time for all our experi-       Touvron et al. (2020). We also interpolate the positional
               ments,unlessstatedotherwise. Fordistillationwefollowthe           embeddings: In principle any classical image scaling tech-
               recommendations from Cho & Hariharan (2019) to select             nique, like bilinear interpolation, could be used. However, a
               the parameters τ and λ. We take the typical values τ = 3.0        bilinear interpolation of a vector from its neighbors reduces
               or τ = 1.0 and λ = 0.1 for the usual (soft) distillation.         its ℓ2-norm compared to its neighbors. These low-norm
                                                                                 vectors are not adapted to the pre-trained transformers and
               Data-Augmentation.       Comparedtomodelsthatintegrate            we observe a signiﬁcant drop in accuracy if we employ
               morepriors (such as convolutions), transformers require a         use directly without any form of ﬁne-tuning. Therefore we
               larger amount of data. Thus, in order to train with datasets      adopt a bicubic interpolation that approximately preserves
               of the same size, we rely on extensive data augmentation.         the norm of the vectors, before ﬁne-tuning the network with
               We evaluate different types of strong data augmentation,          either AdamW (Loshchilov & Hutter, 2017) or SGD. These
               with the objective to reach a data-efﬁcient training regime.      optimizers have a similar performance for the ﬁne-tuning
                                                                                 stage, see Table 7.
               Auto-Augment(Cubuketal.,2018),Rand-Augment(Cubuk
               et al., 2019), and random erasing (Zhong et al., 2020) im-        Bydefault and similar to ViT we train DeiT models with
               prove the results. For the two latter we use the timm (Wight-     at resolution 224 and ﬁne-tune at resolution 384. We detail
               man, 2019) customizations, and after ablation we choose           howtodothisinterpolation in Section 3.
               Rand-AugmentinsteadofAutoAugment. Overallourexper-
               iments conﬁrm that transformers require a strong data aug-        Training time.    Atypical training of 300 epochs takes 37
               mentation: almost all the data-augmentation methods that          hours with 2 nodes or 53 hours on a single 8-GPU node
               weevaluate prove to be useful. One exception is dropout,          for the DeiT-B. As a comparison point, a similar training
               which we exclude from our training procedure.                     with a RegNetY-16GF (Radosavovic et al., 2020) (84M
                                                                                 parameters) is 20% slower. DeiT-S and DeiT-Ti are trained
               Regularization & Optimizers.        Wehaveconsidered dif-         in less than 3 days on 4 GPU. Then, optionally we ﬁne-tune
               ferent optimizers and cross-validated different learning rates    the model at a larger resolution. This takes 20 hours on
               and weight decays. Transformers are sensitive to the set-         8GPUstoﬁne-tuneaDeiT-Bmodelatresolution384×384,
               ting of optimization hyper-parameters.       Therefore, dur-      whichcorrespondsto25epochs. Nothavingtorelyonbatch-
               ing cross-validation, we tried 3 different learning rates         normallows one to reduce the batch size without impacting
                    −4       −4       −5                                         performance, which makes it easier to train larger models.
               (5.10   , 3.10   , 5.10  ) and 3 weight decay (0.03, 0.04,
               0.05). We scale the learning rate according to the batch size     Note that, since we use repeated augmentation (Berman
               with the formula: lr       = lr ×batchsize, similarly to          et al., 2019; Hoffer et al., 2020) with 3 repetitions, we only
                                    scaled    512
               Goyal et al. (2017) except that we use 512 instead of 256 as      see one third of the images during a single epoch.
               the base value. The best results use the AdamW optimizer
               with a much smaller weight decay than in ViT.                     7. Conclusion
               Wehave employed stochastic depth (Huang et al., 2016),            Wehaveintroducedadata-efﬁcient training procedure for
               whichfacilitates the convergence of transformers, especially      image transformers so that do not require very large amount
               deep ones (Fan et al., 2019; 2020). For vision transform-         of data to be trained, thanks to improved training and in par-
               ers, they were ﬁrst adopted in the training procedure by          ticular a novel distillation procedure. Convolutional neural
               Wightman(2019). Regularization like Mixup (Zhang et al.,          networks have been optimized, both in terms of architecture
               2017) and Cutmix (Yun et al., 2019) improve performance.          andoptimization, during almost a decade, including through
               Wealso use repeated augmentation (Berman et al., 2019;            extensive architecture search prone to overﬁting.
               Hoffer et al., 2020), which is one of the key ingredients of
               our proposed training procedure.                                  For DeiT we relied on existing data augmentation and regu-
                                                                                 larization strategies pre-existing for convnets, not introduc-
               Exponential Moving Average (EMA).           Weevaluate the        ing any signiﬁcant architectural change beyond our novel
               EMAofournetworkobtainedaftertraining. Therearesmall               distillation token. Therefore we expect that further research
               gains, which vanish after ﬁne-tuning: the EMA model has           onimagetransformers will bring further gains.
               an edge of is 0.1 accuracy points, but when ﬁne-tuned the
               two models reach the same (improved) performance.
                                        Training data-efﬁcient image transformers & distillation through attention
               8. Acknowledgements                                                daugment: Practical automated data augmentation with a
                                                                                  reduced search space. arXiv preprint arXiv:1909.13719,
               ManythankstoRossWightmanforsharinghisViTcodeand                    2019.
               bootstrapping the training method with the community, as
               well as for valuable feedback that helped us to ﬁx different    Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
               aspects of this paper. Thanks to Vinicius Reis, MannatSingh,       Pre-training of deep bidirectional transformers for lan-
               Ari Morcos, Mark Tygert, Gabriel Synnaeve, and other col-          guage understanding. arXiv preprint arXiv:1810.04805,
               leagues atFacebook for brainstorming and some exploration          2018.
               onthis axis. Thanks to Ross Girshick and Piotr Dollar for       Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
               constructive comments.                                             D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
                                                                                  Heigold, G., Gelly, S., et al. An image is worth 16x16
               References                                                         words: Transformersforimagerecognitionatscale. arXiv
                                                                                  preprint arXiv:2010.11929, 2020.
               Abnar, S., Dehghani, M., and Zuidema, W. Transferring           Fan, A., Grave, E., and Joulin, A. Reducing transformer
                 inductive biases through knowledge distillation. arXiv           depth on demand with structured dropout. arXiv preprint
                 preprint arXiv:2006.00555, 2020.                                 arXiv:1909.11556, 2019.
               Ba,J.L.,Kiros,J.R.,andHinton,G.E. Layernormalization.           Fan, A., Stock, P., Graham, B., Grave, E., Gribonval, R.,
                 arXiv preprint arXiv:1607.06450, 2016.                            ´
                                                                                  Jegou, H., and Joulin, A. Training with quantization
               Bello, I.  Lambdanetworks: Modeling long-range inter-              noise for extreme model compression. arXiv preprint
                 actions without attention. International Conference on           arXiv:2004.07320, 2020.
                 Learning Representations, 2021.                               Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin,
               Bello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V. At-    Y. N. Convolutional sequence to sequence learning. arXiv
                 tention augmented convolutional networks. International          preprint arXiv:1705.03122, 2017.
                 Conference on Computer Vision, 2019.                                            ´
                                                                               Goyal, P., Dollar, P., Girshick, R. B., Noordhuis, P.,
                               ´                                                  Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and
               Berman, M., Jegou, H., Vedaldi, A., Kokkinos, I., and
                 Douze, M. Multigrain: a uniﬁed image embedding for               He, K. Accurate, large minibatch sgd: Training imagenet
                 classes and instances. arXiv preprint arXiv:1902.05509,          in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
                 2019.                                                         Hanin, B. and Rolnick, D. How to start training: The effect
                             ´                                                    of initialization and architecture. Advances in Neural
               Beyer, L., Henaff, O. J., Kolesnikov, A., Zhai, X., and
                 van den Oord, A. Are we done with imagenet? arXiv                Information Processing Systems, 2018.
                 preprint arXiv:2006.07159, 2020.                              He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
               Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,        ing for image recognition. In Conference on Computer
                 A., and Zagoruyko, S. End-to-end object detection with           Vision and Pattern Recognition, 2016.
                 transformers. In European Conference on Computer Vi-          He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.
                 sion, 2020.                                                      Bagoftricks for image classiﬁcation with convolutional
               Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D.,       neural networks. In Conference on Computer Vision and
                 and Sutskever, I. Generative pretraining from pixels. In         Pattern Recognition, 2019.
                 International Conference on Machine Learning, 2020a.          Hendrycks, D. and Gimpel, K. Gaussian error linear units
               Chen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan,         (gelus). arXiv preprint arXiv:1606.08415, 2016.
                 Z., Cheng, Y., and jing Liu, J. Uniter: Universal image-      Hinton, G. E., Vinyals, O., and Dean, J.           Distilling
                 text representation learning. In European Conference on          the knowledge in a neural network.        arXiv preprint
                 ComputerVision, 2020b.                                           arXiv:1503.02531, 2015.
               Cho, J. H. and Hariharan, B. On the efﬁcacy of knowledge        Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoeﬂer,
                 distillation. In International Conference on Computer            T., and Soudry, D. Augment your batch: Improving
                 Vision, 2019.                                                    generalization through instance repetition. In Conference
               Cordonnier, J.-B., Loukas, A., and Jaggi, M. On the rela-          onComputerVision and Pattern Recognition, 2020.
                 tionship between self-attention and convolutional layers.     Horn, G. V., Mac Aodha, O., Song, Y., Shepard, A., Adam,
                 arXiv preprint arXiv:1911.03584, 2020.                           H., Perona, P., and Belongie, S. J. The inaturalist chal-
                                             ´                                    lenge 2018 dataset. arXiv preprint arXiv:1707.06642,
               Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le,
                 Q. V. Autoaugment: Learning augmentation policies                2018.
                 from data. arXiv preprint arXiv:1805.09501, 2018.             Horn, G. V., Mac Aodha, O., Song, Y., Shepard, A., Adam,
               Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Ran-             H., Perona, P., and Belongie, S. J. The inaturalist chal-
                                         Training data-efﬁcient image transformers & distillation through attention
                 lenge 2019 dataset. arXiv preprint arXiv:1707.06642,              skaya, A., and Shlens, J. Stand-alone self-attention in
                 2019.                                                             vision models. In Advances in Neural Information Pro-
               Hu, H., Gu, J., Zhang, Z., Dai, J., and Wei, Y. Relation net-       cessing Systems, 2019.
                 works for object detection. In Conference on Computer          Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do im-
                 Vision and Pattern Recognition, 2018.                             agenet classiﬁers generalize to imagenet? arXiv preprint
               Hu, J., Shen, L., and Sun, G. Squeeze-and-excitation net-           arXiv:1902.10811, 2019.
                 works. arXiv preprint arXiv:1709.01507, 2017.                  Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
               Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger,             Ma,S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
                 K. Q. Deep networks with stochastic depth. In European            M., Berg, A. C., and Fei-Fei, L. Imagenet large scale
                 Conference on Computer Vision, 2016.                              visual recognition challenge. International journal of
               Krause, J., Stark, M., Deng, J., and Fei-Fei, L. 3d object          ComputerVision, 2015.
                 representations for ﬁne-grained categorization. In In-         Simonyan, K. and Zisserman, A. Very deep convolutional
                 ternational IEEE Workshop on 3D Representation and                networks for large-scale image recognition. In Interna-
                 Recognition, 2013.                                                tional Conference on Learning Representations, 2015.
               Krizhevsky, A. Learning multiple layers of features from         Sun, C., Myers, A., Vondrick, C., Murphy, K., and Schmid,
                 tiny images. Technical report, CIFAR, 2009.                       C. Videobert: A joint model for video and language rep-
               Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet           resentation learning. In Conference on Computer Vision
                 classiﬁcation with deep convolutional neural networks.            andPattern Recognition, 2019.
                 In Advances in Neural Information Processing Systems,          Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wo-
                 2012.                                                             jna, Z. Rethinking the inception architecture for com-
               Li, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,           puter vision. Conference on Computer Vision and Pattern
                 K.-W. VisualBERT: a simple and performant baseline for            Recognition, 2016.
                 vision and language. arXiv preprint arXiv:1908.03557,          Tan, M. and Le, Q. V. Efﬁcientnet: Rethinking model
                 2019a.                                                            scaling for convolutional neural networks. arXiv preprint
               Li, X., Wang, W., Hu, X., and Yang, J. Selective kernel             arXiv:1905.11946, 2019.
                 networks. In Conference on Computer Vision and Pattern         Touvron, H., Vedaldi, A., Douze, M., and Jegou, H. Fixing
                 Recognition, 2019b.                                               the train-test resolution discrepancy. Advances in Neural
               Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran,         Information Processing Systems, 2019.
                                                                                                                               ´
                 A., Heigold, G., Uszkoreit, J., Dosovitskiy, A., and Kipf,     Touvron, H., Vedaldi, A., Douze, M., and Jegou, H. Fix-
                 T. Object-centric learning with slot attention. arXiv             ing the train-test resolution discrepancy: Fixefﬁcientnet.
                 preprint arXiv:2006.15055, 2020.                                  arXiv preprint arXiv:2003.08237, 2020.
               Loshchilov, I. and Hutter, F. Fixing weight decay regular-       Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
                 ization in adam. arXiv preprint arXiv:1711.05101, 2017.           L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-
               Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining     tion is all you need. In Advances in Neural Information
                 task-agnostic visiolinguistic representations for vision-         Processing Systems, 2017.
                 and-language tasks. In Advances in Neural Information          Wang, X., Girshick, R. B., Gupta, A., and He, K. Non-local
                 Processing Systems, 2019.                                         neural networks. Conference on Computer Vision and
               Nilsback, M.-E. and Zisserman, A. Automated ﬂower clas-             Pattern Recognition, 2018.
                 siﬁcation over a large number of classes. In Indian Con-       Wei, L., Xiao, A., Xie, L., Chen, X., Zhang, X., and Tian, Q.
                 ference on Computer Vision, Graphics and Image Pro-               Circumventing outliers of autoaugment with knowledge
                 cessing, 2008.                                                    distillation. European Conference on Computer Vision,
               Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,          2020.
                 Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,      Wightman,R. Pytorchimagemodels. https://github.
                 L., et al. Pytorch: An imperative style, high-performance         com/rwightman/pytorch-image-models,
                 deep learning library. In Advances in Neural Information          2019.
                 Processing Systems, 2019.                                      Wu,B.,Xu,C.,Dai,X.,Wan,A.,Zhang,P.,Tomizuka, M.,
               Radosavovic, I., Kosaraju, R. P., Girshick, R. B., He, K., and      Keutzer, K., and Vajda, P. Visual transformers: Token-
                      ´                                                            based image representation and processing for computer
                 Dollar, P. Designing network design spaces. Conference
                 onComputerVision and Pattern Recognition, 2020.                   vision. arXiv preprint arXiv:2006.03677, 2020.
               Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev-       Yuan, L., Tay, F., Li, G., Wang, T., and Feng, J. Revisit
                                                                                   knowledgedistillation: a teacher-free framework. Confer-
                                       Training data-efﬁcient image transformers & distillation through attention
                 ence on Computer Vision and Pattern Recognition, 2020.
              Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and
                Yoo, Y. Cutmix: Regularization strategy to train strong
                 classiﬁers with localizable features.   arXiv preprint
                 arXiv:1905.04899, 2019.
                               ´
              Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz,
                 D. mixup: Beyond empirical risk minimization. arXiv
                 preprint arXiv:1710.09412, 2017.
              Zhang, H., Wu, C., Zhang, Z., Zhu, Y., Zhang, Z., Lin,
                 H., Sun, Y., He, T., Muller, J., Manmatha, R., Li, M.,
                 and Smola, A. Resnest: Split-attention networks. arXiv
                 preprint arXiv:2004.08955, 2020.
              Zhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y. Ran-
                 domerasing data augmentation. In Conference on Artiﬁ-
                 cial Intelligence, 2020.
