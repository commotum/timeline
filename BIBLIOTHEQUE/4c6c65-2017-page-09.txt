         93% of the sample scenes in the test set. In the counting task, the RN achieved similar performance,
         reporting the correct number of connected systems for 95% of the test scene samples. In comparison,
         an MLP with comparable number of parameters was unable to perform better than chance for both
         tasks. Moreover, using this task to learn to infer relations results in transfer to unseen motion capture
         data, where RNs predict the connections between body joints of a walking human (see supplementary
         information for experimental details and example videos).
         6 Discussion and Conclusions
         This work showed how the RN, a dedicated module for computing inter-entity relations, can be
         plugged into broader deep learning architectures to substantially improve performance on tasks that
         demand rich relational reasoning. Our CLEVR results included super-human performance at 95.5%
         overall. Our bAbI results demonstrated broad reasoning capabilities, solving 18/20 tasks with no
         catastrophic failures. Together these results demonstrate the ﬂexibility and power of this simple
         neural network building block.
          One of the most interesting aspects of the work is that RN module inclusion in relatively simple
         CNN- and LSTM-based VQA architectures raised the performance on CLEVR from 68.5% to 95.5%
         and achieved state-of-the-art, super-human performance. We speculate that the RN provided a more
         powerful mechanism for ﬂexible relational reasoning, and freed up the CNN to focus more exclusively
         on processing local spatial structure. This distinction between processing and reasoning is important.
         Powerful deep learning architectures, such as ResNets, are highly capable visual processors, but they
         may not be the most appropriate choice for reasoning about arbitrary relations.
          Akey contribution of this work is that the RN was able to induce, through the learning process,
         upstream processing to provide a set of useful object-like representations. Note, the input data
         and target objective functions did not specify any particular form or semantics of the internal
         object representations. This demonstrates the RN’s rich capacity for structured reasoning even with
         unstructured inputs and outputs.
          Future work should apply RNs to a variety of problems that can beneﬁt from structure learning
         and exploitation, such as rich scene understanding in RL agents, modeling social networks, and
         abstract problem solving. Future work could also improve the eﬃciency of RN computations. Though
         our results show that no knowledge about the particular relations among objects are necessary, RNs
         can exploit such knowledge if available or useful. For example, if two objects are known to have no
         actual relation, the RN’s computation of their relation can be omitted. An important direction is
         exercising this option in circumstances with strict computational constraints, where, for instance,
         attentional mechanisms could be used to ﬁlter unimportant relations and thus bound the otherwise
         quadratic complexity of the number of considered pairwise relations.
          Relation Networks are a simple and powerful approach for learning to perform rich, structured
         reasoning in complex, real-world domains.
         Acknowledgments
         Wewould like to thank Murray Shanahan, Ari Morcos, Scott Reed, Daan Wierstra, Alex Lerchner,
         and many others on the DeepMind team, for critical feedback and discussions.
                              9
