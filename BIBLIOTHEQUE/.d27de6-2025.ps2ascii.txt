                      Call for Action: Towards the Next Generation of Symbolic
                                                          Regression Benchmark
                   Guilherme Seidyo Imai Aldeia                         HengzheZhang                              Geoffrey Bomarito
                      guilherme.aldeia@ufabc.edu.br               hengzhe.zhang@ecs.vuw.ac.nz                 geoffrey.f.bomarito@nasa.gov
                        Federal University of ABC               Victoria University of Wellington            NASALangleyResearchCenter
                        Santo Andr√©, S√£o Paulo, BR                        Wellington, NZ                         Hampton,Virginia, USA
                             Miles Cranmer                              Alcides Fonseca                             BogdanBurlacu
                            mc2473@cam.ac.uk                         me@alcidesfonseca.com                University of Applied Sciences Upper
                         University of Cambridge                LASIGE, Faculdade de Ci√™ncias da                          Austria
                              Cambridge, UK                           Universidade de Lisboa                         UpperAustria, AT
                                                                             Lisboa, PT
                                                William G. La Cava                     Fabr√≠cio Olivetti de Fran√ßa
                                        william.lacava@childrens.harvard.edu                folivetti@ufabc.edu.br
                                    Computational Health Informatics Program              Federal University of ABC
                                              Boston Children‚Äôs Hospital                  Santo Andr√©, S√£o Paulo, BR
                                               Harvard Medical School
                                             Boston, Massachusetts, USA
                                                                                                                                                     ÀÜ
               Abstract                                                               is a vector of adjustable parameters. Searching for the function Ì†µÌ±ì
               Symbolic Regression (SR) is a powerful technique for discovering       addsadegree-of-freedomtodatafittingthatdiffersfromtraditional
               interpretable mathematical expressions. However, benchmarking          parametric models that starts from a fixed Ì†µÌ±ì (Ì†µÌ±•,Ì†µÌºÉ) and tries to find
                                                                                      ÀÜ
               SRmethodsremainschallenging due to the diversity of algorithms,        Ì†µÌºÉ that minimizes a loss function. Currently, many SR algorithms
               datasets, and evaluation criteria. In this work, we present an up-     implement a variation of the original genetic programming (GP)
               dated version of SRBench. Our benchmark expands the previous           as proposed by Koza [35, 36]. Since then, it has been successfully
               one by nearly doubling the number of evaluated methods, refin-         applied to many real-world applications, such as, scientific dis-
               ing evaluation metrics, and using improved visualizations of the       covery in engineering [44], healthcare [41], physics [25], among
               results to understand the performances. Additionally, we analyze       others [10, 37, 53, 55, 60]. Its application in physics can be seen as
               trade-offs between model complexity, accuracy, and energy con-        ‚Äúautomating Kepler‚Äù ‚Äîdiscovering analytical expressions through
               sumption. Our results show that no single algorithm dominates          trial and error‚Äî, particularly evident in recent works that use SR
               across all datasets. We propose a call for action from SR community    to recover laws directly from experimental data [45], underscoring
               in maintaining and evolving SRBench as a living benchmark that         its potential as a tool for automated scientific reasoning.
               reflects the state-of-the-art in symbolic regression, by standardiz-     EquationdiscoveryisanNP-hardproblem[62],andimplementa-
               ing hyperparameter tuning, execution constraints, and computa-         tions rely on various approaches to solve it, such as GP [8, 10, 12, 15,
               tional resource allocation. We also propose deprecation criteria to    18, 20, 34, 39, 40, 51, 56, 61, 64], Neural Networks [4, 54], Transform-
               maintain the benchmark‚Äôs relevance and discuss best practices for      ers [29, 42, 57], Bayesian models [28], iterative methods [11, 31, 47],
               improving SR algorithms, such as adaptive hyperparameter tuning        andevenexhaustive approaches [3, 30]. In its original implemen-
               andenergy-ef√ècient implementations.                                    tation, GP relied on generating ephemeral random constants as
                                                                                      part of the expression, which could lead to sub-optimal exploration
               CCSConcepts                                                            of the search space if these constants were not properly set. This
         arXiv:2505.03977v1  [cs.LG]  6 May 2025                                      was alleviated with the use of linear scaling [32] by scaling and
               ‚Ä¢ Computing methodologies ‚Üí Symbolic and algebraic al-                 translating the solution before measuring the loss function. Modern
               gorithms; ‚Ä¢ General and reference ‚Üí Empirical studies; ‚Ä¢               implementations rely on the optimization of the numerical param-
               Human-centeredcomputing‚ÜíVisualizationdesignandevalua-                  eters by either enforcing the creation of a model that is linear in
               tion methods.                                                          the parameters [2, 8, 12, 15] or applying a non-linear optimization
               Keywords                                                               methodtofit the parameters [18, 34].
               symbolic regression, benchmark, srbench                                  Despite recent advances in the field of SR, La Cava et al. [38] ob-
                                                                                      served a lack of consensus regarding a standardized set of datasets
               1 Introduction                                                         andbenchmarkingmethodologies, making it challenging to accu-
               Symbolic regression (SR) [35, 37] is a supervised machine learning     rately establish the current state-of-the-art (SotA). Another issue is
                                                                     ÀÜ   ÀÜ            the lack of a unified programming interface and clear instructions
               technique that searches for a mathematical expression Ì†µÌ±ì (x,Ì†µÌºÉ) that   oninstalling and using each approach properly, creating barriers
                                                                ÀÜ   ÀÜ          ÀÜ
               fits a set of points x ‚àà X,Ì†µÌ±¶ ‚àà Y such as Ì†µÌ±¶ ‚âà Ì†µÌ±ì (x,Ì†µÌºÉ), where Ì†µÌºÉ
                                   Ì†µÌ±ñ      Ì†µÌ±ñ
                                                                                                                                            Imai Aldeia et al.
               to new experimentation and reproducibility. To address these chal-       of individual algorithms. Section 5 discusses the results, and Sec-
               lenges, SRBench 1 was proposed as a living benchmark for SR,             tion 6 presents our conclusions and call to action for advancing the
               comparing different SR implementations and standard ML regres-           benchmarking of symbolic regression methods.
               sion models. This benchmark enforced the adoption of a common
               Python API and installation scripts within a sandbox environment,        2 Relatedwork
               makingit easier to install and evaluate different algorithms.
                  Running a benchmark is a laborious yet important task. Not               In 2018, Orzechowski et al. [50] surveyed many SR papers show-
               only is it necessary to compare methods, but it also serves as a         ingthatmethodswerebeingbenchmarkedonsimpleproblemswith
               tool for measuring progress in the field. While the SRBench was          alackofstandardization,whichcoulddiminishtheperceptionofSR
               a significant step toward comparing and understanding the cur-           achievements outside of its field. They also acknowledged previous
               rent SotA in SR, designing effective benchmarks often requires           efforts to conduct benchmarks for SR methods, then conducted an
               further refinements and consideration of new factors to ensure           experiment with four SR algorithms comparing with different ML
               their usefulness and longevity. An effective benchmark should con-       methods across more than 100 datasets of up to 1000 samples.
               tain diverse subclasses of problems, identifying which algorithms           LaCavaetal.[38] followed up the benchmark efforts and pro-
               perform best in each scenario, mapping problems to algorithms            posed SRBench, a joint-effort to achieve a large scale comparison
               rather than providing a one-size-fits-all solution. Additionally, the    of 14 modern SR algorithms comprising GP-based, deterministic,
               benchmarkmustchallenge contemporary SR algorithms while re-              and deep learning based approaches. In this paper, the authors
               maining computationally feasible and representative of real-world        mention that determining a state-of-the-art should not be the sole
               challenges, ensuring that results extend beyond artificial test cases.   focus of research ‚Äî however, promising avenues of investigation
               SRBenchfalls short in these aspects, as it relies on more than 200       cannot be well-informed without empirical evidence. In SRBench,
               datasets without a detailed categorization of their specific chal-       the authors increased the number of problems and their dimension-
               lenges. Furthermore, it presents final results using only aggregated     alities, using datasets from PMLB [49, 52], an open-source library
               metrics, which can hide finer details in performance comparisons.        for benchmarking ML methods. A new ground-truth track was
                  In this paper, we propose several improvements to the existing        also added with more than 100 physics equations from Feynman
               SRBench (hereafter referred to as SRBench 1.0), identifying chal-        lectures [21, 22] with data generated synthetically. They provided
               lenges that must be addressed to develop a more refined version of       a more informative view of the current state of the field. Given
               the benchmark (SRBench 2.0). Additionally, this paper serves as a        the large number of problems and algorithms, they performed 10
               call to action, encouraging the community to engage in discussions       individual runs for each dataset, and the budget was based on eval-
               onbest practices for benchmarking and evaluating SR algorithms.          uations ‚Äî which is hard to measure uniformly across the variety of
               Such involvement is crucial for the field to achieve a more mature       different approaches with different computational runtime.
               state and drive further advancements.                                       Further discussions emerged in top-tier conferences related to
                  The primary improvements proposed include: expanding the              SR (such as GECCO), with criticisms about aggregated views of
               benchmark to incorporate new SR algorithms, bringing the total           the results [16, 17]. de Franca et al. [14] benchmarked a smaller
               to 25; increasing the number of independent runs from 10 to 30           selection of SR algorithms, including more recent approaches, on a
               to enhance the statistical significance of the results; selecting 24     proposedsetofartificiallygenerateddatasetstomeasuretheperfor-
               datasets, divided into two tracks: (i) black-box (drawn from SR-         manceinspecific tasks: overall accuracy performance, robustness
               Bench 1.0) and (ii) phenomenological & first-principles (sourced         to noise, capabilities of selecting the relevant features, extrapola-
               from [10]); and refining the reporting methodology to provide a          tion performance, and interpretability. With a smaller selection of
               moredetailed comparison of the nuances among top-performing              algorithms and datasets it was possible to make precise statements
               algorithms, rather than simply ranking them and designating a            of the current state of SR and the current weak points that should
               single SotA method.                                                      be addressed in the future. The authors also introduced a runtime
                  This call to action aims to highlight the challenges and issues en-   limit instead of using the number of evaluations, stimulating the
               countered during benchmarking and to propose features that could         ef√ècient implementation of the algorithms. The authors noticed
               streamline the process. Addressing these issues requires collabo-        that even though SR shares many shortcomings with traditional
               ration from method‚Äôs authors/maintainers to develop and extend           MLmodels, they have a potential to overcome it using domain
               a commonAPIthatfacilitates the expansion of SRBench in future            knowledge and allowing a customized experience to the user [27].
               iterations. To our knowledge, this is the largest study comparing        Regarding the benchmarks, the authors feel that there is still room
               SRmethodsunderaunifiedexperimental setup.                                for improvements on how to craft datasets that correctly mimic the
                  Thepaperisorganized as follows: Section 2 reviews previously          challenges faced in the real-world.
               published benchmarks on symbolic regression, highlighting dif-              Matsubaraetal. [46] noticed that the Feynman dataset [60] used
               ferences to our work. Section 3 presents our proposed update to          in the SRBench‚Äôs ground-truth track did not adequately depict the
               SRBench, with each subsection addressing a key aspect of our up-         original physical phenomena because of how the values were sam-
               date to the benchmark. Section 4 reports our results, using different    pled ‚Äì‚Äî the data generation was originally done by uniformly sam-
               levels of aggregation to gain deeper insights into the performance       pling standardized values. They proposed to replace the synthetic
                                                                                        data generation process with normal or logarithmic distributions
                                                                                        that mimicwhatwewouldempiricallyobserveifweweretocollect
               1https://cavalab.org/srbench/                                            the data in practice, and evaluated six different SR methods.
                  Call for Action: Towards the Next Generation of Symbolic Regression Benchmark
                      ≈Ωegklitz and Po≈°√≠k [63] proposes a benchmark with open-source                           Theblack-box track is a selection of 12 regression datasets from
                  andease-of-use principles. They evaluated four SR methods on five                       the PMLB1.0project [52]. To make this selection, we first excluded
                  synthetic and four real-world problems. Abdalla et al. [1] bench-                       datasets where all previously benchmarked algorithms achieved
                  marks only five SR methods focusing on benchmarking SR for                              Ì†µÌ±Ö2 > 0.99, or if a simple linear regression was also capable of
                  learning equations in civil and construction engineering, where                         achieving this same Ì†µÌ±Ö2 threshold. Next, we created a numerical
                  equations are traditionally derived using linear regression.                            feature vector for each dataset containing the number of samples,
                      Thing and Koksbang [58] proposed an unified tool for bench-                         numberoffeatures,andtheperformanceofeachalgorithmtestedin
                  markingSRincosmology,including12SRmethodsand28datasets                                  theSRBench1.0.Wethenappliedthet-SNEalgorithmtoreducethis
                  representingcosmologicalandastroparticlesphysicsproblems,find-                          metadata of the datasets to two dimensions and created 12 clusters
                  ing that most methods performed poorly in the benchmark. They                           using the Ì†µÌ±ò-means algorithm over the latent encodings. Finally,
                  argue that using standard datasets (i.e. PMLB) is prudent for aca-                      wepickedthedataset closest to each centroid. The final selection
                  demic comparisons, but these ensembles of datasets do not provide                       of datasets consists of different combinations of horizontal and
                  a representative measure for specific problems. They conducted                          vertical dimensionality and, as a byproduct, different codomains
                  off-the-shelf evaluations, which are useful because they do not re-                     ‚Äîanimportantfactor in proper model fitting, as these datasets may
                  quire extra effort from the user, and, to improve reproducibility,                      deviate from the traditional assumption of a normal distribution,
                  they package the 12 methods into Docker containers.                                     requiring algorithms to adapt accordingly.
                                                                                                              Weshould also note that in the original SRBench, half of the
                  3 Methods                                                                               black-box problems were variations of the Friedman datasets [23],
                                                                                                          which introduced bias in the results, as reported by de Fran√ßa [16].
                                                                                                          Tomitigate this issue, we restricted the selection of these datasets,
                  3.1      Dataset selection                                                              allowing a maximumof25%ofthechosendatasetstobelongtothis
                  For this version of the benchmark, we propose two distinct tracks:                      class. The goal is to ensure that the datasets provide a diverse rep-
                  black-box and phenomenological & first-principles. Table 1 lists the                    resentation of SR applications while enabling fine-grained analyses
                  datasets used in our benchmark, along with the number of samples                        to better understand why certain methods outperform others.
                  (# Rows), features (# Cols), and the range of values for the target                         Thephenomenological & first-principles track includes publicly
                  feature(codomain).Forthelattertrack,allcodomainsconsistofreal                           available datasets from Russeil et al. [53] and Cranmer [10]. The
                  values, so instead we provide the data sources for these datasets.                      former provides real-world measurements used to derive empirical
                  All datasets contain no missing values, have only numeric features,                     relationships, while the latter source data underlying well-known
                  andarestandardized before training.                                                     first-principles equations. All these datasets are particularly chal-
                                                                                                          lenging because of the small number of points and the diverse and
                                                                                                          unknownnaturaldistribution of the phenomena.
                  Table 1: Metadata for each benchmark track. Dataset names                               3.2      Benchmarkedmethods
                  matchtheircorrespondingPMLBentries                                                      In this benchmark, we have extended the list of methods from the
                                                                                                          original SRBench adding recently published methods, reported in
                     Black-box                    #Rows      #Cols     Codomain                           Table 2, highlighting whether the algorithm supports parameter
                                                                         +                                                                            2
                     1028_SWD                         1000        11   Z                                  optimization, a time-limit argument , whether it returns multiple
                                                                         +
                     1089_USCrime                       47        14   Z                                  solutions (e.g. a Pareto Front (PF)) or just a single solution, the
                                                                         +
                     1193_BNG_lowbwt                 31104        10   R                                  hardware the algorithm runs on, whether grid search to finetune
                     1199_BNG_echoMonths             17496        10   R
                                                                         +
                     192_vineyard                       52         3   R                                  the hyper-parameters is possible, and the programming language
                                                                         +
                     210_cloud                         108         6   R
                                                                         +                                used in the implementation. For methods implemented in other
                     522_pm10                          500         8   R
                                                                         +
                     557_analcatdata_apnea1            475         4   Z                                  programming languages, the authors also implemented Python
                     579_fri_c0_250_5                  250         6   R                                  bindings to facilitate their use.
                     606_fri_c2_1000_10               1000        11   R                                      Agridsearch was performed for all methods, except for three
                     650_fri_c0_500_50                 500        51   R
                                                                         +
                     678_visualizing_environmental     111         4   R                                  ‚ÄîBingo,Brush,andGP-GOMEA‚Äîduetoincompatibilitywithscikit-
                     Phenomenological&            #Rows      #Cols     Datasource                         learn‚Äôs GridSearchCV interface at the time. We adopted the grid
                     first-principles                                                                     searchsettings fromtheoriginalSRBenchanddefinedanewsearch
                     first_principles_absorption        14         2   Russeil et al. [53]                space for methods that lacked one. An exception is FFX, which
                     first_principles_bode               8         2   Bonnet [5]                         has no tunable hyperparameters. Additionally, we included the de-
                     first_principles_hubble            32         2   Hubble [26]                        fault (off-the-shelf) configuration as one of the candidate settings,
                     first_principles_ideal_gas         30         4   Generated, 10% noise
                     first_principles_kepler             6         2   Kepler [33]                        since preliminary results suggest that, for some problems, is outper-
                     first_principles_leavitt           26         2   Leavitt and Pickering [43]         forms tuned configurations. For further details, refer to the online
                     first_principles_newton            30         4   Generated, 10% noise
                     first_principles_planck           100         3   Generated, 10% noise               resources in Section 3.5.
                     first_principles_rydberg           50         3   Generated, 1% noise
                     first_principles_schechter         27         2   Generated, 20% noise
                     first_principles_supernovae_zr    236         2   Russeil et al. [53]                2In case it does not, we adjusted the hyperparameters to reasonable values to stop at
                     first_principles_tully_fisher      18         2   Tully and Fisher [59]              an approximate time limit.
                                                                                                                                                                                              Imai Aldeia et al.
                              Table 2: Algorithms evaluated, their original references, and relevant characteristics pertinent to benchmarking.
                         Algorithm       Const.     Time      Multiple      Runs      Language       Description
                                          Opt.      limit     solutions       on
                                                                                            ++
                            AFP[56]         ‚úó         ‚úì            ‚úó         CPU          C          Age-fitness Pareto (AFP) optimization, meaning model age is used as an objective, with constants
                                                                                                     randomly changed
                                                                                            ++
                             AFP_fe         ‚úó         ‚úì            ‚úó         CPU          C          AFPwithco-evolvedfitness estimates
                                                                                            ++
                       AFP_ehc[39]          ‚úì         ‚úì            ‚úó         CPU          C          AFPwithepigenetic hill climbing for constants optimization as local search
                          Bingo [51]        ‚úì         ‚úì           PF         CPU        Python       Evolves acyclic graphs with non-linear optimization, using islands for managing parallel populations
                                                                                            ++
                           Brush [9]        ‚úì         ‚úì           PF         CPU          C          GPwithmulti-armedbandits for controlling search space exploration
                            BSR[28]         ‚úó         ‚úì            ‚úó         CPU        Python       Bayesian model with priors for operators and coef√ècients is used to sample expression trees
                            E2E[29]         ‚úó         ‚úó            ‚úó         GPU        Python       Generator using pre-trained transformers, using BFGS and subsampling for tuning parameters
                                                                                            ++
                         EPLEX[40]          ‚úó         ‚úì            ‚úó         CPU          C          GPwithÌ†µÌºñ-lexicase parent selection
                            EQL[54]         ‚úó         ‚úó            ‚úó         CPU        Python       Shallowneuralnetworkusingmathematicaloperatorsasactivationfunctions,andperformsapruning
                                                                                                     to refine the network to an expression
                                                                                            ++
                           FEAT[8]          ‚úì         ‚úì           PF         CPU          C          GPalgorithm withÌ†µÌºñ-lexicase selection and linear combination of expressions using L1-OLS
                            FFX[47]         ‚úì         ‚úó            ‚úó         CPU        Python       Non-evolutionary, deterministic approach, that generates a set of base functions and fits a regularized
                                                                                                     OLStocombinethem
                         Genetic            ‚úó         ‚úì           ‚úì          CPU        Python       GPusingContext-Free Grammars to guide the generation process in an ef√ècient manner
                       Engine [20]
                                                                                            ++
                     GPGomea[61]            ‚úì         ‚úó           ‚úì          CPU          C          GPwithlinkagelearning used to propagate patterns and avoid their disruption
                            GPlearn         ‚úó         ‚úó           ‚úì          CPU        Python       Canonical GP implementation
                        GPZGD[18]           ‚úì         ‚úì            ‚úó         CPU           C         GPwithZ-scorestandardization and stochastic gradient descent for parameter optimization
                          ITEA[12]          ‚úì         ‚úó            ‚úó         CPU        Haskell      Mutation-based algorithm with constrained representation and OLS parameter optimization
                      NeSymRes[4]           ‚úì         ‚úì            ‚úó         GPU        Python       Pre-trained encoder-decoders generate equation skeletons, optimized with non-linear optimization
                                                                                            ++
                        Operon[34]          ‚úì         ‚úì           PF         CPU          C          GPalgorithm with weighted terminals and non-linear OLS parameter optimization
                        Ps-Tree [64]        ‚úó         ‚úó            ‚úó         CPU        Python       GPalgorithm that evolves Piecewise trees with SR expressions as leaves
                           PySR[10]         ‚úì         ‚úì            ‚úó         CPU          Julia      Evolve-simplify-optimize loop with islands to manage parallel populations
                         Qlattice [6]       ‚úì         ‚úì            ‚úó         CPU        Python       Uses a learned probability distribution updated over iterations to sample expressions, with parameter
                                                                                                     optimization. Closed source software
                                                                                            ++
                       Rils-rols [31]       ‚úì         ‚úì            ‚úó         CPU          C          Iterative generation of perturbations and parameter optimization with OLS and local search selection
                                                                                                     of next candidates
                            TIR [15]        ‚úì         ‚úì           PF         CPU        Haskell      GPwithcrossover and mutation that uses a constrained representation capable of tuning non-linear
                                                                                                     parameters with OLS
                          TPSR[57]          ‚úì         ‚úì            ‚úó         GPU        Python       Monte-Carlo Tree Search planning with non-linear optimization wrapper for generative models E2E
                                                                                                     andNeSymRes
                          uDSR[42]          ‚úì         ‚úó            ‚úó         GPU        Python       Unification of pre-trained transformers, GP, and linear models, into a framework that decomposes the
                                                                                                     problem
                        Hyper-parameter optimization is performed before each run                                      possible thresholds. In this plot, the Ì†µÌ±•-axis represents a threshold
                     using a 3-fold cross-validation on the training data (75%) and se-                                valueoftheÌ†µÌ±Ö2 andtheÌ†µÌ±¶-axisthepercentageofrunsthataparticular
                     lecting the best configuration based on the average Ì†µÌ±Ö2 score. The                                algorithmobtainedthatvalueorhigher.Thisplotcangiveabroader
                     modelis then trained with optimal hyperparameters using the en-                                   view of the likelihood of successfully achieving a high accuracy
                     tire training data, and evaluated on a held-out test data (25%). We                               for each algorithm, while keeping all the information about each
                     implementedaPythonwrappertotuneeverymethodintothesame                                             algorithm in the same plot. Using this same plot, we can calculate
                     grid search pipeline using scikit-learn.                                                          the area under the curve (AUC) for each individual algorithm as an
                                                                                                                       aggregatedmeasure:avalueof1.0indicatesthatall30runsachieved
                                                                                                                       the maximumÌ†µÌ±Ö2 = 1, whereas a value of 0.0 indicates that all runs
                     3.3      Performanceanalysis                                                                      resulted in Ì†µÌ±Ö2 ‚â§ 0. The AUC provides an estimate of the method‚Äôs
                    Werepeatedtheseexperiments30timesforeachdatasetwithfixed                                           potential to achieve high performance across multiple runs. The
                     and distinct seeds, storing all necessary information to process the                              performance plot can be generated for each individual dataset as
                     aggregated results. For SRBench 2.0, we adopted the performance                                   a result of the 30 independent runs or we can plot the aggregated
                     profile plot [19], which describes the empirical distribution of the                              performance over all datasets using an aggregation function (e.g.,
                     obtained results. This plot illustrates the probability of achieving a                            mean, median, max, min) for each dataset. In this paper, we use
                     performance greater than or equal to a given Ì†µÌ±Ö2 threshold for all                                the maximumvalueastheaggregation function, which reflects the
                  Call for Action: Towards the Next Generation of Symbolic Regression Benchmark
                  best-case performance of each algorithm across the 30 runs. The
                                                                                                                ‚àí2
                  project website reports both individual and aggregated plots.                               10
                                                                                                                ‚àí3
                      Tomeasuremodelsize, we convert the models to a SymPy [48]                               10
                                                                                                                ‚àí4
                  compatible expression and count the number of nodes. This is                                10
                                                                                                                ‚àí5
                  necessarysincetheinternalcomplexitymeasuresofeachalgorithm                                  10
                                                                                                                ‚àí6
                  maydifferfromthestandardwayofcountingnodes.Theexpression                                    10
                                                                                                             gy consumption (kWh)
                                                                                                                ‚àí7
                  wasnotsimplified(exceptfortrivialsimplificationssuchasconstant                              10
                  merging) before counting the nodes, as simplification using SymPy                          Ener
                  is unreliable and can increase the model size in the process [13].
                                                                                                                 1
                                                                                                                 0
                  3.4      Benchmarkinfrastructure                                                               1
                                                                                                                 2
                  Conducting extensive experiments across multiple algorithms and                                0
                  datasets requires substantial computational resources. Variability in                          1
                                                                                                               raining time (s)
                                                                                                                 3
                                                                                                               T
                  hardwareworkloadornon-homogeneousclusternodescanmakeit                                         0
                                                                                                                 1
                  challengingtoestablishafixedcomputationalbudgetorensurethat                                                                                                  1 Hour
                                                                                                                                                T
                  all methods are evaluated under identical conditions. Using fixed                                                       n
                                                                                                                                                               es
                                                                                                                     ee
                                                                                                                           on
                                                                                                                                                                     ols
                                                                                                                     r
                                                                                                                                             YSR
                  resourcescanhelpmitigatethe"hardwarelottery"effect[24],where                                       T
                                                                                                                                                                              TIR
                                                                                                                                                   GOMEA
                                                                                                                                                                        FFX
                                                                                                                                                                                       AFP
                                                                                                                                                            E2E
                                                                                                                                                                                         ITEA
                                                                                                                                                               EQL
                                                                                                                                                     FEA
                                                                                                                                                  P
                                                                                                                                     Bingo
                                                                                                                                            TPSR
                                                                                                                                                   -
                                                                                                                                        Brush
                                                                                                                                                                     Rils-R
                                                                                                                       PS-
                                                                                                                                                                               EPLEX
                                                                                                                                            gplear
                                                                                                                         QLattice
                                                                                                                                                               AFP_FE
                                                                                                                              Oper
                                                                                                                              G. Engine
                                                                                                                                                                                           GPZGD
                  an approach succeeds due to its compatibility with the available                                                                                              AFP_EHC
                                                                                                                                                                NeSymR
                  software and hardware rather than its inherent merit.                                                                             GP
                      Experiments were executed on a single computing cluster. Each                       Figure 1: Median energy consumption (kWh) and training
                  job was allocated 10 GB of RAM, and GPUs were provided for                              runtimeforeachalgorithm.
                  methodsthatsupportthem(seetherunsoncolumninTable2).Jobs
                  werenotallowedtospawnsubprocessesorutilize multiple cores.
                  Thetimebudgetwassetto6hoursforhyperparametersearch and                                  calculating the proportion of runs where the maximum Ì†µÌ±Ö2 exceeds
                  1 hour for training the algorithm with the optimal configuration,                       a given threshold Ì†µÌ±•.
                  after which a SIGALRM signal was sent to terminate the process.                            Figure 3 shows a cluster map of the AUC from the performance
                      Energyconsumptionwasestimatedafterhyperparametertuning                              plot for each pair of dataset and algorithm on the test set. The
                  to avoid additional overhead, using the eco2AI library [7], which                       size of each cell is proportional to the size of the best-performing
                  derives estimates from standard Linux commands monitoring CPU                           final expression across the 30 runs. Higher values and smaller cells
                  and memory usage. While not exact, this provides a rough yet                            indicate better performance. At the top of this plot, we can observe
                  informative profile of resource usage.                                                  a hierarchical clustering of the algorithms, grouping the methods
                  3.5      DataAvailability                                                               based on performance, and the datasets based on their dif√èculty
                                                                                                          to solve. The best algorithm for each dataset is highlighted with a
                      Thedatasets were submitted to PMLB [49, 52]. We released all                        black edge color on its cell.
                  experiment scripts and parameters to ensure transparency and                               For the phenomenological & first-principles track, Figure 4 dis-
                  reproducibility. We also restructured SRBench using containerized                       plays the Pareto front on the test set of the most accurate solutions
                  environments to ensure consistent execution across systems and                          obtained by SR algorithms and compared to the current accepted
                  simplify standalone use. The project is available at https://github.                    hypothesis (star). We observe that noise ‚Äîwhether inherent in real-
                  com/cavalab/srbench/tree/srbench_2025.                                                  world data or synthetically added in the case of generated data‚Äî
                                                                                                          can cause the ground-truth hypotheses to fall short of achieving
                  4 Results                                                                               a perfect Ì†µÌ±Ö2. Asterisks next to the algorithm names indicate those
                                                                                                          closest (based on Euclidean distance over the normalized axes) to
                      Within the specified time budget, the total runtime on a single                     the ground-truth. The final equations closest to the governing mod-
                  corewouldbe1year,43weeks,and5days.Usingeco2AItomonitor                                  els are shown in Table 3 along with their corresponding Ì†µÌ±Ö2 scores
                  thefinaltrainingphase,wereporttheenergyandtimeconsumption                               andsizes.
                  inFig.1.Wenoticethatthesemeasurementsareinfluencednotonly                               5 Discussion
                  bythechoiceofprogramminglanguageandimplementationdetails
                  but also by the internal decisions of the algorithms to terminate
                  execution before reaching the maximum time.                                             5.1     Energyconsumptionandexecutiontime
                      In Figure 2, we present the performance plot with the AUC val-
                  ues based on the highest empirically observed Ì†µÌ±Ö2 on the test set                       Energyconsumptionprofiles in Figure 1 reveal distinct patterns for
                  across 30 runs, representing an optimistic perspective on model                         different algorithms.
                  performance, assuming a user performs the same number of runs.                             Thetoptwomostenergy-hungryalgorithmsarePython-based,
                                                                                                                                                              ++
                  Theplot shows the empirically observed likelihood of obtaining a                        and the third, Operon, is implemented in C            but includes an inner
                  maximumÌ†µÌ±Ö2 score for black-box problems under suf√ècient repe-                           hyperparameter tuning step. Some methods, such as FEAT and
                  titions. The probability on the Ì†µÌ±¶-axis, P[Ì†µÌ±Ö2 ‚â• Ì†µÌ±•], is estimated by                   Brush, implement a max_stall parameter, which halts execution
                                                                                                                                                                                                                                  Imai Aldeia et al.
                             1.0                                                                                                             1.0
                                                                                                                                            )
                                                                                                                                            7
                                                                                                                                                          G. Engine
                            )
                                                                                                                                            6
                            5
                                                                                                                                            .
                            .
                                                                                                                                            0
                            0
                                                                                                                                                                                FFX
                             0.8                                                                                                             0.8
                                                                                                                                            ‚àí
                            ‚àí
                                                                                                                                                                                                     gplearn
                                                                                                                                            5
                            0
                                                                                                                                            .
                            [
                                                                                                                                            0
                                                                                                                                            [
                            ‚àà
                             
                             0.6                                                                                                             0.6
                                                                                                                                            ‚àà
                            C
                                                                                                                                             
                                                                NeSymRes
                                                                                                                                                                                                                           PYSR
                                                                                      E2E
                            U
                                                                                                                                            C
                                           uDSR
                            A
                                                                                                                                            U
                             
                            -
                                                                                                                                            A
                             
                                                                                                                                             
                            ]
                             0.4                                                                                                             0.4
                                                                                                                                            -
                            x                                                                                                                
                                                                                                                                            ]
                                                                                                           BSR                              x
                            ‚â•
                           2
                                                                                                                                            ‚â•
                                                                                                                                                                                             AFP_FE
                            R            uDSR              E2E               TPSR                                                                        G. Engine         gplearn
                                                                                                                                           2
                                         (AUC=0.22)        (AUC=0.41)        (AUC=0.47)                                                                  (AUC=0.51)        (AUC=0.57)        (AUC=0.66)
                             0.2                                                                                                             0.2
                            [                                                                                                               R
                            P
                                                                                                                                                                                                                                                AFP_FE
                                                                                                                                            [
                                                                                                                                 TPSR
                                         NeSymRes          BSR                                                                                           FFX               PYSR
                                                                                                                                            P
                                         (AUC=0.3)         (AUC=0.46)                                                                                    (AUC=0.51)        (AUC=0.59)
                                 0.0                0.2                 0.4                0.6                 0.8                1.0            0.0                0.2                 0.4                0.6                 0.8                1.0
                                                                                                                                                                                      EQL
                                                                  TIR
                             1.0                                                                                                             1.0
                            )
                                                                                                                                            )
                            7
                                   AFP                                                                                                           Bingo       QLattice
                                                       AFP_EHC
                            .
                                                                                                                                            0
                                                                                                                                            .
                            0
                                                                                                                                            1
                            ‚àí
                                                                                                                                                                                                     ITEA
                                                                                PS-Tree
                                                                                                                                            ‚àí
                             0.8                                                                                                             0.8
                            7
                                                                                                                                            7
                                                                                            Operon
                                                                                                                                            .
                            6
                            .
                                                                                                                                                                                                                   Brush
                                                                                                                                            0
                            0
                                                                                                                                            [
                            [
                                                                                                         EPLEX
                                                                                                                                            ‚àà
                             0.6                                                                                                             0.6
                            ‚àà
                                                                                                                                                                                                                                   GP-GOMEA
                                                                                                                                             
                             
                                                                                                                                            C
                            C
                                                                                                                     GPZGD
                                                                                                                                            U
                            U
                                                                                                                                            A
                                                                                                                                             
                            A
                             
                                                                                                                                            -
                                                                                                                                 FEAT
                                                                                                                                             
                            -
                             0.4                                                                                                             0.4
                             
                                                                                                                                            ]
                            ]
                                         AFP               PS-Tree           GPZGD
                                                                                                                                                         Bingo             ITEA              GP-GOMEA
                            x                                                                                                               x                                                                                        Rils-Rols
                                         (AUC=0.67)        (AUC=0.68)        (AUC=0.69)
                                                                                                                                                         (AUC=0.7)         (AUC=0.71)        (AUC=0.72)
                                                                                                                                            ‚â•
                            ‚â•
                                         AFP_EHC
                                                           Operon            FEAT
                                                                                                                                                         QLattice          Brush             Rils-Rols
                                                                                                                                           2
                           2
                                                           (AUC=0.68)        (AUC=0.69)
                                         (AUC=0.67)                                                                                                      (AUC=0.7)         (AUC=0.71)        (AUC=0.72)
                            R0.2                                                                                                            R0.2
                                                                                                                                            [
                            [
                                                           EPLEX
                                         TIR                                                                                                             EQL
                                                                                                                                            P
                            P
                                                           (AUC=0.69)
                                         (AUC=0.68)                                                                                                      (AUC=0.71)
                                 0.0                0.2                 0.4                0.6                 0.8                1.0            0.0                0.2                 0.4                0.6                 0.8                1.0
                                                                                2                                                                                                              2
                                                                              R  (max)                                                                                                       R  (max)
                        Figure 2: Performance plots for the black-box track, where the lines represent the probability of obtaining a given empirically
                        observedÌ†µÌ±Ö2 value when running the experiments multiple time (i.e., max aggregation).
                        Table 3: Equation from the Pareto front closest to the gov-                                                          times than other GPU-based methods. uDSR was not compatible
                        erning models.                                                                                                       with the library during the experiments.
                                Dataset        Ì†µÌ±Ö2     Size Symbolicmodel                                                                    5.2        Performanceanalysisoftheblack-box track
                             absorption        1.0     6      0.24 + 1.76tanhÌ†µÌ±•                                                              Theperformance curves in Figure 2 displays the expected probabil-
                                     bode      1.0     8      0.34Ì†µÌ±í1.51¬∑Ì†µÌ±õ ‚àí 0.87                                                           ity of achieving maximum performance at different Ì†µÌ±Ö2 thresholds
                                  hubble       0.86 3         0.090 + Ì†µÌ∞∑                                                                     across a variety of problems. Some methods (shown in the top-left
                               ideal_gas       0.99 14        0.69 ¬∑ logÌ†µÌ±õ + 1.64 ‚àí 0.89 + 0.48 ‚àóÌ†µÌ±í‚àíÌ†µÌ±â                                       subplot, with AUCbelow0.5)exhibitaprobabilitylowerthan1.0of
                                                                                                                                             achieving a strictly positive Ì†µÌ±Ö2, suggesting that they may struggle
                                   kepler      1.00 10        1.98 ‚àó tanh (0.66 ¬∑ Ì†µÌ±é ‚àí 0.56) + 0.78                                          to generalize; fail to capture intrinsic data structures, or produce
                                   leavitt     0.97 3         ‚àí0.94 ¬∑ logÌ†µÌ±É                                                                  low-quality models.
                                 newton        0.90 16        0.34 ¬∑        Ì†µÌ±ö1 0.09 + 1.13                                                       Mostofthetop-performing algorithms incorporate constant op-
                                                                       0.83¬∑Ì†µÌ±ö2‚àí Ì†µÌ±ö1                                                         timization as a local search step ‚Äîusing linear, gradient-based, or
                                                                              ‚àö                                   Ì†µÌ±õÌ†µÌ±¢+0.3
                                  planck       1.00 24        0.023 ¬∑ log ( Ì†µÌ±õÌ†µÌ±¢ + 0.38 ‚àí 0.04) ‚àí 0.31 ¬∑ Ì†µÌ±á+0.94 + 0.41                      non-linear methods‚Äì, highlighting the importance of parameter op-
                                 rydberg       1.00 21        1.01 ¬∑ Ì†µÌ±í0.1¬∑Ì†µÌ±í1.73¬∑Ì†µÌ±õ1‚àí1.31¬∑Ì†µÌ±õ2 ‚àí Ì†µÌ±í‚àí0.69¬∑Ì†µÌ±õ1                                 timizationforachievinghighperformance.AmongthehighestAUC
                               schechter       1.00 13        0.748‚àí0.274¬∑(log (163.992 ¬∑ Ì†µÌ∞ø + 106.737)+1.414¬∑Ì†µÌ∞ø)                            scores are GP-based algorithms such as Brush, GP-GOMEA, and
                        supernovae_zr          1.00 29         (sin (0.2 ¬∑ Ì†µÌ±• ¬∑ Ì†µÌ±í‚àíÌ†µÌ±• )   ‚àí 0.04)         ¬∑   (Ì†µÌ±•   + 0.72 ¬∑                 GPZGD.Notably,GPZGDisaGPalgorithmthatstandardizesthe
                                                              sin (5.46 ¬∑ Ì†µÌ±• + 0.76) ‚àí 0.16) ‚àí sin (Ì†µÌ±• + 0.18)                               features using Z-score and performs parameter optimization. Other
                            tully_fisher       0.93 3         ‚àí0.93 ¬∑ ŒîÌ†µÌ±â                                                                    methods, such as Rils-Rols, also performs constant optimization.
                                                                                                                                                  TheselectionofdatasetsinFigure3revealsthe‚Äúnofreelunch‚Äùas-
                                                                                                                                             pectofthecurrentstateofSR‚Äîthereisnoalgorithmthatperforms
                                                                                                                                             exceptionally well on all datasets. Likewise, there is no algorithm
                        if no improvement is observed in an inner validation partition af-                                                   that fails to solve all problems. Hierarchical clustering shows two
                        ter a number of iterations. While this feature does not necessarily                                                  main clusters of good- and bad-performing algorithms based on
                        correlate with these methods being the best performers in terms of                                                   the AUC of the performance curves. A closer inspection of Fig-
                        energy consumption, it could be further explored for more compu-                                                     ure 3 allows for a better understanding of challenges for existing
                        tationally demanding algorithms.                                                                                     algorithms.
                             GPU-basedalgorithms, such as TPSR, E2E, and NeSymRes, do                                                             Furthermore, the blue cells reveals a cluster of datasets in which
                        not show much difference in energy consumption compared to                                                           thetop-performingalgorithmsobtainasimilarlygoodresult.Atthe
                        CPU-basedalgorithms. In particular, TPSR, which uses traditional                                                     sametime, we can readily see the trade-off between performance
                        parameter optimization methods, demonstrated higher execution                                                        AUCandsize.Forexample,inthe557 dataset, we can see that FFX
                                                             Call for Action: Towards the Next Generation of Symbolic Regression Benchmark
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             0.0                  0.2                 0.4                  0.6                 0.8                 1.0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 AUC Score
                                                                     0.04             0.04              0.04              0.14             0.18              0.06              0.00             0.00              0.16             0.07              0.11              0.27             0.26              0.20              0.26             0.25              0.31             0.22              0.25              0.24             0.20              0.29              0.22             0.26              0.14
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               522_pm10
                                                                     0.13             0.08              0.14              0.37             0.34              0.18              0.03             0.04              0.25             0.14              0.23              0.13             0.21              0.24              0.11             0.18              0.15             0.32              0.21              0.23             0.30              0.32              0.37             0.22              0.36
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               678_visualizing_environmental
                                                                     0.35             0.32              0.39              0.32             0.48              0.33              0.26             0.13              0.17             0.26              0.25              0.33             0.36              0.32              0.36             0.38              0.38             0.56              0.45              0.39             0.51              0.48              0.42             0.52              0.40
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               192_vineyard
                                                                     0.03             0.05              0.08              0.27             0.26              0.20              0.05             0.38              0.39             0.27              0.15              0.38             0.38              0.36              0.40             0.38              0.40             0.40              0.39              0.38             0.37              0.38              0.38             0.37              0.20
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1028_SWD
                                                                     0.06             0.04              0.11              0.43             0.40              0.36              0.20             0.47              0.46             0.45              0.44              0.42             0.46              0.44              0.46             0.46              0.46             0.45              0.46              0.45             0.47              0.46              0.45             0.45              0.43
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1199_BNG_echoMonths
                                                                     0.53             0.35              0.78              0.84             0.74              0.69              0.72             0.19              0.54             0.47              0.59              0.66             0.73              0.74              0.69             0.74              0.74             0.66              0.66              0.59             0.66              0.66              0.72             0.66              0.77
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               210_cloud
                                                                     0.42             0.19              0.07              0.71             0.62              0.39              0.46             0.60              0.35             0.31              0.46              0.62             0.66              0.57              0.69             0.60              0.64             0.56              0.71              0.71             0.70              0.67              0.72             0.10              0.00
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1089_USCrime
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           dataset
                                                                     0.20             0.05              0.13              0.55             0.56              0.49              0.38             0.59              0.60             0.58              0.57              0.58             0.59              0.58              0.60             0.60              0.60             0.62              0.60              0.56             0.60              0.60              0.59             0.61              0.55
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1193_BNG_lowbwt
                                                                     0.04             0.03              0.07              0.09             0.00              0.07              0.06             0.91              0.78             0.36              0.17              0.63             0.80              0.84              0.86             0.88              0.80             0.86              0.88              0.84             0.85              0.87              0.89             0.84              0.86
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               557_analcatdata_apnea1
                                                                     0.29             0.04              0.28              0.47             0.57              0.08              0.52             0.00              0.57             0.29              0.26              0.86             0.84              0.94              0.95             0.93              0.95             0.92              0.96              0.94             0.82              0.91              0.90             0.96              0.79
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               650_fri_c0_500_50
                                                                     0.24             0.07              0.74              0.40             0.74              0.53              0.42             0.57              0.55             0.14              0.06              0.87             0.86              0.94              0.94             0.95              0.88             0.94              0.94              0.94             0.92              0.90              0.93             0.94              0.86
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               579_fri_c0_250_5
                                                                     0.00             0.04              0.86              0.36             0.68              0.19              0.47             0.94              0.85             0.11              0.11              0.91             0.88              0.92              0.92             0.93              0.97             0.97              0.97              0.92             0.96              0.92              0.97             0.97              0.87
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               606_fri_c2_1000_10
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Best AUC on dataset
                                                                                                                                                                                                                                                                                                                                                                                                                        T
                                                                                                                                               n
                                                                         es
                                                                                                                                                                                                                                                                                                                                                                   ee
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              on
                                                                                                                                                                 SR
                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ols
                                                                                                                                                                                                                                                                           TIR
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Small (size < 50)
                                                                                                                                                                                                                                                                                                                                                                   r
                                                                                                                                                                                                    FFX
                                                                                                                                                                                                                                       AFP
                                                                                                                                                                                   E2E
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                YSR
                                                                                                                                                                                                                                                                                                                                                                                    EQL
                                                                                                                                                                 B
                                                                                                                                                                                                                                                                                                                                                                   T
                                                                                                                                                                                                                                                                                                                                                                                                                                         ITEA
                                                                                                                                                                                                                                                                                                                                                                                                                        FEA
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                P
                                                                                                            TPSR
                                                                                          uDSR
                                                                                                                                                                                                                                                                                            Bingo
                                                                                                                                                                                                                                                                                                                                                                                                      Brush
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Medium (size < 100)
                                                                                                                                                                                                                                                                                                              EPLEX
                                                                                                                                                                                                                      AFP_FE
                                                                                                                                                                                                                                                                                                                                                                   PS-
                                                                                                                                                                                                                                                                                                                                                 GPZGD
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Oper
                                                                                                                                                                                                                                                                                                                                GOMEA
                                                                                                                                               gplear
                                                                                                                                                                                                                                                                                                                                -
                                                                                                                                                                                                                                                                                                                                                                                                                                                           QLattice
                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Rils-R
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Large (size >= 100)
                                                                                                                                                                                                                                                         AFP_EHC
                                                                                                                              G. Engine
                                                                         NeSymR
                                                                                                                                                                                                                                                                                                                                GP
                                                             Figure3:ClustermapoftheAreaUndertheCurve(AUC)ofExpectedPerformancesacrossthe30independentrunsissegregated
                                                             byalgorithmanddataset.Highervaluesindicatebetterperformance,whilelargercellsrepresentworsemodelsize.
                                                                                                     Absorption                                                                                            Bode                                                                                       Hubble                                                                                     Ideal Gas                                                                                         Kepler                                                                                        Leavitt
                                                                                                                                                                                                                                                                                                                                                                                                                                                          17.5
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            GPZGD
                                                                                                                                                                20.0                                                                                                                                                                                             25                                                                                                                                                                                          10
                                                                                                                                                                                                              x
                                                                                                                                                                                                                                                                   50
                                                                                                                                                                                                                                                                                                                                                                               n‚ãÖR‚ãÖT
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        3
                                                                                                                                                                                                                                                                                 D‚ãÖH
                                                                                                                                                                                   0.4+0.3‚ãÖe
                                                                                    logx                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   log         P‚ãÖŒ±+Œ¥
                                                                       12
                                                                                                                                                                                                                                                                                           0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  10
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  k‚ãÖa
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                FEAT
                                                                                                                                                                                                                                                                                                                                                                                   V
                                                                                                                                                                                                                                                                                                                                                                                                                                                          15.0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                             1000‚ãÖ
                                                                                                                                 QLattice                                                                                      QLattice                                                                                           Bingo                                                                                    QLattice                                                                                                                                                                                     QLattice
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ‚àö
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   7.5
                                                                                                                                                                17.5
                                                                                                                                                                                                                                                                   40
                                                                       10
                                                                                                                                                                                                                                                                                                                                                                 20
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                8
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Rils-Rols
                                                                                                                                                                                                                                                                                                                                                                                                                                                          12.5
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     QLattice*
                                                                                                                                                                15.0
                                                                                                                                 Rils-Rols
                                                                                                                                                                                                                                                                                                                                                                                                              Rils-Rols*
                                                                          8
                                                                                                                                                                                                                                                                   30
                                                                                                                                      ITEA*
                                                                                                                                                                                                                                                                                                                                                                                                                                                          10.0
                                                                                                                                                                12.5
                                                                                                                                                                                                                                                                                                                                                                 15
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                6
                                                                          6
                                                                                                                                                                                                                                                                                                                            Operon
                                                                                                                                                                                                                                                                   20
                                                                                                                                                                                                                                                                                                                                                                                                                                                             7.5
                                                                    Model Size
                                                                                                                                                                10.0
                                                                                                                                                                                                                          Genetic Engine
                                                                                                                                                                                                                                                                                                             Genetic Engine
                                                                          4
                                                                                                                                                                                                                                                                                                                                                                 10
                                                                                                                                                                                                                                                                                                                                                                                 AFP_FE                                                                                        gplearn                                                                                                                           ITEA*
                                                                                                                                                                                                                                                                                                                                                                                                                                                             5.0                                                                                                4
                                                                                                                                                                                                                                                                   10
                                                                                                                                                                   7.5
                                                                                                                                                                                                                                                                                                                     gplearn*
                                                                          2
                                                                                                                                                                                                                             Rils-Rols*
                                                                                                                                                                                                                                                                                                                                                                                                                                                             2.5
                                                                                    0.7                0.8                 0.9                 1.0                                     0.4               0.6               0.8               1.0                            0.0                             0.5                             1.0                                  0.96                         0.98                        1.00                          0.90                           0.95                           1.00                                   0.94               0.96                0.98
                                                                                                         Newton                                                                                          Planck                                                                                      Rydberg                                                                                    Schechter                                                                               Supernovae Zr                                                                                      Tully Fisher
                                                                                                                                                                                                                                                                                                                                                                                                                                                               50
                                                                                                                                                                                                                                                                                                                             QLattice                                                                                      Operon*
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Operon
                                                                       25
                                                                                                                                                                                                   3
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2.5
                                                                                                                                                                                                                                                                                                                                                                                                           ‚àíL
                                                                                                                                                                                                                                                                                           1                                                                                                                                                                                             A
                                                                                    G‚ãÖm ‚ãÖm
                                                                                                                                                                                                                                                                                                                                                                               phi
                                                                                                                                                                                   2‚ãÖh* ‚ãÖnu
                                                                                             1      2
                                                                                                                                                                                                              1
                                                                                                                                                                                                                                                                                                                                                                                    ast
                                                                                                                                                                                                                                                                                                                                                                                             L
                                                                                                                                                                                                                                                                                                                                                                                                  Œ±
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           k‚ãÖŒîV
                                                                                                                                                                                                                                                                   25
                                                                                                                                                                                                                                                                                                                                                                                                           L
                                                                                                                                                                                                      ‚ãÖ
                                                                                                                                                                                                                                                                                                                                                                                                            ast
                                                                                                                                                                                                                                                                                                                                                                                         ‚ãÖ           ‚ãÖ e
                                                                                                                                                                                                                                                                                          1       1
                                                                                                                                                                                                                                                                                                                                                                                                                                Bingo                                               (C‚ãÖt)       (‚àíD‚ãÖt)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             10
                                                                                                                                                                                                           h‚ãÖnu
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         GPZGD
                                                                                             2
                                                                                                                                                                                            2
                                                                                                                                                                                                                                                                                                                                                                 12
                                                                                                                                                                                                                                                                                                                                                                                                                                                                             B‚ãÖe          +e
                                                                                                                                                                                                                                                                                 (R ‚ãÖ(        ‚àí ))
                                                                                                                                                                                                                                                                                                                                                                                L           L
                                                                                                                             GP-GOMEA                                                                                      GP-GOMEA                                                                                                                                                                                                                                                                                                                                                                                     QLattice
                                                                                            r
                                                                                                                                                                                         (c )
                                                                                                                                                                                                                                                                                    H
                                                                                                                                                                                                                                                                                                                                                                                  ast         ast
                                                                                                                                                                                                           k ‚ãÖT
                                                                                                                                                                                                                                                                                           2       2
                                                                                                                                                                                                            B
                                                                                                                                                                                                         e      ‚àí1
                                                                                                                                                                     40
                                                                                                                                                                                                                                                                                          n      n
                                                                                                                                                                                                                                                                                                                                                                                                                                                               40
                                                                                                                                                                                                                                                                                           1       2
                                                                                                                                                                                                                                                                                                                                 Bingo
                                                                                                                                                                                                                                                                                                         gplearn
                                                                                                                                                                                                                                                                                                                                                                                                                                 AFP
                                                                                                                                                                                                                                    Bingo
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       QLattice
                                                                       20
                                                                                                                       Rils-Rols*
                                                                                                                                                                                                                                                                   20
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                8
                                                                                                                                                                                                                                   PYSR*
                                                                                                                                                                                                                                                                                                                                                                                                                                                               30
                                                                                                                                                                     30
                                                                                                                                                                                                                                                                                                                                                                 10
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Genetic Engine
                                                                                                                                                                                                                                                                                                                         Operon*
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Bingo
                                                                       15
                                                                                                                                                                                                                                Operon
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                6
                                                                                                                                                                                                                                                                   15
                                                                                                                                                                                                                                                                                                                                                                                                                                                               20
                                                                                                                                                                     20
                                                                    Model Size
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Genetic Engine
                                                                                                                                                                                                                                                                                                                                                                    8
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          PYSR*
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NeSymRes
                                                                                       Genetic Engine                                                                                Genetic Engine                                                                                Genetic Engine                                                                                ITEA                                                                                                                                                                                                                                     ITEA*
                                                                       10
                                                                                                                                                                                                                                                                                                                                                                                                                                                               10
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                4
                                                                                                                                                                                                                                                                   10
                                                                                                                                                                     10
                                                                                                                                                                                                                                                                                                                                                                    6
                                                                                        0.4               0.6               0.8                1.0                           0.94                0.96                 0.98                 1.00                              0.997               0.998               0.999               1.000                                   0.94              0.96               0.98              1.00                             0.8                            0.9                            1.0                             0.0                             0.5                            1.0
                                                                                                          R2 Test                                                                                       R2 Test                                                                                       R2 Test                                                                                       R2 Test                                                                                       R2 Test                                                                                       R2 Test
                                                             Figure 4: Pareto plots for the phenomenological & first-principles track, with model sizes onÌ†µÌ±¶ axis, and Ì†µÌ±Ö2 on Ì†µÌ±• axis. The star
                                                             markerdenotestheground-truthexpressionperformance,alsodenotedintheboxinsideeachsubplot.Onlythefirstfrontis
                                                             plotted for clarity.
                                                             obtained the best AUC score, while Rils-Rols obtained a slightly                                                                                                                                                                                                                                       specific datasets. This highlights that no single algorithmdominates
                                                            worseAUCresultbutwithasmallerexpression.Interestingly,some                                                                                                                                                                                                                                              across all objectives; each has its own strengths depending on the
                                                             methodsthatperformpoorlyoverallstillachievethebestresultson                                                                                                                                                                                                                                            problem characteristics.
                                                                                                                                            Imai Aldeia et al.
               5.3    Towardsaparameter-lessexperience                                  deprecated. This guideline may also encourage the development
               Setting the hyper-parameters of SR algorithms often requires care-       of more robust algorithms that perform consistently well across
               ful experimentation with a number of different settings that may         diverse problems.
               affect the final result. For general users, this creates a significant      Another important point is the fair comparison between GPU-
               barrier: they must either rely on default values or conduct extensive    based and CPU-based approaches, as well as single-core versus
               experimentation to find the best configurations. The field should        multi-coreimplementations.Whileeveryalgorithmwithamax_time
               movetowardaparameter-lessexperiencebydevelopingalgorithms                hyperparameter respects its limit, some terminate far earlier (e.g.,
               that require as few hyperparameters as possible, unless they are         NeSymRes), potentially skewing performance evaluations. A more
               intuitive and domain-related. Good practices involve carefully se-       precise implementation of time management would allow for a
               lecting default parameters and, if possible, integrating internal        fairer comparisonofcomputationalef√èciencyacrossdifferentmeth-
               hyperparameter tuning (such as Operon with Optuna) or making             ods.
               parameters adaptive (as in PySR).                                           Additionally, this benchmark could provide significantly more
                  Weproposestandardizing the hyperparameter tuning by requir-           data-driven insights for algorithm development by incorporating
               ing each method to define a small set (e.g., four) of hyperparameter     appropriate metadata (e.g., a set of keywords), offering a direct
               configurations. A simple grid search is then applied using these         waytolinkmethodologiestoresults. Similarly, algorithms could
               configurations, each limited to a maximum runtime (e.g., one hour).      generate post-run metadata (e.g., results of internal grid search,
               Theoff-the-shelf configuration should automatically be included          number of local search iterations) as part of their output. This
               bytheexperiment scripts, as we observed some methods perform             would facilitate a deeper understanding of algorithmic behavior
               best with their default settings.                                        andenablemoreprecisefine-tuningofspecific methods, ultimately
                                                                                        leading to more informed improvements in SR techniques.
               5.4    Solving empirical and theoretical problems
               Thephenomenological & first-principles track provides a detailed         6 Conclusionsandfuturework
               view of the trade-off between model size and accuracy, using es-
               tablished models as reference points. Fig. 4 shows that, for almost         This paper updates the current SRBench aiming to improve the
               every problem, no SR method could discover an expression that            benchmarkofsymbolicregression as a community accepted stan-
               strictly dominates the original equation. This occurs because meth-      dard. We increased to almost twice the number of SR algorithms
               ods either generate a larger expression with higher accuracy or a        than the previous version, and provided an alternative and more
               smaller expression with lower accuracy. Table 3 further illustrates      detailed visualizations depicting the current state of SR.
               this challenge ‚Äî only ITEA successfully retrieved one of the first-         Toensureaconstant improvement of SRBench, we need active
               principles equations (Leavitt). In other cases, SR methods either        participation of the community to ensure compatibility with the
               produced a more complex yet more accurate expression or a sim-           current experiments and a correctly working implementation. We
               pler but less precise model. For the Schechter dataset, Operon found     call for action from researchers to actively contribute to discussions,
               reasonable alternatives without significantly deviating from the         share newideas,provideconstructivecriticism, and propose correc-
               complexity-accuracy balance of the original equation.                    tions. A continuously evolving benchmark will help drive progress
                  Wenotice that some first-principle equations fails to achieve         in SR. Achieving a mature state for the field should be a shared
               perfect Ì†µÌ±Ö2 due to noise in the original data ‚Äî notably in the Hubble    priority, especially given the growing emphasis on transparency in
               andTully-Fisher datasets. In such cases, obtaining a higher Ì†µÌ±Ö2 with     machine learning and the increasing interest in scientific ML.
               a more complex model often indicates that the model is fitting the
               noise rather than the underlying relationship. This is evident in the    Acknowledgments
               equations found for Hubble and Tully-Fisher, which deviate from          Toeveryoneinvolved in the discussions to improve SRBench. To
               the ground-truth equation by only a small edit distance.                 PMLBmaintainersforthehelp. To all past researchers who some-
                  In contrast, the Absorption and Bode datasets represent phe-          how enabled us to use data or methods from their experiments,
               nomenological problems with no known ground-truth equations.             makingscience open.
               Apossible venue for future investigation, is the incorporation of           Alcides Fonseca is supported by FCT through the LASIGE Re-
               the uncertainties information of the data, leading to a better mea-      search Unit, ref. UID/000408/2025. Fabricio Olivetti de Franca is
               surement for the model accuracy.                                         supported by Conselho Nacional de Desenvolvimento Cient√≠fico e
               5.5    Currentdif√èculties and limitations                                Tecnol√≥gico (CNPq) grant 301596/2022-0. Guilherme Imai Aldeia is
               Duringthebenchmarkingprocess,anumberofbugshadtobefixed                   supportedbyCoordena√ß√£odeAperfei√ßoamentodePessoaldeN√≠vel
               for most evaluated algorithms. Few of the corresponding reposi-          Superior (CAPES) finance Code 001 and grant 88887.802848/2023-
               tories are being maintained, requiring a joint decision of which         00.
               algorithm to deprecate and remove from further benchmarking.             References
                  Weproposeestablishing deprecation rules to maximize bench-
               markingefforts.Asimplecriterioncouldbe:ifanalgorithm‚Äôsrepos-              [1] Jamal A Abdalla, MZ Naser, Saleh M Alogla, Alireza Ghasemi, and Ahmad Naser.
               itory is no longer actively maintained and it does not appear on at          2024. Evaluation and Benchmarking of Performance of Machine Learning and
                                                                                            SymbolicRegression:Datasets,SoftwareToolsandPredictionModels. ESGeneral
               least one of the Pareto fronts in the black-box datasets, it should be       7 (2024), 1352.
                   Call for Action: Towards the Next Generation of Symbolic Regression Benchmark
                     [2] Ignacio Arnaldo, Krzysztof Krawiec, and Una-May O‚ÄôReilly. 2014. Multiple               [23] Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting
                         regression genetic programming. In Proceedings of the 2014 annual conference on              machine. The Annals of Statistics 29, 5 (Oct. 2001). doi:10.1214/aos/1013203451
                         genetic and evolutionary computation. 879‚Äì886.                                         [24] Sara Hooker. 2020. The Hardware Lottery. arXiv:2009.06489 [cs.CY] https:
                     [3] Deaglan J. Bartlett, Harry Desmond, and Pedro G. Ferreira. 2024. Exhaustive                  //arxiv.org/abs/2009.06489
                         Symbolic Regression. IEEE Transactions on Evolutionary Computation 28, 4 (Aug.         [25] Viktor Hru≈°ka, Aneta Furmanov√°, and Michal Bedna≈ô√≠k. 2025. Analytical formu-
                         2024), 950‚Äì964. doi:10.1109/tevc.2023.3280250                                                lae for design of one-dimensional sonic crystals with smooth geometry based
                     [4] Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Gi-                   on symbolic regression. Journal of Sound and Vibration 597 (2025), 118821.
                         ambattista Parascandolo. 2021. Neural Symbolic Regression that scales. In Pro-               doi:10.1016/j.jsv.2024.118821
                         ceedings of the 38th International Conference on Machine Learning (Proceedings         [26] Edwin Hubble. 1929. A relation between distance and radial velocity among
                         of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.).                 extra-galactic nebulae. Proceedings of the National Academy of Sciences 15, 3
                         PMLR,936‚Äì945. https://proceedings.mlr.press/v139/biggio21a.html                              (March 1929), 168‚Äì173. doi:10.1073/pnas.15.3.168
                     [5] C. Bonnet. 1764. Contemplation de la nature. Number v. 2 in Contemplation de           [27] Leon Ingelse and Alcides Fonseca. 2023. Domain-Aware Feature Learning with
                         la nature. M.M. Rey. https://books.google.com/books?id=Sm8GAAAAQAAJ                          Grammar-GuidedGenetic Programming. Springer Nature Switzerland, 227‚Äì243.
                     [6] Kevin Ren√© Brol√∏s, Meera Vieira Machado, Chris Cave, Jaan Kasak, Valdemar                    doi:10.1007/978-3-031-29573-7_15
                         Stentoft-Hansen, Victor Galindo Batanero, Tom Jelen, and Casper Wilstrup. 2021.        [28] Ying Jin, Weilin Fu, Jian Kang, Jiadong Guo, and Jian Guo. 2020. Bayesian
                         An Approach to Symbolic Regression Using Feyn. arXiv:2104.05417 [cs.LG]                      Symbolic Regression. arXiv:1910.08892 [stat.ME] https://arxiv.org/abs/1910.
                         https://arxiv.org/abs/2104.05417                                                             08892
                     [7] S. A. Budennyy, V. D. Lazarev, N. N. Zakharenko, A. N. Korovin, O. A. Plosskaya,       [29] Pierre-Alexandre Kamienny, St√©phane d‚ÄôAscoli, Guillaume Lample, and Fran-
                         D. V. Dimitrov, V. S. Akhripkin, I. V. Pavlov, I. V. Oseledets, I. S. Barsola, I. V.         cois Charton. 2022. End-to-end Symbolic Regression with Transformers. In
                         Egorov,A.A.Kosterina,andL.E.Zhukov.2022. eco2AI:CarbonEmissionsTrack-                        Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal,
                         ing of Machine Learning Models as the First Step Towards Sustainable AI. Dok-                Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?
                         ladyMathematics106,S1(Dec.2022),S118‚ÄìS128. doi:10.1134/s1064562422060230                     id=GoOuIrDHG_Y
                     [8] William La Cava, Tilak Raj Singh, James Taggart, Srinivas Suri, and Jason Moore.       [30] Lukas Kammerer, Gabriel Kronberger, Bogdan Burlacu, Stephan M. Winkler,
                         2019. Learning concise representations for regression by evolving networks of                Michael Kommenda, and Michael Affenzeller. 2020. Symbolic Regression by
                         trees. In International ConferenceonLearningRepresentations. https://openreview.             Exhaustive Search: Reducing the Search Space Using Syntactical Constraints and
                         net/forum?id=Hke-JhA9Y7                                                                      Ef√ècient Semantic Structure Deduplication. Springer International Publishing,
                     [9] CavaLab. 2025. Brush: An Interpretable Machine Learning Library.         https:              79‚Äì99. doi:10.1007/978-3-030-39958-0_5
                         //github.com/cavalab/brush/tree/multi_armed_bandits Accessed: 2025-03-29.              [31] Aleksandar Kartelj and Marko Djukanoviƒá. 2023. RILS-ROLS: robust symbolic
                   [10] Miles Cranmer. 2023. Interpretable Machine Learning for Science with PySR                     regression via iterated local search and ordinary least squares. Journal of Big
                         and SymbolicRegression.jl. arXiv:2305.01582 [astro-ph.IM] https://arxiv.org/                 Data 10, 1 (May 2023). doi:10.1186/s40537-023-00743-2
                         abs/2305.01582                                                                         [32] Maarten Keijzer. 2003. Improving symbolic regression with interval arithmetic
                   [11] Fabr√≠cio Olivetti de Fran√ßa. 2018. A greedy search tree heuristic for symbolic                and linear scaling. In European Conference on Genetic Programming. Springer,
                         regression. Information Sciences 442 (2018), 18‚Äì32.                                          70‚Äì82.
                   [12] F.O.deFrancaandG.S.I.Aldeia.2021. Interaction‚ÄìTransformationEvolutionary                [33] Johannes Kepler. 1619. Harmonices Mundi. Lincii Austri√¶.
                         Algorithm for Symbolic Regression. Evolutionary Computation 29, 3 (09 2021),           [34] Michael Kommenda, Bogdan Burlacu, Gabriel Kronberger, and Michael Affen-
                         367‚Äì390. doi:10.1162/evco_a_00285 arXiv:https://direct.mit.edu/evco/article-                 zeller. 2020. Parameter identification for symbolic regression using nonlinear
                         pdf/29/3/367/1959462/evco_a_00285.pdf                                                        least squares. Genetic ProgrammingandEvolvableMachines 21,3(2020),471‚Äì501.
                   [13] Fabricio Olivetti de Franca and Gabriel Kronberger. 2023. Reducing Overpa-              [35] John R. Koza. 1992. Genetic Programming: On the Programming of Computers by
                         rameterization of Symbolic Regression Models with Equality Saturation. In                    MeansofNatural Selection. MIT Press, Cambridge, MA, USA.
                         Proceedings of the Genetic and Evolutionary Computation Conference (Lisbon,            [36] JohnRKoza.1994. Geneticprogrammingasameansforprogrammingcomputers
                         Portugal) (GECCO ‚Äô23). Association for Computing Machinery, New York, NY,                    bynatural selection. Statistics and computing 4 (1994), 87‚Äì112.
                         USA,1064‚Äì1072. doi:10.1145/3583131.3590346                                             [37] Gabriel Kronberger, Bogdan Burlacu, Michael Kommenda, Stephan M. Winkler,
                   [14] F. O. de Franca, M. Virgolin, M. Kommenda, M. S. Majumder, M. Cranmer, G.                     and Michael Affenzeller. 2024. Symbolic Regression. Chapman & Hall / CRC
                         Espada,L.Ingelse,A.Fonseca,M.Landajuela,B.Petersen,R.Glatt,N.Mundhenk,                       Press.
                         C. S. Lee, J. D. Hochhalter, D. L. Randall, P. Kamienny, H. Zhang, G. Dick, A.         [38] William La Cava, Bogdan Burlacu, Marco Virgolin, Michael Kommenda, Patryk
                         Simon, B. Burlacu, Jaan Kasak, Meera Machado, Casper Wilstrup, and W. G. La                  Orzechowski, Fabr√≠cio Olivetti de Fran√ßa, Ying Jin, and Jason H Moore. 2021.
                         Cavaz.2024. SRBench++:PrincipledBenchmarkingofSymbolicRegressionWith                         Contemporary symbolic regression methods and their relative performance.
                         Domain-Expert Interpretation. IEEE Transactions on Evolutionary Computation                  Advances in neural information processing systems 2021, DB1 (2021), 1.
                         (2024), 1‚Äì1. doi:10.1109/TEVC.2024.3423681                                             [39] William La Cava, Kourosh Danai, and Lee Spector. 2016. Inference of compact
                   [15] Fabr√≠cioOlivettideFran√ßa.2022. Transformation-interaction-rationalrepresenta-                 nonlinear dynamic models by epigenetic local search. Engineering Applications
                         tion for symbolic regression. In Proceedings of the Genetic and Evolutionary Com-            of Artificial Intelligence 55 (2016), 292‚Äì306.
                         putation Conference (Boston, Massachusetts) (GECCO ‚Äô22). Association for Com-          [40] William La Cava, Thomas Helmuth, Lee Spector, and Jason H. Moore.
                         puting Machinery, New York, NY, USA, 920‚Äì928. doi:10.1145/3512290.3528695                    2019.   A Probabilistic and Multi-Objective Analysis of Lexicase Selection
                   [16] Fabr√≠cio Olivetti de Fran√ßa. 2023. Transformation-Interaction-Rational Represen-              and epsilon-Lexicase Selection.    Evolutionary Computation 27, 3 (09 2019),
                         tation for Symbolic Regression: A Detailed Analysis of SRBench Results. ACM                  377‚Äì402. doi:10.1162/evco_a_00224 arXiv:https://direct.mit.edu/evco/article-
                         Trans.Evol.Learn.Optim.3,2,Article7(June2023),19pages. doi:10.1145/3597312                   pdf/27/3/377/1858632/evco_a_00224.pdf
                   [17] GrantDick.2022. Geneticprogramming,standardisation,andstochasticgradient                [41] William G. La Cava, Paul C. Lee, Imran Ajmal, Xiruo Ding, Priyanka Solanki,
                         descent revisited: Initial findings on srbench. In Proceedings of the Genetic and            Jordana B. Cohen, Jason H. Moore, and Daniel S. Herman. 2023. A flexible
                         Evolutionary Computation Conference Companion. 2265‚Äì2273.                                    symbolic regression method for constructing interpretable clinical prediction
                   [18] Grant Dick, Caitlin A. Owen, and Peter A. Whigham. 2020. Feature standardisa-                 models. npj Digital Medicine 6, 1 (June 2023). doi:10.1038/s41746-023-00833-8
                         tion and coef√ècient optimisation for effective symbolic regression. In Proceedings     [42] Mikel Landajuela, Chak Shing Lee, Jiachen Yang, Ruben Glatt, Claudio P San-
                         of the 2020 Genetic and Evolutionary Computation Conference (Canc√∫n, Mex-                    tiago, Ignacio Aravena, Terrell Mundhenk, Garrett Mulcahy, and Brenden K
                         ico) (GECCO ‚Äô20). Association for Computing Machinery, New York, NY, USA,                    Petersen. 2022.   A Unified Framework for Deep Symbolic Regression. In
                         306‚Äì314. doi:10.1145/3377930.3390237                                                         Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed,
                   [19] Elizabeth D Dolan and Jorge J Mor√©. 2002. Benchmarking optimization software                  A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,
                         with performance profiles. Mathematical programming 91 (2002), 201‚Äì213.                      Inc., 33985‚Äì33998. https://proceedings.neurips.cc/paper_files/paper/2022/file/
                   [20] Guilherme Espada, Leon Ingelse, Paulo Canelas, Pedro Barbosa, and Alcides                     dbca58f35bddc6e4003b2dd80e42f838-Paper-Conference.pdf
                         Fonseca. 2022. Datatypes as a More Ergonomic Frontend for Grammar-Guided               [43] Henrietta S. Leavitt and Edward C. Pickering. 1912. Periods of 25 Variable Stars
                         Genetic Programming. In GPCE ‚Äô22: Concepts and Experiences, Auckland, NZ,                    in the Small Magellanic Cloud. Harvard College Observatory Circular 173 (March
                         December 6 - 7, 2022, Bernhard Scholz and Yukiyoshi Kameyama (Eds.). ACM, 1.                 1912), 1‚Äì3.
                   [21] R.P. Feynman, R.B. Leighton, and M.L. Sands. 2006. The Feynman Lectures on              [44] Lianyi Liu, Sifeng Liu, Yingjie Yang, Xiaojun Guo, and Jinghe Sun. 2024. A
                         Physics. Number vol. 2 in The Feynman Lectures on Physics. Pearson/Addison-                  generalized grey model with symbolic regression algorithm and its application
                         Wesley. https://books.google.com.br/books?id=AbruAAAAMAAJ                                    in predicting aircraft remaining useful life. Engineering Applications of Artificial
                   [22] R.P. Feynman, R.B. Leighton, and M. Sands. 2015. The Feynman Lectures on                      Intelligence 136 (2024), 108986. doi:10.1016/j.engappai.2024.108986
                         Physics, Vol. I: The New Millennium Edition: Mainly Mechanics, Radiation, and          [45] NourMakkeandSanjayChawla.2024. Interpretable scientific discovery with
                         Heat. Number vol. 1 in The Feynman Lectures on Physics. Basic Books. https:                  symbolic regression: a review. Artificial Intelligence Review 57, 1 (Jan. 2024).
                         //books.google.com.br/books?id=d76DBQAAQBAJ                                                  doi:10.1007/s10462-023-10622-0
                                                                                                                                                                          Imai Aldeia et al.
                  [46] Yoshitomo Matsubara, Naoya Chiba, Ryo Igarashi, and Yoshitaka Ushiku. 2024.             of the Genetic and Evolutionary Computation Conference (Melbourne, VIC, Aus-
                        Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Dis-             tralia) (GECCO ‚Äô24). Association for Computing Machinery, New York, NY, USA,
                        covery.  Journal of Data-centric Machine Learning Research (2024).   https:            961‚Äì970. doi:10.1145/3638529.3654087
                        //openreview.net/forum?id=qrUdrXsiXX                                              [54] SubhamSahoo,ChristophLampert,andGeorgMartius.2018. LearningEquations
                  [47] Trent McConaghy. 2011. FFX: Fast, scalable, deterministic symbolic regression           for Extrapolation and Control. In Proceedings of the 35th International Conference
                        technology. In Genetic Programming Theory and Practice IX. Springer, 235‚Äì260.          onMachineLearning (Proceedings of Machine Learning Research, Vol. 80), Jennifer
                  [48] Aaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ond≈ôej ƒåert√≠k, Sergey B.          DyandAndreasKrause(Eds.).PMLR,4442‚Äì4450. https://proceedings.mlr.press/
                        Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sar-            v80/sahoo18a.html
                        taj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger, Richard P. Muller,     [55] Michael Schmidt and Hod Lipson. 2009. Distilling free-form natural laws from
                        Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson, Fabian Pe-             experimental data. science 324, 5923 (2009), 81‚Äì85.
                        dregosa, Matthew J. Curry, Andy R. Terrel, ≈†tƒõp√°n Rouƒçka, Ashutosh Saboo,         [56] MichaelSchmidtandHodLipson.2010. Age-FitnessParetoOptimization. Springer
                        Isuru Fernando, Sumith Kulal, Robert Cimrman, and Anthony Scopatz. 2017.               NewYork,129‚Äì146. doi:10.1007/978-1-4419-7747-2_8
                        SymPy: symbolic computing in Python. PeerJ Computer Science 3 (Jan. 2017),        [57] Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, and Chandan K. Reddy.
                        e103. doi:10.7717/peerj-cs.103                                                         2023. Transformer-based Planning for Symbolic Regression. In Thirty-seventh
                  [49] Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz,               Conference on Neural Information Processing Systems. https://openreview.net/
                        and Jason H. Moore. 2017. PMLB: a large benchmark suite for machine learning           forum?id=0rVXQEeFEL
                        evaluation and comparison. BioData Mining 10, 1 (11 Dec 2017), 36. doi:10.1186/   [58] METhingandSMKoksbang.2025. cp3-bench:atoolforbenchmarkingsymbolic
                        s13040-017-0154-4                                                                      regression algorithms demonstrated with cosmology. Journal of Cosmology and
                  [50] Patryk Orzechowski, William La Cava, and Jason H. Moore. 2018. Where are                Astroparticle Physics 2025, 01 (2025), 040.
                        wenow?alargebenchmarkstudyofrecent symbolic regression methods. In                [59] R. B. Tully and J. R. Fisher. 1977. A New Method of Determining Distance to
                        Proceedings of the Genetic and Evolutionary Computation Conference (Kyoto,             Galaxies. Astronomy and Astrophysics 500 (Feb. 1977), 105‚Äì117.
                        Japan) (GECCO ‚Äô18). Association for Computing Machinery, New York, NY, USA,       [60] Silviu-Marian Udrescu and Max Tegmark. 2020. AI Feynman: A physics-inspired
                        1183‚Äì1190. doi:10.1145/3205455.3205539                                                 methodforsymbolic regression. Science Advances 6, 16 (2020), eaay2631.
                  [51] David L Randall, Tyler S Townsend, Jacob D Hochhalter, and Geoffrey F Bo-          [61] M.Virgolin, T. Alderliesten, C. Witteveen, and P. A. N. Bosman. 2021. Improving
                        marito. 2022. Bingo: a customizable framework for symbolic regression with             Model-Based Genetic Programming for Symbolic Regression of Small Expres-
                        geneticprogramming.InProceedingsoftheGeneticandEvolutionaryComputation                 sions. Evolutionary Computation 29, 2 (2021), 211‚Äì237. doi:10.1162/evco_a_00278
                        Conference Companion. 2282‚Äì2288.                                                  [62] Marco Virgolin and Solon P. Pissis. 2022. Symbolic Regression is NP-hard.
                  [52] Joseph D Romano, Trang T Le, William La Cava, John T Gregg, Daniel J Gold-              arXiv:2207.01018 [cs.NE] https://arxiv.org/abs/2207.01018
                        berg, Praneel Chakraborty, Natasha L Ray, Daniel Himmelstein, Weixuan Fu,         [63] Jan ≈Ωegklitz and Petr Po≈°√≠k. 2020. Benchmarking state-of-the-art symbolic
                        and Jason H Moore. 2021. PMLB v1.0: an open source dataset collection for              regressionalgorithms. GeneticProgrammingandEvolvableMachines22,1(March
                        benchmarking machine learning methods. arXiv preprint arXiv:2012.00058v2               2020), 5‚Äì33. doi:10.1007/s10710-020-09387-0
                        (2021).                                                                           [64] Hengzhe Zhang, Aimin Zhou, Hong Qian, and Hu Zhang. 2022. PS-Tree: A
                  [53] Etienne Russeil, Fabricio Olivetti de Franca, Konstantin Malanchev, Bogdan              piecewise symbolic regression tree. Swarm and Evolutionary Computation 71
                        Burlacu, Emille Ishida, Marion Leroux, Cl√©ment Michelin, Guillaume Moinard,            (2022), 101061. doi:10.1016/j.swevo.2022.101061
                        andEmmanuelGangler.2024. Multiview Symbolic Regression. In Proceedings
