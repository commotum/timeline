                         Published as a conference paper at ICLR 2021
                         3.1  IMAGE CLASSIFICATION FROM SCRATCH
                         WeÔ¨Årst evaluate SAM‚Äôs impact on generalization for today‚Äôs state-of-the-art models on CIFAR-10
                         and CIFAR-100 (without pretraining): WideResNets with ShakeShake regularization (Zagoruyko
                         &Komodakis, 2016; Gastaldi, 2017) and PyramidNet with ShakeDrop regularization (Han et al.,
                         2016; Yamada et al., 2018). Note that some of these models have already been heavily tuned in
                         prior work and include carefully chosen regularization schemes to prevent overÔ¨Åtting; therefore,
                         signiÔ¨Åcantly improving their generalization is quite non-trivial. We have ensured that our imple-
                         mentations‚Äô generalization performance in the absence of SAM matches or exceeds that reported in
                         prior work (Cubuk et al., 2018; Lim et al., 2019)
                         All results use basic data augmentations (horizontal Ô¨Çip, padding by four pixels, and random crop).
                         Wealso evaluate in the setting of more advanced data augmentation methods such as cutout regu-
                         larization (Devries & Taylor, 2017) and AutoAugment (Cubuk et al., 2018), which are utilized by
                         prior work to achieve state-of-the-art results.
                         SAMhasasingle hyperparameter œÅ (the neighborhood size), which we tune via a grid search over
                                                                                                 3
                         {0.01,0.02,0.05,0.1,0.2,0.5} using 10% of the training set as a validation set . Please see ap-
                         pendix C.1 for the values of all hyperparameters and additional training details. As each SAM
                                                                                          ÀÜ
                         weight update requires two backpropagation operations (one to compute (w) and another to com-
                         pute the Ô¨Ånal gradient), we allow each non-SAM training run to execute twice as many epochs as
                         each SAMtraining run, and we report the best score achieved by each non-SAM training run across
                         either the standard epoch count or the doubled epoch count4. We run Ô¨Åve independent replicas of
                         eachexperimentalconditionforwhichwereportresults(eachwithindependentweightinitialization
                         and data shufÔ¨Çing), reporting the resulting mean error (or accuracy) on the test set, and the associ-
                         ated 95% conÔ¨Ådence interval. Our implementations utilize JAX (Bradbury et al., 2018), and we
                                                                                5
                         train all models on a single host having 8 NVidia V100 GPUs . To compute the SAM update when
                         parallelizing across multiple accelerators, we divide each data batch evenly among the accelerators,
                         independently compute the SAM gradient on each accelerator, and average the resulting sub-batch
                         SAMgradientstoobtain the Ô¨Ånal SAM update.
                         As seen in Table 1, SAM improves generalization across all settings evaluated for CIFAR-10 and
                         CIFAR-100. For example, SAM enables a simple WideResNet to attain 1.6% test error, versus
                         2.2% error without SAM. Such gains have previously been attainable only by using more complex
                         modelarchitectures (e.g., PyramidNet) and regularization schemes (e.g., Shake-Shake, ShakeDrop);
                         SAMprovides an easily-implemented, model-independent alternative. Furthermore, SAM delivers
                         improvements even when applied atop complex architectures that already use sophisticated regular-
                         ization: for instance, applying SAM to a PyramidNet with ShakeDrop regularization yields 10.3%
                         error on CIFAR-100, which is, to our knowledge, a new state-of-the-art on this dataset without the
                         use of additional data.
                         Beyond CIFAR-{10, 100}, we have also evaluated SAM on the SVHN (Netzer et al., 2011) and
                         Fashion-MNIST datasets (Xiao et al., 2017). Once again, SAM enables a simple WideResNet to
                         achieve accuracy at or above the state-of-the-art for these datasets: 0.99% error for SVHN, and
                         3.59%forFashion-MNIST.Details are available in appendix B.1.
                         To assess SAM‚Äôs performance at larger scale, we apply it to ResNets (He et al., 2015) of different
                         depths (50, 101, 152) trained on ImageNet (Deng et al., 2009). In this setting, following prior work
                         (Heetal., 2015; Szegedy et al., 2015), we resize and crop images to 224-pixel resolution, normalize
                         them,andusebatchsize4096,initiallearningrate1.0,cosinelearningrateschedule,SGDoptimizer
                         withmomentum0.9,labelsmoothingof0.1,andweightdecay0.0001. WhenapplyingSAM,weuse
                         œÅ = 0.05 (determined via a grid search on ResNet-50 trained for 100 epochs). We train all models
                         on ImageNet for up to 400 epochs using a Google Cloud TPUv3 and report top-1 and top-5 test
                         error rates for each experimental condition (mean and 95% conÔ¨Ådence interval across 5 independent
                         runs).
                            3We found œÅ = 0.05 to be a solid default value, and we report in appendix C.3 the scores for all our
                         experiments, obtained with œÅ = 0.05 without further tuning.
                            4Training for longer generally did not improve accuracy signiÔ¨Åcantly, except for the models previously
                         trained for only 200 epochs and for the largest, most regularized model (PyramidNet + ShakeDrop).
                            5Because SAM‚Äôs performance is ampliÔ¨Åed by not syncing the perturbations, data parallelism is highly
                         recommendedtoleverage SAM‚Äôs full potential (see Section 4 for more details).
                                                                    5
