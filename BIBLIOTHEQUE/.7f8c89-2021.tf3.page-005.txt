                PCT: Point cloud transformer                                                                                             191
                Fig. 2 PCT architecture. The encoder mainly comprises an Input Embedding module and four stacked Attention module. The decoder mainly
                comprises multiple Linear layers. Numbers above each module indicate its output channels. MA-Pool concatenates Max-Pool and Average-Pool.
               LBR combines Linear, BatchNorm, and ReLU layers. LBRD means LBR followed by a Dropout layer.
                  Normal estimation. For the task of normal                     Weempirically set d = 128, a relatively small value,
                                                                                                       e
                estimation, we use the same architecture as in                  for computational eﬃciency.          We simply use the
                segmentation by setting N = 3, without the object               point’s 3D coordinates as its input feature description
                                               s
                category encoding, and regard the output point-wise             (i.e., d  = 3) (as doing so still outperforms other
                                                                                        p
                score as the predict normal.                                    methods)butadditionalpoint-wise input information,
                3.2    Naive PCT                                                such as point normals, could also be used.
                                                                                   For the naive implementation of PCT, we adopt
               The simplest way to modify Transformer [6] for point             self-attention (SA) as introduced in the original
                cloud use is to treat the entire point cloud as a               Transformer [6].      Self-attention, also called intra-
                sentence and each point as a word, an approach                  attention, is a mechanism that calculates semantic
               we now explain. This naive PCT is achieved by                    aﬃnities between diﬀerent items within a sequence
                implementing a coordinate-based point embedding                 of data. The architecture of the SA layer is depicted
                and instantiating the attention layer with the self-            in Fig. 3 by switching to the dotted data ﬂows.
                attention introduced in Ref. [6].                               Following the terminology in Ref. [6], let Q,K,V
                  First, we consider a naive point embedding, which             be the query, key, and value matrices, respectively,
                ignores interactions between points.            Like word       generated by linear transformations of the input
                embedding in NLP, point embedding aims to place                 features Fin ∈ RN×de as follows:
                points closer in the embedding space if they are                         (Q,K,V)=F ·(W ,W ,W )
                more semantically similar. Speciﬁcally, we embed                                          in      q     k    v
                                                                                                          N×da              N×de
                a point cloud P into a de-dimensional space Fe ∈                               Q,K∈R            ,   V ∈R                 (2)
                 N×d                                                                                      de×da              de×de
                      e                                                                    W,W ∈R               ,   W ∈R
                R      , using a shared neural network comprising two                         q     k                  v
                cascaded LBRs, each with a d -dimensional output.               where W , W , and W are the shared learnable
                                                   e                                       q     k           v
                Fig. 3 Architecture of Oﬀset-Attention. Numbers above tensors are numbers of dimensions N and feature channels D/Da, with switches
                showing alternatives of Self-Attention or Oﬀset-Attention: dotted lines indicate Self-Attention branches.
