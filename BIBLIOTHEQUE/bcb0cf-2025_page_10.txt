                              Published as a conference paper at ICLR 2025
                              Early Exit Method            TheMINDmodelincorporatessequential operations in the introspection network to
                              dynamically adjust the computational depth for each input. Although these sequential steps are introduced,
                              their impact on runtime is significantly mitigated by reducing FLOPs during the main computation.
                              Unlike traditional early exit methods such as those used by BranchyNet (Teerapittayanon et al., 2016), which
                              rely on static thresholds for early exits, MIND’s early exit mechanism is driven by real-time change. The
                              introspection network evaluates the internal state of the model during inference, dynamically adjusting the
                              number of layers and iterations required. Simpler inputs trigger earlier exits, conserving computational
                              resources without sacrificing accuracy, whereas complex inputs utilize additional layers and iterations for
                              morerefined processing.
                              Despite the sequential nature of the introspection process, MIND achieves superior performance compared to
                              BranchyNet and ResNet-50, as shown in Table 6. MIND delivers 88.2% Top-1 accuracy on ImageNet with
                              average of only 1.05G FLOPs, maintaining an inference time of 20ms. This design contrasts with ResNet-50’s
                              fixed skip connections, giving MIND more flexible and efficient computation paths, which effectively handles
                              diverse input complexities with minimal overhead. MIND offers a fundamentally different and more efficient
                              computation process than traditional architectures like ResNet.
                              The MIND model’s approach differs fundamentally from recent early exit strategies employed in large
                              language models, such as CALM (Schuster et al., 2022) and LayerSkip (Elhoushi et al., 2024). While
                              these state-of-the-art models demonstrate sophisticated exit mechanisms for complex language tasks, MIND
                              focusesonlightweightvisionarchitecturesandsimplerlanguageprocessing. Wepresentdetailedexperimental
                              comparisons of our model with these two early exit strategies in Table 10 given in Appendix F.2
                              Table 6: Comparison of MIND model with early exit methods (BranchyNet, SDN) and DEQ variants
                              (Anderson, Broyden) on CIFAR-100, Caltech101, and SUN397 datasets, as well as their computational
                              efficiency on ImageNet.
                                Method                                      Solver                    Accuracy↑                    Inference Time     Top-1 Accuracy
                                                                                         CIFAR-100      Caltech101     SUN397                           (ImageNet)
                                BranchyNet (Teerapittayanon et al., 2016)   —               68.2%          78.6%        68.5%          25.0ms             64.24%
                                SDN(Huangetal.,2016)                        —               79.5%          91.3%        69.2%          23.5ms              68.5%
                                DEQ(Baietal., 2019)                         Anderson        82.7%          92.6%        71.4%         148.0ms              78.5%
                                DEQ(Baietal., 2019)                         Broyden         83.1%          92.9%        71.8%         186.0ms              81.8%
                                MIND(Ours)                                  FPI            85.7%           93.5%        72.8%          20.0ms              88.2%
                              5     CONCLUSIONS
                              In this paper, we introduced the MIND model, a dynamic architecture that adaptively adjusts computational
                              depth based on input complexity. Through its introspection network and Fixed-Point Iteration (FPI) layers,
                              the MINDmodelachieves a balance between accuracy and efficiency, outperforming traditional static models
                              across various tasks with fewer parameters and reduced computation. Our results demonstrate an approach
                              to model building that reduces computational overhead for simpler inputs while scaling up effectively for
                              complex ones. Future directions, including introspection refinements and broader applications, along with
                              current limitations, are detailed in Appendices H and I
                              ACKNOWLEDGEMENTS
                              ThisworkwassupportedbyNIHaward2R01EB006841,NSFaward2112455,andTaighdeÉireann(Research
                              Ireland) Grant 20/FFP-P/8853. SP is grateful to Alex Neumann for invaluable conversations and physical
                              perspective on the topic.
                              REFERENCES
                              Praveen Agarwal, Mohamed Jleli, and Bessem Samet. Banach contraction principle and applications. Fixed
                                 Point Theory in Metric Spaces: Recent Advances and Applications, pp. 1–23, 2018.
                                                                                            10
