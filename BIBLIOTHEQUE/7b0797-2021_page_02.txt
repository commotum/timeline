                                         Training data-efﬁcient image transformers & distillation through attention
               Weaddressanother question: how to distill these models?          without using any convolution. This performance is remark-
               Weintroduce a token-based strategy, DeiT , that advanta-         able since convnet methods for image classiﬁcation have
                                                            ⚗
               geously replaces the usual distillation for transformers.        beneﬁted from years of tuning and optimization (He et al.,
               In summary, our work makes the following contributions:          2019; Wightman, 2019). Nevertheless, according to Doso-
                                                                                vitskiy et al. (2020), a pre-training phase on a large volume
               • We show that our neural networks that contain no con-          of curated data is required for the learned transformer to be
                 volutional layer can achieve competitive results against       effective. In our paper we achieve a strong performance with
                 the state of the art on ImageNet with no external data.        ImageNet-1k and report decent results even on CIFAR-10.
                 Theyare learned on a single node with 4 GPUs in three
                      1
                 days . Our two new models DeiT-S and DeiT-Ti have              TheTransformer architecture,         introduced by Vaswani
                 fewer parameters and can be seen as the counterpart of         et al. (Vaswani et al., 2017) for machine translation is cur-
                 ResNet-50 and ResNet-18.                                       rently the reference model for all natural language process-
               • Weintroduce a new distillation procedure based on a dis-       ing (NLP) tasks. Many improvements of convnets for image
                 tillation token, which plays the same role as the class to-    classiﬁcation are inspired by transformers. For example,
                                                                                Squeeze and Excitation (
                 ken, except that it aims at reproducing the label estimated                               Hu et al., 2017), Selective Ker-
                 by the teacher. Both tokens interact in the transformer        nel (Li et al., 2019b), Split-Attention Networks (Zhang et al.,
                 through attention. This transformer-speciﬁc strategy out-      2020)andStand-AloneSelf-Attention(Ramachandranetal.,
                 performs vanilla distillation by a signiﬁcant margin.          2019) exploit mechanism akin to transformers self-attention
               • Our models pre-learned on Imagenet are competitive             (SA) mechanism. Moreover, Cordonnier et al. (Cordonnier
                 when transferred to different downstream tasks such            et al., 2020) study the link between SA and convolution.
                 as ﬁne-grained classiﬁcation, on several popular public        KnowledgeDistillation       (Hinton et al., 2015) refers to the
                 benchmarks: CIFAR-10, CIFAR-100, Oxford-102 ﬂow-
                 ers, Stanford Cars and iNaturalist-18/19.                      training paradigm in which a student model leverages “soft”
                                                                                labels coming from a strong teacher network. This is the
                                                                                output vector of the teacher’s softmax function rather than
               2. Related work                                                  just the maximum of scores, wich gives a “hard” label. Such
               Image Classiﬁcation      is so core to computer vision that      a training improves the performance of the student model
               it is often used as a benchmark to measure progress in           (alternatively, it can be regarded as a form of compression of
               image understanding. Any progress usually translates to          the teacher model into a smaller one – the student). On the
               improvement in other related tasks such as detection or          one hand the teacher’s soft labels will have a similar effect
               segmentation. Since 2012’s AlexNet (Krizhevsky et al.,           to labels smoothing (Yuan et al., 2020). On the other hand as
               2012), convnets have dominated this benchmark and have           shownbyWeietal.(2020)theteacher’s supervision takes
               becomethedefactostandard. The evolution of the state of          into account the effects of the data augmentation, which
               the art on the ImageNet dataset (Russakovsky et al., 2015)       sometimes causes a misalignment between the real label
               reﬂects the progress with convolutional architectures and        and the image. For example, let us consider image with a
               optimization methods (Simonyan & Zisserman, 2015; Tan            “cat” label that represents a large landscape and a small cat
               &Le,2019;Touvronetal., 2019).                                    in a corner. If the cat is no longer on the crop of the data
                                                                                augmentation it implicitly changes the label of the image.
               Despite several attempts to use transformers for image clas-     Knowledgedistillation can transfer inductive biases (Abnar
               siﬁcation (Chen et al., 2020a), until now their performance      et al., 2020) in a soft way in a student model using a teacher
               has been inferior to that of convnets. Nevertheless hybrid       modelwheretheywouldbeincorporated in a hard way. In
               architectures that combine convnets and transformers, in-        our paper we study the distillation of a transformer student
               cluding the self-attention mechanism, have exhibited com-        byeither a convnet or a transformer teacher, motivated by
               petitive results in image classiﬁcation (Bello et al., 2019;     inducing convolutional bias into transformers.
               Bello, 2021; Wu et al., 2020), detection (Carion et al., 2020;
               Huetal., 2018), video processing (Sun et al., 2019; Wang         3. Vision transformer: overview
               et al., 2018), unsupervised object discovery (Locatello et al.,
               2020), and text-vision tasks (Chen et al., 2020b; Li et al.,     In this section, we brieﬂy recall preliminaries associated
               2019a; Lu et al., 2019).                                         with the vision transformer (Dosovitskiy et al., 2020;
               Recently Vision transformers (ViT) (Dosovitskiy et al.,          Vaswani et al., 2017), denoted by ViT. We further discuss
               2020) closed the gap with the state of the art on ImageNet,      positional encoding and resolution.
                  1Wecanaccelerate the learning of the larger model DeiT-B by   Multi-head Self Attention layers (MSA).        Theattention
               training it on 8 GPUs in two days.                               mechanism is based on a trainable associative memory with
