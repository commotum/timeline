                                      TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
              E. AugmentedInference Pipeline
              E.1. Augmented Inference
              Recent work has shown that scaling test-time compute can significantly improve the performance of LMs. One of the
              most commontechniques to do this is by sampling multiple responses, and then selecting the best response using a ranker.
              However, while sampling is very effective in domains with multiple possible solutions (programs in code) or multiple
              possible paths to the final answer (math), it can be detrimental when generating answers directly, as there is no way to
              directly enforce diversity across samples while ensuring coherence within samples. As an alternative inference-time scaling,
              weuseanaugmentedinference strategy that generates multiple prediction candidates by using geometric transformations,
              combined with a greedy decoding scheme.
                                                             K
              For a given task with training examples (x ,y )    and test input x  , we use invertible geometric transformations to
                                                       k   k k=1                test
              produce equivalent transformed versions of the task, as shown in Figure 5. Let T be some set set of invertible geometric
              transformations (e.g., rotations and reflections). For each transformation t ∈ T , we apply t to all training demonstrations
              and the test input and run our model with these transformed inputs. We then apply the inverse transformation to obtain the
              final prediction for that transformation.
                                                 y˜ ∼ LM(t(d     )) := [t(x ),t(y ),...,t(x  )]                                (6)
                                                             input        1     1          test
                                                yt = t−1(y˜)                                                                   (7)
              Wefurther augment our predictions by permuting the order of training examples. For each transformation g, we sample
              n=2differentpermutations of the demonstration sequence, resulting in n·|T | total predictions per task. This is to mitigate
              any bias in the model’s processing of the demonstration sequence. (Bober-Irizar & Banerjee, 2024) also find transpose and
              rotation is helpful to produce extra prediction candidates.
              E.2. Ensembling Predictions (Voting Strategy)
                                                                                                               n·|T |
              Weemployahierarchicalvotingstrategy to determine the final prediction from the set of candidates {y} . This approach
                                                                                                               i=1
              involves two stages of voting to progressively narrow down the best candidates: first, by selecting the most frequent
              predictions within each transformation, and then by conducting an overall vote across transformation-specific candidates to
              identify the top-2 most frequent predictions. The details of each stage are as follows:
                1. Intra Transformation Voting: We group predictions by their corresponding transformation t and select the top-3 most
                  frequent predictions within each group. If fewer than 3 unique predictions exist within a group, we supplement the
                  candidates by computing additional predictions through:
                     • Row-basedmajority: For each row in the predicted output grid, we take the most frequent row values across all
                       predictions in the transformation group.
                     • Column-based majority: Similarly, for each column in the predicted output grid, we take the most frequent
                       column values across all predictions in the transformation group.
                2. Global Voting: Using the selected transformation-specific candidates obtained from (1), we conduct an overall vote to
                  select the top-2 most frequent predictions for submission. In case of a tie, predictions with the identity transformation
                  are given priority.
                                                                       20
