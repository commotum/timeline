# Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective (2025)
Source: d405d6-2025.pdf

## Core reasons
- The paper frames its contribution as an analysis of existing RoPE extensions from an attention perspective.
- It focuses on empirical study of those extensions on long-context perplexity testing and attention patterns.

## Evidence extracts
- "In this paper, we are driven to offer a straightforward yet in-depth understanding of RoPE extensions from an attention perspective and on two benchmarking tasks." (p. 8955)
- "As a start, we strive to primarily study these methods on a long-context perplexity test (PPL) and empirically compare their corresponding attention patterns." (p. 8955)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
