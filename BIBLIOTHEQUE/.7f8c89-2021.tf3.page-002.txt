                  188                                                                                                  M.-H. Guo, J.-X. Cai, Z.-N. Liu, et al.
                  for point cloud learning based on the principles of                              with rigid transformations. Therefore, relative
                  traditional Transformer. The key idea of PCT is                                  coordinates are generally more robust. Secondly,
                  using the inherent order invariance of Transformer to                            the Laplacian matrix (the oﬀset between degree
                  avoid the need to deﬁne the order of point cloud data                            matrix and adjacency matrix) has been proven to
                  and conduct feature learning through the attention                               be very effective in graph convolution learning [9].
                  mechanism. As shown in Fig. 1, the distribution of                               From this perspective, we regard the point cloud as
                  attention weights is highly related to part semantics,                           a graph with the “float” adjacency matrix as the
                  and it does not seriously attenuate with spatial                                 attention map. Also, the attention map in our work
                  distance.                                                                        will be scaled with all the sum of each rows to 1. So
                     Point clouds and natural language are rather                                  the degree matrix can be understood as the identity
                  diﬀerent kinds of data, so our PCT framework must                                matrix. Therefore, the offset-attention optimization
                  make several adjustments for this. These include:                                process can be approximately understood as a
                  •    Coordinate-basedinputembeddingmodule.                                       Laplace process, which will be discuss detailed
                       In Transformer, a positional encoding module is                             in Section 3.3. In addition, we have conducted
                       applied to represent the word order in natural                              sufficient comparative experiments, introduced in
                       language. This can distinguish the same word                                Section 4, on oﬀset-attention and self-attention to
                       in diﬀerent positions and reﬂect the positional                             prove its eﬀectiveness.
                       relationships between words. However, pointclouds                      •    Neighbor embedding module. Obviously,
                       do not have a fixed order. In our PCT framework,                            every word in a sentence contains basic semantic
                       we merge the raw positional encoding and the                                information. However, the independent input
                       input embedding into a coordinate-based input                               coordinates of the points are only weakly related
                       embedding module. It can generate distinguishable                           to the semantic content. Attention mechanism is
                       features, since each point has a unique coordinate                          eﬀective in capturing global features, but it may
                       which represents its spatial position.                                      ignore local geometric information which is also
                  •    Optimized oﬀset-attention module.                            The            essential for point cloud learning. To address this
                       oﬀset-attention module approach we proposed                                 problem, we use a neighbor embedding strategy
                       is an eﬀective upgrade over the original self-                              to improve upon point embedding. It also assists
                       attention. It works by replacing the attention                              the attention module by considering attention
                       feature with the oﬀset between the input of self-                           between local groups of points containing
                       attention module and attention feature. This has                            semantic information instead of individual points.
                       two advantages. Firstly, the absolute coordinates                         Withtheaboveadjustments,thePCTbecomesmore
                       of the same object can be completely diﬀerent                          suitable for point cloud feature learning and achieves
                                                                                              thestate-of-the-art performanceonshapeclassification,
                                                                                              part segmentation, semantic segmentation, and normal
                                                                                              estimation tasks. All experiments are implemented
                                                                                              with Jittor [10] deep learning fremework. Codes are
                                                                                              available at https://github.com/MenghaoGuo/PCT.
                                                                                                 Themaincontributionsofthispaperaresummarized
                                                                                              as following:
                                                                                              1.   Weproposedanoveltransformerbasedframework
                                                                                                   named PCT for point cloud learning, which is
                                                                                                   exactly suitable for unstructured, disordered point
                                                                                                   cloud data with irregular domain.
                                                                                              2.   We proposed oﬀset-attention with implicit
                                                                                                   Laplace operator and normalization reﬁnement
                  Fig. 1    Attention map and part segmentation generated by PCT.                  which is inherently permutation-invariant and
                  First three columns: point-wise attention map for diﬀerent query                 moresuitableforpointcloudlearningcomparedto
                  points (indicated by I), yellow to blue indicating increasing attention          the original self-attention module in Transformer.
                  weight. Last column: part segmentation results.
