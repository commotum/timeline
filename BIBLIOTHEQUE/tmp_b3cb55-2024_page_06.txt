                           Figure 2: Examples of Re-ARC challenges and the corresponding code.
                     volving "basic spatial and semantic concepts" from the same domain as the
                     original ARC-AGI dataset. It contains 176 additional tasks [12].
                     Our best scoring model also included ARC-Heavy [13], which uses LLMs to
                     generate a large amount of synthetic tasks.
                     3.2  Data Modeling
                     In order to apply LLMs to ARC-AGI puzzles, we need to tokenize the data in
                     a manner suitable for our model. This process requires careful consideration
                     of two main challenges:
                     First, due to the limited context size in typical LLM architectures, an in-
                     crease of inference time and decline in performance on long context tasks [16],
                     we require a representation that minimizes the number of tokens the model
                     needs to process. Secondly, it is widely recognized that numerous common
                     failure modes in Large Language Models (LLMs) stem from tokenization [8,
                     9, 10]. For instance, standard tokenization techniques group numbers (some
                     but not all combinations) of one, two or three succeeding digits into dedi-
                     cated “grouped-digit tokens” [17]. These kinds of merges would complicate
                     the puzzles unnecessarily.
                     To address this, we opted to simplify the token set available to the model.
                     In particular, we reduced the number of tokens available from over 120.000
                     to 64 or less tokens (see Table 2).
                                                    6
