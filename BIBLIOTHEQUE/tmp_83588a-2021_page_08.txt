                                                                        Stabilizing Equilibrium Models by Jacobian Regularization
                                                                                                                                                CIFAR-10 Classification Comparison
                      Table 2. Results on CIFAR-10 and ImageNet classﬁcation. The
                      CIFAR-10accuracystandard deviation is calculated with 5 runs.                                                                                              2.8                            untime
                      JRstands for Jacobian regularization. † indicates unregularized                                                   2.3               2.5
                                                                                                                           or (%)
                      modelhard-stopped at inference time.                                                                          6.2                                 6.4                6.9                  elative R
                                                                                                                                                      5.0      1.4
                                                       CIFAR-10classiﬁcation                                                                1.0                                                1.1 1.1
                                                                             Size        Accuracy         NFEs                                                               0.7
                                 ResNet-18 (He et al., 2016)                 10M 93.0(±0.1)%                 -             Classification Err
                                ResNet-101 (He et al., 2016)                 40M 93.8(±0.3)%                 -                     ResNet-101       DenseNet-121           MDEQ         MDEQ+JR (ours)          Memory (GB) or R
                             DenseNet-121 (Huang et al., 2017)                8M      95.0 (±0:1)%           -                      Error (%)             Memory (GB)                Runtime (relative)
                         monotoneDEQ(Winston&Kolter,2020)                     1M      89.4 (± 0.2)%         24          Figure 6. With the proposed regularization, DEQ models are com-
                                   MDEQ(Baietal.,2020)                       10M 93.6(±0.2)%                17
                                    MDEQearlystopped†                        10M           89.1%            6†          petitive with popular explicit networks in accuracy, memory, and
                           MDEQ+JR(ours)(Baietal.,2020)                      10M 93.1(±0.3)%                 6          runtime. Lower bars are better.
                                                   (Full) ImageNet classiﬁcation
                                                                             Size       Top-1 Acc.        NFEs                WikiText-103 DEQ Max Eigenvalue vs Train Iters (train NFE=16)
                                                                                                                           )
                                                                                                                           f
                                                                                                                           J
                                                                                                                           (
                                                                                                                            12
                                 ResNet-18 (He et al., 2016)                 13M           70.2%             -                          Trans. DEQ
                                                                                                                           ρ
                                                                                                                            
                                                                                                                                        Trans. DEQ (train NFE=30)
                           Inception-V2 (Ioffe & Szegedy, 2015)              12M           74.8%             -             e
                                                                                                                                                                                                              2
                                                                                                                           u
                                                                                                                            10
                                                                                                                                                                                                           10
                                                                                                                           l
                                                                                                                                        Trans.DEQ+reg.(ours)
                                 ResNet-50 (He et al., 2016)                 26M           75.1%             -             a
                                                                                                                           v
                                                                                                                           n
                                ResNet-101 (He et al., 2016)                 52M           77.1%             -               8
                                                                                                                           e
                                                                                                                           g
                             DenseNet-264 (Huang et al., 2017)               74M           79.7%             -             i
                                                                                                                           e
                                                                                                                            
                                                                                                                             6
                                                                                                                           )
                               MDEQ-small(Baietal., 2020)                    18M           75.4%            27             .
                                                                                                                           s
                                                                                                                           b
                               MDEQ-large(Baietal., 2020)                    63M           77.5%            30             a
                                                                                                                             4
                                                                                                                           (
                                                                                                                            
                                 MDEQ-small+JR(ours)                         17M           74.5%            14             t
                                                                                                                           s
                                                                                                                           e
                                                                                                                             2
                                 MDEQ-large+JR(ours)                         62M           76.8%            15             g
                                                                                                                           r
                                                                                                                                                                                                                 Valid. perplexity (log-scale)
                                                                                                                           a
                                                                                                                           L
                                                                                                                                                                                                              1
                                                                                                                                                                                                           10
                                                                                                                             0
                                                                                                                                  0             20            40            60            80            100
                      <30leadstoincreasingly bad generalization performance,                                                                    Training Iterations (thousand steps)
                      and when NFEs drops below 20, model training frequently                                           Figure 7. Empiricalevidenceofhowourmethodconstrainsρ(Jf ).
                                                                                                                                                                                                                θ
                      diverge as a result of extremely noisy gradients. We provide                                      In contrast, insufﬁcient NFEs (e.g., T=16) at training time cause a
                      morecomprehensive results in Table 5 in the Appendix.                                             DEQ-Transformer model to explode early in the training phase.
                      LikeDEQs,theregularizedDEQsarememoryefﬁcient,con-
                      suming about 45% less training memory than Transformer-                                           and 1b, where we show that early stopping at threshold
                      XL.Moreover,weﬁndtheJacobian-regularized DEQsre-                                                  T =6still yields good convergence with Jacobian regular-
                      duce over 50% memory consumption of the original DEQs                                             ization. We also demonstrate a more stable backward pass
                      at inference time (when both using Broyden’s method) due                                          convergence throughout training in Appendix B. On the
                      to faster/stabler convergence, suggesting its effectiveness in                                    muchlarger-scale ImageNet, where we deal with 224×224
                      addressing the hidden solver cost issue discussed in Sec. 3.4.                                    images, the factor of reduction in NFE is not as strong (e.g.,
                                                                                                                        from 27 to 14 iterations, due to the receptive ﬁeld issue;
                      5.3. CIFAR-10 and ImageNet Classiﬁcation                                                          we’ll explain this in Section 5.5) but still yields a roughly
                      We additionally conduct experiments on vision tasks                                               2×acceleration. ThisshowsthattheJacobianregularization
                      using the recent multiscale deep equilibrium networks                                             is effective in large-scale computer vision tasks, and in the
                      (MDEQ)(Baietal.,2020), which drive multiple feature res-                                          presence of multiple equilibrium points. However, we also
                      olutions to their equilibria simultaneously. Because of the                                       note that as with DEQ-Transformers on WikiText-103, we
                      need to maintain high- and low-resolutional feature maps at                                       notice a small slip in accuracy, which may be a result of
                      all iterative steps and generally higher channel dimensions in                                    constraining model parameterizations.
                      f , MDEQsaresubstantially slower than conventional net-                                           Figure 6 provides a visual comparison of different models
                        θ                                                                                               with respect to three metrics: performance, inference speed,
                      works like ResNets (which operate on progressively down-
                      sampled feature maps). This makes acceleration vital to                                           and training memory. These are reported on the CIFAR-
                      broader adoption of multiscale implicit models.                                                   10 dataset. For the ﬁrst time, we have an implicit-depth
                      The results of applying Jacobian regularization on multi-                                         model that runs with a competitive level of speed and accu-
                      scale DEQs for image classiﬁcation are shown in Table 2.                                          racy as large explicit networks such as ResNet-101, while
                      OnCIFAR-10,whereastheunregularizedDEQmodelsused                                                   consuming much less memory.
                      17 NFEs to reach the reported competitive level of perfor-                                        5.4. Effect of Jacobian Regularization on ρ(J )
                                                                                                                                                                                                  f
                      mance, our DEQ with Jacobian regularization can converge                                                                                                                     θ
                      well even within 6 iterations (in fact, we ﬁnd smaller NFE                                        In addition to the synthetic study, we also verify that the Ja-
                      values still trains, but signiﬁcantly hurts generalization per-                                   cobian regularization is indeed effectively constraining con-
                      formance). This improvement is also obvious in Figure 1a                                          ditioning of Jf . Note that the underlying Jacobian matrices
                                                                                                                                              θ
