                        on specific rows but is typically expensive. LFS shows mixed reliability: it matches or
                        surpassesMVonsomesymbolictasks(e.g.,AIMEvariantsforGPT-OSS-120BandQwen3-
                        235B) but underperforms on several models. FFS delivers substantial token savings (com-
                        monlyreducingMVtokenusagebytensofpercentupto∼90%),yetitsaccuracyimpactis
                        model-dependent—competitiveinsomecases(e.g.,Dapo-Qwen-32B,Qwen3-235B,certain
                        R1entries) and substantially lower in others (e.g., DeepSeek, Qwen3).
                        Model-familycomparison
                        • Qwen-derivedmodels(Qwen3-32B,Qwen3-235B-Instruct): MVandLFSoftenleadon
                          math tasks (AIME24/25), while FFS gives the largest efficiency gains but is not uni-
                          formlytheaccuracywinner.
                        • DeepSeek family (DeepSeek, R1, R1-Distill-Qwen): MV is the most stable high-
                          accuracy choice; LFS sometimes rivals MV, and FFS reduces compute dramatically but
                          withvariable accuracy trade-offs.
                        • General-purpose large models (GPT-OSS-120B, QwQ-32B): MV/LFS usually secure
                          top accuracy; FFS offers strong efficiency with mixed accuracy effects (near-match in
                          somecases,notabledropsinothers).
                        Task-level behavior.  GPQA is relatively robust across decoding methods (small accu-
                        racy spread); FFS often preserves GPQA performance while saving tokens. AIME24 and
                        the AIME25subsets are more sensitive: MV and LFS usually perform better on structured
                        symbolic reasoning, whereas FFS can be competitive for certain large models but may de-
                        gradeaccuracyforothers.
                        Takeaway. There is no single best decoding method across models and tasks. MV re-
                        mainsthesafestoptionforaccuracyathighercomputecost;LFSisaviablemiddleground
                        forsymbolicproblems;BSisoccasionallyusefulbutexpensive;FFSisattractivewhencom-
                        pute is constrained but requires careful evaluation per model/task due to mixed accuracy
                        outcomes.
                        C Model-wisePlots
                        Figures 6 and 7 contain the FFS and LFS curves, for each of the eight models individually.
                        Figure 6: Accuracy versus token usage for different models. FFS-k variants are shown in
                        distinct colors (one color per k). Marker size encodes the value of N, with larger markers
                        representing larger N.
                                                                  13
