more

8
ES
% es
a é
% Xx 5 s
>. e\\$lble &
% e\eeees 2 5
® a Q a
% oO é
2 & ‘
&
% ‘ Oy \ | gor
% ‘atio
50, @o\ Bea y
49 ie) oe Row e Tom
f Pay if y RS fro}
cy group is
aun qysne> Y
4 OuM f 4
Yy event fh
p lapPened
pend pikarteraacy v
ow | trey
4
. Z Yas
2
eS a
3 ¢ be

2Uaq3

2
os

Poveda
puores

(a) For span type answers

dig

% SS
2
he >
2 Pee g
Ne
> Vex 2
at » Me € eI
sey Yee a
AWE
. By Os >
Pp Vey BS hee g
& Dag =
Ey 85599) 5 me
z
joy Capi Oley wae
apy

how many __ }yards

quened

08) | oe? sen? . “ Was
% »
a) as £ 3 Vi,
> G & Se
S
ep
of
aN €/ x &
< $ a o
> a
g
aa
¥ s &
>
3 4,

ayy

(b) For number type answers

Figure 1: Distribution of the most popular question prefixes for two different subsets of the training data.

bers, and dates. In addition, it defines functions
that operate on these elements, such as counters and
filters.’ Following Krishnamurthy et al. (2017),
we use the argument and return types of these
functions to automatically induce a grammar to
constrain the parser. We also add context-specific
rules to produce strings occurring in both question
and paragraph, and those paragraph strings that are
neighbors of question tokens in the GloVe embed-
ding space (Pennington et al., 2014), up to a cosine
distance of d.° The complete set of functions used
in our language and their induced grammar can be
found in the code release.

Training and inference During training, the
KDG parser maximizes the marginal likelihood of
a set of (possibly spurious) question logical forms
that evaluate to the correct answer. We obtain this
set by performing an exhaustive search over the
grammar up to a preset tree depth. At test time, we
use beam search to produce the most likely logical
form, which is then executed to predict an answer.

5.2 SQuAD-style Reading Comprehension

We test four different SQuAD-style reading com-
prehension models on DROP: (1) BiDAF (Seo
et al., 2017), which is the adversarial baseline

"For example filter_number_greater takes a set of
predicate-argument structures, the name of a relation, and a
number, and returns all those structures where the numbers
in the argument specified by the relation are greater than the
given number.

Sd — 0.3 was manually tuned on the development set.

we used in data construction (66.8% EM on
SQuAD 1.1); (2) QANet (Yu et al., 2018), cur-
rently the best-performing published model on
SQuAD 1.1 without data augmentation or pre-
training (72.7% EM); (3) QANet + ELMo, which
enhances the QANet model by concatenating pre-
trained ELMo representations (Peters et al., 2018)
to the original embeddings (78.7% EM); (4) BERT
(Devlin et al., 2019), which recently achieved im-
provements on many NLP tasks with a novel pre-
training technique (84.7% EM).”

These models require a few minor adaptations
when training on DROP. While SQUAD provides
answer indices in the passage, our dataset only
provides the answer strings. To address this, we use
the marginal likelihood objective function proposed
by Clark and Gardner (2018), which sums over
the probabilities of all the matching spans.'° We
also omitted the training questions which cannot
be answered by a span in the passage (45%), and
therefore cannot be represented by these systems.

For the BiDAF baseline, we use the implementa-
tion in AllenNLP but change it to use the marginal
objective. For the QANet model, our settings differ
from the original paper only in the batch size (16
v.s. 32) and number of blocks in the modeling layer

°The first three scores are based on our own im-
plementation, while the score for BERT is based on
an open-source implementation from Hugging Face:
https://github.com/huggingface/pytorch-pretrained-bert

‘For the black-box BERT model, we convert DROP to
SQuaAD format by using the first match as the gold span.

2373
