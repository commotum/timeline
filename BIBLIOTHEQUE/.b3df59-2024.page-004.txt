                                                                               TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                 the loss is also taken over the outputs of the in-context                                                                              In-Context Examples                             Test                 Model Predictions
                                 demonstrations, which encourages the model to correctly
                                                                                                                                                                                                                                            L
                                 predict the demonstrationoutputsafterseeingtheprevious                                                                                                                                                     C
                                                                                                                                                                                                                                            I
                                 demonstrations:
                                                                                                                                                               C
                                                                                                                                                               R
                                                                                                                                                               A
                                                                                K
                                             outputs             label         X                                                                                                                                                            T
                                         L               =L + L (y |x ,y ,...,x ;θ)                                                                                                                                                         T
                                                                                                                                                                                                                                            T
                                             LM                  LM                       LM k 1 1                             k                                                                                          ?
                                                                               k=1
                                                                                                                                                                                                                                            L
                                                                                                                                                                                                                     < [ ] { < ( ) 
                                                                                                                                                                     (< [ ] { < ( ) 
         ( < { ( ) { } 

                            • Loss on inputs and outputs The loss is taken over all                                                                                                                                                              } ]
                                                                                                                                                                                                                                            C
                                                                                                                                                                                                                                            I
                                                                                                                                                                                                                     > } [ ] ( { }
                                                                                                                                                               H
                                                                                                                                                                     > } [ ] ( { }   
*       } ( < > ) >      
*
                                 tokens, encouraging the model to learn the structure of x                                                                     B
                                                                                                                                                               B
                                                                                                                                                                                                                                            T
                                                                                                                                                                            ) >                       )
                                                                                                                                                                                                                            ?
                                                                                                                                                                                                                                            T
                                                                                                                                                                                                                                                 ) >   
                                 as well as y:                                                                                                                                                                                              T
                                                                            K                                                                             Figure 3. Example of ARC and BBH tasks that the model success-
                                          all            outputs          X
                                      L =L                           +            L (x |x ,y ,...,y                                  ; θ)                  fully solves only after applying TTT.
                                          LM             LM                           LM k 1 1                               k−1
                                                                          k=1                                                                              4.2. Experimental Details
                                This method, which requires learners to generate task                                                                      Model architecture & optimization                                                      For our abla-
                                 inputs as well as outputs, is analogous to existing unsu-                                                                 tion experiments, we use the 1B-parameter Llama-3.2
                                 pervised TTT objectives (Sun et al., 2020).                                                                               model (Llama Team, 2024). For our final results in Sec-
                                                                                                                                                           tion 4.6, we use the 8B Llama 3 model. We use Low-Rank
                            WefindinSections 4.3 and 5.3 that the first method (taking                                                                     Adaptation (LoRA; Hu et al., 2022) for parameter-efficient
                            the loss over both demonstration and test outputs) works                                                                       test-time training. More details are given in Appendix C.2.
                            best.
                                                                                                                                                           Fine-tuning before TTT                                  While TTToffers task-specific
                            3.3. Parametrization                                                                                                           adaptation, the initial capabilities of the base model signif-
                            Once we have the test-time training dataset D                                                               (con-              icantly influence its final performance (Section 4.4). We
                                                                                                                               TTT                         developed several approaches for generating synthetic train-
                            structed via either the in-context or direct I/O approach), we                                                                 ing data to enhance the base model’s abstract reasoning
                            perform a small number of gradient steps on task-specific                                                                      capabilities through fine-tuning, exploring both automated
                            LoRA adapters (Hu et al., 2022). This approach allows                                                                          and semi-automated methods for task generation. This is
                            computationally efficient adaptation while maintaining the                                                                     complementary to TTT as the base model is fine-tuned on
                            model’s general capabilities. By default, we learn task-                                                                       tasks distinct from those tested on, when TTT is applied. De-
                            specific LoRA adapters for each ARC or BBH task at test-                                                                       tails on our data generation strategies, as well as the effects
                            time. That is, we obtain K different LoRA adapters, where                                                                      of various data sources and model sizes on performance, are
                            Kis the number of test tasks. We also experiment with                                                                          provided in Appendix B. The fine-tuned base model serves
                            using a single shared LoRA adapter from the aggregated                                                                         as the foundation for all subsequent experiments.
                            dataset of few-shot examples drawn from multiple tasks
                            (bottom row in Figure 2)—a test-time version of meta-ICL                                                                       Evaluation                  Thesuccess criterion requires producing an
                            (Minetal., 2022a). We find that the shared adapter degrades                                                                    exact match for all test outputs (no partial credit). Following
                            performance on ARC, whereas it improves performance on                                                                         the standard ARC scoring criteria, we use the pass@2 met-
                            BBH.Wediscussthisinmoredetail in Section 5.3.                                                                                  ric and produce 2 attempts for each test input. The original
                            4. Abstraction and Reasoning Corpus                                                                                            training and validation sets consist of 400 tasks each. How-
                                                                                                                                                           ever, for efficient evaluation purposes, we randomly pick 80
                            4.1. Background                                                                                                                balanced ARC tasks from the ARC validation set, including
                                                                                                                                                           20 easy, 20 medium, 20 hard, 20 expert tasks according to
                            TheAbstraction and Reasoning Corpus (ARC) aims to eval-                                                                        the classification in (LeGris et al., 2024) (see Appendix A.2
                            uate the abstract reasoning capabilities of language models                                                                    for this task list). Except for our final results, we use this
                            through their ability to solve visual puzzles. Each puzzle                                                                     subset of ARC tasks throughout our experiments. We limit
                            (henceforth referred to as a task) consists of input-output                                                                    D           to have a maximum of 250 examples per task for ef-
                                                                                                                                                               TTT
                            pairs of 2D grids (up to 30×30 in size) containing shapes or                                                                   ficiency reasons. Appendix C.2 provides additional details
                            patterns in up to 10 different colors, as displayed in Figure 3.                                                               onthe hyperparameters.
                            Theoutput of each pair is obtained by applying an intuitive
                            and shared transformation or rule y = f(x). Each task has                                                                      Inference                Oneofthemostcommontechniquestoscale
                            2-7 demonstration examples and 1-3 test examples.                                                                              inference-time compute is to use temperature sampling to
                                                                                                                                                     4
