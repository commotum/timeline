                        Published as a conference paper at ICLR 2024
                            Category    Count   Examples
                            Bug         442     “Bug”(179); “type:bug” (114); “bug” (57); “type: bug” (48);
                                                “Bug:beetle:” (23); “status: confirmed bug” (20);;
                            Feature     167     “type:enhancement” (47); “Enhancement” (25); “New feature” (24);
                                                “Feature Request” (22); “type: enhancement” (19);
                                                “Enhancement :star:” (15); “New Feature” (7); “enhancement” (6);
                            Regression  39      “type: regression” (14); “Regression” (14); “regression” (8);
                            Other       1641    “help wanted” (71); “good first issue” (66); “printing” (58);
                                                “extensions:autodoc” (58); “Easy” (57); “Easy to Fix” (54);
                                                “domains:py” (27); “core” (26); “sets” (23); “Wrong Result” (23);
                                                “units” (22); “Good first issue” (21);
                              Table 13: Categories of tags associated with issues from SWE-bench’s task instances.
                             Model            Retrieval Setting   Generations  Applies   Fixed  Patch Fix %
                             ChatGPT-3.5      BM2513k                2,270       604      363      60.1%
                             ChatGPT-3.5      “Oracle”               1,262       500      222      44.4%
                             ChatGPT-3.5      “Oracle”-collapsed     1,811       939      420     44.73%
                             Claude 2         BM2513k                2,281       988      302     30.57%
                             Claude 2         “Oracle”               2,138      1,441     360     24.98%
                             Claude 2         “Oracle”-collapsed     2,242      1,564     465     29.73%
                             GPT-4            BM2527k                 573         85      59      69.41%
                             GPT-4            “Oracle”                462        195      121     62.05%
                             GPT-4            “Oracle”-collapsed     2,292      1,116     684     61.29%
                             SWE-Llama13b     BM2513k                2,010      1,230     369      30.0%
                             SWE-Llama13b     “Oracle”               2,125      1,532     378     24.67%
                             SWE-Llama7b      BM2513k                2,139      1,187     340     28.64%
                             SWE-Llama7b      “Oracle”               2,119      1,503     298     19.83%
                        Table 14: Statistics for how many patches for 2,294 task instances were generated, applied suc-
                        cessfully, and required a post-generation fix to apply successfully for each [model, retrieval setting]
                        combination during evaluation. The GPT-4 BM25 27k and “Oracle” settings were ran on the 25%
                        subset. The GPT-4 “Oracle”-collapsed setting was run on the full SWE-bench test set.
                        A.6   DEVELOPMENT SET CHARACTERIZATION
                        In addition to the evaluation test set, we also provide a development set for evaluating models and
                        tuning hyperparameters before running on the final test set. Following the style of tables and graphs
                        frombefore,wepresentsimilarstatisticstocharacterizethe225developmenttaskinstances(slightly
                        more than 10% of the main evaluation set) collected from 6 open source repositories with licenses
                        permitting such usage. The development set was collected following the exact same set of method-
                        ologies and filters as the main evaluation set. In addition to the pre-existing steps, we also filter the
                        development set to keep task instances that were created after January 1, 2019. Similar to Table 12,
                        in Table 15, we briefly summarize the purpose and licenses of the 6 selected repositories.
                        Following Table 13, we also list the tags associated with the the development set tasks in Table 16,
                        againshowcasingthediversityandcoverageoftasktypesbeyondfixingbugs. Comparedtothemain
                        evaluation tasks, we can also see tags (e.g., “Crash :collision:”, “io”) that refer to issues presenting
                        problems which are unique to the repositories in the development set.
                        Following Table 1, we present the same set of repository-specific average statistics for the 6 repos-
                        itories in the development set in Table 17. Across the entire development set, each task instance
                        has 19.9 average / 2 median F2P tests. There are 171.3 average / 79.0 median P2P tests, and 191.2
                        average / 101.0 median tests in total per task instance.
                                                                   23
