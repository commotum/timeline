                                         DROP:AReadingComprehensionBenchmark
                                         Requiring Discrete Reasoning Over Paragraphs
                                                               ♣                       ♦∗                      ♥
                                               DheeruDua ,YizhongWang ,PradeepDasigi ,
                                                                  ♥+                      ♣                           ♠
                                         Gabriel Stanovsky           , Sameer Singh , and Matt Gardner
                                                        ♣University of California, Irvine, USA
                                                          ♦Peking University, Beijing, China
                                    ♥Allen Institute for Artiﬁcial Intelligence, Seattle, Washington, USA
                                      ♠Allen Institute for Artiﬁcial Intelligence, Irvine, California, USA
                                              +University of Washington, Seattle, Washington, USA
                                                                     ddua@uci.edu
                                           Abstract                               this new benchmark, which we call DROP, a sys-
                                                                                  temis given a paragraph and a question and must
                        Reading comprehension has recently seen                   performsomekindofDiscreteReasoningOverthe
                        rapidprogress, withsystemsmatchinghumans                  text in the Paragraph to obtain the correct answer.
                        onthemostpopulardatasetsforthetask. How-                     These questions that require discrete reasoning
                        ever, a large body of work has highlighted                (such as addition, sorting, or counting; see Table 1)
                        the brittleness of these systems, showing that            are inspired by the complex, compositional ques-
                        there is much work left to be done. We in-                tions commonly found in the semantic parsing lit-
                        troduce a new English reading comprehension
                        benchmark, DROP, which requires Discrete                  erature. We focus on this type of questions because
                        Reasoning Over the content of Paragraphs. In              they force a structured analysis of the content of the
                        this crowdsourced, adversarially-created, 96k-            paragraph that is detailed enough to permit reason-
                        question benchmark, a system must resolve                 ing. Our goal is to further paragraph understand-
                        referencesinaquestion,perhapstomultiplein-                ing; complex questions allow us to test a system’s
                        put positions, and perform discrete operations            understanding of the paragraph’s semantics.
                        over them (such as addition, counting, or sort-              DROP is also designed to further research on
                        ing). These operations require a much more
                        comprehensiveunderstandingofthecontentof                  methods that combine distributed representations
                        paragraphs than what was necessary for prior              with symbolic, discrete reasoning. In order to
                        datasets.   We apply state-of-the-art methods             do well on this dataset, a system must be able to
                        from both the reading comprehension and se-               ﬁndmultiple occurrences of an event described in
                        mantic parsing literatures on this dataset and            a question (presumably using some kind of soft
                        showthatthebestsystemsonlyachieve32.7%                    matching), extract arguments from the events, then
                        F on our generalized accuracy metric, while
                          1                                                       perform a numerical operation such as a sort, to
                        expert human performance is 96.4%. We ad-                 answer a question like “Who threw the longest
                        ditionally present a new model that combines
                        reading comprehension methods with simple                 touchdown pass?”.
                        numerical reasoning to achieve 47.0% F .                     Weconstructed this dataset through crowdsourc-
                                                                   1
                                                                                  ing, ﬁrst collecting passages from Wikipedia that
                    1   Introduction                                              are easy to ask hard questions about, then encour-
                    The task of reading comprehension, where sys-                 aging crowd workers to produce challenging ques-
                    tems must understand a single passage of text well            tions. This encouragement was partially through
                    enough to answer arbitrary questions about it, has            instructions given to workers, and partially through
                    seen signiﬁcant progress in the last few years, so            the use of an adversarial baseline: we ran a base-
                    muchthat the most popular datasets available for              line reading comprehension method (BiDAF) (Seo
                    this task have been solved (Chen et al., 2016; De-            et al., 2017) in the background as crowd workers
                    vlin et al., 2019). We introduce a substantially              werewritingquestions,requiringthemtogiveques-
                    morechallenging English reading comprehension                 tions that the baseline system could not correctly
                    dataset aimed at pushing the ﬁeld towards more                answer. This resulted in a dataset of 96,567 ques-
                    comprehensive analysis of paragraphs of text. In              tions from a variety of categories in Wikipedia,
                                                                                  with a particular emphasis on sports game sum-
                        ∗                                                         maries and history passages. The answers to the
                        WorkdoneasaninternattheAllenInstituteforArtiﬁcial
                    Intelligence in Irvine, California.                           questions are required to be spans in the passage or
                                                                             2368
                                                      Proceedings of NAACL-HLT 2019, pages 2368–2378
                                                                               c
                                Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics
                  question, numbers, or dates, which allows for easy       et al., 2019), tracking entity state changes (Mishra
                  and accurate evaluation metrics.                         et al., 2018; Ostermann et al., 2018) or a particular
                     Wepresent an analysis of the resulting dataset        kind of “multi-step” reasoning over multiple doc-
                  to show what phenomena are present. We ﬁnd               uments (Welbl et al., 2018; Khashabi et al., 2018).
                  that many questions combine complex question se-         Similar facets are explored in medical domain
                                                                                                              ˇ
                  mantics with SQuAD-style argument ﬁnding; e.g.,          datasets (Pampari et al., 2018; Suster and Daele-
                  in the ﬁrst question in Table 1, BiDAF correctly         mans,2018)whichcontainautomaticallygenerated
                  ﬁnds the amount the painting sold for, but does not      queries on medical records based on predeﬁned
                  understand the question semantics and cannot per-        templates. We applaud these efforts, which offer
                  form the numerical reasoning required to answer          good avenues to study these additional phenomena.
                  the question. Other questions, such as the ﬁfth          However, we are concerned with paragraph under-
                  question in Table 1, require ﬁnding all events in the    standing, which on its own is far from solved, so
                  passage that match a description in the question,        DROPhasnoneoftheseadditional complexities.
                  then aggregating them somehow (in this instance,         It consists of single passages of text paired with
                  by counting them and then performing an argmax).         independent questions, with only linguistic facil-
                                                                                                                   1
                  Very often entity coreference is required. Table 1       ity required to answer the questions.     One could
                  gives a number of different phenomena, with their        argue that we are adding numerical reasoning as
                  proportions in the dataset.                              an “additional complexity”, and this is true; how-
                     Weused three types of systems to judge base-          ever, it is only simple reasoning that is relatively
                  line performance on DROP: (1) heuristic baselines,       well-understood in the semantic parsing literature,
                  to check for biases in the data; (2) SQuAD-style         and we use it as a necessary means to force more
                  reading comprehension methods; and (3) semantic          comprehensive passage understanding.
                  parsers operating on a pipelined analysis of the pas-       Many existing algebra word problem datasets
                  sage. Thereadingcomprehensionmethodsperform              also contain similar phenomena to what is in
                  the best, with our best baseline achieving 32.7%         DROP(Koncel-Kedziorski et al., 2015; Kushman
                  F onourgeneralized accuracy metric, while ex-            et al., 2014; Hosseini et al., 2014; Clark et al., 2016;
                    1
                  pert human performance is 96.4%. Finally, we             Ling et al., 2017). Our dataset is different in that it
                  contribute a new model for this task that combines       uses much longer contexts, is more open domain,
                  limited numerical reasoning with standard reading        and requires deeper paragraph understanding.
                  comprehension methods, allowing the model to an-            Semantic parsing The semantic parsing litera-
                  swer questions involving counting, addition and          ture has a long history of trying to understand com-
                  subtraction. This model reaches 47% F , a 14.3%          plex, compositional question semantics in terms of
                                                            1
                  absolute increase over the best baseline system.         somegroundedknowledgebaseorother environ-
                     Thedataset, code for the baseline systems, and        ment (Zelle and Mooney, 1996; Zettlemoyer and
                  a leaderboard with a hidden test set can be found        Collins, 2005; Berant et al., 2013a, inter alia). It
                  at https://allennlp.org/drop.                            is this literature that we modeled our questions on,
                                                                           particularly looking at the questions in the Wik-
                  2   Related Work                                         iTableQuestions dataset (Pasupat and Liang, 2015).
                  QuestionansweringdatasetsWithsystemsreach-               If we had a structured, tabular representation of
                                                                           the content of our paragraphs, DROP would be
                  ing human performance on the Stanford Question           largely the same as WikiTableQuestions, with simi-
                  Answering Dataset (SQuAD) (Rajpurkar et al.,             lar (possibly even simpler) question semantics. Our
                  2016), many follow-on tasks are currently being          noveltyisthatwearetheﬁrsttocombinethesecom-
                  proposed. All of these datasets throw in additional      plex questions with paragraph understanding, with
                  complexities to the reading comprehension chal-          the aim of encouraging systems that can produce
                  lenge, around tracking conversational state (Reddy       comprehensive structural analyses of paragraphs,
                  et al., 2019; Choi et al., 2018), requiring passage      either explicitly or implicitly.
                  retrieval (Joshi et al., 2017; Yang et al., 2018; Tal-      Adversarial dataset construction We continue
                  morandBerant,2018), mismatched passages and
                                                         ´                    1Somequestions in our dataset require limited sports do-
                  questions (Saha et al., 2018; Kocisky et al., 2018;      mainknowledgetoanswer;weexpectthatthereareenough
                  Rajpurkaretal.,2018), integrating knowledgefrom          such questions that systems can reasonably learn this knowl-
                  external sources (Mihaylov et al., 2018; Zhang           edge from the data.
                                                                       2369
                      Reasoning     Passage (some parts shortened)                           Question                 Answer       BiDAF
                      Subtraction   That year, his Untitled (1981), a painting of a haloed,  How many more dol-       4300000      $16.3
                      (28.8%)       black-headed man with a bright red skeletal body, de-    lars was the Untitled                 million
                                    picted amid the artists signature scrawls, was sold by   (1981) painting sold
                                    RobertLehrmanfor$16.3million,wellaboveits$12             for than the 12 million
                                    million high estimate.                                   dollar estimation?
                      Comparison    In1517,theseventeen-year-oldKingsailedtoCastile.         Where did Charles        Castile      Aragon
                      (18.2%)       There, his Flemish court .... In May 1518, Charles       travel to ﬁrst, Castile
                                    traveled to Barcelona in Aragon.                         or Barcelona?
                      Selection     In 1970, to commemorate the 100th anniversary of the     Who was the Uni-         Don          Baker
                      (19.4%)       founding of Baldwin City, Baker University professor     versity professor that   Mueller
                                    and playwright Don Mueller and Phyllis E. Braun,         helped produce The
                                    Business Manager, produced a musical play entitled       Ballad Of Black Jack,
                                    TheBalladOfBlackJacktotellthestoryoftheevents            Ivan Boyd or Don
                                    that led up to the battle.                               Mueller?
                      Addition      Before the UNPROFORfully deployed, the HV clashed        WhatdatedidtheJNA        3 March      2 March
                      (11.7%)       with an armed force of the RSK in the village of Nos     form a battlegroup to    1992         1992
                                                                     ˇ                       counterattack after the
                                    Kalik, located in a pink zone near Sibenik, and captured
                                    the village at 4:45 p.m. on 2 March 1992. The JNA        village of Nos Kalik
                                    formed a battlegroup to counterattack the next day.      wascaptured?
                      Count         Denver would retake the lead with kicker Matt Prater     Which kicker kicked      John         Matt
                      (16.5%)       nailing a 43-yard ﬁeld goal, yet Carolina answered as    the most ﬁeld goals?     Kasay        Prater
                      and Sort      kicker John Kasay ties the game with a 39-yard ﬁeld
                      (11.7%)       goal. ... Carolina closed out the half with Kasay nail-
                                    ing a 44-yard ﬁeld goal. ... In the fourth quarter, Car-
                                    olina sealed the win with Kasay’s 42-yard ﬁeld goal.
                      Coreference    JamesDouglaswasthesecondsonofSirGeorgeDou-              How many years af-       10           1553
                      Resolution    glas of Pittendreich, and Elizabeth Douglas, daughter    ter he married Eliza-
                      (3.7%)        David Douglas of Pittendreich. Before 1543 he mar-       beth did James Dou-
                                    ried Elizabeth, daughter of James Douglas, 3rd Earl of   glas succeed to the ti-
                                    Morton. In1553JamesDouglassucceededtothetitle            tle and estates of his
                                    andestates of his father-in-law.                         father-in-law?
                      Other         Although the movement initially gathered some 60,000     How many adherents       15000        60,000
                      Arithmetic    adherents, the subsequent establishment of the Bulgar-   were left after the es-
                      (3.2%)        ian Exarchate reduced their number by some 75%.          tablishment of the Bul-
                                                                                             garian Exarchate?
                      Set of        According to some sources 363 civilians were killed in   What were the 3 vil-     Kavadarci,   Negotino
                      spans         Kavadarci, 230 in Negotino and 40 in Vatasha.            lages that people were   Negotino,    and 40 in
                      (6.0%)                                                                 killed in?               Vatasha      Vatasha
                      Other         ThisAnnualFinancialReportisourprincipalﬁnancial          WhatdoesAFRstand         Annual       one of the
                      (6.8%)        statement of accountability. The AFR gives a compre-     for?                     Financial    Big Four
                                    hensive view of the Department’s ﬁnancial activities ...                          Report       audit ﬁrms
                    Table 1: Example questions and answers from the DROP dataset, showing the relevant parts of the associated
                    passage and the reasoning required to answer the question.
                    a recent trend in creating datasets with adversarial            Wepresent one such model in Section 6. Other re-
                    baselines in the loop (Paperno et al., 2016; Min-               lated work along these lines has been done by Reed
                    ervini and Riedel, 2018; Zellers et al., 2018; Zhang            and de Freitas (2016), Neelakantan et al. (2016),
                    et al., 2019; Zellers et al., 2019). In our case, in-           and Liang et al. (2017).
                    stead of using an adversarial baseline to ﬁlter auto-
                    matically generated examples, we use it in a crowd-             3    DROPDataCollection
                    sourcing task, to teach crowd workers to avoid easy
                    questions, raising the difﬁculty level of the ques-             In this section, we describe our annotation proto-
                    tions they provide.                                             col, which consists of three phases. First, we auto-
                                                                                    matically extract passages from Wikipedia which
                       Neural symbolic reasoning DROP is designed                   are expected to be amenable to complex questions.
                    to encourage research on methods that combine                   Second, we crowdsource question-answer pairs on
                    neural methods with discrete, symbolic reasoning.               these passages, eliciting questions which require
                                                                               2370
                    discrete reasoning. Finally, we validate the devel-            Statistic                    Train       Dev      Test
                    opmentandtest portions of DROP to ensure their                 Numberofpassages              5565       582       588
                    quality and report inter-annotator agreement.                  Avg. passage len [words]    213.45    191.62    195.12
                                                                                   Numberofquestions           77,409     9,536     9,622
                    Passage extraction       Wesearched Wikipedia for              Avg. question len [words]    10.79     11.17     11.23
                    passages that had a narrative sequence of events,              Avg. questions / passage     13.91     16.38     16.36
                                                                                   Question vocabulary size    29,929     8,023     8,007
                    particularly with a high proportion of numbers, as
                    our initial pilots indicated that these passages were           Table 2: Dataset statistics across the different splits.
                    the easiest to ask complex questions about. We
                    found that National Football League (NFL) game                workers and gradually reduced our worker pool to
                    summaries and history articles were particularly              workers who understood the task and annotated it
                    promising, and we additionally sampled from any               well. Each HIT paid 5 USD and could be com-
                   Wikipedia passage that contained at least twenty               pleted within 30 minutes, compensating a trained
                    numbers.2 This process yielded a collection of
                    about 7,000 passages.                                         worker with an average pay of 10 USD/ hour.
                                                                                     Overall, we collected a total of 96,567 question-
                    Question collection       WeusedAmazonMechani-                answer pairs with a total Mechanical Turk budget
                    cal Turk3 to crowdsource the collection of question-          of 60k USD (including validation). The dataset
                    answer pairs, where each question could be an-                wasrandomlypartitioned by passage into training
                    sweredinthecontextofasingleWikipediapassage.                  (80%), development (10%) and test (10%) sets, so
                    In order to allow some ﬂexibility during the annota-          all questions about a particular passage belong to
                    tion process, in each human intelligence task (HIT)           only one of the splits.
                    workers were presented with a random sample of                Validation      In order to test inter-annotator agree-
                    5 of our Wikipedia passages, and were asked to                ment and to improve the quality of evaluation
                    produce a total of at least 12 question-answer pairs          against DROP, we collected at least two additional
                    on any of these.                                              answers for each question in the development and
                       Wepresented workers with example questions                 test sets.
                    from ﬁve main categories, inspired by ques-                      In a separate HIT, workers were given context
                    tions from the semantic parsing literature (addi-             passages and a previously crowdsourced question,
                    tion/subtraction, minimum/maximum,counting,se-                and were asked to either answer the question or
                    lection and comparison; see examples in Table 1),             mark it as invalid (this occurred for 0.7% of the
                    to elicit questions that require complex linguistic           data, which we subsequently ﬁltered out). We
                    understanding and discrete reasoning. In addition,            found that the resulting inter-annotator agreement
                    to further increase the difﬁculty of the questions            wasgoodandonparwithotherQAtasks;overall
                    in DROP, we employed a novel adverserial anno-                Cohen’s κ was 0.74, with 0.81 for numbers, 0.62
                    tation setting, where workers were only allowed               for spans, and 0.65 for dates.
                    to submit questions which a real-time QA model
                                               4
                    BiDAFcouldnot solve.                                          4    DROPDataAnalysis
                       Next, each worker answered their own question
                    with one of three answer types: spans of text from            In the following, we quantitatively analyze proper-
                    either question or passage, a date (which was com-            ties of passages, questions, and answers in DROP.
                    moninhistoryandopen-domaintext)andnumbers,                    Different statistics of the dataset are depicted in Ta-
                    allowed only for questions which explicitly stated            ble 2. Notably, questions have a diverse vocabulary
                    a speciﬁc unit of measurement (e.g., “How many                of around 30k different words in our training set.
                    yards did Brady run?”), in an attempt to simplify             Question analysis        To assess the question type
                    the evaluation process.                                       distribution, we sampled 350 questions from the
                       Initially, we opened our HITs to all United States         training and development sets and manually anno-
                       2We used an October 2018 Wikipedia dump, as well as        tated the categories of discrete operations required
                    scraping of online Wikipedia.                                 to answer the question. Table 1 shows the distri-
                       3www.mturk.com                                             bution of these categories in the dataset. In addi-
                       4WhileBiDAFisnolongerstate-of-the-art,performanceis
                    reasonable and the AllenNLP implementation (Gardner et al.,   tion, to get a better sense of the lexical diversity of
                    2017) made it the easiest to deploy as a server.              questions in the dataset, we ﬁnd the most frequent
                                                                             2371
                     AnswerType           Percent            Example        removes articles and does other simple normaliza-
                                                                            tion, and our F score is based on that used by
                     NUMBER                  66.1                  12                         1
                     PERSON                  12.2          Jerry Porter     SQuAD.SinceDROPisnumeracy-focused,wede-
                     OTHER                    9.4               males       ﬁne F to be 0 when there is a number mismatch
                     OTHERENTITIES            7.3            Seahawks              1
                     VERBPHRASE               3.5   Tomarrived at Acre      betweenthegoldandpredictedanswers, regardless
                     DATE                     1.5        3 March 1992       of other word overlap. When an answer has multi-
                  Table 3: Distribution of answer types in training set,    ple spans, we ﬁrst perform a one-to-one alignment
                  according to an automatic named entity recognition.       greedily based on bag-of-word overlap on the set
                                                                            of spans and then compute average F over each
                                                                                                                      1
                                                                            span. When there are multiple annotated answers,
                  trigram patterns in the questions per answer type.        both metrics take a max over all gold answers.
                  Weﬁndthatthedataset offers a huge variety of lin-         5.1   Semantic Parsing
                  guistic constructs, with the most frequent pattern
                  (“Whichteamscored”)appearinginonly4%ofthe                 Semantic parsing has been used to translate nat-
                  span type questions. For number type questions,           ural language utterances into formal executable
                  the 5 most frequent question patterns all start with      languages (e.g., SQL) that can perform discrete
                  “Howmany”,indicating the need to perform count-           operations against a structured knowledge repre-
                  ing and other arithmetic operations. A distribution       sentation, such as knowledge graphs or tabular
                  of the trigrams containing the start of the questions     databases (Zettlemoyer and Collins, 2005; Berant
                  are shown in Figure 1.                                    et al., 2013b; Yin and Neubig, 2017; Chen and
                                                                            Mooney,2011,inter alia). Since many of DROP’s
                  Answeranalysis        Todiscern the level of passage      questions require similar discrete reasoning, it is
                  understanding needed to answer the questions in           appealing to port some of the successful work in
                  DROP,weannotatethesetofspansinthepassage                  semantic parsing to the DROP dataset. Speciﬁ-
                  that are necessary for answering the 350 questions        cally, we use the grammar-constrained semantic
                  mentioned above. We ﬁnd that on an average 2.18           parsing model built by Krishnamurthy et al. (2017)
                  spans need to be considered to answer a question          (KDG) for the WIKITABLEQUESTIONS tabular
                  and the average distance between these spans is           dataset (Pasupat and Liang, 2015).
                  26 words, with 20% of samples needing at least            Sentence representation schemes           We experi-
                  3 spans (see appendix for examples). Finally, we          mented with three paradigms to represent para-
                  assess the answer distribution in Table 3, by run-        graphs as structured contexts: (1) Stanford de-
                  ning the part-of-speech tagger and named entity           pendencies (de Marneffe and Manning, 2008, Syn
                                           5
                  recognizer from spaCy to automatically partition          Dep); which capture word-level syntactic relations,
                  all the answers into various categories. We ﬁnd that      (2) Open Information Extraction (Banko et al.,
                  a majority of the answers are numerical values and        2007, Open IE), a shallow semantic representation
                  proper nouns.                                             which directly links predicates and arguments; and
                                                                                                                           `
                  5    Baseline Systems                                     (3) Semantic Role Labeling (Carreras and Marquez,
                                                                            2005, SRL), which disambiguates senses for pol-
                  In this section we describe the initial baselines         ysemouspredicates and assigns predicate-speciﬁc
                  that we evaluated on the DROP dataset. We used                              6
                                                                            argument roles.     To adhere to KDG’s structured
                  three types of baselines: state-of-the-art semantic       representation format, we convert each of these rep-
                  parsers (§5.1), state-of-the-art reading comprehen-       resentations into a table, where rows are predicate-
                  sion models (§5.2), and heuristics looking for an-        argument structures and columns correspond to
                  notation artifacts (§5.3). We use two evaluation          different argument roles.
                  metrics to compare model performance: Exact-              Logical form language         Our logical form lan-
                  Match, and a numeracy-focused (macro-averaged)            guageidentiﬁes ﬁve basic elements in the table rep-
                  F score, which measures overlap between a bag-
                    1                                                       resentation: predicate-argument structures (i.e., ta-
                  of-words representation of the gold and predicted         blerows),relations(column-headers),strings,num-
                  answers. We employ the same implementation of
                  Exact-Match accuracy as used by SQuAD, which                  6WeusedtheAllenNLPimplementationsofstate-of-the-
                                                                            art modelsforalloftheserepresentations(Gardneretal.,2017;
                     5https://spacy.io/                                     Dozat et al., 2017; He et al., 2017; Stanovsky et al., 2018).
                                                                        2372
                                                                        ero           st
                                                                                                                                                                e
                                                                                                                                                                                  d
                                                                        m             r                                                                        e
                                                                                                                                                                                  i
                                                                                                                                                                 h
                                                                                                                                                                 t
                                                                                                                                                               h
                                                              s                                                                                    ce
                                                                                                                                                               t
                                                                                                                                                                                  d
                                                              a                                                                                     n
                                                                                                                                                    e
                                                               w                                   d                                                 r
                                                           e                                       e                                                 e
                                                           h                         d            r                                                    
                                                            t                        e                                                                i
                                                        e                                        co                                                   d
                                                         ht                         n            s                                                                                              e
                                                                                                                                                                                                r
                                                                                    e                                                             e
                                                      e                                                                                                                                        e
                                                                          e                                                                       h
                                                                                                                                                   t
                                                      ht              chchire am    p                                                                                                         w
                                                    e                 i             p                                                                                               s
                                                                                                                                                                     d
                                                                                                                                                                                    l
                                                                                                                                               e
                                                                                                                                                                   d
                                                    h                   h h   e                                                                                      i
                                                                                   a                                                                               i
                                                     t                h     theret                                                                                d
                                                                   g                                                                            h
                                                                          t                                                                                       i
                                                                                                                                                                e
                                                                                                                                                                     d
                                                                                                                                                                                   a
                                                                        w                                                                       t
                                                                   n                                                                                               d
                                                                                   h                                                                              d
                                                                      w                                                                                         h
                                                                                                        d                                                     e
                                                                                                                                                                                   o
                                                  e             s  o                                    a                                                       t
                                                  h                 l                                  h                                                      h
                                                                                                                                                                                  g
                                                                 a                                                                                            t
                                                   t                                           r                                                            e
                                                               d w                            e                                                             h
                                                                                                                                                                     s
                                                               i                                                                                            t
                                                                                                                                                        r
                                                                                              y                                            s
                                                                                                                                            a
                                                                                                                                                         e
                                                                d                                                                                                    n
                                                                                                                                                         t
                                                                                             a                                               w
                                                                                                                                                         f
                                                                                             l                                                                        o
                                                                                                                                                                      i
                                                              s                                                                                           a
                                               e           n  i                                                                                       r
                                                                                            p                                                                         t
                                                h                                                                                                      e
                                                t           o                                                                             e
                                                                                                                                                       t
                                                                                                                                           r
                                                                                                                                                                      p
                                                                                                                                                       f
                                                             w                                                p                            a
                                                                                                             u                                          a
                                                                          e                                 o
                                                          di               e                               r                                                          ce
                                                                          r                               g                                                       s
                                                                                                                                                                      r
                                                           d                                                                                                         s
                                                                           r                                                                       s
                                                                                                                                                                     s
                                                                          e                                                                                       e
                                                                                                                                                                      e
                                                                                                                                                                     e
                                                                                                                                                    d
                                                                                                                                                     r
                                                                                                                                                                               d
                                                                           e                                                                                          n
                                                                                                                                         d
                                                                                                                                                                      t
                                                                                                                                                                  s
                                                                                                                                         i
                                                                       w  h                                                                          a
                                                                                                                                                                      i
                                                                                                                                                                               l
                                                                     n                                                                            e
                                                                                                                                          d
                                                                                                                                                  l
                                                                                                                                                      y
                                                                                                                                                                   s
                                                        s                                                                                                            m
                                                                                                                                                                       n
                                                                                                                                                                    f
                                                                                                                                                   p
                                                                        o n wdid                                                                                     i
                                                         a           e     w                                                                                           i
                                                                                                                                                                      w
                                                                                                                                                    o
                                                                                                                                                                              e
                                                                                                                                                             s
                                                                                                                                                                   a
                                             e                            i                                                                                          t
                                                                                                                                                    e
                                                                                                                                                                s
                                             h            w           h                                                                                              o
                                                                        h                                                                                     h
                                              t                                                                                                      p
                                                                                                                                                                   p
                                                                                                                                                                y
                                                                                                                                                              t
                                                                                                                                                                               
                                                                                                  e                                                               s
                                                                                                                                                                  i
                                                                                                                                                s
                                                                                                                                                                 a
                                                                       w                          g                                                            n
                                                                                                                                        t
                                                                                                                                                 e
                                                                                                                                        o
                                                                                                 a                                                s
                                                                                                                                         n
                                                                                                                                                               o
                                                                                                                                                                 d
                                                                                                                                                  s
                                                                                         ch                                                        a
                                                                                                                                                                                                          e
                                                     d                                   i                                                          p                                                    h
                                                                                                                                                                m
                                                                                                                                                                                                        t
                                                       cke                                                    om                                            e
                                                        ki                              h                    fr                                              r
                                                                                                                                                              o
                                                                                                                                                        n
                                                                                                                                                  e
                                                                                                                                                         w
                                                                                                                                                   r
                                                                                      w                                                t
                                                                                                                                                               m
                                                                                                                                       o
                                                                                                                                                   a
                                                                                                                                                         o
                                                                                                                                        n
                                                                                                                                                                                                s
                                                                                                                                                                                               a
                                                                                                                                                            chd
                                                                                                                                                                                              w
                                                                                                                                                             u
                                                                                                                                                e
                                                      d                                                                                                      o
                                                                                                                                                 r
                                                       a                                                                                                      t
                                                       h                                                                                          e
                                                                                                      p                                            w
                                                                                                    ou
                                                                                                   r          s
                                                                                                   g         i
                                                                                                                                                        t
                                                                                                                                                         n
                                                                                                                                                          ce
                                                                                                                                                           r
                                                                                                                                                            e
                                           eht    thgu ca                                                                                                    p
                                                                                                                                                  f
                                                                                                                                                  o
                                                                                                                                        p
                                                                                                                                       e
                                                                                                                                      o
                                                                                                                                     p
                                                                                                                                    l
                                                                                                             in                     e
                                                              o h w                                                                                                how many         y
                                                                                                                                                                                     a
                                                                                                                                                                                      r
                                                                                                                                                                                       ds
                                                                                                                                                             i
                                                                                                                                                             n
                                                                                                  ev
                                                                                                    en
                                                                                                      t      h                                   rep
                                                                                                              a                                 ce
                                                                                                               pp
                                                                                                                 en                            n
                                                    werht                                         quar            ed                          t             chuot
                                                                                                     ter                                                                                     lo
                                                                                                       back                                               d                                    nge
                                                                                                                                                                                                 r
                                                                                                                                                         o
                                                                                                                                         from
                                            eht                                                             th                                     id   w
                                                                                                              re                                   d
                                                                                                               w                                      sn                                                w
                                                                                                                                                                                                         a
                                                                                                                                                                                                          s
                                                                                                                                                                   y
                                                                                                                                                                             p
                                                                                wh             w                                         eht         ew           ae         o             d
                                                                                                                                                                              i
                                                                                                                                                    r
                                                                                                                                                                                            i
                                                                                                                                                                                             
                                                                                                                                                   e
                                                                                                                                                                              n
                                                            s                     a             as                                                     w          sr           t             ere
                                                                                                                                                                               s
                                                                                                                                                       a
                                              tsr more   derco                    t                     th                                ercos       sessap                                   nce
                                                                  aeyidi                                  e                              d            d     etfa                  di
                                                                                                                                                                                   d
                                                                 r  d s ve         h                                                                        r        d
                                                      ht                ne         ap                                                             b                  di
                                                      e                t            pen                                                        neewte
                                                                ht                   ed                                                                 eht                            th
                                                                                                                                                                                        e
                                                                e eht ah     s             rst                                                                ti     ht
                                                                      ppe    eco                                                                                     e
                                                                     den     nd
                                                         (a) For span type answers                                                                 (b) For number type answers
                                     Figure 1: Distribution of the most popular question preﬁxes for two different subsets of the training data.
                              bers, and dates. In addition, it deﬁnes functions                                              we used in data construction (66.8% EM on
                              that operate on these elements, such as counters and                                           SQuAD 1.1); (2) QANet (Yu et al., 2018), cur-
                                         7
                              ﬁlters.         Following Krishnamurthy et al. (2017),                                         rently the best-performing published model on
                              we use the argument and return types of these                                                  SQuAD 1.1 without data augmentation or pre-
                              functions to automatically induce a grammar to                                                 training (72.7% EM); (3) QANet + ELMo, which
                              constrain the parser. We also add context-speciﬁc                                              enhances the QANet model by concatenating pre-
                              rules to produce strings occurring in both question                                            trained ELMo representations (Peters et al., 2018)
                              and paragraph, and those paragraph strings that are                                            to the original embeddings (78.7% EM); (4) BERT
                              neighbors of question tokens in the GloVe embed-                                               (Devlin et al., 2019), which recently achieved im-
                              ding space (Pennington et al., 2014), up to a cosine                                           provements on many NLP tasks with a novel pre-
                                                      8                                                                                                                               9
                              distance of d. The complete set of functions used                                              training technique (84.7% EM).
                              in our language and their induced grammar can be                                                    These models require a few minor adaptations
                              found in the code release.                                                                     whentraining on DROP. While SQuAD provides
                              Training and inference                            During training, the                         answer indices in the passage, our dataset only
                              KDGparsermaximizesthemarginallikelihood of                                                     providestheanswerstrings. Toaddressthis, weuse
                              a set of (possibly spurious) question logical forms                                            themarginallikelihoodobjectivefunctionproposed
                              that evaluate to the correct answer. We obtain this                                            by Clark and Gardner (2018), which sums over
                                                                                                                             the probabilities of all the matching spans.10 We
                              set by performing an exhaustive search over the                                                also omitted the training questions which cannot
                              grammaruptoapresettree depth. At test time, we                                                 be answered by a span in the passage (45%), and
                              use beam search to produce the most likely logical                                             therefore cannot be represented by these systems.
                              form, which is then executed to predict an answer.                                                  For the BiDAF baseline, we use the implementa-
                              5.2       SQuAD-styleReadingComprehension                                                      tion in AllenNLP but change it to use the marginal
                              Wetest four different SQuAD-style reading com-                                                 objective. For the QANet model, our settings differ
                              prehension models on DROP: (1) BiDAF (Seo                                                      from the original paper only in the batch size (16
                              et al., 2017), which is the adversarial baseline                                               v.s. 32) and number of blocks in the modeling layer
                                   7For example filter number greatertakesasetof                                                   9The ﬁrst three scores are based on our own im-
                              predicate-argument structures, the name of a relation, and a                                   plementation, while the score for BERT is based on
                              number, and returns all those structures where the numbers                                     an open-source implementation from Hugging Face:
                              in the argument speciﬁed by the relation are greater than the                                  https://github.com/huggingface/pytorch-pretrained-bert
                              given number.                                                                                      10For the black-box BERT model, we convert DROP to
                                   8d = 0.3 was manually tuned on the development set.                                       SQuADformatbyusingtheﬁrstmatchasthegoldspan.
                                                                                                                      2373
                  (6 v.s. 7) due to the GPU memory limit. We adopt           methods and symbolic reasoning. The model is
                  the ELMorepresentations trained on 5.5B corpus             trained by marginalizing over all execution paths
                  for the QANet+ELMo baseline and the large un-              that lead to the correct answer.
                  cased BERT model for the BERT baseline. The                6.1   ModelDescription
                  hyper-parameters for our NAQANet model (§6) are            Our NAQANet model follows the typical archi-
                  the same as for the QANet baseline.
                                                                             tecture of previous reading comprehension mod-
                  5.3    Heuristic Baselines                                 els, which is composed of embedding, encoding,
                  Arecent line of work (Gururangan et al., 2018;             passage-question attention, and output layers. We
                  Kaushik and Lipton, 2018) has identiﬁed that pop-          use the original QANet architecture for everything
                  ular crowdsourced NLP datasets (such as SQuAD              up to the output layer. This gives us a question rep-
                  (Rajpurkar et al., 2016) or SNLI (Bowman et al.,           resentation Q ∈ Rm×d, and a projected question-
                                                                                                             ¯      n×d
                  2015)) are prone to have artifacts and annotation          aware passage representation P ∈ R         . We have
                  biases which can be exploited by supervised algo-          four different output layers, for the four different
                  rithms that learn to pick up these artifacts as signal     kinds of answers the model can produce:
                  instead of more meaningful semantic features. We           Passage span      As in the original QANet model,
                  estimate artifacts by training the QANet model de-         to predict an answer in the passage we apply three
                  scribed in Section 5.2 on a version of DROP where          repetitions of the QANet encoder to the passage
                  either the question or the paragraph input repre-                          ¯
                                                                             representation P and get their outputs as M , M ,
                  sentation vectors are zeroed out (question-only                                                            0    1
                                                                             M respectively. Then the probabilities of the start-
                  and paragraph-only, respectively). Consequently,             2
                                                                             ing and ending positions from the passage can be
                  the resulting models can then only predict answer          computed as:
                  spans from either the question or the paragraph.                     p start
                     In addition, we devise a baseline that estimates                p       =softmax(FFN([M0;M1]),             (1)
                  the answer variance in DROP. We start by counting                    p end
                                                                                     p       =softmax(FFN([M0;M2])              (2)
                  the unigram and bigram answer frequency for each           where FFN is a two-layer feed-forward network
                  whquestion-word in the train set (as the ﬁrst word         with the RELU activation.
                  in the question). The majority baseline then pre-
                  dicts an answerasthesetof3mostcommonanswer                 Question span      Some questions in DROP have
                  spans for the input question word (e.g., for “when”,       their answer in the question instead of the passage.
                  these were “quarter”, “end” and “October”).                Topredict an answer from the question, the model
                                                                             ﬁrst computes a vector hP that represents the infor-
                  6    NAQANet                                               mation it ﬁnds in the passage:
                  DROPisdesignedtoencouragemodelsthatcom-                                    P                  P¯
                                                                                           α =softmax(W P),                     (3)
                  bine neural reading comprehension with symbolic                            P       P¯
                  reasoning. None of the baselines we described in                          h =α P                              (4)
                  Section 5 can do this. As a preliminary attempt            Then it computes the probabilities of the starting
                  toward this goal, we propose a numerically-aware           and ending positions from the question as:
                  QANetmodel,NAQANet,whichallowsthestate-                         q start                       |Q|     P
                  of-the-art reading comprehension system to pro-               p       =softmax(FFN([Q;e           ⊗h ]),      (5)
                                                                                   q end                        |Q|     P
                  duce three new answer types: (1) spans from the                p      =softmax(FFN([Q;e           ⊗h ])       (6)
                  question; (2) counts; (3) addition or subtraction                                                        |Q|
                  over numbers. To predict numbers, the model ﬁrst           where the outer product with the identity (e      ⊗·)
                                                                             simply repeats hP for each question word.
                  predicts whether the answer is a count or an arith-
                  metic expression. It then predicts the speciﬁc num-        Count We model the capability of counting as
                  bers involved in the expression. This can be viewed        a multi-class classiﬁcation problem. Speciﬁcally,
                  as the neural model producing a partially executed         weconsider ten numbers (0–9) in this preliminary
                  logical form, leaving the ﬁnal arithmetic to a sym-        model and the probabilities of choosing these num-
                                                                                                                                 P
                  bolic system. While this model can currently only          bers is computed based on the passage vector h :
                  handle a very limited set of operations, we believe
                                                                                          count                     P
                  this is a promising approach to combining neural                      p      =softmax(FFN(h ))                (7)
                                                                        2374
                     Arithmetic       expression       Many questions in                   Method                    Dev               Test
                     DROP require the model to locate multiple                                                    EM        F1      EM        F1
                     numbers in the passage and add or subtract them                       Heuristic Baselines
                     to get the ﬁnal answer. To model this process,                        Majority              0.09     1.38     0.07     1.44
                     weﬁrst extract all the numbers from the passage                       Q-only                4.28     8.07     4.18     8.59
                     and then learn to assign a plus, minus or zero for                    P-only                0.13     2.27     0.14     2.26
                     each number. In this way, we get an arithmetic                        Semantic Parsing
                     expression composed of signed numbers, which                          SynDep                9.38    11.64     8.51    10.84
                                                                                           OpenIE                8.80    11.31     8.53    10.77
                     can be evaluated to give the ﬁnal answer.                             SRL                   9.28    11.72     8.98    11.45
                        Todothis,weﬁrstapplyanotherQANetencoder                            SQuAD-styleRC
                     to M and get a new passage representation M .                         BiDAF                26.06    28.85    24.75    27.49
                           2                                                     3         QANet                27.50    30.44    25.50    28.36
                     Thenweselect an index over the concatenation of                       QANet+ELMo           27.71    30.33    27.08    29.67
                     M andM ,togetarepresentationforeachnumber                             BERT                 30.10    33.36    29.45    32.70
                        0         3
                                               th
                     in this passage. The i       number can be represented                NAQANet
                          N                                                                +QSpan               25.94    29.17    24.98    28.18
                     as h    and the probabilities of this number being
                          i                                                                +Count               30.09    33.92    30.04    32.75
                     assigned a plus, minus or zero are:                                   +Add/Sub             43.07    45.71    40.40    42.96
                                    sign                                                   Complete Model       46.20    49.24    44.07    47.01
                                                                N
                                  p      =softmax(FFN(h ))                     (8)
                                    i                           i                          Human                     -        -   94.09    96.42
                     Answer type prediction            We use a categorical            Table4: Performanceofthedifferentmodelsonourde-
                     variable to decide between the above four answer                  velopment and test set, in terms of Exact Match (EM),
                     types, with probabilities computed as:                            and numerically-focused F (§5). Both metrics are cal-
                                                                                                                      1
                                 type                         P    Q                   culated as the maximum against a set of gold answers.
                               p      =softmax(FFN([h ,h ]))                   (9)
                               Q                                                               12
                     where h is computed over Q, in a similar way as                   levels.     For example, BERT, the current state-of-
                                    P
                     wedidforh . Attest time, we ﬁrst determine this                   the-art on SQuAD, drops by more than 50 abso-
                     answer type greedily and then get the best answer                 lute F1 points. This is a positive indication that
                     from the selected type.                                           DROPisindeedachallenging reading comprehen-
                     6.2    Weakly-Supervised Training                                 sion dataset, which opens the door for tackling new
                                                                                       and complex reasoning problems on a large scale.
                     For supervision, DROP contains only the answer                       The best performance is obtained by our
                     string, not which of the above answer types is                    NAQANetmodel. Table6showsthatourgainsare
                     used to arrive at the answer. To train our model,                 obtained on the challenging and frequent number
                     we adopt the weakly supervised training method                    answer type, which requires various complex types
                     widely used in the semantic parsing literature (Be-               of reasoning. Future work may also try combining
                     rant et al., 2013a). We ﬁnd all executions that eval-             ourmodelwithBERT.Furthermore,weﬁndthatall
                     uate to the correct answer, including matching pas-               heuristic baselines do poorly on our data, hopefully
                     sage spans and question spans, correct count num-                 attesting to relatively small biases in DROP.
                     bers, as well as sign assignments for numbers. Our                Difﬁculties of building semantic parsers                  We
                     training objective is then to maximize the marginal
                     likelihood of these executions.11                                 see that all the semantic parsing baselines perform
                                                                                       quite poorly on DROP. This is mainly because of
                     7    Results and Discussion                                       our pipeline of extracting tabular information from
                     The performance of all tested models on the                       paragraphs, followed by the denotation-driven log-
                     DROPdatasetispresentedinTable4. Mostnotably,                      ical form search, can yield logical forms only for
                     all models perform signiﬁcantly worse than on                     a subset of the training data. For SRL and syntac-
                     other prominent reading comprehension datasets,                   tic dependency sentence representation schemes,
                     while human performance remains at similar high                      12Humanperformance was estimated by the authors collec-
                                                                                       tively answering 560 questions from the test set, which were
                        11Due to the exponential search space and the possible         then evaluated using the same metric as learned systems. This
                     noise, we only search the addition/subtraction of two numbers.    is in contrast to holding out one gold annotation and evaluating
                     Giventhislimitedsearchspace,thesearchandmarginalization           it against the other annotations, as done in prior work, which
                     are exact.                                                        underestimates human performance relative to systems.
                                                                                  2375
                                              Phenomenon                          Passage Highlights                                                               Question                                                               Answer                  Our
                                                                                                                                                                                                                                                                model
                                              Subtraction                         . . .    Twenty-ﬁve of his 150 men were                                          How many of Bartolom de Ams-                                               125                  145
                                             + Coreference                        sick, and his advance stalled ...                                                queta’s 150 men were not sick?
                                              Count + Filter                      . . .  Macedonians were the largest ethnic                                       Howmanyethnicities had less than                                              3                   2
                                                                                  group in Skopje, with 338,358 inhabi-                                           10000people?
                                                                                  tants ... Then came ... Serbs (14,298
                                                                                  inhabitants), Turks (8,595), Bosniaks
                                                                                  (7,585) and Vlachs (2,557) ...
                                              Domain                              . . .   Smith was sidelined by a torn pec-                                       HowmanyquartersdidSmithplay?                                                  0                   2
                                              knowledge                           toral muscle suffered during practice ...
                                              Addition                            . . .  culminating in the Battle of Vienna                                       What year did the Great Turkish                                           1698                1668
                                                                                  of 1683, which marked the start of the                                          Warend?
                                                                                  15-year-long Great Turkish War ...
                                      Table 5: Representative examples from our model’s error analysis. We list the identiﬁed semantic phenomenon,
                                      the relevant passage highlights, a gold question-answer pair, and the erroneous prediction by our model.
                                      the search was able to yield logical forms for 34%                                                                              Type                          (%)             Exact Match                                 F1
                                      of the training data, whereas with OpenIE, it was                                                                                                                            QN+ BERT QN+ BERT
                                      only 25%. On closer examination of a sample of                                                                                  Date                          1.57            28.7             38.7             35.5             42.8
                                      60questions and the information extracted by the                                                                                Numbers                      61.94            44.0             14.5             44.2             14.8
                                      SRLscheme(thebestperforming of the three), we                                                                                   Single Span                  31.71            58.2             64.6             64.6             70.1
                                      found that only 25% of the resulting tables con-                                                                                >1Spans                       4.77               0               0             17.13             25.0
                                      tained information needed to the answer the ques-                                                                          Table 6: Dev set performance breakdown by different
                                      tions. These observations show that high quality                                                                            answer types; our model (NAQANet, marked as QN+)
                                      information extraction is a strong prerequisite for                                                                        vs. BERT, the best-performing baseline.
                                      building semantic parsers for DROP. Additionally,
                                      the fact that this is a weakly supervised semantic
                                      parsing problem also makes training hard. The                                                                               8        Conclusion
                                      biggest challenge in this setup is the spuriousness
                                      of logical forms used for training, where the logical                                                                      We have presented DROP, a dataset of com-
                                      form evaluates to the correct denotation but does                                                                           plex reading comprehension questions that require
                                      not actually reﬂect the semantics of the question.                                                                          Discrete Reasoning Over Paragraphs. This dataset
                                      This makes it hard for the model trained on these                                                                           is substantially more challenging than existing
                                      spurious logical forms to generalize to unseen data.                                                                        datasets, with the best baseline achieving only
                                      From the set of logical forms for a sample of 60                                                                            32.7% F1, while humans achieve 96%. We hope
                                      questions analyzed, we found that only 8 questions                                                                          this dataset will spur research into more compre-
                                      (13%) contained non-spurious logical forms.                                                                                 hensive analysis of paragraphs, and into methods
                                                                                                                                                                  that combine distributed representations with sym-
                                                                                                                                                                  bolic reasoning. We have additionally presented
                                                                                                                                                                  initial work in this direction, with a model that
                                      Error Analysis                            Finally, in order to better under-                                                augmentsQANetwithlimitednumericalreasoning
                                      stand the outstanding challenges in DROP, we con-                                                                           capability, achieving 47% F1 on DROP.
                                      ducted an error analysis on a random sample of
                                      100 erroneous NAQANet predictions. The most                                                                                 Acknowledgments
                                      commonerrorswereonquestionswhichrequired
                                      complex type of reasoning, such as arithmetic                                                                              Wewould like to thank Noah Smith, Yoav Gold-
                                      operations (evident in 51% of the errors), count-                                                                           berg, andJonathanBerantforinsightfuldiscussions
                                      ing (30%), domain knowledge and common sense                                                                                that informed the direction of this work. The com-
                                      (23%), co-reference (6%), or a combination of dif-                                                                          putations on beaker.orgweresupportedinpart
                                      ferent types of reasoning (40%). See Table 5 for                                                                            by credits from Google Cloud.
                                      examples of some of the common phenomena.
                                                                                                                                                       2376
                 References                                             Suchin Gururangan, Swabha Swayamdipta, Omer
                 Michele Banko, Michael J. Cafarella, Stephen Soder-      Levy, Roy Schwartz, Samuel Bowman, and Noah A.
                    land, Matthew G Broadhead, and Oren Etzioni.          Smith. 2018.   Annotation artifacts in natural lan-
                    2007. Openinformationextractionfromtheweb. In         guage inference data. In Proc. of NAACL.
                    IJCAI.                                              Luheng He, Kenton Lee, Mike Lewis, and Luke S.
                 JonathanBerant,AndrewChou,RoyFrostig,andPercy            Zettlemoyer. 2017.   Deep semantic role labeling:
                    Liang. 2013a. Semantic parsing on freebase from       Whatworksandwhat’snext. In ACL.
                    question-answer pairs. In EMNLP.                    Mohammad Javad Hosseini, Hannaneh Hajishirzi,
                 JonathanBerant,AndrewChou,RoyFrostig,andPercy            Oren Etzioni, and Nate Kushman. 2014. Learning
                    Liang. 2013b. Semantic parsing on freebase from       to solve arithmetic word problems with verb catego-
                    question-answer pairs. In Proceedings of the 2013     rization. In Proceedings of the 2014 Conference on
                    Conference on Empirical Methods in Natural Lan-       Empirical Methods in Natural Language Processing
                    guage Processing, pages 1533–1544.                    (EMNLP),pages523–533.
                 Samuel R. Bowman, Gabor Angeli, Christopher Potts,     Mandar S. Joshi, Eunsol Choi, Daniel S. Weld, and
                    and Christopher D. Manning. 2015. A large anno-       Luke S. Zettlemoyer. 2017. Triviaqa: A large scale
                    tated corpus for learning natural language inference. distantly supervised challenge dataset for reading
                    In EMNLP.                                             comprehension. In ACL.
                                        ´    `                          Divyansh Kaushik and Zachary Chase Lipton. 2018.
                 Xavier Carreras and Lluıs Marquez. 2005. Introduc-
                    tion to the conll-2005 shared task: Semantic role la- Howmuchreading does reading comprehension re-
                    beling. In Proceedings of CONLL, pages 152–164.       quire?   a critical investigation of popular bench-
                 Danqi Chen, Jason Bolton, and Christopher D. Man-        marks. In EMNLP.
                    ning.2016. Athoroughexaminationofthecnn/daily       Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
                    mail reading comprehension task.                      Shyam Upadhyay, and Dan Roth. 2018. Looking
                 David L Chen and Raymond J Mooney. 2011. Learn-          beyond the surface: A challenge set for reading
                    ing to interpret natural language navigation instruc- comprehension over multiple sentences. In NAACL-
                    tions from observations. In AAAI, volume 2, pages     HLT.
                    1–2.                                                    ´         ´
                                                                        Tomas Kocisky, Jonathan Schwarz, Phil Blunsom,
                                                                                                            ´
                 Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen       ChrisDyer,KarlMoritzHermann,GaborMelis,and
                    tau Yih, Yejin Choi, Percy Liang, and Luke S. Zettle- EdwardGrefenstette. 2018. The narrativeqa reading
                    moyer. 2018. Quac: Question answering in context.     comprehension challenge. TACL, 6:317–328.
                    In EMNLP.                                           Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish
                 Christopher Clark and Matt Gardner. 2018.   Simple       Sabharwal, Oren Etzioni, and Siena Dumas Ang.
                    and effective multi-paragraph reading comprehen-      2015. Parsing algebraic word problems into equa-
                    sion. In ACL.                                         tions. TACL, 3:585–597.
                 Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sab-    Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
                    harwal, Oyvind Tafjord, Peter Turney, and Daniel      ner. 2017. Neural semantic parsing with type con-
                    Khashabi. 2016. Combining retrieval, statistics, and  straints for semi-structured tables. In EMNLP.
                    inference to answer elementary science questions.   Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
                    In Thirtieth AAAI Conference on Artiﬁcial Intelli-    Regina Barzilay. 2014. Learning to automatically
                    gence.                                                solve algebra word problems. In Proceedings of the
                 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and            52nd Annual Meeting of the Association for Compu-
                    KristinaToutanova.2019. Bert: Pre-trainingofdeep      tational Linguistics (Volume 1: Long Papers), vol-
                    bidirectional transformers for language understand-   ume1,pages271–281.
                    ing. NAACL, abs/1810.04805.                         Chen Liang, Jonathan Berant, Quoc Le, Kenneth D.
                 Timothy Dozat, Peng Qi, and Christopher D. Manning.      Forbus, and Ni Lao. 2017.   Neural symbolic ma-
                    2017.  Stanford’s graph-based neural dependency       chines: Learning semantic parsers on freebase with
                    parser at the conll 2017 shared task. In CoNLL        weaksupervision. In ACL.
                    Shared Task.
                                                                        WangLing,DaniYogatama,ChrisDyer,andPhilBlun-
                 Matt Gardner, Joel Grus, Mark Neumann, Oyvind            som. 2017. Program induction by rationale genera-
                    Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew       tion: Learning to solve and explain algebraic word
                    Peters, Michael Schmitz, and Luke S. Zettlemoyer.     problems. In ACL.
                    2017. Allennlp: A deep semantic natural language
                    processing platform. In Proceedings of Workshop     Marie-Catherine de Marneffe and Christopher D. Man-
                    for NLP Open Source Software (NLP-OSS). Associ-       ning. 2008. The stanford typed dependencies repre-
                    ation for Computational Linguistics.                  sentation. In CFCFPE@COLING.
                                                                   2377
                  Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish    MinJoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
                    Sabharwal. 2018. Can a suit of armor conduct elec-      Hannaneh Hajishirzi. 2017. Bidirectional attention
                    tricity? a new dataset for open bookquestionanswer-     ﬂowformachinecomprehension. ICLR.
                    ing. In EMNLP.                                        Gabriel Stanovsky, Julian Michael, Luke S. Zettle-
                  Pasquale Minervini and Sebastian Riedel. 2018. Ad-        moyer, and Ido Dagan. 2018. Supervised open in-
                    versarially regularising neural nli models to integrate formation extraction. In NAACL-HLT.
                    logical background knowledge. In CoNLL.                      ˇ
                                                                          Simon Suster and Walter Daelemans. 2018. Clicr: a
                  Bhavana Dalvi Mishra, Lifu Huang, Niket Tandon,           dataset of clinical case reports for machine reading
                    Wen-tau Yih, and Peter Clark. 2018. Tracking state      comprehension.
                    changes in procedural text: A challenge dataset and   Alon Talmor and Jonathan Berant. 2018. The web as
                    models for process paragraph comprehension.             a knowledge-base for answering complex questions.
                  Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever.       In NAACL-HLT.
                    2016.   Neural programmer: Inducing latent pro-       Johannes Welbl, Pontus Stenetorp, and Sebastian
                    grams with gradient descent. ICLR.                      Riedel. 2018. Constructing datasets for multi-hop
                  SimonOstermann, Ashutosh Modi, Michael Roth, Ste-         reading comprehension across documents.     TACL,
                    fan Thater, and Manfred Pinkal. 2018. Mcscript: a       6:287–302.
                    novel dataset for assessing machine comprehension     Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
                    using script knowledge. LREC Proceedings, 2018.         gio, William W. Cohen, Ruslan Salakhutdinov, and
                  AnusriPampari,PreethiRaghavan,JenniferLiang,and           ChristopherD.Manning.2018. Hotpotqa: Adataset
                    Jian Peng. 2018. emrqa: A large corpus for question     for diverse, explainable multi-hop question answer-
                    answering on electronic medical records.                ing. In EMNLP.
                                       ´                                  PengchengYinandGrahamNeubig.2017. Asyntactic
                  Denis Paperno, German Kruszewski, Angeliki Lazari-        neural model for general-purpose code generation.
                    dou, Quan Ngoc Pham, Raffaella Bernardi, San-           In ACL’17.
                    dro Pezzelle, Marco Baroni, Gemma Boleda, and
                                 ´
                    Raquel Fernandez. 2016.      The lambada dataset:     AdamsWeiYu,DavidDohan,Minh-ThangLuong,Rui
                    Wordprediction requiring a broad discourse context.     Zhao, Kai Chen, Mohammad Norouzi, and Quoc V.
                    ACL.                                                    Le. 2018.    Qanet: Combining local convolution
                  Panupong Pasupat and Percy Liang. 2015. Composi-          with global self-attention for reading comprehen-
                    tional semantic parsing on semi-structured tables. In   sion. ICLR.
                    ACL.                                                  John M. Zelle and Raymond J. Mooney. 1996. Learn-
                  Jeffrey Pennington, RichardSocher, andChristopherD.       ing to parse database queries using inductive logic
                    Manning.2014. Glove: Globalvectorsforwordrep-           programming. In AAAI/IAAI, Vol. 2.
                    resentation. In EMNLP.                                Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin
                  Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt        Choi. 2019. From recognition to cognition: Visual
                    Gardner, Christopher Clark, Kenton Lee, and Luke        commonsensereasoning. CVPR, abs/1811.10830.
                    Zettlemoyer. 2018. Deep contextualized word repre-    RowanZellers,YonatanBisk,RoySchwartz,andYejin
                    sentations. In NAACL-HLT.                               Choi. 2018. Swag: A large-scale adversarial dataset
                  Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.       for grounded commonsense inference. In EMNLP.
                    Know what you don’t know: Unanswerable ques-          LukeS.ZettlemoyerandMichaelCollins.2005. Learn-
                    tions for squad. In ACL.                                ing to map sentences to logical form: Structured
                  PranavRajpurkar,JianZhang,KonstantinLopyrev,and           classiﬁcation with probabilistic categorial grammars.
                    Percy Liang. 2016. Squad: 100, 000+ questions for       In UAI.
                    machine comprehension of text. In EMNLP.              Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng
                  Siva Reddy, Danqi Chen, and Christopher D. Manning.       Gao, Kevin Duh, and Benjamin Van Durme. 2019.
                    2019. Coqa: A conversational question answering         ReCoRD:Bridgingthegapbetweenhumanandma-
                    challenge. TACL.                                        chine commonsense reading comprehension.
                  Scott E. Reed and Nando de Freitas. 2016.    Neural
                    programmer-interpreters. ICLR.
                  Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and
                    Karthik Sankaranarayanan. 2018. Duorc: Towards
                    complex language understanding with paraphrased
                    reading comprehension. In ACL.
                                                                     2378
