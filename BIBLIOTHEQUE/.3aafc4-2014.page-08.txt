                it is easy to state the question that light-cone complexity is answering.  That question is the fol-
                lowing: “how much could I possibly predict about the conﬁgurations in a’s future, given complete
                information about a’s past?” The reason to focus on light-cones, rather than other sets of points, is
                that the light-cones are automatically determined once we know the causal structure: there seems
                to be little arbitrariness about them.
                    Ontheotherhand, depending on the application, an obvious drawback of light-cone complexity
                is that it can’t tell us the “inherent” complexity of an object x, without knowing about x’s past
                and future.   If we wanted to use a complexity measure to make inferences about x’s past and
                future, this might be seen as question-begging.    A less obvious drawback arises if we consider a
                dynamical system that changes slowly with time: for example, a version of the coﬀee automaton
                where just a single cream particle is randomly moved at each time step. Consider such a system
                in its “late” stages: that is, after the coﬀee and cream have fully mixed.    Even then, Shalizi et
                al.’s LCC(a) measure will remain large, but not for any “interesting” reason: only because a’s past
                light-cone will contain almost the same (random) information as its future light-cone, out to a very
                large distance!  Thus, LCC seems to give an intuitively wrong answer in these cases (though no
                doubt one could address the problem by redeﬁning LCC in some suitable way).
                    The computational situation for LCC seems neither better nor worse to us than that for (say)
                apparent complexity or resource-bounded sophistication. Since the light-cones P (a) and V (a) are
                formally inﬁnite, a ﬁrst step in estimating LCC(a)—as Shalizi et al. point out—is to impose some
                ﬁnite cutoﬀ t on the number of steps into a’s past and future one is willing to look.    Even then,
                one needs to estimate the mutual information I VPt(a) : VFt(a) between the truncated light-cones
                P (a) and F (a), a problem that na¨ıvely requires a number of samples exponential in t. One could
                 t          t
                address this problem by simply taking t extremely small (Shalizi et al. set t = 1).   Alternatively,
                if a large t was needed, one could use the same Kolmogorov-complexity-based approach that we
                adopt in this paper for apparent complexity. That is, one ﬁrst replaces the mutual information by
                the mutual algorithmic information
                                 KVPt(a) : VFt(a) = K VPt(a) + K VFt(a) − K VPt(a),VFt(a),               (8)
                and then estimates K (x) using some computable proxy such as gzip ﬁle size.
                2.5   Synthesis
                It seems like we have a bestiary of diﬀerent complexity notions.      Fortunately, the four notions
                discussed above can all be related to each other; let us discuss how.
                    First, one can view apparent complexity as a kind of “resource-bounded” sophistication.      To
                see this, let f be any smoothing function. Then K (f (x)), the Kolmogorov complexity of f (x), is
                essentially equal to K (Sf,x), where
                                                    S    := {y : f (y) = f (x)}.                                 (9)
                                                      f,x
                Thus, if instead of minimizing over all models S for x that satisfy some condition, we consider only
                the particular model S     above, then sophistication reduces to apparent complexity.     Note that
                                       f,x
                this argument establishes neither that apparent complexity is an upper bound on sophistication,
                nor that it’s a lower bound. Apparent complexity could be larger, if the minimization found some
                model S for x with K(S)  K(S           ).  But conversely, sophistication could also be larger, if
                                                     f,x
                                                                 8
