# A Simple and Effective Positional Encoding for Transformers (2021)
Source: 793aa4-2021.pdf

## Core reasons
- The paper critiques standard absolute positional embeddings, arguing they mainly suffer from being added at the input, which frames a limitation of prior positional encoding practice.
- It proposes DIET, a decoupled positional attention method that changes how positional information is incorporated directly into attention heads.

## Evidence extracts
- "Wearguethatabsolutepositionembeddingsmainly suffer from being addedattheinput." (p. 1)
- "Weproposethefollowing simple absolute position encoding method that adds position information to the token attention matrix directly in each attention head." (p. 4)

## Classification
Class name: Positional Encoding Improvement Proposal
Class code: 1

$$
\boxed{1}
$$
