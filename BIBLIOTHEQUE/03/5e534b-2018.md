# Universal Transformers (2019)
Source: 5e534b-2018.pdf

## Core reasons
- The paper points to a limitation of feed-forward Transformers in generalizing on simple algorithmic tasks, motivating a change in computation.
- It introduces the Universal Transformer as a parallel-in-time recurrent self-attentive sequence model that generalizes the Transformer, i.e., a new computation mechanism.

## Evidence extracts
- "successes, however, popular feed-forward sequence models like the Transformer
fail to generalize in many simple tasks that recurrent models handle with ease, e.g.
copyingstringsorevensimplelogicalinferencewhenthestringorformulalengths
exceed those observed at training time." (p. 1)
- "Inthispaper,weintroducetheUniversalTransformer(UT),aparallel-in-timerecurrentself-attentive
sequencemodelwhichcanbecastasageneralizationoftheTransformermodel," (p. 2)

## Classification
Class name: Computation & Reasoning Mechanism Proposal
Class code: 3

$$
\boxed{3}
$$
