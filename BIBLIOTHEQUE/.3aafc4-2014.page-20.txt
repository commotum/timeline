                 7     Conclusions and Further Work
                 Of the metrics considered in this project, the coarse-graining approaches such as apparent com-
                 plexity provide the most eﬀective estimate of complexity that produces results which mirror human
                 intuition.  However, this metric suﬀers from the disadvantage that it is based on human intuition
                 and perceptions of complexity. Ideally, a complexity metric would be found which produces sim-
                 ilar results without relying on such assumptions.        The OSCR approach seems promising for its
                 independence from these assumptions and for its theoretical foundations.             It is possible that a
                 diﬀerent implementation of this algorithm could produce better results than the one we used for
                 this project.
                     It would also be worthwhile to investigate other complexity metrics, beyond those already
                 explored in this paper.     Shalizi et al. [12] propose a metric based on the concept of light cones.
                 They deﬁne C(x), the complexity of a point x in the spacetime history, as the mutual information
                 between descriptions of its past and future light cones.       Letting P (x) be the past light cone and
                 F(x) the future light cone, C (x) = H (P (x)) + H (F (x)) − H (P (x),F (x)).             This metric is of
                 particular interest because it avoids the problem of artifacts created by coarse-graining; it can also
                 be approximated in a way that avoids the use of gzip. Running experiments with the automaton
                 using the light cone metric, and comparing the results to those generated using coarse-graining,
                 could provide more information about both metrics.
                     Ultimately, numerical simulation is of limited use in reasoning about the problem of complexity.
                 Approximation algorithms can provide only an upper bound, not a lower bound, on Kolmogorov
                 complexity and sophistication. To show that a system really does become complex at intermediate
                 points in time, it is necessary to ﬁnd a lower bound for the system’s complexity. Future theoretical
                 work could help provide such a lower bound, and could also generate further insight into the origins
                 of complexity in closed systems.
                 8     Acknowledgments
                 We thank Alex Arkhipov, Charles Bennett, Ian Durham, Dietrich Leibfried, Aldo Pacchiano, and
                 Luca Trevisan for helpful discussions.
                 9     Appendix: The Non-Interacting Case
                 Let’s consider the non-interacting coﬀee automaton on an n × n grid with periodic boundary
                 conditions.    At each time step, each cream particle moves to one of the 4 neighboring pixels
                 uniformly at random. Let a (x,y) be the number of cream particles at point (x,y) after t steps.
                                                t
                 Claim 1. For all x,y,t, we have E[a (x,y)] ≤ 1.
                                                          t
                 Proof. Byinductionont. Ift = 0, thena (x,y) ∈ {0,1}. Furthermore, bylinearity of expectation,
                                                              0
                                          E[a (x−1,y)]+E[a (x+1,y)]+E[a (x,y−1)]+E[a (x,y+1)]
                        E[a     (x,y)] =      t                  t                   t                  t           .
                            t+1                                               4
                                                                     20
