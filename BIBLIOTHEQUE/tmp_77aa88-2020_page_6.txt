                        Published as a conference paper at ICLR 2021
                                                                       CIFAR-10           CIFAR-100
                              Model                  Augmentation   SAM       SGD      SAM       SGD
                              WRN-28-10(200epochs)   Basic         2.7       3.5      16.5     18.8
                                                                      ±0.1      ±0.1     ±0.2      ±0.2
                              WRN-28-10(200epochs)   Cutout        2.3       2.6      14.9     16.9
                                                                      ±0.1      ±0.1     ±0.2      ±0.1
                              WRN-28-10(200epochs)   AA            2.1       2.3      13.6     15.8
                                                                     ±<0.1      ±0.1     ±0.2      ±0.2
                              WRN-28-10(1800epochs)  Basic         2.4       3.5      16.3     19.1
                                                                      ±0.1      ±0.1     ±0.2      ±0.1
                              WRN-28-10(1800epochs)  Cutout        2.1       2.7      14.0     17.4
                                                                      ±0.1      ±0.1     ±0.1      ±0.1
                              WRN-28-10(1800epochs)  AA            1.6      2.2       12.8     16.1
                                                                      ±0.1     ±<0.1     ±0.2      ±0.2
                              Shake-Shake (26 2x96d) Basic         2.3       2.7      15.1     17.0
                                                                     ±<0.1      ±0.1     ±0.1      ±0.1
                              Shake-Shake (26 2x96d) Cutout        2.0       2.3      14.2     15.7
                                                                     ±<0.1      ±0.1     ±0.2      ±0.2
                              Shake-Shake (26 2x96d) AA            1.6       1.9      12.8     14.1
                                                                     ±<0.1      ±0.1     ±0.1      ±0.2
                              PyramidNet             Basic         2.7       4.0      14.6     19.7
                                                                      ±0.1      ±0.1     ±0.4      ±0.3
                              PyramidNet             Cutout        1.9       2.5      12.6     16.4
                                                                      ±0.1      ±0.1     ±0.2      ±0.1
                              PyramidNet             AA            1.6       1.9      11.6     14.6
                                                                      ±0.1      ±0.1     ±0.1      ±0.1
                              PyramidNet+ShakeDrop   Basic         2.1       2.5      13.3     14.5
                                                                      ±0.1      ±0.1     ±0.2      ±0.1
                              PyramidNet+ShakeDrop   Cutout        1.6       1.9      11.3     11.8
                                                                     ±<0.1      ±0.1     ±0.1      ±0.2
                              PyramidNet+ShakeDrop   AA            1.4      1.6       10.3     10.6
                                                                     ±<0.1     ±<0.1     ±0.1      ±0.1
                        Table 1: Results for SAM on state-of-the-art models on CIFAR-{10, 100} (WRN = WideResNet;
                        AA=AutoAugment;SGDisthestandardnon-SAMprocedureusedtotrainthesemodels).
                        As seen in Table 2, SAM again consistently improves performance, for example improving the
                        ImageNettop-1errorrateofResNet-152from20.3%to18.4%. Furthermore,notethatSAMenables
                        increasing the number of training epochs while continuing to improve accuracy without overﬁtting.
                        Incontrast, thestandardtrainingprocedure(withoutSAM)generallysigniﬁcantlyoverﬁtsastraining
                        extends from 200 to 400 epochs.
                                    Model     Epoch          SAM           Standard Training (No SAM)
                                                        Top-1     Top-5      Top-1       Top-5
                                   ResNet-50   100     22.5      6.28       22.9        6.62
                                                          ±0.1      ±0.08      ±0.1        ±0.11
                                               200     21.4      5.82       22.3        6.37
                                                          ±0.1      ±0.03      ±0.1        ±0.04
                                               400     20.9      5.51       22.3        6.40
                                                          ±0.1      ±0.03      ±0.1        ±0.06
                                  ResNet-101   100     20.2      5.12       21.2        5.66
                                                          ±0.1      ±0.03      ±0.1        ±0.05
                                               200     19.4      4.76       20.9        5.66
                                                          ±0.1      ±0.03      ±0.1        ±0.04
                                               400    19.0       4.65       22.3        6.41
                                                         ±<0.01     ±0.05      ±0.1        ±0.06
                                  ResNet-152   100    19.2       4.69      20.4         5.39
                                                         ±<0.01     ±0.04      ±<0.0       ±0.06
                                               200     18.5      4.37       20.3        5.39
                                                          ±0.1      ±0.03      ±0.2        ±0.07
                                               400    18.4       4.35      20.9         5.84
                                                         ±<0.01     ±0.04      ±<0.0       ±0.07
                               Table 2: Test error rates for ResNets trained on ImageNet, with and without SAM.
                        3.2  FINETUNING
                        Transfer learning by pretraining a model on a large related dataset and then ﬁnetuning on a smaller
                        target dataset of interest has emerged as a powerful and widely used technique for producing high-
                        quality models for a variety of different tasks. We show here that SAM once again offers con-
                        siderable beneﬁts in this setting, even when ﬁnetuning extremely large, state-of-the-art, already
                        high-performing models.
                        In particular, we apply SAM to ﬁnetuning EfﬁcentNet-b7 (pretrained on ImageNet) and
                        EfﬁcientNet-L2(pretrainedonImageNetplusunlabeledJFT;inputresolution475)(Tan&Le,2019;
                        Kornblith et al., 2018; Huang et al., 2018). We initialize these models to publicly available check-
                        points6 trained with RandAugment (84.7% accuracy on ImageNet) and NoisyStudent (88.2% ac-
                        curacy on ImageNet), respectively. We ﬁnetune these models on each of several target datasets by
                        training each model starting from the aforementioned checkpoint; please see the appendix for details
                        of the hyperparameters used. We report the mean and 95% conﬁdence interval of top-1 test error
                        over 5 independent runs for each dataset.
                          6https://github.com/tensorflow/tpu/tree/master/models/official/
                        efficientnet
                                                                 6
