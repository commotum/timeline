                                                           TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                                                                                                                    Generated Generators + Annotations
                                                   Examples Generators + Annotations from the training set
                                                          Category

                                                                                                                                   Category

                                                          Extension, Lines

                                                                                                                                   Recoloring, Lines

                                                          Summary

                                                                                                                                   Summary

                                                          Connecting blocks with colored lines

                                                                                                                                   lines are re-colored to red while blocks kept the same

                                                          Description

                                                                                                                                   Description

                                                          In the input we see randomly placed blocks and we need to 
                                                                                                                                   In the input, there are blocks and full lines from edges to edges. 
                                                          connect them to their horizontal or vertical neighbors with lines 
                                                                                                                                   The lines are re-colored to red while the blocks are kept the same.
                                                          of the same color.
                                                                                                                                  def generate(diff_lb: float, diff_ub: float) -> dict:

                                                         def generate(diff_lb: float, diff_ub: float) -> dict:

                                                                                                            LLM
                                                                                                                                      # set the size limits

                                                             # set the size limits

                                                                                                                                      dim_bounds = (4, 30)

                                                             dim_bounds = (3, 30)

                                                                                                                                      # get the possible colors for the blocks

                                                             # get the possible colors for the blocks

                                                                                                                                      colopts = remove(2, interval(0, 10, 1))

                                                             colopts = remove(8, interval(0, 10, 1))

                                                                                                                                      # get random size for the canvas

                                                             # get random size for the canvas

                                                                                                                                      h = unifint(diff_lb, diff_ub, dim_bounds)

                                                             h = unifint(diff_lb, diff_ub, dim_bounds)

                                                                                                                                      w = unifint(diff_lb, diff_ub, dim_bounds)

                                                             w = unifint(diff_lb, diff_ub, dim_bounds)

                                                                                                                                      # pick a random color for the background

                                                             # get random background color

                                                                                                                                      ...

                                                             ...

                                                                                                                                      return {'input': gi, 'output': go}

                                                             return {'input': gi, 'output': go}

                     Figure 11. LLM based synthetic tasks generation: Given some seed task descriptions and task generator functions in Python, we
                     generate more generator functions to produce novel tasks. We use three different approaches: (1) few-shot prompting with only generators,
                     (2) few-shot prompting with generators and task descriptions, (3) two-stage approach: first generate free form descriptions, then condition
                     onthemtogenerate more generators (shown in Figure 12).
                     B. Fine-Tuning Before TTT
                     While test-time training facilitates task-specific adaptation, the base model’s capabilities impacts the final performance.
                     Wedeveloped several approaches for generating synthetic training data to enhance the base model’s abstract reasoning
                     capabilities through fine-tuning, exploring both automated and semi-automated methods for task generation. In this section,
                     wedetail our fine-tuning data generation strategies and analyze the impact of different data sources and model sizes on final
                     performance.
                     B.1. Preparing Fine-tuning Data
                     (Hodel, 2024) provides domain-specific language (DSL), REARC, as well as the transformation fi that solves the task-i,
                     and the data generation function g that are implemented in this DSL for each training task in the Dtrain dataset. These
                                                                          i                                                                                                    ARC
                     functions enable sampling of new input-output pairs that maintains the same underlying transformation principle:
                                                                                                d = (x,y) ∼ eval(gi)                                                                                    (1)
                     where d represents a newly generated input-output pair that can be solved using the same transformation function fi as the
                     original task-i4.
                     (a) Using Existing Generators                      Thegenerator functions g in REARC already provide an effective data augmentation tool
                     by producing different instantiations of same tasks. We generate extra samples from these training tasks by running the code
                     manytimesandrandomlysplitting these new examples (d ∼ eval(gi)) to a set of train and test examples. These augmented
                     examples are already provided with their DSL release.
                     (b) Few-shot Prompting an LLM                           Additionally, we used several approaches to generate novel tasks using an LM (in our
                     case, an ensemble of GPT4 and GPT4-o).
                     Thesimplest approach generates new task generators using few-shot examples:
                                                                                              g′ ∼ LM(g ,g ,...,g )                                                                                     (2)
                                                                                                              1     2          m
                     where g′ is a new generator function and g1,...,gm are existing generator functions (shown in Figure 11). We sample
                     different m examples by uniformly from existing training set. We repeat this process multiple times to get a good amount of
                     tasks.
                     Weaugmentthegeneratorfunctions with task descriptions and jointly generate both descriptions and generators:
                                                                                      ′   ′
                                                                                   (s ,g ) ∼ LM(s ,g ,s ,g ,...s ,g )                                                                                   (3)
                                                                                                           1    1    2    2         m m
                     where si represents the description of task i.
                          4Wecanverify the generated examples by asserting f (x) = y.
                                                                                                  i
                                                                                                               13
