                              Published as a conference paper at ICLR 2024
                              Difficulty does not correlate with issue resolution date. In Table 7 we show model results in the
                              “oracle” retrieval setting, partitioned by date, for PRs created before or after 2023. We find that for
                              most models there’s little difference in performance before or after this date, with the exception of
                              GPT-4. We consider this result to be largely promising as it suggests that despite models having
                              been exposed to some version of an repository’s codebase, they are unlikely to “cheat” to address
                              issues simply by generating a more recent version of the repository.
                              Table 7: We compare performance on task instances from before and after 2023 in the “Oracle”
                              retrieval setting. Most models show little difference in performance. ∗Due to budget constraints,
                              GPT-4isevaluated on a 25% random subset of SWE-bench tasks, which may impact performance.
                                                                                          ∗
                                                    Claude 2     ChatGPT-3.5       GPT-4       SWE-Llama7b         SWE-Llama13b
                                   Before 2023         4.87           0.49           1.96            2.95                 3.98
                                   After 2023          4.23           0.77            0.0            3.46                 3.85
                              Finetuned models are sensitive to context distribution shifts. The finetuned models SWE-Llama
                              7b and 13b perform surprisingly poorly with BM25 retrieved context. As these models were fine-
                              tuned using the “oracle” retrieval as context, we suspect this shift in context makes it difficult for
                              the model to perform reliably. For instance, SWE-Llama was trained to edit every file included as
                              context whereas in the BM25 setting many files provided in context are not expected to be changed.
                              Generatingpatchesiseasierthangeneratingwholefiles. Modelsareoftentrainedusingstandard
                              code files and likely rarely see patch files. We generally formulate our task to have models generate
                              patch files as opposed to recreating the entire file with their proposed change, since patch files will
                              usually be a much more efficient representation of a file change. As shown in Table 5, we observe
                              that models still struggle with generating well-formatted patch files. So we experiment with asking
                              models to instead regenerate entire files with their proposed changes to resolve the issue. In this
                              setting, we find that models generally perform worse at this task than when generating patch files;
                              for instance, Claude 2 scores at 2.2% compared to 4.8% in the main table for “oracle” retrieval.
                              Even when controlling for instance length, generating on the shorter half of the task instances by
                              input tokens yields 3.9% compared to 7.8% for generating patches with Claude 2.
                              Language models tend to generate shorter, simpler edits. Model generated patch files tend to
                              add and remove fewer lines than their respective gold patch. As shown in Table 8, compared to an
                              averagegoldpatch,modelgeneratedpatchfilesthatapplycorrectlyarelessthanhalfthetotallength
                              (74.5 versus 30.1 lines) of gold edit patch files, and rarely edit more than a single file.
                              Table 8: Average edits of model generated patches in the “oracle” retrieval setting across success-
                              fully applied patches. For the task instances specific to each model, we calculate the same statistics
                              across the gold patches. Avg Gold shows statistics macro-averaged over each models’ respective
                              gold patches. All Gold shows statistics for all gold patches unconditioned on model performance.
                                                Model               Total Lines   Added    Removed      Functions   Files
                                                Claude 2               19.6         4.2       1.9          1.1       1.0
                                                   Gold                44.1        12.0       5.8          2.1       1.2
                                                ChatGPT-3.5            30.1         3.8       2.7          1.6       1.0
                                                   Gold                39.6         9.5       6.1          1.9       1.2
                                                GPT-4                  20.9         4.4       1.5          1.0       1.0
                                                   Gold                33.6         8.4       3.8          1.9       1.1
                                                SWE-Llama13b           17.6         1.6       1.2          1.2       1.1
                                                   Gold                37.8        10.0       4.4          1.9       1.1
                                                SWE-Llama7b            16.7         1.3       1.2          1.2       1.1
                                                   Gold                40.2        11.3       4.9          1.9       1.1
                                                AvgGold                39.1        10.2       5.0          1.9       1.1
                                                All Gold               74.5        22.3       10.5         3.0       1.7
                                                                                    7
