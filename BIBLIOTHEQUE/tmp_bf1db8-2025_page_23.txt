                             Models        Gemini2.0Flash-Lite   Gemini2.0Flash   GPT4o    DeepSeekR1     o3-mini (high)
                          Over-count %            53.0                50.5          17.5        6.0            1.5
                         Under-count %            43.0                38.5          66.5       15.5            8.5
                  Table 7: Percentage of problems from the Object Counting task where the models over-counted or under-counted.
                    In Table 7, we report the percentage of cases             Prompt Template
                 whereeachofthemodelseitherover-counted the                   Here are three (post, reply) pairs from Red-
                  numberofobjects or under-counted, for the subset            dit. Your task is to decide whether
                 where the sum of two sets must be reported. Inter-           each reply is sarcastic. Specifically, label
                  estingly, we observe a that different models have           each pair with a "0" or "1", where
                  different failure modes on this task. The Gemini            a "1" indicates that the reply is sarcastic,
                  models tend to mostly over-count when they are              and a "0" indicates that the reply
                 wrong, whereas GPT4o, DeepSeekR1ando3-mini                   does not contain sarcasm, and provide your
                  tend to under-count when they are wrong.                    final answer as a comma-separated set of
                                                                              labels (e.g., "1,0,0" or "0,0,0").
                                                                              POST1: post1_text
                                                                              REPLY1: reply1_text
                  A.15   SARCTriples                                          POST2: post2_text
                                                                              REPLY2: reply2_text
                                                                              POST3: post3_text
                                                                              REPLY3: reply3_text
                  SARC(Self-Annotated Corpus for Sarcasm) (Kho-
                  dak et al., 2017) is a large dataset of sarcasm re-     A.16    Shuffled Objects
                  sponses mined from the Reddit social media / fo-
                  rum platform. Many Reddit users end a post or           The original task in BBH is as follows: there are
                  reply with the token “/s” when they have intended       Npeopleeachassigned to an object/person (e.g., a
                  the preceding text to be interpreted sarcastically      dance partner, a book, a color, etc.). For example,
                  or satirically. This allowed positive examples of       Alice has a green book, Bob has a red book, etc.
                  user-intended sarcasm to be mined.                      Then, there are multiple switch operations where
                                                                          pairs of people switch together what they are as-
                    Forking off the SARC dataset, we construct a          signed to (e.g., Alice and Bob switch their books).
                  challenging task for LLMs that requires reading         Attheend, one needs to predict the object/person
                  three independent examples from SARC, and clas-         assigned to one of the N people (e.g., at the end,
                  sifying each into binary label, where a positive        what color is the book that Bob has?).
                  label indicates sarcasm. The SARC authors created          Wecreated two variants of this problem. In the
                  a balanced test set with 64,666 examples. Many          first variant, we keep everything the same except
                  of these examples can only be understood with           that we add switch actions that have no effect. For
                  an image or an article link that accompanied the        example, we add Then, Person1 and Person2
                  original post or reply. On the other hand, some         switch their books.          Then, Person2 and
                  examples, usually with longer textual content, can      Person1 switch their books. We add many
                  be understood on their own. We design our de-           of these no-effect operations so that the problem
                  rived benchmark to consist mainly of the latter         becomes a long-context reasoning problem similar
                  type. To achieve this, we filter out examples with      to the approach in Vodrahalli et al. (2024).
                  either (1) less than 100 characters or (2) without a       The second variant extends the first variant, in
                  reply, resulting in 679 examples from the original      which we assign names to some of the switch ac-
                  test set, with 48.4% positive label rate. We sam-       tions as they occur and use those names later. For
                  ple (uniformly-at-random) 600 examples from this        example, the first time Person1 switches with
                  set, group them (uniformly-at-random) into groups       Person2occurs, we replace the text with Person1
                  of three, and pass the text of each 3-tuple of post,    switches with Person2 (let’s call this
                  reply pair to the following prompt:                     Action K), and the next time the same switch hap-
                                                                     26495
