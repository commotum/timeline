                                  Published as a conference paper at ICLR 2024
                                  SWE-BENCH: CAN LANGUAGE MODELS RESOLVE
                                  REAL-WORLDGITHUBISSUES?
                                                           * 1,2                 * 1,2                           1,2
                                    Carlos E. Jimenez              JohnYang               Alexander Wettig
                                                   1,2                 3                 1,2                               1,2
                                    ShunyuYao             KexinPei          Ofir Press         Karthik Narasimhan
                                    1Princeton University         2Princeton Language and Intelligence              3University of Chicago
                                                                                      ABSTRACT
                                             Language models have outpaced our ability to evaluate them effectively, but for
                                             their future development it is essential to study the frontier of their capabilities.
                                             Wefindreal-world software engineering to be a rich, sustainable, and challenging
                                             testbed for evaluating the next generation of language models. To this end, we in-
                                             troduce SWE-bench, an evaluation framework consisting of 2,294 software engi-
                                             neering problems drawn from real GitHub issues and corresponding pull requests
                                             across 12 popular Python repositories. Given a codebase along with a description
                                             of an issue to be resolved, a language model is tasked with editing the codebase
                                             to address the issue. Resolving issues in SWE-bench frequently requires under-
                                             standing and coordinating changes across multiple functions, classes, and even
                                             files simultaneously, calling for models to interact with execution environments,
                                             process extremely long contexts and perform complex reasoning that goes far be-
                                             yond traditional code generation tasks. Our evaluations show that both state-of-
                                             the-art proprietary models and our fine-tuned model SWE-Llama can resolve only
                                             the simplest issues. The best-performing model, Claude 2, is able to solve a mere
                                             1.96% of the issues. Advances on SWE-bench represent steps towards LMs that
                                             are more practical, intelligent, and autonomous.
                                  1    INTRODUCTION
                                  Language models (LMs) are rapidly being deployed in commercial products such as chatbots and
                                  coding assistants. At the same time, existing benchmarks have become saturated (Kiela et al., 2021;
                                  Ottetal., 2022) and fail to capture the frontier of what state-of-the-art LMs can and cannot do. There
                                  is a need for challenging benchmarks that more accurately reflect real-world applications of LMs to
                                  help shape their future development and usage (Srivastava et al., 2023).
                                                                                    Language Model
                                       Issue
                                                                                                                         Unit Tests
                                     data leak in GBDT due to warm

                                                                                                                       Pre PR Post PR   Tests
                                     start (This is about the non-

                                     histogram-based version of...
                                                                                                                                        join_struct_col
                                                                            Generated PR
                                                                                                +20 -12
                                                                                                                                        vstack_struct_col
                                       Codebase
                                                                               sklearn
                                                                                                                                        dstack_struct_col
                                                                                  gradient_boosting.py
                                        sklearn/
                                                       reqs.txt
                                                                                                                                        matrix_transform
                                        examples/                                 helper.py
                                                       setup.cfg
                                                                                                                                        euclidean_diff
                                                                               utils
                                        README.rst
                                                       setup.py
                                  Figure 1: SWE-bench sources task instances from real-world Python repositories by connecting
                                  GitHub issues to merged pull request solutions that resolve related tests. Provided with the issue
                                  text and a codebase snapshot, models generate a patch that is evaluated against real tests.
                                  Building a good benchmark is difficult since tasks must be challenging enough to stump existing
                                                                                                                 ´
                                  models, but model predictions must also be easy to verify (Martınez-Plumed et al., 2021). Coding
                                       ∗Equal contribution. Correspondence to {carlosej,jy1682}@princeton.edu.
                                        Data, code, and leaderboard at swebench.com
                                                                                              1
