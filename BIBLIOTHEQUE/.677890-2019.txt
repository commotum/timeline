<<<PAGE 1>>>
                         OK-VQA:AVisualQuestionAnsweringBenchmarkRequiring
                                                         External Knowledge
                                            ∗1                           2               2,3                           2
                         Kenneth Marino , MohammadRastegari , Ali Farhadi                   and Roozbeh Mottaghi
                                                        1Carnegie Mellon University
                                                     2PRIOR@AllenInstitute for AI
                                                         3University of Washington
                                    Abstract
                                                                                                           Q: Which American 
                Visual Question Answering (VQA) in its ideal form lets                                    president is associated 
             us study reasoning in the joint space of vision and language                                with the stuffed animal 
             and serves as a proxy for the AI task of scene understand-                                         seen here?
             ing. However, most VQA benchmarks to date are focused
             onquestions such as simple counting, visual attributes, and                                    A: Teddy Roosevelt
             object detection that do not require reasoning or knowl-
             edge beyond what is in the image. In this paper, we address                       Outside Knowledge
             the task of knowledge-based visual question answering and
             provide a benchmark, called OK-VQA, where the image             Another lasting, popular legacy of Roosevelt is the stuffed toy bears—teddy bears—
                                                                             named after him following an incident on a hunting trip in Mississippi in 1902.
             content is not sufﬁcient to answer the questions, encour-
             aging methods that rely on external knowledge resources.        Developed apparently simultaneously by toymakers ... and named after President 
                                                                             Theodore "Teddy" Roosevelt, the teddy bear became an iconic children's toy, 
             Our new dataset includes more than 14,000 questions that        celebrated in story, song, and film.
             require external knowledge to answer. We show that the          At the same time in the USA, Morris Michtom created the first teddy bear, after 
             performance of the state-of-the-art VQA models degrades         being inspired by a drawing of Theodore "Teddy" Roosevelt with a bear cub.
             drastically in this new setting. Our analysis shows that our
             knowledge-based VQA task is diverse, difﬁcult, and large
             compared to previous knowledge-based VQA datasets. We         Figure 1: We propose a novel dataset for visual question
             hope that this dataset enables researchers to open up new     answering, where the questions require external knowledge
             avenues for research in this domain.                          resources to be answered. In this example, the visual con-
                                                                           tent of the image is not sufﬁcient to answer the question. A
                                                                           set of facts about teddy bears makes the connection between
             1. Introduction                                               teddy bear and the American president, which enables an-
                                                                           swering the question.
                The ﬁeld of Visual Question Answering (VQA) has
             made amazing strides in recent years, achieving record        tions are about simple counting, colors, and other visual
             numbersonstandardVQAdatasets[20,4,11,17]. Asorigi-            detection tasks, so do not require much logical reasoning
             nally conceived, VQA is not only a fertile ground for vision  or association with external knowledge. The most difﬁcult
             and language research, but is also a proxy to evaluate AI     andinterestingquestionsideallyrequireknowingmorethan
             models for the task of open-ended scene understanding. In     what the question entails or what information is contained
             its ideal form, VQA would require not only visual recog-      in the images.
             nition, but also logical reasoning and incorporating knowl-
             edgeabouttheworld. However,currentVQAdatasets(e.g.,              Consider the question in Figure 1, which asks about the
             [3, 47]) are focused mainly on recognition, and most ques-    relation between the teddy bear and an American president.
                                                                           The information in the image here is not complete for an-
               ∗Workdoneduringinternship at Allen Institute for AI         swering the question. We need to link the image content
                                                                        3195
<<<PAGE 2>>>
             to external knowledge sources, such as the sentences at the      beenextendedtothevideodomainaswell[16,28,35]. Re-
             bottom of the ﬁgure taken from Wikipedia. Given the ques-        cently, [13, 9] address the problem of question answering in
             tion, image, and Wikipedia sentences, there is now enough        aninteractiveenvironment. Noneoftheseapproaches,how-
             information to answer the question: Teddy Roosevelt!             ever, is designed for leveraging external knowledge so they
                More recent research has started to look at how to in-        cannot handle the cases that the image does not represent
             corporate knowledge-based methods into VQA [29, 30, 36,          the full knowledge to answer the question.
             37]. These methods have investigated incorporating knowl-           The problem of using external knowledge for answer-
             edge bases and retrieval methods into VQA datasets with a        ing questions has been tackled by [38, 36, 37, 23, 30, 29].
             set of associated facts for each question. In this work, we go   These methods only handle the knowledge that is repre-
             one step forward and design a VQA dataset which requires         sented by subject-relation-object or visual concept-relation-
             VQAtoperformreasoningusingunstructured knowledge.                attribute triplets, and rely on supervision to do the retrieval
                To enable research in this exciting direction, we in-         of facts. In contrast, answering questions in our dataset re-
             troduce a novel dataset, named Outside Knowledge VQA             quires handling unstructured knowledge resources.
             (OK-VQA), which includes only questions that require ex-         VQAdatasets. In the past few years several datasets have
             ternal resources for answering them. On our dataset, we can      been proposed for visual question answering [26, 3, 12, 44,
             start to evaluate the reasoning capabilities of models in sce-   31, 47, 34, 21, 18, 37]. The DAQUAR dataset [26] includes
             narios wheretheanswercannotbeobtainedbyonlylooking               template-based and natural questions for a set of indoor
             at the image. Answering OK-VQA questions is a challeng-          scenes. [3] proposed the VQA dataset, which is two or-
             ing task since, in addition to understanding the question and    ders of magnitude larger than DAQUAR and includes more
             the image, the model needs to: (1) learn what knowledge is       diverse images and less constrained answers. FM-IQA [12]
             necessarytoanswerthequestions,(2)determinewhatquery              is another dataset that includes multi-lingual questions and
             to do to retrieve the necessary knowledge from an outside        answers. Visual Madlibs [44], constructs ﬁll-in-the-blank
             source of knowledge, and (3) incorporate the knowledge           templates for natural language descriptions.   COCO-QA
             from its original representation to answer the question.         [31] is constructed automatically by converting image de-
                TheOK-VQAdatasetconsistsofmorethan14,000ques-                 scriptions to questions. The idea of Visual 7W [47] is to
             tions that cover a variety of knowledge categories such as       provide object-level grounding for question-answer pairs as
             science & technology, history, and sports. We provide cat-       opposed to image-level associations between images and
             egory breakdowns of our dataset, as well as other relevant       QA pairs.   Visual Genome [21] provides dense annota-
             statistics to examine its properties. We also analyze state-     tions for image regions, attributes, relationships, etc. and
             of-the-art models and show their performance degrades on         provide free-form and region-based QA pairs for each im-
             this new dataset. Furthermore, we provide results for a set      age. MovieQA [34] is a movie-based QA dataset, where
             of baseline approaches that are based on simple knowledge        the QAs are based on information in the video clips, subti-
             retrieval. Our dataset is diverse, difﬁcult, and to date the     tles, scripts, etc. CLEVR [18] is a synthetic VQA dataset
             largest VQA dataset focused on knowledge-based VQA in            that mainly targets visual reasoning abilities. In contrast
             natural images.                                                  to all these datasets, we focus on questions that cannot be
                Our contributions are: (a) we introduce the OK-VQA            answered by the information in the associated image and
             dataset, which includes only questions that require external     require external knowledge to be answered.
             resources to answer; (b) we benchmark some state-of-the-            Most similar to our dataset is FVQA [37]. While that
             art VQA models on our new dataset and show the perfor-           work also tackles the difﬁcult problem of creating a VQA
             manceofthesemodelsdegradesdrastically; (c) we propose            datasetrequiringoutsideknowledge,theirmethodannotates
             a set of baselines that exploit unstructured knowledge.          questions by selecting a fact (a knowledge triplet such as
             2. Related Work                                                  “dogis mammal")fromaﬁxedknowledgebase. Whilethis
                                                                              dataset is still quite useful for testing methods’ ability to in-
             Visual Question Answering (VQA). Visual question an-             corporate a knowledge base into a VQA system, our dataset
             swering (VQA) has been one of the most popular topics in         tests methods’abilitytoretrieverelevantfactsfromtheweb,
             the computer vision community over the past few years.           from a database, or some other source of knowledge that
             Early approaches to VQA combined recurrent networks              was not used to create the questions. Another issue is that
             with CNNs to integrate textual and visual data [27, 1].          triplets are not sufﬁcient to represent general knowledge.
             Attention-based models [11, 25, 39, 40, 41, 47] better guide     Building knowledge bases & Knowledge-based reason-
             the model in answering the questions by highlighting im-         ing. Several knowledge bases have been created using vi-
             age regions that are relevant to the question. Modular net-      sual data or for visual reasoning tasks [46, 8, 10, 32, 49, 48].
             works [
                     2, 15, 19] leverage the compositional nature of the      These knowledge bases are potentially helpful resources
             language in deep neural networks. These approaches have          for answering questions in our dataset. Knowledge-based
                                                                           3196
<<<PAGE 3>>>
                              Vehicles and                     Brands, Companies                   Objects, Material and                Sports and Recreation                  Cooking and Food
                            Transportation                         and Products                            Clothing
                      Q: What sort of vehicle uses         Q: When was the soft drink           Q: What is the material used         Q: What is the sports position        Q: What is the name of the 
                      this item?                           company shown first created?         to make the vessels in this          of the man in the orange shirt?       object used to eat this food? 
                      A: firetruck                         A: 1898                              picture?                              A: goalie                            A: chopsticks
                                                                                                A: copper
                          Geography, History,               People and Everyday Life                Plants and Animals                 Science and Technology                Weather and Climate
                        Language and Culture
                      Q: What days might I most            Q: Is this photo from the 50’s       Q: What phylum does this             Q: How many chromosomes               Q: What is the warmest outdoor 
                      commonly go to this building?        or the 90’s?                         animal belong to?                    do these creatures have?              temperature at which this kind 
                      A: Sunday                            A: 50’s                              A: chordate, chordata                A: 23                                 of weather can happen?
                                                                                                                                                                           A: 32 degrees
                   Figure 2: Dataset examples. Some example questions and their corresponding images and answers have been shown. We
                   showoneexamplequestionfor each knowledge category.
                   question answering has received much more attention in the                                        of these questions.
                   NLPcommunity(e.g., [5, 43, 42, 6, 33, 22, 7]).                                                        GiventhatcurrentVQAdatasetsdonottestexactlywhat
                                                                                                                     we are looking for, we collect a new dataset. We use ran-
                   3. OK-VQADataset                                                                                  dom images from the COCO dataset [24], using the origi-
                                                                                                                     nal 80k-40k training and validation splits for our train and
                        In this section we explain how we collect a dataset which                                    test splits. The visual complexity of these images compared
                   better measures performance of VQA systems requiring ex-                                          to other datasets make them ideal for labeling knowledge-
                   ternal knowledge.              The common VQA datasets such as                                    based questions.
                   [3, 14] do not require much knowledge to answer a large
                   majorityofthequestions. Thedatasetmostlycontainsques-                                                 Intheﬁrstroundoflabeling,weaskedMTurkworkersto
                   tions such as “How many apples are there?”, “What animal                                          write a question given an image. Similar to [3], we prompt
                   is this?”, and “What color is the bowl?”. While these are                                         users to come up with questions to fool a “smart robot.”
                   perfectly reasonable tasks for open-ended visual recogni-                                         Wealso ask in the instructions that the question should be
                   tion, they do not test our algorithms’ ability to reason about                                    related to the image content. In addition, we prompt users
                   a scene or draw on information outside of the image. Thus,                                        not to ask what is in an image, or how many of something
                   for our goal of combining visual recognition with informa-                                        there is, and specify that the question should require some
                   tion extraction from sources outside the image, we would                                          outside knowledge. In a second round of labeling, we asked
                   not be able to evaluate knowledge-based systems as most                                           5differentMTurkworkerstolabeleachquestion-imagepair
                   questions do not require outside knowledge.                                                       with an answer.
                        To see this speciﬁcally, we examine the “age annota-                                             Although this prompt yielded many high-quality ques-
                   tions” that are provided for 10,000 questions in the VQA                                          tions, it also yielded a lot of low quality questions, for ex-
                   dataset [1]. For each question and image pair, an MTurk                                           ample, ones that asked basic questions such as counting,
                   worker was asked how old someone would need to be to                                              did not require looking at the image, or were nonsensical.
                   answer the question. While this is not a perfect metric, it                                       To ensure that the dataset asked these difﬁcult knowledge-
                   is a reasonable approximation of the difﬁculty of a ques-                                         requiring questions, the MTurk provided questions were
                   tion and how much a person would have to know to an-                                              manually ﬁltered to get only questions requiring knowl-
                   swer a question. The analysis shows that more than 78% of                                         edge. From a pool of 86,700 questions, we ﬁltered down
                   the questions can be answered by people who are 10 years                                          to 34,921 questions.
                   old or younger. This suggests that very little background                                             One more factor to consider was the potential bias in
                   knowledge is actually required to answer the vast majority                                        the dataset. As discussed in many works, including [14],
                                                                                                                3197
<<<PAGE 4>>>
                                               Numberof          Number        Knowledge                                                   Answer         Avg. A       Avg. Q
                                               questions        of images         based?                        Goal                         type          length       length
                     DAQUAR[26]                  12,468           1,449              ✘             visual: counts, colors, objects           Open            1.1         11.5
                  Visual Madlibs [44]           360,001           10,738             ✘              visual: scene, objects, person        FITB/MC            2.8          4.9
                     Visual 7W [47]             327,939           47,300             ✘           visual: object-grounded questions            MC             2.0          6.9
                     VQA(v2)[14]                  1.1M            200K               ✘                  visual understanding              Open/MC            1.2          6.1
                     MovieQA[34]                 14,944           408V               ✘            text+visual story comprehension             MC             5.3          9.3
                       CLEVR[18]                999,968          100,000             ✘                    logical reasoning                  Open            1.0         18.4
                      KB-VQA[36]                  2,402             700              ✓            visual reasoning with given KB             Open            2.0          6.8
                       FVQA[37]                   5,826           2,190              ✓            visual reasoning with given KB             Open            1.2          9.5
                    OK-VQA(ours)                 14,055           14,031             ✓         visual reasoning with open knowledge          Open            1.3          8.1
                 Table 1: Comparison of various visual QA datasets. We compare OK-VQA with some other VQA datasets. The bottom
                 three rows correspond to knowledge-based VQA datasets. A length: answer length; Q length: question length; MC: multiple
                 choice; FITB: ﬁll in the blanks; KB: knowledge base.
                 the VQAv1 dataset had a lot of bias. Famously, questions                             the best of our knowledge, the largest VQA dataset speciﬁ-
                 beginning with “Is there a ...” had a very strong bias to-                           cally targeted for knowledge-based VQA on natural scenes.
                 wards “Yes.” Similarly, in our unﬁltered dataset, there were                         Knowledge category. Requiring knowledge for VQA is a
                 a lot of questions with a bias towards certain answers. For                          good start, but there are many different types of knowledge
                 instance, in a lot of images where there is snowfall, the                            that humanshaveabouttheworldthatcouldcomeintoplay.
                 question would ask “What season is it?” Although there                               There is common-sense knowledge: water is wet, couches
                 were other images (such as ones with deciduous trees with                            are found in living rooms. There is geographical knowl-
                 multi-colored leaves) with different answers, there was a                            edge: the Eiffel Tower is in Paris, scientiﬁc knowledge:
                 clear bias towards “winter.” To alleviate this problem, for                          humans have 23 chromosomes, and historical knowledge:
                 train and test, we removed questions so that the answer dis-                         George Washington is the ﬁrst U.S. president. To get a bet-
                 tribution was uniform; speciﬁcally, we removed questions                             ter understanding of the kinds of knowledge our dataset re-
                 if there were more than 5 instances of that answer as the                            quires, we asked ﬁve MTurk workers to annotate each ques-
                 most common answer. This had the effect of removing a                                tion as belonging to one of ten categories of knowledge that
                 lot of the answer bias. It also had the effect of making the                         wespeciﬁed: Vehicles and Transportation; Brands, Compa-
                 dataset harder by limiting the number of times VQA algo-                             nies and Products; Objects, Materials and Clothing; Sports
                 rithms would see questions with a particular answer, mak-                            and Recreation; Cooking and Food; Geography, History,
                 ing outside information more important. We also removed                              Language and Culture; People and Everyday Life, Plants
                 questions which had no inter-annotator agreement on the                              and Animals; Science and Technology; and Weather and
                 answer. Performing this ﬁltering brought us down to 9,009                            Climate. If no one category had a plurality of workers, it
                 questions in train and 5,046 questions in test for a total of                        wascategorized as “Other". This also ensured that the ﬁnal
                 14,055 questions.                                                                    category labels are mutually exclusive. We show the distri-
                     Figure 2 shows some of the collected questions, images,                          bution of questions across categories in Figure 3.
                 and answers from our dataset. More will be provided in                               ComparisonwithotherVQAdatasets. InTable1welook
                 the supplementary material. You can see that these ques-                             at a number of other visual question answering datasets and
                 tions require at least one piece of background knowledge                             compare them to our dataset in a number of different ways.
                 to answer. For instance, in the bottom left question, the                            In the top section, we look at a number of datasets which
                 system needs to recognize that the image is of a christian                           do not explicitly try to include a knowledge component in-
                 churchandknowthatthosechurchesholdreligiousservices                                  cluding the ubiquitous VQAv2 dataset [14], the ﬁrst ver-
                 on Sundays. That latter piece of knowledge should be ob-                             sion of which was one of the ﬁrst datasets to investigate
                 tained from external knowledge resources, and it cannot be                           visual question answering. Compared to these datasets, we
                 inferred from the image and question alone.                                          have a comparable number of questions to DAQUAR [26]
                                                                                                      as well as MovieQA [
                                                                                                                                    34], and many more questions than
                 4. Dataset Statistics                                                                knowledge-based datasets KB-VQA [36] and FVQA [37].
                                                                                                      Wehave fewer questions compared to CLEVR [18] where
                     In this section, we explore the statistical properties of                        the images, questions and answers are automatically gen-
                 ourdataset, and compare to other visual question answering                           erated, as well compared to more large-scale human an-
                 datasets to show that our dataset is diverse, difﬁcult, and, to                      notated visual datasets such as VQAv2 [14], and Visual
                                                                                                  3198
<<<PAGE 5>>>
                                     KNOWLEDGE CATEGORIES                                                        Knowledge Category          Highest relative            Highest relatively 
                                                                       1. Vehicles and                                                       frequency question          frequency answers
                           10. Weather and     Other                   Transportation                                                        words
                               Climate          12%                         16%
                                 3%                                                                              1. Vehicles and             bus, train, truck,          jet, double decker, 
                        9. Science and                                                2. Brands, 
                         Technology                                                Companies and                 Transportation              buses, jet                  take off, coal, freight
                             2%                                                       Products                   2. Brands, Companies        measuring, founder,         ebay, logitech, gift 
                                                                                         3%
                                                                                       3. Objects,               and Companies               advertisements,             shop, flickr, sprint
                       8. Plants and                                                  Material and                                           poster, mobile
                         Animals                                                        Clothing                 3. Objects, Material        scissors, toilets, disk,    sew, wrench, quilt, 
                           17%                                                            8%
                      7. People and                                                                              and Clothing                teddy, sharp                teddy, bib
                      Everyday Life                                                                              4. Sports and               tennis, players, player,    umpire, serve, 
                           9%                                                       4. Sports and                Recreation                  baseball, bat               catcher, ollie, pitcher
                            6. Geography                                             Recreation
                              History,                                                  12%
                           Language and                                    5. Cooking and                        5. Cooking and Food         dish, sandwich, meal,       donut, fork, meal, 
                               Culture                                          Food                                                         cook, pizza                 potato, vitamin c
                                 3%                                             15%
                                                                                                                 6. Geography, History,      denomination, nation,  prom, spire, illinois, 
                                                                                                                 Language and Culture        festival, century,          past, bern
                   Figure 3: Breakdownofquestionsintermsofknowledge                                                                          monument
                   categories. We show the percentage of questions falling                                       7. People and               expressing, emotions,       hello, overall, twice, 
                   into each of our 10 knowledge categories.                                                     Everyday Life               haircut, sunburned,         get married, cross leg
                                                                                                                                             punk
                                                                                                                 8. Plants and Animals       animals, wild, cows,        herbivore, zebra, 
                   Madlibs [44]. Since we manually ﬁltered our dataset to                                                                    habitat, elephants          herd, giraffe, ivory
                   avoid the pitfalls of other datasets and to ensure our ques-                                  9. Science and              indoor, mechanical,         surgery, earlier, 1758, 
                   tions are knowledge-based and because we ﬁltered down                                         Technology                  technology, voltage,        thumb, alan turing
                   commonanswerstoemphasizethelongtailofanswers,our                                                                          connect
                   dataset is more time-intensive and expensive to collect. We                                   10. Weather and             weather, clouds,            stormy, noah, chilly, 
                                                                                                                 Climate                     forming, sunrise,           murky, oasis
                   trade off size in this case for knowledge and difﬁculty.                                                                  windy
                       We can see from the average question lengths and av-
                   erage answer lengths that our questions and answers are                                      Figure 4: For each category we show the question words
                   about comparable to KB-VQA [36] and FVQA [37] and                                            and answers that have the highest relative frequency across
                   longer than the other VQA datasets with the exception of                                     our knowledge categories (i.e. frequency in category di-
                   DAQUARandCLEVR(whicharepartiallyandfully auto-                                               vided by overall frequency).
                   matedfromtemplatesrespectively). Thismakessensesince                                         37 and 25 times respectively), so overall, we still captured
                   wewouldexpectknowledge-basedquestionstobelongeras                                            quite a lot of the variation in scenes.
                   they are typically not able to be as short as common ques-                                       Finally, we show in Figure 4 the question words and an-
                   tions in other datasets such as “How many objects are in the                                 swers in each category that are the most “unique” to get
                   image?” or “What color is the couch?”.                                                       a better idea of what types of questions we have in each
                   Question statistics.            We also collected statistics for our                         of these categories.           We calculate these for each knowl-
                   dataset by looking at the number of questions, and by look-                                  edge category by looking at the number of appearances
                   ing at which were most frequent for each knowledge cate-                                     within the category over the total number in the dataset
                   gory. OK-VQA has 12,591 unique questions out of 14,055                                       to see which question words and answers had the highest
                   total, and 7,178 unique question words. This indicates that                                  relative frequency in their category. When looking at the
                   we get a variety of different questions and answers in our                                   question words, we see words speciﬁc to categories such
                   dataset.     We also looked at the variety of images in our                                  as bus in Vehicles and Transportation, sandwich in Cook-
                   dataset. As we stated earlier, our images come from the                                      ing and Food, and clouds in Weather and Climate. We also
                   COCOimagedataset, so our dataset contains the same ba-                                       see that the answers are also extremely related to each cat-
                   sic distribution of images. However, we only use a subset                                    egory, such as herbivore in Plants and Animals, and umpire
                   of COCO images, so we want to see if we still get a wide                                     in Sports and Recreation. In the supplemental material, we
                   distribution of images. For this, we ran a Places2 [45] clas-                                also show the most common question words and answers.
                   siﬁer on our images and looked at the top-1 scene class for
                   eachimageandcomparedthattoCOCOoverall. Outof365                                              5. Benchmarking
                   scenes, our dataset contains all but 5 classes: hunting lodge,
                   mansion,movietheater,ruinandvolcano. Theseclassesap-                                             In this section, we evaluate current state-of-the-art VQA
                   pear infrequently in the overall COCO dataset (10, 22, 28,                                   approaches and provide results for some baselines, includ-
                                                                                                           3199
<<<PAGE 6>>>
                    Method         OK-VQA      VT        BCP    OMC       SR      CF    GHLC      PEL      PA      ST     WC     Other
                    Q-Only           14.93     14.64    14.19   11.78   15.94   16.92    11.91   14.02   14.28   19.76   25.74   13.51
                     MLP             20.67     21.33    15.81   17.76   24.69   21.81    11.91   17.15   21.33   19.29   29.92   19.81
                ArticleNet (AN)       5.28     4.48      0.93    5.09    5.11    5.69    6.24     3.13    6.95    5.00    9.92    5.33
                   BAN[20]           25.17     23.79    17.67   22.43   30.58   27.90    25.96   20.33   25.60   20.95   40.16   22.46
                  MUTAN[4]           26.41     25.36    18.95   24.02   33.23   27.73    17.59   20.09   30.44   20.48   39.38   22.46
                  BAN+AN             25.61     24.45    19.88   21.59   30.79   29.12    20.57   21.54   26.42   27.14   38.29   22.16
                 MUTAN+AN            27.84     25.56    23.95   26.87   33.44   29.94    20.71   25.05   29.70   24.76   39.84   23.62
                BAN/ANoracle         27.59     26.35    18.26   24.35   33.12   30.46    28.51   21.54   28.79   24.52    41.4   25.07
              MUTAN/ANoracle         28.47     27.28    19.53   25.28   35.13   30.53    21.56   21.68   32.16   24.76    41.4   24.85
             Table 2: Benchmark results on OK-VQA. We show the results for the full OK-VQA dataset and for each knowledge
             category: VehiclesandTransportation(VT);Brands,CompaniesandProducts(BCP);Objects,MaterialandClothing(OMC);
             Sports and Recreation (SR); Cooking and Food (CF); Geography, History, Language and Culture (GHLC); People and
             Everyday Life (PEL); Plants and Animals (PA); Science and Technology (ST); Weather and Climate (WC); and Other.
             ing knowledge-based ones.                                       truth answersappearinthearticleandineachsentence. The
                MUTAN [4]: Multimodal Tucker Fusion (MUTAN)                  architecture is shown in Figure 5. To ﬁnd the answer to a
             model [4], a recent state-of-the-art tensor-based method for    question, we pick the top scoring word among the retrieved
             VQA.Speciﬁcally,weusetheattentionversionofMUTAN,                sentences. More speciﬁcally, we take the highest value of
             andchoosetheparameterstomatchthesinglebestperform-              awi.asent, where awi is the score for the word being the
             ing model of [4].                                               answer and asent is the score for the sentence including the
                       20]: Bilinear Attention Networks for VQA. A re-       answer.
                BAN[
             cent state-of-the art VQA method that uses a co-attention          ForamoredetaileddescriptionofArticleNetseethesup-
             mechanism between the question features and the bottom-         plementary material.
             up detection features of the image. We modify some hy-             MUTAN+AN:WeaugmentMUTANwiththetopsen-
             perparameters to improve performance on our dataset (see        tence hidden states (h       in Figure 5) from ArticleNet
                                                                                                    sent
             supplemental material).                                         (AN). During VQA training and testing, we take the top
                MLP:TheMLPhas3hiddenlayers with ReLU activa-                 predictedsentences(ignoringduplicatesentences),andfeed
                                                                                                                                    33].
             tions and hidden size 2048 that concatenates the image and      theminthememoryofanend-to-endmemorynetwork[
             question features after a skip-thought GRU after one fully      Theoutputofthememorynetworkisconcatenatedwiththe
             connected layer each. Like MUTAN, it uses fc7 features          output of the ﬁrst MUTAN fusion layer.
             from ResNet-152.                                                   BAN + AN: Similarly, we incorporate the ArticleNet
                Q-Only: The same model as MLP, but only takes the            hidden states into BAN and incorporate it into VQA
             question features.                                              pipeline with another memory network. We concatenate
                ArticleNet (AN): We consider a simple knowledge-             output of the memory network with the BAN hidden state
             based baseline that we refer to as ArticleNet. The idea is      right before the ﬁnal classiﬁcation network. See the supple-
             to retrieve some articles from Wikipedia for each question-     mental material for details.
             imagepairandthentrainanetworktoﬁndtheanswerinthe                   MUTAN/ANoracle: As an upper bound check, and to
             retrieved articles.                                             see potentially how much VQA models could beneﬁt from
                Retrieving articles is composed of three steps. First, we    the knowledge retrieved using ArticleNet, we also provide
             collect possible search queries for each question-image pair.   results on an oracle, which simply takes the raw ArticleNet
             Wecomeupwithall possible queries for each question by           and MUTAN predictions, taking the best answer (compar-
             combiningwordsfromthequestionandwordsthatareiden-               ing to ground truth) from either.
             tiﬁed by pre-trained image and scene classiﬁers. Second,           BAN/ANoracle: SimilartotheMUTAN/ANoracle,but
             weusetheWikipediasearchAPItogetthetopretrievedar-               wetake the best answer from the raw ArticleNet and BAN
             ticle for each query. Third, for each query and article, we     instead, again taking the best answer for each question.
             extract a small subset of each article that is most relevant    Benchmark results. We report the results using the com-
             for the query by selecting the sentences within the article     mon VQA evaluation metric [
                                                                                                            3], but use each of our an-
             that best correspond to our query based on the frequency of     swerannotationstwice, since we have 5 answer annotations
             those query words in the sentence.                              versus 10 in [3]. We also stem the answers using Porter
                Once the sentences have been retrieved, the next step is     stemming to consolidate answers that are identical except
             to ﬁlter and encode them for use in VQA. Speciﬁcally, we        for pluralization and conjugation as in [37]. We also show
             train ArticleNet to predict whether and where the ground        the breakdowns for each of our knowledge categories. The
                                                                          3200
<<<PAGE 7>>>
                                           Image feature                        title
                      question                    V
                                                           +       h            GRU              h
                                                                     QV             title          title
                                    GRU          h
                          Q             Q          Q
                                                                                                               +         a
                                                                                                                          title
                                     Key                                     a    … a
                                                                              w        w
                                                                               1        N
                                 Input                                                             +          h          aart
                                 Hidden State                                                                  art
                                 Output                                      sentence                          +         asent
                                 GRU
                                 FC                                h            GRU              h
                                 FC (x2)                             QV             sent           sent
                                 Copy (no action)
                                                                            a     … a
                                 Add                                          w        w                                    x5
                             +                                                 1        N
             Figure 5: ArticleNet architecture. ArticleNet takes in the question Q and visual features V . All modules within the dotted
             line box share weights. The output of the GRUs is used to classify each word as the answer or not awi. The ﬁnal GRU hidden
             states h     and h      are put through fully connected layers to predict if the answer is in the sentence a     or title a   ,
                     title      sent                                                                                     sent          title
             and then are combined together and used to classify if the answer is in the article aart.
             results are reported in Table 2.                                              Method       VQAscoreonOK-VQA
                Theﬁrstobservationisthatnomethodgetsclosetonum-                           ResNet152               26.41
             bers on standard VQA dataset such as VQA [14] (where                         ResNet50                24.74
             the best real open-ended result for the 2018 competition                     ResNet18                23.64
             is 72.41). Moreover, state-of-the-art models such as MU-                      Q-Only                 14.93
             TAN[4]andBAN[20],whicharespeciﬁcallydesignedfor
             VQA to learn high-level associations between the image            Table 3: Results on OK-VQAwithdifferentvisualfeatures.
             and question, get far worse numbers on our dataset. This
             suggests that OK-VQA cannot be solved simply by coming
             up with a clever model, but actually requires methods that        the other VQAbaselines, suggesting that visual features are
             incorporate information from outside the image.                   indeed necessary and our procedure for reducing answer
                It is interesting to note that although the performance of     bias was effective.
             the raw ArticleNet is low, it provides improvement when           Visual feature ablation. We also want to demonstrate the
             combined with the state-of-the-art models (MUTAN + AN             difﬁculty of the dataset from the perspective of visual fea-
             and BAN + AN). From the oracle numbers, we can see                tures, so we show MUTAN results using different ResNet
             that the knowledge retrieved by ArticleNet provides com-          architectures. The previously reported result for MUTAN
             plementaryinformationtothestate-of-the-artVQAmodels.              is based on ResNet152. We also show the results using ex-
             TheseoraclesareoptimisticupperboundsusingArticleNet,              tracted features from ResNet50 and ResNet18 in Table 3.
             but they show that smarter knowledge-retrieval approaches         From this table it can be seen that going from ResNet50
             could have stronger performance on our dataset. Note that         to ResNet152 features only has a marginal improvement,
             ArticleNet is not directly trained on VQA and can only pre-       and similarly going from ResNet18 to ResNet50. How-
             dict answers within the articles it has retrieved. So the rela-   ever, going from ResNet18 to no image (Q-Only) causes
             tively low performance on VQA is not surprising.                  a large drop in performance. This suggests that our dataset
                Looking at the category breakdowns, we see that Arti-          is indeed visually grounded, but better image features do
             cleNet is particularly helpful for brands, science, and cook-     not hugely improve the results, suggesting the difﬁculty lies
             ing categories, perhaps suggesting that these categories are      in the retrieving the relevant knowledge and reasoning re-
             better represented in Wikipedia. It should be noted that          quired to answer the questions.
             the major portion of our dataset requires knowledge outside       Scale ablation. Finally, we investigate the degree to which
             Wikipedia such as commonsense or visual knowledge.                the size of our dataset relates to its difﬁculty as opposed to
                The Q-Only baseline performs signiﬁcantly worse than           the nature of the questions themselves. We ﬁrst take a ran-
                                                                            3201
<<<PAGE 8>>>
                                                   Q: What fruit family is this                                            Q: What type of liquid does                                              Q: How many chromosomes 
                                                   from?                                                                   this animal produce?                                                     do these creatures have?
                                                   GT Ans: citrus,orange                                                   GT Ans: milk                                                             GT Ans: 46,23,23 pairs
                                                   MUTAN : fruit                                                           MUTAN: beef                                                              MUTAN : 3
                                                   MUTAN+AN: citrus                                                        MUTAN+AN: milk                                                           MUTAN+AN: 23
                                          Retrieved Sentences                                                      Retrieved Sentences                                                     Retrieved Sentences
                                                        The orange (specifically the                                             Cows of certain breeds that are                                          Human cells have 23 pairs of 
                           Query: fruit orange          sweet orange) is the fruit of the               Query: cow               kept for the milk they give are           Query: chromosomes             chromosomes 22 pairs of 
                                                        citrus species citrus x sinensis in                                      called dairy cows or milking                                             autosomes and one pair of sex 
                                                        the family Rutaceae                                                      cows (formerly milch cows)                                               chromosomes …
                                                        The citrus sinensis is subdivided                                        Milk is a pale liquid produced                                           Most eukaryotic cells have a set 
                          Query: orange family          into four classes with distinct              Query: liquid cow           by the mammary glands of                  Query: chromosomes             of chromosomes 46 in humans 
                                                        characteristics common                                                   mammals                                                                  with the genetic material 
                                                        oranges …                                                                                                                                         spread among them
                                                        [But] most seedless citrus fruits                                        A cow will produce large                                                 X inactivation is when one of 
                               Query: fruit             require a stimulus from                    Query: produce cow            amounts of milk over its                  Query: chromosomes             the two x chromosomes in 
                                                        pollination to produce fruit                                             lifetime                                                                 females is almost completely 
                                                                                                                                                                                                          inactivated
                      Figure 6: Qualitative results. We show the result of MUTAN+AN comparedtotheMUTANbaselineanswerandtheground
                      truth answer (‘GT Ans’). We show the query words that were used by ArticleNet (pink boxes) and the corresponding most
                      relevant sentences (blue boxes).
                      dom subdivision of our training set and train MUTAN on                                                                                      OKVQA Scale Ablation
                      progressively smaller subsets of the training data and eval-                                                           30
                      uate on our original test set. Figure 7 shows the results.
                      Qualitativeexamples. Weshowsomequalitativeexamples                                                                     25
                      in Figure 6 to see how outside knowledge helps VQA sys-                                                               e20
                      temsinafewexamples. WecompareMUTAN+ANmethod                                                                           r
                                                                                                                                            o
                                                                                                                                            c
                      withMUTAN.Theleftexampleaskswhat“fruitfamily”the                                                                      S
                                                                                                                                            A 15
                      fruit in the image (oranges) comes from. We see that two                                                              Q
                                                                                                                                            V10
                      sentences that directly contain the information that oranges                                                          OK
                      are citrus fruits are retrieved —“The orange ... is a fruit                                                              5
                      of the citrus species” and “The citrus sinensis is subdivided
                      into four classes [including] common oranges”.                                                                           0
                           The middle example asks what liquid the animal (cow)                                                                         5      10     20      30      40      50      60      70      80      90 100
                      produces. The ﬁrst and third sentences tell us that cows                                                                                                   % Training Questions
                      produce milk, and the second sentence tells us that milk is                                                      Figure 7: Results on OK-VQA using different sizes of the
                      a liquid. This gives the combined MUTAN+AN method                                                                training set.
                      enough information to correctly answer milk.
                           The example on the right asks how many chromosomes
                      humans have. It is somewhat ambiguous whether it means                                                           ing on external knowledge resources. We show that the
                      how many individual chromosomes or how many pairs, so                                                            performance of state-of-the-art VQA models signiﬁcantly
                      workers labeled both as answers. The retrieved articles are                                                      dropsonOK-VQA. Weanalyzethepropertiesandstatistics
                      helpful here, retrieving two different articles referring to 23                                                  of the dataset and show that background knowledge can im-
                      pairs of chromosomesand46chromosomestotal. Thecom-                                                               prove results on our dataset. Our experimental evaluations
                      bined MUTAN+AN method correctly answers 23, while                                                                showthat the proposed benchmark is quite challenging and
                      MUTANguesses3.                                                                                                   that there is a large room for improvement.
                                                                                                                                       Acknowledgements: We would like to thank everyone who took time to
                      6. Conclusion                                                                                                    review this work and provide helpful comments. This work is in part sup-
                                                                                                                                       ported by NSF IIS-165205, NSF IIS-1637479, NSF IIS-1703166, Sloan
                           We address the task of knowledge-based visual ques-                                                         Fellowship, NVIDIAArtiﬁcial Intelligence Lab, and Allen Institute for ar-
                      tion answering. We introduce a novel benchmark called                                                            tiﬁcial intelligence. Thanks to Aishwarya Agrawal, Gunnar Sigurdsson,
                      OK-VQA for this task. Unlike the common VQA bench-                                                               Victoria Donley, Achal Dave, and Eric Kolve who provided valuable as-
                      marks,theinformationprovidedinthequestionandthecor-                                                              sistance, advice and feedback. Kenneth Marino is supported by the De-
                      responding images of OK-VQA is not sufﬁcient to answer                                                           partment of Defense (DoD) through the National Defense Science & En-
                      the questions, and answering the questions requires reason-                                                      gineering Graduate Fellowship (NDSEG) Program.
                                                                                                                                  3202
<<<PAGE 9>>>
             References                                                        [18] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,
                                                                                    Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr:
              [1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret           Adiagnosticdatasetforcompositionallanguageandelemen-
                  Mitchell, CLawrenceZitnick,DeviParikh,andDhruvBatra.              tary visual reasoning. In CVPR, 2017. 2, 4
                  Vqa: Visual question answering. IJCV, 2017. 2, 3             [19] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,
              [2] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan           JudyHoffman,LiFei-Fei,C.LawrenceZitnick,andRossB.
                  Klein. Neural module networks. In CVPR, 2016. 2                   Girshick. Inferring and executing programs for visual rea-
              [3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret           soning. In ICCV, 2017. 2
                  Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi         [20] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilin-
                  Parikh. VQA: visual question answering. In ICCV, 2015.            ear Attention Networks. arXiv preprint arXiv:1805.07932,
                  1, 2, 3, 6                                                        2018. 1, 6, 7
              [4] HediBen-younes,RémiCadene,MatthieuCord,andNicolas            [21] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
                  Thome.Mutan: Multimodaltuckerfusionforvisualquestion              Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
                  answering. In ICCV, 2017. 1, 6, 7                                 tidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and
              [5] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy              Li Fei-Fei. Visual genome: Connecting language and vision
                  Liang. Semantic parsing on freebase from question-answer          using crowdsourced dense image annotations. IJCV, 2017. 2
                  pairs. In EMNLP, 2013. 3                                     [22] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,
              [6] Antoine Bordes, Sumit Chopra, and Jason Weston. Question          James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain
                  answering with subgraph embeddings. In EMNLP, 2014. 3             Paulus, and Richard Socher. Ask me anything: Dynamic
              [7] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bor-            memorynetworksfornaturallanguageprocessing. InICML,
                  des. Reading wikipedia to answer open-domain questions.           2016. 3
                  arXiv, 2017. 3                                               [23] Guohao Li, Hang Su, and Wenwu Zhu. Incorporating exter-
              [8] XinleiChen,AbhinavShrivastava,andAbhinavGupta. Neil:              nal knowledge to answer open-domain visual questions with
                  Extracting visual knowledge from web data. In ICCV, 2013.         dynamic memorynetworks. arXiv, 2017. 2
                  2                                                            [24] Tsung-YiLin,MichaelMaire,SergeJ.Belongie,LubomirD.
              [9] AbhishekDas,SamyakDatta,GeorgiaGkioxari,StefanLee,                Bourdev,RossB.Girshick,JamesHays,PietroPerona,Deva
                  Devi Parikh, and Dhruv Batra. Embodied question answer-           Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft
                  ing. arXiv, 2017. 2                                               COCO:commonobjectsincontext. InECCV,2014. 3
             [10] Santosh Divvala, Ali Farhadi, and Carlos Guestrin. Learning  [25] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.
                  everything about anything: Webly-supervised visual concept        Hierarchical question-image co-attention for visual question
                  learning. In CVPR, 2014. 2                                        answering. In NIPS, 2016. 2
             [11] AkiraFukui,DongHukPark,DaylenYang,AnnaRohrbach,              [26] Mateusz Malinowski and Mario Fritz. A multi-world ap-
                  Trevor Darrell, and Marcus Rohrbach.   Multimodal com-            proach to question answering about real-world scenes based
                  pact bilinear pooling for visual question answering and vi-       onuncertain input. In NIPS, 2014. 2, 4
                  sual grounding. In EMNLP, 2016. 1, 2                         [27] Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz.
             [12] Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei             Ask your neurons: A neural-based approach to answering
                  Wang, and Wei Xu. Are you talking to a machine? dataset           questions about images. In ICCV, 2015. 2
                  andmethodsformultilingualimagequestion. InNIPS,2015.         [28] Jonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, and Bo-
                  2                                                                 hyung Han. Marioqa: Answering questions by watching
             [13] Daniel Gordon, Aniruddha Kembhavi, Mohammad Raste-                gameplay videos. In ICCV, 2017. 2
                  gari, Joseph Redmon, Dieter Fox, and Ali Farhadi. IQA: Vi-   [29] Medhini Narasimhan, Svetlana Lazebnik, and Alexander G
                  sual question answering in interactive environments. arXiv,       Schwing. Out of the box: Reasoning with graph convolution
                  2017. 2                                                           nets for factual visual question answering. NIPS, 2018. 2
             [14] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-      [30] Medhini Narasimhan and Alexander G. Schwing. Straight
                  tra, and Devi Parikh. Making the v in vqa matter: Elevating       to the facts: Learning knowledge base retrieval for factual
                  the role of image understanding in visual question answer-        visual question answering. In ECCV, 2018. 2
                  ing. In CVPR, 2017. 3, 4, 7                                  [31] Mengye Ren, Ryan Kiros, and Richard S. Zemel. Exploring
             [15] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor               models and data for image question answering. In NIPS,
                  Darrell, and Kate Saenko. Learning to reason: End-to-end          2015. 2
                  module networks for visual question answering. In ICCV,      [32] Fereshteh Sadeghi, Santosh K Divvala, and Ali Farhadi.
                  2017. 2                                                           Viske: Visual knowledge extraction and question answering
             [16] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and           byvisual veriﬁcation of relation phrases. In CVPR, 2015. 2
                  Gunhee Kim. TGIF-QA: Toward spatio-temporal reasoning        [33] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob
                  in visual question answering. In CVPR, 2017. 2                    Fergus. End-to-end memory networks. In NIPS, 2015. 3, 6
             [17] Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach,     [34] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,
                  Dhruv Batra, and Devi Parikh.    Pythia v0. 1: the win-           Antonio Torralba, Raquel Urtasun, and Sanja Fidler.
                  ning entry to the vqa challenge 2018.     arXiv preprint          Movieqa: Understandingstoriesinmoviesthroughquestion-
                  arXiv:1807.09956, 2018. 1                                         answering. In CVPR, 2016. 2, 4
                                                                            3203
<<<PAGE 10>>>
      [35] Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, and
        Song-ChunZhu. Jointvideoandtextparsingforunderstand-
        ing events and answering queries. IEEE MultiMedia, 2014.
        2
      [36] Peng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and
        AntonvandenHengel. Explicit knowledge-based reasoning
        for visual question answering. In IJCAI, 2017. 2, 4, 5
      [37] Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel,
        and Anthony R. Dick. Fvqa: Fact-based visual question an-
        swering. TPAMI, 2017. 2, 4, 5, 6
      [38] Qi Wu, Peng Wang, Chunhua Shen, Anthony R. Dick, and
        Anton van den Hengel. Ask me anything: Free-form vi-
        sual question answering based on knowledge from external
        sources. In CVPR, 2016. 2
      [39] Caiming Xiong, Stephen Merity, and Richard Socher. Dy-
        namic memory networks for visual and textual question an-
        swering. In ICML, 2016. 2
      [40] Huijuan Xu and Kate Saenko. Ask, attend and answer: Ex-
        ploring question-guided spatial attention for visual question
        answering. In ECCV, 2016. 2
      [41] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and
        Alexander J. Smola. Stacked attention networks for image
        question answering. In CVPR, 2016. 2
      [42] Xuchen Yao and Benjamin Van Durme. Information extrac-
        tion over structured data: Question answering with freebase.
        In ACL, 2014. 3
      [43] Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng
        Gao. Semantic parsing via staged query graph generation:
        Question answering with knowledge base. In ACL-IJCNLP,
        2015. 3
      [44] Licheng Yu, Eunbyung Park, Alexander C. Berg, and
        TamaraL.Berg. Visualmadlibs: Fillintheblankdescription
        generation and question answering. In ICCV, 2015. 2, 4, 5
      [45] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
        and Antonio Torralba. Places: A 10 million image database
        for scene recognition. TPAMI, 2017. 5
      [46] Yuke Zhu, Alireza Fathi, and Li Fei-Fei. Reasoning about
        object affordances in a knowledge base representation. In
        ECCV,2014. 2
      [47] Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-
        Fei. Visual7w: Grounded question answering in images. In
        CVPR,2016. 1, 2, 4
      [48] Yuke Zhu, Joseph J. Lim, and Li Fei-Fei. Knowledge acqui-
        sition for visual question answering via iterative querying. In
        CVPR,2017. 2
      [49] YukeZhu,CeZhang,ChristopherRé,andLiFei-Fei. Build-
        ingalarge-scalemultimodalknowledgebaseforvisualques-
        tion answering. arXiv, 2015. 2
                                  3204
