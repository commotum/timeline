                            Preprint, Under Review.
                            Figure 2: Dynamic Planning Agent Architecture. Our agent is a single, monolithic LLM whose
                            conceptual policies are realized through its unified output format. The decision to plan (ϕθ) is made
                            implicitly by the model’s choice to begin its generation with a <plan> token. This single output
                            string is then parsed to extract the action (a ) and, if present, the new plan (p ), thereby executing the
                                                                       t                                t
                            acting (πθ) and planning (ψθ) policies.
                            LLMagentscanbeeffectively steered through adaptive planning, enabling integration of external
                            human-generated plans post-RL training.
                            3    ACONCEPTUALFRAMEWORKFORDYNAMICPLANNINGWITHLLM
                                 AGENTS
                            Deciding when to allocate test-time compute for planning is a central challenge for LLM agents.
                            To address this in a principled way, we first establish a conceptual framework that formalizes the
                            underlying cost-benefit trade-offs. This framework provides the theoretical motivation for our
                            practical training methodology, which uses reinforcement learning to teach an agent to implicitly
                            master this dynamic planning skill.
                            Consider a sequential decision-making environment modelled as a Partially-Observable Markov
                            Decision Process ⟨S,A,T,R,O,γ⟩ (states, actions, stochastic transitions, rewards, observations,
                            discount factor). An LLM agent with parameters θ acts within this framework by generating tokens.
                            Specifically, at each timestep t, the agent receives an observation o , described in natural language,
                                                                                                t
                            andmaintains an internal context c = (o ,history) which includes the current observation, a history
                                                              t      t
                            of previous observations and actions, and any existing plan p    . Formally, the agent’s behaviour is
                                                                                         t−1
                            decomposedinto a decision policy ϕ , a planning policy ψ , and an acting policy π :
                                                                 θ                     θ                        θ
                                                  ϕ (d | c ,p     ),   ψ (p | c ,p     ),  π (a | c ,p )
                                                    θ  t    t  t−1       θ  t   t  t−1       θ  t   t   t
                            Importantly, these three policies are not separate architectural components but rather a conceptual
                            decompositionoftheunifiedoutputfromasingle,monolithicLLM(Figure2). Thedecisionpolicyϕ
                                                                                                                               θ
                            corresponds to the decision d ∈ {0,1}, where d = 1 signifies that a new plan p will be generated
                                                         t                   t                               t
                            bythe planning policy ψ . If d = 0, the agent continues with the existing plan p     . Thus the plan
                            selection mechanism is: θ      t                                                  t−1
                                                        p =d ·ψ (p |c ,p         ) +(1−d )·p
                                                         t     t   θ  t   t   t−1           t    t−1
                            Finally, the acting policy π generates action a based on c and p .
                                                       θ                   t           t      t
                            3.1   WHENSHOULDANAGENTPLAN?
                            Intuitively, an agent should only plan when the expected benefit outweighs its cost. We quantify this
                            state-dependent trade-off using a simple cost-benefit analysis:
                            The expected benefit of planning, or the Planning Advantage, measures how much the agent’s
                            expected future rewards improve by adopting a new plan generated by ψθ (i.e., if the decision dt = 1
                                                                             4
