Keeping Neural Networks Simple by Minimizing
the Description Length of the Weights

Geoffrey E. Hinton and Drew van Camp
Department of Computer Science
University of Toronto

) King

College Road

‘TYoronlo M58 LA4, Canada

Abstract

Supervised neural networks generalize well if
there is much less information in the weights
than there is in the output vectors of the train
ing cases. So during learning, it is impor-
(ant to keep the weights simple by penaliz-
ing the amount of information hey contain,
‘The amount of information in a weight can
be controlled by adding Gaussian noise and
the noise level can be adapled during learning
lo oplimize the Urade-olf between the expected
squared error of the network and the amount
of information in the weights. We describe
a method of computing the derivatives of the
expected squared error and of the amount of
information in the noisy weights in a uel-
work Uhat conlains a layer of uou-linear hidden
unils, Provided Uhe oulpul units are linewr, Uhe
exach derivatives can be computed eflicienty
without lime-cousuming Monte Carlo simula-
lions. ‘The idea of minimizing Uhe amount of
information thal is required lo communicate
the weights of a neural uelwork leads lo a
number of interesting schemes for encoding the
weights,

1 Introduction

In many practical learning tasks Unere is little available
iraining dala so any reasonably complicated model will
lend lo overfil the data and give poor generalization lo
new dala. ‘To avoid overfilting we need lo ensure Unal
there is less information in Uhe weights (han Uhere is in
ihe oulpul veclors of the (raining cases. Researchers
have cousidered many possible ways of limiling the in-
formation in the weights:

© Limit the number of connections in the network
(and hope that each weight does nol have boo much
information in il).

Divide the connections into subsets, and force the
weighs within a subset lo be identical, If this
eight-sharing” is based on an analysis of the nat-
ural symmetries of the lask it can be very effective
(Lang, Waibel and Hinton (1990); LeCun 1989).

Quantize all he weights in the network so thal a
probability mass, p, can be assigned lo each quan-
lized value. ‘The number of bils in a weight is then
—logp, provided we ignore the cost of defining the
quantization, Unfortunately Uhis method leads lo
a difficull search space because the cost of a weight
does nol have a smooth derivative,

2 Applying the Minimum Description
Length Principle

When filling models lo data, il is always possible lo fit
ihe training dala beiler by using a more complex model,
bul this may make (he model worse al filling new dala.
So we need some way of deciding when extra complex-
ily in he model is nol worth the improvement in the
dala-fil, ‘Ihe Minimum Description Length Principle
(Rissunen, 1986) asserls Unat the best model of some
dala is Uhe one Uhal minimizes Uhe combined cost of
describing the model and describing Uhe iisfit between
dhe model and the dala. For supervised neural nelworks
with a predelermined architecture, (he model cost is the
number of bils il takes to describe the weights, and the
dala-misfil, cost is Uhe umber of bits it lakes bo describe
the discrepancy belween the correct oulpul and the oul-
pul of Uhe neural nelwork on each Uraining case. We can
think in lerits of a sender who can see both the input
veclor and the correct oulpul and a receiver who can
only see the input vector. ‘Ihe sender first fils a neural
network, of pre-arranged archileclure, lo Uhe complete
sel of Uraining cases, hen sends the weights lo the re-
ceiver. For each Uraining case Uhe sender also sends the
discrepancy between the nel’s output and the correct
oulpul, By adding this discrepancy lo the oulpul of
the net, the receiver can generale exactly the correct
output.

