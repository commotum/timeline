# Constitutional AI: Harmlessness from AI Feedback (2023)
Source: 84b790-2022.pdf

## Core reasons
- Describes a new AI safety approach that shapes outputs according to principles, indicating an alignment/training focus.
- Discusses reinforcement learning from human feedback as the standard alignment method, framing the work around training methodology.

## Evidence extracts
- "Anthropic has uncovered a new approach to AI safety that shapes the outputs of AI systems according to a set of principles." (p. 1)
- "The current industry standard for aligning models with human preferences is called reinforcement learning from human feedback (RLHF)." (p. 1)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
