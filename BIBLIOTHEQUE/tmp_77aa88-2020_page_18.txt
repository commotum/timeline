                             Published as a conference paper at ICLR 2021
                                                                    20%       40%      60%       80%
                                                             0     15.0%     31.2%    52.3%     73.5%
                                                           0.01    13.7%     28.7%    50.1%     72.9%
                                                           0.02    12.8%     27.8%    48.9%     73.1%
                                                           0.05    11.6%     25.6%    47.1%     21.0%
                                                            0.1    4.6%      6.0%      8.7%     56.1%
                                                            0.2    5.3%      7.4%     23.3%     77.1%
                                                            0.5    17.6%     40.9%    80.1%     89.9%
                              Table 8: Validation accuracy of the bootstrapped-SAM for different levels of noise and different œÅ
                                                                                          CIFAR-10            CIFAR-100
                                     Model                          Augmentation      œÅ = 0.05     SGD rho=0.05         SGD
                                     WRN-28-10(200epochs)           Basic                 2.7       3.5       16.5       18.8
                                     WRN-28-10(200epochs)           Cutout                2.3       2.6       14.9       16.9
                                     WRN-28-10(200epochs)           AA                    2.1       2.3       13.6       15.8
                                     WRN-28-10(1800epochs)          Basic                 2.4       3.5       16.3       19.1
                                     WRN-28-10(1800epochs)          Cutout                2.1       2.7       14.0       17.4
                                     WRN-28-10(1800epochs)          AA                    1.6       2.2       12.8       16.1
                                     WRN26-2x6ss                    Basic                 2.4       2.7       15.1       17.0
                                     WRN26-2x6ss                    Cutout                2.0       2.3       14.2       15.7
                                     WRN26-2x6ss                    AA                    1.7       1.9       12.8       14.1
                                     PyramidNet                     Basic                 2.1       4.0       15.4       19.7
                                     PyramidNet                     Cutout                1.6       2.5       13.1       16.4
                                     PyramidNet                     AA                    1.4       1.9       12.1       14.6
                                     PyramidNet+ShakeDrop           Basic                 2.1       2.5       13.3       14.5
                                     PyramidNet+ShakeDrop           Cutout                1.6       1.9       11.3       11.8
                                     PyramidNet+ShakeDrop           AA                    1.4       1.6       10.3       10.6
                             Table 9: Results for the CIFAR-10/CIFAR-100 experiments, using œÅ = 0.05 for all mod-
                             els/datasets/augmentations
                             both types of updates becomes weaker. Fortunately, the model trained without the second order
                             terms reaches a lower test error, showing that the most efÔ¨Åcient method is also the one providing the
                             best generalization on this example. The reason for this is quite unclear and should be analyzed in
                             follow up work.
                             C.5    CHOICE OF P-NORM
                             Our theorem is derived for p = 2, although generalizations can be considered for p ‚àà [1,+‚àû] (the
                             expression of the bound becoming way more involved). Empirically, we validate that the choice
                             p = 2 is optimal by training a wide ResNet on CIFAR-10 with SAM for p = ‚àû (in which case we
                                   ÀÜ                                                   ÀÜ              œÅ
                             have (w) = œÅsign(‚àá L (w))) and p = 2 (giving (w) =                           2(‚àá L (w))). We do
                                                      w S                                       ||‚àá L (w)||      w S
                                                                                                   w S      2
                             not consider the case p = 1 which would give us a perturbation on a single weight. As an additional
                                                                                                                     ÀÜ          œÅ
                             ablation study, we also use random weight perturbations of a Ô¨Åxed Euclidean norm: (w) =             2 z
                                                                                                                              ||z||
                             with z ‚àº N(0,I ). We report the test accuracy of the model in Ô¨Ågure 6.                               2
                                               d
                             Weobserve that adversarial perturbations outperform random perturbations, and that using p = 2
                             yield superior accuracy on this example.
                             C.6    SEVERAL ITERATIONS IN THE INNER MAXIMIZATION
                             Toempiricallyverifythatthelinearizationoftheinnerproblemissensible,wetrainedaWideResNet
                             ontheCIFARdatasetsusingavariant of SAM that performs several iterations of projected gradient
                             ascent to estimate max L(w + ). We report the evolution of max L(w + ) ‚àí L(w) during
                                                                                                      
                             training (where L stands for the training error rate computed on the current batch) in Figure 7, along
                                                                                18
