# TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models (2025)
Source: ce62ee-2025.pdf

## Core reasons
- Introduces TReB as a comprehensive benchmark with 26 subtasks to evaluate table reasoning.
- Provides an open-sourced dataset and an evaluation framework with multiple inference modes to benchmark LLMs.

## Evidence extracts
- "In this paper, we fill in this gap, presenting a comprehensive table rea- soning evolution benchmark, TReB, which mea- sures both shallow table understanding abilities and deep table reasoning abilities, a total of 26 sub-tasks." (p. 1)
- "Tosumup,ourworkmakesthreekeycontributions to the LLMandtableminingcommunities: (1)anopen-sourced, high-quality dataset with comprehensive task settings1; (2) a robust evaluation framework integrating diverse inference modes ; and (3) detailed benchmarking results for state-of- the-art LLMs." (p. 2)

## Classification
Class name: Data, Benchmarks & Measurement
Class code: 4

$$
\boxed{4}
$$
