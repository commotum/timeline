                               MMMU-Pro:AMoreRobustMulti-disciplineMultimodal
                                                        Understanding Benchmark
                                                              *                    *                    *
                                                 Xiang Yue , Tianyu Zheng , Yuansheng Ni ,
                              YuboWang,KaiZhang,ShengbangTong,YuxuanSun,BotaoYu,GeZhang,
                                                HuanSun,YuSu,WenhuChen,GrahamNeubig
                                           https://mmmu-benchmark.github.io/#leaderboard
                                          Abstract                                 This question has profound implications for the
                                                                                development and deployment of AI systems in real-
                       This paper introduces MMMU-Pro, a robust                 world applications. If models rely on superficial
                       version of the Massive Multi-discipline Multi-           cuesratherthantruemultimodalunderstanding(Du
                       modalUnderstandingandReasoning(MMMU)                     et al., 2023; Yuksekgonul et al., 2023), we risk over-
                       benchmark. MMMU-Pro rigorously assesses                  estimating their capabilities and potentially deploy-
                       multimodalmodels’trueunderstandingandrea-
                       soning capabilities through a three-step process         ing systems that fail in unpredictable ways when
                       based on MMMU: (1) filtering out questions               faced with novel scenarios (Wu and Xie, 2024;
                       answerable by text-only models, (2) augment-             Tongetal., 2024b).
                       ing candidate options, and (3) introducing a                To address this concern and push the bound-
                       vision-only input setting where questions are            aries of multimodal AI evaluation, we introduce
                       embedded within images. This setting chal-               MMMU-Pro, a more robust and challenging ver-
                       lenges AI to truly “see" and “read" simulta-
                       neously, testing a core human cognitive skill            sion of the MMMU benchmark. MMMU-Pro is
                       of seamlessly integrating visual and textual in-         designed to more accurately and rigorously assess
                       formation. Results show that model perfor-               a model’s true multimodal understanding and rea-
                       mance is substantially lower on MMMU-Pro                 soningcapabilities across a wide range of academic
                       than on MMMU,rangingfrom16.8%to26.9%                     disciplines. The development of MMMU-Pro is
                       across models. We explore the impact of OCR              motivated by key observations, including the text-
                       prompts and Chain of Thought (CoT) reason-               only solvability of some benchmark questions, lim-
                       ing, finding that OCR prompts have minimal
                       effect while CoT generally improves perfor-              ited option space in multiple-choice formats (Wang
                       mance. MMMU-Proprovidesamorerigorous                     et al., 2024), and the need to challenge models’
                       evaluation tool, closely mimicking real-world            ability to jointly understand different modalities in
                       scenarios and offering valuable directions for           a more integrated way.
                       future multimodal research.                                 MMMU-Proemploysarigorousthree-stepcon-
                   1 Introduction                                               struction process (as shown in Figure 1) that builds
                                                                                upon MMMU(Yueetal.,2024): (1) filtering out
                   Recent advances in multimodal large language                 questions answerable by text-only language mod-
                   models (MLLMs)haveledtoprogress in tackling                  els, (2) augmenting candidate options to reduce
                   complex reasoning tasks that combine textual and             the effectiveness of guessing based on the options,
                   visual information (Yin et al., 2023a; Jin et al.,           and (3) introducing a vision-only input setting (as
                   2024). Models like GPT-4o (OpenAI, 2024b) have               shown in Figure 4) where models are presented
                   achieved impressive results, e.g., on the Massive            with questions embedded in a screenshot or photo.
                   Multi-discipline Multimodal Understanding and                   Theintroduction of the vision-only input setting
                   Reasoning (MMMU)benchmark(Yueetal.,2024),                    is particularly crucial, as it tests a fundamental hu-
                   reaching an accuracy of 69.1% on college-level               mancognitive ability: the seamless integration and
                   questions that integrate text and images.                    switching between visual and textual information.
                      While these achievements are significant, they            This setting challenges models to develop the ca-
                   raise a critical question: Do the current bench-             pability to truly “see” and “read” simultaneously,
                   mark results truly reflect a deep, multifaceted un-          mirroring how humans effortlessly process com-
                   derstandingofdiversesubjects,orarethesemodels                plex scenes where text and images are intertwined.
                   exploiting subtle shortcuts and statistical patterns         This ability is crucial for tasks ranging from inter-
                   to arrive at correct answers without genuine com-            preting scientific diagrams (Li et al., 2024d) to nav-
                   prehension and reasoning?                                    igating graphical user interfaces (Liu et al., 2024b;
                      *Equal contributions. Contact: xyue2@andrew.cmu.edu       Zheng et al., 2024; Koh et al., 2024). Moreover,
                                                                           15134
              Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15134–15186
                                           July 27 - August 1, 2025 ©2025 Association for Computational Linguistics
