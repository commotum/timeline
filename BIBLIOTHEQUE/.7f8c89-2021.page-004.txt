                190                                                                                  M.-H. Guo, J.-X. Cai, Z.-N. Liu, et al.
                                                                                  N×de
                  Various other methods also employ attention and               R        is ﬁrst learned via the Input Embedding
               Transformer. Yan et al. [27] proposed PointASNL                  module.      The point-wise d -dimensional feature
                                                                                                                  o
                                                                                                          N×do
                to deal with noise in point cloud processing, using             representation F ∈ R             output by PCT is then
                                                                                                    o
                a self-attention mechanism to update features for               formed by concatenating the attention output of
                local groups of points. Hertz et al. [28] proposed              each attention layer through the feature dimension,
                PointGMM for shape interpolation with both multi-               followed by a linear transformation:
                layer perceptron (MLP) splits and attentional splits.                      F =AT1(F )
                  Unlike the above methods, our PCT is based on                              1            e
                                                                                            Fi = ATi(Fi−1),       i = 2,3,4              (1)
               Transformer rather than using self-attention as an                          F =concat(F ,F ,F ,F )·W
                auxiliary module. While a framework by Wang and                              o              1    2   3   4      o
                                                                                            i
                Solomon [29] uses Transformer to optimize point cloud           where AT represents the ith attention layer, each
                registration, our PCT is a more general framework               having the same output dimension as its input, and
                                                                                W is the weights of the linear layer.               Various
               which can be used for various point cloud tasks.                    o
                                                                                implementations of input embedding and attention
                3    Transformer             for       point        cloud       will be explained later.
                     representation                                                To extract an eﬀective global feature vector
                                                                                Fg representing the point cloud, we choose to
                In this section, we ﬁrst show how the point cloud               concatenate the outputs from two pooling operators:
                representation learned by our PCT can be applied                a max-pooling (MP) and an average-pooling (AP) on
                to various tasks of point cloud processing, including           the learned point-wise feature representation [26].
                point cloud classiﬁcation, part segmentation, and                  Classiﬁcation.        The details of classiﬁcation
                normal estimation. Thereafter, we detail the design             network using PCT is shown in Fig. 2. To classify
                of PCT. We ﬁrst introduce a naive version of PCT by             a point cloud P into Nc object categories (e.g.,
                directly applying the original Transformer [6] to point         desk, table, chair), we feed the global feature
                clouds. We then explain full PCT with its special               Fg to the classiﬁcation decoder, which comprises
                attention mechanism, and neighbor aggregation to                two cascaded feed-forward neural networks LBRs
                provide enhanced local information.                             (combining Linear, BatchNorm (BN), and ReLU
                3.1    Point cloud processing with PCT                          layers) each with a dropout probability of 0.5,
                                                                                ﬁnalized by a Linear layer to predict the ﬁnal
                                                                                                              N
                Encoder.       The overall architecture of PCT is               classiﬁcation scores C ∈ R c. The class label of the
                presented in Fig. 2. PCT aims to transform (encode)             point cloud is determined as the class with maximal
                the input points into a new higher dimensional feature          score.
                space, which can characterize the semantic aﬃnities                Segmentation. For the task of segmenting the
                between points as a basis for various point cloud               point cloud into N parts (e.g., table top, table legs; a
                                                                                                     s
                processing tasks.     The encoder of PCT starts by              part need not be contiguous), we must predict a part
                embedding the input coordinates into a new feature              label for each point, we ﬁrst concatenate the global
                space.   The embedded features are later fed into               feature Fg with the point-wise features in Fo. To
               4 stacked attention module to learn a semantically               learn a common model for various kinds of objects,
                rich and discriminative representation for each point,          we also encode the one-hot object category vector
                followed by a linear layer to generate the output               as a 64-dimensional feature and concatenate it with
                feature. Overall, the encoder of PCT shares almost              the global feature, following most other point cloud
                the same philosophy of design as the original                   segmentation networks [21]. As shown in Fig. 2, the
               Transformer, except that the positional embedding is             architecture of the segmentation network decoder
                discarded, since the point’s coordinates already                is almost the same as that for the classiﬁcation
                contain this information. We refer the reader to                network, except that dropout is only performed on
                Ref. [6] for details of the original NLP Transformer.           the ﬁrst LBR. We then predict the ﬁnal point-wise
                                                                       N×d                                     N×N
                  Formally, given an input point cloud P ∈ R                    segmentation scores S ∈ R           s for the input point
               with N points each having a d-dimensional feature                cloud: Finally, the part label of a point is also
                description, a d -dimensional embedded feature F ∈              determined as the one with maximal score.
                                 e                                      e
