
[[Page 1]]
Transformer-XL: Attentive Language Models
BeyondaFixed-LengthContext
⇤12
⇤12
ZihangDai, Zhilin Yang, Yiming Yang , Jaime Carbonell ,
, Zhilin Yang, Yiming Yang , Jaime Carbonell ,
ZihangDai
, Zhilin Yang
, Yiming Yang , Jaime Carbonell ,
2
QuocV.Le ,RuslanSalakhutdinov
1Carnegie Mellon University, 2Google Brain
{dzihang,zhiliny,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com
Abstract
Term Memory (LSTM) networks (Hochreiter and
Schmidhuber, 1997), have been a standard solu-
Transformers have a potential of learning
tion to language modeling and obtained strong
longer-term dependency, but are limited by a
results on multiple benchmarks.
ﬁxed-length context in the setting of language
wide adaption, RNNs are difﬁcult to optimize
modeling.
We propose a novel neural ar-
chitecture Transformer-XL that enables learn-
due to gradient vanishing and explosion (Hochre-
ing dependency beyond a ﬁxed length with-
iter et al., 2001), and the introduction of gat-
out disrupting temporal coherence.
It con-
ing in LSTMs and the gradient clipping tech-
sists of a segment-level recurrence mechanism
nique (Graves, 2013) might not be sufﬁcient to
and a novel positional encoding scheme. Our
fully address this issue.
methodnotonlyenablescapturinglonger-term
work has found that LSTM language models use
dependency, but also resolves the context frag-
200 context words on average (Khandelwal et al.,
mentation problem. As a result, Transformer-
2018), indicating room for further improvement.
XLlearnsdependencythat is 80% longer than
RNNs and 450% longer than vanilla Trans-
On the other hand, the direct connections be-
formers, achieves better performance on both
tween long-distance word pairs baked in atten-
short and long sequences, and is up to 1,800+
tion mechanisms might ease optimization and en-
times faster than vanilla Transformers during
able the learning of long-term dependency (Bah-
evaluation. Notably, we improve the state-of-
danau et al., 2014; Vaswani et al., 2017).
the-art results of bpc/perplexity to 0.99 on en-
cently, Al-Rfou et al. (2018) designed a set of aux-
wiki8, 1.08 on text8, 18.3 on WikiText-103,
iliary losses to train deep Transformer networks
21.8 on One Billion Word, and 54.5 on Penn
Treebank (without ﬁnetuning). When trained
for character-level language modeling, which out-
only on WikiText-103, Transformer-XL man-
perform LSTMs by a large margin. Despite the
ages to generate reasonably coherent, novel
success, the LM training in Al-Rfou et al. (2018)
text articles with thousands of tokens.
Our
is performed on separated ﬁxed-length segments
code, pretrained models, and hyperparameters
of a few hundred characters, without any informa-
are available in both Tensorﬂow and PyTorch1.
tion ﬂow across segments. As a consequence of
1
Introduction
the ﬁxed context length, the model cannot capture
any longer-term dependency beyond the prede-
Language modeling is among the important prob-
ﬁned context length. In addition, the ﬁxed-length
lemsthatrequire modeling long-term dependency,
segments are created by selecting a consecutive
with successful applications such as unsupervised
chunk of symbols without respecting the sentence
pretraining (Dai and Le, 2015; Peters et al., 2018;
or any other semantic boundary. Hence, the model
Radford et al., 2018; Devlin et al., 2018). How-
lacks necessary contextual information needed to
ever, it has been a challenge to equip neural
well predict the ﬁrst few symbols, leading to inef-
networks with the capability to model long-term
ﬁcient optimization and inferior performance. We
dependency in sequential data.
Recurrent neu-
refer to this problem as context fragmentation.
ral networks (RNNs), in particular Long Short-
To address the aforementioned limitations of
⇤Equal contribution. Order determined by swapping the
ﬁxed-length contexts, we propose a new architec-
one in Yang et al. (2017).
1https://github.com/kimiyoung/
ture called Transformer-XL (meaning extra long).
transformer-xl
We introduce the notion of recurrence into our
2978
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988
c
Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics
1
1
1
Despite the
Empirically, previous
Re-

[[Page 2]]
deep self-attention network. In particular, instead
of computing the hidden states from scratch for
each new segment, we reuse the hidden states ob-
tained in previous segments. The reused hidden
states serve as memory for the current segment,
which builds up a recurrent connection between
the segments. As a result, modeling very long-
term dependency becomes possible because in-
formation can be propagated through the recur-
rent connections.
Meanwhile, passing informa-
tion from the previous segment can also resolve
the problem of context fragmentation. More im-
portantly, we show the necessity of using relative
positional encodings rather than absolute ones, in
order to enable state reuse without causing tem-
poral confusion. Hence, as an additional techni-
cal contribution, we introduce a simple but more
effective relative positional encoding formulation
that generalizestoattentionlengthslongerthanthe
one observed during training.
Transformer-XL obtained strong results on ﬁve
datasets, varying from word-level to character-
level language modeling. Transformer-XL is also
able to generate relatively coherent long text arti-
cles with thousands of tokens (see Appendix E),
trained on only 100M tokens.
Our main technical contributions include intro-
ducing the notion of recurrence in a purely self-
attentive model and deriving a novel positional en-
codingscheme. Thesetwotechniquesformacom-
plete set of solutions, as any one of them alone
does not address the issue of ﬁxed-length con-
texts.
Transformer-XL is the ﬁrst self-attention
model that achieves substantially better results
than RNNsonbothcharacter-level and word-level
language modeling.
2
Related Work
In the last few years, the ﬁeld of language mod-
eling has witnessed many signiﬁcant advances,
including but not limited to devising novel ar-
chitectures to better encode the context (Bengio
et al., 2003; Mikolov et al., 2010; Merity et al.,
2016; Al-Rfou et al., 2018), improving regulariza-
tion andoptimizationalgorithms(GalandGhahra-
mani, 2016) , speeding up the Softmax computa-
tion(Graveetal.,2016a),andenrichingtheoutput
distribution family (Yang et al., 2017).
To capture the long-range context in language
modeling, a line of work directly feeds a repre-
sentation of the wider context into the network
as an additional input.
Existing works range
from ones where context representations are man-
ually deﬁned (Mikolov and Zweig, 2012; Ji et al.,
2015; Wang and Cho, 2015) to others that rely on
document-level topics learned from data (Dieng
et al., 2016; Wang et al., 2017).
More broadly, in generic sequence modeling,
how to capture long-term dependency has been a
long-standing research problem. From this per-
spective, since the ubiquitous adaption of LSTM,
many efforts have been spent on relieving the
vanishing gradient problem, including better ini-
tialization (Le et al., 2015), additional loss sig-
nal (Trinh et al., 2018), augmented memory struc-
ture (Ke et al., 2018) and others that modify the in-
ternal architecture of RNNs to ease the optimiza-
tion (Wu et al., 2016; Li et al., 2018). Different
from them, our work is based on the Transformer
architecture and shows that language modeling as
a real-world task beneﬁts from the ability to learn
longer-term dependency.
3
Model
Given a corpus of tokens x =(x ,...,x ), the
1
T
task of language modeling is to estimate the joint
probability P(x), which is often auto-regressively
Q
factorized as P(x)= tP(xt | x<t). With the
factorization, the problem reduces to estimating
each conditional factor. In this work, we stick to
the standard neural approach to modeling the con-
ditional probability. Speciﬁcally, a trainable neu-
ral network is used to encode the context x<t into
a ﬁxed size hidden state, which is multiplied with
the wordembeddingstoobtainthelogits. Thelog-
its are then fed into the Softmax function, yielding
a categorical probability distribution over the next
token.
3.1
Vanilla Transformer Language Models
In order to apply Transformer or self-attention to
language modeling, the central problem is how to
train a Transformer to effectively encode an arbi-
trarily long context into a ﬁxed size representation.
Given inﬁnite memory and computation, a sim-
ple solution would be to process the entire con-
text sequence using an unconditional Transformer
decoder, similar to a feed-forward neural network.
However,thisisusuallyinfeasiblewiththelimited
resource in practice.
Onefeasible but crude approximation is to split
the entire corpus into shorter segments of man-
2979

[[Page 3]]
x
x
x
x
x
x
x
x
12345678123456123456123456
2345678123456123456123456
345678123456123456123456
45678123456123456123456
5678123456123456123456
678123456123456123456
78123456123456123456
8123456123456123456
x
123456123456123456
x
23456123456123456
x
3456123456123456
x
456123456123456
x
56123456123456
x
6123456123456
1
2
3
4
5
6
7
8
1
2
3
4
5
6
Segment 1
Segment 2
Limited Context
(a) Train phase.
Figure 1: Illustration of the vanilla model with a segment length 4.
ageable sizes, and only train the model within
each segment, ignoring all contextual information
from previous segments. This is the idea adopted
by Al-Rfou et al. (2018). We call it the vanilla
model and visualize it in Fig.
1a.
Under this
training paradigm, information never ﬂows across
segments in either the forward or backward pass.
There are two critical limitations of using a ﬁxed-
length context. First, the largest possible depen-
dency length is upper bounded by the segment
length, which is a few hundred on character-level
language modeling (Al-Rfou et al., 2018). There-
fore, although the self-attention mechanism is less
affected by the vanishing gradient problem com-
pared to RNNs, the vanilla model is not able to
fully exploit this optimization advantage. Second,
though it is possible to use padding to respect the
sentence or other semantic boundaries, in practice
it has been standard practice to simply chunk long
text into ﬁxed-length segments due to improved
efﬁciency (Peters et al., 2018; Devlin et al., 2018;
Al-Rfou et al., 2018). However, simply chunking
a sequence into ﬁxed-length segments will lead to
the context fragmentation problem as discussed in
Section 1.
During evaluation, at each step, the vanilla
modelalsoconsumesasegmentofthesamelength
as in training, but only makes one prediction at the
last position. Then, at the next step, the segment
is shifted to the right by only one position, and the
newsegmenthastobeprocessedall from scratch.
As shown in Fig. 1b, this procedure ensures that
each prediction utilizes the longest possible con-
text exposedduringtraining,andalsorelievescon-
text fragmentation issue encountered in training.
However, this evaluation procedure is extremely
expensive. We will show that our proposed archi-
tecture is able to substantially improve the evalua-
tion speed.
2980
x
123456123456
x
23456123456
x
3456123456
x
456123456
x
56123456
x
6123456
x
123456
x
23456
x
3456
x
456
x
56
x
1
2
3
4
5
6
1
2
3
4
5
6
Limited Context
Limited Context
(b) Evaluation phase.
3.2
Segment-Level Recurrence with State
Reuse
To address the limitations of using a ﬁxed-length
context, we propose to introduce a recurrence
mechanism to the Transformer architecture. Dur-
ing training, the hidden state sequence computed
for the previous segment is ﬁxed and cached to
be reused as an extended context when the model
processes the next new segment, as shown in Fig.
2a. Although the gradient still remains within a
segment, this additional input allows the network
to exploit information in the history, leading to an
ability of modeling longer-term dependency and
avoiding context fragmentation. Formally, let the
two consecutive segments of length L be s⌧ =
[x⌧,1,···,x
] and s⌧+1 =[x⌧+1,1,···,x
]
⌧,L⌧+1,L
⌧+1,L
respectively. Denoting the n-th layer hidden state
sequence produced for the ⌧-th segment s⌧ by
hn 2 RL⇥d, where d is the hidden dimension.
⌧
Then, the n-th layer hidden state for segment s⌧+1
is produced (schematically) as follows,
⇥
n�1
n�1⇤
en�1
h= SG(h) � h,
= SG(h) � h,
h
= SG(h
) � h
,
⌧⌧+1
⌧+1
⌧+1
n
n
n
n�1
> en�1
> en�1
>
q⌧+1,k⌧+1,v⌧+1 = h⌧+1Wq ,h⌧+1Wk,h⌧+1Wv ,
hn
=Transformer-Layer(qn
, kn
, vn
).
⌧+1
⌧+1
⌧+1
⌧+1
where the function SG(·) stands for stop-gradient,
the notation [hu � hv] indicates the concatenation
of two hidden sequences along the length dimen-
sion, and W· denotes model parameters. Com-
pared to the standard Transformer, the critical dif-
ference lies in that the key kn
and value vn
⌧+1
⌧+1
en�1
are conditioned on the extended context h⌧+1 and
hence hn�1 cached from the previous segment.
⌧
Weemphasize this particular design by the green
paths in Fig. 2a.
With this recurrence mechanism applied to ev-
ery two consecutive segments of a corpus, it es-
sentially creates a segment-level recurrence in the
hidden states. As a result, the effective context be-
ing utilized can go way beyond just two segments.
However,noticethat the recurrent dependency be-
tween hn
and hn�1 shifts one layer downwards
⌧+1
⌧

[[Page 4]]
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
12345678123456789101112123456789101112
2345678123456789101112123456789101112
345678123456789101112123456789101112
45678123456789101112123456789101112
5678123456789101112123456789101112
678123456789101112123456789101112
78123456789101112123456789101112
8123456789101112123456789101112
123456789101112123456789101112
23456789101112123456789101112
3456789101112123456789101112
456789101112123456789101112
56789101112123456789101112
6789101112123456789101112
789101112123456789101112
1
2
3
4
5
6
7
8
1
2
3
4
5
6
7
Fixed (No Grad)
New Segment
Fixed (No Grad)
(a) Training phase.
Figure 2: Illustration of the Transformer-XL model with a segment length 4.
per-segment, which differs from the same-layer
recurrence in conventional RNN-LMs.
Conse-
quently, the largest possible dependency length
grows linearly w.r.t. the number of layers as well
as the segment length, i.e., O(N ⇥ L), as vi-
sualized by the shaded area in Fig.
2b.
This
is analogous to truncated BPTT (Mikolov et al.,
2010), a technique developed for training RNN-
LMs. However, different from truncated BPTT,
our method caches a sequence of hidden states in-
stead of the last one, and should be applied to-
gether with the relative positional encoding tech-
nique described in Section 3.3.
Besides achieving extra long context and re-
solving fragmentation, another beneﬁt that comes
with the recurrence scheme is signiﬁcantly faster
evaluation.
Speciﬁcally, during evaluation, the
representations from the previous segments can
be reused instead of being computed from scratch
as in the case of the vanilla model. In our ex-
periments on enwiki8, Transformer-XL is up to
1,800+ times faster than the vanilla model during
evaluation (see Section 4).
Finally, notice that the recurrence scheme does
not need to be restricted to only the previous seg-
ment. In theory, we can cache as many previous
segments as the GPU memory allows, and reuse
all of them as the extra context when processing
the current segment. Thus, we can cache a prede-
ﬁned length-M old hidden states spanning (pos-
sibly) multiple segments, and refer to them as the
memorymn 2RM⇥d,duetoaclearconnectionto
⌧cally into the initial embedding, one can inject the
⌧
the memory augmented neural networks (Graves
et al., 2014; Weston et al., 2014). In our experi-
ments, we set M equal to the segment length dur-
ing training, and increase it by multiple times dur-
ing evaluation.
3.3
Relative Positional Encodings
While we found the idea presented in the pre-
vious subsection very appealing, there is a cru-
cial technical challenge we haven’t solved in or-
2981
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
89101112123456789101112
9101112123456789101112
101112123456789101112
1112123456789101112
12123456789101112
123456789101112
23456789101112
3456789101112
456789101112
56789101112
6789101112
789101112
89101112
9101112
101112
1112
8
9
10
11
12
1
2
3
4
5
6
7
8
9
10
11
12
New Segment
Extended Context
(b) Evaluation phase.
der to reuse the hidden states. That is, how can
wekeepthepositional information coherent when
we reuse the states? Recall that, in the standard
Transformer, the information of sequence order is
provided by a set of positional encodings, denoted
as U 2 RLmax⇥d, where the i-th row Ui corre-
sponds to the i-th absolute position within a seg-
ment and Lmax prescribes the maximum possible
length to be modeled. Then, the actual input to the
Transformer is the element-wise addition of the
wordembeddingsandthepositional encodings. If
we simply adapt this positional encoding to our
recurrence mechanism, the hidden state sequence
would be computed schematically by
h⌧+1 = f(h⌧,Es
+U1:L)
⌧+1
h⌧ = f(h⌧�1,Es⌧ +U1:L),
where E
2 RL⇥d is the word embedding se-
s⌧
quence of s⌧, and f represents a transformation
function. Notice that, both Es⌧ and Es⌧+1 are as-
sociated with the same positional encoding U1:L.
As a result, the model has no information to dis-
tinguish the positional difference between x⌧,j and
x⌧+1,j for any j =1,...,L, resulting in a sheer
performance loss.
In order to avoid this failure mode, the funda-
mental idea is to only encode the relative posi-
tional information in the hidden states. Concep-
tually, the positional encoding gives the model a
temporal clue or “bias” about how information
should be gathered, i.e., where to attend. For the
same purpose, instead of incorporating bias stati-
cally into the initial embedding, one can inject the
same information into the attention score of each
layer. More importantly, it is more intuitive and
generalizable to deﬁne the temporal bias in a rela-
tive manner. Forinstance, whenaqueryvectorq
⌧,i
attends on the key vectors k⌧,i, it does not need
to know the absolute position of each key vector
to identify the temporal order of the segment. In-
stead, it sufﬁces to know the relative distance be-
tweeneachkeyvectork⌧,j anditselfq⌧,i, i.e. i�j.
Practically, one can create a set of relative posi-

[[Page 5]]
tional encodings R 2 RLmax⇥d, where the i-th row
Ri indicates a relative distance of i between two
positions. By injecting the relative distance dy-
namicallyintotheattentionscore,thequeryvector
can easily distinguish the representations of x⌧,j
and x⌧+1,j from their different distances, making
the state reuse mechanism feasible. Meanwhile,
wewon’tloseanytemporalinformation,astheab-
solute position can be recovered recursively from
relative distances.
Previously, the idea of relative positional encod-
ings has been explored in the context of machine
translation (Shaw et al., 2018) and music gener-
ation (Huang et al., 2018). Here, we offer a dif-
ferent derivation, arriving at a new form of rel-
ative positional encodings, which not only has a
one-to-one correspondence to its absolute coun-
terpart but also enjoys much better generalization
empirically (see Section 4). Firstly, in the standard
Transformer (Vaswani et al., 2017), the attention
score between query qi and key vector kj within
the same segment can be decomposed as
Aabs = E>W>WkEx +E>W>WkUj
i,jxiqjxiq
i,j
xi
q
xi
q
j
|
{z
}
|
{z
}
(a)(b)arrive at the Transformer-XL architecture.For
(b)arrive at the Transformer-XL architecture.For
(a)
(b)
+U>W>WkEx +U>W>WkUj.
i
q
i
q
jiq
|
{z
}
|
{z
}
(d)procedure for a N-layer Transformer-XL with a
(d)
(c)(d)procedure for a N-layer Transformer-XL with a
(c)
Following the idea of only relying on rela-single attention head here. For n =1,...,N:
Following the idea of only relying on rela-single attention head here. For n =1,...,N:
Following the idea of only relying on rela-single attention head here. For n =1,...,N:
Following the idea of only relying on rela-single attention head here. For n =1,...,N:
Following the idea of only relying on rela-
tive positional information, we propose to re-
parameterize the four terms as follows
rel
>
>
>
>
Ai,j = ExiWq Wk,EExj +ExiWq Wk,RRi�jnn > nn >n
Ai,j = ExiWq Wk,EExj +ExiWq Wk,RRi�jnn > nn >n
Ai,j = ExiWq Wk,EExj +ExiWq Wk,RRi�jnn > nn >n
Ai,j = ExiWq Wk,EExj +ExiWq Wk,RRi�jnn > nn >n
Ai,j = ExiWq Wk,EExj +ExiWq Wk,RRi�j
|
{z
}
|
{z
}
(a)(b)
+u>W E +v>W R .k,R
+u>W E +v>W R .
k,E xjk,Ri�jnnn
k,E xjk,Ri�jnnn
k,E xjk,Ri�jnnn
k,E xjk,Ri�jnnn
k,E xj
k,R
i�j
|
{z
}
|
{z
}
(d)on =LayerNorm(Linear(an)+hn�1)
(d)
(c)(d)on =LayerNorm(Linear(an)+hn�1)
(c)
• The ﬁrst change we make is to replace all ap-nn
• The ﬁrst change we make is to replace all ap-nn
• The ﬁrst change we make is to replace all ap-nn
• The ﬁrst change we make is to replace all ap-nn
• The ﬁrst change we make is to replace all ap-
pearances of the absolute positional embedding
Uj for computing key vectors in term (b) and
(d) with its relative counterpart Ri�j. This es-
sentially reﬂects the prior that only the relative
distance matters for where to attend. Note that
Risasinusoidencodingmatrix(Vaswanietal.,
2017) without learnable parameters.
• Secondly, we introduce a trainable parameter
u 2 Rd to replace the query U>W> in term
qreduces the cost to be linear w.r.t. the sequence
i
(c). In this case, since the query vector is the
samefor all query positions, it suggests that the
attentive bias towards different words should re-
main the same regardless of the query position.
With a similar reasoning, a trainable parameter
v 2 Rd is added to substitute U>W> in term
q
i
(d).
2982
• Finally, we deliberately separate the two weight
matrices Wk,E and Wk,R for producing the
content-based key vectors and location-based
key vectors respectively.
Under the new parameterization, each term has
an intuitive meaning: term (a) represents content-
based addressing, term (b) captures a content-
dependent positional bias, term (c) governs a
global content bias, and (d) encodes a global po-
sitional bias.
In comparison, the formulation in Shaw et al.
(2018) only has terms (a) and (b), dropping the
two bias terms (c) and (d). Moreover, Shaw et al.
(2018) merge the multiplication WkR into a sin-
ˆ
gle trainable matrix R, which abandons the induc-
tive bias built into the original sinusoid positional
encoding (Vaswani et al., 2017). In contrast, our
relative positional embedding R adapts the sinu-
soid formulation. As a beneﬁt of the inductive
bias, a model trained on a memory of some certain
length can automatically generalize to a memory
several times longer during evaluation.
Equipping the recurrence mechanism with our
proposedrelativepositionalembedding,weﬁnally
arrive at the Transformer-XL architecture.
For
completeness, we summarize the computational
procedure for a N-layer Transformer-XL with a
single attention head here. For n =1,...,N:
⇥
n�1
n�1⇤
en�1
h⌧= SG(m⌧ )�h⌧
= SG(m⌧ )�h⌧
n
n
n
n�1
n> en�1
n > en�1
n>
q⌧,k⌧,v⌧ =h⌧
Wq ,h⌧ Wk,E ,h⌧ Wv
n
n > n
n >
n
A⌧,i,j =q⌧,i k⌧,j +q⌧,i Wk,RRi�j
+u>k⌧,j +v>Wn Ri�j
k,R
n
n
n
a⌧ =Masked-Softmax(A⌧)v⌧
on =LayerNorm(Linear(an)+hn�1)
⌧
⌧
⌧
n
n
h =Positionwise-Feed-Forward(o )
⌧
⌧
0
:
with h⌧
= Es⌧ deﬁned as the word embed-
ding sequence. In addition, it is worth mention-
ing that a naive way to compute A requires com-
puting Wn Ri�j for all pairs (i,j), whose cost
k,R
is quadratic w.r.t.
the sequence length.
How-
ever, noticing that the value of i � j only ranges
from zero to the sequence length, we show a sim-
ple computation procedure in Appendix B, which
reduces the cost to be linear w.r.t. the sequence
length.
4
Experiments
4.1
MainResults
WeapplyTransformer-XL to a variety of datasets
on both word-level and character-level language

[[Page 6]]
Model
#Param PPL
Model
Grave et al. (2016b) - LSTM
-
48.7
Cooijmans et al. (2016) - BN-LSTM
Bai et al. (2018) - TCN
-
45.2
Chungetal. (2016) - LN HM-LSTM
Dauphin et al. (2016) - GCNN-8
-
44.9
Zilly et al. (2016) - RHN
Grave et al. (2016b) - Neural cache
-
40.8
Krause et al. (2016) - Large mLSTM
Dauphin et al. (2016) - GCNN-14
-
37.2
Al-Rfou et al. (2018) - 12L Transformer
Merity et al. (2018) - QRNN
151M
33.0
Al-Rfou et al. (2018) - 64L Transformer
Raeetal. (2018) - Hebbian + Cache
-
29.9
Ours - 24L Transformer-XL
Ours - Transformer-XL Standard
151M
24.0
Baevski and Auli (2018) - Adaptive Input⇧
247M
20.5
Table 3: Comparison with state-of-the-art results on
Ours - Transformer-XL Large
257M
18.3
text8.
Table 1: Comparison with state-of-the-art results on
WikiText-103. ⇧ indicates contemporary work.
Model
Shazeer et al. (2014) - Sparse Non-Negative
Chelba et al. (2013) - RNN-1024 + 9 Gram
Model
#Param bpc
Kuchaiev and Ginsburg (2017) - G-LSTM-2
Dauphin et al. (2016) - GCNN-14 bottleneck
Haetal. (2016) - LN HyperNetworks
27M
1.34
Jozefowicz et al. (2016) - LSTM
Chungetal. (2016) - LN HM-LSTM
35M
1.32
Jozefowicz et al. (2016) - LSTM + CNN
Zilly et al. (2016) - RHN
46M
1.27
Shazeer et al. (2017) - Low-Budget MoE
Mujika et al. (2017) - FS-LSTM-4
47M
1.25
Shazeer et al. (2017) - High-Budget MoE
Krause et al. (2016) - Large mLSTM
46M
1.24
Shazeer et al. (2018) - Mesh Tensorﬂow
Knol (2017) - cmix v13
-
1.23
Baevski and Auli (2018) - Adaptive Input⇧
Al-Rfou et al. (2018) - 12L Transformer
44M
1.11
Baevski and Auli (2018) - Adaptive Input⇧
Ours - 12L Transformer-XL
41M
1.06
Ours - Transformer-XL Base
Al-Rfou et al. (2018) - 64L Transformer
235M 1.06
Ours - Transformer-XL Large
Ours - 18L Transformer-XL
88M
1.03
Ours - 24L Transformer-XL
277M 0.99
Table4: Comparisonwithstate-of-the-artresultsonOne
Table2: Comparisonwithstate-of-the-artresultsonen-
Billion Word. ⇧ indicates contemporary work.
wik8.
performing the 12-layer vanilla Transformer from
modeling to have a comparison with state-of-the-
Al-Rfou et al. (2018) by 0.05, while both Trans-
art systems, including WikiText-103(Merityetal.,
former variants have a large margin over conven-
2016), enwik8 (LLC, 2009), text8 (LLC, 2009),
tional RNN-based models. Notably, our 12-layer
One Billion Word (Chelba et al., 2013), and Penn
architecture achieves the same result as the 64-
Treebank (Mikolov and Zweig, 2012).
layer network from Al-Rfou et al. (2018), using
WikiText-103isthelargestavailableword-level
only 17% of the parameter budget. In order to see
language modeling benchmark with long-term de-
whether better performances can be obtained by
pendency. It contains 103M training tokens from
increasing the model size, we train 18-layer and
28K articles, with an average length of 3.6K to-
24-layer Transformer-XLs with increased model
kens per article, which allows testing the abil-
sizes. With the attention length 784 during train-
ity of long-term dependency modeling. We set
ing and 3,800 during evaluation, we obtained a
the attention length to 384 during training and
new SoTA result and our method is the ﬁrst to
1600duringevaluation. Weadoptedadaptivesoft-
break through 1.0 on widely-studied character-
max and input representations (Baevski and Auli,
level benchmarks. Different from Al-Rfou et al.
2018; Grave et al., 2016a). As shown in Table 1,
(2018), Transformer-XL does not need any auxil-
Transformer-XLreducesthepreviousstate-of-the-
iary losses, and thus all beneﬁts are credited to a
art (SoTA) perplexity from 20.5 to 18.3, which
better architecture.
demonstrates the superiority of the Transformer-
Similar to but different from enwik8, text8 con-
XLarchitecture.
tains 100M processed Wikipedia characters cre-
Thedataset enwik8 contains 100M bytes of un-
ated by lowering case the text and removing any
processed Wikipedia text.
We compare our ar-
character other than the 26 letters a through z, and
chitecture with the previous results in Table 2.
space. Due to the similarity, we simply adapt the
Under the model size constraint, the 12-layer
best model and the same hyper-parameters on en-
Transformer-XL achieves a new SoTA result, out-
wik8totext8withoutfurthertuning. Thecompari-
2983
#Param bpc
-
1.36
35M
1.29
45M
1.27
45M
1.27
44M
1.18
235M 1.13
277M 1.08
#Param PPL
33B
52.9
20B
51.3
-
36.0
-
31.9
1.8B
30.6
1.04B
30.0
⇠5B 34.1
⇠5B 28.0
4.9B
24.0
0.46B
24.1
1.0B
23.7
0.46B
23.5
0.8B
21.8

[[Page 7]]
Model
#Param PPL
et al. (2018) are absolute. “Full” and “half” losses
refer to applying a cross entropy loss to all or the
Inan et al. (2016) - Tied Variational LSTM
24M
73.2
Zilly et al. (2016) - Variational RHN
23M
65.4
recent half positions in the segment. We found
ZophandLe(2016)-NASCell
25M
64.0
that absolute encodings only work well with half
Merity et al. (2017) - AWD-LSTM
24M
58.8
losses because half losses exclude positions with
Phametal. (2018) - Efﬁcient NAS
24M
58.6
Liu et al. (2018) - Differentiable NAS
23M
56.1
veryshortattentionlengthsduringtrainingforbet-
Yangetal. (2017) - AWD-LSTM-MoS
22M 55.97
ter generalization.
Table 6 shows that both the
Melis et al. (2018) - Dropout tuning
24M
55.3
recurrence mechanism and our encoding scheme
Ours - Transformer-XL
24M 54.52
are necessary to achieve the best performance, as
Merity et al. (2017) - AWD-LSTM+Finetune†
24M
57.3
well as generalizing to longer attention sequences
†
Yangetal. (2017) - MoS+Finetune
22M 54.44
during evaluation time. Although the backprop-
agation length during training is only 128, with
Table 5: Comparison with state-of-the-art results on
the two techniques the attention length can be in-
PennTreebank. † indicates using two-step ﬁnetuning.
creased to 640 at test time. In the standard setting
with151Mparameters,theperplexitydecreasesas
sonwithpreviousmethodsissummarizedinTable
the attention length increases.
3. Again, Transformer-XLachievesthenewSoTA
Since the recurrence mechanism costs addi-
result with a clear margin.
tional memory, we also compare Transformer-XL
One Billion Word does not preserve any long-
with baselines under the same GPU memory con-
term dependency because sentences have been
straints.
As shown in Table 10 in Appendix A,
shufﬂed. Consequently, this dataset mainly tests
despite using a shorter backpropagation length,
the ability of modeling only short-term depen-
Transformer-XLremainssuperiortothebaselines.
dency. The comparison between Transformer-XL
The second study targets at isolating the ef-
and the other methods is shown in Table 4. Al-
fects of resolving the context fragmentation prob-
thoughTransformer-XLismainlydesignedtobet-
lem from the beneﬁt of capturing longer context
ter capture longer-term dependency, it dramati-
length. In order to achieve this goal, we deliber-
cally improves the single-model SoTA from 23.7
ately choose a dataset that does not require long-
to 21.8.
Speciﬁcally, Transformer-XL signiﬁ-
term dependency, so that any improvement from
cantly outperforms a contemporary method using
establishing the recurrence can be attributed to
vanilla Transformers (Baevski and Auli, 2018),
solving the context fragmentation.
suggesting the advantage of Transformer-XL is
weperformthis controlled experiment on the One
generalizable to modeling short sequences.
Billion Word dataset, which can only beneﬁt from
We also report the results on word-level Penn
removing the context fragmentation.
Treebank in Table 5.
Similar to AWD-LSTM
a 20-layer Transformer-XL with ⇠0.3B parame-
(Merity et al., 2017), we apply variational dropout
ters for 400K steps. As shown in Table 7, using
and weight average to Transformer-XL. With
segment-level recurrence substantially improves
proper regularization, Transformer-XL achieves a
performance even when long-term dependency is
new SoTA result among models without two-step
not needed, which is consistent with our previous
ﬁnetuning. Penn Treebank has only 1M training
discussion that the recurrence mechanism resolves
tokens, which implies that Transformer-XL also
the context fragmentation problem. Moreover, our
generalizes well even on small datasets.
relative positional encodings is also superior to
Shawetal. (2018) on short sequences.
4.2
Ablation Study
Weconduct two sets of ablation studies to exam-
4.3
Relative Effective Context Length
ine the effects of two proposed techniques used in
Transformer-XL: the recurrence mechanism and
Khandelwal et al. (2018) proposed a method to
the new positional encoding scheme.
evaluate the Effective Context Length (ECL) of a
The ﬁrst study is performed on WikiText-103,
sequence model.
ECL is the longest length to
which requires modeling long-term dependency.
which increasing the context span would lead to
The results are reported in Table 6. Among the
a gain more than a threshold. However, ECL ig-
comparedencodingschemes,Shawetal.(2018)is
nores the fact that it is harder to get improve-
relative, while Vaswani et al. (2017) and Al-Rfou
ment when a model already achieves a lower per-
2984
Speciﬁcally,
We train

[[Page 8]]
Remark
Transformer-XL (128M)
-
-
-
-
-
-
-
Transformer (128M)
Transformer-XL (151M)
Table 6: Ablation study on WikiText-103. For the ﬁrst two blocks, we use a slightly smaller model (128M parame-
ters). † indicates that the corresponding row is reduced to the same setting as the Transformer network in (Al-Rfou
et al., 2018), except that two auxiliary losses are not implemented in our experiments. “PPL init” refers to using
the same length as training. “PPL best” indicates the perplexity obtained by using the optimal length. “Attn Len”
is the shortest possible attention length during evaluation to achieve the corresponding result (PPL best). Increas-
ing the attention length during evaluation improves performance only when our positional encoding is used. The
“Transformer-XL (151M)” setting uses a standard parameter budget as previous work (Merity et al., 2018), where
weobserveasimilar effect when increasing the attention length during evaluation.
Method
Ours
With Shaw et al. (2018) encodings
Without recurrence
Table 7: Ablation study on One Billion Word, a dataset
without long-term dependency.
Model
Transformer-XL 151M
QRNN
LSTM
Transformer-XL 128M
- use Shaw et al. (2018) encoding
- remove recurrence
Transformer
Table8: Relativeeffectivecontextlength(RECL)com-
parison. See text for the deﬁnition of RECL and r. The
ﬁrst three models and the last four models are com-
pared as two model groups when we calculate RECL
(RECLiscomputedonamodelgroupratherthanasin-
glemodel). Eachgrouphasthesameparameterbudget.
plexity using only a shorter context, and thus it
is not suitable for fair comparison among mul-
tiple models. We instead propose a new metric
called Relative Effective Context Length (RECL).
RECL is deﬁned on a model group instead of a
single model, and the gain of a long context is
measurebytherelativeimprovementoverthebest
short context model. As such, the model group
shares the same baseline to enable fair compari-
Recurrence
Encoding
Loss
PPLinit
PPLbest
Attn Len
3
Ours
Full
27.02
26.77
500
3
Shawetal. (2018)
Full
27.94
27.94
256
3
Ours
Half
28.69
28.33
460
7
Ours
Full
29.59
29.02
260
7
Ours
Half
30.10
30.10
120
7
Shawetal. (2018)
Full
29.75
29.75
120
7
Shawetal. (2018)
Half
30.50
30.50
120
7
Vaswani et al. (2017)
Half
30.97
30.97
120
†
7
Al-Rfou et al. (2018)
Half
31.16
31.16
120
23.09
640
3
Ours
Full
23.43
23.16
450
23.35
300
PPL
son. RECL also has a parameter r, which means
constraining the comparison on top-r hard exam-
25.2
25.7
ples. See AppedixCformoredetailsaboutRECL.
27.1
As shown in Table 8, Transformer-XL manages
to model dependency of 900 words long on av-
erage with r =0.1. The RECL of Transformer-
XL is 80% and 450% longer than recurrent net-
works and Transformer respectively. Both the re-
r =0.1 r =0.5 r =1.0
currence mechanism and our positional encodings
900
800
700
contribute to a longer RECL. This further substan-
500
400
300
tiates our argument that Transformer-XL is able to
400
300
200
modellonger-term dependency.
700
600
500
400
400
300
300
300
300
4.4
Generated Text
128
128
128
Trained only on WikiText-103 which is medium-
sized, Transformer-XL is already able to generate
relatively coherent articles with thousands of to-
kens without manual cherry picking, despite mi-
nor ﬂaws. Please refer to Appendix E for samples.
4.5
Evaluation Speed
Finally, we compare the evaluation speed of our
model with the vanilla Transformer model (Al-
Rfou et al., 2018). As shown in Table 9, due to
the state reuse scheme, Transformer-XL achieves
an up to 1,874 times speedup during evaluation.
5
Conclusions
Transformer-XL obtains strong perplexity results,
models longer-term dependency than RNNs and
Transformer, achieves substantial speedup during
2985

[[Page 9]]
Attn Len
HowmuchAl-Rfouetal.(2018)isslower
3,800
2,800
1,800
800
Table 9: Slowdown in terms of running time during
evaluation. Evaluation is based on per-token time on
one GPU.
evaluation, and is able to generate coherent text
articles.
We envision interesting applications of
Transformer-XL in the ﬁelds of text generation,
unsupervised feature learning, image and speech
modeling.
Acknowledgments
ZD and YY were supported in part by National
Science Foundation (NSF) under the grant IIS-
1546329 and by the DOE-Ofﬁce of Science un-
der the grant ASCR #KJ040201.
were supported in part by the Ofﬁce of Naval
Research grant N000141812861, the NSF grant
IIS1763562, the Nvidia fellowship, and the Siebel
scholarship.
References
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy
Guo, and Llion Jones. 2018. Character-level lan-
guage modeling with deeper self-attention.
preprint arXiv:1808.04444.
Alexei Baevski and Michael Auli. 2018. Adaptive in-
put representations for neural language modeling.
arXiv preprint arXiv:1809.10853.
DzmitryBahdanau,KyunghyunCho,andYoshuaBen-
gio. 2014.
Neural machine translation by jointly
learning to align and translate.
arXiv:1409.0473.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun.
2018. An empirical evaluation of generic convolu-
tional and recurrent networks for sequence model-
ing. arXiv preprint arXiv:1803.01271.
YoshuaBengio,RéjeanDucharme,PascalVincent,and
Christian Jauvin. 2003. A neural probabilistic lan-
guagemodel. Journalofmachinelearningresearch,
3(Feb):1137–1155.
CiprianChelba,TomasMikolov,MikeSchuster,QiGe,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
2016. Hierarchical multiscale recurrent neural net-
1,874x
works. arXiv preprint arXiv:1609.01704.
1,409x
773x
˘
TimCooijmans, Nicolas Ballas, César Laurent, Çaglar
363x
Gülçehre,
and Aaron Courville. 2016.
Re-
current batch normalization.
arXiv preprint
arXiv:1603.09025.
AndrewMDaiandQuocVLe.2015. Semi-supervised
sequence learning. In Advances in neural informa-
tion processing systems, pages 3079–3087.
Yann N Dauphin, Angela Fan, Michael Auli, and
David Grangier. 2016.
Language modeling with
gated convolutional networks.
arXiv preprint
arXiv:1612.08083.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
KristinaToutanova.2018. Bert: Pre-trainingofdeep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.
Adji B Dieng, Chong Wang, Jianfeng Gao, and John
Paisley. 2016.
Topicrnn: A recurrent neural net-
ZY and RS
work with long-range semantic dependency. arXiv
preprint arXiv:1611.01702.
Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in neural information
processing systems, pages 1019–1027.
Edouard Grave, Armand Joulin, Moustapha Cissé,
David Grangier, and Hervé Jégou. 2016a. Efﬁcient
softmax approximation for gpus.
arXiv preprint
arXiv:1609.04309.
arXiv
Edouard
Grave,
Armand Joulin,
and
Nicolas
Usunier. 2016b.
Improving neural language
models with a continuous cache.
arXiv preprint
arXiv:1612.04426.
Alex Graves. 2013.
Generating sequences with
recurrent
neural
networks.
arXiv
preprint
arXiv:1308.0850.
Alex Graves, Greg Wayne, and Ivo Danihelka.
arXiv preprint
2014.
Neural turing machines.
arXiv preprint
arXiv:1410.5401.
David Ha, Andrew Dai, and Quoc V Le. 2016. Hyper-
networks. arXiv preprint arXiv:1609.09106.
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jür-
gen Schmidhuber, et al. 2001. Gradient ﬂow in re-
current nets: the difﬁculty of learning long-term de-
pendencies.
Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory.
Neural computation,
9(8):1735–1780.
Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob
Uszkoreit, Noam Shazeer, Curtis Hawthorne, An-
drewMDai,MatthewDHoffman,andDouglasEck.
2986

[[Page 10]]
2018. An improved relative self-attention mecha-
Stephen Merity, Nitish Shirish Keskar, and Richard
nism for transformer with application to music gen-
Socher.2017. Regularizingandoptimizinglstmlan-
eration. arXiv preprint arXiv:1809.04281.
guage models. arXiv preprint arXiv:1708.02182.
Stephen Merity, Nitish Shirish Keskar, and Richard
HakanInan, Khashayar Khosravi, and Richard Socher.
Socher. 2018.
2016.
Tying word vectors and word classiﬁers:
modeling at multiple scales.
A loss framework for language modeling.
arXiv
arXiv:1803.08240.
preprint arXiv:1611.01462.
Stephen Merity, Caiming Xiong, James Bradbury, and
YangfengJi,TrevorCohn,LingpengKong,ChrisDyer,
Richard Socher. 2016.
and Jacob Eisenstein. 2015. Document context lan-
models. arXiv preprint arXiv:1609.07843.
guage models. arXiv preprint arXiv:1511.03962.
Tomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
ˇ
Cernocky, and Sanjeev Khudanpur. 2010. Recur-
`
Shazeer, and Yonghui Wu. 2016.
Exploring
rent neural network based language model.
the limits of language modeling.
arXiv preprint
Eleventh Annual Conference of the International
arXiv:1602.02410.
Speech Communication Association.
Nan Rosemary Ke, Anirudh Goyal ALIAS PARTH
Tomas Mikolov and Geoffrey Zweig. 2012. Context
GOYAL,
Olexa
Bilaniuk,
Jonathan
Binas,
dependentrecurrentneuralnetworklanguagemodel.
Michael C Mozer, Chris Pal, and Yoshua Ben-
SLT, 12(234-239):8.
gio. 2018. Sparse attentive backtracking: Temporal
credit assignment through reminding. In Advances
Asier Mujika, Florian Meier, and Angelika Steger.
in Neural Information Processing Systems, pages
2017. Fast-slow recurrent neural networks. In Ad-
7650–7661.
vances in Neural Information Processing Systems,
pages 5915–5924.
Urvashi Khandelwal, He He, Peng Qi, and Dan Ju-
rafsky. 2018. Sharp nearby, fuzzy far away: How
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
neural language models use context. arXiv preprint
Gardner, Christopher Clark, Kenton Lee, and Luke
arXiv:1805.04623.
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.
Bryon Knol. 2017.
cmix v13.
http://www.
Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V
byronknoll.com/cmix.html.
Le, and Jeff Dean. 2018. Efﬁcient neural architec-
ture search via parameter sharing.
Ben Krause, Liang Lu, Iain Murray, and Steve Renals.
arXiv:1802.03268.
2016. Multiplicative lstm for sequence modelling.
arXiv preprint arXiv:1609.07959.
AlecRadford,KarthikNarasimhan,TimSalimans,and
Ilya Sutskever. 2018.
Oleksii Kuchaiev and Boris Ginsburg. 2017. Factor-
standing by generative pre-training. URL https://s3-
ization tricks for lstm networks.
arXiv preprint
us-west-2.amazonaws.com/openai-assets/research-
arXiv:1703.10722.
covers/languageunsupervised/language
standing paper. pdf.
Quoc V Le, Navdeep Jaitly, and Geoffrey E Hin-
ton. 2015.
A simple way to initialize recurrent
Jack W Rae, Chris Dyer, Peter Dayan, and Tim-
networks of rectiﬁed linear units.
arXiv preprint
othy P Lillicrap. 2018.
arXiv:1504.00941.
ing with activation memorization.
arXiv:1803.10049.
Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo
Gao. 2018. Independently recurrent neural network
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
(indrnn): Building a longer and deeper rnn. In Pro-
2018. Self-attention with relative position represen-
ceedings of the IEEE Conference on Computer Vi-
tations. arXiv preprint arXiv:1803.02155.
sion and Pattern Recognition, pages 5457–5466.
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
Tran, Ashish Vaswani, Penporn Koanantakool, Peter
2018.
Darts:
Differentiable architecture search.
Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff
arXiv preprint arXiv:1806.09055.
Young,etal. 2018. Mesh-tensorﬂow: Deep learning
for supercomputers. In Advances in Neural Infor-
MultiMedia LLC. 2009.
Large text compression
mation Processing Systems, pages 10434–10443.
benchmark.
NoamShazeer, Azalia Mirhoseini, Krzysztof Maziarz,
ˇ
Gábor Melis, Charles Blundell, Tomáš Kocisky,Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
`
Gábor Melis, Charles Blundell, Tomáš Kocisky,
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
Karl Moritz Hermann, Chris Dyer, and Phil Blun-
Dean. 2017. Outrageously large neural networks:
som. 2018. Pushing the bounds of dropout. arXiv
The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1805.09208.
preprint arXiv:1701.06538.
2987
An analysis of neural language
arXiv preprint
Pointer sentinel mixture
In
arXiv preprint
Improving language under-
under-
Fast parametric learn-
arXiv preprint

[[Page 11]]
Noam Shazeer, Joris Pelemans, and Ciprian Chelba.
2014. Skip-gram language modeling using sparse
non-negative matrix probability estimation.
preprint arXiv:1412.1454.
Trieu H Trinh, Andrew M Dai, Thang Luong, and
Quoc V Le. 2018. Learning longer-term dependen-
cies in rnns with auxiliary losses.
arXiv:1803.00144.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.
Tian Wang and Kyunghyun Cho. 2015.
context language modelling.
arXiv:1511.03729.
Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen,
Jiaji Huang, Wei Ping, Sanjeev Satheesh, and
Lawrence Carin. 2017. Topic compositional neural
language model. arXiv preprint arXiv:1712.09783.
Jason Weston, Sumit Chopra, and Antoine Bor-
des. 2014.
Memory networks.
arXiv:1410.3916.
Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua
Bengio, and Ruslan R Salakhutdinov. 2016.
multiplicative integration with recurrent neural net-
works. In Advances in neural information process-
ing systems, pages 2856–2864.
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and
William WCohen.2017. Breakingthesoftmaxbot-
tleneck: A high-rank rnn language model. arXiv
preprint arXiv:1711.03953.
Julian
Georg Zilly,
Rupesh Kumar Srivastava,
Jan Koutník, and Jürgen Schmidhuber. 2016.
Recurrent highway networks.
arXiv:1607.03474.
Barret Zoph and Quoc V Le. 2016. Neural architecture
search with reinforcement learning. arXiv preprint
arXiv:1611.01578.
arXiv
arXiv preprint
Larger-
arXiv preprint
arXiv preprint
On
arXiv preprint
2988