             AFastLearningAlgorithmforDeepBeliefNets                    1535
             is important to notice that Pn depends on the current model parameters,
                                  n  θ
             and the way in which P changes as the parameters change is being ig-
                                  θ
             noredbycontrastivedivergencelearning.Thisproblemdoesnotarisewith
             P0 because the training data do not depend on the parameters. An empiri-
             cal investigation of the relationship between the maximum likelihood and
             thecontrastivedivergencelearningrulescanbefoundinCarreira-Perpinan
             andHinton(2005).
               Contrastive divergence learning in a restricted Boltzmann machine is
             efﬁcient enough to be practical (Mayraz & Hinton, 2001). Variations that
             usereal-valuedunitsanddifferentsamplingschemesaredescribedinTeh,
             Welling, Osindero, and Hinton (2003) and have been quite successful for
             modelingtheformationoftopographicmaps(Welling,Hinton,&Osindero,
             2003) for denoising natural images (Roth & Black, 2005) or images of bio-
             logical cells (Ning et al., 2005). Marks and Movellan (2001) describe a way
             of using contrastive divergence to perform factor analysis and Welling,
             Rosen-Zvi, and Hinton (2005) show that a network with logistic, binary
             visible units and linear, gaussian hidden units can be used for rapid doc-
             ument retrieval. However, it appears that the efﬁciency has been bought
             at a high price: When applied in the obvious way, contrastive divergence
             learning fails for deep, multilayer networks with different weights at each
             layer because these networks take far too long even to reach conditional
             equilibriumwithaclampeddatavector.Wenowshowthattheequivalence
             betweenRBMsandinﬁnitedirectednetswithtiedweightssuggestsanef-
             ﬁcientlearningalgorithmformultilayernetworksinwhichtheweightsare
             not tied.
             4 AGreedyLearningAlgorithmforTransformingRepresentations
             Anefﬁcientwaytolearnacomplicatedmodelistocombineasetofsimpler
             modelsthatarelearnedsequentially.Toforceeachmodelinthesequenceto
             learn something different from the previous models, the data are modiﬁed
             insomewayaftereachmodelhasbeenlearned.Inboosting(Freund,1995),
             each model in the sequence is trained on reweighted data that emphasize
             the cases that the preceding models got wrong. In one version of principal
             componentsanalysis,thevarianceinamodeleddirectionisremoved,thus
             forcingthenextmodeleddirectiontolieintheorthogonalsubspace(Sanger,
             1989). In projection pursuit (Friedman & Stuetzle, 1981), the data are trans-
             formedbynonlinearlydistortingonedirectioninthedataspacetoremove
             all nongaussianity in that direction. The idea behind our greedy algorithm
             is to allow each model in the sequence to receive a different representation
             of the data. The model performs a nonlinear transformation on its input
             vectors and produces as output the vectors that will be used as input for
             the next model in the sequence.
               Figure 5 shows a multilayer generative model in which the top two
             layers interact via undirected connections and all of the other connections
