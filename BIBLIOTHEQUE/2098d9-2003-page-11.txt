                                               ANEURALPROBABILISTIC LANGUAGEMODEL
                     processors. Then the K backward phases are initiated, to obtain the K partial gradient vectors ∂L
                          ∂L                                                                                          ∂a
                     and ∂x. After exchanging these gradient vectors among the processors, each processor can complete
                     the backward phase and update parameters. This method mainly saves time because of the savings
                     in network communication latency (the amount of data transferred is the same). It may lose in con-
                     vergence time if K is too large, for the same reason that batch gradient descent is generally much
                     slower than stochastic gradient descent (LeCun et al., 1998).
                     4. Experimental Results
                     Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041
                     words, from a large variety of English texts and books. The ﬁrst 800,000 words were used for
                     training, the following 200,000 for validation (model selection, weight decay, early stopping) and
                     the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation,
                     distinguishing between upper and lower case, and including the syntactical marks used to separate
                     texts and paragraphs). Rare words with frequency ≤ 3 were merged into a single symbol, reducing
                     the vocabulary size to |V| = 16,383.
                         Anexperiment was also run on text from the Associated Press (AP) News from 1995 and 1996.
                     The training set is a stream of about 14 million (13,994,528) words, the validation set is a stream
                     of about 1 million (963,138) words, and the test set is also a stream of about 1 million (963,071)
                     words. The original data has 148,721 different words (including punctuation), which was reduced
                     to |V| = 17964 by keeping only the most frequent words (and keeping punctuation), mapping upper
                     case to lower case, mapping numeric forms to special symbols, mapping rare words to a special
                     symbol and mapping proper nouns to another special symbol.
                         For training the neural networks, the initial learning rate was set to εo = 10−3 (after a few trials
                                                                                                              ε
                     with a tiny data set), and gradually decreased according to the following schedule: ε =   o  where
                                                                                                         t   1+rt
                     t represents the number of parameter updates done and r is a decrease factor that was heuristically
                     chosen to be r = 10−8.
                     4.1 N-GramModels
                     Theﬁrstbenchmark against which the neural network was compared is an interpolated or smoothed
                     trigram model (Jelinek and Mercer, 1980). Let q = l(freq(w       ,w    )) represents the discretized
                                                                      t         4  t−1   t−2
                     frequency of occurrence of the input context (w     ,w   ).  Then the conditional probability esti-
                     mates have the form of a conditional mixture:    t−1  t−2
                         ˆ
                        P(w |w    ,w    )=α (q)p +α (q)p (w)+α (q)p (w|w                 )+α (q )p (w |w      ,w    )
                            t  t−1   t−2      0  t  0    1  t  1   t     2  t  2   t  t−1     3  t   3  t  t−1   t−2
                     with conditional weights α (q ) ≥ 0,∑ α (q )=1. The base predictors are the following: p =
                                                 i  t        i i  t                                                 0
                     1/|V|, p (i) is a unigram (relative frequency of word i in the training set), p (i|j) is the bigram
                              1                                                                     2
                     (relative frequency of word i when the previous word is j), and p (i|j,k) is the trigram (relative
                                                                                         3
                     frequency of word i when the previous 2 words are j and k). The motivation is that when the
                     frequency of (w    ,w    ) is large, p is most reliable, whereas when it is lower, the lower-order
                                     t−1   t−2            3
                     statistics of p , p ,orevenp aremorereliable. Thereisadifferent setof mixture weights αforeach
                                  2   1          0
                     of the discrete values of q (which are context frequency bins). They can be easily estimated with
                                               t
                      4. We used l(x)=d−log((1+x)/T)e where freq(w   ,w  ) is the frequency of occurrence of the input context and
                         T is the size of the training corpus.    t−1  t−2
                                                                    1147
