                                                            20                                                                                          20                                                                                     20                                      
                                                                                                                                                                                                                         ResNet-20                                      residual-110
                                                                                                                                                                                                                         ResNet-32                                      residual-1202
                                                                                                                                                                                                                         ResNet-44
                                                                                                                                                                                                                         ResNet-56
                                                                                                                              56-layer                                                                                   ResNet-110
                                                           )                                                                                           )                                                                                     )
                                                            (%10
                                                           r                                                                                           r (%10                                                                                r (%10
                                                           ro                                                                                          ro                                                                20-layer            ro
                                                           er                                                                 20-layer                 r                                                                                     r
                                                                                                                                                       e                                                                                     e
                                                              5      plain-20                                                                            5                                                               110-layer              5
                                                                     plain-32
                                                                     plain-44
                                                                     plain-56                                                                                                                                                                   1
                                                              0                                                                                          0                                                                                      0 
                                                               0          1           2           3          4           5           6                     0          1           2           3          4           5           6                     4             5            6
                                                                                               iter. (1e4)                                                                                 iter. (1e4)                                                        iter. (1e4)
                             Figure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error
                             of plain-110 is higher than 60% and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202 layers.
                                                                                                                                                           
                                       3                                                                                               plain-20                                                   training data                           07+12                            07++12
                                                                                                                                       plain-56
                                      d                                                                                                ResNet-20                                                      test data                     VOC07test                         VOC12test
                                      st2                                                                                              ResNet-56
                                                                                                                                       ResNet-110                                                     VGG-16                                73.2                              70.4
                                       1                                                                                                                                                          ResNet-101                                76.4                              73.8
                                          
                                        0                   20                   40                  60                   80                  100
                                                                                   layer index (original)                                                                     Table 7. Object detection mAP (%) on the PASCAL VOC
                                                                                                                                                           
                                       3                                                                                               plain-20                               2007/2012 test sets using baseline Faster R-CNN. See also Ta-
                                                                                                                                       plain-56
                                      d                                                                                                ResNet-20                              ble 10 and 11 for better results.
                                      st2                                                                                              ResNet-56
                                                                                                                                       ResNet-110
                                       1                                                                                                                                                                metric                      mAP@.5                      mAP@[.5,.95]
                                                                                                                                                                                                     VGG-16                              41.5                              21.2
                                        0                   20                   40                  60                   80                  100
                                                                          layer index (sorted by magnitude)                                                                                       ResNet-101                             48.4                              27.2
                             Figure 7. Standard deviations (std) of layer responses on CIFAR-
                             10. The responses are the outputs of each 3×3 layer, after BN and                                                                                Table 8. Object detection mAP (%) on the COCO validation set
                             before nonlinearity. Top: the layers are shown in their original                                                                                 using baseline Faster R-CNN. See also Table 9 for better results.
                             order. Bottom: the responses are ranked in descending order.
                                                                                                                                                                              have similar training error. We argue that this is because of
                             networks such as FitNet [35] and Highway [42] (Table 6),                                                                                         overﬁtting. The 1202-layer network may be unnecessarily
                             yet is among the state-of-the-art results (6.43%, Table 6).                                                                                      large (19.4M) for this small dataset. Strong regularization
                             Analysis of Layer Responses. Fig. 7 shows the standard                                                                                           such as maxout [10] or dropout [14] is applied to obtain the
                             deviations (std) of the layer responses. The responses are                                                                                       best results ([10, 25, 24, 35]) on this dataset. In this paper,
                             the outputs of each 3×3 layer, after BN and before other                                                                                         weusenomaxout/dropout and just simply impose regular-
                             nonlinearity (ReLU/addition).                                             For ResNets, this analy-                                               ization via deep and thin architectures by design, without
                             sis reveals the response strength of the residual functions.                                                                                     distracting from the focus on the difﬁculties of optimiza-
                             Fig. 7 shows that ResNets have generally smaller responses                                                                                       tion. But combining with stronger regularization may im-
                             than their plain counterparts. These results support our ba-                                                                                     prove results, which we will study in the future.
                             sic motivation (Sec.3.1) that the residual functions might                                                                                       4.3. Object Detection on PASCAL and MS COCO
                             be generally closer to zero than the non-residual functions.                                                                                            Our method has good generalization performance on
                             Wealso notice that the deeper ResNet has smaller magni-                                                                                          other recognition tasks. Table 7 and 8 show the object de-
                             tudes of responses, as evidenced by the comparisons among                                                                                        tection baseline results on PASCAL VOC 2007 and 2012
                             ResNet-20, 56, and 110 in Fig. 7. When there are more                                                                                            [5] and COCO[26]. WeadoptFasterR-CNN [32]asthede-
                             layers, an individual layer of ResNets tends to modify the                                                                                       tection method. Here we are interested in the improvements
                             signal less.                                                                                                                                     of replacing VGG-16 [41] with ResNet-101. The detection
                             Exploring Over 1000 layers. We explore an aggressively                                                                                           implementation (see appendix) of using both models is the
                             deep model of over 1000 layers. We set n = 200 that                                                                                              same, so the gains can only be attributed to better networks.
                             leads to a 1202-layer network, which is trained as described                                                                                     Mostremarkably, on the challenging COCO dataset we ob-
                             above. Our method shows no optimization difﬁculty, and                                                                                           tain a 6.0%increaseinCOCO’sstandardmetric(mAP@[.5,
                                             3                                                                                                                                .95]), which is a 28% relative improvement. This gain is
                             this 10 -layer network is able to achieve training error
                             <0.1% (Fig. 6, right).                                    Its test error is still fairly good                                                    solely due to the learned representations.
                             (7.93%, Table 6).                                                                                                                                       Based on deep residual nets, we won the 1st places in
                                   But there are still open problems on such aggressively                                                                                     several tracks in ILSVRC&COCO2015competitions: Im-
                             deep models. The testing result of this 1202-layer network                                                                                       ageNet detection, ImageNet localization, COCO detection,
                             is worse than that of our 110-layer network, although both                                                                                       and COCOsegmentation. The details are in the appendix.
                                                                                                                                                                      8
