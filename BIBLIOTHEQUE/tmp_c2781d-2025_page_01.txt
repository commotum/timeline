                         Preprint, Under Review.
                         LEARNING WHEN TO PLAN: EFFICIENTLY
                         ALLOCATING TEST-TIME COMPUTE FOR LLM AGENTS
                                         1∗                  1,2,6∗               3                  4
                          Davide Paglieri , Bartłomiej Cupiał     , Jonathan Cook , Ulyana Piterbarg ,
                                    5                      1                        3
                          Jens Tuyls , Edward Grefenstette , Jakob Nicolaus Foerster ,
                                              1           ¨     1
                          Jack Parker-Holder , Tim Rocktaschel
                          1AICentre, University College London, 2IDEAS NCBR, 3University of Oxford,
                          4NewYorkUniversity, 5Princeton University, 6University of Warsaw,
                          d.paglieri@cs.ucl.ac.uk
                                                                ABSTRACT
                                 Training large language models (LLMs) to reason via reinforcement learning
                                 (RL) significantly improves their problem-solving capabilities. In agentic settings,
                                 existing methods like ReAct prompt LLMs to explicitly plan before every action;
                                 however, we demonstrate that always planning is computationally expensive and
                                 degrades performance on long-horizon tasks, while never planning further limits
                                 performance. To address this, we introduce a conceptual framework formalizing
                                 dynamic planning for LLM agents, enabling them to flexibly decide when to
                                 allocate test-time compute for planning. We propose a simple two-stage training
                                 pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for
                                 dynamicplanning,and(2)RLtorefinethiscapabilityinlong-horizonenvironments.
                                 ExperimentsontheCrafterenvironmentshowthatdynamicplanningagentstrained
                                 withthisapproacharemoresample-efficientandconsistentlyachievemorecomplex
                                 objectives. Additionally, we demonstrate that these agents can be effectively
                                 steered by human-written plans, surpassing their independent capabilities. To
                                 our knowledge, this work is the first to explore training LLM agents for dynamic
                                 test-time compute allocation in sequential decision-making tasks, paving the way
                                 for more efficient, adaptive, and controllable agentic systems.
                         1   INTRODUCTION
                         Akeyinsight from recent work on LLM reasoning is the role of test-time compute — the ability
                         to allocate additional computational resources to more difficult problems (Snell et al., 2025; Guo
                         et al., 2025). For humans, difficult tasks often require deliberate thinking. Similarly, LLMs benefit
                         from dedicating extra processing to explicitly reason through steps via chain-of-thought (Wei et al.,
                         2022). In settings like math problem-solving and code generation, reasoning can enable models to
                         explore possible answers before settling on a response in a manner akin to search (Xiang et al., 2025;
        arXiv:2509.03581v2  [cs.AI]  30 Sep 2025Prystawski et al., 2023; Ruis et al., 2025). Reasoning LLMs trained to effectively use additional
                         test-time compute on single-step tasks have also been shown to make extremely effective zero-shot
                         agents (Yao et al., 2023b). However, a critical open question remains: Can we further improve an
                         LLM’sability to effectively allocate test-time compute on sequential decision-making tasks? On the
                         challenging agentic benchmark BALROG(Paglieri et al., 2025a), reasoning models have thus far only
                         shownmarginal gains over models immediately producing the next action (Paglieri et al., 2025b).
                         In agentic tasks, planning naturally emerges as a multi-step analogue to single-step chain-of-thought
                         reasoning. Rather than committing immediately to a single next action, an agent can invest computa-
                         tional resources to better understand the current state and anticipate the outcome of future actions
                         sequences. Plans can serve as a guide for subsequent actions and improve the strategic coherence of
                         future behaviour. However, introducing explicit planning presents its own critical challenge: deciding
                         precisely when an agent should plan. This decision must carefully balance the performance improve-
                         ments gained from more informed decision-making against the computational cost and additional
                         variance in behaviour incurred by frequent replanning.
                            ∗Equal contribution.
                                                                     1
