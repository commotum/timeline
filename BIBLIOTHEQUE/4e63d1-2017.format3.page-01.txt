                                           End-to-EndDifferentiable Proving
                                        TimRocktäschel                                Sebastian Riedel
                                       University of Oxford             University College London & Bloomsbury AI
                                tim.rocktaschel@cs.ox.ac.uk                      s.riedel@cs.ucl.ac.uk
                                                                     Abstract
                                   Weintroduce neural networks for end-to-end differentiable proving of queries to
                                   knowledge bases by operating on dense vector representations of symbols. These
                                   neuralnetworksareconstructedrecursivelybytakinginspirationfromthebackward
                                   chaining algorithm as used in Prolog. Speciﬁcally, we replace symbolic uniﬁcation
                                   with a differentiable computation on vector representations of symbols using a
                                   radial basis function kernel, thereby combining symbolic reasoning with learning
                                   subsymbolic vector representations. By using gradient descent, the resulting neural
                                   network can be trained to infer facts from a given incomplete knowledge base.
                                   It learns to (i) place representations of similar symbols in close proximity in a
                                   vector space, (ii) make use of such similarities to prove queries, (iii) induce logical
                                   rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We
                                   demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural
                                   link prediction model, on three out of four benchmark knowledge bases while at
                                   the same time inducing interpretable function-free ﬁrst-order logic rules.
                          1    Introduction
                          Current state-of-the-art methods for automated Knowledge Base (KB) completion use neural link pre-
                          dictionmodelstolearndistributedvectorrepresentationsofsymbols(i.e. subsymbolicrepresentations)
                          for scoring fact triples [1–7]. Such subsymbolic representations enable these models to generalize
                          to unseen facts by encoding similarities: If the vector of the predicate symbol grandfatherOf is
                          similar to the vector of the symbol grandpaOf, both predicates likely express a similar relation.
                          Likewise, if the vector of the constant symbol LISA is similar to MAGGIE, similar relations likely
                          hold for both constants (e.g. they live in the same city, have the same parents etc.).
                          This simple form of reasoning based on similarities is remarkably effective for automatically complet-
        arXiv:1705.11040v2  [cs.NE]  4 Dec 2017ing large KBs. However, in practice it is often important to capture more complex reasoning patterns
                          that involve several inference steps. For example, if ABE is the father of HOMER and HOMER is a
                          parent of BART, we would like to infer that ABE is a grandfather of BART. Such transitive reasoning
                          is inherently hard for neural link prediction models as they only learn to score facts locally. In
                          contrast, symbolic theorem provers like Prolog [8] enable exactly this type of multi-hop reasoning.
                          Furthermore, Inductive Logic Programming (ILP) [9] builds upon such provers to learn interpretable
                          rules from data and to exploit them for reasoning in KBs. However, symbolic provers lack the ability
                          to learn subsymbolic representations and similarities between them from large KBs, which limits
                          their ability to generalize to queries with similar but not identical symbols.
                          While the connection between logic and machine learning has been addressed by statistical relational
                          learning approaches, these models traditionally do not support reasoning with subsymbolic repre-
                          sentations (e.g. [10]), and when using subsymbolic representations they are not trained end-to-end
                          from training data (e.g. [11–13]). Neural multi-hop reasoning models [14–18] address the aforemen-
                          tioned limitations to some extent by encoding reasoning chains in a vector space or by iteratively
                          reﬁning subsymbolic representations of a question before comparison with answers. In many ways,
                          these models operate like basic theorem provers, but they lack two of their most crucial ingredients:
