                  self-attention mechanisms to model hierarchical           recognition of D languages as a transduction task:
                                                                                             n
                  structures is limited. Shen et al. (2019) show that       Givenavalidstring,weaskthemodeltopredictthe
                  the performance of Transformers on tasks such             next possible symbols auto-regressively. To illus-
                  as logical inference (Bowman et al., 2015) and            trate, consider an input string “[ ( ) ] ( [” in the D2
                  ListOps (Nangia and Bowman, 2018) is either poor          language, we seek to predict the set of next valid
                  or worse than LSTMs. Tran et al. (2018) have              brackets in the string– (, [, or ]. We consider an
                  also reported similar results on SA, concluding that      input to be accurately recognized only if the model
                  recurrence is necessary to model hierarchical struc-      correctly predicts the set of all possible brackets at
                                                                      +
                  tures. In comparison, our results show that SA            each position in the input sequence. Throughout
                  outperforms LSTMonD languagesexceptforD                   the paper, we refer to a clause as a substring, in
                                            n                          2
                  on longer sequences. Papadimitriou and Jurafsky           which the number of closing and opening brackets
                  (2020) posit that the ability of neural models to         of each type of bracket are equal.
                  learn hierarchical structures can be attributed to a         We train two multi-headed self-attention net-
                  “looking back” capability, rather than directly en-       works(i.e., only the encoder part of a Transformer),
                  coding hierarchies. Our analysis sheds light on the       one of which incorporates an additional starting
                  ability of SA to learn hierarchical structures by ele-    symbolinthevocabulary(SA+),andtheotherdoes
                  gantly attending to the correct preceding symbol.                 −
                                                                            not (SA ). For each model, the number of layers is
                  2    Related Work                                         2, the number of attention heads h = 4 and model
                                                                            dimension d = 256. We use learnable embeddings
                                                       n n   n n m m        to convert each input symbol to a 256-dimensional
                  Formal languages such as a b ,a b c d                     vector. We also add residual connections around
                                         n n n    n+m n m
                  (context-free) and a b c ,a          b c    (context-     each layer followed by layer normalization, similar
                  sensitive) have been extensively studied and recog-       to the standard Transformer (Vaswani et al., 2017).
                  nized using RNNs (Elman, 1990; Das et al., 1992;          Wetrain two unidirectional LSTMs, one with the
                                     ¨
                  Steijvers and Grunwald, 1996). But the perfor-            starting symbol (LSTM+) and the other without
                  manceofsamerecurrent architectures on Dn lan-             it (LSTM−). The LSTMs use 320-dimensional
                  guages is poor and suffers from the lack of gen-          hidden states and a 320-dimensional vector for
                  eralization. Sennhauser and Berwick (2018) and            learned input embeddings. Our SA and LSTM
                  Bernardy (2018) study the capability of RNNs to                                                         1
                  predict the next possible closing parenthesis at each     variants all have around 1.6M parameters . We
                  position in the D string and found that the gener-        use Adam(KingmaandBa,2015)foroptimization.
                                    n                                               +         −
                  alization at higher recursion depths is poor. Hao         For SA andSA ,wevarythelearningrateη as
                  et al. (2018) reported that stack-augmented LSTMs                                −0.5              −1.5
                  achieve better generalization on Dn languages                η = const · min(itr     , itr · warmup     ),  (1)
                  but the network computation does not emulate a            whereitrrefers to the iteration number and warmup
                  stack. Morerecently,Suzgunetal.(2019)proposed             is set to 10k. We tuned the hyper-parameter const,
                  memory-augmented recurrent neural networks and            using the values [0.01, 0.1, 1.0, 10], and used 0.1.
                  deﬁned a sequence classiﬁcation task for the recog-       For LSTMs,weuseaninitiallearning rate of 0.001
                  nition of D languages. Yu et al. (2019) explored
                              n                                             but with no learning rate scheduling.
                  the use of attention-based seq2seq framework for             Were-generate the synthetic dataset for our ex-
                  D languagesandfoundthatthegeneralization to
                    2                                                       periments through the probabilistic context-free
                  sequences with higher depths is still lacking. Be-        grammar(PCFG)alreadydescribed in the existing
                  sides empirical investigations, formal languages          literature (Suzgun et al., 2019). For instance, the
                  have been studied theoretically for understanding         PCFGforDyck-2languagecanbedeﬁnedas: (1)
                  the complexity of neural networks (Siegelmann             S −→ [S], (2) S −→ {S}, (3) S −→ SS, and (4)
                                       ´
                  and Sontag, 1992; Perez et al., 2019), mostly under       S −→ ε, each with probability p = 0.25. For each
                  assumptions that cannot be met in an experiment–          D language, we train on 32k sequences of length
                  inﬁnite precision or unbounded computation time.            n
                                                                            2-50, validate on 3.2k sequences of length 52-74,
                  3    Experiments                                          andevaluateon10ksequencesdividedequallyover
                                                                            the length intervals 76-100 and 102-126.
                  We follow prior works (Gers and Schmidhuber,                 1Wefounddropouttobedetrimental to the performance,
                  2001; Suzgun et al., 2019), and formulate the             and hence we removed it from all models.
                                                                       4302
