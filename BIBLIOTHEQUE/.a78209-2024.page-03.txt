                                                                 Response       Purpose      Granularity                 Hallucination Types               Modality            Scenario
                                        Datasets               Generated by                                   Object     Attribute   Scene Text    Fact                          Task
                             FactCC(Kryscinski et al., 2020)      Synthetic      Check.        Sentence                                              ✔        Text            Text2Text
                             QAGS(Wangetal.,2020)                  Model         Check.        Summary                                               ✔        Text            Text2Text
                             HaluEval (Li et al., 2023a)          ChatGPT         Det.         Response                                              ✔        Text            Text2Text
                             POPE(Lietal., 2023b)                     -           Eval.        Response          ✔                                           Multi.           Image2Text
                             HaELM(Wangetal.,2023c)                   -           Det.         Response                                                      Multi.           Image2Text
                             AMBER(Wangetal.,2023b)                   -           Eval.        Response          ✔          ✔                                Multi.           Image2Text
                             MHaluBench(Ours)                     MMLMs           Det.      Res.,Seg.,Claim      ✔          ✔             ✔          ✔       Multi.     Image2Text/Text2Image
                          Table 1: A comparison of benchmarks w.r.t existing fact-checking or hallucination evaluation. “Check.” indicates verifying
                          factual consistency, “Eval.” denotes evaluating hallucinations generated by different LLMs, and its response is based on different
                          LLMsundertest, while “Det.” embodies the evaluation of a detector’s capability in identifying hallucinations.
                              such as incorrect objects, attributes, or scene                                    ulously curated to include a balanced distribu-
                              text. An example in Figure 1 (a) includes an                                       tion of instances across three pivotal tasks, which
                              MLLMinaccuratelydescribing an athlete’s uni-                                       encompasses 200 exemplars for the task of IC
                              form color, showcasing an attribute-level conflict                                 200 for VQA, and an additional 220 dedicated
                              due to MLLMs’ limited ability to achieve fine-                                     to Text-to-Image Generation. The comparison of
                              grained text-image alignment.                                                      MHaluBenchwithotherbenchmarksisdetailed in
                          • Fact-Conflicting Hallucination. Outputs from                                         Table 1 and the statistical details are provided in
                              MLLMs may contradict established factual                                           Figure 3 and Figure 4.
                              knowledge. Image-to-text models can generate                                       3.1      Hallucinatory Example Collection
                              narratives that stray from the actual content by in-
                              corporating irrelevant facts, while text-to-image                                  Image-to-Text Generation.                       WefocusonICand
                              models may produce visuals that fail to reflect                                    VQAtasks,drawingsamplesfromtheMS-COCO
                              the factual knowledge contained in text prompts.                                   2014 validation set (Lin et al., 2014) and the
                              These discrepancies underline the struggle of                                      TextVQA test set (Singh et al., 2019). We com-
                              MLLMstomaintain factual consistency, repre-                                        pile generative outputs from mPLUG (Ye et al.,
                              senting a significant challenge in the domain.                                     2023), LLaVA (Liu et al., 2023c), and MiniGPT-
                                                                                                                 4 (Zhu et al., 2023) to form the core dataset for
                          Unified Detection Problem Formulation.                                    Uni-         MHaluBench. These models are representative of
                          fied detection of multimodal hallucination necessi-                                    current leading MLLMs, characterized by their di-
                          tates the check of each image-text pair a = {v,x},                                     verse content generation capabilities and a notable
                          wherein v denotes either the visual input provided                                     presence of hallucinations, as depicted in Figure 8.
                          to an MLLM, or the visual output synthetic by it.                                      Text-to-Image Generation.                          We source initial
                          Correspondingly, x signifies the MLLM’s gener-                                         captions from DrawBench (Saharia et al., 2022)
                          ated textual response based on the v or the tex-                                       and T2I-CompBench (Huang et al., 2023a). These
                          tual user query for synthesizing v. Within this                                        captions are augmented through ChatGPT to in-
                          task, each x may contain multiple claims, de-                                          clude more specific information such as objects,
                          noted as {c }                   . The objective for hallucina-
                                             i  i=1···n                                                          attributes, and factual details, among others. The re-
                          tion detectors is to assess each claim from a to                                       fined caption guides the DALL-E 2 (Ramesh et al.,
                          determine whether it is “hallucinatory” or “non-                                       2022) and DALL-E 3 model (Betker et al., 2023)
                          hallucinatory”, providing a rationale for their judg-                                  in producing visually detailed images.
                          ments based on the provided definition of halluci-
                          nation. Text hallucination detection from LLMs                                         3.2      SegmentandClaimExtraction
                          denotes a sub-case in this setting, where v is null.                                   Beyondevaluating overall responses, we introduce
                          3 ConstructionofMHaluBench                                                             segmentation at both the segment and claim levels
                                                                                                                 for a multi-granular assessment of hallucinations,
                          Tofacilitate research in this area, we introduce the                                   enabling more precise feedback to improve model
                          meta-evaluation benchmark MHaluBench, which                                            performance (Lightman et al., 2023). We leverage
                          encompasses the content from image-to-text and                                         ChatGPT’s advanced instruction-following ability
                          text-to-image generation, aiming to rigorously as-                                     to extract detailed segments and related claims.
                          sess the advancements in multimodal hallucina-                                         For image-to-text tasks, we split and extract the
                          tion detectors. Our benchmark has been metic-                                          model’s textual output into segments and claims;
                                                                                                          3237
