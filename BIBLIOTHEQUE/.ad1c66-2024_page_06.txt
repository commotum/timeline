                                  T. Han et al.                                                                                                                                                                                                                                   International Journal of Applied Earth Observation and Geoinformation 133 (2024) 104105 
                                  Fig. 8. Different graph transformer architectures. Unlike GCN and Naive GT, AGT takes into account edge information and incorporates structural details. Furthermore, it optimizes
                                  the structure of General GT and avoids the dual-branch pathway strategy by using structural weights.
                                  Ì†µÌ∞πÌ†µÌ±É , the AGT block aggregates neighbor structural features and utilizes                                                                                                                                                                    The absolute position embedding is significantly useful at learning
                                  graph attention to generate new features for all points. During this                                                                                                                                                                         contextual representations of tokens in different positions. However,
                                  process, the relationships between points are dynamically adjusted to                                                                                                                                                                        it is not straightforward to capture vertex spatial information in the
                                  align with the optimal object structure.                                                                                                                                                                                                     graph. To this end, we consider topological priors represented by
                                            Analysis: Transformer is able to perform a weighted linear trans-                                                                                                                                                                  relative position information as relative position embedding. It contains
                                  formation on tokens based on their importance to update the current                                                                                                                                                                          more explicit knowledge between pairs of vertices. Graph Laplacian
                                  token. Graph neural networks (GNNs) update the feature of central                                                                                                                                                                            Matrix represents connectivity in terms of both adjacency and node
                                  vertex by aggregating the neighbor features on the graph. From the                                                                                                                                                                           degree of graph. It has to be said that the Laplacian matrix primarily
                                  connectivity structure, it can be observed that the Transformer and                                                                                                                                                                          focuses on local information and may have limited adaptability to dy-
                                  GNNs are coupled. Previous graph attention networks are sparse, con-                                                                                                                                                                         namic graph structures. In dynamic graphs, the relationships between
                                  sidering only neighbor vertices, while Transformer is a fully connected                                                                                                                                                                      nodes and edges undergo frequent changes. Therefore, more flexible
                                  architecture that takes into account all vertices. In contrast, Graph                                                                                                                                                                        approaches that can capture both global and local variations might be
                                  Transformer introduces the topological structural properties of graph                                                                                                                                                                        more suitable.
                                  on the basis of global context information, providing the model with                                                                                                                                                                                   Webelieve the edges connecting them should be considered in the
                                  structural spatial priors in high-dimensional space. We analyze different                                                                                                                                                                    correlation. We represent the relational information between (Ì†µÌ±ñ,Ì†µÌ±ó) as a
                                  model designs, as shown in Fig. 8.                                                                                                                                                                                                           vector Ì†µÌ±äÌ†µÌ±ñÌ†µÌ±ó. Let Ì†µÌ±äÌ†µÌ±ñÌ†µÌ±ó serve as an implicit position embedding, which is
                                            Whenconsidering the graph, the neighbor aggregation function can                                                                                                                                                                   able to complement global relationships while focusing on neighbors.
                                  be formulated to Eq. (6):                                                                                                                                                                                                                    Wemodify the attention score formula as follows:
                                  Ì†µÌ∞π‚Ä≤ = Ì†µÌºé(Ì†µÌªºÌ†µÌ∞π + ‚àë (Ì†µÌªΩÌ†µÌ∞π ))                                                                                                                                                                                         (6)                       Ì†µÌª∑         =Ì†µÌ±Ñ ‚ãÖÌ†µÌ∞æ +Ì†µÌ±ä                                                                                                                                                                                             (9)
                                      Ì†µÌ±ñ                         Ì†µÌ±ñ                                  Ì†µÌ±ó                                                                                                                                                                             Ì†µÌ±ñÌ†µÌ±ó               Ì†µÌ∞π              Ì†µÌ∞π                  Ì†µÌ±ñÌ†µÌ±ó
                                                                           Ì†µÌ±ó‚ààÌ†µÌ±Å(Ì†µÌ±ñ)                                                                                                                                                                                                                      Ì†µÌ±ñ              Ì†µÌ±ó
                                                                                                                                                                                                                                                                               Ì†µÌ∞æ            incorporates Ì†µÌ±ä , as explained in the previous context, allowing
                                  Further, the process of combining graph and Transformer (Naive Graph                                                                                                                                                                              Ì†µÌ∞πÌ†µÌ±ó                                                     Ì†µÌ±ñÌ†µÌ±ó
                                  Transformer) can be represented as updating the features of vertex Ì†µÌ±ñ                                                                                                                                                                        Eq. (9) to eliminate unnecessary addition computations. Position infor-
                                  through its neighbor vertex Ì†µÌ±ó, as described in the Eq. (7):                                                                                                                                                                                 mation is able to communicate directly with the attention mechanism
                                                     ‚àë                                                                                                                                                                                                                         in our approach. In order to explicitly capture the effect of neighbor-
                                  Ì†µÌ∞π‚Ä≤ =                            Ì†µÌ∞¥Ì†µÌ±°Ì†µÌ±°Ì†µÌ±íÌ†µÌ±õÌ†µÌ±°Ì†µÌ±ñÌ†µÌ±úÌ†µÌ±õ(Ì†µÌ±Ñ                    , Ì†µÌ∞æ          , Ì†µÌ±â         )                                                                                                             (7)                       ing points on Ì†µÌ±ñ, we still apply explicit positional embedding to Ì†µÌ±ÑÌ†µÌ∞π ,
                                      Ì†µÌ±ñ                                                              Ì†µÌ∞π            Ì†µÌ∞π           Ì†µÌ∞π                                                                                                                                                                                                                                                                                                                                                                    Ì†µÌ±ñ
                                                  Ì†µÌ±ó‚ààÌ†µÌ±Å(Ì†µÌ±ñ)                                              Ì†µÌ±ñ            Ì†µÌ±ó           Ì†µÌ±ó                                                                                                                                         preventing the imbalance of neighboring point features, as shown in
                                  This indicates that attention weight for each pair of (Ì†µÌ±ñ,Ì†µÌ±ó) is calculated                                                                                                                                                                  Eq. (4).
                                  based on the Ì†µÌ∞πÌ†µÌ±ñ and Ì†µÌ∞πÌ†µÌ±ó, and then the feature of vertex Ì†µÌ±ñ is updated                                                                                                                                                                               The third issue is the over-smoothing and over-squashing in GNNs.
                                  through the weighted accumulation of all attention weights for Ì†µÌ±ó. But                                                                                                                                                                       The receptive field of vertex becomes larger, leading to an increase in
                                  we still need to pay attention to the following aspects:                                                                                                                                                                                     the number of shared neighbors between two vertices. Therefore, the
                                            Firstly, leveraging edge features is a key factor in achieving a                                                                                                                                                                   feature embeddings of two vertices become more similar. As shown in
                                  dynamic graph. Edge features represent relationships between pairs of                                                                                                                                                                        Fig. 7, we add different MLPs as linear layers to increase the expres-
                                  vertices, reflecting implicit attention scores. While the score obtained                                                                                                                                                                     sive power of the network. These MLP layers serve as pre-processing
                                  after multiplication of Ì†µÌ±ÑÌ†µÌ∞π and Ì†µÌ∞æÌ†µÌ∞π represents the implicit information                                                                                                                                                                    step without message passing. Furthermore, we introduce residual skip
                                                                                                                  Ì†µÌ±ñ                        Ì†µÌ±ó                                                                                                                                 connections in both the overall network and AGT blocks, allowing
                                  about edge ‚ü®Ì†µÌ±ñ,Ì†µÌ±ó‚ü©, not all edges are necessarily available. An elementary                                                                                                                                                                   the updated features to reference the mappings from previous layer.
                                  attempt is to incorporate edge features into the attention calculation                                                                                                                                                                       This is a long-range residual connections from the input to last layers
                                  process as Eq. (8):                                                                                                                                                                                                                          instead of multiple short-range connections in the original Transformer.
                                  Ì†µÌ∞π‚Ä≤ = ‚àë (Ì†µÌ±†Ì†µÌ±úÌ†µÌ±ìÌ†µÌ±°Ì†µÌ±öÌ†µÌ±éÌ†µÌ±•(Ì†µÌ±Ñ                                                 ‚ãÖ Ì†µÌ∞æ           ) ‚ãÖ Ì†µÌ∞∏            Ì†µÌ±â        )                                                                                            (8)                       It alleviates the over-smoothing problems in the message passing while
                                      Ì†µÌ±ñ                                                              Ì†µÌ∞π              Ì†µÌ∞π                Ì†µÌ±ñ,Ì†µÌ±ó     Ì†µÌ∞π
                                                  Ì†µÌ±ó‚ààÌ†µÌ±Å(Ì†µÌ±ñ)                                              Ì†µÌ±ñ              Ì†µÌ±ó                          Ì†µÌ±ó                                                                                                                        alleviating vanishing gradient issues.
                                            This introduces a new issue, as the network must maintain a sep-                                                                                                                                                                             AnalyzingEq.(6),asageneralformofneighbormessagepassingop-
                                  arate pipeline to propagate edge attributes, which is redundant (See                                                                                                                                                                         eration, an assumption is proposed: the local topology of the graph may
                                  Fig. 8(c)). On the contrary, in our dynamic graph structure, the relation-                                                                                                                                                                   lead to over-squashing. If a vertex has many neighbors, it may reduce
                                  ships between edges are concealed in the weighted features Ì†µÌ±äÌ†µÌ±ñÌ†µÌ±ó, and                                                                                                                                                                       the mapping of features from distant points, leading to an information
                                  these edge relationships are learnable. Therefore, using the weighted                                                                                                                                                                        bottleneck, specific arguments can be found in Ref. Topping et al.
                                  features containing edge attributes and neighbor point features as Ì†µÌ∞æ                                                                                                                                                                        (2021). However, our approach introduces a new way of association
                                  and Ì†µÌ±â, attention scores can be computed based on similarity and                                                                                                                                                                             by incorporating edge weights while preserving the local geometric
                                  importance, see Eq. (4). Note that this edge attribute is sparse, in                                                                                                                                                                         topology. The attention weight enhances the feedback from available
                                  contrast to the fully connected Transformer.                                                                                                                                                                                                 vertices and also increases the available edges from distant vertices. In
                                            Secondly, position information is essential. In Transformer, the                                                                                                                                                                   addition, virtual nodes are also able to alleviate over-squashing to some
                                  structural bias is explicitly given as a form of position embedding.                                                                                                                                                                         extent, as will be analyzed in detail in the next section.
                                                                                                                                                                                                                                                                     6 
