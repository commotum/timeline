                                     Seed tokens                 Generated by the 
                                  (first í µí±— + å µí±˜ tokens)        corresponding rules
                     Recursive     2    0    3    1      6    3    6    2     3    0    4         2            =â„Ž(         ,     ,     ,     )
                           CoT 2        0    3    1      6    5    0    6     6    0    0         1       =        +      +      +        modå µí±š
                         Semi-     2    0    3    1      6    5    5    2     1    4    1         0
                     Recursive
                                                            Input                               Output
                    Figure 2: An example of the three subtasks of POSGEN. This figure shows the process of generating the 12th token
                    showninthe red boxes for each subtask. In this example, h is a modular addition task with the modulus m = 7 and
                    the difficulty-controlling parameters j = 1,k = 3. The output token depends on: (1) only the local j + k tokens in
                    the recursive task; (2) k local tokens and the beginning j tokens in the CoT task; and (3) k local tokens and j tokens
                    with a varied dependency distance in the semi-recursive task.
                    wedesign POSGEN,whichcontrolsthedifficulty                           Based on the equation for each subtask, when
                    in generating tokens throughout the sequence to be                given the first j + k tokens, one can generate a
                    identical, which effectively distinguishes the two                sequence with unlimited length as the ground truth
                    types of TSTL failures. Failures in this benchmark                sequence. We show an example of POSGEN in Fig-
                         2    Huawei Proprietary - Restricted Distribution
                    are only due to the inability to recognize new token              ure 2. As a TSTL benchmark, we train a model on
                    positions in TSTL scenarios.                                      a subtask with sequence length up to L, and evalu-
                       Our POSGEN framework comprises three sub-                      ate the modelâ€™s accuracy on a longer sequence with
                    tasks, with each extracting the general token de-                 length Lâ€² > L generated by the same rule on the
                    pendency pattern of a different type of reason-                   unseen positions L < m â‰¤ Lâ€², which we refer to
                    ing task. Suppose that we define a fixed function                 as the â€œOOD Accuracyâ€ (OOD Acc). This met-
                            j+k                                                       ric measures how well a model can recognize the
                    h : V        â†’V,whereVisthe modelâ€™s vocabu-                       OODpositionsandcontinue following the genera-
                    lary and j,k are predefined constants controlling
                    the taskâ€™s difficulty. The three subtasks of POS-                 tion rule learned during training. As a benchmark
                    GENareasfollows:                                                  for position embeddings, a standard usage of this
                       1. Recursive. This task simulates the token de-                benchmarkis to train a small Transformer (e.g., a
                                                                                      2-layer Transformer as used in our experiments)
                           pendency pattern of generating a Fibonacci-                with different position embeddings on its training
                           style sequence, where new tokens depend                    set with only short sequences, and test its OOD
                           on j + k neighboring tokens only: x =
                                                                            l         Accuracy on the test set with longer sequences.
                           h(x           , Â· Â· Â· , x  ) when l â‰¥ j + k.
                               lâˆ’(j+k))           lâˆ’1                                 Weprovideourexperiment setting for POSGEN in
                       2. Chain-of-Thought (CoT). This task simu-                     moredetails in Section 6.1.1 and Appendix C.1.
                           lates the token dependency pattern of CoT rea-             6 Experiments
                           soning (Wei et al., 2022), where new tokens
                           depend on k neighboring tokens (simulating                 Weevaluate RESONANCE ROPE onthree different
                           the previous reasoning step) and j tokens in               TSTLtasks: a small-scale evaluation on our pro-
                           the front (simulating the original question):              posed POSGEN task, and LLM-scale evaluations
                           x =h(x ,Â·Â·Â· ,x            , x    , Â· Â· Â· , x  ) when
                            l         0          jâˆ’1    lâˆ’k          lâˆ’1              withLLaMA2-Chat(Touvronetal.,2023b)onboth
                           l â‰¥ j +k.                                                  language modeling perplexity and real-world long
                       3. Semi-recursive. This task simulates the to-                 context applications.
                           ken dependency pattern of the last-letter con-             6.1    Synthetic Task Evaluation
                           catenation task (Zhou et al., 2023), where                 6.1.1     ExperimentSetup
                           newtokens depend on both k neighboring to-
                           kens (simulating the current progress) and j               Wefirst apply RESONANCE ROPE on RoPE and
                           tokens with varied distances according to a                YaRN,assessing the modelâ€™s performance on POS-
                           specific rule (simulating the word sequence):               GENforunseen position recognition. We test on
                           x =h(x                     , Â· Â· Â· , x               ,     a modular addition task, which was proved to be
                            l         âŒŠlâˆ’(j+k)/2âŒ‹âˆ’j            âŒŠlâˆ’(j+k)/2âŒ‹âˆ’1
                           x    , Â· Â· Â· , x  ) when l â‰¥ j + k.                        learnable by a one-layer Transformer (Nanda et al.,
                            lâˆ’k          lâˆ’1                                          2023). We configured j = 1,k = 3, and defined
                                                                                  591
