                     Published as a conference paper at ICLR 2024
                     APPENDIX
                     In the appendix, we provide more thorough details regarding the dataset construction process, eval-
                     uation pipeline, and characterization of the SWE-bench benchmark.
                     A BENCHMARKDETAILS
                     ThissectioncomplementsSection2withamoretechnicalandfine-grainedsummaryofthedatacol-
                     lection, execution-based validation, and evaluation procedures, along with a fuller characterization
                     of the task instances.
                     A.1  HIGH LEVEL OVERVIEW
                     Pull request scraping. From a list of the top 5,000 most downloaded PyPI libraries during August
                     2023, we select the top 100 packages, identify each library’s corresponding open-source GitHub
                     repository, verify whichpackageshavelicensesallowingforfreesoftwareuse,andcollectallPRsfor
                     these repositories via the GitHub developer API. We elect to source problems from well-trafficked
                     repositories because widespread use usually suggests that the repository has extensive documenta-
                     tion, structured open-source development guidelines, and working, well-formatted code.
                     Task instance construction. We construct candidate task instances from PRs that satisfy three
                     conditions. First, the PR’s status must be Merged. A Merged status indicates that the PR’s associated
                     codechangeswereacceptedandincorporatedintoitsparentrepository. Second,thePRresolvesone
                     or more issues in its repository. An issue is defined according to its canonical usage in GitHub as
                     a digital ticket for tracking bugs, enhancements, or any general development goals for a software
                     project. We scan a PR’s title, body, and commit messages for linked issues (i.e. “fixes #24”). Third,
                     the PR must introduce one or more new tests. A new test is counted when a PR’s code changes edits
                     a file path containing a testing-related keyword (e.g. “test”, “testing”).
                     APRthatsatisfies these criteria is then converted into a candidate task instance such as the example
                     in Figure 7. The codebase C is identified by the repository’s owner/name moniker and the pull
                     request’s base commit. Recovering the actual codebase from this information is straightforward.
                     Wecreate mirrors of the original GitHub repositories, where each mirror is uniquely identified as
                     owner name. Cloning a repository’s corresponding mirror and checking out the base commit
                     yields C in its pre-PR state. The problem statement P is an aggregate of all related issues’ titles and
                     descriptions along with any subsequent comments written before the timestamp of the PR’s initial
                     commit to avoid leakage of solution details. A PR’s code changes are separated into a test patch
                     and a gold patch δ. T consists of all tests from files edited in the test patch. As shown in Figure 7,
                     both T and δ are stored as patch files. Further details about parsing PR and semantic data is in
                     Appendix A.2.
                     Execution-based validation. We verify the usability of a task instance via execution. For each
                     candidate, we first define a virtual environment to serve as an execution context, then install C
                     before applying any patches, and finally run T once before and once after the solution δ is applied.
                     Acandidateisremovedfromconsiderationforthefinaldatasetifanystepintheverificationprocess
                     fails. In addition, to ensure that a solution δ is non-trivial, we compare the pre-solution and post-
                     solution validation logs to check for whether there are one or more tests in T where the status
                     changes from fail to pass. Lastly, we exclude task instances with tests that invoke newly created
                     functions or classes first introduced in the solution δ. Since naming such constructs is typically
                     an arbitrary process and usually not explicitly specified in the problem statement, resolving tests
                     such as these may be an impossible task even for human developers. Information about execution
                     contexts, codebase installation, determining test statuses from logs, and more are in Appendix A.3.
                     ContinuousUpdates. SWE-bench’scollection process is easily extensible to any open source code
                     repositories, allowing for easy and low-maintenance extension to new programming languages and
                     code domains. This design also provides SWE-bench with temporal robustness; as new language
                     models trained on more recent source code are released over time, SWE-bench can simply be up-
                     dated to produce new task instances based on PRs created after any LM’s training date.
                        https://hugovk.github.io/top-pypi-packages/
                                                           15
