                          Published as a conference paper at ICLR 2021
                          James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
                            Maclaurin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy
                            programs, 2018. URL http://github.com/google/jax.
                          Niladri S Chatterji, Behnam Neyshabur, and Hanie Sedghi. The intriguing role of module criticality
                            in the generalization of deep networks. In International Conference on Learning Representations,
                            2020.
                          Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
                            Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing Gradi-
                            ent Descent Into Wide Valleys. arXiv e-prints, art. arXiv:1611.01838, November 2016.
                          Pengfei Chen, Benben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing
                            deep neural networks trained with noisy labels. CoRR, abs/1905.05040, 2019. URL http:
                            //arxiv.org/abs/1905.05040.
                                                                            ´
                          Ekin Dogus Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le.           Au-
                            toaugment: Learning augmentation policies from data. CoRR, abs/1805.09501, 2018. URL
                            http://arxiv.org/abs/1805.09501.
                          J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
                            Image Database. In CVPR09, 2009.
                          Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks
                            with cutout. CoRR, abs/1708.04552, 2017. URL http://arxiv.org/abs/1708.04552.
                          Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
                            for deep nets. arXiv preprint arXiv:1703.04933, 2017.
                          AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
                            Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
                            reit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at
                            Scale. arXiv e-prints, art. arXiv:2010.11929, October 2020.
                          Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
                          John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
                            stochastic optimization. Journal of machine learning research, 12(7), 2011.
                          Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
                            deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
                            arXiv:1703.11008, 2017.
                          Xavier Gastaldi.   Shake-shake regularization.  CoRR, abs/1705.07485, 2017.      URL http:
                            //arxiv.org/abs/1705.07485.
                          Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An Investigation into Neural Net Optimiza-
                            tion via Hessian Eigenvalue Density. arXiv e-prints, art. arXiv:1901.10159, January 2019.
                          Dongyoon Han, Jiwhan Kim, and Junmo Kim.          Deep pyramidal residual networks.     CoRR,
                            abs/1610.02915, 2016. URL http://arxiv.org/abs/1610.02915.
                                                                                                       ¨
                          Ethan Harris, Antonia Marcu, Matthew Painter, Mahesan Niranjan, Adam Prugel-Bennett, and
                            Jonathon Hare.    FMix: Enhancing Mixed Sample Data Augmentation.        arXiv e-prints, art.
                            arXiv:2002.12047, February 2020.
                          KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
                            nition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.
                          Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
                            lecture 6a overview of mini-batch gradient descent.
                                               ¨
                          Sepp Hochreiter and Jurgen Schmidhuber. Simplifying neural nets by discovering ﬂat minima. In
                            Advances in neural information processing systems, pp. 529–536, 1995.
                                                                        10
