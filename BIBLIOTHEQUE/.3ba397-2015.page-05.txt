                                                     layer name output size          18-layer             34-layer              50-layer                101-layer               152-layer
                                                        conv1      112×112                                                     7×7,64,stride 2
                                                                                                                            3×3maxpool,stride 2
                                                      conv2 x       56×56        3×3,64             3×3,64              1×1,64                1×1,64                1×1,64 
                                                                                                                                                                                     
                                                                                   3×3,64 ×2            3×3,64 ×3             3×3,64       ×3         3×3,64       ×3          3×3,64      ×3
                                                                                                                            1×1,256               1×1,256               1×1,256 
                                                      conv3 x       28×28        3×3,128            3×3,128             1×1,128               1×1,128               1×1,128 
                                                                                  3×3,128 ×2            3×3,128 ×4            3×3,128      ×4         3×3,128      ×4         3×3,128      ×8
                                                                                                                           1×1,512              1×1,512                1×1,512 
                                                      conv4 x       14×14        3×3,256            3×3,256            1×1,256              1×1,256                1×1,256 
                                                                                  3×3,256 ×2            3×3,256 ×6            3×3,256       ×6       3×3,256       ×23        3×3,256      ×36
                                                                                                                           1×1,1024             1×1,1024               1×1,1024 
                                                      conv5 x         7×7        3×3,512            3×3,512            1×1,512               1×1,512                1×1,512 
                                                                                  3×3,512 ×2            3×3,512 ×3            3×3,512       ×3        3×3,512       ×3        3×3,512       ×3
                                                                                                                             1×1,2048                1×1,2048                 1×1,2048
                                                                      1×1                                             average pool, 1000-d fc, softmax
                                                              FLOPs                  1.8×109              3.6×109               3.8×109                 7.6×109                 11.3×109
                     Table 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down-
                     sampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.
                                                                                                                                                                                                                  
                                       60                                                                                          60
                                       50                                                                                          50
                                      )                                                                                           )
                                       (%                                                                                          (%
                                      r40                                                                                         r40
                                      ro                                                                                          ro
                                      er                                                                                          er
                                                                                                        34-layer
                                                                                                                                                                                                    18-layer
                                       30                                                                                          30
                                                plain-18                                                18-layer                            ResNet-18
                                                plain-34                                                                                    ResNet-34                                               34-layer
                                       20                                                                                          20 
                                          0            10           20            30           40            50                       0            10           20            30           40            50
                                                                          iter. (1e4)                                                                                 iter. (1e4)
                     Figure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain
                     networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to
                     their plain counterparts.
                                                                   plain              ResNet                                   reducing of the training error3. The reason for such opti-
                                          18layers                 27.94               27.88                                   mization difﬁculties will be studied in the future.
                                          34layers                 28.54               25.03                                   Residual Networks. Next we evaluate 18-layer and 34-
                     Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation.                                         layer residual nets (ResNets). The baseline architectures
                     Here the ResNets have no extra parameter compared to their plain                                          are the same as the above plain nets, expect that a shortcut
                     counterparts. Fig. 4 shows the training procedures.                                                       connection is added to each pair of 3×3 ﬁlters as in Fig. 3
                                                                                                                               (right). In the ﬁrst comparison (Table 2 and Fig. 4 right),
                                                                                                                               weuseidentity mapping for all shortcuts and zero-padding
                     34-layer plain net has higher training error throughout the                                               for increasing dimensions (option A). So they have no extra
                     whole training procedure, even though the solution space                                                  parameter compared to the plain counterparts.
                     of the 18-layer plain network is a subspace of that of the                                                     We have three major observations from Table 2 and
                     34-layer one.                                                                                             Fig. 4. First, the situation is reversed with residual learn-
                          We argue that this optimization difﬁculty is unlikely to                                             ing – the 34-layer ResNet is better than the 18-layer ResNet
                     be caused by vanishing gradients. These plain networks are                                                (by 2.8%). More importantly, the 34-layer ResNet exhibits
                     trained with BN [16], which ensures forward propagated                                                    considerably lower training error and is generalizable to the
                     signals to have non-zero variances. We also verify that the                                               validation data. This indicates that the degradation problem
                     backward propagated gradients exhibit healthy norms with                                                  is well addressed in this setting and we manage to obtain
                     BN. So neither forward nor backward signals vanish. In                                                    accuracy gains from increased depth.
                     fact, the 34-layer plain net is still able to achieve compet-                                                  Second, compared to its plain counterpart, the 34-layer
                     itive accuracy (Table 3), suggesting that the solver works                                                     3Wehaveexperimentedwithmoretrainingiterations(3×)andstillob-
                     to some extent. We conjecture that the deep plain nets may                                                served the degradation problem, suggesting that this problem cannot be
                     haveexponentiallylowconvergencerates,whichimpactthe                                                       feasibly addressed by simply using more iterations.
                                                                                                                          5
