# Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding (2024)
Source: a75818-2024.pdf

## Core reasons
- The paper explicitly frames itself as a survey that fills a gap by reviewing length extrapolation methods from the positional encoding perspective, rather than proposing a new positional encoding or architecture.
- It presents a systematic review of existing methods and recent trends, indicating a foundations-style synthesis instead of a new dataset, benchmark, or computation mechanism.

## Evidence extracts
- "Despite the great research efforts, a systematic survey is still lacking. To fill this gap, we delve into these advances in a unified notation from the perspective of positional encoding (PE), as it has been considered the primary factor on length extrapolation." (p. 1)
- "This survey presented a systematic review of exist- ing methods and recent trends in length extrapola- tion of Transformers." (p. 10)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
