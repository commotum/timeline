# Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks (2019)
Source: 5dd41f-2018.pdf

## Core reasons
- Introduces the Set Transformer as an attention-based set-input neural network architecture for permutation-invariant data, which is a model architecture contribution.
- Proposes an inducing point method to make self-attention scalable to large sets, focusing on architectural efficiency rather than datasets or positional encoding.

## Evidence extracts
- "In this paper, we introduced the Set Transformer, an
attention-based set-input neural network architecture." (p. 8)
- "We also
proposed an inducing point method for self-attention, which
makesourapproachscalable to large sets." (p. 8)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
