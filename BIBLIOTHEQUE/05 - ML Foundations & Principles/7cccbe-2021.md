# Muesli: Combining Improvements in Policy Optimization (2021)
Source: 7cccbe-2021.pdf

## Core reasons
- Introduces a new policy update method (Muesli) that combines regularized policy optimization with model learning, making the contribution an RL optimization method rather than a positional-encoding or dimensional-transformer change.
- Emphasizes the design of a policy optimization update and its evaluation on RL benchmarks, aligning with foundational ML methodology.

## Evidence extracts
- "W e propose a no v el polic y update that combines re gularized polic y optimization with model learn- ing as an auxiliary loss." (p. 1)
- "The o v er - all update, named Muesli, then combines the clipped MPO tar gets and polic y-gradients into a dir ect method ( V ieillard et al. , 2020 ) for re gularized polic y optimization." (p. 2)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
