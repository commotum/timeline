# Recurrent Neural Network Regularization (2015)
Source: 3b6d87-2014.pdf

## Core reasons
- The paper introduces a regularization technique for RNNs/LSTMs, centered on applying dropout to LSTMs to improve generalization.
- The contribution is a training/regularization method rather than positional encoding changes, dimensional lifting, or new computation mechanisms.

## Evidence extracts
- "We present a simple regularization technique for Recurrent Neural Networks" (p. 1)
- "and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs," (p. 1)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
