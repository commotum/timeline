# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2019)
Source: 532e97-2018.pdf

## Core reasons
- Introduces BERT as a new language representation model based on Transformers, a foundational model contribution rather than a positional encoding or domain-lifting change.
- The core contribution is a pre-training methodology for bidirectional Transformer representations, which aligns with ML foundations and training principles rather than new datasets or measurement.

## Evidence extracts
- "We introduce a new language representa- tion model called BERT, which stands for Bidirectional Encoder Representations from Transformers." (Abstract)
- "Instead, we pre-train BERT using two unsuper- vised tasks, described in this section." (Section 3.1 Pre-training BERT)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
