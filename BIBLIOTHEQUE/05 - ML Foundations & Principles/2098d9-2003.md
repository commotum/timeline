# A Neural Probabilistic Language Model (2003)
Source: 2098d9-2003.pdf

## Core reasons
- Introduces a language modeling approach that jointly learns distributed word representations and a sequence probability function, which is a foundational modeling contribution.
- Defines the core method as associating each word with a distributed feature vector and expressing joint word-sequence probabilities in terms of those vectors.

## Evidence extracts
- "The model learns simultaneously (1) a distributed representation for each word along
with (2) the probability function for word sequences, expressed in terms of these representations." (p. 1)
- "1. associate with each word in the vocabulary a distributed word feature vector (a real-
valued vector in Rm),
2. express the joint probability function of word sequences in terms of the feature vectors
of these words in the sequence, and
3. learn simultaneously the word feature vectors and the parameters of that probability
function." (p. 3)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
