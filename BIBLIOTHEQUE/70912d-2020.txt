                                                  Generative Pretraining from Pixels
                   MarkChen1 AlecRadford1 RewonChild1 JeffWu1 HeewooJun1 PrafullaDhariwal1 DavidLuan1
                                                                     Ilya Sutskever1
                                        Abstract                                ported strong results using a single layer of learned features
                    Inspired by progress in unsupervised representa-            (Coates et al., 2011), or even random features (Huang et al.,
                    tion learning for natural language, we examine              2014; May et al., 2017). The approach fell out of favor as
                    whether similar models can learn useful repre-              the state of the art increasingly relied on directly encoding
                    sentations for images. We train a sequence Trans-           prior structure into the model and utilizing abundant su-
                    formertoauto-regressively predict pixels, without           pervised data to directly learn representations (Krizhevsky
                    incorporatingknowledgeofthe2Dinputstructure.                et al., 2012; Graves & Jaitly, 2014). Retrospective study of
                    Despitetrainingonlow-resolutionImageNetwith-                unsupervised pre-training demonstrated that it could even
                    outlabels, weﬁndthataGPT-2scalemodellearns                  hurt performance in modern settings (Paine et al., 2014).
                    strong image representations as measured by lin-            Instead, unsupervised pre-training ﬂourished in a differ-
                    ear probing, ﬁne-tuning, and low-data classiﬁca-            ent domain. After initial strong results for word vectors
                    tion. On CIFAR-10, we achieve 96.3% accuracy                (Mikolov et al., 2013), it has pushed the state of the art
                    with a linear probe, outperforming a supervised             forward in Natural Language Processing on most tasks (Dai
                    WideResNet,and99.0%accuracywithfullﬁne-                     &Le, 2015; Peters et al., 2018; Howard & Ruder, 2018;
                    tuning, matching the top supervised pre-trained             Radford et al., 2018; Devlin et al., 2018). Interestingly, the
                    models. An even larger model trained on a mix-              training objective of a dominant approach like BERT, the
                    ture of ImageNet and web images is competitive              prediction of corrupted inputs, closely resembles that of the
                    with self-supervised benchmarks on ImageNet,                Denoising Autoencoder, which was originally developed for
                    achieving 72.0% top-1 accuracy on a linear probe            images.
                    of our features.                                            Asahigherdimensional,noisier,andmoreredundantmodal-
                                                                                ity than text, images are believed to be difﬁcult for genera-
               1. Introduction                                                  tive modeling. Here, self-supervised approaches designed to
                                                                                encourage the modeling of more global structure (Doersch
               Unsupervised pre-training played a central role in the resur-    et al., 2015) have shown signiﬁcant promise. A combination
               gence of deep learning. Starting in the mid 2000’s, ap-          of new training objectives (Oord et al., 2018), more recent
               proaches such as the Deep Belief Network (Hinton et al.,         architectures (Gomez et al., 2017), and increased model ca-
               2006) and Denoising Autoencoder (Vincent et al., 2008)           pacity (Kolesnikov et al., 2019) has allowed these methods
               were commonly used in neural networks for computer vi-           to achieve state of the art performance in low data settings
                                                                                   ´
               sion (Lee et al., 2009) and speech recognition (Mohamed          (Henaff et al., 2019) and sometimes even outperform super-
               et al., 2009). It was believed that a model which learned        vised representations in transfer learning settings (He et al.,
               the data distribution P(X) would also learn beneﬁcial fea-       2019; Misra & van der Maaten, 2019; Chen et al., 2020).
               tures for the subsequent supervised modeling of P(Y |X)          Given that it has been a decade since the original wave of
               (Lasserreetal.,2006;Erhanetal.,2010). However,advance-           generative pre-training methods for images and considering
               ments such as piecewise linear activation functions (Nair        their substantial impact in NLP, this class of methods is due
               &Hinton, 2010), improved initializations (Glorot & Ben-          for a modern re-examination and comparison with the recent
               gio, 2010), and normalization strategies (Ioffe & Szegedy,       progress of self-supervised methods. We re-evaluate genera-
               2015; Ba et al., 2016) removed the need for pre-training in      tive pre-training on images and demonstrate that when using
               order to achieve strong results. Other research cast doubt       a ﬂexible architecture (Vaswani et al., 2017), a tractable and
               on the beneﬁts of deep unsupervised representations and re-      efﬁcient likelihood based training objective (Larochelle &
                  1OpenAI, San Francisco, CA, USA. Correspondence to: Mark      Murray, 2011; Oord et al., 2016), and signiﬁcant compute
               Chen<mark@openai.com>.                                           resources (2048 TPU cores), generative pre-training is com-
                                                                                petitive with other self-supervised approaches and learns
                                                              Generative Pretraining from Pixels
               Figure 1. An overview of our approach. First, we pre-process raw images by resizing to a low resolution and reshaping into a 1D sequence.
               Wethenchoseoneoftwopre-training objectives, auto-regressive next pixel prediction or masked pixel prediction. Finally, we evaluate
               the representations learned by these objectives with linear probes or ﬁne-tuning.
               representations that signiﬁcantly improve the state of the          for the downstream task rather than because of better pre-
               art in low-resolution unsupervised representation learning          training.
               settings.                                                           Webegin this section by deﬁning the auto-regressive and
               This is especially promising as our architecture uses a dense       BERTobjectives in the context of images. Next, we outline
               connectivity pattern which does not encode the 2D spatial           implementation details for our transformer decoder. Finally,
               structure of images yet is able to match and even outperform        wedescribe how the transformer is used for ﬁne-tuning and
               approacheswhichdo. Wereportasetofexperimentscharac-                 howfeatures are extracted for linear probes.
               terizing the performance of our approach on many datasets           2.1. Pre-training
               and in several different evaluation settings (low data, linear
               evaluation, full ﬁne-tuning). We also conduct several exper-        Given an unlabeled dataset X consisting of high dimen-
               iments designed to better understand the achieved perfor-           sional data x = (x1,...,xn), we can pick a permutation π
               manceofthese models. We investigate how representations             of the set [1,n] and model the density p(x) auto-regressively
               are computedinsideourmodelviatheperformanceoflinear                 as follows:
               probes as a function of model depth as well as studying how                               n
               scaling the resolution and parameter count of the approach                      p(x) = Yp(xπ |xπ ,...,xπ          , θ)
                                                                                                                 i    1       i−1
               affects performance.                                                                     i=1
               2. Approach                                                         Whenworkingwithimages,wepicktheidentitypermuta-
                                                                                   tion πi = i for 1 ≤ i ≤ n, also known as raster order. We
               Our approach consists of a pre-training stage followed by           train our model by minimizing the negative log-likelihood
               a ﬁne-tuning stage. In pre-training, we explore both the            of the data:
               auto-regressive and BERT objectives. We also apply the                                LAR = E [−logp(x)]
               sequence Transformer architecture to predict pixels instead                                    x∼X
               of language tokens.                                                 We also consider the BERT objective, which samples a
               Onewaytomeasurerepresentationqualityistoﬁne-tunefor                 sub-sequence M ⊂ [1,n] such that each index i indepen-
               image classiﬁcation. Fine-tuning adds a small classiﬁcation         dently has probability 0.15 of appearing in M. We call M
               headtothemodel,usedtooptimizeaclassiﬁcationobjective                the BERT mask, and we train our model by minimizing
               and adapts all weights. Pre-training can be viewed as a             the negative log-likelihood of the “masked” elements xM
               favorable initialization or as a regularizer when used in           conditioned on the “unmasked” ones x[1,n]\M:
               combination with early stopping (Erhan et al., 2010).                     LBERT = E E X−logp xi|x[1,n]\M
               Another approach for measuring representation quality uses                            x∼XMi∈M
               the pre-trained model as a feature extractor. In particular,        In pre-training, we pick one of LAR or LBERT and mini-
               given labeled examples (X,Y), the model is applied to X             mize the loss over our pre-training dataset.
               to produce features fX. Then, a linear classiﬁer is trained         2.2. Architecture
               on (fX,Y). Linear probing captures the intuition that good
               features should linearly separate the classes of transfer tasks.    Thetransformer decoder takes an input sequence x ,...,x
                                                                                                                                         1      n
               Furthermore, linear probes help disentangle feature quality         of discrete tokens and produces a d-dimensional embedding
               from model architecture: in ﬁne-tuning, one model may               for each position. The decoder is realized as a stack of
               outperform another because its architecture is more suited          Lblocks, the l-th of which produces an intermediate em-
                                                                                   bedding hl,...,hl also of dimension d. We use the GPT-2
                                                                                              1      n
                                                            Generative Pretraining from Pixels
               (Radford et al., 2019) formulation of the transformer de-       always at the ﬁnal layer:
                                                            l
               coder block, which acts on an input tensor h as follows:
                                                                                                        fl = hnli
                              l                 l                                                               i i
                            n =layer norm(h )
                              l    l                          l                where 0 ≤ l ≤ L. We will show in the experiments section
                            a =h +multihead attention(n )
                           l+1     l                      l                    that the best features often lie in the middle of the network.
                          h    =a +mlp(layer norm(a ))                         As in ﬁne-tuning, we project these intermediate features
               In particular, layer norms precede both the attention and       to produce class logits. Because we view the features as
               mlp operations, and all operations lie strictly on residual     ﬁxed when linear probing, this projection contains the only
               paths. We ﬁnd that such a formulation allows us to scale the    trainable weights, so we can only optimize LCLF.
               transformer with ease.                                          3. Methodology
               The only mixing across sequence elements occurs in the
               attention operation, and to ensure proper conditioning when     Although supervised pre-training is the dominant paradigm
               training the AR objective, we apply the standard upper          for image classiﬁcation, curating large labeled image
               triangular mask to the n×n matrix of attention logits. When     datasets is both expensive and time consuming. Instead
               using the BERT objective, no attention logit masking is         of further scaling up labeling efforts, we can instead as-
               required: after applying content embeddings to the input        pire to learn general purpose representations from the much
               sequence, we zero out the positions in M.                       larger set of available unlabeled images and ﬁne-tune them
               Additionally, since we learn independent position embed-        for classiﬁcation. We investigate this setting using Ima-
               dings for each sequence element, our BERT model has no          geNet as a proxy for a large unlabeled corpus, and small
               positional inductive biases (i.e. it is permutation invariant). classic labeled datasets (CIFAR-10, CIFAR-100, STL-10)
               Put another way, any spatial relationships between posi-        as proxies for downstream tasks. For our largest model, we
               tions must be learned by the model at train time. This is       use an additional 100 million unlabeled web images, ﬁltered
               not entirely true for the AR model, as choosing the raster      to be similar to ImageNet.
               order also ﬁxes a prespeciﬁed ordering of the condition-        Even in cases where labels are available, unsupervised or
               als. Nevertheless, permutation invariance is a property in      self-supervised pre-training can still provide beneﬁts in data
               strong contrast to convolutional neural networks, which in-     efﬁciency or on ﬁne-tuning speed. We investigate this set-
               corporate the inductive bias that features should arise from    ting by pre-training without labels and then ﬁne-tuning or
               spatially proximate elements.                                   linear probing with labels.
               Following the ﬁnal transformer layer, we apply a layer norm     3.1. Dataset and Data Augmentation
                L                  L                                  L
               n =layer norm(h ), and learn a projection from n to             WeusetheImageNetILSVRC2012trainingdataset,split-
               logits parameterizing the conditional distributions at each     ting off 4% as our experimental validation set and report
               sequence element. When training BERT, we simply ignore          results on the ILSVRC 2012 validation set as our test set.
               the logits at unmasked positions.                               For CIFAR-10, CIFAR-100 and STL-10, we split off 10%
               2.3. Fine-tuning                                                of the provided training set instead. We ignore the provided
               Whenﬁne-tuning, we average pool nL across the sequence          unlabeled examples in STL-10, which constitute a subset of
               dimension to extract a d-dimensional vector of features per     ImageNet.
               example:                                                        No data augmentation is used when pre-training on web
                                        L       L                              images, and lightweight data augmentation is used when
                                       f  =hn ii
                                                i                              pre-training or ﬁne-tuning on ImageNet. Speciﬁcally, when
               Welearnaprojection from fL to class logits, which we use
               to minimize a cross entropy loss L      .                       employingdataaugmentation, werandomlyresizeanimage
                                                  CLF                          such that the shorter sidelength is in the range [256,384]
               While ﬁne-tuning on LCLF yields reasonable downstream           and then take a random 224 × 224 crop. When evaluating
               performance, we ﬁnd empirically that the joint objective        on ImageNet, we resize the image such that the shorter
                                     LGEN +LCLF                                sidelength is 224, and use the single 224 × 224 center crop.
               L      ∈{L ,L            } works even better. Similar ﬁnd-      Whenfull-network ﬁne-tuning on CIFAR-10 and CIFAR-
                GEN         AR BERT                                            100, weusetheaugmentationpopularizedbyWideResidual
               ings were reported by Radford et al. (2018).                    Networks: 4 pixels are reﬂection padded on each side, and
               2.4. Linear Probing                                             a 32×32cropisrandomlysampledfromthepaddedimage
               Extracting ﬁxed features for linear probing follows a similar   or its horizontal ﬂip (Zagoruyko & Komodakis, 2016).
               procedure to ﬁne-tuning, except that average pooling is not     Once optimal hyperparameters are found, we fold our ex-
                                                             Generative Pretraining from Pixels
               perimental validation set back into the training set, retrain     a cosine schedule. No dropout is used.
               the model, and report numbers on the respective test set.         Whenﬁne-tuning, we use the same batch size and Adam
               3.2. Context Reduction                                            hyperparameters. Here, we do not employ a cosine sched-
               Because the memory requirements of the transformer de-            ule, and early stop once we reach the maximum validation
               coder scale quadratically with context length when using          accuracy. Again, no dropout is used.
               dense attention, we must employ further techniques to re-         WhenrunningalinearprobeonImageNet,wefollowrecent
               duce context length. If we naively trained a transformer on       literature and use SGD with momentum 0.9 and a high
                                        2                                        learning rate (we try the values 30, 10, 3, ... in the manner
               a sequence of length 224 ×3, our attention logits would be        described above) (He et al., 2019). We train for 1000000
               tens of thousands of times larger than those used in language
               models and even a single layer would not ﬁt on a GPU. To          iterations with a cosine learning rate schedule. Finally, when
               deal with this, we ﬁrst resize our image to a lower resolution,   running a linear probe on CIFAR-10, CIFAR-100, or STL-
               which we call the input resolution (IR). Our models have          10, we use the L-BFGS algorithm for consistency with prior
                               2         2           2                           results (Pedregosa et al., 2011).
               IRs of either 32 × 3, 48 × 3, or 64 × 3.
               An IR of 322 × 3 is still quite computationally intensive.        4. Experiments and Results
               While working at even lower resolutions is tempting, prior
               workhasdemonstratedhumanperformanceonimageclassi-                 We begin with experiments and results from the autore-
               ﬁcationbeginstodroprapidlybelowthissize(Torralbaetal.,            gressive formulation of iGPT. Comparisons with the BERT
               2008). Instead, motivated by early color display palettes,        formulation appear in Section 4.6.
               wecreate our own 9-bit color palette by clustering (R, G,         4.1. What Representation Works Best in a Generative
               B) pixel values using k-means with k = 512. Using this                ModelWithoutLatentVariables?
               palette yields an input sequence length 3 times shorter than
               the standard (R, G, B) palette, while still encoding color
               faithfully. A similar approach was applied to spatial patches
               byRanzatoetal.(2014). Wecalltheresultingcontextlength
                  2       2      2
               (32 or 48 or 64 ) the model resolution (MR). Note that
               this reduction breaks permutation invariance of the color
               channels, but keeps the model spatially invariant.
               3.3. Model
               Our largest model, iGPT-XL, contains L = 60 layers and
               uses an embedding size of d = 3072 for a total of 6.8B pa-
               rameters. Our next largest model, iGPT-L, is essentially          Figure 2. Representation quality depends on the layer from which
               identical to GPT-2 with L = 48 layers, but contains a             weextract features. In contrast with supervised models, the best
               slightly smaller embedding size of d = 1536 (vs 1600)             representations for these generative models lie in the middle of the
               for a total of 1.4M parameters. We use the same model             network. We plot this unimodal dependence on depth by showing
               codeasGPT-2,exceptthatweinitialize weights in the layer-          linear probes for iGPT-L on CIFAR-10, CIFAR-100, and STL-10.
               dependent fashion as in Sparse Transformer (Child et al.,
               2019) and zero-initialize all projections producing logits.       In supervised pre-training, representation quality tends to
               Wealsotrain iGPT-M, a 455M parameter model with L =               increase monotonically with depth, such that the best rep-
               36andd=1024andiGPT-S,a76Mparametermodelwith                       resentations lie at the penultimate layer (Zeiler & Fergus,
               L=24andd=512tostudytheeffectofmodelcapacity                       2014). Indeed, since a linear layer produces class logits
               onrepresentation quality in a generative model.                   from pre-logits, a good classiﬁer necessarily achieves high
                                                                                 accuracy on a linear probe of its pre-logits. If a downstream
               3.4. Training                                                     task also involves classiﬁcation, it is empirically validated
               Whenpre-training iGPT-XL, we use a batch size of 64 and           that penultimate features perform well.
               train for 2M iterations, and for all other models we use          Withgenerative pre-training, it is not obvious whether a task
               a batch size of 128 and train for 1M iterations. We use           like pixel prediction is relevant to image classiﬁcation. This
               Adamwithβ1 = 0.9andβ2 = 0.95andsequentiallytrythe                 suggests that the penultimate layer of a model trained for
               learning rates 0.01, 0.003, 0.001, 0.0003, ..., stopping once     pixel prediction might not produce the most useful repre-
               the ﬁnal validation loss starts increasing. The learning rate     sentations for classiﬁcation. Latent variable models such as
               is warmed up for one epoch, and then decays to 0 following        VAEscanavoidthisissue by explicitly learning a represen-
                                                                                 tation of the input data, but deep autoregressive generative
                                                                    Generative Pretraining from Pixels
                 models have the same width and connectivity pattern at                   Table 1. Comparing linear probe accuracies between our models
                 every layer. Our ﬁrst experiment studies how representa-                 and state-of-the-art models utilizing unsupervised ImageNet trans-
                 tion quality varies over one set of candidate representations:           fer or supervised ImageNet transfer.
                 different layers of a generative model. We observe a very
                 different behavior from supervised learning: representations                  Model            Acc     UnsupTransfer      SupTransfer
                 ﬁrst improveasafunctionofdepth,andthen,startingaround                         CIFAR-10
                 the middle layer, begin to deteriorate until the penultimate                  ResNet-152       94             √                 √
                 layer (Figure 2).                                                             SimCLR           95.3
                 Thisbehaviorpotentiallysuggeststhatthesegenerativemod-                        iGPT-L           96.3           √
                 els operate in two phases. In the ﬁrst phase, each position                   CIFAR-100
                 gathers information from its surrounding context in order                     ResNet-152       78.0           √                 √
                 to build a more global image representation. In the second                    SimCLR           80.2           √
                 phase, this contextualized input is used to solve the condi-                  iGPT-L           82.8
                 tional next pixel prediction task. This could resemble the                    STL-10
                 behavior of encoder-decoder architectures common across                       AMDIM-L          94.2           √
                 deep learning, but learned within a monolithic architecture                   iGPT-L           95.5           √
                 via a pre-training objective.
                 Consequently, when evaluating a generative model with                    several model capacities, with higher capacity models
                 a linear probe, it is important to search for the best layer.            achieving better validation losses. This highlights the im-
                 Taking the ﬁnal layer on CIFAR-10 decreases performance                  portance of scale for our approach. Note that for a given
                 by 2.4%, the difference between a baseline and a state-of-               validation loss value, bigger models also perform better.
                 the-art result. In all settings, we ﬁnd that the dependence of
                 representation quality on depth is strongly unimodal.                    4.3. Linear Probes on CIFAR and STL-10
                 4.2. Better Generative Models Learn Better                               In addition to CIFAR-10, we also evaluate linear probes on
                      Representations                                                     CIFAR-100 and STL-10 (Figure 2) to check whether the
                                                                                          learned representations are useful across multiple datasets.
                                                                                          Forthisevaluationsetting, we achieve state-of-the-art across
                                                                                          the entire spectrum of pre-training approaches (Table 1).
                                                                                          For example, on CIFAR-10, our model achieves 96.3%, out-
                                                                                          performing both SimCLR (pre-trained on ImageNet without
                                                                                          labels) and a ResNet-152 (pre-trained on ImageNet with
                                                                                          labels). In fact, on all three datasets a linear classiﬁer ﬁt to
                                                                                          the representations of iGPT-L outperforms the end-to-end
                                                                                          supervised training of a WideResNet baseline.
                                                                                          Note that our model is trained at the same input resolution
                Figure 3. Plot of representation quality as a function of validation      (IR) as CIFAR, whereas models trained at the standard Im-
                 generative loss. Each line tracks a model throughout generative          ageNet IR may experience distribution shock upon linear
                 pre-training: the dotted markers denote checkpoints at steps 65K,        evaluation. As a counterpoint, though STL-10 has an IR
                                                                                                2
                131K, 262K, 524K, and 1000K. The positive slope suggests a link           of 96 ×3, we still outperform AMDIM-L when we down-
                 between improved generative performance and improved represen-           sample to 322 × 3 before linear probing. We also note that
                 tation quality. Larger models produce better representations than        ﬁne-tuning should allow models trained at high IR to adjust
                 smaller ones both at the end of training and at the same value of        to low resolution input.
                 validation loss. iGPT-XL is not shown since it was trained on a
                 different dataset.                                                       4.4. Linear Probes on ImageNet
                 Usingthelinearprobeasatoolformeasuringrepresentation                     Recently, there has been a resurgence of interest in unsuper-
                 quality, we investigate whether better generative models (as             vised and self-supervised learning on ImageNet, evaluated
                 measured by log-prob on held-out data) also learn better                 using linear probes on ImageNet. This is a particularly
                 representations.                                                         difﬁcult setting for us, since we cannot efﬁciently train at
                                                                                          the standard ImageNet input resolution (IR). Indeed, when
                                                                                                                                                        2
                 In Figure 3, we see that as validation loss on the auto-                 training iGPT-L with a model resolution (MR) of 32 , we
                 regressive objective decreases throughout training, linear               achieve only 60.3% best-layer linear probe accuracy. As
                 probe accuracy increases as well. This trend holds across                with CIFAR-10, scale is critical to our approach: iGPT-
                                                                  Generative Pretraining from Pixels
                Table 2. Comparing linear probe accuracies between our models          Table 3. Comparing ﬁne-tuning performance between our models
                and state-of-the-art self-supervised models. A blank input resolu-     and state-of-the-art models utilizing supervised ImageNet transfer.
                tion (IR) corresponds to a model working at standard ImageNet          Wealsoinclude AutoAugment, the best performing model trained
                resolution. We report the best performing conﬁguration for each        end-to-end on CIFAR. Table results: AutoAugment (Cubuk et al.,
                contrastive method, ﬁnding that our models achieve comparable          2019), SimCLR (Chen et al., 2020), GPipe (Huang et al., 2019),
                performance.                                                           EfﬁcentNet (Tan & Le, 2019)
                     Method           IR     Params (M)      Features    Acc               Model             Acc     UnsupTransfer      SupTransfer
                     Rotation        orig.        86          8192      55.4               CIFAR-10
                                      2                                                    AutoAugment       98.5
                     iGPT-L         32 ·3        1362         1536      60.3               SimCLR            98.6          √
                     BigBiGAN        orig.        86          8192      61.3                                                                 √
                                      2                                                    GPipe             99.0
                     iGPT-L         48 ·3        1362         1536      65.2               iGPT-L            99.0          √
                     AMDIM           orig.       626          8192      68.1
                     MoCo            orig.       375          8192      68.6
                                      2                                                    CIFAR-100
                     iGPT-XL        64 ·3        6801         3072      68.7               iGPT-L            88.5          √
                     SimCLR          orig.        24          2048      69.3               SimCLR            89.0          √
                     CPCv2           orig.       303          8192      71.5
                                      2                                                    AutoAugment       89.3
                     iGPT-XL        64 ·3        6801         15360     72.0               EfﬁcientNet       91.7                            √
                     SimCLR          orig.       375          8192      76.5
                Machieves 54.5% accuracy and iGPT-S achieves 41.9%                     onthese datasets, though we do not use sophisticated data
                accuracy.                                                              augmentationtechniques. In fact, 99.0% ties GPipe, the best
                                                                                       model which pre-trains using ImageNet labels.
                Theﬁrst obvious optimization is to increase MR while stay-             OnImageNet,weachieve66.3%accuracyafter ﬁne-tuning
                                                                                2
                ing within accelerator memory limits. With a MR of 48 ,                           2
                iGPT-L achieves a best-layer accuracy of 65.2% using 1536              at MR32 ,abumpof6%overlinearprobing. Whenﬁne-
                                                                                                         2
                features and with a MR of 642, iGPT-XL achieves a best-                tuning at MR 48 , we achieve 72.6% accuracy, with a simi-
                layer accuracy of 68.7% using 3072 features.                           lar 7% bump over linear probing. However, our models still
                                                                                       slightly underperform Isometric Neural Nets (Sandler et al.,
                Since contrastive methods report their best results on 8192            2019), which achieves 70.2% at an IR of 282 × 3.
                features, we would ideally evaluate iGPT with an embed-                Finally, as a baseline for ImageNet ﬁne-tuning, we train
                dingdimension8192forcomparison. Trainingsuchamodel                     the classiﬁcation objective from a random initialization. At
                is prohibitively expensive, so we instead concatenate fea-                     2
                tures from multiple layers as an approximation. However,               MR48 , a model with tuned learning rate and dropout
                our features tend to be correlated across layers, so we need           achieves 53.2% after 18 epochs, 19.4% worse than the pre-
                moreofthemtobecompetitive. If we concatenate features                  trained model. Comparatively, the pre-trained model is
                from 5 layers centered at the best single layer of iGPT-XL,            muchquickertoﬁne-tune, achieving the same 53.2% loss
                we achieve an accuracy of 72.0% using 15360 features,                  in roughly a single epoch.
                which is competitive with recent contrastive learning ap-              When ﬁne-tuning, it is important to search over learning
                proaches (Table 2). Note that we require more parameters               rates again, as the optimal learning rate on the joint training
                and compute to achieve this accuracy, but we work at low               objective is often an order of magnitude smaller than that
                resolution and without utilizing knowledge of the 2D input             for pre-training. We also tried regularizing with dropout,
                structure.                                                             though we did not observe any clear beneﬁts. It is easy to
                4.5. Full Fine-tuning                                                  overﬁt the classiﬁcation objective on small datasets, so we
                                                                                       employ early stopping based on validation accuracy.
                Toachieve even higher accuracy on downstream tasks, we                 4.6. BERT
                adapt the entire model for classiﬁcation through ﬁne-tuning.
                Building off of the previous analysis, we tried attaching the          Given the success of BERT in language, we train iGPT-L
                classiﬁcation head to the layer with the best representations.         at an input resolution of 322 × 3 and a model resolution
                                                                                             2
                Though this setup trains faster than one with the head at-             of 32 (Figure 4). On CIFAR-10, we observe that linear
                tached at the end, the latter is able to leverage greater model        probe accuracy at every layer is worse than that of the auto-
                depth and eventually outperforms.                                      regressive model, with best-layer performance more than
                On CIFAR-10, iGPT-L achieves 99.0% accuracy and on                     1%lower. Best-layer accuracy on ImageNet is 6% lower.
                CIFAR-100, it achieves 88.5% accuracy after ﬁne-tuning.                However, during ﬁne-tuning, BERT makes up much of this
                Weoutperform AutoAugment, the best supervised model                    gap. A fully ﬁne-tuned CIFAR-10 model achieves 98.6%
                                                          Generative Pretraining from Pixels
                                                                             Table 4. Comparing performance on low-data CIFAR-10. By lever-
                                                                             aging many unlabeled ImageNet images, iGPT-L is able to outper-
                                                                             form methods such as Mean Teacher (Tarvainen & Valpola, 2017)
                                                                             and MixMatch (Berthelot et al., 2019) but still underperforms the
                                                                             state of the art methods (Xie et al., 2019; Sohn et al., 2020). Our
                                                                             approach to semi-supervised learning is very simple since we only
                                                                             ﬁt a logistic regression classiﬁer on iGPT-L’s features without any
                                                                             data augmentationorﬁne-tuning-asigniﬁcantdifferencefromspe-
                                                                             cially designed semi-supervised approaches. Other results reported
                                                                             from FixMatch (Sohn et al., 2020).
                                                                                Model             40labels     250 labels  4000 labels
              Figure 4. Comparison of auto-regressive pre-training with BERT    MeanTeacher                   32.3 ± 2.3    9.2 ± 0.2
                                                              2                 MixMatch         47.5 ± 11.5  11.0 ± 0.9    6.4 ± 0.1
              pre-training using iGPT-L at an input resolution of 32 × 3. Blue  iGPT-L           26.8 ± 1.5   12.4 ± 0.6    5.7 ± 0.1
              bars display linear probe accuracy and orange bars display ﬁne-   UDA              29.0 ± 5.9    8.8 ± 1.1    4.9 ± 0.2
              tune accuracy. Bold colors show the performance boost from        FixMatch RA      13.8 ± 3.4    5.1 ± 0.7    4.3 ± 0.1
              ensembling BERT masks. We see that auto-regressive models         FixMatch CTA     11.4 ± 3.4    5.1 ± 0.3    4.3 ± 0.2
              produce much better features than BERT models after pre-training,
              but BERTmodelscatchupafter ﬁne-tuning.
                                                                             Asisstandard in the low-data setting, we sample 5 random
                                                                             subsets and report mean and standard deviation accuracies
                                                                             (Table 4). OnCIFAR-10,weﬁndthatwith4labelsperclass,
              accuracy, only 0.4% behind its auto-regressive counterpart,    weachieve 73.2% accuracy outperforming MixMatch with
              while a fully ﬁne-tuned ImageNet model achieves 66.5%,         muchlowervariance between runs and with 25 labels per
              slightly surpassing auto-regressive performance.               class, we achieve 87.6% accuracy, though still signiﬁcantly
              Finally, because inputs to the BERT model are masked at        lower than the state of the art, FixMatch.
              training time, we must also mask them at evaluation time to    Although we have established that large models are neces-
              keep inputs in-distribution. This masking corruption may       sary for producing good representations, large models are
              hinder the BERT model’s ability to correctly predict image     also difﬁcult to ﬁne-tune in the ultra-low data regime. In-
              classes. Therefore, we also try an evaluation scheme where     deed, we ﬁnd that iGPT-L quickly memorizes a 40-example
              wesample5independentmasksforeachinputandtakethe                training set and fails to generalize well, achieving only
              modal prediction, breaking ties at random. In this setting,    42.1% accuracy. We expect adapting recent approaches
              CIFAR-10results are largely unchanged, but on ImageNet,        to semi-supervised learning will help in this setting.
              wegainalmost1%onourlinearprobesandﬁne-tunes.
              4.7. Low-Data CIFAR-10 Classiﬁcation                           5. Related Work
              Evaluations of unsupervised representations often reuse su-    Many generative models have been developed and evalu-
              pervised learning datasets which have thousands to millions    ated for their representation learning capabilities. Notably,
              of labeled examples. However, a representation which has       GANs(Goodfellowetal., 2014; Radford et al., 2015; Don-
              robustly encoded a semantic concept should be exceedingly      ahue et al., 2016) and VAEs (Kingma & Welling, 2013;
              data efﬁcient. As inspiration, we note that humans are able    Kingmaetal., 2014; Higgins et al., 2017) have been well-
              to reliably recognize even novel concepts with a single ex-    studied.
              ample (Carey and Bartlett 1978). This motivates evaluating     As of yet, most generative model based approaches have
              performance in a low-data regime as well. It is also a more    not been competitive with supervised and self-supervised
              realistic evaluation setting for the potential practical use-  methods in the image domain. A notable exception is Big-
              fulness of an approach since it better matches the common      BiGAN(Donahue&Simonyan,2019)whichﬁrstdemon-
              real-world scenario of an abundance of raw data but a lack     strated that sufﬁciently high ﬁdelity generative models learn
              of labels.                                                     imagerepresentations which are competitive with other self-
              In contrast with recent approaches for low-data classiﬁca-     supervised methods.
              tion, we do not make use of pseudo-labeling or data aug-       Manyself-supervised approaches focus on designing aux-
              mentation. Instead, we work directly on a subset of the raw    iliary objectives which support the learning of useful rep-
              supervised dataset, extracting features using our pre-trained  resentations without attempting to directly model the input
              model, and training a linear classiﬁer on those features.      data. Examples include surrogate classiﬁcation (Dosovit-
                                                            Generative Pretraining from Pixels
               skiy et al., 2015), jigsaw puzzle solving (Noroozi & Favaro,    However, our experiments also demonstrate several areas
               2016), and rotation prediction (Gidaris et al., 2018). A clus-  for improvement. We currently model low resolution in-
               ter of similar approaches based on contrastive losses com-      puts with self-attention. By comparison, most other self-
               paring various views and transformations of input images        supervised results use CNN based encoders that easily work
               have recently driven signiﬁcant progress in self-supervised     with high resolution images. It is not immediately obvious
               learning (Hjelm et al., 2018; Bachman et al., 2019; Tian        howtobestbridge the gap between performant autoregres-
               et al., 2019).                                                  sive and discriminative models. Additionally, we observed
               Among contrastive approaches, our work is most similar          that our approach requires large models in order to learn
               to Contrast Predictive Coding (Oord et al., 2018) which         high quality representations. iGPT-L has 2 to 3 times as
               also utilizes a autoregressive prediction objective, but in a   many parameters as similarly performing models on Ima-
               learned latent space, and to Selﬁe (Trinh et al., 2019) which   geNet and uses more compute.
               trains a bidirectional self-attention architecture on top of a  Although dense self-attention was a deliberate choice for
               standard convolutional network to differentiate correct vs      this work due to it being domain agnostic and widely used in
               wrongpatches.                                                   NLP,it becomes very memory and computationally expen-
               Ourworkisdirectly inspired by the success of generative         sive due to its quadratic scaling with sequence length. We
               pre-training methods developed for Natural Language Pro-        mitigated this via the context reduction techniques discussed
               cessing. These methods predict some parts of a piece of text    in section 3.2 but it is still a signiﬁcant limitation. Future
               conditioned on other parts. Our work explores two training      workcould instead address this via architectural changes by
               objectives in this framework, autoregressive prediction as      exploring more efﬁcient self-attention approaches. Several
               originally explored for modern neural sequence models by        promising techniques have recently been developed such as
               Dai&Le(2015),andadenoisingobjective,similartoBERT               local 2D relative attention (Bello et al., 2019; Ramachan-
               (Devlin et al., 2018). The context in-painting approach of      dran et al., 2019), sparse attention patterns (Child et al.,
               Pathak et al. (2016) also explores pre-training by predict-     2019), locality sensitive hashing (Kitaev et al., 2020), and
               ing corruptions but predicts large regions of high-resolution   multiscale modeling (Menick & Kalchbrenner, 2018).
               images.                                                         Finally, our results, considered together with Donahue &
               Kolesnikov et al. (2019); Goyal et al. (2019) conducted         Simonyan(2019),suggestrevisitingtherepresentationlearn-
               rigorous investigations of existing self-supervised methods.    ing capabilities of other families of generative models such
               Several of our ﬁndings are consistent with their results, in-   as ﬂows (Dinh et al., 2014; Kingma & Dhariwal, 2018)
               cluding the beneﬁts of scale and the non-monotonic perfor-      and VAEs in order to study whether they show similarly
               manceofrepresentations with depth in certain architectures.     competitive representation learning capabilities.
               Expressive autoregressive models tractably optimizing like-     References
               lihood were ﬁrst applied to images by Uria et al. (2013)
               and popularized by Oord et al. (2016) serving for the ba-       Ba,J.L.,Kiros,J.R.,andHinton,G.E. Layernormalization.
               sis of several papers similarly adapting transformers to the       arXiv preprint arXiv:1607.06450, 2016.
               problem of generative image modeling (Parmar et al., 2018;      Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning
               Child et al., 2019).                                               representations by maximizing mutual information across
               Keetal.(2018)introduced the pixel-by-pixel CIFAR10 task            views. In Advances in Neural Information Processing
               and ﬁrst benchmarked the performance of a 1D sequence              Systems, pp. 15509–15519, 2019.
               transformer on a competitive image classiﬁcation dataset.       Bello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V.
               Rives et al. (2019) similarly investigates whether the recent      Attention augmented convolutional networks. In Proceed-
               success of unsupervised pre-training in NLP applies to other       ings of the IEEE International Conference on Computer
               domains, observing promising results on protein sequence           Vision, pp. 3286–3295, 2019.
               data.
               6. Discussion and Conclusion                                    Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N.,
                                                                                  Oliver, A., and Raffel, C. A.     Mixmatch: A holistic
               Ourresults suggest that generative image modeling contin-          approach to semi-supervised learning. In Advances in
               ues to be a promising route to learn high-quality unsuper-         Neural Information Processing Systems, pp. 5050–5060,
               vised image representations. Simply predicting pixels learns       2019.
               state of the art representations for low resolution datasets.   Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A
               In high resolution settings, our approach is also competitive      simple framework for contrastive learning of visual rep-
               with other self-supervised results on ImageNet.                    resentations. arXiv preprint arXiv:2002.05709, 2020.
                                                              Generative Pretraining from Pixels
               Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-           Gomez,A.N.,Ren,M.,Urtasun,R., and Grosse, R. B. The
                  erating long sequences with sparse transformers. arXiv             reversible residual network: Backpropagation without
                  preprint arXiv:1904.10509, 2019.                                   storing activations. In Advances in neural information
                                                                                     processing systems, pp. 2214–2224, 2017.
               Coates, A., Ng, A., and Lee, H. An analysis of single-
                  layer networks in unsupervised feature learning. In Pro-        Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
                  ceedings of the fourteenth international conference on            Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
                  artiﬁcial intelligence and statistics, pp. 215–223, 2011.         Y. Generative adversarial nets. In Advances in neural
                                                                                     information processing systems, pp. 2672–2680, 2014.
               Cubuk, E., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V.
                  Autoaugment: Learning augmentation strategies from              Goyal, P., Mahajan, D., Gupta, A., and Misra, I. Scaling and
                  data, 2019.                                                        benchmarking self-supervised visual representation learn-
                                                                                     ing. In Proceedings of the IEEE International Conference
               Dai, A. M.andLe,Q.V. Semi-supervisedsequencelearning.                 onComputerVision, pp. 6391–6400, 2019.
                  In Advances in neural information processing systems,           Graves, A. and Jaitly, N. Towards end-to-end speech recog-
                  pp. 3079–3087, 2015.                                               nition with recurrent neural networks. In International
               Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:            conference on machine learning, pp. 1764–1772, 2014.
                  Pre-training of deep bidirectional transformers for lan-        He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-
                  guage understanding. arXiv preprint arXiv:1810.04805,              mentumcontrast for unsupervised visual representation
                  2018.                                                              learning. arXiv preprint arXiv:1911.05722, 2019.
               Dinh, L., Krueger, D., and Bengio, Y. Nice: Non-linear               ´
                  independent components estimation.         arXiv preprint       Henaff, O. J., Razavi, A., Doersch, C., Eslami, S., and Oord,
                  arXiv:1410.8516, 2014.                                            A. v. d. Data-efﬁcient image recognition with contrastive
                                                                                     predictive coding.    arXiv preprint arXiv:1905.09272,
               Doersch, C., Gupta, A., and Efros, A. A. Unsupervised                 2019.
                  visual representation learning by context prediction. In        Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,
                  Proceedings of the IEEE International Conference on                Botvinick, M., Mohamed, S., and Lerchner, A. beta-
                  ComputerVision, pp. 1422–1430, 2015.                              vae: Learning basic visual concepts with a constrained
               Donahue, J. and Simonyan, K. Large scale adversarial rep-            variational framework. 2017.
                  resentation learning. In Advances in Neural Information         Hinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning
                  Processing Systems, pp. 10541–10551, 2019.                         algorithm for deep belief nets. Neural computation, 18
                                 ¨     ¨                                            (7):1527–1554, 2006.
               Donahue, J., Krahenbuhl, P., and Darrell, T. Adversarial
                  feature learning. arXiv preprint arXiv:1605.09782, 2016.        Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal,
               Dosovitskiy, A., Fischer, P., Springenberg, J. T., Riedmiller,        K., Bachman, P., Trischler, A., and Bengio, Y. Learning
                  M., and Brox, T. Discriminative unsupervised feature               deep representations by mutual information estimation
                  learning with exemplar convolutional neural networks.              and maximization. arXiv preprint arXiv:1808.06670,
                  IEEEtransactions on pattern analysis and machine intel-            2018.
                  ligence, 38(9):1734–1747, 2015.                                 Howard, J. and Ruder, S.        Universal language model
                                                                                     ﬁne-tuning for text classiﬁcation.        arXiv preprint
               Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vin-           arXiv:1801.06146, 2018.
                  cent, P., and Bengio, S. Why does unsupervised pre-
                  training help deep learning? Journal of Machine Learn-          Huang, P.-S., Avron, H., Sainath, T. N., Sindhwani, V., and
                  ing Research, 11(Feb):625–660, 2010.                               Ramabhadran, B. Kernel methods match deep neural net-
                                                                                    works on timit. In 2014 IEEE International Conference
               Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep-           on Acoustics, Speech and Signal Processing (ICASSP),
                  resentation learning by predicting image rotations. arXiv          pp. 205–209. IEEE, 2014.
                  preprint arXiv:1803.07728, 2018.
                                                                                  Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen,
               Glorot, X. and Bengio, Y. Understanding the difﬁculty                 M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe:
                  of training deep feedforward neural networks. In Pro-              Efﬁcient training of giant neural networks using pipeline
                  ceedings of the thirteenth international conference on             parallelism. In Advances in Neural Information Process-
                  artiﬁcial intelligence and statistics, pp. 249–256, 2010.          ing Systems, pp. 103–112, 2019.
                                                           Generative Pretraining from Pixels
               Ioffe, S. and Szegedy, C. Batch normalization: Accelerating    Menick,J.andKalchbrenner,N. Generatinghighﬁdelityim-
                 deep network training by reducing internal covariate shift.     ages with subscale pixel networks and multidimensional
                 arXiv preprint arXiv:1502.03167, 2015.                          upscaling. arXiv preprint arXiv:1812.01608, 2018.
               Ke, N. R., GOYAL, A. G. A. P., Bilaniuk, O., Binas, J.,        Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and
                 Mozer, M. C., Pal, C., and Bengio, Y. Sparse attentive          Dean,J. Distributed representations of words and phrases
                 backtracking: Temporal credit assignment through re-            and their compositionality. In Advances in neural infor-
                 minding. In Advances in neural information processing           mation processing systems, pp. 3111–3119, 2013.
                 systems, pp. 7640–7651, 2018.                                Misra, I. and van der Maaten, L. Self-supervised learn-
               Kingma, D. P. and Dhariwal, P. Glow: Generative ﬂow               ing of pretext-invariant representations. arXiv preprint
                 with invertible 1x1 convolutions. In Advances in Neural         arXiv:1912.01991, 2019.
                 Information Processing Systems, pp. 10215–10224, 2018.       Mohamed, A.-r., Dahl, G., and Hinton, G. Deep belief
               Kingma,D.P.andWelling,M. Auto-encoding variational                networks for phone recognition. 2009.
                 bayes. arXiv preprint arXiv:1312.6114, 2013.                 Nair, V. and Hinton, G. E. Rectiﬁed linear units improve
               Kingma,D.P., Mohamed,S., Rezende, D. J., and Welling,             restricted boltzmannmachines. InProceedingsofthe27th
                 M. Semi-supervised learning with deep generative mod-           international conference on machine learning (ICML-10),
                 els. In Advances in neural information processing sys-          pp. 807–814, 2010.
                 tems, pp. 3581–3589, 2014.                                   Noroozi, M. and Favaro, P. Unsupervised learning of visual
               Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The            representations by solving jigsaw puzzles. In European
                 efﬁcient transformer. arXiv preprint arXiv:2001.04451,          Conference on Computer Vision, pp. 69–84. Springer,
                 2020.                                                           2016.
                                                                              Oord, A. v. d., Kalchbrenner, N., and Kavukcuoglu,
               Kolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-          K. Pixel recurrent neural networks.      arXiv preprint
                 supervised visual representation learning. In Proceedings       arXiv:1601.06759, 2016.
                 of the IEEE conference on Computer Vision and Pattern        Oord, A. v. d., Li, Y., and Vinyals, O. Representation learn-
                 Recognition, pp. 1920–1929, 2019.                               ing with contrastive predictive coding. arXiv preprint
               Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet         arXiv:1807.03748, 2018.
                 classiﬁcation with deep convolutional neural networks.       Paine, T. L., Khorrami, P., Han, W., and Huang, T. S. An
                 In Advances in neural information processing systems,           analysis of unsupervised pre-training in light of recent
                 pp. 1097–1105, 2012.                                            advances. arXiv preprint arXiv:1412.6597, 2014.
               Larochelle, H. and Murray, I. The neural autoregressive        Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer,
                 distribution estimator. In Proceedings of the Fourteenth        N., Ku, A., and Tran, D. Image transformer. arXiv
                 International Conference on Artiﬁcial Intelligence and          preprint arXiv:1802.05751, 2018.
                 Statistics, pp. 29–37, 2011.
               Lasserre, J. A., Bishop, C. M., and Minka, T. P. Principled    Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and
                 hybrids of generative and discriminative models. In 2006        Efros, A. A. Context encoders: Feature learning by
                 IEEEComputerSociety Conference on Computer Vision               inpainting. In Proceedings of the IEEE conference on
                 andPattern Recognition (CVPR’06), volume 1, pp. 87–             computer vision and pattern recognition, pp. 2536–2544,
                 94. IEEE, 2006.                                                 2016.
                                                                              Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
               Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. Convo-          Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
                 lutional deep belief networks for scalable unsupervised         Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
                 learning of hierarchical representations. In Proceedings        napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
                 of the 26th annual international conference on machine          Scikit-learn: Machine learning in Python. Journal of
                 learning, pp. 609–616, 2009.                                    Machine Learning Research, 12:2825–2830, 2011.
               May,A.,Garakani, A. B., Lu, Z., Guo, D., Liu, K., Bellet,      Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,
                 A., Fan, L., Collins, M., Hsu, D., Kingsbury, B., et al.        C., Lee, K., and Zettlemoyer, L. Deep contextualized
                 Kernel approximation methods for speech recognition.            wordrepresentations. arXiv preprint arXiv:1802.05365,
                 arXiv preprint arXiv:1701.03577, 2017.                          2018.
                                                            Generative Pretraining from Pixels
               Radford, A., Metz, L., and Chintala, S. Unsupervised rep-       Trinh, T. H., Luong, M.-T., and Le, Q. V. Selﬁe: Self-
                 resentation learning with deep convolutional generative          supervised pretraining for image embedding.         arXiv
                 adversarial networks. arXiv preprint arXiv:1511.06434,           preprint arXiv:1906.02940, 2019.
                 2015.                                                         Uria, B., Murray, I., and Larochelle, H. Rnade: The real-
               Radford, A., Narasimhan, K., Salimans, T., and Sutskever,          valued neural autoregressive density-estimator. In Ad-
                 I. Improving language understanding by generative pre-           vances in Neural Information Processing Systems, pp.
                 training. 2018.                                                  2175–2183, 2013.
               Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and       Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
                 Sutskever, I. Languagemodelsareunsupervisedmultitask             L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
                 learners. 2019.                                                  tion is all you need. In Advances in neural information
                                                                                  processing systems, pp. 5998–6008, 2017.
               Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev-      Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A.
                 skaya, A., and Shlens, J. Stand-alone self-attention in          Extracting and composing robust features with denoising
                 vision models. arXiv preprint arXiv:1906.05909, 2019.            autoencoders. In Proceedings of the 25th international
               Ranzato, M., Szlam, A., Bruna, J., Mathieu, M., Collobert,         conference on Machine learning, pp. 1096–1103, 2008.
                 R., and Chopra, S. Video (language) modeling: a baseline      Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le,
                 for generative models of natural videos. arXiv preprint          Q. V. Unsupervised data augmentation. arXiv preprint
                 arXiv:1412.6604, 2014.                                           arXiv:1904.12848, 2019.
               Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick,     Zagoruyko, S. and Komodakis, N. Wide residual networks.
                 C.L.,Ma,J.,andFergus,R. Biologicalstructureandfunc-              arXiv preprint arXiv:1605.07146, 2016.
                 tion emerge from scaling unsupervised learning to 250         Zeiler, M. D. and Fergus, R. Visualizing and understand-
                 million protein sequences. bioRxiv, pp. 622803, 2019.            ing convolutional networks. In European conference on
               Sandler, M., Baccash, J., Zhmoginov, A., and Howard, A.            computer vision, pp. 818–833. Springer, 2014.
                 Non-discriminative data or weak model? on the relative
                 importance of data and model resolution. In Proceedings       A. Experimental details
                 of the IEEEInternationalConferenceonComputerVision
                 Workshops, pp. 0–0, 2019.                                     A.1. Hyperparameters
               Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N.,     In Table 5, we present the learning rates used to train each
                 Cubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C. Fix-     model in the paper. When using too high a learning rate,
                 match: Simplifying semi-supervised learning with consis-      weobserveanirrecoverable loss spike early on in training.
                 tency and conﬁdence. arXiv preprint arXiv:2001.07685,         Conversely, with too low a learning rate, training is stable
                 2020.                                                         but loss improves slowly and eventually underperforms. As
                                                                               weincrease model size, the irrecoverable loss spike occurs
               Tan, M. and Le, Q. V. Efﬁcientnet: Rethinking model             at even lower learning rates. This motivates our procedure
                 scaling for convolutional neural networks. arXiv preprint     of sequentially searching learning rates from large to small
                 arXiv:1905.11946, 2019.                                       and explains why larger models use lower learning rates
                                                                               than smaller models at ﬁxed input resolution.
               Tarvainen, A. and Valpola, H. Mean teachers are better role     WeusedanAdamβ of0.95instead of the default 0.999
                 models: Weight-averaged consistency targets improve                                 2
                 semi-supervised deep learning results. In Advances in         because the latter causes loss spikes during training. We
                 neural information processing systems, pp. 1195–1204,         did not use weight decay because applying a small weight
                 2017.                                                         decay of 0.01 did not change representation quality.
               Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview     OniGPT-S,wefoundsmallgainsinrepresentation quality
                 coding. arXiv preprint arXiv:1906.05849, 2019.                fromusingﬂoat32insteadofﬂoat16,fromuntyingthetoken
                                                                               embedding matrix and the matrix producing token logits,
               Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny    and from zero initializing the matrices producing token and
                 images: A large data set for nonparametric object and         class logits. We applied these settings to all models.
                 scene recognition. IEEE transactions on pattern analysis      WhentrainingBERTmodels,oneadditionalhyperparameter
                 andmachineintelligence, 30(11):1958–1970, 2008.               is the masking probability, set to 15% in Devlin et al. (2018).
                                                             Generative Pretraining from Pixels
               Table 5. Learning rates used for each model, objective, and input
               resolution (IR) combination.
                   Model         Objective         IR      Learning Rate
                   iGPT-S      auto-regressive  322 ×3         0.003
                   iGPT-M      auto-regressive  322 ×3         0.003
                   iGPT-L      auto-regressive  322 ×3         0.001
                   iGPT-L      auto-regressive  482 ×3          0.01
                   iGPT-XL     auto-regressive  642 ×3        0.0003
                   iGPT-S          BERT         322 ×3          0.01
                   iGPT-M          BERT         322 ×3         0.003
                   iGPT-L          BERT         322 ×3         0.001
               Wealsotried higher masking rates of 20%, 25%, 30%, and
               35%, ﬁnding that 20% matched the performance of 15%,
               though higher probabilities decreased performance.
