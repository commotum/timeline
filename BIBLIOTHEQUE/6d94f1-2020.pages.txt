
--- PAGE 1 ---
1 of 16
COMPUTER SCIENCE
AI Feynman: A
physics-inspired method for
symbolic regression
Silviu-Marian
Udrescu
1
and Max
Tegmark
1
,
2
*
A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression
that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions
of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In
this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network
fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the
Feynman Lectures on P
h
y
s
i
c
s
,
and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-
based test set, we improve the state-of-the-art success rate from 15
to 90%.
INTRODUCTION
In 1601, Johannes Kepler got access to the worldÂ’s best data tables
on planetary orbits, and after 4 years and about 40 failed attempts to
fit the Mars data to various ovoid shapes, he launched a scientific
revolution
by discovering
that MarsÂ’
orbit was
an ellipse
(
1
). This
was an example of symbolic regression: discovering a symbolic ex
-
pression that accurately matches a given dataset. More specifically,
we are given a table of numbers, whose rows are of the form {
x
1
,Â…,
x
n
,
y
} where
y
=
f
(
x
1
, Â…,
x
n
), and our task is to discover the correct
symbolic expression for the unknown mystery function
f
, optionally
including the complication of noise.
Growing
datasets
have
motivated
attempts
to
automate
such
r
e
gression tasks, with notable success. For the special case where the
unknown function
f
is a linear combination of known functions
of {
x
1
, Â…,
x
n
}, symbolic regression reduces to simply solving a syst
e
m
of linear equations. Linear regression (where
f
is simply an affine
function) is ubiquitous in the scientific literature, from finance to
psychology. The case where
f
is a linear combination of monomials
in {
x
1
, Â…,
x
n
and to polynomial fitting more generally. There are countless other
examples of popular regression functions that are linear combina
-
tions of known functions, ranging from Fourier expansions to w
a
v
e
l
e
t
transforms. Despite these successes with special cases, the general
symbolic
regression
problem
remains
unsolved,
and
it
is
easy
to
see
why: If we encode functions as strings of symbols, then the number
of such strings grows exponentially with string length, so if we simp
l
y
test all strings by increasing length, it may take longer than the age
of our universe until we get to the function we are looking for.
This combinatorial challenge of an exponentially large search
space characterizes many famous classes
of problems,
from code
breaking and RubikÂ’s cube to the natural selection problem of find
-
ing those genetic codes that produce the most evolutionarily fit or
-
ganisms. This has motivated genetic algorithms (
2
,
3
) for targeted
searches in exponentially large spaces, which replace the above-
mentioned brute-force search by biology-inspired strategies of
mutation, selection, inheritance, and recombination; crudely s
p
e
a
k
-
ing, the role of genes is played by useful symbol strings that may
form part of the sought-after formula or program. Such algorithms
have been successfully applied to areas ranging from design of
antennas (
4
,
5
) and vehicles (
6
) to wireless routing (
7
), vehicle routi
n
g
(
8
), robot navigation (
9
), code breaking (
10
), discovering partial
differential equations (
11
), investment strategy (
12
), marketing (
13
),
classification (
14
), RubikÂ’s cube (
15
), program synthesis (
16
), and
metabolic networks (
17
).
The symbolic regression problem for mathematical functions
(the focus of this paper) has been
tackled with a variety of methods
(
18
Â–
20
), including sparse regression (
21
Â–
24
) and genetic algorithms
(
25
,
26
).
By
far,
the most
successful
of these
is, as
we will
see
in
R
e
sults, the genetic algorithm outlined in (
27
) and implemented in
the commercial Eureqa software (
26
).
The purpose of this paper was to further improve on this state
of
the art, using physics-inspired strategies enabled by neural networks.
Our most important contribution is using neural networks to dis
-
cover
hidden
simplicity
such
as
symmetry
or
separability
in
the
mystery
data,
which
enables
us
to
recursively
break harder
problems
into simpler ones with fewer variables.
The rest of this paper is organized as follows. In Results, we present
the results of applying our algorithm, which recursively combines
six
strategies,
finding
major improvements over
the state-of-the-art
Eureqa algorithm. In Discussion, we summarize our conclusions a
n
d
discuss opportunities for further progress.
RESULTS
In this section, we present our results and the algorithm by which
they were obtained.
Overall algorithm
Generic functions
f
(
x
1
, Â…,
x
n
) are extremely complicated and near
impossible for symbolic regression to discover. However, functions
appearing in physics and many other scientific applications often
have some of the following simplifying properties that make them
easier to discover:
(1) Units:
f
and the variables upon which it depends have known
physical units.
(2) Low-order polynomial:
f
(or part thereof) is a polynomial of
low degree.
(3) Compositionality:
f
is a composition of a small set of elementary
functions, each typically taking no more than two arguments.
1
Department of Physics and Center for Brains, Minds & Machines, Massachusetts
Institute of Technology, Cambridge, MA 02139, USA.
2
Theiss Research, La Jolla, CA
92037, USA.
*Corresponding author. Email: tegmark@mit.edu
Copyright Â© 2020
The
Authors,
some
rights reserved;
exclusive licensee
American Association
for the Advancement
of Science. No claim to
original
U.S.
Government
Works. Distributed
under a Creative
Commons Attribution
NonCommercial
License 4.0 (CC BY-NC).

--- PAGE 2 ---
2 of 16
(4)
Smoothness:
f
is
continuous and
perhaps even
analytic in
i
t
s
domain.
(5) Symmetry:
f
exhibits translational, rotational, or scaling sym
-
metry with respect to some of its variables.
(6)
Separability:
f
can
be
written
as
a
sum
or
product
of
two
parts
with no variables in common.
The
question
of
why
these
properties
are
common
remains
cont
r
o
-
versial and not fully understood (
28
,
29
). However, as we will see
below, this does not
prevent us from discovering and exploiting
t
h
e
s
e
properties to facilitate symbolic regression.
Property (1) enables dimensional analysis, which often transforms
the problem into a simpler one with fewer independent variables.
Property (2) enables polynomial fitting, which quickly solves the
problem by solving a system of linear equations to determine the
polynomial coefficients. Property (3) enables
f
to be represented as
a
parse
tree
with
a
small
number
of
node
types,
sometimes enabling
f
or
a
subexpression
to
be
found via
a
brute-force
search.
Property
(4) enables approximating
f
using a feed-forward neural network
with a smooth activation function. Property (5) can be confirmed
using said neural network and enables the problem to be transformed
into a simpler one with one independent variable less (or even fewe
r
for
n
> 2 rotational symmetry). Property (6) can be confirmed using
said
neural
network
and
enables
the
independent
variables
to
be
p
a
r
t
i
-
tioned into two disjoint sets and the problem to be transformed into
two simpler ones, each involving the variables from one of these s
e
t
s
.
The overall algorithm (available at https://github.com/SJ001/
AI-Feynman) is schematically illustrated in Fig.
1. It consists of a
series of modules that try to exploit each of the above-mentioned
properties. Like a human scientist, it tries many different strategies
(modules) in turn, and if it cannot solve the full problem in one fell
swoop, it tries to transform it and divide it into simpler pieces that
can be tackled separately, recursively relaunching the full algorithm
on each piece. Figure
2 illustrates an example of how a particular
mystery dataset (NewtonÂ’s law of gravitation with nine variables) is
solved.
Below,
we
describe
each
of
these
algorithm
modules
in
turn.
Dimensional analysis
Our dimensional analysis module exploits the well-known fact that
many problems in physics can be simplified by requiring the units
of the two sides of an equation to match. This often transforms the
problem into a simpler one with a smaller number of variables that
are all
dimensionless. In
the
best-case scenario, the transformed
problem involves solving for a function of zero variables, i.e., a con
-
stant. We automate dimensional analysis as follows.
Table
3 show the physical units of all variables appearing in our
100 mysteries, expressed as products of the fundamental units (met
e
r
,
second, kilogram, kelvin, and volt) to various integer powers. We,
thus,
represent
the
units
of
each
variable
by
a
vector
u
of
five
integ
e
r
s
as in the table. For a mystery of the form
y
=
f
(
x
1
, Â…,
x
n
), we define
the matrix
M
whose
i
th
column is the
u
vector corresponding to the
variable
x
i
, and define the vector
b
as the
u
vector corresponding to
y
. We now let the vector
p
be a solution to the equation
Mp
=
b
, and
the
columns
of
the
matrix
U
form
a
basis
for
the
null
space,
so
that
MU
= 0, and define a new mystery
y
=
f
(
x
1
, Â…,
x
n
) where
x
'
i
i
=
j
n
x
j
U
ij
,
y
'
y
y
*
,
y
*
i
=1
n
x
i
p
i
.
(1)
By construction, the new variables
x
i
and
y
are dimensionless,
and the number
n
of new variables is equal to the dimensionality of
the
null space. When
n
> 0, we have the freedom
to
choose
any
b
a
s
i
s
we want for the null space and also to replace
p
by a vector of the
form
p
+
Ua
for any vector
a
; we use this freedom to set as many
elements
as possible
in
p
and
U
equal to
zero,
i.e.,
to
make
the
new
variables depend on as few old variables as possible. This choice is
useful because it typically results in the resulting powers of the
d
i
mensionless variables being integers, making the final expression
much easier to find than when the powers are fractions or irrational
numbers.
Polynomial fit
Many functions
f
(
x
1
, Â…,
x
n
) in physics and other sciences either are
low-order polynomials, e.g.,
the kinetic energy
K
=
m
_
2
(
v
x
2
+
v
y
2
+
v
z
2
)
,
Dimensional
analysis
Polynomial
fit
Brute
force
Tr
ain neura
l
networ
k
Equate
variables
Tr
ansform
x
&
y
Solved
?
No
Ye
s
Solved?
No
Ye
s
Solved
?
No
Ye
s
Solved
?
No
Ye
s
x
-0.570631
0.883785
-1.145615
1.571480
...
y
-0.55358
3
0.81760
1
0.54618
0
-2.16671
1
...
f
-1.677797
2.518988
.
-0.053256
.
-2.761942
.
...
Data
Equatio
n
Fai
l
Symmetry
?
No
Ye
s
Separable
?
No
Ye
s
Make two new datasets
with fewer variable
s
T
ry new data with
fewer variables
T
ry transformed
data
T
ry new data with
fewer variables
Solved?
Solved?
No
Ye
s
No
Ye
s
Solved?
Ye
s
No
Fig. 1. Schematic illustration of our AI Feynman algorithm.
It is iterative as de
-
scribed in the text, with four of the steps capable of generating new mystery data
-
sets that get sent to fresh instantiations of the algorithm, which may or may not
return a solution.

--- PAGE 3 ---
3 of 16
o
r
have parts
that
are,
e.g.,
the
denominator
of
the
gravitational
f
o
r
c
e
F
=
Gm
1
m
2
___________________
(
x
1
x
2
)
2
+
(
y
1
y
2
)
2
+
(
z
1
z
2
)
2
.
We therefore include a module
t
h
a
t
tests
whether a mystery can be solved by a low-order polynomial. Our
method uses the standard method of solving a system of linear e
q
u
a
-
tions to find the best-fit polynomial coefficients. It tries fitting the
mystery data to polynomials of degree 0, 1,...,
d
max
= 4 and declares
success if the best-fitting polynomial gives root mean square (rms)
fitting error
e
p
(we discuss the setting of this threshold below).
Brute force
Our brute-force symbolic regression model simply tries all possible
symbolic expressions within some class, in order of increasing com
-
plexity, terminating either when the maximum fitting error drops
below a threshold
Â™
p
or after a maximum runtime
t
max
has been
exceeded. Although this module alone could solve all our mysteries
in principle, it would, in many cases, take longer than the age of our
universe in practice. Our brute-force method is, thus, typically most
helpful once a mystery has been transformed/broken apart into
simpler pieces by the modules described below.
We generate the expressions to try by representing them as strings
of
symbols,
trying
first
all
strings
of
length
1,
then
all
of
length
2,
etc.,
saving time by
only generating
those strings
that are syntactically
correct. The symbols used are the independent variables as well a
subset of those listed in Table
1, each representing a constant or a
function. We minimize string length by using reverse Polish notation,
so that parentheses become unnecessary. For example,
x
+
y
can be
expressed
as
the
string
Â“
xy+
Â”,
the
number 2/3
can
be
expressed
a
s
the string Â“
0<<1>>/
Â”, and the relativistic momentum form
u
l
a
mv
/

_
1
v
2
/
c
2
can be expressed as the string Â“
 P Y   Y Y  F F   Ã­ 5 
Â”.
Inspection of Table
1 reveals that many of the symbols are red
u
n
-
dant.
For
example,
Â“
1
Â”
=
Â“
0>
Â”
and
Â“
x
Â”
=
Â“
  [ Ã­
Â”.
p
= 2
arcsin
1,
so
if we drop the symbol Â“
P
Â”, mysteries involving
p
can still get solved
with
P
replaced by Â“
1N1>*
Â”Â—it just takes longer.
Since there are
s
n
strings of length
n
using an alphabet of
s
symbols,
there can be a substantial cost both from using too many symbols
(increasing
s
)
and
from
using
too
few
symbols
(increasing
the
req
u
i
r
e
d
n
or even making a solution impossible). As a compromise, our
brute-force
module
tries
to
solve
the
mystery
using three
different
symbol
subsets as
explained in the
caption of
Table
1. To
exploit the
fact that many equations or parts thereof have multiplicative or addi
-
tive constants, our brute-force method comes in two variants that a
u
t
o
-
matically solves for such constants, thus allowing the algorithm t
o
focus on the symbolic expression and not on numerical constants.
Although the problem of overfitting is most familiar when
searching
a
continuous
parameter
space,
the
same
phenomenon
c
a
n
occur when searching our discrete space of symbol strings. To mitigate
this, we follow the prescription in (
30
) and define the winning function
to be the one with rms fitting error
Â™
<
Â™
b
that has the smallest total
description length
DL

log
2
N
+
log
2
[
max
1,
Â™

Â™

]
(2)
where
Â™
d
= 10
15
, and
N
is the rank
of the string on the list
of all
strings tried. The two terms correspond roughly to the number of
bits required to store the symbol string and the prediction errors,
respectively, if the hyperparameter
l
is set to equal the number of
data points
N
d
. We use
l
=
N

1/2
in our experiments below to prior
i
tize
simpler formulas. If the mystery has been generated using a neural
network (see below), we set the precision threshold
Â™
b
to 10 times
the validation error, otherwise we set it to 10
5
.
Neural networkÂ–based tests and
transformations
Even after applying the dimensional analysis, many mysteries are
still too complex to be solved by the polyfit or brute-force modules
in a reasonable amount of time. However, if the mystery function
f
(
x
1
, Â…,
x
n
) can be found to have simplifying properties, it may be
possible to transform it into one or more simpler mysteries that can
be more easily solved. To search for such properties, we need to be
able to evaluate
f
at points {
x
1
, Â…,
x
n
} of our choosing where we
Dimensional
analysi
s
Polynomial
fit
T
ranslationa
l
symmetry
Multiplicativ
e
separability
T
ranslationa
l
symmetry
Invert
Polynomia
l
fit
Fig. 2. Example: How our AI Feynman algorithm discovered mystery Equation 5
.
Given a mystery table with many examples of the gravitational force
F
together
with the nine independent variables
G
,
m
1
,
m
2
,
x
1
,...,
z
2
, this table was recursively
transformed into simpler ones until the correct equation was found. First, dimens
i
o
n
a
l
analysis generated a table of six dimensionless independent variables
a = m
2
/
m
1
,...,
f = z
1
/
x
1
and
the
dimensionless
dependent
variable
F
Ã·
Gm
1
2
/
x
1
2
.
T
hen,
a
neur
a
l
n
e
t
-
work was trained to fit this function, which revealed two translational symmetries
(each eliminating one variable, by defining
g
Ã›i
cd
and
h
Ã›i
e  f
) as well as multi
-
plicative separability, enabling the factorization
(
a
,
b
,
g
,
h
) =
G
(
a
)
H
(
b
,
g
,
h
), thus
splitting the problem into two simpler ones. Both
G
and
H
then were solved by
polynomial fitting, the latter after applying one of a series of simple transformations
(
i
n
t
h
i
s
case, inversion). For many other mysteries, the final step was instead solved
using brute-force symbolic search as described in the text.

--- PAGE 4 ---
4 of 16
typically have
no
data. For
example, to test
whether
a
function
f
has
translational symmetry, we need to test if
f
(
x
1
,
x
2
) =
f
(
x
1
+
a
,
x
2
+
a
)
for various constants
a
, but if a given data point has its two variables
separated by
x
2
x
1
= 1.61803, we typically have no other examples
in our dataset with exactly that variable separation. To perform our
tests, we thus need an accurate high-dimensional interpolation be
-
tween our data point.
Neural network training
To obtain such an interpolating function for a given mystery, we
train a neural network to predict the output given its input. We t
r
a
i
n
a feed-forward, fully connected neural network with six hidden layers
with soft plus activation functions, the first three having 128 neurons
and the last three having 64 neurons. For each mystery, we generated
100,000 data points, using 80% as the training set and the remainder
as the validation set, training for 100 epochs with learning rate 0.005
and batch size 2048. We use the rms error loss function and the
Adam optimizer with a weight decay of 10
2
. The learning rate and
momentum
schedules
were implemented as described in (
31
,
32
)
u
sing the FastAI package (
33
), with a ration of 20 between the maximum
and minimum learning rates, and using 10% of the iterations for the
last part of the training cycle. For the momentum, the maximum
b
1
value was 0.95 and the minimum 0.85, while
b
2
= 0.99.
If the neural network were expressive enough to be able to per
-
fectly fit the mystery function, and the training process would never
get stuck in a local minimum, then one might naively expect the r
m
s
validation error
Â™
NN
0
to scale as
f
rms
Â™
/
N

1/2
in the limit of ample
d
ata,
with a constant prefactor depending on the number of function ar
-
guments and the functionÂ’s complexity. Here,
f
rms
is the rms of the
f
values in the dataset,
N
d
is the number of data points, and
Â™
is the
relative rms noise on the independent variable as explored in the
Â“Dependence on noise levelÂ” section. For realistic situations, one
expects limited expressibility and convergence to keep
Â™
NN
0
above
some
positive
floor
even
as
N
d


and
Â™

0.
In
practice,
we
o
b
tained
Â™
NN
0
values between 10
3
f
rms
and 10
5
f
rms
across the range
of tested equations.
Translational symmetry and
generalizations
We
test
for translational
symmetry
using
the neural
network as de
-
tailed in Algorithm 1. We first check if the
f
(
x
1
,
x
2
,
x
3
,Â…) =
f
(
x
1
+
a
,
x
2
+
a
,
x
3
) to within a precision
Â™
sym
. If that is the case, then
f
de
-
pends on
x
1
and
x
2
only through their difference, so we replace these
two input variables by a single new variable
x
1
x
2
x
1
. Otherwise,
we repeat this test for all pairs of input variables and also test whethe
r
any variable pair can be replaced by its sum, product, or ratio. The
ratio case corresponds to scaling symmetry, where two variables can
be simultaneously rescaled without changing the answer. If any of
these simplifying properties is found, the resulting transformed
mystery (with one fewer input variables) is iteratively passed into a
fresh instantiation of our full AI Feynman symbolic regression algo
-
rithm, as illustrated in Fig.
1. After experimentat
i
o
n
,
w
e
c
h
o
s
e
t
h
e
p
r
e
-
cision threshold
Â™
sym
to be seven times the neural
n
e
t
w
o
r
k
v
a
l
i
d
a
t
i
o
n
error, which roughly optimized the training set performance. (If the
noise were Gaussian, even a cut at 4 rather than 7 standard devia
-
tions would produce negligible false positives.)
Separability
We test for separability using the neural network as exemplified in
Algorithm 2. A function is separable if it can be split into two parts
with no variables in common. We test for both additive and multi
-
plicative separability, corresponding to these two parts being added
and multiplied, respectively (the logarithm of
a multiplicatively
sep
-
arable function is additively separable).
For
example,
to
test
whether
a
function
of
two
variables
is
multi
-
plicatively separable, i.e., of the form
f
(
x
1
,
x
2
) =
g
(
x
1
)
h
(
x
2
) for some
univariate functions
g
and
h
, we first select two constants
c
1
and
c
2
; for
numerical robustness, we choose
c
i
to be the means of all the values
of
x
i
in the mystery dataset,
i
= 1,2. We then compute the quantity
D
sep
(
x
1
,
x
2
)
f
rms
1

f
(
x
1
,
x
2
)
f
(
x
1
,
c
2
)
f
(
c
1
,
x
2
)
f
(
c
1
,
c
2
)

(3)
for each data point. This is a measure of nonseparability, since it
vanishes if
f
is multiplicatively separable. The equation is conside
r
e
d
separable if the rms average
D
sep
over the mystery dataset is less than
an accuracy threshold
Â™
sep
, which is chosen to be
N
= 10 times the
neural network validation error. [We also check whether the func
-
tion is
multiplicatively separable up to
an additive constant:
f
(
x
1
,
x
2
) =
a
+
g
(
x
1
)
h
(
x
2
), where
a
is a constant. As a backup, we retain the
above-mentioned simpler test for multiplicative separability, which
proved more robust when
a
= 0.]
If separability is found, we define the two new univariate mysteri
e
s
y
f
(
x
1
,
c
2
)
and
y
f
(
c
1
,
x
2
)/
f
(
c
1
,
c
2
).
We
pass
the
first
one,
y
,
back
to fresh instantiations of our full AI Feynman symbolic regression
algorithm, and if it gets solved, we redefine
y
y
/
y
c
num
, where
c
num
represents any multiplicative numerical constant that appears in
y
.
We then pass
y
back to our algorithm, and if it gets solved, the final
solution is
y
=
y
y
/
c
num
. We test for additive separability analogousl
y
,
simply replacing
*
and / by + and  above; also,
c
num
will represent
Table 1. Functions optionally included in brute-force search.
The
following three subsets are tried in turn: Â“+*/><~SPLICERÂ”, Â“+*/> 0~Â”
and Â“+*/><~REPLICANTS0Â”.
Symbol
Meaning
Arguments
+
Add
2
*
Multiply
2
Subtract
2
/
Divide
2
>
Increment
1
<
Decrement
1

Negate
1
0
0
0
1
1
0
R
sqrt
1
E
exp
1
P
p
0
L
ln
1
I
invert
1
C
cos
1
A
abs
1
N
arcsin
1
T
arctan
1
S
sin
1

--- PAGE 5 ---
5 of 16
an additive numerical
constant in
this case.
If we succeed in solving
the two parts, then the full solution to the original mystery is the
sum of the two parts minus the numerical constant. When there are
more than two variables
x
i
, we are testing all the possible subsets of
variables that can lead to separability and proceed as above for the
newly created two mysteries.
Setting variables equal
We also exploit the neural network to explore the effect of setting
two
input
variables
equal
and
attempting
to
solve
the
correspondi
n
g
new mystery
y
with one fewer variable. We try this for all variable
pairs, and if the resulting new mystery is solved, we try solving the
mystery
y
y
/
y
that has the found solution divided out.
As an example, this technique solves the Gaussian probability d
i
s
-
tribution mystery I.6.2. After making
q
and
s
equal and dividing the
initial
equation by the
result, we
are getting rid of the
denominator,
and the remaining part of the equation is an exponential. After taki
n
g
the logarithm of this (see the below section), the resulting express
i
o
n
can be easily solved by the brute-force method.
Extra transformations
In addition, several transformations are applied to the dependent
and independent variables, which proved to be useful for solving
certain equations. Thus, for each equation, we ran the brute force
and polynomial fit on a modified version of the equation in which
the dependent variable was transformed by one of the following
functions: square root, raise to the power of 2, log, exp, inverse, sin,
cos, tan, arcsin, arccos, and arctan. This reduces the number of symb
o
l
s
needed by the brute force by one, and in certain cases, it even
allows the polynomial fit to solve the equation, when the brute
force would otherwise fail. For example, the formula for the dis
-
tance between two points in the three-dimensional (3D) Euclidean
space:

___________________________
(
x
1
x
2
)
2
+
(
y
1
y
2
)
2
+
(
z
1
z
2
)
2
,
once raised to
the
power
of 2 becomes just a polynomial that can be easily discovered by the
polynomial fit algorithm. The same transformations are also applied
to the dependent variables, one at a time. In addition, multiplication
and division by 2 were added as transformations in this case.
It should be noted that, like most machine-learning methods, the
AI Feynman algorithm has some hyperparameters that can be
tuned to optimize performance on the problems at hand. They were
all introduced above, but for convenience, they are also summar
i
z
e
d
in Table2.
The Feynman Symbolic Regression Database
To facilitate quantitative testing of our and other symbolic regression
algorithms, we created the 6-gigabyte Feynman Symbolic Regression
Database (FSReD) and made it freely available for download at
https://space.mit.edu/home/tegmark/aifeynman.html. For each
r
e
gression mystery, the database contains the following:
1) Data table: A table of numbers, whose rows are of the form
{
x
1
,
x
2
, Â…,
y
}, where
y
=
f
(
x
1
,
x
2
,
); the challenge is to discover the
correct analytic expression for the mystery function
f
.
2) Unit table: A table specifying the physical units of the input
and output variables as 6D vectors of the form seen in Table3.
3) Equation: The analytic expression for the mystery function
f
,
for answer checking.
To
test
an
analytic regression algorithm using the
database,
its
t
a
s
k
is
to predict
f
for
each mystery taking
the data
table
(and
optionally
the
unit
table)
as
input.
Of
course,
there
are
typically
many
symboli
c
a
l
l
y
different ways of expressing the same function. For example, if the
mystery function
f
is
(
u
+
v
)/(1
+
uv
/
c
2
),
then
the
symbolically
dif
-
ferent
expression
(
v
+
u
)/(1
+
uv
/
c
2
)
should
count
as
a correct
solution.
The rule for evaluating an analytic regression method is therefore
that a mystery function
f
is deemed correctly solved by a candidate
expression
f
if algebraic simplification of the expression
f
f
(say,
with the
Simplify
function in Â“MathematicaÂ” or the
simplify
function in the Python SymPy package) produces the symbol Â“0. Â”
To sample equations from a broad range of physics areas, the
database is generated using 100 equations from the seminal
Feynm
a
n
Lectures on Physics
(
34
Â–
36
), a challenging three-volume course cove
r
-
ing classical mechanics, electromagnetism, and quantum mechanics
as well as a selection of other core physics topics; we prioritized the
most complex equations, excluding ones involving derivatives or
integrals. The equations are listed in Tables
4 and 5 and can be seen
to
involve
between
one
and
nine
independent
variables
as
well
as
t
h
e
elementary functions +, , , /, sqrt, exp, log, sin, cos, arsin, and t
a
n
h
.
The numbers appearing in these equations are seen to be simple
rational numbers as well as
e
and
p
.
We also included in the database a set of 20 more challenging
Â“bonusÂ” equations, extracted from other seminal physics books:
Classical Mechanics
by Goldstein
et
al.
(
37
);
Classical Electrodynamics
by Jackson (
38
);
Gravitation and Cosmology: Principles and Applications
of the General Theory of Relativity
by Weinberg (
39
); and
Quantum
Field Theory and the Standard Model
by Schwartz (
40
). These equa
-
tions were selected for being both famous and complicated.
The data table provided for each mystery equation contains
10
5
rows corresponding to randomly generated input variables.
These are sampled uniformly between one and five. For certain
equations, the range of sampling was slightly adjusted to avoid
u
n
physical result, such as division by zero, or taking the square root
of a negative number. The range used for each equation is listed
i
n
the FSReD.
Table 2. Hyperparameters in our algorithm and the setting we use in
this paper.
Symbol
Meaning
Setting
Â™
br
Tolerance in brute-force
module
10
5
Â™
pol
Tolerance in polynomial
fit module
10
4
Â™
NN
0
Validation error
tolerance for neural
network use
10
2
Â™
sep
Tolerance for
separability
10
Â™
NN
Â™
sym
Tolerance for symmetry
7
Â™
NN
Â™
bf
sep
Tolerance in brute-force
module after
separability
10
Â™
NN
Â™
pol
sep
Tolerance in polynomial
fit module after
separability
10
Â™
NN
l
Importance of accuracy
relative to complexity
N

1/2

--- PAGE 6 ---
6 of 16
Table 3. Unit table used for our automated dimensional analysis.
Variables
Units
m
s
kg
T
V
a
,
g
Acceleration
1
2
0
0
0
h
, ,
L
,
J
z
Angular momentum
2
1
1
0
0
A
Area
2
0
0
0
0
k
b
Boltzmann constant
2
2
1
1
0
C
Capacitance
2
2
1
0
2
q
,
q
1
,
q
2
Charge
2
2
1
0
1
j
Current density
0
3
1
0
1
I
,
I
0
Current Intensity
2
3
1
0
1
r
,
r
0
Density
3
0
1
0
0
q
,
q
1
,
q
2
,
s
,
n
Dimensionless
0
0
0
0
0
g
_,
k
f
,
g
,
c
,
b
,
a
Dimensionless
0
0
0
0
0
p
g
,
n
0
,
d
,
f
,
m
Dimensionless
0
0
0
0
0
n
0
,
d
,
f
,
m
,
Z
1
,
Z
2
Dimensionless
0
0
0
0
0
D
Diffusion coefficient
2
1
0
0
0
m
drift
Drift velocity
constant
0
1
1
0
0
p
d
Electric dipole
moment
3
2
1
0
1
E
f
Electric field
1
0
0
0
1
Â™
Electric permitivity
1
2
1
0
2
E
,
K
,
U
Energy
2
2
1
0
0
E
den
Energy density
1
2
1
0
0
F
E
Energy flux
0
3
1
0
0
F
,
N
n
Force
1
2
1
0
0
w
,
w
0
Frequency
0
1
0
0
0
k
G
Grav. coupling
(
Gm
1
m
2
)
3
2
1
0
0
H
Hubble constant
0
1
0
0
0
L
ind
Inductance
2
4
1
0
2
n
rho
Inverse volume
3
0
0
0
0
x
,
x
1
,
x
2
,
x
3
Length
1
0
0
0
0
y
,
y
1
,
y
2
,
y
3
Length
1
0
0
0
0
z
,
z
1
,
z
2
,
r
,
r
1
,
r
2
Length
1
0
0
0
0
l
,
d
1
,
d
2
,
d
,
f
f
,
a
f
Length
1
0
0
0
0
I
1
,
I
2
,
I
*
,
I
*0
Light intensity
0
3
1
0
0
B
,
B
x
,
B
y
,
B
z
Magnetic field
2
1
0
0
1
m
m
Magnetic moment
4
3
1
0
1
M
Magnetization
1
3
1
0
1
m
,
m
0
,
m
1
,
m
2
Mass
0
0
1
0
0
m
e
Mobility
0
1
1
0
0
p
Momentum
1
1
1
0
0
G
NewtonÂ’
s constant
3
2
1
0
0
P
*
Polarization
0
2
1
0
1
P
Power
2
3
1
0
0
c
ontin
ued on nex
t page

--- PAGE 7 ---
7 of 16
Algorithm comparison
We reviewed the symbolic regression literature for publicly availa
b
l
e
software against which our method could be compared. To the best of our
knowledge,
the best competitor by
far is the
commercial
Eureqa softw
a
r
e
sold by Nutonian Inc. at https://www.nutonian.com/products/eureqa,
implementing an improved version of the generic search algorithm
outlined in (
27
).
We compared the
AI Feynman and Eureqa algorithms by apply
-
ing them both to the Feynman Database for symbolic regression,
allowing a maximum of 2 hours of central processing unit (CPU)
time per mystery. Tables
4 and 5 show that Eureqa solved 71% of
the 100 basic mysteries, while AI Feynman solved 100%.
For this comparison, the AI Feynman algorithm was run using
the hyperparameter settings in Table
2. For Eureqa, each mystery w
a
s
run on four CPUs. The symbols used in trying to solve the equations
were +, ,
*
, /, constant, integer constant, input variable, sqrt, exp,
log, sin, and cos. To help Eureqa gain speed, we included the addi
-
tional functions arcsin and arccos only for those mysteries requiri
n
g
them, and we used only 300 data points (since it does not use a n
e
u
r
a
l
network, adding additional data does not help much). The time taken
t
o solve
an equation using our algorithm, as presented in Tables
4
and 5, corresponds to the time needed for an equation to b
e
s
o
l
v
e
d
using a set of symbols that can actually solve it (see Table
1). Equations
I.15.3t
and
I.48.2
were
solved
using
the
second
set
of
symbols,
so
the
overall
time
needed
for
these
two
equations
is
1
hour
longer
than
the
one
listed
in
the
tables.
Equations
I.15.3x
and
II.35.21 were
solved using the third set of symbols, so the overall time taken is
2
hours longer than the one listed here.
Closer inspection of these tables reveals that the greatest improve
-
ment of our algorithm over Eureqa is for the most complicated
m
y
s
teries,
where
our
neural
network
enables
eliminating
variables
by discovering symmetries and separability. The neural network
b
e
comes even more important when we rerun AI Feynman without
the dimensional analysis module: It now solves 93% of the mysteri
e
s
and makes very heavy use of the neural network to discover separa
-
bility and translational symmetries. Without dimensional analysis,
many of the mysteries retain variables that appear only raised to some
power or in a multiplicative prefactor, and AI Feynman tends to
recursively discover them and factor them out one by one. For example,
the neural network strategy is used six times when solving
F
=
Gm
1
m
2
(
x
2
x
1
)
2
+
(
y
2
y
1
)
2
+
(
z
2
z
1
)
2
without dimensional analysis: three times to discover translat
i
o
n
a
l
symmetry that replaces
x
2
x
1
,
y
2
y
1
, and
z
2
z
1
by new variables,
once
to
group
together
G
and
m
1
into
a
new
variable
a
,
once
to
g
r
o
u
p
together
a
and
m
2
into a new variable
b
, and one last time to discover
separability
and
factor
out
b
.
This
shows
that
although
dimensional
analysis often provides major time savings, it is usually not necessary
for successfully solving the problem.
Inspection of how AI Feynman and Eureqa make progress over
time reveals interesting differences. The progress of AI Feynman
over time corresponds to repeatedly reducing the number of inde
-
pendent variables, and every time this occurs, it is virtually guaranteed
to be a step in the right direction. In contrast, genetic algorithms
such as Eureqa make progress over time by finding successively bett
e
r
approximations, but there is no guarantee that more accurate symb
o
l
i
c
expressions are closer to the truth when viewed as strings of symb
o
l
s
.
Specifically, by virtue of being a genetic algorithm, Eureqa has the
Variables
Units
m
s
kg
T
V
p
F
Pressure
1
2
1
0
0
R
Resistance
2
3
1
0
2
m
S
Shear modulus
1
2
1
0
0
L
rad
Spectral radiance
0
2
1
0
0
k
spring
Spring constant
0
2
1
0
0
s
den
Surface charge
density
0
2
1
0
1
T
,
T
1
,
T
2
Temperature
0
0
0
1
0
k
Thermal conductivity
1
3
1
1
0
t
,
t
1
Time
0
1
0
0
0
t
Torque
2
2
1
0
0
A
vec
Vector potential
1
1
0
0
1
u
,
v
,
v
1
,
c
,
w
Velocity
1
1
0
0
0
V
,
V
1
,
V
2
Volume
3
0
0
0
0
r
c
,
r
c
0
Volume charge
density
1
2
1
0
1
V
e
Voltage
0
0
0
0
1
k
Wave number
1
0
0
0
0
Y
Young modulus
1
2
1
0
0

--- PAGE 8 ---
8 of 16
Table 4. Tested Feynman equations, part 1.
Abbreviations in the Â“Methods usedÂ”
column: da,
dimensional analysis; bf,
brute force; pf,
polyfit; ev,
set two
variables equal; sym,
symmetry; sep,
separability. Suffixes denote the type of symmetry or separability (symÂ–,
translational symmetry; sep*,
multiplicative
separability; etc.) or the preprocessing before brute force (e.g., bf-inverse means inverting the mystery function before bf).
Feynman Eq.
Equation
S
olution Time (s)
Methods Used
Data Needed
Solved By Eureqa
Solved W/o
da
Noise
Tolerance
I.6.20a
f
=
e
q
2
/2
/
_
2
p

16
bf
10
No
Yes
10
2
I.6.20
f
=
e
q
2
_
2
s
2
/
_
2
ps
2
2992
ev, bf-log
10
2
No
Yes
10
4
I.6.20b
f
=
e
(
q
q
1
)
2
_
2
s
2
/
_
2
ps
2
4792
symÂ–, ev, bf-log
10
3
No
Yes
10
4
I.8.14
d
=
__________________
(
x
2
x
1
)
2
+
(
y
2
y
1
)
2
544
da, pf-squared
10
2
No
Yes
10
4
I.9.18
F
=
G
m
1
m
2
___________________
(
x
2
x
1
)
2
+
(
y
2
y
1
)
2
+
(
z
2
z
1
)
2
5975
da, symÂ–, symÂ–,
sep, pf-inv
10
6
No
Yes
10
5
I.10.7
m
=
m
0
_
_
1
v
2
_
c
2
14
da, bf
10
No
Yes
10
4
I.11.19
A
=
x
1
y
1
+
x
2
y
2
+
x
3
y
3
184
da, pf
10
2
Yes
Yes
10
3
I.12.1
F
=
m
N
n
12
da, bf
10
Yes
Yes
10
3
I.12.2
F
=
q
1
q
2
_
4
p
Â™
r
2
17
da, bf
10
Yes
Yes
10
2
I.12.4
E
f
=
q
1
_
4
p
Â™
r
2
12
da
10
Yes
Yes
10
2
I.12.5
F
=
q
2
E
f
8
da
10
Yes
Yes
10
2
I.12.11
F
=
q
(
E
f
+
Bv
sin
q
)
19
da, bf
10
Yes
Yes
10
3
I.13.4
K
=
1
_
2
m
(
v
2
+
u
2
+
w
2
)
22
da, bf
10
Yes
Yes
10
4
I.13.12
U
=
G
m
1
m
2
1
_
r
2
1
_
r
1
20
da, bf
10
Yes
Yes
10
4
I.14.3
U
=
mgz
12
da
10
Yes
Yes
10
2
I.14.4
U
=
k
spring
x
2
_
2
9
da
10
Yes
Yes
10
2
I.15.3x
x
1
=
x
ut
_
_
1
u
2
/
c
2
22
da, bf
10
No
No
10
3
I.15.3t
t
1
=
t
ux
/
c
2
_
_
1
u
2
/
c
2
20
da, bf
10
2
No
No
10
4
I.15.10
p
=
m
0
v
_
_
1
v
2
/
c
2
13
da, bf
10
No
Yes
10
4
I.16.6
v
1
=
u
+
v
_
1
+
uv
/
c
2
18
da, bf
10
No
Yes
10
3
I.18.4
r
=
m
1
r
1
+
m
2
r
2
_
m
1
+
m
2
17
da, bf
10
Yes
Yes
10
2
I.18.12
t
=
rF
sin
q
15
da, bf
10
Yes
Yes
10
3
I.18.16
L
=
mrv
sin
q
17
da, bf
10
Yes
Yes
10
3
I.24.6
E
=
1
_
4
m
(
w
2
+
w
0
2
)
x
2
22
da, bf
10
Yes
Yes
10
4
I.25.13
V
e
=
q
_
C
10
da
10
Yes
Yes
10
2
c
ontin
ued on nex
t page

--- PAGE 9 ---
9 of 16
Feynman Eq.
Equation
S
olution Time (s)
Methods Used
Data Needed
Solved By Eureqa
Solved W/o
da
Noise
Tolerance
I.26.2
q
1
= arcsin
(
n
sin
q
2
)
530
da, bf-sin
10
2
Yes
Yes
10
2
I.27.6
f
f
=
1
_
1
_
d
1
+
n
_
d
2
14
da, bf
10
Yes
Yes
10
2
I.29.4
k
=
w

_
c
8
da
10
Yes
Yes
10
2
I.29.16
x
=
______________________
x
1
2
+
x
2
2
2
x
1
x
2
cos
(
q
1
q
2
)
2135
da, symÂ–,
bf-squared
10
3
No
No
10
4
I.30.3
I
*
=
I
*
0
sin
2
(
n
q
/
2)
_
sin
2
(
q
/
2)
118
da, bf
10
2
Yes
Yes
10
3
I.30.5
q
=
arcsin
l

_
nd
529
da, bf-sin
10
2
Yes
Yes
10
3
I.32.5
P
=
q
2
a
2
_
6
p
Â™
c
3
13
da
10
Yes
Yes
10
2
I.32.17
P
=
1
_
2
Â™
cE
f
2
(8
p
r
2
/
3
)
(
w
4
/
(
w
2
w
0
2
)
2
)
698
da, bf-sqrt
10
No
Yes
10
4
I.34.8
w
=
qvB
_
p
13
da
10
Yes
Yes
10
2
I.34.10
w
=
w
0
_
1
v
/
c
13
da, bf
10
No
Yes
10
3
I.34.14
w
=
1
+
v
/
c
_
_
1
v
2
/
c
2
w
0
14
da, bf
10
No
Yes
10
3
I.34.27
E
=
w
8
da
10
Yes
Yes
10
2
I.37.4
I
*
=
I
1
+
I
2
+
2
_
I
1
I
2
cos
d
7032
da, bf
10
2
Yes
No
10
3
I.38.12
r
=
4
p
Â™
2
_
m
q
2
13
da
10
Yes
Yes
10
2
I.39.10
E
=
3
_
2
p
F
V
8
da
10
Yes
Yes
10
2
I.39.11
E
=
1
_
g
1
p
F
V
13
da, bf
10
Yes
Yes
10
3
I.39.22
P
F
=
n
k
b
T
_
V
16
da, bf
10
Yes
Yes
10
4
I.40.1
n
=
n
0
e
mgx
_
k
b
T
20
da, bf
10
No
Yes
10
2
I.41.16
L
rad
=
w
3
_
p
2
c
2
e
w

_
k
b
T
1
22
da, bf
10
No
No
10
5
I.43.16
v
=
m
drift
q
V
e
_
d
14
da
10
Yes
Yes
10
2
I.43.31
D
=
m
e
k
b
T
11
da
10
Yes
Yes
10
2
I.43.43
k
=
1
_
g
1
k
b
v
_
A
16
da, bf
10
Yes
Yes
10
3
I.44.4
E
=
n
k
b
T
ln
V
2
_
V
1
18
da, bf
10
Yes
Yes
10
3
I.47.23
c
=
_
g
pr
_
r

14
da, bf
10
Yes
Yes
10
2
c
ontin
ued on nex
t page

--- PAGE 10 ---
10 of 16
advantage of not searching the space of symbolic expressions blindl
y
like our brute-force module, but rather with the possibility of a net
drift toward more accurate (Â“fitÂ”) equations. The flip side of this is
that if Eureqa finds a fairly accurate yet incorrect formula with a
quite
different
functional form, it
risks
getting
stuck
near
that local
optimum. This reflects a fundamental challenge for genetic app
r
o
a
c
h
e
s
symbolic regression: If the final formula is composed of separate p
a
r
t
s
that are not summed but combined in some more complicated way
(as a ratio, say), then each of the parts may be useless fits on their
own and unable to evolutionarily compete.
Dependence on
data size
To
investigate
the
effect
of
changing
the
size
of
the
dataset,
we
re
-
peatedly reduced the size of each dataset by a factor of 10 until our AI
Feynman algorithm failed to solve it. As seen in Tables
4 and 5, most
e
quations are discovered by the polynomial fit and brute-force methods
using only 10 data points. One hundred data points are needed in some
cases because the algorithm may otherwise overfit when the true equa
-
tion is complex, Â“discoveringÂ” an incorrect equation that is too simple.
As expected, equations that require the use of a neural network
to
be
solved
need
substantially
more
data
points
(between
10
2
and
10
6
) for the network to be able to learn the mystery function accu
-
rately enough (i.e., obtaining rms accuracy better than 10
3
). Note
that expressions requiring the neural network are typically more
complex, so one might intuitively expect them to require larger d
a
t
a
-
sets for the correct equation to be discovered without overfitting,
even when using alternate approaches such as genetic algorithms.
Dependence on
noise level
Since real data are almost always afflicted with measurement errors
or other forms of noise, we investigated the robustness of our algo
-
rithm. For each mystery, we added independent Gaussian random
noise to its dependent variable
y
, of standard deviation
Â™
y
rms
, where
y
rms
denotes the rms
y
value for the mystery before noise has been
added. We initially set the relative noise level
Â™
= 10
6
and then re
-
peatedly multiplied
Â™
by 10 until the AI Feynman algorithm could
no longer solve the mystery. As seen in Tables
4 and 5, most of the
equations can still be recovered exactly with an
Â™
value of 10
4
o
r
less,
while almost half of them are still solved for
Â™
= 10
2
.
For these noise experiments, we adjusted the threshold for the
brute-force
and
polynomial
fit
algorithms
when
the
noise
level
changed, such that not finding a solution at all was preferred over
f
inding an approximate solution. These thresholds were not optimized
for each mystery individually, so a better choice of these thresholds
might allow the exact equation to be recovered with an even higher
noise level for certain equations. In future work, it will also be
i
n
t
e
r
esting
to
quantify
performance
of
the
algorithm
on
data
with
noise added to the independent variables, as well as directly on
r
e
al-world data.
Bonus mysteries
The 100 basic mysteries discussed above should be viewed as a t
r
a
i
n
-
ing
set
for our AI
Feynman
algorithm,
since
we made improvements
to its implementation and hyperparameters to optimize performance.
In contrast, we can view the 20 bonus mysteries as a test set, since
we deliberately selected and analyzed them only after the AI Feynm
a
n
algorithm and its hyperparameter settings (Table
2) had been final
-
ized. The bonus mysteries are interesting also by virtue of being
s
u
b
s
t
a
n
t
i
a
l
l
y
more complex and difficult in order to better identify
the limitations of our method.
Table
6 shows that Eureqa solved only 15% of the bonus mysteries
,
while AI Feynman solved 90%. The fact that the success percentage
differs more between the two methods for the bonus mysteries than
for the basic mysteries reflects the increased equation complexity,
which requires our neural networkÂ–based strategies for a larger f
r
a
c
-
tion of the cases.
To shed light on the limitations of the AI Feynman algorithm, it
is interesting to consider the two mysteries for which it failed. The
radiated gravitational wave power mystery was reduced to the form
y
=
32
a
2
(1
+
a
)
_
5
b
5
by
dimensional
analysis,
corresponding
to
the
string
Â“
aaa
>
*
*
bbbbb
*
*
*
*
/Â” in reverse Polish notation (ignoring the
multiplicative
prefactor
32
_
5
).
This
would
require about 2
years for
the brute-force method, exceeding our allotted time limit. The Jacks
o
n
2.11 mystery was reduced to the form
a
1
_
4
p
a
_
b
(
1
a
2
)
2
by
dimensional
analysis, corresponding to the string Â“
aP
0 > > > >
*
\
abaa
*
<
aa
*
<
*
*
/
*
Â” in reverse Polish notation, which would require about 100 t
i
m
e
s
the age of our universe for the brute-force method.
It is likely that both of these mysteries can be solved with relat
i
v
e
l
y
minor improvements of our algorithm. The first mystery would
have
been solved had the algorithm not failed to discover that
a
2
(1 +
a
)/
b
5
is separable. The large dynamic range induced by the fifth power in
the denominator caused the neural network to miss the separability
tolerance threshold; potential solutions include temporarily limiti
n
g
the
parameter
range or analyzing
the logarithm
of the absolute
valu
e
(to discover additive separability).
If we had used different units in the second mystery, where 1/4
p
Â™
was replaced by the Coulomb constant
k
, the costly 4
p
factor (requir
-
i
n
g
seven symbols Â“
PPPP
+ + +Â” or Â“
P
0 > > > >
*
Â”) would have dis
-
appeared. Moreover, if we had used a different set of function symb
o
l
s
that included Â“
Q
Â” for squaring, then brute force could quickly have
discovered that
a
a
_
b
(
1
a
2
)
2
is solved by Â“
aabaQ
<
Q
*
/
Â”. Similarl
y
,
introducing a symbol  denoting exponentiation, enabling the
string
Feynman Eq.
Equation
S
olution Time (s)
Methods Used
Data Needed
Solved By Eureqa
Solved W/o
da
Noise
Tolerance
I.48.20
E
=
m
c
2
_
_
1
v
2
/
c
2
108
da, bf
10
2
No
No
10
5
I.50.26
x
=
x
1
[
cos
(
w
t
) +
a
cos (
w
t
)
2
]
29
da bf
10
Yes
Yes
10
2

--- PAGE 11 ---
11 of 16
Table 5. Tested Feynman equations, part 2 (same notation as in Table
4).
Feynman Eq.
Equation
Solution Time
(s)
Methods Used
Data Needed
Solved By
Eureqa
Solved W/o da
Noise
Tolerance
II.2.42

=
k
(
T
2
T
1
)
A
_
d
54
da, bf
10
Yes
Yes
10
3
II.3.24
F
E
=
P
_
4
p
r
2
8
da
10
Yes
Yes
10
2
II.4.23
V
e
=
q
_
4
p
Â™
r
10
da
10
Yes
Yes
10
2
II.6.11
V
e
=
1
_
4
p
Â™
p
d
cos
q

_
r
2
18
da, bf
10
Yes
Yes
10
3
II.6.15a
E
f
=
3
_
4
p
Â™
p
d
z
_
r
5
_
x
2
+
y
2
2801
da, sm, bf
10
4
No
Yes
10
3
II.6.15b
E
f
=
3
_
4
p
Â™
p
d
_
r
3
cos
q
sin
q
23
da, bf
10
Yes
Yes
10
2
II.8.7
E
=
3
_
5
q
2
_
4
p
Â™
d
10
da
10
Yes
Yes
10
2
II.8.31
E
den
=
Â™
E
f
2
_
2
8
da
10
Yes
Yes
10
2
II.10.9
E
f
=
s
den
_
Â™

1
_
1
+
c
13
da, bf
10
Yes
Yes
10
2
II.11.3
x
=
qE
f
_
m
(
w
0
2
w
2
)
25
da, bf
10
Yes
Yes
10
3
II.11.17
n
=
n
0
1
+
p

E
f
cos
q

_
k
b
T
28
da, bf
10
Yes
Yes
10
2
II.11.20
P
*
=
n
r
p

2
E
f
_
3
k
b
T
18
da, bf
10
Yes
Yes
10
3
II.11.27
P
*
=
n
a

_
1
n
a
/
3
Â™
E
f
337
da bf-inverse
10
2
No
Yes
10
3
II.11.28
q
=
1
+
n
a

_
1
(
n
a
/
3)
1708
da, sym*, bf
10
2
No
Yes
10
4
II.13.17
B
=
1
_
4
p
Â™
c
2
2
I
_
r
13
da
10
Yes
Yes
10
2
II.13.23
r
c
=
r
c
0
_
_
1
v
2
/
c
2
13
da, bf
10
2
No
Yes
10
4
II.13.34
j
=
r
c
0
v
_
_
1
v
2
/
c
2
14
da, bf
10
No
Yes
10
4
II.15.4
E
=
m
M
B
cos
q
14
da, bf
10
Yes
Yes
10
3
II.15.5
E
=
p
d
E
f
cos
q
14
da, bf
10
Yes
Yes
10
3
II.21.32
V
e
=
q
_
4
p
Â™
r
(1
v
/
c
)
21
da, bf
10
Yes
Yes
10
3
II.24.17
k
=
_
w
2
_
c
2
p
2
_
d
2
62
da bf
10
No
Yes
10
5
II.27.16
F
E
=
Â™
cE
f
2
13
da
10
Yes
Yes
10
2
II.27.18
E
den
=
Â™
E
f
2
9
da
10
Yes
Yes
10
2
c
ontin
ued on nex
t page

--- PAGE 12 ---
12 of 16
for
a
b
to be shortened from Â“
aLb
*
E
Â” to Â“
ab
 , Â” would enable brute
force to solve many mysteries faster, including Jackson 2.11.
Last, a powerful strategy that could ameliorate both of these fail
-
ures would be to add symbols corresponding to parameters that are
numerically optimized over. This strategy is currently implemented
in Eureqa, but not AI Feynman, and could make a useful upgrade as
long as it is done in a way that does not unduly slow down the sym
-
bolic brute-force search. In summary, the two failures of the AI
Feynman Eq.
Equation
Solution Time
(s)
Methods Used
Data Needed
Solved By
Eureqa
Solved W/o da
Noise
Tolerance
II.34.2a
I
=
qv
_
2
p
r
11
da
10
Yes
Yes
10
2
II.34.2
m
M
=
qvr
_
2
11
da
10
Yes
Yes
10
2
II.34.11
w
=
g
_
qB
_
2
m
16
da, bf
10
Yes
Yes
10
4
II.34.29a
m
M
=
qh
_
4
p
m
12
da
10
Yes
Yes
10
2
II.34.29b
E
=
g
_
m
M
B
J
z
_
18
da, bf
10
Yes
Yes
10
4
II.35.18
n
=
n
0
______________________
exp
(
m
m
B
/
(
k
b
T
)
)
+
exp
(
m
m
B
/
(
k
b
T
)
)
30
da, bf
10
No
Yes
10
2
II.35.21
M
=
n
r
m
M
tanh
m
M
B
_
k
b
T
1597
da, halve-input,
bf
10
Yes
No
10
4
II.36.38
f
=
m
m
B
_
k
b
T
+
m
m
a
M
_
Â™
c
2
k
b
T
77
da bf
10
Yes
Yes
10
2
II.37.1
E
=
m
M
(1 +
c
)
B
15
da, bf
10
Yes
Yes
10
3
II.38.3
F
=
YA
x
_
d
47
da, bf
10
Yes
Yes
10
3
II.38.14
m

=
Y
_
2(1
+
s
)
13
da, bf
10
Yes
Yes
10
3
III.4.32
n
=
1
_
e
w

_
k
b
T
1
20
da, bf
10
No
Yes
10
3
III.4.33
E
=
w

_
e
w

_
k
b
T
1
19
da, bf
10
No
Yes
10
3
III.7.38
w
=
2
m
M
B
_
13
da
10
Yes
Yes
10
2
III.8.54
p
g
=
sin
Et
_
2
39
da, bf
10
No
Yes
10
3
III.9.52
p
g
=
p

E
f
t
_
sin
((
w
w
0
)
t
/
2)
2
___________
(
(
w
w
0
)
t
/
2)
2
3162
da, symÂ–, sm, bf
10
3
No
Yes
10
3
III.10.19
E
=
m
M
_
B
x
2
+
B
y
2
+
B
z
2
410
da, bf-squared
10
2
Yes
Yes
10
4
III.12.43
L
=
n
11
da, bf
10
Yes
Yes
10
3
III.13.18
v
=
2
E
d
2
k
_
16
da, bf
10
Yes
Yes
10
4
III.14.14
I
=
I
0
(
e
q
V
e
_
k
b
T
1)
18
da, bf
10
No
Yes
10
3
III.15.12
E
= 2
U
(1  cos
(
kd
))
14
da, bf
10
Yes
Yes
10
4
III.15.14
m
=
2
_
2
E
d
2
10
da
10
Yes
Yes
10
2
III.15.27
k
=
2
pa

_
nd
14
da, bf
10
Yes
Yes
10
3
III.17.37
f
=
b
(1 +
a
cos
q
)
27
bf
10
Yes
Yes
10
3
III.19.51
E
=
m
q
4
_
2
(4
p
Â™
)
2
2
1
_
n
2
18
da, bf
10
Yes
Yes
10
5
III.21.20
j
=
r
c
0
qA
vec
_
m
13
da
10
Yes
Yes
10
2

--- PAGE 13 ---
13 of 16
Table 6. Tested bonus equations.
Goldstein 8.56 is for the special case where the vectors
p
and
A
are parallel.
Source
Equation
Solved
Solved by Eureqa
Methods used
Rutherford scattering
A
=
Z
1
Z
2

c
_
4
E
sin
2


_
2
2
Yes
No
da, bf-sqrt
Friedman equation
H
=
_
8
p
G
_
3
r
k
f
c
2
_
a
f
2
Yes
No
da, bf-squared
Compton scattering
U
=
E
_
1
+
E
_
m
c
2
(1
cos
q
)
Yes
No
da, bf
Radiated gravitational wave
power
P
=
32
_
5
G
4
_
c
5
(
m
1
m
2
)
2
(
m
1
+
m
2
)
___________
r
5
No
No
Â–
Relativistic aberration
q
1
=
arccos
cos
q
2
v
_
c
_
1
v
_
c
cos
q
2
Yes
No
da, bf-cos
N-slit diffraction
I
=
I
0
sin
(
a
/
2)
_
a
/
2
sin
(
N
d
/
2)
_
sin
(
d
/
2)
2
Yes
No
da, sm, bf
Goldstein 3.16
v
=
______________
2
_
m
E
U
L
2
_
2
m
r
2
Yes
No
da, bf-squared
Goldstein 3.55
k
=
m
k
G
_
L
2
1
+
_
1
+
2
E
L
2
_
m
k
G
2
cos
q
1
q
2
Yes
No
da, symÂ–, bf
Goldstein 3.64 (ellipse)
r
=
d
(1
a
2
)
_
__________
1
+
a
cos
(
q
1
q
2
)
Yes
No
da, symÂ–, bf
Goldstein 3.74 (Kepler)
t
=
2
p
d
3/2
_
_
G
(
m
1
+
m
2
)
Yes
No
da, bf
Goldstein 3.99
a
=
___________
1
+
2
Â™
2
E
L
2
_
m
(
Z
1
Z
2
q
2
)
2
Yes
No
da, sym*, bf
Goldstein 8.56
E
=
___________________
(
p
q
A
vec
)
2
c
2
+
m
2
c
4
+
q
V
e
Yes
No
da, sep+, bf-squared
Goldstein 12.80
E
=
1
_
2
m
[
p
2
+
m
2
w
2
x
2
1
+
a
x
_
y
]
Yes
Yes
da, bf
Jackson 2.11
F
=
q
_
4
p
Â™
y
2
4
p
Â™
V
e
d
qd
y
3
_
(
y
2
d
2
)
2
No
No
Â–
Jackson 3.45
V
e
=
q
____________
(
r
2
+
d
2
2
dr
cos
a
)
1
_
2
Yes
No
da, bf-inv
Jackson 4.60
V
e
=
E
f
cos
q
a
1
_
a
+
2
d
3
_
r
2
r
Yes
No
da, sep*, bf
Jackson 11.38 (Doppler)
w
0
=
_
1
v
2
_
c
2
_
1
+
v
_
c
cos
q
w
Yes
No
da, cos-input, bf
Weinberg 15.2.1
r
=
3
_
8
p
G
c
2
k
f
_
a
f
2
+
H
2
Yes
Yes
da, bf
Weinberg 15.2.2
p
f
=
1
_
8
p
G
c
4
k
f
_
a
f
2
+
c
2
H
2
(1
2
a
)
Yes
Yes
da, bf
Schwarz 13.132 (Klein-Nishina)
A
=


2
2
_
m
2
c
2

0
_


2

0
_


+


_

0
sin
2

Yes
No
da, sym/, sep*, sin-input, bf

--- PAGE 14 ---
14 of 16
Feynman algorithm signal not unsurmountable obstacles, but
m
o
tivation for further work.
In
addition,
we
tested
the
performance
of
our
algorithm on
the
mystery functions presented in (
41
) (we wish to thank the anonymous
reviewer
who
brought
this
dataset
to
our
attention).
Some
equations
appear twice; we included them only once. Our algorithm again
outperformed
Eureqa,
discovering
66.7%
of
the
equations,
while
Eureqa discovered 48.9%. The fact that the AI Feynman algorithm
performs
less
well
on
this
test
set
than
on
genuine
physics
formulas
traces back to the fact that most of the equations presented in (
41
)
are rather
arbitrary compositions
of elementary functions unlikely t
o
occur in real-world problems, thus lacking the symmetries, separa
-
bility,
etc.,
that
the
neural network
part
of
our
algorithm
is
able
t
o
exploit.
DISCUSSION
We have presented a novel physics-inspired algorithm for solving
multidimensional analytic regression problems: finding a symbolic
expression that matches data from an unknown algebraic function.
Our key innovation lies in combining traditional fitting techniques
with
a
neural
networkÂ–based
approach
that
can
repeatedly
reduce
a
problem to simpler ones, eliminating dependent variables by dis
-
covering properties such as symmetries and separability in the un
-
known function. To facilitate quantitative benchmarking of our and
other symbolic regression algorithms, we created a freely downloadable
database with 100 regression mysteries drawn from the
Feynman
Lectures on Physics
and a bonus set of an additional 20 mysteries
selected for difficulty and fame.
Key findings
The preexisting state-of-the-art symbolic regression software Eureqa
(
26
) discovered 68% of the Feynman equations and 15% of the bonus
equations, while our AI Feynman algorithm discovered 100 and 9
0
%
,
respectively, including KeplerÂ’s ellipse equation mentioned in the
Introduction (third entry in Table
6). Most of the 100 Feynman
equations could be solved even if the data size was reduced to merely
10
2
data points or had percent-level noise added, but the most complex
equations needing neural network fitting required more data and
less noise.
Compared with the genetic algorithm of Eureqa, the most inter
-
esting improvements are seen for the most difficult mysteries where
the neural network strategy is repeatedly deployed. Here, the progress
of AI Feynman over time corresponds to repeatedly reducing the
problem to simpler ones with fewer variables, while Eureqa and o
t
h
e
r
genetic
algorithms
are
forced
to
solve
the
full
problem
by
exploring
a vast search space, risking getting stuck inlocal optima.
Opportunities for
further work
Both the successes and failures of our algorithm motivate further
work to make it better, and we will now briefly comment on promis
-
ing improvement strategies. Although we mostly used the same
e
l
e
mentary function options (Table
1) and hyperparameter settings
(Table
2) for all mysteries, these could be strategically chosen based
on
an
automated
preanalysis
of
each
mystery.
For
example,
obs
e
r
v
e
d
oscillatory behavior could suggest including sin and cos, and lack
thereof could suggest saving time by excluding them.
Our code could also be straightforwardly integrated into a larger
program discovering equations involving derivatives and integrals,
which frequently occur in physics equations. For example, if we sus
-
pect that our formula contains a partial differential equation, then
the user can simply estimate various derivatives from the data (or its
interpolation, using a neural network) and include them in the AI
Feynman algorithm as independent variables, thus discovering the
differential equation in question.
We saw how, even if the mystery data have very low noise, substantial
de facto noise was introduced by imperfect neural network fitting,
complicating subsequent solution steps. It will therefore be valuable
to explore better neural network architectures, ideally reducing fitt
i
n
g
noise to the 10
6
level. This may be easier than in many other con
-
texts, since we do not care whether the neural network generalizes
poorly outside the domain where we have data: As long as it is highl
y
accurate within this domain, it serves our purpose of correctly fact
o
r
-
ing separable functions, etc.
Our brute-force method can be better integrated with a neural net
-
work search for hidden simplicity. Our implemented symmetry search
simply tests whether two input variables
a
and
b
can be rep
l
a
c
e
d
b
y
a bivariate function of them, specifically +, ,
*
, or /, correspondi
n
g
to length 3 strings Â“
ab
+Â”, Â“
ab
Â”, Â“
ab
*
Â”, and Â“
ab
/Â”. This can be readily
generalized to longer strings involving two or more variables, for
example, bivariate functions
ab
2
or
e
a
cos
b
.
A
second
example
of
improved
brute-force
use
is
if
the
neural
network reveals that the function can be exactly solved after setting
some variable
a
equal to something else (say zero, one, or another
variable). A brute-force search can now be performed in the vicinity
of the discovered exact expression: For example, if the expression is
valid for
a
= 0, the brute-force search can insert additive terms that
vanish for
a
= 0 and multiplicative terms that equal unity for
a
= 0,
thus being likely to discover the full formula much faster than an
unrestricted brute-force search from scratch.
Last but not least, it is likely that marrying the best features
from both our method and genetic algorithms can spawn a metho
d
that outperforms both. Genetic algorithms such as Eureqa per
-
form quite
well
even
in
the presence of substantial noise, whether
they output not merely one hopefully correct formula, but rather
a Pareto frontier, a sequence of increasingly complex formulas
that provide progressively better accuracy. Although it may not
be clear which of these formulas is correct, it is more likely that
the correct formula is one of them than any particular one that
an
algorithm might guess.
When
our neural
network
identifies
separability, a so generated Pareto frontier could thus be used to
generate candidate formulas for one factor, after which each one
could be substituted back and tested as above, and the best solu
-
tion to
the
full
expression
would
be
retained.
Our
brute-force
algorithm
can similarly
be
upgraded
to
return
a
Pareto
frontier
instead of a single formula.
In summary, symbolic regression algorithms are getting better
and are likely to continue improving. We look forward to the day
when, for the first time in the history of physics, a computer, just
like Kepler, discovers a useful and hitherto unknown physics formula
through symbolic regression!
MATERIALS
AND
METHODS
The materials used for the symbolic regression tests are all in
t
h
e
FSReD, available at https://space.mit.edu/home/tegm
a
r
k
/
aifeynman.html. The method by which we have implemented o
u
r
algorithm is as a freely available software package made available a
t

--- PAGE 15 ---
15 of 16
https://github.com/SJ001/AI-Feynman; pseudocode is provided bel
o
w
f
o
r
s
y
m
m
e
t
r
y
and separability exploitation.
Algorithm 1 AI Feynman: Translational symmetry
Require
Dataset
D
= {(
x
,
y
)}.
Require net
: trained neural network
Require NN
error
: the neural network validation error
a=1
for
i
in
len(
x
)
do
:
for
j
in
len(
x
)
do
:
if
i < j:
x
t
=
x
x
t
[i] =
x
t
[i]+a
x
t
[j] =
x
t
[j]+a
error
=RMSE(
net
(
x
),
net
(
x
t
))
error
=
error
/RMSE(
net
(
x
))
if
error
<7 Ã—
NN
error
:
x
t
[
i
] =
x
t
[
i
]
x
t
[
j
]
x
t
= delete(
x
t
,j)
return
x
t
, i, j
Algorithm 2 AI Feynman: Additive separability
Require
Dataset
D
= {(
x
,
y
)}
Require net
: trained neural network
Require NN
error
: the neural network validation error
x
eq
=
x
for
i
in
len(
x
)
do
:
x
eq
[
i
] = mean(
x
[
i
])
for
i
in
len(
x
)
do
:
c
=combinations([1,2,...,len(
x
)],i)
for
idx
1
in
c
do
:
x
1
=
x
x
2
=
x
idx
2
= k
in
[1,len(
x
)]
not in
idx
1
for
j
in
idx
1
:
x
1
[
j
] = mean(
x
[
j
])
for
j
in
idx
2
:
x
2
[
j
] = mean(
x
[
j
])
error
=RMSE(
net
(
x
),
net
(
x
1
)+
net
(
x
2
)-
net
(
x
eq
))
error
=
error
/RMSE(
net
(
x
))
if error
<10 Ã—
NN
error
:
x
1
= delete(
x
1
,index
2
)
x
2
= delete(
x
2
,index
1
)
return
x
1
,
x
2
, index
1
, index
2
REFERENCES AND NOTES
1.
A.
KoyrÃ©,
The Astronomical Revolution: Copernicus-Kepler-Borelli
(Routledge, 2013).
2.
N.
M. Amil, N.
Bredeche, C.
GagnÃ©, S.
Gelly, M.
Schoenauer, O.Teytaud,
European
Conference on Genetic Programming
(Springer, 2009), pp.
327Â–338.
3.
S.
K. Pal, P.
P. Wang,
Genetic algorithms for pattern recognition
(CRC press, 2017).
4.
J.
D. Lohn, W.
F. Kraus, D.
S. Linden,
IEEE Antenna & Propagation Society Mtg.
3
,
814 (2002).
5.
D.
S.
Linden,
Proceedings 2002 NASA/DoD Conference on Evolvable Hardware
(IEEE, 2002),
pp.
147Â–151.
6.
H.
Yu, N.
Yu, The Pennsylvania State University, (University Park, 2003) pp.
1Â–9.
7.
S.
Panthong,
S.
Jantarang,
CCECE 2003-Canadian Conference on Electrical and Computer
Engineering. Toward a Caring and Humane Technology (Cat. No. 03CH37436)
(IEEE, 2003),
vol. 3, pp.
1597Â–1600.
8.
B.
Oh, Y.
Na, J.
Yang, S.
Park, J.
Nang, J.
Kim, Genetic algorithm-based dynamic vehicle
route search using car-to-car communication.
Adv. Electr. Comput. En.
10
, 81Â–86
(2010).
9.
A.
Ram, R.
Arkin, G.
Boone, M.
Pearce, Using genetic algorithms to
learn reactive control
parameters for
autonomous robotic navigation.
Adapt. Behav.
2
, 277Â–305 (1994).
10.
B.
Delman, Genetic algorithms in cryptography. Rochester Institute of Technology (2004).
11.
S. Kim, P. Lu, S. Mukherjee, M. Gilbert, L. Jing, V. Ceperic,  M. Soljacic, arXiv
preprint arXiv:1912.04825 (2019).
12.
R.
J.
Bauer,
Genetic algorithms and investment strategies
, (John Wiley & Sons, ed. 1, 1994),
vol. 19, p.
320.
13.
R.
Venkatesan, V.
Kumar, A genetic algorithms approach to
forecasting of
wireless
subscribers.
Int. J.
Forecast.
18
, 625Â–646 (2002).
14.
W.
L. Cava, T.
R. Singh, J.
Taggart, S.
Suri, J.
Moore,
International Conference on Learning
Representations
(2019), https://openreview.net/forum?id=Hke-JhA9Y7.
15.
S.
McAleer,
F.
Agostinelli,
A.
Shmakov,
P.
Baldi,
International Conference on Learning
Representations
(2019); https://openreview.net/forum?id=Hyfn2jCcKm.
16.
J.
R. Koza, J.
R. Koza,
Genetic Programming: On the Programming of Computers by Means of
Natural Selection
(MIT Press, 1992), vol. 1.
17.
M.
D.
Schmidt,
R.
R.
Vallabhajosyula,
J.
W.
Jenkins,
J.
E.
Hood,
A.
S.
Soni,
J.
P.
Wikswo,
H.
Lipson, Automated refinement and
inference of
analytical models for
metabolic
networks.
Phys. Biol.
8
, 055011 (2011).
18.
R.
K.
McRee,
Proceedings of the 12th Annual Conference Companion on Genetic and
Evolutionary Computation
(ACM, New
York, NY, USA, 2010), GECCO Â‘10, pp.
1983Â–1990;
http://doi.acm.org/10.1145/1830761.1830841.
19.
S.
Stijven,
W.
Minnebo,
K.
Vladislavleva,
Proceedings of the 13th Annual Conference
Companion on Genetic and Evolutionary Computation
(ACM, New
York, NY, USA, 2011),
GECCO Â‘11, pp.
623Â–630; http://doi.acm.org/10.1145/2001858.2002059.
20.
W.
Kong,
C.
Liaw,
A.
Mehta,
D.
Sivakumar,
International Conference on Learning
Representations
(2019); https://openreview.net/forum?id=rkluJ2R9KQ.
21.
T.
McConaghy,
Genetic Programming Theory and Practice IX
(Springer, 2011),
pp.
235Â–260.
22.
I.
Arnaldo,
U.-M.
OÂ’Reilly,
K.
Veeramachaneni,
Proceedings of the 2015 Annual Conference
on Genetic and Evolutionary Computation
(ACM, 2015), pp.
983Â–990.
23.
S.
L.
Brunton, J.
L.
Proctor, J.
N.
Kutz, Discovering governing equations from
data by
sparse identification of
nonlinear dynamical systems.
Proc. Natl. Acad. Sci. U.S.A.
113
,
3932Â–3937 (2016).
24.
M.
Quade, M.
Abel, J.
Nathanutz, S.
L.
Brunton, Sparse identification of
nonlinear
dynamics for
rapid model recovery.
Chaos
28
, 063116 (2018).
25.
D.
P. Searson, D.
E. Leahy, M.
J. Willis,
Proceedings of the International multiconference of
engineers and computer scientists
(Citeseer, 2010), vol. 1, pp.
77Â–80.
26.
R.
Praksova, Eureqa: Software review.
Genet. Program. Evol. M.
12
, 173Â–178 (2011).
27.
M.
Schmidt, H.
Lipson, Distilling free-form natural laws from
experimental data.
Science
324
, 81Â–85 (2009).
28.
H.
Mhaskar, Q.
Liao, T.
Poggio, Technical report, Center for Brains, Minds and Machines
(CBMM), arXiv (2016).
29.
H.
W.
Lin, M.
Tegmark, D.
Rolnick, Why does deep and
cheap learning work so
well?
J.
Stat. Phys.
168
, 1223Â–1247 (2017).
30.
T.
Wu, M.
Tegmark, Toward an
artificial intelligence physicist for
unsupervised learning.
Phys. Rev. E.
100
, 033311 (2019).
31.
L.
N.
Smith,
N.
Topin,
Super-convergence: Very fast training of residual networks using large
learning rates
(2018); https://openreview.net/forum?id=H1A5ztj3b.
32.
L.
N. Smith, A disciplined approach to neural network hyper-parameters: Part 1 Â– learning
rate, batch size, momentum, and weight decay. arXiv:1803.09820 (2018).
33.
J.
Howard
et
al
., Fastai, https://github.com/fastai/fastai (2018).
34.
R.
Feynman,
R.
Leighton,
M.
Sands,
The Feynman Lectures on Physics: The New Millennium
Edition: Mainly Mechanics, Radiation, and Heat
, vol. 1 (Basic Books, 1963);
https://books.google.com/books?id=d76DBQAAQBAJ.
35.
R.
Feynman,
R.
Leighton,
M.
Sands,
The Feynman Lectures on Physics
, vol. 2
in The
Feynman Lectures on Physics (Pearson/Addison-Wesley, 1963b);
https://books.google.com/books?id=AbruAAAAMAAJ.
36.
R.
Feynman,
R.
Leighton,
M.
Sands,
The Feynman Lectures on Physics
, vol. 3
in The
Feynman Lectures on Physics (Pearson/Addison-Wesley, 1963);
https://books.google.com/books?id=_6XvAAAAMAAJ.
37.
H.
Goldstein,
C.
Poole,
J.
Safko,
Classical Mechanics
(Addison Wesley, 2002);
https://books.google.com/books?id=tJCuQgAACAAJ.
38.
J.
D.
Jackson,
Classical electrodynamics
(Wiley, New
York, NY, ed. 3, 1999);
http://cdsweb.cern.ch/record/490457.
39.
S.
Weinberg,
Gravitation and Cosmology: Principles and Applications of the General Theory
of Relativity
(New York: Wiley, 1972).

--- PAGE 16 ---
16 of 16
40.
M.
Schwartz,
Quantum Field Theory and the Standard Model, Quantum Field Theory and the
Standard Model
(Cambridge Univ. Press, 2014);
https://books.google.com/books?id=HbdEAgAAQBAJ.
41.
J.
McDermott,
D.
R.
White,
S.
Luke,
L.
Manzoni,
M.
Castelli,
L.
Vanneschi,
W.
Jaskowski,
K.
Krawiec,
R.
Harper,
K.
De
Jong,
Proceedings of the 14th Annual Conference on Genetic a
n
d
Evolutionary Computation
(ACM, 2012), pp.
791Â–798.
Acknowledgments:
We thank R.
Domingos, Z.
Dong, M.
Skuhersky, A.
Tan, and T.
Wu for the
helpful comments, and the Center for Brains, Minds, and Machines (CBMM) for hospitality.
Funding:
This work was supported by The Casey and Family Foundation, the Ethics and
Governance of AI Fund, the Foundational Questions Institute, the Rothberg Family Fund for
Cognitive Science, and the Templeton World Charity Foundation Inc. The opinions expressed
in this publication are those of the authors and do not necessarily reflect the views of the
Templeton World Charity Foundation Inc.
Author contributions:
Concept, supervision, and
project management: M.T.
Design of methodology, programming, experimental validation, data
curation, data analysis, validation, and manuscript writing: S.-M.U. and M.T.
Competing interests:
The authors declare that they have no competing interests.
Data and materials availability:
All
data needed to evaluate the conclusions in the paper are present in the paper, at https://
space.mit.edu/home/tegmark/aifeynman.html, and at https://github.com/SJ001/AI-Feynman.
Any additional datasets, analysis details, and material recipes are available upon request.
Submitted 7 June 2019
Accepted 3 January 2020
Published 15 April 2020
10.1126/sciadv.aay2631
Citation:
S.-M.
Udrescu, M.
Tegmark, AI Feynman: A physics-inspired method for symbolic regression.
Sci. Adv.
6
, eaay2631 (2020).
