                                  • Adamwithweightdecay0.01,
                                  • Adamwithdropout(0.1) in the feed-forward networks of the transformer,
                                  • AdamWwithweightdecay0.01.
                           Table 9 presents the best performance of 5 models for each configuration. On average, dropout has
                           an adverse effect on learning, but there is no clear benefit of using weight decay, or AdamW over
                           Adam. Importantly, the separation in performance between single-epoch unlimited training, training
                           on smaller data budgets with more repetitions and two-set training persists across optimizers: the
                           effects we present are robust.
                           Table 9: Modular multiplication with different optimizers. Correctly predicted GCD of the best (of 5)
                           models for various optimizers. The effects we observe are robust under change of optimizer, with a very small
                           degradation for dropout for both the unlimited (single-epoch) and limited DB.
                                                                      One-set                   Two-set
                                                              Unlimited   50M    25M    Unlimited   50M    25M
                                          Adam                   28        49     61       70        72     63
                                          Adamwd=0.01            30        56     61       70        70     66
                                          AdamWwd=0.01           29        50     58       69        72     67
                                          Adamdropout=0.1        24        40     49       66        66     66
                                                                           13
