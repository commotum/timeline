                                                  Agent57: Outperforming the Atari Human Benchmark
               gamesofSkiing or Solaris. The game of Skiing is a canon-        R2D2achievestheoptimalscorewhileNGUperformssim-
               ical example due to its peculiar reward structure. The goal     ilar to a random policy. One shortcoming of NGU is that it
               of the game is to run downhill through all gates as fast as     collects the same amount of experience following each of
               possible. A penalty of ﬁve seconds is given for each missed     its policies, regardless of their contribution to the learning
               gate. The reward, given only at the end, is proportional to     progress. Some games require a signiﬁcantly different de-
               the time elapsed. Therefore long-term credit assignment         gree of exploration to others. Intuitively, one would want
               is needed to understand why an action taken early in the        to allocate the shared resources (both network capacity and
               game (e.g. missing a gate) has a negative impact in the         data collection) such that end performance is maximized.
               obtained reward. Secondly, exploration: efﬁcient explo-         WeproposeallowingNGUtoadaptitsexplorationstrategy
               ration can be critical to effective learning in RL. Games       over the course of an agent’s lifetime, enabling specializa-
               like Private Eye, Montezuma’s Revenge, Pitfall! or Venture      tion to the particular game it is learning. This is the ﬁrst
               are widely considered hard exploration games (Bellemare         signiﬁcant improvement we make to NGU to allow it to be
               et al., 2016; Ostrovski et al., 2017) as hundreds of actions    a more general agent.
               mayberequiredbeforeaﬁrstpositiverewardisseen. Inor-             Recent work on long-term credit assignment can be cate-
               der to succeed, the agents need to keep exploring the envi-     gorizedintoroughlytwotypes: ensuringthatgradientscor-
               ronment despite the apparent impossibility of ﬁnding pos-       rectly assign credit (Ke et al., 2017; Weber et al., 2019; For-
               itive rewards. These problems are particularly challenging      tunatoetal.,2019)andusingvaluesortargetstoensurecor-
               in large high dimensional state spaces where function ap-       rect credit is assigned (Arjona-Medina et al., 2019; Hung
               proximation is required.                                        et al., 2019; Liu et al., 2019; Harutyunyan et al., 2019; Fer-
               Exploration algorithms in deep RL generally fall into three     ret et al., 2020). NGU is also unable to cope with long-term
               categories:  randomized value functions (Osband et al.,         credit assignment problems such as Skiing or Solaris where
               2016; Fortunato et al., 2017; Salimans et al., 2017; Plap-      it fails to reach 100% HNS. Advances in credit assignment
               pert et al., 2017; Osband et al., 2018), unsupervised policy    in RL often involve a mixture of both approaches, as val-
               learning (Gregor et al., 2016; Achiam et al., 2018; Eysen-      ues and rewards form the loss whilst the ﬂow of gradients
               bach et al., 2018) and intrinsic motivation (Schmidhuber,       through a model directs learning.
               1991; Oudeyer et al., 2007; Barto, 2013; Bellemare et al.,      In this work, we propose tackling the long-term credit as-
               2016; Ostrovski et al., 2017; Fu et al., 2017; Tang et al.,     signment problem by improving the overall training sta-
               2017; Burda et al., 2018; Choi et al., 2018; Savinov et al.,    bility, dynamically adjusting the discount factor, and in-
               2018; Puigdomènech Badia et al., 2020). Other work com-         creasing the backprop through time window. These are
               bines handcrafted features, domain-speciﬁc knowledge or         relatively simple changes compared to the approaches pro-
               privileged pre-training to side-step the exploration prob-      posed in previous work, but we ﬁnd them to be effective.
               lem, sometimes only evaluating on a few Atari games (Ay-        Muchrecentworkhasexploredthisproblemofhowtody-
               tar et al., 2018; Ecoffet et al., 2019). Despite the encourag-  namically adjust hyperparameters of a deep RL agent, e.g.,
               ing results, no algorithm has been able to signiﬁcantly im-     approaches based upon evolution (Jaderberg et al., 2017),
               prove performance on challenging games without deterio-         gradients (Xu et al., 2018) or multi-armed bandits (Schaul
               rating performanceontheremaininggameswithoutrelying             et al., 2019). Inspired by Schaul et al. (2019), we propose
               on human demonstrations (Pohlen et al., 2018). Notably,         using a simple non-stationary multi-armed bandit (Garivier
               amongst all this work, intrinsic motivation, and in partic-     &Moulines, 2008) to directly control the exploration rate
               ular, Never Give Up (NGU; Puigdomènech Badia et al.,            and discount factor to maximize the episode return, and
               2020) has shown signiﬁcant recent promise in improving          then provide this information to the value network of the
               performance on hard exploration games. NGU achieves             agent as an input. Unlike Schaul et al. (2019), 1) it controls
               this by augmenting the reward signal with an internally         the exploration rate and discount factor (helping with long-
               generated intrinsic reward that is sensitive to novelty at two  termcredit assignment), and 2) the bandit controls a family
               levels: short-term novelty within an episode and long-term      of state-action value functions that back up the effects of
               novelty across episodes. It then learns a family of policies    exploration and longer discounts, rather than linearly tilt-
               for exploring and exploiting (sharing the same parameters),     ing a common value function by a ﬁxed functional form.
               with the end goal of obtaining the highest score under the
               exploitative policy. However, NGU is not the most gen-          In summary, our contributions are as follows:
               eral agent: much like R2D2 and MuZero are able to per-
               form strongly on all but few games, so too NGU suffers           1. A new parameterization of the state-action value func-
               in that it performs strongly on a smaller, different set of        tion that decomposes the contributions of the intrinsic
               games to agents such as MuZero and R2D2 (despite be-               and extrinsic rewards. As a result, we signiﬁcantly in-
               ing based on R2D2). For example, in the game Surround              creasethetrainingstabilityoveralargerangeofintrinsic
                                                                                  reward scales.
