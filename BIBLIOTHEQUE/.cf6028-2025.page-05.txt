                                                1.0               Mean accuracy            1.0              Mean accuracy            1.0                                       1.0              Mean accuracy
                                                0.8               ±1 std. deviation        0.8              ±1 std. deviation        0.8                                       0.8              ±1 std. deviation
                                                0.6                                        0.6                                       0.6
                                                                                                                                                                               0.6
                                                0.4                                        0.4                                       0.4
                                               verage accuracy                            verage accuracy0.2                        verage accuracy                           verage accuracy0.4
                                               A0.2                                       A                                         A0.2        Mean accuracy                 A
                                                                                           0.0                                                  ±1 std. deviation              0.2
                                                0.0                                                                                  0.0
                                                  4000 6000 8000 10000120001400016000        4000 6000 8000 10000120001400016000          6000 8000 10000120001400016000               4000    6000   8000   10000
                                                       Average completion tokens                 Average completion tokens                 Average completion tokens                 Average completion tokens
                                                      (a) AIME2024                             (b) AIME2025-I                            (c) AIME2025-II                       (d) GPQADiamond
                                              Figure 3: Mean accuracy vs. average completion tokens for different datasets averaged
                                              across all models
                                              single gold answer, representational ambiguities can arise. For instance, the fraction 1 may
                                                                                                                                                                                                         2
                                              be written as “1/2” or “0.5,” both of which are semantically equivalent but syntactically
                                              distinct. In our setting, accuracy evaluation is simplified because three of the four datasets
                                              are derived from AIME, where answers are restricted to three-digit integers. The remain-
                                              ing dataset (GPQA) is multiple-choice with options limited to A–D. For both cases, we
                                              explicitly instruct the model to provide its final answer within delimiters, which facilitates
                                              reliable extraction of the predicted value.
                                              Token consumption. We count token consumption in two distinct ways, capturing dif-
                                              ferent aspects of the utilized compute. Total tokens refer to the total number of tokens
                                              generated across all the traces in order to arrive at the generated answer. Sequential to-
                                              kens, on the other hand, refer to the number of tokens that must necessarily be produced
                                              in a sequence, and are dependentonthepreviouslygeneratedtokens. Forinstance,during
                                              vanilla decoding using greedy or stochastic sampling, generating a trace x would involve
                                              a sequential token count of |x| since xi can only be generated after all of Xj, j < i are gen-
                                              erated. For an inference strategy which requires generating N complete traces x1, x2, ...,
                                              xN,thesequentialtokencountwouldbemaxN |xi|sinceallthetokensinthelongesttrace
                                                                                                                                 i=1
                                              wouldbedependentontheonebeforeit. Therefore,whiletotaltokensmeasuretheoverall
                                              compute used, sequential token count gives an estimate of the minimum possible latency
                                              in generating a given output (assuming each token generation takes the same time).
                                              2.5      MeasuringProblemDifficulty
                                              In order to devise a granular recipe for the appropriate scaling strategy to use at test-time,
                                              it is crucial to take into account the problem difficulty. The direct way to measure difficulty
                                              of a question is to simply calculate the task accuracy for a given problem, averaged across
                                              all models and sampled traces. Another, more indirect approach would be to calculate
                                              the average tokens generated for the task, again averaged across all models and outputs.
                                              Interestingly, we find that both these metrics are correlated (Figure 3), and that this overall
                                              trend holds across all datasets, where a reasoning (as well as a non-reasoning) models
                                              “thinks” longer on harder (as measured through accuracy) problems.
                                                    Finding
                                                    Bothreasoningandnon-reasoningmodelsthinklongerforharderproblems.
                                              Whilebothreasoningandnon-reasoningmodelsexpendmoretokensonharderproblems,
                                              longer trace lengths alone do not guarantee improved quality. Recent research suggests
                                              that excessive deliberation can harm performance by propagating early mistakes, con-
                                              tributing to the growing view that more compute is not always better (Gema et al., 2025).
                                              3      Results
                                              3.1      Beamsearchshowsinverseornoscaling
                                              Wenoticethatacrosstwoofthemodelfamilies—short-horizonandnon-reasoning—beam
                                              search exhibits a consistent inverse-scaling pattern: performance degrades monotonically as
                                                                                                                                5
