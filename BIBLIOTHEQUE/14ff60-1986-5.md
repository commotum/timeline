# Learning representations by back-propagating errors (1986)
Source: 14ff60-1986.pdf

## Core reasons
- Proposes a new learning procedure (back-propagation) for neural networks and explains how it adjusts weights to minimize error, which is a foundational training method.
- Focuses on learning representations via weight adjustment in multi-unit networks rather than datasets, benchmarks, positional encodings, or higher-dimensional transformer adaptations.

## Evidence extracts
- "We describe a new learning procedure, back-propagation, for networks of neurone-like units." (Section Abstract)
- "The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector." (Section Abstract)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
oxed{5}
$$
