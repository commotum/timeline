                       to the ﬁne spatial and temporal resolutions required in real-world scenarios. In this context, deep
                       learning methods are receiving strongly growing attention [40, 4, 18] and show promise to account
                       for those components of the solutions that are difﬁcult to resolve or are not well captured by our
                       physical models. Physical models typically come in the form of PDEs and are discretized in order to
                       be processed by computers. This step inevitably introduces numerical errors. Despite a vast amount
                       of work [15, 2] and experimental evaluations [7, 41], analytic descriptions of these errors remain
                       elusive for most real-world applications of simulations.
                       In our work, we speciﬁcally target the numerical errors that arise in the discretization of PDEs. We
                       showthat, despite the lack of closed-form descriptions, discretization errors can be seen as functions
                       with regular and repeating structures and, thus, can be learned by neural networks. Once trained,
                       such a network can be evaluated locally to improve the solution of a PDE-solver, i.e., to reduce its
                       numerical error.
                       The core of most numerical methods contains some form of iterative process – either in the form
                       of repeated updates over time for explicit solvers or even within a single update step for implicit
                       solvers. Hence, we focus on iterative PDE solving algorithms [17]. We show that neural networks
                       can achieve excellent performance if they take the reaction of the solver into account. This interaction
                       is not possible with supervised learning on pre-computed data alone. Even small inference errors
                       of a supervised model can quickly accumulate over time [57, 29], leading to a data distribution that
                       differs from the distribution of the pre-computed data. For supervised learning methods, this causes
                       deteriorated inference at best and solver explosions at worst.
                       We demonstrate that neural networks can be successfully trained if they can interact with the
                       respective PDE solver during training. To achieve this, we leverage differentiable simulations [1, 58].
                       Differentiable simulations allow a trained model to autonomously explore and experience the physical
                       environment and receive directed feedback regarding its interactions throughout the solver iterations.
                       Hence, our work ﬁts into the broader context of machine learning as differentiable programming, and
                       wespeciﬁcally target recurrent interactions of highly non-linear PDEs with deep neural networks.
                       This combination bears particular promise: it improves generalizing capabilities of the trained models
                       by letting the PDE-solver handle large-scale changes to the data distribution such that the learned
                       modelcanfocusonlocalized structures not captured by the discretization. While physical models
                       generalize very well, learned models often specialize in data distributions seen at training time.
                       However, we will show that, by combining PDE-based solvers with a learned model, we can arrive
                       at hybrid methods that yield improved accuracy while handling solution manifolds with signiﬁcant
                       amounts of varying physical behavior.
                       We show the advantages of training via differentiable physics for explicit and implicit solvers
                       applied to a broad class of canonical PDEs. For explicit and semi-implicit solvers, we consider
                       advection-diffusion systems as well as different types of Navier-Stokes variants. We showcase
                       models trained with up to 128 steps of a differentiable simulator and apply our model to complex
                       three-dimensional (3D) ﬂows, as shown in Fig. 1. Additionally, we present a detailed empirical study
                       of different approaches for training neural networks in conjunction with iterative PDE-solvers for
                       recurrent rollouts of several hundred time steps. On the side of implicit solvers, we consider the
                       Poisson problem [37], which is an essential component of many PDE models. Here, our method
                       outperforms existing techniques on predicting initial guesses for a conjugate gradient (CG) solver by
                       receiving feedback from the solver at training time. The source code for this project is available at
                       https://github.com/tum-pbs/Solver-in-the-Loop.
                       Previous Work  Combiningmachinelearning techniques with PDE models has a long history in
                       machine learning [13, 28, 8]. More recently, deep-learning-based methods were successfully applied
                       to infer stencils of advection-diffusion problems [4], to discover PDE formulations [35, 42, 52], and
                       to analyze families of Poisson equations [36]. While identifying governing equations represents an
                       interesting and challenging task, we instead focus on a general method to improve the solutions of
                       chosen spaces of solutions.
                       Other studies have investigated the similarities of dynamical systems and deep learning methods
                       [65] and employed conservation laws to learn systems described by Hamiltonian mechanics [18, 12].
                       Existing studies have also identiﬁed discontinuities in ﬁnite-difference solutions with deep learning
                       [46] and focused on improving the iterative behavior of linear solvers [24]. So-called Koopman
                       operators likewise represent an interesting opportunity for deep learning algorithms [40, 32]. While
                       these methods replace the PDE-based time integration with a learned version, our models rely on and
                                                               2
