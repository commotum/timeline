# How Can Self-Attention Networks Recognize Dyck-n Languages? (2020)
Source: 712425-2020.pdf

## Core reasons
- The paper studies recognizing Dyck-n formal languages using self-attention networks, framing the work as an empirical analysis of SA capability on hierarchical languages.
- It reports evidence about SA's ability to learn hierarchical structures without recursion, which is a foundational capability claim rather than a new positional encoding, dimension lifting, or computation mechanism.

## Evidence extracts
- "We focus on the recognition of Dyck-n (Dn) languages with self-attention (SA) networks," (p. 1)
- "which provides evidence on the ability of SA to learn hierarchies without recursion." (p. 1)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
