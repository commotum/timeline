                        Published as a conference paper at ICLR 2025
                        4.5   PERFORMANCE WITH FIXED-DEPTH MODELS
                        Weevaluatedthreefixed-depthCNNmodels—shallow(4layers),medium(9layers),anddeep(16layers)—on
                        the CIFAR-100 dataset to compare their performance against the adaptive MIND model. Table 4 shows the
                        performance of these traditional architectures, which lack adaptive mechanisms, reporting Top-1 and Top-5
                        accuracy, inference time, and FLOPs.
                        TheMINDmodeloutperformsallfixed-depthmodelsinbothaccuracyandefficiency. Notably, the MIND
                        modelwithcomplexinputsachieves comparable Top-5 accuracy to the 16-layer deep network while requiring
                        significantly fewer FLOPs and faster inference. MIND’s dynamic computation, which eliminates redundant
                        processing based on input complexity, stands in stark contrast to the rigid, predetermined structures of
                        conventional CNNs (LeCun et al., 2015) and LSTMs (Hochreiter & Schmidhuber, 1995; 1997).
                        Table 4: Performance comparison of the MIND model and fixed-depth CNN models with varying input
                        complexity on the CIFAR-100 dataset. The table includes Top-1 and Top-5 accuracy, average layers used,
                        FPI iterations, inference time, and FLOPs.
                          Model               Input Complexity   Top-1↑   Top-5↑    Avg. Layers↑   Avg. FPI↑    Inference  FLOPs
                                                                  Acc.     Acc.         Used       Iterations  Time(s)↓     (G)↓
                          MIND                    Simple         87.2%     93.9%        1.37          2.81       0.032       0.92
                          MIND                    Medium         86.8%     92.5%        2.15          4.63       0.041       1.21
                          MIND                    Complex        85.3%     91.4%        2.89          6.42       0.048       1.27
                          Shallow (4 layers)         —           53.5%     63.2%        4.00           —         0.020       0.80
                          Medium(9layers)            —           74.5%     82.3%        9.00           —         0.070       1.50
                          Deep(16layers)             —           79.6%     91.7%       16.00           —         0.120       2.00
                        4.6   COMPARISON WITH FIXED COMPRESSION TECHNIQUES
                        Pruning and Quantization       Unlike static compression techniques such as pruning (Liang et al., 2021)
                        and quantization (Gholami et al., 2022), which uniformly reduce model size or precision, the MIND model
                        dynamically allocates computational resources based on input complexity. By adapting its computational
                        graph per input, MIND efficiently processes inputs of varying complexity without compromising performance.
                        MINDadjuststhenumberofprocessed tokens and computational depth in real-time, optimizing resource
                        utilization. This flexibility enables it to handle diverse inputs—from brief queries to extensive docu-
                        ments—without the need for separate models or fixed compression ratios.
                        AsshowninTable5,MINDoutperformstraditionalstaticmethodslikebaselineResNet-50,prunedResNet-50
                        (50%sparsity), and 8-bit quantized ResNet-50 in both accuracy and computational efficiency. Specifically,
                        MINDachievesaTop-1accuracyof88.0%usingsignificantly fewer parameters (5.31M) and FLOPs (1.05G).
                        Bytailoring computation to each input’s requirements, MIND’s adaptive computation framework effectively
                        balances efficiency and performance, avoiding the underfitting or overfitting that can result from uniform
                        reductions applied by static methods.
                        Table 5: Comparative analysis of MIND and ResNet-50 variants. The table includes the number of parameters,
                        FLOPs,andTop-1/Top-5accuracy.
                                       Method                            Params(M)     AvgFLOPs         Accuracy↑
                                                                                                      Top-1    Top-5
                                       ResNet-50                             25.6         4.12G       76.0%    94%
                                       Pruned ResNet-50 (50% sparsity)       12.8         2.06G       64.8%   73.5%
                                       8-bit Quantized ResNet-50             25.6         4.12G       75.5%   85.1%
                                       MIND(Simple)                                       0.80G       88.5%   96.5%
                                       MIND(Medium)                          5.31         1.05G       88.0%   96.2%
                                       MIND(Complex)                                      2.00G       87.5%   96.0%
                                                                         9
