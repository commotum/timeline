          Published as a conference paper at ICLR 2025
          Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85–117, 2015.
          Eric Schulz and Samuel J. Gershman. The algorithmic architecture of exploration in the human brain. Current
           opinion in neurobiology, 55:7–14, 2019.
          Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald
           Metzler. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:
           17456–17472, 2022.
          Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
           Asimplewaytopreventneural networks from overfitting. Journal of Machine Learning Research, 15(1):
           1929–1958, 2014.
          Mingxing Tan and Quoc Le. EfficientNetV2: Smaller models and faster training. In International Conference
           onMachineLearning, pp. 10096–10106. PMLR, 2021.
          Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early
           exiting from deep neural networks. In 23rd International Conference on Pattern Recognition (ICPR), pp.
           2464–2469. IEEE, 2016.
          Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser,
           and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30,
           2017.
          Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD
           Birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
           URLhttps://www.vision.caltech.edu/datasets/cub_200_2011/.
          DayongWang,AdityaKhosla,RishabGargeya,HumayunIrshad,andAndrewH.Beck. Deeplearningfor
           identifying metastatic breast cancer. arXiv preprint arXiv:1606.05718, 2016.
          Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Skip connections matter: On the
           transferability of adversarial examples generated with ResNets. arXiv preprint arXiv:2002.05990, 2020.
          BingXu,NaiyanWang,TianqiChen,andMuLi. Empiricalevaluationofrectifiedactivationsinconvolutional
           network. arXiv preprint arXiv:1505.00853, 2015.
          YongYu,XiaoshengSi,ChanghuaHu,andJianxunZhang. Areviewofrecurrentneuralnetworks: LSTM
           cells and network architectures. Neural computation, 31(7):1235–1270, 2019.
          Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Junsuk Choe, and Sanghyuk Chun. Re-
           labeling Imagenet: from single to multi-labels, from global to localized labels. In Proceedings of the
           IEEE/CVFConferenceonComputerVisionandPatternRecognition, pp. 2340–2350, 2021.
                             15
