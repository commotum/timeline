                                  The LLM ARChitect: Solving ARC-AGI
                                              Is A Matter of Perspective
                                                 ∗†1                  ∗†2                          †3
                                Daniel Franzen      , Jan Disselhoff     , and David Hartmann
                                                    1dfranzen.it@gmail.com
                                                   2JanDissel.it@gmail.com
                                             3Lambda, Inc., davidh@lambdal.com
                                                              Abstract
                                    Large language models (LLMs) have made impressive progress, but
                                 they still struggle with abstract reasoning tasks like the Abstraction
                                 and Reasoning Corpus (ARC-AGI). Prior approaches have not been
                                 able to achieve high scores on ARC-AGI, suggesting a gap between
                                 current AI systems and human-level reasoning. In this work we ap-
                                 proach the problem using tailored data augmentation techniques, op-
                                 timizing the data format as well as training performance, and lever-
                                 aging our generative model both as a predictor and as a classifier for
                                 good solutions. We are able to maximize performance despite limited
                                                                             1
                                 computational resources, achieving 53.5 (56.5 ) points on the private
                                 evaluation set during the Kaggle ARC Prize 2024 Contest. Addition-
                                 ally, we are able to solve 72.5 out of 100 randomly split-off tasks from
                                 the public evaluation set (while merging the other 300 tasks into our
                                 training data). The code of our submission is publicly available in the
                                                    2                                          3
                                 Kaggle competition and in the Github repository of this paper.
                              ∗Contributing equally to the Kaggle ARC Prize 2024.
                              †Contributing equally to this paper.
                              1Run finished after kaggle submission deadline
                              2Team “the ARChitects”: https://www.kaggle.com/competitions/arc-prize-2024
                              3Github Repository: https://github.com/da-fr/arc-prize-2024
                                                                  1
