                               Algorithm 1 Depth-First Probability-Guided Sampling for LLMs
                               The algorithm presented here assumes that the model supports internal
                               caching for already seen sequences and only needs to process the newly added
                               tokens.
                               Our actual implementation differs from this simple variant, as we are using
                               unsloth, which does not support dynamic caching and requires us to prune
                               the key-value-cache of the transformer ourselves.
                               Furthermore, we use various performance optimizations, like a simultane-
                               ous initial forward pass of the best known sequence including prompt and
                               prediction (which is much faster than token-by-token generation) as well
                               as aggregating the sequences during backtracking to avoid the unnecessary
                               processing of sequences that would be discarded later.
                                1: procedureDFS_sample(model,prompt,threshold,max_len,eos_id)
                                2:     Input: model is the language model
                                3:     Input: prompt is the prompt that should be completed
                                4:     Input: threshold is the maximum negative log probability allowed
                                5:     Input: max_len is the maximum length (including the prompt)
                                6:     Input: eos_id is the index of the end of sentence token
                                7:
                                8:     function Explore(tokens,score)
                                9:          if tokens[−1] = eos_id or |tokens| ≥ max_len then
                               10:              return (score,tokens)
                               11:          end if
                               12:
                               13:          next_token_logits ← model.predict_logits(tokens)[−1]
                               14:          next_token_log_prob ← −log_softmax(logits)
                               15:          valid_sequences ← ∅
                               16:
                               17:          for each possible next token t do
                               18:              next_score ← score+next_token_log_prob[t]
                               19:              if next_score ≤ threshold then
                               20:                  next_tokens ← current_tokens+[t]
                               21:                  continuations ← Explore(next_tokens,next_score)
                               22:                  valid_sequences ← valid_sequences∪continuations
                               23:              end if
                               24:          end for
                               25:     end function
                               26:
                               27:     return Explore(prompt,0.0)
                               28: end procedure
                                                                           20
