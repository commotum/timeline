

--- Page 1 ---

                                ImageNet: ALarge-Scale Hierarchical Image Database
                                 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li and Li Fei-Fei
                                         Dept. of Computer Science, Princeton University, USA
                                 {jiadeng, wdong, rsocher, jial, li, feifeili}@cs.princeton.edu
                                     Abstract                                content-basedimagesearchandimageunderstandingalgo-
                                                                             rithms, as well as for providing critical training and bench-
                The explosion of image data on the Internet has the po-      marking data for such algorithms.
             tential to foster more sophisticated and robust models and         ImageNetusesthehierarchicalstructureofWordNet[9].
             algorithms to index, retrieve, organize and interact with im-   Each meaningful concept in WordNet, possibly described
             ages and multimedia data. But exactly how such data can         by multiple words or word phrases, is called a “synonym
             be harnessed and organized remains a critical problem. We       set” or “synset”. There are around 80,000 noun synsets
             introduce here a new database called “ImageNet”, a large-       in WordNet. In ImageNet, we aim to provide on aver-
             scale ontology of images built upon the backbone of the         age 500-1000 images to illustrate each synset. Images of
             WordNetstructure. ImageNet aims to populate the majority        each concept are quality-controlled and human-annotated
             of the 80,000 synsets of WordNet with an average of 500-        as described in Sec. 3.2. ImageNet, therefore, will offer
             1000 clean and full resolution images. This will result in      tens of millions of cleanly sorted images. In this paper,
             tens of millions of annotated images organized by the se-       wereport the current version of ImageNet, consisting of 12
             mantic hierarchy of WordNet. This paper offers a detailed       “subtrees”: mammal, bird, ﬁsh, reptile, amphibian, vehicle,
             analysis of ImageNet in its current state: 12 subtrees with     furniture, musical instrument, geological formation, tool,
             5247 synsets and 3.2 million images in total. We show that      ﬂower, fruit. These subtrees contain 5247 synsets and 3.2
             ImageNet is much larger in scale and diversity and much         million images. Fig. 1 shows a snapshot of two branches of
             more accurate than the current image datasets. Construct-       the mammal and vehicle subtrees. The database is publicly
             ing such a large-scale database is a challenging task. We       available at http://www.image-net.org.
             describe the data collection scheme with Amazon Mechan-            The rest of the paper is organized as follows: We ﬁrst
             ical Turk. Lastly, we illustrate the usefulness of ImageNet     show that ImageNet is a large-scale, accurate and diverse
             through three simple applications in object recognition, im-    image database (Section 2). In Section 4, we present a few
             ageclassiﬁcation and automatic object clustering. We hope       simple application examples by exploiting the current Ima-
             that the scale, accuracy, diversity and hierarchical struc-     geNet, mostly the mammal and vehicle subtrees. Our goal
             ture of ImageNet can offer unparalleled opportunities to re-    is to show that ImageNet can serve as a useful resource for
             searchers in the computer vision community and beyond.          visual recognition applications such as object recognition,
                                                                             imageclassiﬁcationandobjectlocalization. Inaddition,the
                                                                             construction of such a large-scale and high-quality database
             1. Introduction                                                 can no longer rely on traditional data collection methods.
                                                                             Sec. 3 describes how ImageNet is constructed by leverag-
                The digital era has brought with it an enormous explo-       ing Amazon Mechanical Turk.
             sion of data. The latest estimations put a number of more       2. Properties of ImageNet
             than 3 billion photos on Flickr, a similar number of video
             clips on YouTube and an even larger number for images in           ImageNet is built upon the hierarchical structure pro-
             the Google Image Search database. More sophisticated and        vided by WordNet. In its completion, ImageNet aims to
             robust models and algorithms can be proposed by exploit-        contain in the order of 50 million cleanly labeled full reso-
             ing these images, resulting in better applications for users    lution images (500-1000 per synset). At the time this paper
             to index, retrieve, organize and interact with these data. But  is written, ImageNet consists of 12 subtrees. Most analysis
             exactly how such data can be utilized and organized is a        will be based on the mammal and vehicle subtrees.
             problem yet to be solved. In this paper, we introduce a new
             image database called “ImageNet”, a large-scale ontology        Scale   ImageNet aims to provide the most comprehensive
             of images. We believe that a large-scale ontology of images     and diverse coverage of the image world. The current 12
             is a critical resource for developing advanced, large-scale     subtrees consist of a total of 3.2 million cleanly annotated
                                                                         1


--- Page 2 ---

                    mammal                  placental               carnivore                canine                    dog              working dog                husky
                      vehicle                      craft                    watercraft                sailing vessel                sailboat                   trimaran
                 Figure 1: A snapshot of two root-to-leaf branches of ImageNet: the top row is from the mammal subtree; the bottom row is from the
                 vehicle subtree. For each synset, 9 randomly sampled images are presented.
                                         Summary of selected subtrees                                   ESP Cat Subtree                     Imagenet Cat Subtree
                          0.2                                                                              376
                                                           Avg. synset    Total # 
                                     Subtree    # Synsets
                                                               size        image
                                    Mammal         1170        737          862K
                         0.15       Vehicle        520         610          317K
                                    GeoForm        176         436          77K                                               1830
                                    Furniture      197         797          157K
                        ercentage0.1Bird           872         809          705K
                        p           MusicInstr     164         672          110K
                         0.05
                                                                                                       ESP Cattle Subtree                 Imagenet Cattle Subtree
                            0                                                                         176
                             0         500        1000        1500       2000        2500
                                            # images per synset
                 Figure 2: Scale of ImageNet. Red curve: Histogram of number                                                 1377
                 of images per synset. About 20% of the synsets have very few
                 images. Over 50% synsets have more than 500 images. Table:                          Figure 3: Comparison of the “cat” and “cattle” subtrees between
                 Summaryofselectedsubtrees. Forcompleteandup-to-datestatis-                          ESP [25] and ImageNet. Within each tree, the size of a node is
                 tics visit http://www.image-net.org/about-stats.                                    proportional to the number of images it contains. The number of
                                                                                                     images for the largest node is shown for each tree. Shared nodes
                 images spread over 5247 categories (Fig. 2). On average                             between an ESP tree and an ImageNet tree are colored in red.
                 over 600 images are collected for each synset. Fig. 2 shows
                 the distributions of the number of images per synset for the                        gorylabelsintoasemantichierarchybyusingWordNet,the
                 current ImageNet 1. To our knowledge this is already the                            density of ImageNet is unmatched by others. For example,
                 largest clean image dataset available to the vision research                        to our knowledgenoexistingvisiondatasetoffersimagesof
                 community,intermsofthetotalnumberofimages,number                                    147 dog categories. Fig. 3 compares the “cat” and “cattle”
                 ofimagespercategoryaswellasthenumberofcategories2.                                  subtrees of ImageNet and the ESP dataset [25]. We observe
                                                                                                     that ImageNet offers much denser and larger trees.
                 Hierarchy        ImageNet organizes the different classes of
                 images in a densely populated semantic hierarchy.                       The         Accuracy We would like to offer a clean dataset at all
                 mainasset of WordNet [9] lies in its semantic structure, i.e.                       levels of the WordNet hierarchy. Fig. 4 demonstrates the
                 its ontology of concepts. Similarly to WordNet, synsets of                          labeling precision on a total of 80 synsets randomly sam-
                 images in ImageNet are interlinked by several types of re-                          pled at different tree depths. An average of 99.7% preci-
                 lations, the “IS-A” relation being the most comprehensive                           sion is achieved on average. Achieving a high precision for
                 and useful. Although one can map any dataset with cate-                             all depths of the ImageNet tree is challenging because the
                                                                                                     lower in the hierarchy a synset is, the harder it is to classify,
                    1About 20% of the synsets have very few images, because either there             e.g. Siamese cat versus Burmese cat.
                 are very few web images available, e.g. “vespertilian bat”, or the synset by
                 deﬁnition is difﬁcult to be illustrated by images, e.g. “two-year-old horse”.
                    2It is claimed that the ESP game [25] has labeled a very large number            Diversity       ImageNet is constructed with the goal that ob-
                 of images, but only a subset of 60K images are publicly available.                  jects in images should have variable appearances, positions,


--- Page 3 ---

                      1                                                                 datasets are needed for the next generation of algorithms.
                   0.95                                                                 Thecurrent ImageNet offers 20× the number of categories,
                                                                                        and 100× the number of total images than these datasets.
                  precision0.9
                             1    2     3     4     5     6    7     8     9            TinyImage TinyImage [24] is a dataset of 80 million
                                             tree depth                                 32 × 32 low resolution images, collected from the Inter-
               Figure 4: Percent of clean images at different tree depth levels in      net by sending all words in WordNet as queries to image
               ImageNet. A total of 80 synsets are randomly sampled at every            search engines. Each synset in the TinyImage dataset con-
               tree depth of the mammal and vehicle subtrees. An independent            tains an average of 1000 images, among which 10-25% are
               group of subjects veriﬁed the correctness of each of the images.         possibly clean images. Although the TinyImage dataset has
               Anaverageof99.7%precision is achieved for each synset.                   hadsuccesswithcertainapplications,thehighlevelofnoise
                                                                                        and low resolution images make it less suitable for gen-
                               ImageNet    TinyImage    LabelMe     ESP    LHill        eral purpose algorithm development, training, and evalua-
                LabelDisam        Y            Y           N         N       Y          tion. Compared to the TinyImage dataset, ImageNet con-
                Clean             Y            N           Y         Y       Y          tains high quality synsets (∼ 99% precision) and full reso-
                DenseHie          Y            Y           N         N       N          lution images with an average size of around 400 × 350.
                FullRes           Y            N           Y         Y       Y
                PublicAvail       Y            Y           Y         N       N          ESPdataset The ESP dataset is acquired through an on-
                Segmented         N            N           Y         N       Y          line game [25]. Two players independently propose labels
               Table 1: Comparison of some of the properties of ImageNet ver-           to one image with the goal of matching as many words as
               sus other existing datasets.  ImageNet offers disambiguated la-          possible in a certain time limit. Millions of images are la-
               bels (LabelDisam), clean annotations (Clean), a dense hierarchy          beled through this game, but its speeded nature also poses a
               (DenseHie), full resolution images (FullRes) and is publicly avail-      major drawback. Rosch and Lloyd [20] have demonstrated
               able (PublicAvail). ImageNet currently does not provide segmen-          that humans tend to label visual objects at an easily acces-
               tation annotations.                                                      sible semantic level termed as “basic level” (e.g. bird), as
               view points, poses as well as background clutter and occlu-              opposed to more speciﬁc level (“sub-ordinate level”, e.g.
               sions. In an attempt to tackle the difﬁcult problem of quan-             sparrow), or more general level (“super-ordinate level”, e.g.
               tifying image diversity, we compute the average image of                 vertebrate). Labels collected from the ESP game largely
               eachsynsetandmeasurelosslessJPGﬁlesizewhichreﬂects                       concentrate at the “basic level” of the semantic hierarchy
               the amount of information in an image. Our idea is that a                as illustrated by the color bars in Fig. 6. ImageNet, how-
               synset containing diverse images will result in a blurrier av-           ever, demonstrates a much more balanced distribution of
               erage image, the extreme being a gray image, whereas a                   images across the semantic hierarchy. Another critical dif-
               synset with little diversity will result in a more structured,           ference between ESP and ImageNet is sense disambigua-
               sharper average image. We therefore expect to see a smaller              tion. When human players input the word “bank”, it is un-
               JPGﬁlesizeoftheaverage image of a more diverse synset.                   clear whether it means “a river bank” or a “ﬁnancial insti-
               Fig. 5 compares the image diversity in four randomly sam-                tution”. At this large scale, disambiguation becomes a non-
               pled synsets in Caltech101 [8] 3 and the mammal subtree of               trivial task. Without it, the accuracy and usefulness of the
               ImageNet.                                                                ESP data could be affected. ImageNet, on the other hand,
                                                                                        does not have this problem by construction. See section 3.2
               2.1. ImageNet and Related Datasets                                       for more details. Lastly, most of the ESP dataset is not pub-
                                                                                        licly available. Only 60K images and their labels can be
                  Wecompare ImageNet with other datasets and summa-                     accessed [1].
               rize the differences in Table 1 4.
                                                                                        LabelMeandLotusHilldatasets LabelMe[21]andthe
               Small image datasets         A number of well labeled small              LotusHilldataset[27]provide30kand50klabeledandseg-
               datasets (Caltech101/256 [8, 12], MSRC [22], PASCAL [7]                  mented images, respectively 5. These two datasets provide
               etc.)  have served as training and evaluation benchmarks                 complementary resources for the vision community com-
               for most of today’s computer vision algorithms. As com-                  pared to ImageNet. Both only have around 200 categories,
               puter vision research advances, larger and more challenging              but the outlines and locations of objects are provided. Im-
                  3WealsocomparewithCaltech256[12]. Theresultindicatesthediver-         ageNet in its current form does not provide detailed object
               sity of ImageNet is comparable, which is reassuring since Caltech256 was outlines (see potential extensions in Sec. 5.1), but the num-
               speciﬁcally designed to be more diverse.                                 ber of categories and the number of images per category
                  4Wefocusourcomparisonsondatasetsofgenericobjects. Specialpur-
               pose datasets, such as FERET faces [19], Labeled faces in the Wild [13]     5All statistics are from [21, 27]. In addition to the 50k images, the
               and the Mammal Benchmark by Fink and Ullman [11] are not included.       Lotus Hill dataset also includes 587k video frames.


--- Page 4 ---

                                                                                                         
                                                                Lossless JPG size in byte
                                          platypus
                                               panda
                                                 okapi
                                         elephant                                         ImageNet
                                                                                          Caltech101
                                                                
                                                             900             1000             1100
                                                                                (a)                                                                                                       (b)                                                                                                                                                      (c)
                                        Figure 5: ImageNet provides diversiﬁed images. (a) Comparison of the lossless JPG ﬁle sizes of average images for four different synsets
                                        in ImageNet ( the mammal subtree ) and Caltech101. Average images are downsampled to 32×32 and sizes are measured in byte. A more
                                        diverse set of images results in a smaller lossless JPG ﬁle size. (b) Example images from ImageNet and average images for each synset
                                        indicated by (a). (c) Examples images from Caltech101 and average images. For each category shown, the average image is computed
                                        using all images from Caltech101 and an equal number of randomly sampled images from ImageNet.
                                                           0.5                                                                                                                                                                                       accuracy of image search results from the Internet is around
                                                                                                                                           10028                   Imagenet                                                                          10% [24]. ImageNet aims to eventually offer 500-1000
                                                           0.4                                                                                                                  ESP                                                                  clean images per synset. We therefore collect a large set
                                                           0.3                                                                                                  197850                                                                               of candidate images. After intra-synset duplicate removal,
                                                           0.2                                                                                                                                                                                       each synset has over 10K images on average.
                                                 percentage                                                                                                                                                                                                    Wecollect candidate images from the Internet by query-
                                                           0.1                                                                                                                                                                                       ing several image search engines.                                                                                       For each synset, the
                                                                0                                                                                                                                                                                    queries are the set of WordNet synonyms. Search engines
                                                                                     1             2              3             4              5             6              7             8              9                                           typically limit the number of images retrievable (in the or-
                                                                                                                                         depth                                                                                                       der of a few hundred to a thousand). To obtain as many im-
                                        Figure 6: Comparison of the distribution of “mammal” labels                                                                                                                                                  ages as possible, we expand the query set by appending the
                                        over tree depth levels between ImageNet and ESP game. The y-                                                                                                                                                 queries with the word from parent synsets, if the same word
                                        axis indicates the percentage of the labels of the corresponding                                                                                                                                             appears in the gloss of the target synset. For example, when
                                        dataset. ImageNet demonstrates a much more balanced distribu-                                                                                                                                                querying “whippet”, according to WordNet’s gloss a “small
                                        tion, offering substantially more labels at deeper tree depth levels.                                                                                                                                        slender dog of greyhound type developed in England”, we
                                        The actual number of images corresponding to the highest bar is                                                                                                                                              also use “whippet dog” and “whippet greyhound”.
                                        also given for each dataset.                                                                                                                                                                                           To further enlarge and diversify the candidate pool, we
                                                                                                                                                                                                                                                     translate the queries into other languages [10], including
                                        already far exceeds these two datasets. In addition, images                                                                                                                                                  Chinese, Spanish, Dutch and Italian. We obtain accurate
                                        in these two datasets are largely uploaded or provided by                                                                                                                                                    translations by WordNets in those languages [3, 2, 4, 26].
                                        users or researchers of the dataset, whereas ImageNet con-                                                                                                                                                   3.2. Cleaning Candidate Images
                                        tains images crawled from the entire Internet. The Lotus
                                        Hill dataset is only available through purchase.                                                                                                                                                                       To collect a highly accurate dataset, we rely on humans
                                                                                                                                                                                                                                                     to verify each candidate imagecollectedinthepreviousstep
                                        3. Constructing ImageNet                                                                                                                                                                                     for a given synset. This is achieved by using the service of
                                                  ImageNet is an ambitious project. Thus far, we have                                                                                                                                                Amazon Mechanical Turk (AMT), an online platform on
                                        constructed 12 subtrees containing 3.2 million images. Our                                                                                                                                                   which one can put up tasks for users to complete and to
                                        goal is to complete the construction of around 50 million                                                                                                                                                    get paid. AMT has been used for labeling vision data [23].
                                        images in the next two years. We describe here the method                                                                                                                                                    With a global user base, AMT is particularly suitable for
                                        weusetoconstruct ImageNet, shedding light on how prop-                                                                                                                                                       large scale labeling.
                                        erties of Sec. 2 can be ensured in this process.                                                                                                                                                                       In each of our labeling tasks, we present the users with
                                                                                                                                                                                                                                                     a set of candidate images and the deﬁnition of the target
                                        3.1. Collecting Candidate Images                                                                                                                                                                             synset (including a link to Wikipedia). We then ask the
                                                                                                                                                                                                                                                     users to verify whether each image contains objects of the
                                                  The ﬁrst stage of the construction of ImageNet involves                                                                                                                                            synset. We encourage users to select images regardless of
                                        collecting candidate images for each synset. The average                                                                                                                                                     occlusions, number of objects and clutter in the scene to


--- Page 5 ---

                                                               #Y    # N  Conf   Conf              4. ImageNet Applications
                                                                           Cat   BCat
                  User1        Y         Y         Y           0     1    0.07   0.23                  In this section, we show three applications of ImageNet.
                                                               1     0    0.85   0.69              Theﬁrstsetofexperimentsunderlinetheadvantagesofhav-
                  User 2       N         Y         Y           1     1    0.46   0.49              ing clean, full resolution images. The second experiment
                  User 3       N         Y         Y           2     0    0.97   0.83              exploits the tree structure of ImageNet, whereas the last ex-
                  User 4       Y         N         Y           0     2    0.02   0.12              periment outlines a possible extension and gives more in-
                  User5        Y         Y         Y           3     0    0.99   0.90              sights into the data.
                  User 6       N         N         Y           2     1    0.85   0.68
                Figure 7: Left: Is there a Burmese cat in the images? Six ran-                     4.1. Non-parametric Object Recognition
                domly sampled users have different answers. Right: The conﬁ-                           Given an image containing an unknown object, we
                dence score table for “Cat” and “Burmese cat”. More votes are                      would like to recognize its object class by querying similar
                needed to reach the same degree of conﬁdence for “Burmese cat”                     images in ImageNet. Torralba et al. [24] has demonstrated
                images.                                                                            that, given a large number of images, simple nearest neigh-
                                                                                                   bor methods can achieve reasonable performances despite a
                ensure diversity.                                                                  high level of noise. We show that with a clean set of full
                    While users are instructed to make accurate judgment,                          resolution images, object recognition can be more accurate,
                we need to set up a quality control system to ensure this                          especially by exploiting more feature level information.
                accuracy. There are two issues to consider. First, human                               Werunfourdifferent object recognition experiments. In
                users make mistakes and not all users follow the instruc-                          all experiments, we test on images from the 16 common
                tions. Second, users do not always agree with each other,                          categories 7 between Caltech256 and the mammal subtree.
                especially for more subtle or confusing synsets, typically at                      Wemeasureclassiﬁcation performance on each category in
                the deeper levels of the tree. Fig. 7(left) shows an example                       the form of an ROC curve. For each category, the negative
                of how users’ judgments differ for “Burmese cat”.                                  set consists of all images from the other 15 categories. We
                                                                                                   nowdescribe in detail our experiments and results(Fig. 8).
                    The solution to these issues is to have multiple users in-                       1. NN-voting + noisy ImageNet First we replicate one
                dependently label the same image. An image is considered                                 of the experiments described in [24], which we refer
                positive only if it gets a convincing majority of the votes.                             to as “NN-voting” hereafter. To imitate the TinyIm-
                Weobserve, however, that different categories require dif-                               age dataset (i.e. images collected from search engines
                ferent levels of consensus among users. For example, while                               without human cleaning), we use the original candi-
                ﬁve users might be necessary for obtaining a good consen-                                date images for each synset (Section 3.1) and down-
                sus on “Burmese cat” images, a much smaller number is                                    sample them to 32 × 32. Given a query image, we re-
                needed for “cat” images. We develop a simple algorithm to                                trieve 100 of the nearest neighbor images by SSD pixel
                dynamically determine the number of agreements needed                                    distance from the mammal subtree. Then we perform
                for different categories of images. For each synset, we ﬁrst                             classiﬁcation by aggregating votes (number of nearest
                randomly sample an initial subset of images. At least 10                                 neighbors) inside the tree of the target category.
                usersareaskedtovoteoneachoftheseimages. Wethenob-
                tain a conﬁdencescoretable,indicatingtheprobabilityofan                              2. NN-voting + clean ImageNet Next we run the same
                imagebeingagoodimagegiventheuservotes(Fig.7(right)                                       NN-voting experiment described above on the clean
                shows examples for “Burmese cat” and “cat”). For each of                                 ImageNet dataset. This result shows that having more
                remaining candidate images in this synset, we proceed with                               accurate data improves classiﬁcation performance.
                the AMT user labeling until a pre-determined conﬁdence
                score threshold is reached. It is worth noting that the con-                         3. NBNN          We also implement the Naive Bayesian
                ﬁdence table gives a natural measure of the “semantic difﬁ-                              Nearest Neighbor (NBNN) method proposed in [5]
                culty” of the synset. For some synsets, users fail to reach a                            to underline the usefulness of full resolution im-
                majority vote for any image, indicating that the synset can-                             ages.    NBNN employs a bag-of-features representa-
                not be easily illustrated by images 6. Fig. 4 shows that our                             tion of images.         SIFT [15] descriptors are used in
                algorithm successfully ﬁlters the candidate images, result-                              this experiment.        Given a query image Q with de-
                ing in a high percentage of clean images per synset.                                     scriptors {d },i = 1,...,M, for each object class
                                                                                                                         i
                                                                                                         C, we compute the query-class distance D                           =
                                                                                                                                                                       C
                                                                                                      7The categories are bat, bear, camel, chimp, dog, elk, giraffe, goat,
                    6An alternative explanation is that we did not obtain enough suitable          gorilla, greyhound, horse, killer-whale, porcupine, raccoon, skunk, zebra.
                candidate images. Given the extensiveness of our crawling scheme, this is          Duplicates (∼ 20 per category ) with ImageNet are removed from the test
                a rare scenario.                                                                   set.


--- Page 6 ---

                                                               1                                                                                                                     1                                                                                                
                                                                                                                                                                                                                                                    independent classi!er
                                                                                                                                                                                  0.9                                                               tree−max classi!er
                                                             0.8
                                                             0.6                                                                                                                  0.8
                                                                                                                                                                                 age AUC
                                                                                                                                                                                  0.7
                                                             0.4                                                                                                                 aver
                                                           true positive rate                                                                                                     0.6
                                                             0.2                                 NBNN
                                                                                                 NBNN−100
                                                                                                 NN−voting + clean ImageNet                                                       0.5  
                                                                                                 NN−voting + noisy ImageNet                                                                     1        2         3        4        5         6        7        8         9
                                                               0                                                                                                                                                              tree height
                                                                0            0.2           0.4          0.6           0.8            1
                                                                                     false positive rate                                                                     Figure 9: Average AUC at each tree height level. Performance
                                                                                (a) average ROC                                                                              comparison at different tree height levels between independently
                                                                                                                                                                             trained classiﬁers and tree-max classiﬁers. The tree height of a
                                                                                                       1                                                       
                                      1                                                                                                                                      node is deﬁned as the length of the longest path to its leaf nodes.
                                    0.8                                                              0.8                                                                     All leaf nodes’ height is 1.
                                    0.6                                                              0.6
                                    0.4                                                              0.4
                                                                 NBNN                                                             NBNN                                       method which we call the “tree-max classiﬁer”. Imagine
                                   true positive rate0.2         NBNN−100                           true positive rate0.2         NBNN−100
                                                                 NN−voting + clean ImageNet                                       NN−voting + clean ImageNet                 youhaveaclassiﬁer at each synset node of the tree and you
                                                                 NN−voting + noisy ImageNet                                       NN−voting + noisy ImageNet
                                      0                                                                0 
                                       0         0.2        0.4       0.6        0.8         1          0         0.2        0.4       0.6        0.8         1              want to decide whether an image contains an object of that
                                                    false positive rate                                              false positive rate
                                                        (b) elk                                                  (c) killer-whale                                            synset or not. The idea is to not only consider the classi-
                                                                                                                                                                             ﬁcation score at a node such as “dog”, but also of its child
                             Figure 8: (a) Object recognition experiment results plotted in                                                                                  synsets, such as “German shepherd”, “English terrier”, etc.
                             ROC curves. Each curve is the result of one of the four experi-                                                                                 The maximum of all the classiﬁer responses in this subtree
                             ments described in Section 4.1. It is an average of all ROC results                                                                             becomes the classiﬁcation score of the query image.
                             of 16 object categories commonly shared between Caltech256 and
                             the mammal subtree. Caltech256 images serve as testing images.                                                                                         Fig. 9 illustrates the result of our experiment on the
                             (b)(c) The ROC curve for “elk” and “killer-whale”.                                                                                              mammalsubtree. Notethat our algorithm is agnostic to any
                                                                                                                                                                             method used to learn image classiﬁers for each synset. In
                                        PM                             C 2                          C                                                                        this case, we use an AdaBoost-based classiﬁer proposed by
                                              i=1 kdi − di k , wheredi isthenearestneighborof                                                                                [6]. For each synset, we randomly sample 90% of the im-
                                        di from all the image descriptors in class C. We order                                                                               ages to form the positive training image set, leaving the rest
                                        all classes by D                           and deﬁne the classiﬁcation score
                                                                             C                                                                                               of the 10% as testing images. We form a common neg-
                                        as the minimum rank of the target class and its sub-                                                                                 ative image set by aggregating 10 images randomly sam-
                                        classes. The result shows that NBNN gives substan-                                                                                   pled from each synset. When training an image classiﬁer
                                        tially better performance, demonstrating the advantage                                                                               for a particular synset, we use the positive set from this
                                        of using a more sophisticated feature representation                                                                                 synset as well as the common negative image set excluding
                                        available through full resolution images.                                                                                            the images drawn from this synset, and its child and parent
                                 4. NBNN-100 Finally, we run the same NBNN experi-                                                                                           synsets.
                                        ment, but limit the number of images per category to                                                                                        Weevaluate the classiﬁcation results by AUC (the area
                                        100. The result conﬁrms the ﬁndings of [24]. Per-                                                                                    under ROC curve). Fig. 9 shows the results of AUC for
                                        formance can be signiﬁcantly improved by enlarging                                                                                   synsets at different levels of the hierarchy, compared with
                                        the dataset. It is worth noting that NBNN-100 out-                                                                                   anindependentclassiﬁerthatdoesnotexploitthetreestruc-
                                        performs NN-voting with access to the entire dataset,                                                                                ture of ImageNet. The plot indicates that images are easier
                                        again demonstrating the beneﬁt of having detailed fea-                                                                               to classify at the bottom of the tree (e.g. star-nosed mole,
                                        ture level information by using full resolution images.                                                                              minivan, polar bear) as opposed to the top of the tree (e.g.
                             4.2. Tree Based Image Classiﬁcation                                                                                                             vehicles, mammal, artifact, etc.). This is most likely due to
                                                                                                                                                                             stronger visual coherence near the leaf nodes of the tree.
                                   Comparedtootheravailabledatasets,ImageNetprovides                                                                                                At nearly all levels, the performance of the tree-max
                             image data in a densely populated hierarchical structure.                                                                                       classiﬁer is consistently higher than the independent clas-
                             Many possible algorithms could be applied to exploit a hi-                                                                                      siﬁer.          This result shows that a simple way of exploiting
                             erarchical data structure (e.g. [16, 17, 28, 18]).                                                                                              the ImageNet hierarchy can already provide substantial im-
                                   In this experiment, we choose to illustrate the usefulness                                                                                provement for the image classiﬁcation task without addi-
                             of the ImageNet hierarchy by a simple object classiﬁcation                                                                                      tional training or model learning.


--- Page 7 ---

                   1  Precision
                      Recall
                  0.8
                  0.6
                  0.4
                  0.2
                   0  T  h   M  t   G  L  w   h  t   j  b   p  m  g   b  t   y  t   A  p  s   c  d   s
                                i                 a  e                    u     r
                      e   u  i   g  o  y      e      t   a  a   o  r  o      a   i  r   u  t  a   o  p
                       x  m   n  e      n  o   l  n      b   c     e   v  s   c  c   m  p  e   m  b   a
                                     l         i   k            p          k     y          a
                       a      i   r  d   x  l  c          y  e      y         h          p            c
                              v             f                          i   e         a          e  b
                           a          e         o                e  h   n         c         l
                        s      a                              c                t         y   t     i   e
                            n         n         p          c     d   o  e   r      l  d      h  l   n
                         l     n                              a                    e                   s
                         o                                 a                           i     a
                                                 t             r      u                l                h
                         n             R         e                                     l      i
                          g             e         r         rr        n                o      rc         u
                                                            i          d                                 t
                          h             t                    a                                 r         t
                                        r                    g                                 a          l
                           o             i                                                                e
                           rn            ever                 e                                 ft
                  Figure 10: Precision and recall of 22 categories from different
                  levels of the hierarchy. Precision is calculated by dividing the area
                  ofcorrectlysegmentedpixelsbytheareaofdetectedpixels. Recall
                  is the fraction of relevant pixel area that is successfully detected.
                  4.3. Automatic Object Localization
                       ImageNet can be extended to provide additional infor-                                   Figure 11: Samples of detected bounding boxes around different
                  mation about each image. One such information is the spa-                                    objects.
                  tial extent of the objects in each image. Two application
                  areas come to mind. First, for training a robust object de-
                  tection algorithm one often needs localized objects in dif-
                  ferent poses and under different viewpoints. Second, hav-
                  ing localized objects in cluttered scenes enables users to use
                  ImageNetasabenchmarkdatasetforobjectlocalization al-
                  gorithms. In this section we present results of localization
                  on22categories from different depths of the WordNet hier-
                  archy. Theresultsalsothrowlightonthediversityofimages
                  in each of these categories.
                       Weusethenon-parametric graphical model described in
                  [14] to learn the visual representation of objects against a
                  global background class. In this model, every input im-
                  age is represented as a “bag of words”.                         The output is
                  a probability for each image patch to belong to the top-
                  ics z of a given category (see [14] for details).                           In or-
                          i
                  der to annotate images with a bounding box we calcu-
                  late the likelihood of each image patch given a category c:
                  p(x|c) = P p(x|z ,c)p(z |c). Finally, one bounding box
                                     i        i          i
                  is put around the region which accumulates the highest like-                                 Figure 12: Left: Average images and image samples of the de-
                  lihood.                                                                                      tected bounding boxes from the ‘tusker’ and ‘stealth aircraft’ cate-
                       We annotated 100 images in 22 different categories of                                   gories. Right: Average images and examples of three big clusters
                  the mammal and vehicle subtrees with bounding boxes                                          after k-means clustering (see Sec. 4.3 for detail). Different view-
                  around the objects of that category. Fig. 10 shows precision                                 points and poses emerge in the “tusker” category. The ﬁrst row
                  and recall values. Note that precision is low due to extreme                                 shows tuskers in side view, front view and in proﬁle. One cluster
                  variability of the objects and because of small objects which                                of aircraft images displays mostly planes on the ground.
                  have hardly any salient regions.                                                             5. Discussion and Future Work
                       Fig. 11 shows sampled bounding boxes on different
                  classes. The colored region is the detected bounding box,                                        Ourfuture work has two goals:
                  while the original image is in light gray.                                                   5.1. Completing ImageNet
                       In order to illustrate the diversity of ImageNet inside
                  each category, Fig. 12 shows results on running k-means                                          The current ImageNet constitutes ∼ 10% of the Word-
                  clustering on the detected bounding boxes after converting                                   Net synsets. To further speed up the construction process,
                  themtograyscaleandrescalingthemto32×32. Allaverage                                           wewillcontinue to explore more effective methods to eval-
                  images, including those for the entire cluster, are created                                  uate the AMT user labels and optimize the number of repe-
                  with approximately 40 images. While it is hard to iden-                                      titions needed to accurately verify each image. At the com-
                  tify the object in the average image of all bounding boxes                                   pletion of ImageNet, we aim to (i) have roughly 50 million
                  (shown in the center) due to the diversity of ImageNet, the                                  clean, diverse and full resolution images spread over ap-
                  average images of the single clusters consistently discover                                  proximately 50K synsets; (ii) deliver ImageNet to research
                  viewpoints or common poses.                                                                  communitiesbymakingitpubliclyavailableandreadilyac-


--- Page 8 ---

               cessible online. We plan to use cloud storage to enable efﬁ-                 References
               cient distribution of ImageNetdata; (iii) extend ImageNetto                   [1] http://www.hunch.net/ jl/.
               include more information such as localization as described                                                      ˜
                                                                                             [2] The Chinese WordNet. http://bow.sinica.edu.tw.
               in Sec. 4.3, segmentation, cross-synset referencing of im-                    [3] The Spanish WordNet. http://www.lsi.upc.edu/ nlp.
                                                                                                                                                        ˜
               ages, as well as expert annotation for difﬁcult synsets and                   [4] A. Artale, B. Magnini, and S. C. Wordnet for italian and its use for
               (iv) foster an ImageNet community and develop an online                           lexical discrimination. In AI*IA97, pages 16–19, 1997.
               platform where everyone can contribute to and beneﬁt from                     [5] O. Boiman, E. Shechtman, and M. Irani.     In defense of nearest-
                                                                                                 neighbor based image classiﬁcation. In CVPR08, pages 1–8, 2008.
               ImageNet resources.                                                           [6] B. Collins, J. Deng, K. Li, and L. Fei-Fei. Towards scalable dataset
                                                                                                 construction: An active learning approach. In ECCV08, pages I: 86–
               5.2. Exploiting ImageNet                                                          98, 2008.
                                                                                             [7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and
                   WehopeImageNetwill become a central resource for a                            A. Zisserman. The PASCAL Visual Object Classes Challenge 2008
               broad of range of vision related research. For the computer                       (VOC2008) Results. http://www.pascal-network.org/
               vision community in particular, we envision the following                         challenges/VOC/voc2008/workshop/.
               possible applications.                                                        [8] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object
                                                                                                 categories. PAMI, 28(4):594–611, April 2006.
                   Atraining resource. Most of today’s object recognition                    [9] C. Fellbaum. WordNet: An Electronic Lexical Database. Bradford
               algorithms have focused on a small number of common ob-                           Books, 1998.
               jects, such as pedestrians, cars and faces. This is mainly due               [10] R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. Learning object
                                                                                                 categories from google’s image search. In ICCV05, pages II: 1816–
               to the high availability of images for these categories. Fig. 6                   1823, 2005.
               has shown that even the largest datasets today have a strong                 [11] M. Fink and S. Ullman. From aardvark to zorro: A benchmark for
               bias in their coverage of different types of objects. Ima-                        mammalimageclassiﬁcation. IJCV, 77(1-3):143–156, May 2008.
               geNet,ontheotherhand,containsalargenumberofimages                            [12] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category
                                                                                                 dataset. Technical Report 7694, Caltech, 2007.
               for nearly all object classes including rare ones. One inter-                [13] G. Huang, M. Ramesh, T. Berg, and E. Learned Miller. Labeled
               esting research direction could be to transfer knowledge of                       faces in the wild: A database for studying face recognition in uncon-
               commonobjectstolearn rare object models.                                          strained environments. Technical Report 07-49, UMass, 2007.
                                                                                            [14] L.-J. Li, G. Wang, and L. Fei-Fei. OPTIMOL: automatic Online
                   A benchmark dataset. The current benchmark datasets                           Picture collecTion via Incremental MOdel Learning. In CVPR07,
               in computer vision such as Caltech101/256 and PASCAL                              pages 1–8, 2007.
               have played a critical role in advancing object recognition                  [15] D. Lowe. Distinctive image features from scale-invariant keypoints.
               and scene classiﬁcation research. We believe that the high                        IJCV, 60(2):91–110, November 2004.
                                                                                            [16] M.MarszalekandC.Schmid. Semantichierarchies for visual object
               quality, diversity and large scale of ImageNet will enable                        recognition. In CVPR07, pages 1–7, 2007.
               it to become a new and challenging benchmark dataset for                     [17] M. Marszalek and C. Schmid. Constructing category hierarchies for
               future research.                                                                  visual recognition. In ECCV08, pages IV: 479–491, 2008.
                                                                                            [18] D. Nister and H. Stewenius. Scalable recognition with a vocabulary
                   Introducing new semantic relations for visual modeling.                       tree. In CVPR06, pages II: 2161–2168, 2006.
               Because ImageNet is uniquely linked to all concrete nouns                    [19] P. Phillips, H. Wechsler, J. Huang, and P. Rauss. The feret database
               of WordNet whose synsets are richly interconnected, one                           and evaluation procedure for face-recognition algorithms.   IVC,
               could also exploit different semantic relations for instance                      16(5):295–306, April 1998.
                                                                                            [20] E. Rosch and B. Lloyd. Principles of categorization. In Cognition
               to learn part models. To move towards total scene under-                          and categorization, pages 27–48, 1978.
               standing, it is also helpful to consider different depths of                 [21] B. Russell, A. Torralba, K. Murphy, and W. Freeman. Labelme:
               the semantic hierarchy.                                                           Adatabase and web-based tool for image annotation. IJCV, 77(1-
                                                                                                 3):157–173, May 2008.
                   Human vision research. ImageNet’s rich structure and                     [22] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost: Joint
               dense coverage of the image world may help advance the                            appearance,shapeandcontextmodelingformulti-classobjectrecog-
               understanding of the human visual system. For example,                            nition and segmentation. In ECCV06, pages I: 1–15, 2006.
               the question of whether a concept can be illustrated by im-                  [23] A. Sorokin and D. Forsyth. Utility data annotation with amazon me-
                                                                                                 chanical turk. In InterNet08, pages 1–8, 2008.
               ages is much more complex than one would expect at ﬁrst.                     [24] A. Torralba, R. Fergus, and W. Freeman. 80 million tiny images: A
               Aligningthecognitivehierarchywiththe“visual”hierarchy                             large data set for nonparametric object and scene recognition. PAMI,
               also remains an unexplored area.                                                  30(11):1958–1970, November 2008.
                                                                                            [25] L. von Ahn and L. Dabbish. Labeling images with a computer game.
                                                                                                 In CHI04, pages 319–326, 2004.
               Acknowledgment                                                               [26] P. Vossen, K. Hofmann, M. de Rijke, E. Tjong Kim Sang, and K. De-
                   Theauthors would like to thank Bangpeng Yao, Hao Su, Barry                    schacht. The Cornetto database: Architecture and user-scenarios. In
                                                                                                 Proceedings DIR 2007, pages 89–96, 2007.
               Chaiandanonymousreviewersfortheirhelpfulcomments. WDis                       [27] B. Yao, X. Yang, and S. Zhu. Introduction to a large-scale general
               supported by Gordon Wu fellowship. RS is supported by the ERP                     purpose ground truth database: Methodology, annotation tool and
               andUptonfellowships. KLisfundedbyNSFgrantCNS-0509447                              benchmarks. In EMMCVPR07,pages169–183,2007.
               and by research grants from Google, Intel, Microsoft and Yahoo!.             [28] A.ZweigandD.Weinshall. Exploitingobjecthierarchy: Combining
                                                                                                 models from different category levels. In ICCV07, pages 1–8, 2007.
               LFFisfundedbyresearch grants from Microsoft and Google.
