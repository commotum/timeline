                ·0550·                                                                                                                                             Optoelectron. Lett. Vol.21 No.9 
                encodes position information among point-wise features               Due to the linearized operation of cosh-attention, the 
                and decodes the all extracted features into a point-wise          constructed  point-wise  transformer  not  only  captures 
                global proposal representation. The other branch is the           spatial contextual information but also achieves satisfac-
                channel-wise multi-head transformer, which aggregates             tory inference speed. According to the left part of Fig.2, 
                local  and  detailed  channel-wise  contextual  correlations      the point-wise multi-head cosh-attention encoding mod-
                for generating a channel-wise proposal.                           ule  also  includes  layer  normalization  operation  and  a 
                                                                                  feedforward network (FFN) with two linear layers and 
                  In  order  to  capture  spatial  context-dependencies 
                                                                                  one  ReLU  activation  layer.  A  stack  of  3  identical 
                among encoded keypoints for a point-wise proposal rep-
                                                                                  multi-head cosh-self-attention encoding modules is used 
                resentation,  a  point-wise  transformer  is  constructed  as 
                                                                                  in the point-wise transformer. 
                illustrated  in  the  upper-right  part  of  Fig.2.  The 
                               [10]
                                                                                     In  the  point-wise  decoding  module,  cosh-cross- 
                cosh-attention     is  used to replace vanilla attention for 
                                                                                  attention  is  used  to  decode  all  the  point-wise  features. 
                lower  spatial  and  temporal  complexity.  Generally,  the 
                                                                                  Specifically, only one zero-initialized queryembedding z 
                point-wise  transformer  includes  a  cosh-self-attention 
                                                                                  is   used  to  calculate  with  key-value  embeddings 
                encoding  module  and  a  cosh-cross-attention  decoding 
                                                                                     '   '
                                                                                   K ,V
                                                                                            from encoding module for obtaining point-wise 
                                                                                     h   h
                module. 
                                                                                  global  proposal  representation.  Different  from  three 
                  The         formulated         point-wise        encoding 
                                                                                  identical  multi-head  cosh-self-attentions  stacked  in  en-
                cosh-self-attention for processing the input features F is 
                                                                                  coding module, just one multi-head cosh-cross-attention 
                     P
                   F  F F
                                
                           pe
                                                                                  is used in decoding module. As shown in the upper-right 
                                                                              (6) 
                                                                                                                  pd
                    Concat PEA ,PEA ,,PEA            F,
                                                                                  part  of  Fig.2,  the  output  z   of  point-wise  decoding 
                                                        
                                      1      2         H
                          P
                                                                                  cross-cosh-attention can be calculated as 
                where  F   denotes  the  output  feature  map  of  the 
                                                                                       pd             pe
                                                                                                                
                                                                                      z    z,F         z
                                                                                                     
                point-wise  encoding  cosh-self-attention,             is  the 
                                                                                              pd
                                                                   
                                                               pe
                                                                                                Concat PDA ,PDA ,,PDA         z,          (10) 
                                                                                                                             
                multi-head    cosh-self-attention    function,    Concat   
                                                                           
                                                                                                          1      2          H
                                                                                  where                   represents      the      point-wise 
                stands for a concatenation operation, and H is the num-
                                                                                                 pd
                ber of attention heads. Denote h as the index of the atten-
                                                                                  cosh-cross-attention  operation  in  decoding  module,  z 
                tion head, and the PEA  is defined as 
                                                                                                                                 pe
                                        h
                                                                                  denotes  the  zero-initialized  vector,  and     denotes  the 
                                                                                                                              F
                                    
                   PEA F s V 
                                         
                           
                                                                                  output  of  the  point-wise  encoding  module.  The  PDA  
                       h         h  h
                                                                                                                                               h
                                                                                  stands  for  the  h-head  point-wise  cosh-cross-attention, 
                                             
                                                                              (7) 
                                s Q ,K      V ,h1,2,,H,
                                           
                                    h    h    h
                                 
                                                                                  which is calculated by the similar formulation as Eq.(9). 
                                                                                          pd
                                                                                  Next, z  proceeds through FFN and layer normalizations 
                                              
                                          Nd
                                   
                where  Q ,K ,V                  are  obtained  by  using 
                           h   h    h
                                                                                  to obtain the final point-wise global proposal representa-
                learnable matrices and rectified linear units (ReLU) ac-
                                                                                  tion. 
                                                                  d
                                                                                     Besides, in order to investigate the contextual correla-
                                                               
                tivation  to  process  input  features.  And         ,  s  de-
                                                             d 
                                                                                  tions  among  feature  channels  to  obtain  a  high-quality 
                                                                  H
                                                                                  channel-wise  global  proposal  representation,  we  intro-
                notes the similarity between Q and K of the vanilla atten-
                                                                                                                        [11]
                                                                                  duce the channel-wise architecture       ,  which achieves a 
                tion. In the cosh-attention operation, the similarity func-
                                                                                  superior detection accuracy on the widely used KITTI 
                tion s(Q, K) is decomposable with re-weighting mecha-
                nism  replacing  the  traditional  softmax  to  achieve  lin-
                                                                                  dataset. Specifically, as shown in the bottom-right part of 
                earized complexity, as shown in   
                                                                                  Fig.2, the channel-wise transformer adopts self-attention 
                                                                                  encoding scheme, which shares almost the same archi-
                                                                
                                                          i  j
                                                              
                                         T
                                      
                                                                           (8) 
                    Q ,K       Q K         2cosh a             ,
                                                                                  tecture  as  the  original  softmax-based  transformer  en-
                             
                     hi    hj      hi  hj
                                                                
                   
                                                              
                                                            B
                                                              
                                                                
                                                                                  coder. 
                                                                                     In  the  decoding  module,  to  emphasize  the  chan-
                where a is a hyper-parameter, i, j=1,…,N denote the row 
                              
                                                                                  nel-wise local information aggregation, the channel-wise 
                       Q ,K
                of the          , respectively, and                    Thus, 
                                                      Bmax i, j .
                                                                  
                         h    h
                                                                                                
                                                                                  transformer uses  Eq.(11)  to  calculate  the  new  chan-
                PEA  can be shown as   
                    h
                                                                                  nel-wise re-weighting for decoding weight vector based 
                                           T
                                            
                                                    
                   PEA F {2Q K V 
                                                                                                                            
                                               
                       h            h    h   h
                                      
                                                                                  on all the channels of key embedding  K . 
                                                                                                                              h
                                                                                                     
                                                                                                       
                                                                                                      
                                                      T
                                                            
                                                                                                   K
                                                                                       c
                                                          
                                                                                                     h
                                                                 
                                cosh Q      cosh K       V 
                                                       
                                        h            h     h
                                                 
                                                                                      w                , h 1,,H,                            (11) 
                                                                                                       
                                                                                                      
                                                                                       h
                                                            
                                                            
                                                                                                      
                                                                                                      
                                                                                                    d
                                                                                                       
                                                                                                      
                                                                                                       
                                                     T
                                                           
                                                         
                                                                                                               
                                                                                             ce     cd     Nd
                                                                 
                                sinh Q      sinh K      V }/
                                        h          h    h
                                                
                                                                                      K F W                 ,                                       (12) 
                                                           
                                                                                        h           K
                                                           
                                                                                                      h
                                                                                           c
                                         T                        T
                                                                                         w
                                                                                  where       is  the  proposed channel-wise re-weighting for 
                                                               
                                                                         
                                {2Q K      cosh Q       cosh K                           h
                                                                   
                                    h   h            h           h
                                                             
                                                                                  decoding weight vector in Ref.[11], ρ(∙) refers to a linear 
                                                    T
                                                   
                                                                                  projection, which calculates d' number of decoding values
                                                                                 (9) 
                                sinh Q     sinh K      }.
                                                     
                                        h          h
                                               
