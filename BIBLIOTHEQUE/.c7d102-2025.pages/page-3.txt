               TONG et al.                                                                                                                                Optoelectron. Lett. Vol.21 No.9·0549· 
               point features for making up the quantization loss of               voxelization and having larger receptive fields. 
                                                                                 
                                                                                 
                                                                                    
                                                                                                                                     
                                                            Fig.1 The overview of PV-DT3D 
                                                                               
               3.2.2 Proposal-aware VSA module                                  points. Thus, the point-wise transformer highlights spa-
               To better exploit the information of high-quality propos-        tial  relationships.  While  the  channel-wise  transformer 
               als for refinement and stable training process, inspired by      investigates interactions among different channels due to 
                                                              [11]
               proposal-to-point  (P.T.P.)  strategy  in  CT3D   ,  we  pre-    the attention weights distributed along channels. Gener-
               sent an improved proposal-aware VSA module. For sam-             ally,  the  two  kinds  of  attention  operations  can  be  ex-
               pled keypoints in the subsequent processing, we calcu-           pressed as 
               late  the  3D  relative  spatial  coordinates  between  key-
                                                                                                                          T
                                                                                   
                                                                                                                           
                                                                                                                    QK
                                                                                      Point-wise Attn  softmax              V
                                                                                   
               points and corresponding proposal points as shown in                                                        
                                                                                                                           
                                                                                                                        d
                                                                                   
                      j         j
                                                                                                                           
                  p  p  p , j 1,2,,9,
                                                                  (2) 
                                                                                                                                          (5)   
                                                                                                                                   .
                     i     i
                                                                                   
                                                                                                                       T
                                                                                                                             
                                                                                                                     Q K
                                                                                   
               where p denotes the spatial coordinates of a keypoint in 
                        i
                                                                                     Channel-wise Attn softmax                V
                                                                                                                             
                                                                                   
                                                 j
                                                                                                                             
               corresponding proposals, and p denotes the 3D coordi-
                                                                                                                         d
                                                                                                                             
                                                                                   
               nate  of  the  corresponding proposal center (or a corner 
                                                                                   From Eq.(5), one finds an interesting fact on the dif-
               point).  By  this  strategy,  the  information  of  proposal 
                                                                                ference of computation loads of these two transformers. 
                  prop  prop  prop  prop  prop  prop  prop
                 x    , y   , z   ,l   , w    , h   ,      will  be  en-
                                                         
                                                                                The size of Point-wise Attn is N×N, while the size of 
                                                                                Channel-wise  Attn  is  d×d.  Generally,  for  point  cloud 
               coded into keypoint features as shown in   
                                                                                scene,  N»d. Thus, the point-wise transformers are suf-
                    prop       1    2       9   r      128
                                                
                                                                     (3) 
                   f      p ,p ,,p ,p              ,
                    i          i   i        i   i
                                                
                                                                                fering from the computation complexity growing quad-
                        r
                                                                                ratically with respect to the size of input point clouds. 
               where p   denotes  the  reflectance  of  the  i-th  sampled 
                        i
                                                                                3.3.2 Dual transformer for proposal refinement 
               keypoint for KITTI dataset. Then, the output of the pro-
                                                                                Inspired by Refs.[11] and [24], the dual transformer en-
               posal-aware VSA module is 
                    pvsa      prop   voxel  raw   BEV      1D
                                                     
                                                                                coder-decoder architecture is proposed for bounding-box 
                                                                            (4) 
                   f      f     , f     , f   , f      .
                    i        i      i      i     i
                                                     
                                                                                refinement,  taking  advantages  of  both  point-wise  and 
                  The  above  scheme  enhances  the  local  correlations 
                                                                                channel-wise  transformers.  Thus,  the  dual  transformer 
               among input points within the same proposal, thus stabi-
                                                                                has the ability to capture the long-range spatial informa-
               lizes the training towards a higher detection accuracy. 
                                                                                tion  and  channel  contextual  dependencies  among  en-
               3.3 Dual transformer for proposal refinement 
                                                                                coded  proposal-aware  keypoint  features  for  higher 
               3.3.1 Transformers for point cloud processing 
                                                                                bounding-box refinement.     
               Due to the inherent permutation  invariant,  transformer 
                                                                                   Specifically,  N  keypoints  within  proposals  are  ran-
               has  been  an  ideal  model  to  process  point  clouds. 
                                                                                domly  sampled  for  refinement. The  keypoint  features 
               Self-attention, as the core component in transformer en-
                                                                                                                                         [10]
                                                                                       Nd
                                                                                 F        are formed by aggregating 64-dimensional         
               coders, has the ability of capturing long-range interac-
                                                                                                                               pvsa
                                                          Nd
                                                                                embeddings  obtained  by  mapping  each  f         with  an 
               tions.  Given  a  point  cloud  scene          of  N  points 
                                                    X
                                                                                                                               i
                                                                                MLP. Then, the encoded keypoints features are fed into 
               and d dimensional features. According to the operating 
                                                                                two parallel branches of dual transformer. As shown in 
               space in point cloud tasks, transformers can be divided 
                                                                                Fig.2,  one  branch  is  the  point-wise  multi-head 
               into  point-wise  transformers  and  channel-wise  trans-
                                                                                               [10]
                                                                                cosh-attention      encoder-decoder  architecture,  which
               formers. The former measures the similarity among input 
