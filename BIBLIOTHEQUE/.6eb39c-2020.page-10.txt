                                                                   Agent57: Outperforming the Atari Human Benchmark
                    Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki,                                 Pohlen, T., Piot, B., Hester, T., Azar, M. G., Horgan, D.,
                       W. M., Donahue, J., Razavi, A., Vinyals, O., Green,                                    Budden, D., Barth-Maron, G., Van Hasselt, H., Quan, J.,
                                                                                                                  ˇ
                       T., Dunning, I., Simonyan, K., et al.                         Population               Vecerík, M., et al. Observe and look further: Achiev-
                       based training of neural networks.                      arXiv preprint                 ing consistent performance on atari.                     arXiv preprint
                       arXiv:1711.09846, 2017.                                                                arXiv:1805.11593, 2018.
                    Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and                              Pritzel, A., Uria, B., Srinivasan, S., Puigdomènech, A.,
                       Dabney, W. Recurrent experience replay in distributed                                  Vinyals, O., Hassabis, D., Wierstra, D., and Blundell,
                       reinforcement learning. In International Conference on                                 C. Neural episodic control. ICML, 2017.
                       Learning Representations, 2018.                                                    Puigdomènech Badia, A., Sprechmann, P., Vitvitskyi, A.,
                    Ke, N. R., Goyal, A., Bilaniuk, O., Binas, J., Charlin,                                   Guo, D., Piot, B., Kapturowski, S., Tieleman, O., Ar-
                       L., Pal, C., and Bengio, Y.                  Sparse attentive back-                    jovsky, M., Pritzel, A., Bolt, A., and Blundell, C. Never
                       tracking: Long-range credit assignment in recurrent net-                               give up: Learning directed exploration strategies.                        In
                       works. arXiv preprint arXiv:1711.02326, 2017.                                          International Conference on Learning Representations,
                                                                                                              2020.
                    Liu, Y., Luo, Y., Zhong, Y., Chen, X., Liu, Q., and                                   Puterman, M. L. Markov decision processes. Handbooks
                       Peng, J. Sequence modeling of temporal credit assign-                                  in operations research and management science, 2:331–
                       mentforepisodicreinforcementlearning. arXivpreprint                                    434, 1990.
                       arXiv:1905.13420, 2019.
                    Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve-                               Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever,
                       ness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,                                I.  Evolution strategies as a scalable alternative to rein-
                       Fidjeland, A. K., Ostrovski, G., et al. Human-level con-                               forcement learning. arXiv preprint arXiv:1703.03864,
                       trol through deep reinforcement learning. Nature, 518                                  2017.
                       (7540):529, 2015.                                                                  Savinov, N., Raichuk, A., Marinier, R., Vincent, D., Polle-
                    Munos,R., Stepleton, T., Harutyunyan, A., and Bellemare,                                  feys, M., Lillicrap, T., and Gelly, S. Episodic curiosity
                       M. Safe and efﬁcient off-policy reinforcement learning.                                through reachability. arXiv preprint arXiv:1810.02274,
                       In Advances in Neural Information Processing Systems,                                  2018.
                       pp. 1046–1054, 2016.                                                               Schaul, T., Borsa, D., Ding, D., Szepesvari, D., Ostrovski,
                    Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep                               G., Dabney, W., and Osindero, S. Adapting behaviour
                       exploration via bootstrapped dqn. In Advances In Neural                                for learning progress, 2019.
                       Information Processing Systems, pp. 4026–4034, 2016.                               Schmidhuber, J. A possibility for implementing curiosity
                    Osband, I., Aslanides, J., and Cassirer, A. Randomized                                    and boredom in model-building neural controllers. In
                       prior functions for deep reinforcement learning. In Ad-                                Proc. of the international conference on simulation of
                       vances in Neural Information Processing Systems, pp.                                   adaptive behavior: From animals to animats, pp. 222–
                       8617–8629, 2018.                                                                       227, 1991.
                                                                                                          Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,
                    Ostrovski, G., Bellemare, M. G., van den Oord, A., and                                    Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis,
                       Munos, R. Count-based exploration with neural density                                  D., Graepel, T., et al.           Mastering atari, go, chess and
                       models. InProceedingsofthe34thInternationalConfer-                                     shogi by planning with a learned model. arXiv preprint
                       ence on Machine Learning-Volume 70, pp. 2721–2730.                                     arXiv:1911.08265, 2019.
                       JMLR.org,2017.
                                                                                                          Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
                    Oudeyer, P.-Y., Kaplan, F., and Hafner, V. V. Intrinsic mo-                               VanDenDriessche,G.,Schrittwieser, J., Antonoglou, I.,
                       tivation systems for autonomous mental development.                                    Panneershelvam, V., Lanctot, M., et al. Mastering the
                       IEEE transactions on evolutionary computation, 11(2):                                  game of go with deep neural networks and tree search.
                       265–286, 2007.                                                                         nature, 529(7587):484–489, 2016.
                    Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen,                           Strehl, A. L. and Littman, M. L. An analysis of model-
                       R. Y., Chen, X., Asfour, T., Abbeel, P., and Andrychow-                                based interval estimation for markov decision processes.
                       icz, M. Parameter space noise for exploration. arXiv                                   Journal of Computer and System Sciences, 74(8):1309–
                       preprint arXiv:1706.01905, 2017.                                                       1331, 2008.
