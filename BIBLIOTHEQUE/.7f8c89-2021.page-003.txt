               PCT: Point cloud transformer                                                                                         189
               3.  Extensive experiments demonstrate that the                   Inspired by the local patch structures used in ViT
                   PCT with explicit local context enhancement               and basic semantic information in language word, we
                   achieves state-of-the-art performance on shape            present a neighbor embedding module that aggregates
                   classiﬁcation, part segmentation, and normal              features from a point’s local neighborhood, which can
                   estimation tasks.                                         capture the local information and obtain semantic
                                                                             information.
               2    Related work                                             2.3    Point-based deep learning
               2.1    Transformer in NLP                                     PointNet [1] pioneered point cloud learning. Sub-
               Bahdanau et al. [11] proposed a neural machine                sequently, Qi et al. [21] proposed PointNet++, which
               translation method with an attention mechanism,               uses query ball grouping and hierarchical PointNet
               in which attention weight is computed through the             to capture local structures. Several subsequent works
               hidden state of an RNN. Self-attention was proposed           considered how to deﬁne convolution operations on
               by Lin et al. [12] to visualize and interpret sentence        point clouds. One main approach is to convert a point
               embeddings. Building on these, Vaswani et al. [6]             cloud into a regular voxel array to allow convolution
               proposed Transformer for machine translation; it is           operations. Tchapmi et al. [2] proposed SEGCloud
               based solely on self-attention, without any recurrence        for pointwise segmentation.        It maps convolution
               or convolution operators. Devlin et al. [13] proposed         features of 3D voxels to point clouds using trilinear
               bidirectional transformers (BERT) approach, which             interpolation and keeps global consistency through
               is one of the most powerful models in the NLP                 fully connected conditional random ﬁelds. Atzmon et
               ﬁeld. More lately, language learning networks such as         al. [4] presented the PCNN framework with extension
               XLNet [14], Transformer-XL [15], and BioBERT [16]             and restriction operators to map between point-
               have further extended the Transformer framework.              based representation and voxel-based representation.
                  However, in natural language processing, the input         Volumetric convolution is performed on voxels for
               is in order, and word has basic semantic, whereas             point feature extraction. MCCNN by Hermosilla et
               point clouds are unordered, and individual points             al. [22] allows non-uniformly sampled point clouds;
               have no semantic meaning in general.                          convolution is treated as a Monte Carlo integration
               2.2    Transformer for vision                                 problem. Similarly, in PointConv proposed by Wu et
                                                                             al. [5], 3D convolution is performed through Monte
               Many frameworks have introduced attention into                Carlo estimation and importance sampling.
               vision tasks. Wang et al. [17] proposed a residual               A diﬀerent approach redeﬁnes convolution to
               attention approach with stacked attention modules             operation on irregular point cloud data. Li et al. [3]
               for image classiﬁcation. Hu et al. [18] presented a           introduced a point cloud convolution network,
               novel spatial encoding unit, the SE block, whose              PointCNN, in which a χ-transformation is trained
               idea was derived from the attention mechanism.                to determine a 1D point order for convolution.
               Zhang et al. [19] designed SAGAN, which uses self-            Tatarchenko et al. [23] proposed tangent convolution,
               attention for image generation. There has also been           which can learn surface geometric features from
               an increasing trend to employ Transformer as a                projected virtual tangent images.         SPG proposed
               module to optimize neural networks. Wu et al. [8]             by Landrieu and Simonovsky [24] divides the
               proposed visual transformers that apply Transformer           scanned scene into similar elements, and establishes
               to token-based images from feature maps for vision            a superpoint graph structure to learn contextual
               tasks. Recently, Dosovitskiy [7], proposed an image           relationships between object parts. Yang et al. [25]
               recognition network, ViT, based on patch encoding             used a parallel framework to extend CNN from the
               andTransformer, showingthatwithsuﬃcienttraining               conventional domain to a curved two-dimensional
               data, Transformer provides better performance than            manifold. However, it requires dense 3D gridded data
               a traditional convolutional neural network. Carion et         as input so is unsuitable for 3D point clouds. Wang et
               al. [20] presented an end-to-end detection transformer        al. [26] designed an EdgeConv operator for dynamic
               that takes CNN features as input and generates                graphs, allowing point cloud learning by recovering
               bounding boxes with a Transformer encoder–decoder.            local topology.
