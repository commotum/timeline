                                   Acknowledgements                                       Finn, C.; Abbeel, P.; and Levine, S. 2017. Model-agnostic meta-
                HH, DM, SO, SWF and QL were supported by AFOSR                            learningforfastadaptationofdeepnetworks. InInternationalCon-
                MURI FA9550-18-1-0502 and ONR grants: N00014-18-                          ference on Machine Learning, 1126–1135. PMLR.
                1-2527, N00014-20-1-2093, and N00014-20-1-2787. HH’s                      Fung, S. W.; and Ruthotto, L. 2019a. A multiscale method for
                work was also supported by the National Science Founda-                   model order reduction in PDE parameter estimation. Journal of
                tion (NSF) Graduate Research Fellowship under Grant No.                   Computational and Applied Mathematics, 350: 19–34.
                DGE-1650604. Any opinion, ﬁndings, and conclusions or                     Fung, S. W.; and Ruthotto, L. 2019b. An uncertainty-weighted
                recommendations expressed in this material are those of the               asynchronous ADMM method for parallel PDE parameter estima-
                authors and do not necessarily reﬂect the views of the NSF.               tion. SIAM Journal on Scientiﬁc Computing, 41(5): S129–S148.
                We thank Zaccharie Ramzi for the fruitful discussions and                 Fung,S.W.;andWendy,Z.2020. Multigridoptimizationforlarge-
                the anonymous referees for helping us improve the quality                 scale ptychographic phase retrieval. SIAM Journal on Imaging Sci-
                of our paper.                                                             ences, 13(1): 214–233.
                                                                                          Geng,Z.;Guo,M.-H.;Chen,H.;Li,X.;Wei,K.;andLin,Z.2021.
                                         References                                       Is Attention Better Than Matrix Decomposition? In International
                                                                                          Conference on Learning Representations.
                Aicher, C.; Foti, N. J.; and Fox, E. B. 2020. Adaptively truncating       Ghaoui, L. E.; Gu, F.; Travacca, B.; Askari, A.; and Tsai, A. Y.
                backpropagation through time to control gradient bias. In Uncer-          2019. Implicit Deep Learning. arXiv preprint arXiv:1908.06315.
                tainty in Artiﬁcial Intelligence, 799–808. PMLR.                          Gholami, A.; Keutzer, K.; and Biros, G. 2019. ANODE: Uncon-
                Anil, C.; Lucas, J.; and Grosse, R. 2019. Sorting out Lipschitz           ditionally accurate memory-efﬁcient gradients for neural ODEs.
                function approximation. In International Conference on Machine            arXiv preprint arXiv:1902.10298.
                Learning, 291–301. PMLR.                                                  Gilton, D.; Ongie, G.; and Willett, R. 2021.     Deep Equilibrium
                Bai, S.; Kolter, J. Z.; and Koltun, V. 2019. Deep equilibrium mod-        Architectures for Inverse Problems in Imaging.      arXiv preprint
                els. In Advances in Neural Information Processing Systems, 690–           arXiv:2102.07944.
                701.                                                                      Gouk, H.; Frank, E.; Pfahringer, B.; and Cree, M. J. 2021. Reg-
                Bai,S.;Koltun,V.;andKolter,J.Z.2020. MultiscaleDeepEquilib-               ularisation of neural networks by enforcing Lipschitz continuity.
                riumModels. AdvancesinNeuralInformationProcessingSystems,                 Machine Learning, 110(2): 393–416.
                33.                                                                       Gould, S.; Hartley, R.; and Campbell, D. 2019. Deep declarative
                                            ´                                             networks: A new hope. arXiv preprint arXiv:1909.04866.
                Banach, S. 1922. Sur les operations dans les ensembles abstraits et       Haber, E. 2014. Computational methods in geophysical electro-
                                     ´            ´
                leur application aux equations integrales. Fund. math, 3(1): 133–         magnetics. SIAM.
                181.
                Chang, B.; Meng, L.; Haber, E.; Ruthotto, L.; Begert, D.; and             Haber, E.; and Ruthotto, L. 2017. Stable architectures for deep
                Holtham, E. 2018. Reversible architectures for arbitrarily deep           neural networks. Inverse Problems, 34(1): 014004.
                residual neural networks. In Proceedings of the AAAI Conference           He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-
                onArtiﬁcial Intelligence, volume 32.                                      ing for image recognition. In Proceedings of the IEEE conference
                Chen, R. T.; Rubanova, Y.; Bettencourt, J.; and Duvenaud, D. K.           oncomputervision and pattern recognition, 770–778.
                2018. Neuralordinarydifferentialequations. InAdvancesinneural             Heaton,H.;Fung,S.W.;Gibali,A.;andYin,W.2021a. Feasibility-
                information processing systems, 6571–6583.                                based Fixed Point Networks. arXiv preprint arXiv:2104.14090.
                Cisse, M.; Bojanowski, P.; Grave, E.; Dauphin, Y.; and Usunier,           Heaton, H.; Fung, S. W.; Lin, A. T.; Osher, S.; and Yin, W. 2020.
                N. 2017. Parseval networks: Improving robustness to adversarial           Projecting to Manifolds via Unsupervised Learning. arXiv preprint
                examples. InInternationalConferenceonMachineLearning,854–                 arXiv:2008.02200.
                863. PMLR.                                                                Heaton, H.; McKenzie, D.; Li, Q.; Fung, S. W.; Osher, S.; and Yin,
                  ´                                                                       W. 2021b. Learn to Predict Equilibria via Fixed Point Networks.
                Csaji, B. C.; et al. 2001. Approximation with artiﬁcial neural net-       arXiv preprint arXiv:2106.00906.
                works. Faculty of Sciences, Eotvos Lorand University, Hungary,
                                               ¨  ¨      `                                Jameson,A.1988. Aerodynamicdesignviacontroltheory. Journal
                24(48): 7.                                                                of scientiﬁc computing, 3(3): 233–260.
                Dupont, E.; Doucet, A.; and Teh, Y. W. 2019. Augmented Neural             Jeon, Y.; Lee, M.; and Choi, J. Y. 2021. Differentiable Forward and
                                                                                 ´
                ODEs. In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d'Alche-           BackwardFixed-Point Iteration Layers. IEEE Access.
                Buc, F.; Fox, E.; and Garnett, R., eds., Advances in Neural Infor-        Kan, K.; Fung, S. W.; and Ruthotto, L. 2020. PNKH-B: A pro-
                mation Processing Systems, volume 32. Curran Associates, Inc.             jected Newton-Krylov method for large-scale bound-constrained
                Elad,M.;Figueiredo,M.A.;andMa,Y.2010. Ontheroleofsparse                   optimization. arXiv preprint arXiv:2005.13639.
                andredundantrepresentationsinimageprocessing. Proceedingsof               Kidger, P.; and Lyons, T. 2020. Universal approximation with deep
                the IEEE, 98(6): 972–982.                                                 narrow networks. In Conference on Learning Theory, 2306–2327.
                Fienup, J. R. 1982. Phase retrieval algorithms: A comparison. Ap-         PMLR.
                plied optics, 21(15): 2758–2769.                                          Klibanov, M. V. 1986. Determination of a compactly supported
                Finlay, C.; Calder, J.; Abbasi, B.; and Oberman, A. 2018. Lipschitz       function from the argument of its Fourier transform. In Doklady
                regularized deep neural networks generalize and are adversarially         Akademii Nauk, volume 289, 539–540. Russian Academy of Sci-
                robust. arXiv preprint arXiv:1808.09540.                                  ences.
                Finlay, C.; Jacobsen, J.-H.; Nurbekyan, L.; and Oberman, A. M.            Krizhevsky, A.; and Hinton, G. 2009. Learning Multiple Layers
                2020.     How to train your neural ODE.            arXiv preprint         of Features from Tiny Images.      Technical report, University of
                arXiv:2002.02798.                                                         Toronto.
                                                                                    6655
