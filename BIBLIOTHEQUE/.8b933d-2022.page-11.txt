              gradient descent. In Advances in Neural Information Processing Systems, pages 3981–3989,
              2016.
            [6] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast
              weights to attend to the recent past. Advances in Neural Information Processing Systems, 29,
              2016.
            [7] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. arXiv preprint
              arXiv:1909.01377, 2019.
            [8] Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Stabilizing equilibrium models by Jacobian
              regularization. arXiv preprint arXiv:2106.14342, 2021.
            [9] Augustin-Louis Cauchy. Résumé d’un mémoir sur la mécanique céleste et sur un nouveau
              calcul appelé calcul des limites. Oeuvres Complétes d’Augustun Cauchy, 12(48-112):3, 1831.
           [10] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya
              Sutskever. Generative pretraining from pixels. In International Conference on Machine
              Learning, pages 1691–1703. PMLR, 2020.
           [11] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary
              differential equations. arXiv preprint arXiv:1806.07366, 2018.
           [12] Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The Helmholtz
              machine. Neural computation, 7(5):889–904, 1995.
           [13] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete
              data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological),
              39(1):1–22, 1977.
           [14] Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models.
              arXiv preprint arXiv:1903.08689, 2019.
           [15] David Duvenaud, J. Zico Kolter, and Matthew Johnson. Deep implicit layers tutorial - neural
              ODEs,deepequilibirum models, and beyond. Neural Information Processing Systems Tutorial,
              2020.
           [16] Gamaleldin F Elsayed, Aravindh Mahendran, Sjoerd van Steenkiste, Klaus Greff, Michael C
              Mozer, and Thomas Kipf. Savi++: Towards end-to-end object-centric learning from real-world
              videos. arXiv preprint arXiv:2206.07764, 2022.
           [17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta-
              tion of deep networks. In International Conference on Machine Learning, pages 1126–1135.
              PMLR,2017.
           [18] V. Fomin, J. Anmol, S. Desroziers, J. Kriss, and A. Tejani. High-level library to help with
              training neural networks in pytorch. https://github.com/pytorch/ignite, 2020.
           [19] Karl Friston. The free-energy principle: a uniﬁed brain theory? Nature Reviews Neuroscience,
              11(2):127–138, 2010.
           [20] Samy Wu Fung, Howard Heaton, Qiuwei Li, Daniel McKenzie, Stanley Osher, and Wotao
              Yin. Fixed point networks: Implicit depth models with Jacobian-free backprop. arXiv preprint
              arXiv:2103.12803, 2021.
           [21] Zhengyang Geng, Xin-Yu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training
              implicit models. Advances in Neural Information Processing Systems, 34, 2021.
           [22] Stephen Gould, Richard Hartley, and Dylan Campbell. Deep declarative networks: A new hope.
              arXiv preprint arXiv:1909.04866, 2019.
           [23] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Grifﬁths. Recasting
              gradient-based meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.
           [24] Klaus Greff, Sjoerd Van Steenkiste, and Jürgen Schmidhuber. Neural expectation maximization.
              arXiv preprint arXiv:1708.03498, 2017.
           [25] Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess,
              Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object repre-
              sentation learning with iterative variational inference. In International Conference on Machine
              Learning, pages 2424–2433. PMLR, 2019.
                               11
