                  B ApproximatingHumanExpertPerformance
                  Establishing a reliable benchmark for human performance on MMMU-Pro is crucial to evaluating the true
                  capabilities of multimodal AI models. Conducting new and rigorous human evaluations, however, is both
                  time-consuming and expensive. To address this issue, we developed an approximation method based on
                  the existing human evaluation data from the original MMMU. The resulting estimates are presented in
                  Table 4.
                                           Overall   Art &    Business   Science   Health &   Human& Tech&
                                                     Design                        Medicine   Social Sci.   Eng.
                                 Low         73.0     77.4      77.9       78.5      65.2        63.6       73.5
                                 Medium      80.8     83.3      88.4       84.9      72.8        75.8       78.2
                                 High        85.4     85.7      89.5       86.0      84.8        81.8       84.4
                  Table 4: Estimated human performance on MMMU-Pro across different disciplines, based on the original MMMU
                  evaluation data. The table presents low, medium, and high performance estimates in terms of overall accuracy and
                  discipline-specific breakdowns.
                    Thevalidity of using this approximation method relies on several key factors. Firstly, the core content
                  and difficulty of the questions in MMMU-Pro remain unchanged from those in the original MMMU,
                  supporting the use of the original human performance data as a valid proxy. Secondly, in the initial
                  MMMUevaluation,humanexpertswererequiredtodocumenttheirproblem-solving processes, which
                  significantly reduced the likelihood of random guessing. For questions lacking detailed solution processes,
                  wesimulated random selection from expanded candidate options and recalculated the accuracy. Finally,
                  humanexperts inherently excel at seamlessly integrating visual and textual information, suggesting that
                  their performance in a purely visual input setting would be analogous to their performance in the original
                  format.
                    Given that the 577 questions in MMMU-Pro are sourced from the MMMU validation set, we extracted
                  the corresponding data from the evaluations of the 90 human experts involved in the original MMMU
                  assessment. We categorized and counted these questions based on whether they included a detailed
                  solution process (w/ Solution) or were subjected to guessing due to the lack of a detailed solution process
                  (w/o Solution). We then counted the correct and incorrect answers in each category, as summarized in
                  Table 5. Specifically, the categorization is defined in Equation 1:
                                           Num       =Num               +Num
                                                total        w/o Solution       w/ Solution
                                                     =Num                     +Num                                           (1)
                                                             w/o Solution(wrong)      w/o Solution(correct)
                                                       +Num                    +Num
                                                               w/ Solution(wrong)      w/ Solution(correct)
                    Using these counts, we can estimate the lower bound of human performance on MMMU-Pro with
                  Equation 2:
                             NumEstimate(correct) = Numw/Solution(correct) + Numw/oSolution Ã— Numw/oSolution             (2)
                                                                                   Num
                                                                                        total
                    This formula considers the number of correctly solved questions with detailed solution processes and
                  the proportion of correctly guessed questions without detailed solution processes, ensuring a conservative
                  estimate.
                    In summary, by leveraging the original MMMU human evaluation data and applying our estimation
                  method, we provide a reasonable approximation of human performance on MMMU-Pro. This approach
                  maintains the human performance benchmark without incurring the substantial costs associated with new
                  expert evaluations.
                                                                      15149
