                          Published as a conference paper at ICLR 2025
                         Table2: PerformancecomparisonoftheMINDmodelwithResNet-50andEfficientNet-B7onCIFAR-100and
                          ImageNet. The MIND model parameter count is shown as “Prediction network/Introspection network/Total”.
                             Model                 #Parameters          Dataset         Top-1          Top-5       Multi-label    Robustness
                                                                                     Accuracy ↑     Accuracy ↑     Accuracy ↑         ↑
                             ResNet-50                25.6M           CIFAR-100         69.9%          84.7%           —             —
                                                                       ImageNet         74.3%          91.6%          68%           69%
                             EfficientNet-B7           66M            CIFAR-100         84.4%          91.1%           —             —
                                                                       ImageNet         84.3%         95.56%          75%           81%
                             MINDmodel         5.01M/0.3M/5.31M       CIFAR-100        85.53%         92.6%            —             —
                                                                       ImageNet        88.3%          96.62%          78%           83%
                          in image classification tasks. We evaluated our models on CIFAR-100 (Krizhevsky, 2009) and ImageNet
                         (Deng et al., 2009) datasets models using Top-1 and Top-5 accuracy metrics. For ImageNet, we also assessed
                          multi-label accuracy (Yun et al., 2021) and robustness to ImageNetV2 (Recht et al., 2019). Table 2 shows that
                          the MINDmodelachievesthehighest Top-1 and Top-5 scores on both datasets. Note that it outperformed
                          both ResNet and EfficientNet-7 on multi-label accuracy and robustness metrics on ImageNet. These results
                          demonstrate superior performance of the MIND model across different datasets and metrics, highlighting its
                          effectiveness in vision tasks. We also show MIND’s performance on different vision datasets in Appendix E.5.
                         Table 3: Performance comparison of different models on WikiText and SQuAD v2.0 datasets. Perplexity
                         (PPL) and bits-per-character (BPC) are reported for language modeling on WikiText-2 and WikiText-103.
                          F1/EMscoresareusedforSQuADv2.0.
                           Model                               WikiText-2↓ WikiText-103 (PPL)↓ SQuADv2.0(F1/EM)↑ Params(M)↓
                                                               PPL     BPC
                           LSTM(Yuetal.,2019)                  99.3     —             48.7                      —                   20
                           LSTM-MIND(ours)                     14.8    0.85           30.5                  72.7 / 70.5             8.6
                           Transformer (Vaswani et al., 2017)  29.2    1.04           18.3                      —                   110
                           BERT-base(Devlin, 2018)              — —                    —                    76.8 / 73.6             110
                           RoBERTa-base(Liu, 2019)              — —                    —                    83.7 / 80.5             125
                           MIND-Transformer (Ours)             14.5    0.80           16.3                 88.7/ 81.01              112
                          4.4   EXPERIMENTS ON LANGUAGE MODELING TASKS
                         Weevaluated performance on language modeling tasks across multiple datasets including WikiText-2 and
                         WikiText-103 datasets (Merity et al., 2016). We compared the MIND model with baseline LSTM models (Yu
                          et al., 2019) and state-of-the-art Transformer-based models (Vaswani et al., 2017; Dai et al., 2019). Table 3
                          presents the perplexity (PPL) and bits-per-character (BPC) results for WikiText-2 and WikiText-103 datasets,
                          along with the F1 and Exact Match (EM) scores for SQuAD v2.0. The LSTM-MIND model significantly
                          reduces perplexity on WikiText-2 (PPL: 14.8) and achieves competitive results on WikiText-103 (PPL: 30.5),
                          outperforming the standard LSTM baseline. Similarly, the MIND-Transformer achieves superior performance
                          across all tasks, with a notable improvement in SQuAD v2.0 (F1: 88.7%, EM: 81.01%) compared to both
                          BERT-base (F1: 76.8%, EM: 73.6%) and RoBERTa-base (F1: 83.7%, EM: 80.5%). Table 7 shows the
                          performance of the LSTM-based MIND model on SQuAD1.1, where it achieves a 95.4% F1 score with
                          confidence interval of 0.2%, further highlighting its strong performance across different datasets.
                         TheMIND-Transformer’sresults demonstrate its ability to outperform leading transformer models in both
                          perplexity and downstream question-answering tasks, despite utilizing fewer parameters (112M compared to
                          RoBERTa’s125M).Theseresults highlight the efficiency of the MIND architecture, which adapts computa-
                          tional resources dynamically, leading to significant gains in performance. We also show additional Ablation
                          onMIND-transformer in Appendix F.1
                                                                               8
