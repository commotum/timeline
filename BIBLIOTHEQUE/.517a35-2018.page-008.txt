                  PLDI’18, June 18–22, 2018, Philadelphia, PA, USA                                            WoosukLee,KihongHeo,RajeevAlur,andMayurNaik
                  Algorithm 3 Weighted Enumerative Search with Divide-and-                                function allows the grammar to condition the expansion
                  ConquerStrategy                                                                         of a production rule on richer information than the parent
                                                                   T    P            T   P                non-terminal as in CFGs.
                        Function weighted_search (G ,G ,pts,Φ,д ,д )
                                                             eu    q    q
                          T         P
                      // д   andд aretheheuristic functions.                                              Probabilistic HOG. Aprobabilistic higher order grammar
                    1: ⟨T,P⟩ := ⟨∅,∅⟩                                                                                 ˆ                  ˆ                ˆ                    ˆ
                    2: repeat                                                                             (PHOG)Gq isatuple ⟨G,q⟩ whereG = ⟨N,Σ,S,C,R,p⟩ is a
                                                                                                                             ˆ        +
                    3:     for all pt ∈ pts do                                                            HOGandq : R → R scores rules that form a probability
                                                                      T             T                     distribution, i.e. ∀A ∈ N,c ∈ C. Í                        q(A[c] → β) = 1.
                    4:        T:=T∪{weighted_search (G ,{pt},Φ,д )}                                                                                     A[c]→β∈R
                                                                  e   q
                    5:     endfor
                    6:     P := P ∪ {weighted_search (GP,∅,Φ,дP)}                                         4.2    Transfer Learning
                    7:     e := LearnDT(T,P,pts)              e    q                                      Wepresent our learning method based on transfer learn-
                    8: until e , ⊥                                                                        ing [23, 24]. Transfer learning is a useful technique when
                    9: return e                                                                           the training and testing data are drawn from different prob-
                  models separately using the two grammars. Those models                                  ability distributions. In our setting, the training data and
                  guide the search for terms and predicates, respectively. To                             testing data are solutions of synthesis problems of which
                  simultaneously enumerate both terms and predicates, the                                 search space P is defined by a context-free grammarG. The
                  algorithm maintains a set of terms (T) and a set of predicates                          training and testing data often follow different probability
                  (P) that are enumerated so far. Initially, they are empty sets                          distributions because of diverse semantic specifications as
                  (line 1). In this algorithm, we use the weighted enumerative                            already shown in Section 2.
                  search described in Section 3.3. In lines 3-5, we enumerate                                Transfer learning reduces the discrepancy between the
                  termsinorderoflikelihoodbyinvokingweighted_search .                                     probability distributions of the training and testing data. We
                                                                                                e         find and construct a common space (other than P) where
                  Thenwegenerateapredicate at line 6. Using the generated                                 the probability distribution of elements corresponding to the
                  terms and predicates so far, the algorithm tries to learn a                             training data is close to those of the testing data.
                  decision tree at line 7 as described in [4]. We find a solution                            Tothis end, we design a feature map that transforms pro-
                  if the function LearnDT finds a condition expression using                              gramsinPtoanotherspaceinwhichcommonfeaturesofthe
                  the terms and predicates. Otherwise, the whole process is                               training and testing data are captured. The new space is also
                  repeated until a correct conditional expression is found.                               defined by a context-free grammar called a pivot grammar.
                      Touseweighted_search forweightedenumerationsof
                                                         e                                                Welearnastatistical program model of the pivot grammar
                  termsandpredicates,weslightlymodifyweighted_search
                                                                                                 e        that assesses the probability of a given testing instance.
                  into the form of a generator [19] (also called semi-coroutine),                            Given a CFG G = ⟨N,Σ,S,R⟩ and a training set D =
                  so that it can yield expressions; it stores the last state when                         {(Φ ,σ ),··· ,(Φ ,σ )} which is a set of pairs of synthesis
                  it returns a candidate term/predicate, and resumes the exe-                                  1   1            n    n
                                                                                                          problems and solutions, a feature map ⟨αN,αΣ⟩ generates
                  cution from that point when it is invoked next time.                                    the pivot grammar G# = ⟨N#,Σ#,S#,R#⟩ and the featured
                                                                                                          training data D# such that
                  4 TransferLearningforPHOGs                                                                         #                              #
                  In this section, we introduce a new learning method to learn                                    N ={αN(A) | A ∈ N},              Σ ={αΣ(t) | t ∈ Σ}
                                                                                                                  S# = αN(S),       R# = {αN(A) →α∆(β) | A → β ∈ R}
                  PHOGsthatgeneralizewellacrosssynthesisproblemswhose                                              D# = {(Φ ,α (σ )),··· ,(Φ ,α (σ ))}
                  solutions have different probability distributions. We first                                                1    ∆ 1              n   ∆ n
                  givepreliminariesforPHOG,thenpresentourtransferlearn-                                   whereαN is the identity function,
                                                                                                                         ϵ                                (β = ϵ)
                                                                                                                        
                  ing method, and lastly, provide actual instances of the learn-                             α∆(β) =  αN(κ1) ·α∆(κ2 ···κ|κ|)             (κ1 ∈ N)
                  ing method used in the experiments.                                                                   
                                                                                                                         αΣ(κ1)·α∆(κ2···κ|κ|)             (κ1 ∈ Σ)
                  4.1     Preliminaries                                                                                 
                                                                                                          and κi denotes the i-th symbol of κ. In short, the feature
                  HigherOrderGrammar. Ahigherordergrammar(HOG)                                            map ⟨αN,αΣ⟩ transforms the original terminals and non-
                   ˆ                             ˆ                                                        terminals into the corresponding feature symbols (described
                  Gisatuple⟨N,Σ,S,C,R,p⟩whereN isasetofnon-terminal                                       in the next section).
                  symbols, Σ is a set of terminal symbols,C is a conditioning
                                                                        ˆ                                                                             ˆ#    #
                  set, S is the start non-terminal symbol, R is a set of rules of                            Next,welearnapivotPHOG⟨G ,q ⟩fromthepivotgram-
                                                                                 ∗                        marG# andfeatured training data D#:
                  the form A[c] → β where A ∈ N,β ∈ (N ∪ Σ) , andc ∈ C.
                  Andpisafunctionoftype(N ∪Σ)∗ →C.                                                                                ˆ#        #   #   #    # ˆ#
                      Thedefinition of HOG is the same as a context-free gram-                                                   G =⟨N ,Σ ,S ,C ,R ,p⟩
                  mar except that the left-hand side of a production rule is                              NotethatthepivotgrammarandpivotPHOGhavethesame
                  parametrized by a contextc ∈ C. The contextc can be com-                                structures as the ordinary grammar and PHOG. Thus, learn-
                  puted by applying the functionp on a sentential form. This                              ing the pivot PHOG is done by the standard learning process
