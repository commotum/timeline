# One-Step Diffusion Distillation via Deep Equilibrium Models (2023)
Source: 95b292-2023.pdf

## Core reasons
- Proposes the Generative Equilibrium Transformer (GET), a DEQ-based transformer trained directly on noise/image pairs to distill diffusion sampling into a single-step generator, which is an original ML architectural/training innovation beyond positional/dimensional tweaks.
- Demonstrates that GET outperforms much larger ViT baselines and more complex distillation schemes in terms of FID, sampling speed, memory, and compute while training offline, underlining a foundational ML improvement in generative modeling efficiency.

## Evidence extracts
- "In this work, our objective is to streamline the distillation of diffusion models while retaining the perceptual quality of the images generated by the original model. To this end, we introduce a simple and effective technique that distills a multi-step diffusion process into a single-step generative model, using solely noise/image pairs. At the heart of our technique is the Generative Equilibrium Transformer (GET), a novel Deep Equilibrium (DEQ) model [5] inspired by the Vision Transformer (ViT) [25, 75]. GET can be interpreted as an infinite depth network using weight-tied transformer layers, which solve for a fixed point in the forward pass. This architectural choice allows for the adaptive application of these layers in the forward pass, striking a balance between inference speed and sample quality. Furthermore, we incorporate an almost parameter-free class conditioning mechanism in the architecture, expanding its utility to class-conditional image generation." (Section 1)
- "We propose a simple yet effective approach to distill diffusion models into generative models capable of sampling with just a single model evaluation. Our method involves training a Generative Equilibrium Transformer (GET) architecture directly on noise/image pairs generated from a pretrained diffusion model, eliminating the need for trajectory information and temporal embedding. GET demonstrates superior performance over more complex online distillation techniques such as progressive distillation [68, 88] in both class-conditional and unconditional settings. In addition, a small GET can generate higher quality images than a 5Ã— larger ViT, sampling faster while using less training memory and fewer compute FLOPs, demonstrating its effectiveness." (Section 7)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
