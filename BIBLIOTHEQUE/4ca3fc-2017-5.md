# Attention Is All You Need (2017)
Source: 4ca3fc-2017.pdf

## Core reasons
- Introduces the Transformer as a new model architecture for sequence transduction that removes recurrence and relies entirely on attention, which is a foundational architectural contribution.
- Frames the contribution as the first sequence transduction model based entirely on attention, indicating a core model design advance rather than positional encoding, dimensional lifting, or dataset creation.

## Evidence extracts
- "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output." (p. 2)
- "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention." (p. 9)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
