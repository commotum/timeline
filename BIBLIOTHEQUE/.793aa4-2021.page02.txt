                           (a) English Transfer Learning on        (b) Cross-lingual Transfer on            (c) Translation on CS-EN
                                      MultiNLI                                 XNLI
                    Figure 1: Performance effect of different positional encoding methods for Transformers (see § 2) on two Natural
                    language Inference datasets from GLUE (Wang et al., 2019), XTREME (Hu et al., 2020) and one Neural Machine
                    Translation dataset WMT 18 (Bojar et al., 2018). Absolute positional encoding (DIET-ABS) can achieve better
                    performance than the relative counterpart (DIET-REL), showing the importance of designing the right position
                    encoding method.
                    ings across different heads and layers of a Trans-              2.1    Transformer
                    former. Based on these observations we propose                  ATransformerblockconsistsoftwotypesoflayers:
                    decoupled positional attention and a new segment                1) Self-attention layer and 2) Feed forward layers.
                    encoding approach (for tasks with multiple seg-
                    ments), and empirically show its superiority.                   Self-Attention Module           Given input sequence
                       Wesummarize our contributions in this paper                  length n, hidden size d, multi-head query-key
                    below.                                                          down-projection size dh, we deﬁne hidden layer
                                                                                    input to this attention head as X ∈ Rn×d, the query
                       • Wetheoretically and empirically analyze the                projection matrix as Wi ∈ Rd×dh, the key projec-
                                                                                                                Q
                          limitation of the absolute position embeddings            tion matrix as Wi ∈ Rd×dh and the value projec-
                                                                                                        K
                                                                                                         i       d×d
                          added to the input. For both absolute and                 tion matrix as W        ∈R h,i∈[h],forhheads.
                          relative information, we show that encoding                                    V
                                                                                    Usually, dh < d as we do multi-head attention
                          position to attention matrix per-head results             with a smaller representation per head (dh = d/h).
                          in superior performance.                                  With that we can write dot-product attention score:
                       • Weproposeasimpleandefﬁcientwaytoen-                                         i           i          i  >
                                                                                                  A =(XW )(XW )
                          code position and segment information. The                                             Q         K
                          proposed encoding matches the SoTA meth-                  This attention score is used to compute the output
                          ods on multiple standard NLP tasks while                  for each head, after scaling and per row normaliza-
                          having a simpler model with lower train-                  tion using softmax:
                          ing/inference costs.                                                                        √
                                                                                           headi = Softmax(Ai/ d)·(XWi )
                       • Ourproposed method can be easily applied to                                                                 V
                          long sequence models (DIET-ABSLIN) and                    Output of all attention heads in a layer are concate-
                          improve all metrics compared with Linformer               nated and passed to the next feed-forward layer
                          (Wangetal., 2020).                                        applied token-wise.
                       • Wepresent ablation studies comparing differ-               2.2    Position Aware Self Attention
                          ent position encoding methods and ways of
                          sharing position encoding parameters across               ManyNLPtasks,suchasmachinetranslation, lan-
                          heads and layers in Transformer.                          guage modeling, are sensitive to the ordering of
                                                                                    input words. Since Transformers are permutation
                    2    Position Encoding for Transformers                         equivariant, we usually additionally include the po-
                    In this section, we brieﬂy review the Transformer               sition information in the input. Below we discuss
                    models (Vaswani et al., 2017) and discuss previ-                someofthepopular position encoding methods.
                    ous improvement of position encoding and analyze                2.2.1    Absolute Position Encodings
                    the limitation of the additive position embedding               Absolute position encodings are computed in the
                    proposed in the initial and widely-adopted Trans-               input layer and are summed with the input token
                    former model.                                                   embeddings. Vaswani et al. (2017) proposed this
                                                                               2975
