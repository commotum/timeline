                           3.4  Agearithmetic
                           Anonymousreviewer2suggested the following task which we include here. The task is to infer the
                           age of a person given a single absolute age and a set of age differences, e.g. “Alice is 20 years old.
                           Alice is 4 years older than Bob. Charlie is 6 years younger than Bob. How old is Charlie?”. Please
                           see the supplementary material for details on the task and results.
                           4   Discussion
                           We have proposed a general relational reasoning model for solving tasks requiring an order of
                           magnitude more complex relational reasoning than the current state-of-the art. BaBi and Sort-of-
                           CLEVRrequire a few steps, Pretty-CLEVR requires up to eight steps and Sudoku requires more
                           than ten steps. Our relational reasoning module can be added to any deep learning model to add a
                           powerful relational reasoning capacity. We get state-of-the-art results on Sudokus solving 96.6% of
                           the hardest Sudokus with 17 givens. We also markedly improve state-of-the-art on the BaBi dataset
                           solving 20/20 tasks in 13 out of 15 runs with a single model trained jointly on all tasks.
                           Onepotential issue with having a loss at every step is that it might encourage the network to learn a
                           greedy algorithm that gets stuck in a local minima. However, the output function r separates the node
                           hidden states and messages from the output probability distributions. The network therefore has the
                           capacity to use a small part of the hidden state for retaining a current best guess, which can remain
                           constant over several steps, and other parts of the hidden state for running a non-greedy multi-step
                           algorithm.
                           Sending messages for all nodes in parallel and summing all the incoming messages might seem like
                           an unsophisticated approach that risk resulting in oscillatory behavior and drowning out the important
                           messages. However, since the receiving node hidden state is an input to the message function, the
                           receiving node can in a sense determine which messages it wishes to receive. As such, the sum can
                           be seen as an implicit attention mechanism over the incoming messages. Similarly the network can
                           learn an optimal message passing schedule, by ignoring messages based on the history and current
                           state of the receiving and sending node.
                           5   Related work
                           Relational networks [Santoro et al., 2017] and interaction networks [Battaglia et al., 2016] are the
                           most directly comparable to ours. These models correspond to using a single step of equation 3.
                           Since it only does one step it cannot naturally do complex multi-step relational reasoning. In order
                           to solve the tasks that require more than a single step it must compress all the relevant relations
                           into a ﬁxed size vector, then perform the remaining relational reasoning in the last forward layers.
                           Relational networks, interaction networks and our proposed model can all be seen as an instance of
                           Graph Neural Networks [Scarselli et al., 2009, Gilmer et al., 2017].
                           GraphneuralnetworkswithmessagepassingcomputationsgobacktoScarsellietal.[2009]. However,
                           there are key differences that we found important for implementing stable multi-step relational
                           reasoning. Including the node features xj at every step in eq. 3 is important to the stability of the
                           network. Scarselli et al. [2009], eq. 3 has the node features, l , inside the message function. Battaglia
                                                                                    n
                           et al. [2016] use an xj in the node update function, but this is an external driving force. Sukhbaatar
                           et al. [2016] also proposed to include the node features at every step. Optimizing the loss at every
                           step in order to learn a convergent message passing algorithm is novel to the best of our knowledge.
                           Scarselli et al. [2009] introduces an explicit loss term to ensure convergence. Ross et al. [2011] trains
                           the inference machine predictors on every step, but there are no hidden states; the node states are the
                           output marginals directly, similar to how belief propagation works.
                           Ourmodelcanalsobeseenasacompletelylearnedmessagepassingalgorithm. Belief propagation
                           is a hand-crafted message passing algorithm for performing exact inference in directed acyclic
                           graphical models. If the graph has cycles, one can use a variant, loopy belief propagation, but it is
                           not guaranteed to be exact, unbiased or converge. Empirically it works well though and it is widely
                           used [Murphy et al., 1999]. Several works have proposed replacing parts of belief propagation with
                           learned modules [Heess et al., 2013, Lin et al., 2015]. Our work differs by not being rooted in loopy
                           BP, and instead learning all parts of a general message passing algorithm. Ross et al. [2011] proposes
                                                                          8
