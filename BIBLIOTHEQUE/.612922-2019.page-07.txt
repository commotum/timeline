                  (6 v.s. 7) due to the GPU memory limit. We adopt           methods and symbolic reasoning. The model is
                  the ELMorepresentations trained on 5.5B corpus             trained by marginalizing over all execution paths
                  for the QANet+ELMo baseline and the large un-              that lead to the correct answer.
                  cased BERT model for the BERT baseline. The                6.1   ModelDescription
                  hyper-parameters for our NAQANet model (§6) are            Our NAQANet model follows the typical archi-
                  the same as for the QANet baseline.
                                                                             tecture of previous reading comprehension mod-
                  5.3    Heuristic Baselines                                 els, which is composed of embedding, encoding,
                  Arecent line of work (Gururangan et al., 2018;             passage-question attention, and output layers. We
                  Kaushik and Lipton, 2018) has identiﬁed that pop-          use the original QANet architecture for everything
                  ular crowdsourced NLP datasets (such as SQuAD              up to the output layer. This gives us a question rep-
                  (Rajpurkar et al., 2016) or SNLI (Bowman et al.,           resentation Q ∈ Rm×d, and a projected question-
                                                                                                             ¯      n×d
                  2015)) are prone to have artifacts and annotation          aware passage representation P ∈ R         . We have
                  biases which can be exploited by supervised algo-          four different output layers, for the four different
                  rithms that learn to pick up these artifacts as signal     kinds of answers the model can produce:
                  instead of more meaningful semantic features. We           Passage span      As in the original QANet model,
                  estimate artifacts by training the QANet model de-         to predict an answer in the passage we apply three
                  scribed in Section 5.2 on a version of DROP where          repetitions of the QANet encoder to the passage
                  either the question or the paragraph input repre-                          ¯
                                                                             representation P and get their outputs as M , M ,
                  sentation vectors are zeroed out (question-only                                                            0    1
                                                                             M respectively. Then the probabilities of the start-
                  and paragraph-only, respectively). Consequently,             2
                                                                             ing and ending positions from the passage can be
                  the resulting models can then only predict answer          computed as:
                  spans from either the question or the paragraph.                     p start
                     In addition, we devise a baseline that estimates                p       =softmax(FFN([M0;M1]),             (1)
                  the answer variance in DROP. We start by counting                    p end
                                                                                     p       =softmax(FFN([M0;M2])              (2)
                  the unigram and bigram answer frequency for each           where FFN is a two-layer feed-forward network
                  whquestion-word in the train set (as the ﬁrst word         with the RELU activation.
                  in the question). The majority baseline then pre-
                  dicts an answerasthesetof3mostcommonanswer                 Question span      Some questions in DROP have
                  spans for the input question word (e.g., for “when”,       their answer in the question instead of the passage.
                  these were “quarter”, “end” and “October”).                Topredict an answer from the question, the model
                                                                             ﬁrst computes a vector hP that represents the infor-
                  6    NAQANet                                               mation it ﬁnds in the passage:
                  DROPisdesignedtoencouragemodelsthatcom-                                    P                  P¯
                                                                                           α =softmax(W P),                     (3)
                  bine neural reading comprehension with symbolic                            P       P¯
                  reasoning. None of the baselines we described in                          h =α P                              (4)
                  Section 5 can do this. As a preliminary attempt            Then it computes the probabilities of the starting
                  toward this goal, we propose a numerically-aware           and ending positions from the question as:
                  QANetmodel,NAQANet,whichallowsthestate-                         q start                       |Q|     P
                  of-the-art reading comprehension system to pro-               p       =softmax(FFN([Q;e           ⊗h ]),      (5)
                                                                                   q end                        |Q|     P
                  duce three new answer types: (1) spans from the                p      =softmax(FFN([Q;e           ⊗h ])       (6)
                  question; (2) counts; (3) addition or subtraction                                                        |Q|
                  over numbers. To predict numbers, the model ﬁrst           where the outer product with the identity (e      ⊗·)
                                                                             simply repeats hP for each question word.
                  predicts whether the answer is a count or an arith-
                  metic expression. It then predicts the speciﬁc num-        Count We model the capability of counting as
                  bers involved in the expression. This can be viewed        a multi-class classiﬁcation problem. Speciﬁcally,
                  as the neural model producing a partially executed         weconsider ten numbers (0–9) in this preliminary
                  logical form, leaving the ﬁnal arithmetic to a sym-        model and the probabilities of choosing these num-
                                                                                                                                 P
                  bolic system. While this model can currently only          bers is computed based on the passage vector h :
                  handle a very limited set of operations, we believe
                                                                                          count                     P
                  this is a promising approach to combining neural                      p      =softmax(FFN(h ))                (7)
                                                                        2374
