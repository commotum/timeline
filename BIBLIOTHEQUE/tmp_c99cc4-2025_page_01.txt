                 Article – Control
                                                                                                                         Transactions of the Institute of
                                                                                                                         Measurement and Control
                 BEVtransformerfor visual 3D object                                                                      1–15
                                                                                                                         TheAuthor(s) 2025
                 detection applied with retentive                                                                        Article reuse guidelines:
                                                                                                                         sagepub.com/journals-permissions
                 mechanism                                                                                               DOI:10.1177/01423312241308367
                                                                                                                         journals.sagepub.com/home/tim
                 Jincheng Pan , Xiaoci Huang, Suyun Luo and Fang Ma
                 Abstract
                 Three-dimensional (3D) vision perception tasks utilizing multiple cameras are pivotal for autonomous driving systems, encompassing both 3D object
                 detection and map segmentation. We introduce a novel approach dubbed RetentiveBEV, leveraging Transformer to learn spatiotemporal features from
                 Bird’s Eye View (BEV) perspectives. These BEV representations form the foundational layer for further autonomous driving tasks. Succinctly, spatial fea-
                 tures within regions of interest (ROIs) are harvested via spatial cross-attention, while temporal dynamics are integrated using temporal self-attention,
                 enriching the BEV with historical data. Our spatial cross-attention is enhanced with a retentive mechanism, prioritizing information surrounding the
                 focal points and enabling the decomposition of this attention mechanism to bolster computational efficiency. On the nuScenes data set test split, our
                 approach achieves a nuScenes Detection Score (NDS) score of 60.4%, without additional training data, which is an 8.7% improvement over the baseline
                 (BEVFormer-base), and is close to the current state-of-the-art method SparseBEV, which gets NDS 65.7% as of August 2024. On the Val split of
                 nuScenes, our method achieves the performance of 55.8 NDS while maintaining a real-time inference speed of 25.3 FPS, and we are currently working
                 on further accelerating inference using TensorRTon the existing basis (the specification of mAP and NDS would be illustrated by equations (12) and
                 (13)). The integration of the retentive mechanism notably boosts the precision and recall in 3D object detection while also expediting the inference
                 process.
                 Keywords
                 3Dobject detection, bird eye’s view, transformer, deep learning, retentive
                 Introduction                                                        efficiency. For example, in complex urban environments where
                                                                                     precise distance measurements are essential for collision avoid-
                 Three-dimensional (3D) spatial perception technology serves         ance and navigation, the reduced accuracy of camera-based
                 as a cornerstone for implementing various autonomous driv-          systems may lead to incorrect positioning of objects and
                 ing features in smart vehicles and robotics. Cameras, by cap-       potential safety hazards. In addition, camera sensors are more
                 turing 2D images, offer rich semantic and textural information      susceptible to adverse weather conditions like fog, rain, or
                 for 3D spatial perception, outperforming light detection and        low-light situations, which can further degrade their perfor-
                 ranging (LiDAR) in aspects like long-range object detection         mance, whereas LiDAR systems can maintain more consistent
                 and cost efficiency. And Camera-based 3D Object Detection           detection capabilities under such conditions (Bijelic et al.,
                 (Huang et al., 2022a, 2022b: 1, 6; Li et al., 2022, 2023b; Liu      2020). These limitations highlight the need for continued
                 et al., 2022; Lu et al., 2023) has witnessed great progress over    research to improve camera-based 3D object detection meth-
                 the past few years. Compared with the LiDAR-based counter-          ods and bridge the performance gap with LiDAR-based
                 parts (Chen et al., 2023; Lang et al., 2019; Lu et al., 2023; Yin   approaches.
                 et al., 2021), camera-based approaches have lower deployment            According to the pipeline, we can divide previous camera-
                 cost and can detect long-range objects. However, despite these      based methods into two paradigms. BEV (Bird’s Eye View)-
                 advancements, camera-based methods still face significant lim-      based methods (Huang et al., 2022a; Huang and Huang,
                 itations in terms of detection accuracy and depth estimation        2022; Li et al., 2022, 2023b; Park et al., 2022) follow a two-
                 comparedtoLiDAR-basedmethods. For instance, as of 2023,             stage pipeline which first constructing an explicit dense BEV
                 on the nuScenes benchmark data set, state-of-the-art camera-        feature from multi-view features and then performing object
                 based detectors achieve an mAP (mean Average Precision) of
                 approximately 40%, whereas leading LiDAR-based detectors
                 can attain mAP scores exceeding 65%, indicating a perfor-           Shanghai University of Engineering Science, China
                 mance gap of about 25% (Caesar et al., 2020; Yin et al.,            Correspondingauthor:
                 2021). This substantial disparity underscores the challenges in     Xiaoci Huang, Shanghai University of Engineering Science, Shanghai
                 depth perception using cameras alone. In practical scenarios,       201620, China.
                 such limitations can critically affect safety and operational       Email: hxc8011@163.com
