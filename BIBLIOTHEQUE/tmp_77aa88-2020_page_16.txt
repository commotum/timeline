                             Published as a conference paper at ICLR 2021
                             Substituting the above value for œÉ back to the inequality and using theorem‚Äôs assumption gives us
                             following inequality:          ‚àö                              ‚àö
                                        LD(w)‚â§(1‚àí1/ n) max LS(w+)+1/ n
                                                                 kk ‚â§œÅ
                                                   v                2
                                                   u                            q         !
                                                   u                                        2
                                                   u1                kwk2           log(n)             n
                                                        klog 1+          2   1+                 +log +2log(6n+3k)
                                                   u                   2
                                                   t4                 œÅ                k               Œ¥
                                                 +                                    n‚àí1
                                                 ‚â§ max LS(w+)+
                                                    kk2‚â§œÅ
                                                   v                                        !
                                                   u                           q        2
                                                   u               kwk2            log(n)              n
                                                   uklog 1+ 22 1+                              +4log +8log(6n+3k)
                                                   u                 œÅ               k                 Œ¥
                                                 +t                                   n‚àí1
                              B ADDITIONALEXPERIMENTAL RESULTS
                              B.1   SVHNANDFASHION-MNIST
                             We report in table 5 results obtained on SVHN and Fashion-MNIST datasets. On these datasets,
                             SAMallows a simple WideResNet to reach or push state-of-the-art accuracy (0.99% error rate for
                             SVHN,3.59%forFashion-MNIST).
                             ForSVHN,weusedalltheavailabledata(73257digitsfortrainingset+531131additionalsamples).
                             For auto-augment, we use the best policy found on this dataset as described in (Cubuk et al., 2018)
                             plus cutout (Devries & Taylor, 2017). For Fashion-MNIST, the auto-augmentation line correspond
                             to cutout only.
                                                         Table 5: Results on SVHN and Fashion-MNIST.
                                                                                         SVHN                  Fashion-MNIST
                                 Model                        Augmentation         SAM         Baseline       SAM         Baseline
                                 Wide-ResNet-28-10            Basic              1.42          1.58         3.98          4.57
                                                                                      ¬±0.02        ¬±0.03         ¬±0.05        ¬±0.07
                                 Wide-ResNet-28-10            Autoaugment        0.99          1.14         3.61          3.86
                                                                                      ¬±0.01        ¬±0.04         ¬±0.06        ¬±0.14
                                 Shake-Shake (26 2x96d)       Basic              1.44          1.58         3.97          4.37
                                                                                      ¬±0.02        ¬±0.05         ¬±0.09        ¬±0.06
                                 Shake-Shake (26 2x96d)       Autoaugment        1.07          1.03         3.59          3.76
                                                                                      ¬±0.02        ¬±0.02         ¬±0.01        ¬±0.07
                              C EXPERIMENTDETAILS
                              C.1   HYPERPARAMETERS FOR EXPERIMENTS
                             Wereportintable6thehyper-parametersselectedbygridsearchfortheCIFARexperiments,andthe
                             onesforSVHNandFashion-MNISTin7. ForCIFAR-10,CIFAR-100,SVHNandFashion-MNIST,
                             weuseabatchsizeof256anddeterminethelearningrateandweightdecayusedtotraineachmodel
                             via a joint grid search prior to applying SAM; all other model hyperparameter values are identical
                             to those used in prior work.
                             For the Imagenet results (ResNet models), the models are trained for 100, 200 or 400 epochs on
                             Google Cloud TPUv3 32 cores with a batch size of 4096. The initial learning rate is set to 1.0 and
                             decayedusingacosineschedule. Weightdecayissetto0.0001withSGDoptimizerandmomentum
                             =0.9.
                             Finally, for the noisy label experiments, we also found œÅ by gridsearch, computing the accuracy
                             on a (non-noisy) validation set composed of a random subset of 10% of the usual CIFAR training
                             samples. We report the validation accuracy of the bootstrapped version of SAM for different levels
                             of noise and different œÅ in table 8.
                                                                                 16
