                           1    Introduction
                           Large Language Models (LLMs) have demonstrated remarkable capabili-
                           ties across diverse tasks, from natural language processing to code gener-
                           ation.  However, the fundamental question of whether these systems can
                           truly "reason" remains contentious in the artificial intelligence community.
                           TheAbstraction and Reasoning Corpus (ARC-AGI) [1], designed specifically
                           to evaluate genuine intelligence in AI systems, provides a stark illustration
                           of this challenge. Although the tasks appear simple to human test takers,
                           both classical algorithmic approaches [2] and modern neural architectures [3]
                           have struggled to achieve high performance on ARC-AGI, painting a bleak
                           picture for artificial reasoning capabilities.
                           Yet, recent developments suggest this picture might be incomplete. The
                           rapid evolution of language models has produced increasingly capable sys-
                           tems at surprisingly small scales, as demonstrated by models like LLaMA-
                           3.2-3B [4] and Nvidia NeMo-Minitron-8B [5]. Moreover, growing evidence
                           suggests that many perceived limitations of these models may be artifacts
                           of implementation or limitations of language rather than fundamental ca-
                           pability gaps. For example, Allen-Zhu and Li [6] show that models often
                           have awareness of their mistakes, even when unable to correct them. Small
                           mistakes in data modelling can significantly inhibit finetuning performance,
                           not because the problem is too complex, but because a simpler structure
                           might be available for the model to learn [7]. Finally, a growing body of
                           research [8, 9, 10] establishes that apparent failures can frequently be traced
                           to tokenization issues rather than reasoning deficits.
                           It seems that models often possess the requisite capabilities but struggle to
                           access them effectively, leading to a counter-intuitive conclusion: The chal-
                           lenge for LLMs lies not in the absence of reasoning ability, but in creating
                           conditions that allow these capabilities to emerge.
                           Building on these insights, we developed an approach specifically tailored to
                           the ARC-AGI dataset. Our method solves 72.5 task out of 100 examples
                           of the public evaluation dataset and 56.5 points in a late submission to the
                           Kaggle competition, suggesting that efÏcient finetuning, proper tokenization
                           and tailored algorithmic support can indeed unlock the latent reasoning ca-
                           pabilities of these systems.
                           2    Pipeline Overview
                           Our approach focuses on efÏciently fine-tuning a large language model to
                           solve the Abstraction and Reasoning Corpus (ARC-AGI) tasks. The ap-
                           proach can be separated into several distinct components: an expanded
                           dataset, secondary fine-tuning at test-time, an inference method optimized to
                           ARC-AGIandaheuristic candidate selection algorithm based on the predic-
                                                                  2
