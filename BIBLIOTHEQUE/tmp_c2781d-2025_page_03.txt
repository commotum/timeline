                        Preprint, Under Review.
                             2. SFT priming demonstrates that including explicit natural language plans in training data
                                significantly improves imitation learning compared to using identical action sequences
                               without plans.
                             3. RLfine-tuning after SFT priming yields planning agents that further outperform baselines
                                in sample efficiency, and learn to plan, execute plans, and replan when necessary.
                             4. Planning agents can be collaboratively steered by humans that produce plans for them. This
                                is only the case following RL and is not achieved by SFT priming alone.
                        Ourworkprovidesclear evidence that dynamic planning facilitates effective allocation of test-time
                        compute in sequential decision making environments, showing that LLM agents can be trained to
                        use additional computational resources intelligently. The ability to steer such planning agents—now
                        capable enough to complete Crafter by collecting diamonds under human guidance—marks a sig-
                        nificant step towards safer and more collaborative LLM agents. Together, these findings suggest a
                        promising path towards more capable, efficient, interpretable, and steerable agentic systems.
                        2   RELATED WORK
                        Classical Planning Methods  Historically, much progress in sequential decision making has in-
                        volved systems that explicitly look ahead before acting. Monte Carlo Tree Search (MCTS)(Coulom,
                        2006), combined with deep neural networks, has driven landmark systems like AlphaGo and
                        MuZero(Silver et al., 2017b;a; Schrittwieser et al., 2020). Model Predictive Control (MPC) iteratively
                        plans short horizons, adapting to new observations (Mayne et al., 2000). Similarly, model-based RL
                        methods, such as World Models, PlaNet, and Dreamer, use imagination rollouts in latent space for
                        effective planning and learning (Ha & Schmidhuber, 2018; Hafner et al., 2019; 2020). Collectively,
                        these approaches underscore the strength of explicit planning, particularly when accurate internal or
                        environmental models are available.
                        LLMReasoningandPlanning Largelanguagemodelshavedemonstratedsignificantreasoning
                        capabilities, particularly through techniques like chain-of-thought (CoT) prompting (Wei et al.,
                        2022). ReAct extends CoT into sequential settings, explicitly prompting models to reason before
                        acting (Yao et al., 2023b). Similar reasoning methods include self-reflective prompting (Shinn et al.,
                        2023; Wang et al., 2023; Hao et al., 2023; Besta et al., 2024; Yao et al., 2023a), automated prompt
                        tuning (Fernando et al., 2024; Hu et al., 2025), and strategic planning demonstrated by CICERO
                        in the Diplomacy game (, FAIR). However, frequent replanning can cause behavioural instability,
                        analogous to RL frame-skipping strategies that advocate less frequent action repetition for improved
                        exploration and consistency (Sharma et al., 2017; Kalyanakrishnan et al., 2021). Recent studies
                        also show diminishing returns and increased brittleness from excessive reasoning (Stechly et al.,
                        2024; Mizrahi et al., 2024; Sui et al., 2025), emphasizing the need for adaptive planning mechanisms.
                        Behaviour cloning with LLMs on data that includes textual reasoning between actions has been
                        showntohelpimitation learning (Yang et al., 2022; Hu & Clune, 2023).
                        Test-Time Compute Scaling   More recently, test–time scaling has shown great promise, spear-
                        headed by the results of OpenAI o1 (Jaech et al., 2024) and DeepSeek R1 (Guo et al., 2025). These
                        gains arise when LLMs improve their own reasoning traces through RL training on tasks with
                        verifiable rewards (Lambert et al., 2024). Methods such as STaR (Zelikman et al., 2022), Quiet-
                        STaR (Zelikman et al., 2024), ScoRE (Kumar et al., 2025), and Meta-CoT (Xiang et al., 2025)
                        showcase iterative self-improvement. Simple prompting strategies like s1 (Muennighoff et al., 2025)
                        and critical insights from (Gandhi et al., 2025), demonstrating the necessity of supervised fine-tuning
                        (SFT) priming with reasoning examples, further support this direction. Moreover, emergent planning
                        capabilities have been observed from RL-trained base models as comments in code tasks (Zhao et al.,
                        2025). While fixed always-planning hierarchical strategies exist (Erdogan et al., 2025), their rigidity
                        motivates research toward adaptive, dynamic approaches.
                        Steering LLM Agents Recent studies have explored methods to steer LLM agents, such as
                        influencing exploration through modulated representational uncertainty (Rahn et al., 2024), adaptively
                        selecting reasoning modes based on task demands (Chen et al., 2024), and improving collaborative
                        decision-making via step-wise RL evaluations (Zhou et al., 2025). Our work demonstrates that
                                                                  3
