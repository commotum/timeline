                            Published as a conference paper at ICLR 2025
                            incorporates the softmax term (α = 0.4), entropy term (β = 0.4), and gradient term (γ = 0.2), achieves the
                            highest Top-1 accuracy of 88% while maintaining computational efficiency with a FLOP count of 1.05G and
                            an inference time of 20 ms. In contrast, removing the gradient term, which captures input sensitivity and
                            enables dynamic adaptation to complex inputs, leads to a severe drop in accuracy to 34.2%. This highlights
                            the critical role of the gradient term in the model’s predictive performance. Similarly, the exclusion of
                            either the softmax term or the entropy term results in moderate decreases in accuracy (71.5% and 68.2%,
                            respectively), underscoring their importance in uncertainty quantification and confidence calibration. These
                            findings demonstrate the complementary contributions of the softmax, entropy, and gradient terms to the
                            overall performance of the model.
                                 Table 11: Impact of hyperparameter configurations on model accuracy, FLOPs, and inference time.
                              Configuration  SoftmaxTerm(α)     EntropyTerm(β)      Gradient Term (γ)  Top-1 Accuracy (%)    FLOPs(G)     Inference Time (ms)
                              Full Model            0.4                0.4                 0.2                 88.0             1.05              20
                              NoSoftmax             0.0                0.67               0.33                 71.5             1.25              22
                              NoEntropy             0.67               0.0                0.33                 68.2             1.30              23
                              NoGradient            0.67               0.33                0.0                 34.2             1.45              25
                            In Appendix F.4, we further validate the effectiveness of the input complexity metrics employed in our
                            approach. Correlation studies reveal that the softmax values exhibit a strong positive correlation (r = 0.82)
                            with human-labeled complexity scores, while the gradient norm component shows a significant correlation
                            (r = 0.79). Both correlations are statistically significant (p < 0.001), providing empirical evidence that the
                            proposed complexity-aware components effectively adapt to varying input complexities. This validation
                            supports the utility of our approach in balancing computational efficiency and predictive accuracy.
                            Theexperiments were conducted in a controlled environment using the ImageNet dataset for classification
                            tasks and NVIDIA A100 GPUs for training and inference. Performance was evaluated based on Top-1
                            accuracy on validation data, computational cost measured in GFLOPs, and inference time per sample in
                            milliseconds. Additionally, the correlation of model outputs with human-labeled complexity scores was used
                            to assess the effectiveness of the proposed input complexity metrics. These results collectively highlight the
                            strengths of our approach in achieving high performance while maintaining computational efficiency.
                            F.4    SOFTMAXREPRESENTATION
                            In our MIND model, the Adaptive Softmax serves a dual purpose: reducing computational complexity, and
                            acting as an intelligent agent for selecting the most appropriate representation of the internal state based on
                            the input complexity. In our architecture, the introspection model functions as a decision-making agent, using
                            Adaptive Softmax to activate layers based on input complexity. It analyzes the input and current internal
                            state to determine task complexity and select the appropriate layers for processing. We implement Adaptive
                            Softmax to align with complexity-based decision-making:
                                     • The shortlist corresponds to commonly needed layer configurations for simpler inputs.
                                     • Subsequent clusters represent increasingly complex configurations for challenging inputs.
                            During training, the introspection model maps input complexities to layer configurations. Adaptive Softmax’s
                            structure aligns with input complexity, quickly selecting simpler configurations for easy inputs and more
                            complex ones when needed. In our experiments, we observed that this approach led to a 28% reduction in
                            average inference time compared to static models of similar capacity, while maintaining or slightly improving
                            accuracy across a range of tasks.
                            Ourexperimentation has also demonstrated a strong correlation between these values and input complexity.
                            Softmax values correlate strongly with input complexity (r = 0.82 with human-labeled scores, r = 0.79 with
                            gradient norm, p < 0.001 for both).
                            This table shows a more nuanced progression of FLOPs, accuracy, and layer usage as softmax entropy
                            increases. Note the non-linear relationship and the plateauing of accuracy for high-complexity inputs.
                                                                                       30
