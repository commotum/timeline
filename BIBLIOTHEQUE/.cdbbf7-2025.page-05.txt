                                                                                            where T                       and K denote 4 × 4 LiDAR-to-
                                                                                                      LiDAR→Camera
                                                                                            camera transform matrix and camera intrinsic parameters,
                                                                                            respectively. This process is depicted in Fig. 3.
                                                                                                This implies that we can obtain paired features of Li-
                                                                                            DAR and image features (Fsparse and F(u,v) , respec-
                                                                                                                               lidar            image
                                                                                            tively) while preserving the sparsity by:
                                                                                                       Fsparse      =Concat(Fsparse,F(u,v) ).                (4)
                                                                                                         combined                  lidar     image
                                                                                                Wefoundthatfusingthesemulti-modal features by only
                                                                                            a simple concatenation significantly enhance detection per-
                                                                                            formance. It does not increase a computational overhead
                                                                                            in the transformer decoder because it does not change the
                                                                                            numberofinput tokens while CMT does increase.
                                                                                            3.3. Multi-modal Sparse Feature Refinement
                 Figure 3. Explicit multi-modal fusion without image depths is              Necessity of additional feature refinement               We found
                                                                                                         ¨
                 available in our voxel-based approach since each valid cell in 3D          that our naıve LiDAR backbone would be insufficient to
                 voxels already possess 3D coordinates, required for LiDAR to               fully encode fine-level geometric features as we only use
                 camera transformation. A LiDAR point can be easily projected               partial computations (about 30%) of a LiDAR backbone, as
                 to the camera space by a pre-defined LiDAR-camera transforma-              shown in Fig. 4 and will be analyzed in Table 4. The com-
                 tion matrix. Similarly, each valid voxel feature has a corresponding       putations are primarily used for adjusting the 3D resolution
                 projected image feature by the same transformation matrix.                 of voxel features in the sparse encoder, and computations
                                                                                            for feature refinement are absent. To remedy this, we em-
                     Unlike CMT, the positional part for keys can be directly               ploy DSVT [33] for our voxel feature refinement, which
                 encoded into 3D positional embedding E             (x,y,z) by us-          can refine sparse features while preserving the sparsity of
                                                                 pos                        the features. After passing DSVT blocks, the result LiDAR
                 ing the voxel feature coordinates (x,y,z). Then, the com-                  features have richer geometric information.
                 bined feature F′ is defined by:
                                     F′ = F +E          (x,y,z).                            Deepfusionmodule Inourmulti-modalfusionapproach,
                                                    pos                                     we explicitly combine LiDAR and camera features while
                     The query q for the first transformer decoder layer is in              preserving the sparsity of the LiDAR features. In this case,
                 the form of a learnable vector following DETR and CMT,                     wecanalsoconsiderapplyingDSVTnotonlytotheLiDAR
                 constructed by the same positional embedding Epos of ran-                  features but to the fused features because the fused features
                 domlyinitializedof3Dcoordinatesof(x,y,z).Thesecoor-                        now have the same form of the LiDAR features. By doing
                 dinates are trained in the training phase and fixed in the test-           so, the additional sparse feature refinement module can fa-
                 ing phase. In this transformer-based architecture, the com-                cilitate the multi-modal fusion as well as refine sparse fea-
                 putational complexity is almost proportional to the number                 tures for encoding fine-level geometric information.
                 of key & value tokens. Therefore, reducing the number of                       To this end, we introduce a deep fusion module (DFM)
                 tokens by receiving sparse features consequently reduces                   to apply the DSVT module into the multi-modal fused fea-
                 the computational cost.                                                    tures. Previous methods typically employ single modality-
                 3.2. Explicit Multi-modal Fusion with Sparse Fea-                          wisefeaturerefinement,followedbymulti-modalfusionac-
                         tures                                                              complished through concatenation of the refined features.
                                                                                            Ontheotherhand,wedirectlyrefineourmulti-modalsparse
                 In our sparse voxel-based approach, multi-modal fusion                     features by extending Eq. 4 to:
                 with image features is more intuitive than in BEV-based ap-
                 proaches. For instance, Liang et al. [21] handle the one-to-                   Fsparse      =DFM(Concat(Fsparse,F(u,v) )),                  (5)
                                                                                                  combined                           lidar      image
                 many mapping between a BEV point and image points by                       where DFM denotes the deep fusion module, which con-
                 adoptingtheconceptofcontinuousfusion.Incontrast,since                      sists of a sequence of DSVT blocks [33].
                 our 3D voxel features carry their 3D positional coordinates
                 (x,y,z), they can be accurately projected to image feature                 3.4. Redundant Feature Elimination
                 space by:
                                                                                            Despite our basic voxel sparsification which removes more
                                T                                             T
                       (u,v,1) = K ·T                           · (x,y,z,1) ,               than 90%featuresintheoriginalvoxelfeatures, the number
                                            LiDAR→Camera
