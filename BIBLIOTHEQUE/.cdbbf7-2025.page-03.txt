                                          Figure 1. Architecture comparison between CMT [39] and our SparseVoxformer.
                significant distinctions from our approach. Firstly, they em-      However, these methods still utilize a BEV space for their
                ploy a sparse height compression module that suppresses            multi-modal fusion, which may result in a loss of 3D geo-
                the z-axis of the 3D voxel features into 2D sparse voxel           metric information.
                features prior to inputting it into a 3D object detector. This        Contrary to previous approaches, Li et al. [17] recently
                can be regarded as using sparse BEV features. Secondly,            introduce a new approach for 3D object detection that uti-
                they do not present a multi-modal setting incorporating a          lizes 3D voxel features. Nevertheless, this method uses
                cameramodality,whereasoursincorporatesasimpleyetef-                dense3Dfeaturesdirectlywithoutmitigatingthelargecom-
                fective multi-modal fusion approach suitable for sparse 3D         putational load, caused by the 3D resolution. Distinct from
                voxel features. Finally, we employ a Transformer-based de-         all the previous approaches, to the best of our knowledge,
                tector for exploiting sparse 3D voxel features, which is also      wearethefirst to adopt sparse 3D voxel features for 3D ob-
                a novel concept that has not been explored in previous liter-      ject detection. Our approach capitalizes on 3D geometric
                ature.                                                             information while maintaining a low computational cost.
                Multi-modal 3D object detection for autonomous driv-               Wang et al. [34] introduce a unified backbone for embed-
                ing   The fusion of LiDAR and camera sensors is a com-             ding multi-modal data using a transformer encoder archi-
                                                                                   tecture. However, they ultimately employ BEV pooling for
                monapproach in 3D object detection for autonomous driv-            the output features of the backbones, which may result in
                ing, known for its synergistic relation. The camera modal-         the loss of 3D geometric information in multi-modal fea-
                ity provides semantic information, supplementing the spar-         tures. A recent method, CMT [39], utilizes BEV features
                sity at long-range distances inherent in the LiDAR modal-          for a LiDAR modality but does not explicitly transform the
                ity. Conversely, the LiDAR modality supplies accurate lo-          image features into a BEV space. Instead, they introduce
                calization information, which is often lacking in the cam-         a cross-modal transformer that receives image and LiDAR
                era modality. However, merging these two types of data to          features as transformer key & value pairs, and implicitly
                form a unified feature set for multi-modal vision tasks can        fuses them using cross-attention. However, LiDAR features
                be challenging due to their inherent heterogeneity. To ad-         are still extracted in the BEV space.
                dressthis,moststate-of-the-artapproachesuseaBEVspace
                as a common ground for multi-modal fusion, following the           Feature sparsification in Transformer-based detection
                developments in the literature regarding single modality           DETR-based architecture has emerged in 2D object detec-
                processing. Liang et al. [22] and Liu et al. [24] transform        tion [4, 16, 26, 46, 47, 49] and has achieved state-of-the-art
                2Dimage features into the BEV space by employing view              performance. This architecture can be readily extended to a
                transformation based on image depth estimation and then            3Dobject detector [1, 5, 37, 39]. In the 2D object detection
                fuse them with LiDAR BEV features. Yang et al. [42] in-            literature, several studies [26, 47] have aimed to reduce the
                troduce a cross-modal fusion approach that utilizes blocks         number of input transformer tokens to enhance efficiency,
                for both LiDAR-to-camera and camera-to-LiDAR fusion.               which appears to be similar with our approach. However,
                However, the multi-modal fusion in these approaches could          these approaches reduce the number of input features by
                be incomplete due to inaccuracies in depth distribution es-        utilizing objectness. On the contrary, our method leverages
                timated from a single image. Additionally, the overhead of         the sparse nature of the LiDAR point cloud, which does not
                image view transformation could be significant.                    require additional knowledge to discriminate unwanted fea-
                   Fu et al. [8] and Song et al. [30] address the issue of in-     tures. To the best of our knowledge, this is the first work
                accurate multi-modalfusion,whichoccursduetothenature               that employssparse3Dvoxelfeaturesdirectlyfor3Dobject
                of depth estimation-based image projection. Yin et al. [43]        detection. Furthermore, unlike the cases of 2D object detec-
                propose two fusion approaches: hierarchical multi-modal            tion, where detection accuracy is slightly compromised, our
                fusion in a BEV space and instance information fusion.             approach even enhances detection accuracy, demonstrating
