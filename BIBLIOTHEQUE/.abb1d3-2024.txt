                                                                        This CVPRpaperisthe Open Access version, provided by the Computer Vision Foundation.
                                                                                                  Except for this watermark, it is identical to the accepted version;
                                                                                        the final published version of the proceedings is available on IEEE Xplore.
                                                                        Point Transformer V3: Simpler, Faster, Stronger
                                                                                                                         1,2                          3                                                4
                                                                                         Xiaoyang Wu                               Li Jiang                  Peng-Shuai Wang
                                                                    5                             1                          2                                       2                          2*                                                  1*
                                          Zhijian Liu                       Xihui Liu                    YuQiao                     Wanli Ouyang                            TongHe                       Hengshuang Zhao
                                                                               1HKU                   2SHAILab                            3CUHK(SZ)                             4PKU                  5MIT
                                                                                   https://github.com/Pointcept/PointTransformerV3
                                                                ScanNet
                                              ScanNet200       Sem. Seg.      ScanNet Eff.
                                               Sem. Seg.          78.6         L.A. 200                                                                                                      MinkUNet           48ms                     Inference Latency
                                     S3DIS 6-Fold        36.0             75.5         ScanNet Eff.
                                      Sem. Seg.   80.8                           62.4    L.A. 20                                                                                             PTv2                                                        146ms
                                  S3DIS                                                     ScanNet Eff.                                                                                     PTv3              44ms             3.3× faster
                                 Sem. Seg.    74.3                                   68.2    L.R. 20%
                                ScanNet     63.5                                       31.1    ScanNet Eff.                                                                                                        Faster Speed
                                Ins. Seg.                                                        L.R. 1%
                                ScanNet200    34.1                                    71.3   SemanticKITTI
                                 Ins. Seg.                                                     Sem. Seg.
                                                  71.2                           81.2                                                                                                        MinkUNet 1.7G                          Memory Consumption
                                         Waymo                           72.1          nuScenes
                                       Vehicle Det.       76.3   71.5                  Sem. Seg.                                                                                             PTv2                                                        12.3G
                                                  Waymo                       Waymo
                                               Pedestrian Det.  Waymo         Sem. Seg.                                                                                                                                    10.2× lower
                                                              Cyclist Det.                                                                                                                   PTv3 1.2G
                                    PTv3    PTv2   FlatFormer   OctFormer    SphereFormer    MinkUNet
                                                Stronger Performance                                                         Wider Receptive Field                                                  Lower MemoryConsumption
                         Figure 1. Overview of Point Transformer V3 (PTv3). Compared to its predecessor, PTv2 [90], our PTv3 shows superiority in the
                         following aspects: 1. Stronger performance. PTv3 achieves state-of-the-art results across a variety of indoor and outdoor 3D perception
                         tasks. 2. Wider receptive field. Benefit from the simplicity and efficiency, PTv3 expands the receptive field from 16 to 1024 points.
                         3. Faster speed. PTv3 significantly increases processing speed, making it suitable for latency-sensitive applications. 4. Lower Memory
                         Consumption. PTv3 reduces memory usage, enhancing accessibility for broader situations.
                                                                       Abstract                                                                       1. Introduction
                               This paper is not motivated to seek innovation within                                                                  Deep learning models have experienced rapid advance-
                         the attention mechanism. Instead, it focuses on overcom-                                                                     ments in various areas, such as 2D vision [24, 38, 78, 86]
                         ing the existing trade-offs between accuracy and efficiency                                                                  andnatural language processing (NLP) [1, 37, 56, 79], with
                         within the context of point cloud processing, leveraging the                                                                 their progress often attributed to the effective utilization of
                         power of scale. Drawing inspiration from recent advances                                                                     scale, encompassing factors such as the size of datasets, the
                         in3Dlarge-scalerepresentationlearning,werecognizethat                                                                        number of model parameters, the range of effective recep-
                         model performance is more influenced by scale than by in-                                                                    tive field, and the computing power allocated for training.
                         tricate design. Therefore, we present Point Transformer V3                                                                         However,incontrasttotheprogressmadein2Dvisionor
                         (PTv3), which prioritizes simplicity and efficiency over the                                                                 NLP,thedevelopmentof3Dbackbones[16,46,62,88]has
                         accuracy of certain mechanisms that are minor to the over-                                                                   beenhinderedintermsofscale,primarilyduetothelimited
                         all performance after scaling, such as replacing the precise                                                                 size and diversity of point cloud data available in separate
                         neighbor search by KNN with an efficient serialized neigh-                                                                   domains [92]. Consequently, there exists a gap in applying
                         bor mapping of point clouds organized with specific pat-                                                                     scaling principles that have driven advancements in other
                         terns. This principle enables significant scaling, expand-                                                                   fields [37]. This absence of scale often leads to a limited
                         ing the receptive field from 16 to 1024 points while remain-                                                                 trade-off between accuracy and speed on 3D backbones,
                         ing efficient (a 3× increase in processing speed and a 10×                                                                   particularly for models based on the transformer architec-
                         improvement in memory efficiency compared with its pre-                                                                      ture [27, 106]. Typically, this trade-off involves sacrific-
                         decessor, PTv2). PTv3 attains state-of-the-art results on                                                                    ing efficiency for accuracy. Such limited efficiency impedes
                         over 20 downstream tasks that span both indoor and out-                                                                      some of these models’ capacity to fully leverage the inher-
                         door scenarios. Further enhanced with multi-dataset joint                                                                    ent strength of transformers in scaling the range of receptive
                         training, PTv3 pushes these results to a higher level.                                                                       fields, hindering their full potential in 3D data processing.
                              *Corresponding author.                                                                                                        Arecent advancement [92] in 3D representation learn-
                                                                                                                                                 4840
             ing has made progress in overcoming the data scale limita-      2. Related Work
             tion in point cloud processing by introducing a synergistic     3D understanding. Conventionally, deep neural architec-
             training approach spanning multiple 3D datasets. Coupled        tures for understanding 3D point cloud data can be broadly
             with this strategy, the efficient convolutional backbone [13]   classified into three categories based on their approach to
             has effectively bridged the accuracy gap commonly associ-       modeling point clouds: projection-based, voxel-based, and
             ated with point cloud transformers [40, 90]. However, point     point-based methods. Projection-based methods project 3D
             cloud transformers themselves have not yet fully benefited      points onto various image planes and utilize 2D CNN-based
             from this privilege of scale due to their efficiency gap com-   backbones for feature extraction [8, 43, 45, 71]. Voxel-
             pared to sparse convolution. This discovery shapes the ini-     based approaches transform point clouds into regular voxel
             tial motivation for our work: to re-weigh the design choices    grids to facilitate 3D convolution operations [53, 70], with
             in point transformers, with the lens of the scaling princi-     their efficiency subsequently enhanced by sparse convolu-
             ple. We posit that model performance is more significantly      tion [13, 25, 84]. However, they often lack scalability in
             influenced by scale than by intricate design.                   terms of the kernel sizes. Point-based methods, by con-
                Therefore, we introduce Point Transformer V3 (PTv3),         trast, process point clouds directly [52, 62, 63, 77, 105] and
             whichprioritizessimplicityandefficiencyovertheaccuracy          have recently seen a shift towards transformer-based archi-
             of certain mechanisms, thereby enabling scalability. Such       tectures [27, 65, 90, 101, 106]. While these methods are
             adjustments have an ignorable impact on overall perfor-         powerful, their efficiency is frequently constrained by the
             manceafterscaling. Specifically, PTv3makesthefollowing          unstructured nature of point clouds, which poses challenges
             adaptations to achieve superior efficiency and scalability:     to scaling their designs.
              • Inspired by two recent advancements [51, 83] and rec-        Serialization-based method.     Recent works [7, 51, 83]
                ognizing the scalability benefits of structuring unstruc-    have introduced approaches diverging from the traditional
                tured point clouds, PTv3 shifts from the traditional spa-    paradigms of point cloud processing, which we catego-
                tial proximity defined by K-Nearest Neighbors (KNN)          rized as serialization-based. These methods structure point
                query, accounting for 28% of the forward time. Instead,      clouds by sorting them according to specific patterns, trans-
                it explores the potential of serialized neighborhoods in     forming unstructured, irregular point clouds into manage-
                point clouds, organized according to specific patterns.      able sequences while preserving certain spatial proximity.
              • PTv3 replaces more complex attention patch interaction       OctFormer [83] inherits order during octreelization, akin to
                mechanisms, like shift-window (impeding the fusion of        z-order, offering scalability but still constrained by the oc-
                attention operators) and the neighborhood mechanism          tree structure itself. FlatFormer [51], on the other hand, em-
                (causing high memory consumption), with a streamlined        ploys a window-based sorting strategy for grouping point
                approach tailored for serialized point clouds.               pillars, akin to window partitioning. However, this design
              • PTv3 eliminates the reliance on relative positional en-      lacks scalability in the receptive field and is more suited to
                coding, which accounts for 26% of the forward time, in       pillar-based 3D object detectors. These pioneering works
                favor of a simpler prepositive sparse convolutional layer.   mark the inception of serialization-based methods.    Our
             We consider these designs as intuitive choices driven by        PTv3 builds on this foundation, defining and exploring the
             the scaling principles and advancements in existing point       full potential of point cloud serialization.
             cloud transformers. Importantly, this paper underscores the     3D representation learning. In contrast to 2D domains,
             critical importance of recognizing how scalability affects      where large-scale pre-training has become a standard ap-
             backbone design, instead of detailed module designs.            proach for enhancing downstream tasks [6], 3D representa-
                This principle significantly enhances scalability, over-     tion learning is still in a phase of exploration. Most stud-
             coming traditional trade-offs between accuracy and effi-        ies still rely on training models from scratch using spe-
             ciency (see Fig. 1). PTv3, compared to its predecessor, has     cific target datasets [94]. While major efforts in 3D rep-
             achieved a 3.3× increase in inference speed and a 10.2×         resentation learning focused on individual objects [57, 68,
             reduction in memory usage. More importantly, PTv3 cap-          69, 87, 103], some recent advancements have redirected at-
             italizes on its inherent ability to scale the range of percep-  tention towards training on real-world scene-centric point
             tion, expanding its receptive field from 16 to 1024 points      clouds[30,36,91,94,107]. Thisshiftsignifiesamajorstep
             while maintaining efficiency. This scalability underpins its    forward in 3D scene understanding. Notably, Point Prompt
             superior performance in real-world perception tasks, where      Training (PPT) [92] introduces a new paradigm for large-
             PTv3achieves state-of-the-art results across over 20 down-      scale representation learning through multi-dataset syner-
             stream tasks in both indoor and outdoor scenarios. Further      gistic learning, emphasizing the importance of scale. This
             augmenting its data scale with multi-dataset training [92],     approach greatly influences our design philosophy and ini-
             PTv3 elevates these results even more. We hope that our         tial motivation for developing PTv3, and we have incorpo-
             insights will inspire future research in this direction.        rated this strategy in our final results.
                                                                          4841
                Outdoor Efficiency (nuScenes)       Training           Inference
                Methods               Params. Latency Memory Latency Memory
                MinkUNet/3[13]         37.9M     163ms      3.3G     48ms       1.7G                      Relative              Index
                                                                                                      Positional Encoding     Operation
                MinkUNet/5[13]        170.3M     455ms      5.6G    145ms       2.1G                                                            KNN Query
                MinkUNet/7[13]        465.0M 1120ms        12.4G    337ms       2.8G
                PTv2/16[90]            12.8M     213ms     10.3G    146ms      12.3G
                PTv2/24[90]            12.8M     308ms     17.6G    180ms      15.2G                                            QKV
                PTv2/32[90]            12.8M     354ms     21.5G    213ms      19.4G                                          Encoding
                PTv3/256(ours)         46.2M     120ms      3.3G     44ms       1.2G                  Relation & Weight
                                                                                                         Encoding              Value                      Unpool
                PTv3/1024(ours)        46.2M     119ms      3.3G     44ms       1.2G                                         Aggregation     Grid Pool
                PTv3/4096(ours)        46.2M     125ms      3.3G     45ms       1.2G                                                                        FFN
               Table 1. Model efficiency. We benchmark the training and infer-              Figure 2. Latency treemap of each components of PTv2. We
               ence efficiency of backbones with various scales of receptive field.         benchmark and visualize the proportion of the forward time of
               The batch size is fixed to 1, and the number after “/” denotes the           each component of PTv2. KNN Query and RPE occupy a total
                                                                  1
               kernel size of sparse convolution and patch size of attention.               of 54% of forward time.
               3. Design Principle and Pilot Study                                          able impact on overall performance. This concept forms
               In this section, we introduce the scaling principle and pilot                the basis of our scaling principle for backbone design, and
               study, which guide the design of our model.                                  wepractice it with our design.
               Scaling principle.        Conventionally, the relationship be-               Breaking the curse of permutation invariance. Despite
               tween accuracy and efficiency in model performance is                        the demonstrated efficiency of sparse convolution, the ques-
               characterized as a “trade-off”, with a typical preference for                tion arises about the need for a scalable point transformer.
               accuracy at the expense of efficiency. In pursuit of this, nu-               While multi-dataset joint training allows for data scaling
               merous methods have been proposed with cumbersome op-                        and the incorporation of more layers and channels con-
               erations. Point Transformers [90, 106] prioritize accuracy                   tributes to model scaling, efficiently expanding the recep-
               and stability by substituting matrix multiplication in the                   tive field to enhance generalization capabilities remains a
               computation of attention weights with learnable layers and                   challenge for convolutional backbones (refer to Tab. 1). It
               normalization, potentially compromising efficiency. Simi-                    is attention, an operator that is naturally adaptive to kernel
               larly, Stratified Transformer [40] and Swin3D [101] achieve                  shape, potentially to be universal.
               improved accuracy by incorporating more complex forms                           However, current point transformers encounter chal-
               of relative positional encoding, yet this often results in de-               lenges in scaling when adhering to the request of permu-
               creased computational speed.                                                 tation invariance, stemming from the unstructured nature of
                   Yet, the perceived trade-off between accuracy and ef-                    point cloud data. In PTv1, the application of the K-Nearest
               ficiency is not absolute, with a notable counterexample                      Neighbors (KNN) algorithm to formulate local structures
               emerging through the engagement with scaling strategies.                     introduced computational complexities. PTv2 attempted to
               Specifically, Sparse Convolution, known for its speed and                    relieve this by halving the usage of KNNcomparedtoPTv1.
               memory efficiency, remains preferred in 3D large-scale                       Despite this improvement, KNN still constitutes a signifi-
               pre-training.    Utilizing multi-dataset joint training strate-              cant computational burden, consuming 28% of the forward
               gies[92], SparseConvolution[13,25]hasshownsignificant                        time (refer to Fig. 2).       Additionally, while Image Rela-
               performance improvements, increasing mIoU on ScanNet                         tive Positional Encoding (RPE) benefits from a grid layout
               semantic segmentation from 72.2% to 77.0% [107]. This                        that allows for the predefinition of relative positions, point
               outperforms PTv2 when trained from scratch by 1.6%, all                      cloud RPE must resort to computing pairwise Euclidean
               whileretainingsuperiorefficiency. However, suchadvance-                      distances and employ learned layers or lookup tables for
               ments have not been fully extended to point transformers,                    mapping such distances to embeddings, proves to be an-
               primarily due to their efficiency limitations, which present                 other source of inefficiency, occupying 26% of the forward
               burdens in model training especially when the computing                      time (see Fig. 2). These extremely inefficient operations
               resource is constrained.                                                     bring difficulties when scaling up the backbone.
                   This observation leads us to hypothesize that model per-                    Inspired by two recent advancements [51, 83], we move
               formance may be more significantly influenced by scale                       away from the traditional paradigm, which treats point
               than by complex design details. We consider the possibility                  clouds as unordered sets. Instead, we choose to “break” the
               oftradingtheaccuracyofcertainmechanismsforsimplicity                         constraints of permutation invariance by serializing point
               and efficiency, thereby enabling scalability. By leveraging                  clouds into a structured format. This strategic transforma-
               the strength of scale, such sacrifices could have an ignor-                  tion enables our method to leverage the benefits of struc-
                                                                                            tured data inefficiency with a compromise of the accuracy
                   1Patch size refers to the number of neighboring points considered to-    of locality-preserving property. We consider this trade-off
               gether for self-attention mechanisms.                                        as an entry point of our design.
                                                                                        4842
               (a) Z-order                                                    (b) Hilbert
               (c) Trans Z-order                                              (d) Trans Hilbert
             Figure 3. Point cloud serialization. We show the four patterns of serialization with a triplet visualization. For each triplet, we show the
             space-filling curve for serialization (left), point cloud serialization var sorting order within the space-filling curve (middle), and grouped
             patchesoftheserializedpointcloudforlocalattention(right). Shiftingacrossthefourserializationpatternsallowstheattentionmechanism
             to capture various spatial relationships and contexts, leading to an improvement in model accuracy and generalization capacity.
             4. Point Transformer V3                                           tionships, potentially capturing special local relationships
             In this section, we present our designs of Point Transformer      that the standard curve may overlook.
             V3 (PTv3), guided by the scaling principle discussed in           Serialized encoding. To leverage the locality-preserving
             Sec. 3. Our approach emphasizes simplicity and speed, fa-         properties of space-filling curves, we employ serialized en-
             cilitating scalability and thereby making it stronger.            coding, a strategy that converts a point’s position into an in-
                                                                               teger reflecting its order within a given space-filling curve.
             4.1. Point Cloud Serialization                                    Due to the bijective nature of these curves, there exists an
                                                                                                  −1     n
                                                                               inverse mapping φ      : Z  7→ Zwhichallowsforthetrans-
             To trade the simplicity and efficiency nature of structured       formation of a point’s position pi ∈ R3 into a serializa-
             data, we introduce point cloud serialization, transforming        tion code. By projecting the point’s position onto a discrete
             unstructured point clouds into a structured format.               space with a grid size of g ∈ R, we obtain this code as
             Space-filling curves. Space-filling curves [59] are paths           −1
             that pass through every point within a higher-dimensional         φ (⌊p/g⌋). Thisencodingisalso adaptable to batched
                                                                               point cloud data. By assigning each point a 64-bit integer
             discrete space and preserve spatial proximity to a certain        to record serialization code, we allocate the trailing k bits
             extent.  Mathematically, it can be defined as a bijective         to the position encoded by φ−1 and the remaining leading
                                    n
             function φ : Z 7→ Z , where n is the dimensionality of            bits to the batch index b ∈ Z. Sorting the points according
             the space, which is 3 within the context of point clouds          to this serialization code makes the batched point clouds
             and also can extend to a higher dimension. Our method             ordered with the chosen space-filling curve pattern within
             centers on two representative space-filling curves:      the      each batch. The whole process can be written as follows:
             z-order curve [54] and the Hilbert curve [29]. The Z-order                                               −1
             curve (see Fig. 3a) is valued for its simplicity and ease                Encode(p,b,g) = (b ≪ k)|φ          (⌊ p / g ⌋),
             of computation, whereas the Hilbert curve (see Fig. 3b)           where ≪denotes left bit-shift and | denotes bitwise OR.
             is known for its superior locality-preserving properties          Serialization. As illustrated in the middle part of triplets
             compared with Z-order curve.                                      in Fig. 3, the serialization of point clouds is accomplished
                Standard space-filling curves process the 3D space by          by sorting the codes resulting from the serialized encoding.
             following a sequential traversal along the x, y, and z axes,      The ordering effectively rearranges the points in a manner
             respectively.  By altering the order of traversal, such as        that respects the spatial ordering defined by the given space-
             prioritizing the y-axis before the x-axis, we introduce re-       filling curve, which means that neighbor points in the data
             ordered variants of standard space-filling curves. To differ-     structure are also likely to be close in space.
             entiate between the standard configurations and the alter-           In our implementation, we do not physically re-order the
             native variants of space-filling curves, we denote the latter     point clouds, but rather, we record the mappings generated
             with the prefix “trans”, resulting in names such as Trans Z-      by the serialization process. This strategy maintains com-
             order (see Fig. 3c) and Trans Hilbert (see Fig. 3d). These        patibility with various serialization patterns and provides
             variants can offer alternative perspectives on spatial rela-      the flexibility to transition between them efficiently.
                                                                            4843
                (a) Reordering                                                         (a) Standard
                                                                                       (b) Shift Dilation
                (b) Padding
                                                                                       (c) Shift Patch
              Figure 4. Patch grouping. (a) Reordering point cloud accord-             (d) Shift Order                 (e) Shuffle Order
              ing to order derived from a specific serialization pattern.   (b)
              Paddingpointcloudsequencebyborrowingpointsfromneighbor-
              ing patches to ensure it is divisible by the designated patch size.                                                  shuffle
              4.2. Serialized Attention                                                 attn   attn    attn    attn     attn    attn    attn   attn
              Re-weigh options of attention mechanism. Image trans-                  Figure 5. Patch interaction. (a) Standard patch grouping with a
              formers[20,49,50],benefitingfromthestructuredandreg-                   regular, non-shifted arrangement; (b) Shift Dilation where points
              ular grid of pixel data, naturally prefer window [49] and              are grouped at regular intervals, creating a dilated effect; (c) Shift
              dot-product [21, 80] attention mechanisms. These meth-                 Patch, which applies a shifting mechanism similar to the shift win-
              odstakeadvantageofthefixedspatialrelationshipsinherent                 dow approach; (d) Shift Order where different serialization pat-
              to image data, allowing for efficient and scalable localized           terns are cyclically assigned to successive attention layers; (e)
              processing. However, this advantage vanishes when con-                 Shuffle Order, where the sequence of serialization patterns is ran-
              fronting the unstructured nature of point clouds. To adapt,            domized before being fed to attention layers.
              previous point transformers [90, 106] introduce neighbor-              patch size increases while still preserving spatial neighbor
              hood attention to construct even-size attention kernels and            relationships to a feasible degree. Although this approach
              adopt vector attention to improve model convergence on                 may sacrifice some neighbor search accuracy when com-
              point cloud data with a more complex spatial relation.                 pared to KNN, the trade-off is beneficial. Given the atten-
                 Inlightofthestructurednatureofserializedpointclouds,                tion’s re-weighting capacity to reference points, the gains
              we choose to revisit and adopt the efficient window and                in efficiency and scalability far outweigh the minor loss in
              dot-product attention mechanisms as our foundational ap-               neighborhood precision (scaling it up is all we need).
              proach. While the serialization strategy may temporarily               Patch interaction. The interaction between points from
              yield a lower performance than some neighborhood con-                  different patches is critical for the model to integrate infor-
              struction strategies like KNN due to a reduction in precise            mation across the entire point cloud. This design element
              spatial neighbor relationships, we will demonstrate that any           counters the limitations of a non-overlapping architecture
              initial accuracy gaps can be effectively bridged by harness-           and is pivotal in making patch attention functional. Build-
              ing the scalability potential inherent in serialization.               ing on this insight, we investigate various designs for patch
                 Evolving from window attention, we define patch atten-              interaction as outlined below (also visualized in Fig. 5):
              tion, a mechanism that groups points into non-overlapping               • In Shift Dilation [83], patch grouping is staggered by
              patches and performs attention within each individual                      a specific step across the serialized point cloud, effec-
              patch. The effectiveness of patch attention relies on two                  tively extending the model’s receptive field beyond the
              major designs: patch grouping and patch interaction.                       immediate neighboring points.
              Patch grouping. Grouping points into patches within se-                 • In Shift Patch, the positions of patches are shifted across
              rialized point clouds has been well-explored in recent ad-                 the serialized point cloud, drawing inspiration from the
              vancements [51, 83]. This process is both natural and ef-                  shift-window strategy in image transformers [49]. This
              ficient, involving the simple grouping of points along the                 method maximizes the interaction among patches.
              serialized order after padding. Our design for patch atten-             • In Shift Order, the serialized order of the point cloud
              tion is also predicated on this strategy as presented in Fig. 4.           data is dynamically varied between attention blocks.
              In practice, the processes of reordering and patch padding                 This technique, which aligns seamlessly with our point
              can be integrated into a single indexing operation.                        cloud serialization method, serves to prevent the model
                 Furthermore, we illustrate patch grouping patterns de-                  fromoverfitting to a single pattern and promotes a more
              rived from the four serialization patterns on the right part of            robust integration of features across the data.
                                                                                                       ∗
              triplets in Fig. 3. This grouping strategy, in tandem with our          • Shuffle Order , building upon Shift Order, introduces a
              serialization patterns, is designed to effectively broaden the             random shuffle to the permutations of serialized orders.
              attention mechanism’s receptive field in the 3D space as the               This method ensures that the receptive field of each at-
                                                                                  4844
                     Initialization      Encoder         Block                             ×	N×	S                Patterns                     S.O.          +S.D.           +S.P.       +Shuffle O.
                           *                     * s                                                             Z                         74.354ms        75.589ms       75.886ms        74.354ms
                       d     n      g              r      *
                       u     o      n        l     e                  m     n         m                          Z+TZ                      76.055ms        76.392ms       76.189ms        76.955ms
                             i      i        o     d                  r               r
                       o     t                                        o     o         o
                       l     a      d                        E              i                P                   H+TH                      76.2            76.1           76.2            76.8
                             z      d        Po    Or        P        N     t         N                                                          60ms           98ms            94ms           60ms
                       C     i                                              n
                             l      e              e                  r               r
                       t                     d     l                  e     e         e
                       n     a      b        i     f                        t                ML                  Z+TZ+H+TH                 76.561ms        76.899ms       76.697ms        77.361ms
                       i     i                     f         xC       y               y
                       Po    r      Em       Gr    u                  La    At        La
                             Se                    Sh                                                            Table 2. Serialization patterns and patch interaction. The first
                                                                                                                 columnindicatesserializationpatterns: ZforZ-order, TZforTrans
                                         Figure 6. Overall architecture.                                         Z-order, H for Hilbert, and TH for Trans Hilbert. In the first row,
                        tention layer is not limited to a single pattern, thus fur-                              S.O. represents Shift Order, which is the default setting also ap-
                        ther enhancing the model’s ability to generalize.                                        plied to other interaction strategies. S.D. stands for Shift Dilation,
                   Wemarkourmainproposalwith∗andunderscoreitssupe-                                               and S.P. signifies Shift Patch.
                   rior performance in model ablation.                                                           PE                   APE           RPE         cRPE           CPE         xCPE
                   Positional encoding.                To handle the voluminous data,                            Perf. (%)          72.1         75.9         76.8          76.6         77.3
                   point cloud transformers commonly employ local atten-                                                                 50ms          72ms         101ms         58ms         61ms
                   tion, which is reliant on relative positional encoding meth-                                  Table 3. Positional encoding. We compare the proposed CPE+
                   ods [40, 101, 106] for optimal performance. However, our                                      with APE, RPE, cRPE, and CPE. RPE and CPE are discussed in
                   observations indicate that RPEs are notably inefficient and                                   OctFormer [83], while cRPE is deployed by Swin3D [101].
                   complex. As a more efficient alternative, conditional posi-                                     P.S.            16        32        64       128      256      1024      4096
                   tional encoding (CPE) [14, 83] is introduced for point cloud
                   transformers, where implemented by octree-based depth-                                          Perf. (%)      75.0      75.6      76.3     76.6      76.8      77.3     77.1
                   wise convolutions [84]. We consider this replacement to be                                      Std. Dev.      0.15      0.22      0.31     0.36      0.28      0.22     0.39
                   elegant, as the implementation of RPE in point cloud trans-                                   Table 4. Patch size. Leveraging the inherent simplicity and effi-
                   formers can essentially be regarded as a variant of large-                                    ciency of our approach, we expand the receptive field of attention
                   kernel sparse convolution. Even so, a single CPE is not suf-                                  well beyond the conventional scope, surpassing sizes used in pre-
                   ficient for the peak performance (there remains potential for                                 vious works such as PTv2 [90], which adopts a size of 16, and
                   an additional 0.5% improvement when coupled with RPE).                                        OctFormer [83], which uses 24.
                   Therefore, we present an enhanced conditional positional                                      with shuffle the permutation of serialized orders for Shift
                   encoding (xCPE), implemented by directly prepending a                                         Order, is integrated into the pooling.
                   sparse convolution layer with a skip connection before the                                    Model architecture. The architecture of PTv3 remains
                   attention layer. Our experimental results demonstrate that                                    consistent with the U-Net [66] framework. It consists of
                   xCPEfullyunleashestheperformancewithaslightincrease                                           four stage encoders and decoders, with respective block
                   in latency of a few milliseconds compared to the standard                                     depths of [2, 2, 6, 2] and [1, 1, 1, 1]. For these stages, the
                   CPE,theperformance gains justify this minor trade-off.                                        grid size multipliers are set at [×2, ×2, ×2, ×2], indicating
                   4.3. Network Details                                                                          the expansion ratio relative to the preceding pooling stage.
                   In this section, we detail the macro designs of PTv3, includ-                                 5. Experiments
                   ingblockstructure,poolingstrategy,andmodelarchitecture                                        5.1. Main Properties
                   (visualized in Fig. 6). Our options for these components are
                   empirical yet also crucial to overall simplicity. Detailed ab-                                WeperformanablationstudyonPTv3,focusingonmodule
                                                                                 Appendix
                                                                                 AppendixAppendix
                                                                                 AppendixAppendix
                                                                                 AppendixAppendix
                   lations of these choices are available in theAppendixAppendix.Appendix
                                                                                 AppendixAppendix
                                                                                 AppendixAppendix                design and scalability. We report the performance using the
                                                                                 AppendixAppendix
                                                                                 Appendix
                                                                                                                 mean
                                                                                                                meanmean
                                                                                                                meanmean
                   Block structure. We simplify the traditional block struc-                                    meanmean
                                                                                                                meanmeanmean results from the ScanNet semantic segmentation val-
                                                                                                                meanmean
                                                                                                                meanmean
                                                                                                                meanmean
                                                                                                                 mean
                                                                                                                                                                                 average
                                                                                                                                                                                 aavverageerage
                                                                                                                                                                                 aavverageerage
                   ture, typically an extensive stack of normalization and acti-                                                                                                 aavverageerage
                                                                                                                 idation and measure the latencies using the aaavvverageerageerage values
                                                                                                                                                                                 aavverageerage
                                                                                                                                                                                 aavverageerage
                                                                                                                                                                                 aavverageerage
                                                                                                                                                                                 average
                   vation layers, by adopting a pre-norm [12] structure, eval-                                   obtained from the full ScanNet validation set (with a batch
                   uated against the post-norm [80] alternative. Additionally,                                   size of 1) on a single RTX 4090. If not specified, default
                                                                                                                                                      Appendix
                                                                                                                                                      AppendixAppendix
                                                                                                                                                      AppendixAppendix
                   weshift from Batch Normalization (BN) to Layer Normal-                                                                             AppendixAppendix
                                                                                                                 settings are presented in AppendixAppendix.Appendix  All of our designs are
                                                                                                                                                      AppendixAppendix
                                                                                                                                                      AppendixAppendix
                                                                                                                                                      AppendixAppendix
                                                                                                                                                      Appendix
                   ization (LN). The proposed xCPE is prepended directly be-                                     enabled by default. Default settings are marked in gray.
                   fore the attention layer with a skip connection.                                              Serialization patterns. In Tab. 2, we explore the impact of
                   Pooling strategy. We keep adopting the Grid Pooling in-                                       various combinations of serialization patterns. Our exper-
                   troduced in PTv2, recognizing its simplicity and efficiency.                                  iments demonstrate that mixtures incorporating a broader
                   Our experiments indicate that BN is essential and cannot                                      rangeofpatternsyieldsuperiorresultswhenintegratedwith
                   be effectively replaced by LN. We hypothesize that BN is                                      our Shuffle Order strategies. Furthermore, the additional
                   crucial for stabilizing the data distribution in point clouds                                 computational overhead from introducing more serializa-
                   during pooling. Additionally, the proposed Shuffle Order,                                     tion patterns is negligible. It is observed that relying on
                                                                                                            4845
                 Indoor Sem. Seg.      ScanNet [17]     ScanNet200 [67]       S3DIS[2]               Outdoor Sem. Seg.       nuScenes [5]   Sem.KITTI[3]       WaymoVal[72]
                 Methods               Val      Test      Val      Test     Area5    6-fold          Methods                 Val     Test    Val      Test     mIoU     mAcc
                 ◦MinkUNet[13]         72.2     73.6     25.0      25.3      65.4     65.4           ◦MinkUNet[13]           73.3     -     63.8       -       65.9      76.6
                 ◦ST[40]               74.3     73.7       -         -       72.0       -            ◦SPVNAS[73]             77.4     -     64.7     66.4        -         -
                 ◦PointNeXt [64]       71.5     71.2       -         -       70.5     74.9           ◦Cylinder3D [108]       76.1    77.2   64.3     67.8        -         -
                 ◦OctFormer[83]        75.7     76.6     32.6      32.6        -        -            ◦AF2S3Net[10]           62.2    78.0   74.2     70.8        -         -
                 ◦Swin3D[101]          76.4      -         -         -       72.5     76.9           ◦2DPASS[98]              -      80.8   69.3     72.9        -         -
                 ◦PTv1[106]            70.6      -       27.8        -       70.4     65.4           ◦SphereFormer [41]      78.4    81.9   67.8     74.8      69.9        -
                 ◦PTv2[90]             75.4     74.2     30.2        -       71.6     73.5           ◦PTv2[90]               80.2    82.6   70.3     72.6      70.6      80.2
                 ◦PTv3(Ours)           77.5     77.9     35.2      37.8      73.4     77.7           ◦PTv3(Ours)             80.4    82.7   70.8     74.2      71.3      80.5
                 •PTv3(Ours)           78.6     79.4     36.0      39.3      74.7     80.8           •PTv3(Ours)             81.2    83.0   72.3     75.5      72.1      81.3
                               Table 5. Indoor semantic segmentation.                                            Table 7. Outdoor semantic segmentation.
                  Method Metric Area1 Area2 Area3 Area4 Area5 Area6 6-Fold                           Indoor Ins. Seg.          ScanNet [17]              ScanNet200 [67]
                            allAcc   92.30   86.00   92.98   89.23    91.24   94.26   90.76          PointGroup [35]     mAP       mAP       mAP mAP           mAP       mAP
                                                                                                                              25        50                25        50
                  ◦PTv2 mACC 88.44 72.81 88.41 82.50 77.85 92.44                      83.13          ◦MinkUNet[13]        72.8       56.9    36.0     32.2       24.5    15.8
                             mIoU 81.14 61.25 81.65 69.06 72.02 85.95                 75.17          ◦PTv2[90]            76.3       60.0    38.3     39.6       31.9    21.4
                            allAcc   93.22   86.26   94.56   90.72    91.67   94.98   91.53          ◦PTv3(Ours)          77.5       61.7    40.9     40.1       33.2    23.1
                  ◦PTv3 mACC 89.92 74.44 94.45 81.11 78.92 93.55                      85.31          •PTv3(Ours)          78.9       63.5    42.1     40.8       34.1    24.0
                             mIoU 83.01 63.42 86.66 71.34 73.43 87.31                 77.70                       Table 8. Indoor instance segmentation.
                            allAcc   93.70   90.34   94.72   91.87    91.96   94.98   92.59
                  •PTv3 mACC 90.70 78.40 94.27 86.61 80.14 93.80                      87.69          Data Efficient [30]  Limited Reconstruction       Limited Annotation
                             mIoU 83.88 70.11 87.40 75.53 74.33 88.74                 80.81          Methods              1% 5% 10% 20% 20                  50    100    200
                               Table 6. S3DIS 6-fold cross-validation.                               ◦MinkUNet[13]        26.0 47.8 56.7 62.9 41.9 53.9 62.2 65.5
                                                                                                     ◦PTv2[90]            24.8 48.1 59.8 66.3 58.4 66.1 70.3 71.2
                 a single Shift Order cannot completely harness the potential                        ◦PTv3(Ours)          25.8 48.9 61.0 67.0 60.1 67.9 71.4 72.7
                 offered by the four serialization patterns.                                         •PTv3(Ours)          31.3 52.6 63.3 68.2 62.4 69.1 74.3 75.5
                 Patch interaction. In Tab. 2, we also assess the effective-                                               Table 9. Data efficiency.
                 ness of each alternative patch interaction design. The de-                        tion [18, 19], which can be advantageous for our PTv3.
                 fault setting enables Shift Order, but the first row represents                   Patch size. In Tab. 4, we explore the scaling of the recep-
                 the baseline scenario using a single serialization pattern, in-                   tive field of attention by adjusting patch size. Beginning
                 dicativeofthevanillaconfigurationsofShiftPatchandShift                            with a patch size of 16, a standard in prior point transform-
                 Dilation (one single serialization order is not shiftable). The                   ers, we observe that increasing the patch size significantly
                 results indicate that while Shift Patch and Shift Dilation are                    enhances performance. Moreover, as indicated in Tab. 1
                 indeed effective, their latency is somewhat hindered by the                       (benchmarked with NuScenes dataset), benefits from op-
                 dependency on attention masks, which compromises effi-                            timization techniques such as flash attention [18, 19], the
                 ciency. Conversely, Shift Code, which utilizes multiple se-                       speed and memory efficiency are effectively managed.
                 rialization patterns, offers a simple and efficient alternative
                 that achieves comparable results to these traditional meth-                       5.2. Results Comparision
                 ods. Notably, when combined with Shuffle Order and all
                 four serialization patterns, our strategy not only shows fur-                     We benchmark the performance of PTv3 against previous
                 ther improvement but also retains its efficiency.                                                                              highest
                                                                                                                                                highesthighest
                                                                                                                                                highesthighest
                                                                                                                                                highesthighest
                                                                                                   SOTA backbones and present the highesthighesthighest results obtained
                                                                                                                                                highesthighest
                                                                                                                                                highesthighest
                                                                                                                                                highesthighest
                                                                                                                                                highest
                 Positional encoding. In Tab. 3, we benchmark our pro-                             for each benchmark. In our tables, Marker ◦refers to a
                 posedCPE+againstconventionalpositionalencoding,such                               model trained from scratch, and •refers to a model trained
                 as APE and RPE, as well as recent advanced solutions like                         with multi-dataset joint training (PPT [92]). An exhaustive
                                                                                                                                                                  Appendix
                                                                                                                                                                  AppendixAppendix
                                                                                                                                                                  AppendixAppendix
                                                                                                                                                                 AppendixAppendix
                                                                                                   comparisonwithearlier works is available in theAppendixAppendix.Appendix
                                                                                                                                                                 AppendixAppendix
                                                                                                                                                                  AppendixAppendix
                                                                                                                                                                  AppendixAppendix
                 cRPE and CPE. The results confirm that while RPE and                                                                                             Appendix
                 cRPE are significantly more effective than APE, they also                         Indoor semantic segmentation. In Tab. 5, we showcase
                 exhibit the inefficiencies previously discussed. Conversely,                      the validation and test performance of PTv3 on the Scan-
                 CPE and CPE+ emerge as superior alternatives. Although                            Net v2 [17] and ScanNet200 [67] benchmarks, along with
                 CPE+ incorporates slightly more parameters than CPE, it                           the Area 5 and 6-fold cross-validation [62] on S3DIS [2]
                 does not compromise our method’s efficiency too much.                             (details see Tab. 6). We report the mean Intersection over
                 Since CPEs operate prior to the attention phase rather than                       Union (mIoU) percentages and benchmark these results
                 during it, they benefit from optimization like flash atten-                       against previous backbones.              Even without pre-training,
                                                                                                4846
                 WaymoObj. Det.          Vehicle L2 Pedestrian L2 Cyclist L2 Mean L2                   Indoor Efficiency (ScanNet)          Training              Inference
                 Methods              # mAP APH mAP APH mAP APH mAPH                                   Methods              Params.   Latency    Memory Latency Memory
                 PointPillars [43]    1 63.6    63.1    62.8   50.3   61.9    59.9     57.8            MinkUNet[13]          37.9M      267ms        4.9G      90ms        4.7G
                 CenterPoint [102]    1 66.7    66.2    68.3   62.6   68.7    67.6     65.5            OctFormer [83]        44.0M      264ms       12.9G      86ms       12.5G
                 SST[22]              1 64.8    64.4    71.7   63.0   68.0    66.9     64.8            Swin3D[101]           71.1M      602ms       13.6G     456ms        8.8G
                 SST-Center [22]      1 66.6    66.2    72.4   65.0   68.9    67.6     66.3            PTv2[90]              12.8M      312ms       13.4G     191ms       18.2G
                 VoxSet [28]          1 66.0    65.6    72.5   65.4   69.0    67.7     66.2            PTv3(ours)            46.2M      151ms        6.8G      61ms        5.2G
                 PillarNet [26]       1 70.4    69.9    71.6   64.9   67.8    66.7     67.2                            Table 11. Indoor model efficiency.
                 FlatFormer [51]      1 69.0    68.6    71.5   65.3   68.6    67.5     67.2
                 PTv3(Ours)           1 71.2    70.8    76.3   70.4   71.5    70.4     70.5          various settings, from 5% to 20% of reconstructions and
                 CenterPoint [102]    2 67.7    67.2    71.0   67.5   71.5    70.5     68.4          from 20 to 200 annotations, PTv3 demonstrates strong per-
                 PillarNet [26]       2 71.6    71.6    74.5   71.4   68.3    67.5     70.2          formance. Moreover, the application of pre-training tech-
                 FlatFormer [51]      2 70.8    70.3    73.8   70.5   73.6    72.6     71.2          nologiesfurtherboostsPTv3’sperformanceacrossalltasks.
                 PTv3(Ours)           2 72.5    72.1    77.6   74.5   71.0    70.1     72.2          Outdoorobjectdetection. InTab.10,webenchmarkPTv3
                 CenterPoint++[102]3 71.8       71.4    73.5   70.8   73.7    72.8     71.6          against leading single-stage 3D detectors on the Waymo
                 SST[22]              3 66.5    66.1    76.2   72.3   73.6    72.8     70.4          Object Detection benchmark. All models are evaluated us-
                 FlatFormer [51]      3 71.4    71.0    74.5   71.3   74.7    73.7     72.0          ingeitheranchor-basedorcenter-baseddetectionheads[99,
                 PTv3(Ours)           3 73.0    72.5    78.0   75.0   72.3    71.4     73.0          102], with a separate comparison for varying numbers of
                 Table 10. Waymo object detection. The colume with head name                         input frames. Our PTv3, engaged with CenterPoint, consis-
                 “#” denotes the number of input frames.                                             tently outperforms both sparse convolutional [26, 102] and
                 PTv3 outperforms PTv2 by 3.7% on the ScanNet test split                             transformer-based [22, 28] detectors, achieving significant
                 and by 4.2% on the S3DIS 6-fold CV. The advantage of                                gains even when compared with the recent state-of-the-art,
                 PTv3 becomes even more pronounced when scaling up the                               FlatFormer [51]. Notably, PTv3 surpasses FlatFormer by
                 model with multi-dataset joint training [92], widening the                          3.3%withasingleframeasinputandmaintainsasuperior-
                 margin to 5.2% on ScanNet and 7.3% on S3DIS.                                        ity of 1.0% in multi-frame settings.
                 Outdoor semantic segmentation. In Tab. 7, we detail the                             Model efficiency. We evaluate model efficiency based on
                 validation and test results of PTv3 for the nuScenes [5, 23]                        averagelatencyandmemoryconsumptionacrossreal-world
                 and SemanticKITTI [3] benchmarks and also include the                               datasets. Efficiency metrics are measured on a single RTX
                 validation results for the Waymo benchmark [72]. Perfor-                            4090, excluding the first iteration to ensure steady-state
                 mance metrics are presented as mIoU percentages by de-                              measurements. We compared our PTv3 with multiple pre-
                 fault, with a comparison to prior models. PTv3 demon-                               vious SOTAs. Specifically, we use the NuScenes dataset to
                 strates enhanced performance over the recent state-of-the-                          assess outdoor model efficiency (see Tab. 1) and the Scan-
                 art model, SphereFormer, with a 2.0% improvement on                                 Net dataset for indoor model efficiency (see Tab. 11). Our
                 nuScenes and a 3.0% increase on SemanticKITTI, both in                              results demonstrate that PTv3 not only exhibits the lowest
                 the validation context. When pre-trained, PTv3’s lead ex-                           latency across all tested scenarios but also maintains rea-
                 tends to 2.8% for nuScenes and 4.5% for SemanticKITTI.                              sonable memory consumption.
                 Indoor instance segmentation.                 In Tab. 8, we present                 6. Conclusion
                 PTv3’s validation results on the ScanNet v2 [17] and                                This paper presents Point Transformer V3, a stride towards
                 ScanNet200 [67] instance segmentation benchmarks. We                                overcoming the traditional trade-offs between accuracy and
                 present the performance metrics as mAP, mAP , and                                   efficiency in point cloud processing. Guided by a novel in-
                                                                                   25                terpretation of the scaling principle in backbone design, we
                 mAP and compare them against several popular back-
                       50                                                                            propose that model performance is more profoundly influ-
                 bones.       To ensure a fair comparison, we standardize
                 the instance segmentation framework by employing Point-                             encedbyscalethanbycomplexdesignintricacies. Byprior-
                 Group[35]across all tests, varying only the backbone. Our                           itizing efficiency over the accuracy of less impactful mech-
                 experiments reveal that integrating PTv3 as a backbone sig-                         anisms, we harness the power of scale, leading to enhanced
                 nificantly enhances PointGroup, yielding a 4.9% increase                            performance. Simply put, by making the model simpler
                 in mAP over MinkUNet. Moreover, fine-tuning a PPT pre-                              and faster, we enable it to become stronger.
                 trained PTv3 provides an additional gain of 1.2% mAP.                               Acknowledgements
                 Indoor data efficient. In Tab. 9, we evaluate the perfor-                           This work is supported in part by the National Natural Sci-
                 mance of PTv3 on the ScanNet data efficient [30] bench-                             ence Foundation of China (No. 622014840), the National
                 mark. This benchmark tests models under constrained con-                            KeyR&DProgramofChina(No. 2022ZD0160101),HKU
                 ditionswithlimitedpercentagesofavailablereconstructions                             Startup Fund, and HKU Seed Fund for Basic Research.
                 (scenes) and restricted numbers of annotated points. Across
                                                                                                 4847
              References                                                          [16] Angela Dai and Matthias Nießner. 3dmv: Joint 3d-multi-
                [1] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,                 view prediction for 3d semantic scene segmentation.  In
                    Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei                ECCV,2018. 1, 15
                    Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta,       [17] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
                    Kai Hui, Sebastian Ruder, and Donald Metzler. Ext5: To-            ber, Thomas Funkhouser, and Matthias Nießner. ScanNet:
                    wards extreme multi-task scaling for transfer learning. In         Richly-annotated 3d reconstructions of indoor scenes. In
                    ICLR, 2022. 1                                                      CVPR,2017. 7, 8, 16
                [2] Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioan-     [18] Tri Dao. Flashattention-2: Faster attention with better par-
                    nis Brilakis, Martin Fischer, and Silvio Savarese. 3d seman-       allelism and work partitioning. arXiv:2307.08691, 2023.
                    tic parsing of large-scale indoor spaces. In CVPR, 2016. 7,        7
                    16                                                            [19] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and
                                                                                                    ´
                                                                                       Christopher Re. FlashAttention: Fast and memory-efficient
                [3] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-             exact attention with IO-awareness. In NeurIPS, 2022. 7
                    zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-      [20] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming
                    mantickitti: A dataset for semantic scene understanding of         Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining
                    lidar sequences. In ICCV, 2019. 7, 8, 16                           Guo. Cswin transformer: A general vision transformer
                [4] Maxim Berman, Amal Rannen Triki, and Matthew B                     backbone with cross-shaped windows. In CVPR, 2022. 5
                                      ´
                    Blaschko. The lovasz-softmax loss: A tractable surrogate      [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
                    for the optimization of the intersection-over-union measure        Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
                    in neural networks. In CVPR, 2018. 13                              Mostafa Dehghani, Matthias Minderer, Georg Heigold,
                [5] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,           Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-
                    Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,               age is worth 16x16 words: Transformers for image recog-
                    Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-            nition at scale. ICLR, 2021. 5
                    modal dataset for autonomous driving. In CVPR, 2020. 7,       [22] Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang,
                    8, 16                                                              Hang Zhao, Feng Wang, Naiyan Wang, and Zhaoxiang
                                                                   ´  ´                Zhang. Embracing single stride 3d object detector with
                [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,
                    Julien Mairal, Piotr Bojanowski, and Armand Joulin.                sparse transformer. In CVPR, 2022. 8
                    Emergingpropertiesinself-supervised vision transformers.      [23] WhyeKitFong,RohitMohan,JuanaValeriaHurtado,Lub-
                    In CVPR, 2021. 2                                                   ingZhou,HolgerCaesar,OscarBeijbom,andAbhinavVal-
                [7] WanliChen,XingeZhu,GuojinChen,andBeiYu.Efficient                   ada. Panoptic nuscenes: A large-scale benchmark for lidar
                    point cloud analysis using hilbert curve. In ECCV, 2022. 2         panoptic segmentation and tracking. RA-L, 2022. 8
                [8] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.         [24] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min
                    Multi-view 3d object detection network for autonomous              Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy
                    driving. In CVPR, 2017. 2                                          Liptchinsky, Ishan Misra, Armand Joulin, et al.    Self-
                [9] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi,              supervised pretraining of visual features in the wild.
                    and Jiaya Jia. Largekernel3d: Scaling up kernels in 3d             arXiv:2103.01988, 2021. 1
                    sparse cnns. In CVPR, 2023. 15                                [25] Benjamin Graham, Martin Engelcke, and Laurens van der
               [10] Ran Cheng, Ryan Razani, Ehsan Taghavi, Enxu Li, and                Maaten. 3dsemanticsegmentationwithsubmanifoldsparse
                    Bingbing Liu. (af)2-s3net: Attentive feature fusion with           convolutional networks. In CVPR, 2018. 2, 3, 15
                    adaptive feature selection for sparse semantic segmentation   [26] ChaoMaGuangshengShi,RuifengLi. Pillarnet: Real-time
                    network. In CVPR, 2021. 7                                          and high-performance pillar-based 3d object detection. In
               [11] Hung-Yueh Chiang, Yen-Liang Lin, Yueh-Cheng Liu, and               ECCV,2022. 8
                    Winston H Hsu. A unified point-based framework for 3d         [27] Meng-HaoGuo,Jun-XiongCai,Zheng-NingLiu,Tai-Jiang
                    segmentation. In 3DV, 2019. 15                                     Mu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud
               [12] RewonChild,ScottGray,AlecRadford,andIlyaSutskever.                 transformer. Computational Visual Media, 2021. 1, 2
                    Generating long sequences with sparse transformers.           [28] ChenhangHe,RuihuangLi,ShuaiLi,andLeiZhang.Voxel
                    arXiv:1904.10509, 2019. 6, 15                                      set transformer: Aset-to-setapproachto3dobjectdetection
                                                                                       from point clouds. In CVPR, 2022. 8
               [13] Christopher Choy, JunYoung Gwak, and Silvio Savarese.                                             ¨
                                                                                  [29] DavidHilbertandDavidHilbert. Uberdiestetigeabbildung
                    4D spatio-temporal convnets: Minkowski convolutional                                   ¨      ¨
                                                                                       einer linie auf ein flachenstuck. Dritter Band: Analysis·
                    neural networks. In CVPR, 2019. 2, 3, 7, 8, 15, 16                 Grundlagen der Mathematik· Physik Verschiedenes: Nebst
               [14] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang,                  Einer Lebensgeschichte, 1935. 4
                    Xiaolin Wei, Huaxia Xia, and Chunhua Shen.        Con-        [30] Ji Hou, Benjamin Graham, Matthias Nießner, and Saining
                    ditional positional encodings for vision transformers.             Xie. Exploring data-efficient 3d scene understanding with
                    arXiv:2102.10882, 2021. 6                                          contrastive scene contexts. In CVPR, 2021. 2, 7, 8, 14, 15
               [15] Pointcept Contributors. Pointcept: A codebase for point       [31] Yuenan Hou, Xinge Zhu, Yuexin Ma, Chen Change Loy,
                    cloud perception research. https://github.com/                     and Yikang Li. Point-to-voxel knowledge distillation for
                    Pointcept/Pointcept,2023. 13                                       lidar semantic segmentation. In CVPR, 2022. 16
                                                                              4848
                [32] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan                [48] Youquan Liu, Lingdong Kong, Xiaoyang Wu, Runnan
                      Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham.                      Chen,XinLi,LiangPan,ZiweiLiu,andYuexinMa. Multi-
                      Randla-net: Efficient semantic segmentation of large-scale               space alignments towards universal lidar segmentation. In
                      point clouds. In CVPR, 2020. 15                                          CVPR,2024. 16
                [33] Zeyu Hu, Mingmin Zhen, Xuyang Bai, Hongbo Fu, and                    [49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
                      Chiew-lan Tai. Jsenet: Joint semantic segmentation and                   Zhang, Stephen Lin, and Baining Guo. Swin transformer:
                      edgedetectionnetworkfor3dpointclouds. InECCV,2020.                       Hierarchical vision transformer using shifted windows. In
                      15                                                                       ICCV,2021. 5
                [34] Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-              [50] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
                      WingFu,andJiayaJia. Hierarchical point-edge interaction                  Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,
                      network for point cloud semantic segmentation. In ICCV,                  et al. Swin transformer v2: Scaling up capacity and resolu-
                      2019. 15                                                                 tion. In CVPR, 2022. 5
                [35] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-              [51] Zhijian Liu, Xinyu Yang, Haotian Tang, Shang Yang, and
                      Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point group-                Song Han. Flatformer: Flattened window attention for ef-
                      ing for 3d instance segmentation. CVPR, 2020. 7, 8, 13,                  ficient point cloud transformer. In CVPR, 2023. 2, 3, 5, 8,
                      14                                                                       14
                [36] Li Jiang, Zetong Yang, Shaoshuai Shi, Vladislav Golyanik,            [52] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu.
                      Dengxin Dai, and Bernt Schiele.        Self-supervised pre-              Rethinking network design and local geometry in point
                      training with masked shape prediction for 3d scene under-                cloud: A simple residual mlp framework. In ICLR, 2022. 2
                      standing. In CVPR, 2023. 2                                          [53] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-
                [37] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B                         volutional neural network for real-time object recognition.
                      Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec                     In IROS, 2015. 2
                      Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for             [54] Guy M Morton. A computer oriented geodetic data base
                      neural language models. arXiv:2001.08361, 2020. 1                        and a new technique in file sequencing. International Busi-
                [38] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi                      ness Machines Company New York, 1966. 4
                      Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer             [55] GakuNarita,TakashiSeno,TomoyaIshikawa,andYohsuke
                                                                                ´
                      Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar,                  Kaji. Panopticfusion: Online volumetric semantic mapping
                      and Ross Girshick. Segment anything. In ICCV, 2023. 1                    at the level of stuff and things. In IROS, 2019. 15
                [39] Lingdong Kong, Youquan Liu, Runnan Chen, Yuexin Ma,                  [56] OpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023.
                      Xinge Zhu, Yikang Li, Yuenan Hou, Yu Qiao, and Ziwei                     1
                      Liu. Rethinking range view representation for lidar seg-            [57] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu,
                      mentation. In ICCV, 2023. 16                                             Yonghong Tian, and Li Yuan. Masked autoencoders for
                [40] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang                    point cloud self-supervised learning. In ECCV, 2022. 2
                      Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified trans-        [58] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jaesik
                      former for 3d point cloud segmentation. In CVPR, 2022. 2,                Park. Fast point transformer. In CVPR, 2022. 15
                      3, 6, 7, 15                                                         [59] Giuseppe Peano and G Peano. Sur une courbe, qui remplit
                [41] Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya                   toute une aire plane. Springer, 1990. 4
                      Jia. Spherical transformer for lidar-based 3d recognition. In       [60] Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Heng-
                      CVPR,2023. 7, 16                                                         shuangZhao,ZhuotaoTian,andJiayaJia. Oa-cnns: Omni-
                [42] Loic Landrieu and Martin Simonovsky. Large-scale point                    adaptive sparse cnns for 3d semantic segmentation.        In
                      cloud semantic segmentation with superpoint graphs. In                   CVPR,2024. 15, 16
                      CVPR,2018. 15                                                       [61] Gilles Puy, Alexandre Boulch, and Renaud Marlet. Using a
                [43] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,                    waffle iron for automotive point cloud semantic segmenta-
                      JiongYang,andOscarBeijbom. Pointpillars: Fastencoders                    tion. In ICCV, 2023. 16
                      for object detection from point clouds. In CVPR, 2019. 2,           [62] CharlesRQi,HaoSu,KaichunMo,andLeonidasJGuibas.
                      8                                                                        Pointnet: Deep learning on point sets for 3d classification
                [44] Huan Lei, Naveed Akhtar, and Ajmal Mian. Seggcn: Effi-                    and segmentation. In CVPR, 2017. 1, 2, 7, 15
                      cient 3d point cloud segmentation with fuzzy spherical ker-         [63] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-
                      nel. In CVPR, 2020. 15                                                   net++: Deep hierarchical feature learning on point sets in a
                [45] BoLi,Tianlei Zhang, and Tian Xia. Vehicle detection from                  metric space. In NeurIPS, 2017. 2, 15, 16
                      3dlidar using fully convolutional network. In RSS, 2016. 2          [64] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai,
                [46] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan                          Hasan Hammoud, Mohamed Elhoseiny, and Bernard
                      Di, and Baoquan Chen.       Pointcnn: Convolution on x-                  Ghanem. Pointnext: Revisiting pointnet++ with improved
                      transformed points. In NeurIPS, 2018. 1, 15                              training and scaling strategies. In NeurIPS, 2022. 7, 15
                [47] Haojia Lin, Xiawu Zheng, Lijiang Li, Fei Chao, Shanshan              [65] DamienRobert,HugoRaguet,andLoicLandrieu. Efficient
                      Wang, Yan Wang, Yonghong Tian, and Rongrong Ji. Meta                     3d semantic segmentation with superpoint transformer. In
                      architecture for point cloud analysis. In CVPR, 2023. 15                 ICCV,2023. 2, 15
                                                                                     4849
                [66] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-            [82] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang,
                     net: Convolutional networks for biomedical image segmen-                and Jie Shan. Graph attention convolution for point cloud
                     tation. In MICCAI, 2015. 6                                              semantic segmentation. In CVPR, 2019. 15
                [67] David Rozenberszki,      Or Litany,    and Angela Dai.            [83] Peng-Shuai Wang. Octformer: Octree-based transformers
                     Language-grounded indoor 3d semantic segmentation in                    for 3D point clouds. In SIGGRAPH, 2023. 2, 3, 5, 6, 7, 8,
                     the wild. In ECCV, 2022. 7, 8                                           14, 15
                [68] Aditya Sanghi.    Info3d: Representation learning on 3d           [84] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,
                     objects using mutual information maximization and con-                  and Xin Tong. O-CNN: Octree-based convolutional neural
                     trastive learning. In ECCV, 2020. 2                                     networks for 3D shape analysis. In SIGGRAPH, 2017. 2, 6
                [69] Jonathan Sauder and Bjarne Sievers. Self-supervised deep          [85] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei
                     learning on point clouds by reconstructing space.        In             Pokrovsky, and Raquel Urtasun. Deep parametric contin-
                     NeurIPS, 2019. 2                                                        uous convolutional neural networks. In CVPR, 2018. 15
                [70] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang,                 [86] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and
                     Manolis Savva, and Thomas Funkhouser. Semantic scene                    TiejunHuang. Imagesspeakinimages: Ageneralistpainter
                     completion from a single depth image. In CVPR, 2017. 2                  for in-context visual learning. In CVPR, 2023. 1
                [71] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and               [87] Yue Wang and Justin M Solomon.         Deep closest point:
                     Erik G. Learned-Miller. Multi-view convolutional neural                 Learning representations for point cloud registration.  In
                     networks for 3d shape recognition. In ICCV, 2015. 2                     ICCV,2019. 2
                [72] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aure-              [88] Wenxuan Wu, Zhongang Qi, and Li Fuxin.          Pointconv:
                     lien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin               Deepconvolutional networks on 3d point clouds. In CVPR,
                     Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in                2019. 1, 15
                     perception for autonomous driving: Waymo open dataset.            [89] Wenxuan Wu, Li Fuxin, and Qi Shan. Pointconvformer:
                     In CVPR, 2020. 7, 8                                                     Revenge of the point-based convolution. In CVPR, 2023.
                [73] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji                  15
                     Lin, HanruiWang,andSongHan. Searchingefficient3dar-               [90] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Heng-
                     chitectures with sparse point-voxel convolution. In ECCV,               shuang Zhao. Point transformer v2: Grouped vector atten-
                     2020. 7, 16                                                             tion and partition-based pooling. In NeurIPS, 2022. 1, 2, 3,
                [74] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and                     5, 6, 7, 8, 15, 16
                     Qian-Yi Zhou. Tangent convolutions for dense prediction           [91] XiaoyangWu,XinWen,XihuiLiu,andHengshuangZhao.
                     in 3d. In CVPR, 2018. 15                                                Maskedscene contrast: A scalable framework for unsuper-
                [75] Lyne Tchapmi, Christopher Choy, Iro Armeni, JunYoung                    vised 3d representation learning. In CVPR, 2023. 2, 14,
                     Gwak,andSilvioSavarese. Segcloud: Semantic segmenta-                    15
                     tion of 3d point clouds. In 3DV, 2017. 15, 16                     [92] Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui
                [76] OpenPCDet Development Team.              Openpcdet:     An              Liu, Kaicheng Yu, and Hengshuang Zhao. Towards large-
                     open-source toolbox for 3d object detection from point                  scale 3d representation learning with multi-dataset point
                     clouds.    https://github.com/open-mmlab/                               prompt training. In CVPR, 2024. 1, 2, 3, 7, 8, 13, 14,
                     OpenPCDet,2020. 14                                                      15, 16
                [77] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,              [93] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han,
                     Beatriz Marcotegui, Franc¸ois Goulette, and Leonidas J                  andMikeLewis. Efficient streaming language models with
                     Guibas. Kpconv: Flexible and deformable convolution for                 attention sinks. arXiv, 2023. 13
                     point clouds. In ICCV, 2019. 2, 15                                [94] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas
                                                             ¨                               Guibas, and Or Litany. Pointcontrast: Unsupervised pre-
                [78] Yonglong Tian, Olivier J Henaff, and Aaron van den Oord.
                     Divide and contrast: Self-supervised learning from uncu-                training for 3d point cloud understanding. In ECCV, 2020.
                     rated data. In CVPR, 2021. 1                                            2, 14, 15
                [79] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier             [95] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin
                                                              ´                              Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei
                     Martinet, Marie-Anne Lachaux, Timothee Lacroix, Bap-
                               `                                                             Wang,andTieyanLiu. Onlayernormalizationinthetrans-
                     tiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar,
                     et al. Llama: Open and efficient foundation language mod-               former architecture. In ICML, 2020. 15
                     els. arXiv:2302.13971, 2023. 1                                    [96] Mutian Xu, Runyu Ding, Hengshuang Zhao, and Xiaojuan
                [80] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob                        Qi. Paconv: Position adaptive convolution with dynamic
                     Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,                   kernel assembling on point clouds. In CVPR, 2021. 15
                     andIllia Polosukhin. Attention is all you need. In NeurIPS,       [97] Xu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, and
                     2017. 5, 6, 15                                                          Shuguang Cui. Pointasnl: Robust point clouds processing
                [81] Chengyao Wang, Li Jiang, Xiaoyang Wu, Zhuotao Tian,                     using nonlocal neural networks with adaptive sampling. In
                     Bohao Peng, Hengshuang Zhao, and Jiaya Jia. Groupcon-                   CVPR,2020. 15
                     trast: Semantic-aware self-supervised representation learn-       [98] XuYan,JiantaoGao,ChaodaZheng,ChaoZheng,Ruimao
                     ing for 3d understanding. In CVPR, 2024. 15                             Zhang, Shuguang Cui, and Zhen Li. 2dpass: 2d priors
                                                                                   4850
                  assisted semantic segmentation on lidar point clouds. In
                  ECCV,2022. 7, 16
             [99] Yan Yan, Yuxing Mao, and Bo Li.  Second: Sparsely
                  embedded convolutional detection. Sensors, 18(10):3337,
                  2018. 8
            [100] Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li,
                  Jinxian Liu, Mengdie Zhou, and Qi Tian. Modeling point
                  clouds with self-attention and gumbel subset sampling. In
                  CVPR,2019. 15
            [101] Yu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu,
                  Hao Pan, Peng-Shuai Wang, Xin Tong, and Baining Guo.
                  Swin3d: A pretrained transformer backbone for 3d indoor
                  scene understanding. arXiv:2304.06906, 2023. 2, 3, 6, 7,
                  8, 15
                                                           ¨    ¨
            [102] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl.
                  Center-based 3d object detection and tracking. In CVPR,
                  2021. 8, 13
            [103] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie
                  Zhou, and Jiwen Lu. Point-BERT: Pre-training 3D point
                  cloud transformers with masked point modeling. In CVPR,
                  2022. 2
            [104] Feihu Zhang, Jin Fang, Benjamin Wah, and Philip Torr.
                  Deep fusionnet for point cloud semantic segmentation. In
                  ECCV,2020. 15
            [105] Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia.
                  Pointweb: Enhancinglocalneighborhoodfeaturesforpoint
                  cloud processing. In CVPR, 2019. 2, 15
            [106] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and
                  Vladlen Koltun. Point transformer. In ICCV, 2021. 1, 2, 3,
                  5, 6, 7, 15, 16
            [107] Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha
                  Zhang, Xianglong He, Tong He, Hengshuang Zhao, Chun-
                  hua Shen, Yu Qiao, et al. Ponderv2: Pave the way for 3d
                  foundataion model with a universal pre-training paradigm.
                  arXiv:2310.08586, 2023. 2, 3
            [108] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin
                  Ma,WeiLi,HongshengLi,andDahuaLin. Cylindricaland
                  asymmetrical 3d convolution networks for lidar segmenta-
                  tion. In CVPR, 2021. 7, 16
                                                                     4851
