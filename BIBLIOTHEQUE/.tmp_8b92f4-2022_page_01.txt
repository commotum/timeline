                 Under review as a conference paper at ICLR 2025
            000  TACKLING THE ABSTRACTION AND REASONING COR-
            001
            002  PUS WITH VISION TRANSFORMERS: THE IMPORTANCE
            003  OF 2D REPRESENTATION, POSITIONS, AND OBJECTS
            004
            005
            006   Anonymousauthors
            007   Paper under double-blind review
            008
            009
            010
            011                            ABSTRACT
            012
            013       The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused
            014       onvisualreasoningintheevaluationofArtificial Intelligence systems. In its orig-
            015       inal framing, an ARC task requires solving a program synthesis problem over
            016       small 2D images using a few input-output training pairs. In this work, we adopt
            017       the recently popular data-driven approach to the ARC and ask whether a Vision
            018       Transformer (ViT) can learn the implicit mapping, from input image to output
            019       image, that underlies the task. We show that a ViT—otherwise a state-of-the-art
                      model for images—fails dramatically on most ARC tasks even when trained on
            020       one million examples per task. This points to an inherent representational defi-
            021       ciency of the ViT architecture that makes it incapable of uncovering the simple
            022       structured mappings underlying the ARC tasks. Building on these insights, we
            023       propose VITARC,aViT-stylearchitecturethatunlockssomeofthevisualreason-
            024       ing capabilities required by the ARC. Specifically, we use a pixel-level input rep-
            025       resentation, design a spatially-aware tokenization scheme, and introduce a novel
            026       object-based positional encoding that leverages automatic segmentation, among
            027       other enhancements. Our task-specific VITARC models achieve a test solve rate
            028       close to 100% on more than half of the 400 public ARC tasks strictly through su-
            029       pervised learning from input-output grids. This calls attention to the importance
            030       ofimbuingthepowerful(Vision)Transformerwiththecorrectinductivebiasesfor
                      abstract visual reasoning that are critical even when the training data is plentiful
            031       and the mapping is noise-free. Hence, VITARC provides a strong foundation for
            032       future research in visual reasoning using transformer-based architectures.
            033
            034  1  INTRODUCTION
            035
            036  Developing systems that are capable of performing abstract reasoning has been a long-standing
            037  challenge in Artificial Intelligence (AI). Abstract Visual Reasoning (AVR) tasks require AI models
            038  to discern patterns and underlying rules within visual content, offering a rigorous test for evalu-
            039  ating AI systems. Unlike other visual reasoning benchmarks such as Visual Question Answering
            040  (VQA) (Antol et al., 2015) and Visual Commonsense Reasoning (VCR) (Kahou et al., 2018) that
            041  rely on natural language inputs or knowledge of real-world physical properties, AVR tasks do not
            042  include any text or background knowledge. Instead, they focus purely on visual abstraction and
            043                  ´     ´
                 pattern recognition (Małkinski & Mandziuk, 2023). One prominent example of AVR is the Abstrac-
            044  tion and Reasoning Corpus (ARC) (Chollet, 2019), which is designed to evaluate an AI’s capacity
            045  for generalization in abstract reasoning. Each ARC task involves transforming input grids into out-
            046  put grids by identifying a hidden mapping often requiring significant reasoning beyond mere pattern
            047  matching(cf. Figure 2). While the ARC’s original setting is one of few-shot learning, there has been
            048  recent interest in studying the ARC in a data-rich setting where task-specific input-output samples
            049  can be generated (Hodel, 2024), allowing for the evaluation of deep learning-based solutions.
            050  In this paper, we explore the potential of vision transformers to solve ARC tasks using supervised
            051  learning. We assess how well transformers can learn complex mappings for a single task when pro-
            052  vided with sufficient training data. Our exploration highlights fundamental representational limita-
            053  tions of vision transformers on the ARC, leading to three high-level findings that we believe provide
                 a strong foundation for future research in visual reasoning using transformer-based architectures:
                                               1
