<html> 
 <head>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">


<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Least-Squares Policy Iteration">

  <meta name="citation_author" content="Lagoudakis, Michail G.">

  <meta name="citation_author" content="Parr, Ronald">

<meta name="citation_publication_date" content="2003">
<meta name="citation_journal_title" content="Journal of Machine Learning Research">
<meta name="citation_issn" content="ISSN 1533-7928">
<meta name="citation_volume" content="4">
<meta name="citation_issue" content="Dec">
<meta name="citation_firstpage" content="1107">
<meta name="citation_lastpage" content="1149">
<meta name="citation_pdf_url" content="http://www.jmlr.org/papers/volume4/lagoudakis03a/lagoudakis03a.pdf">
 
  <!--#include virtual="/css-scroll.txt"-->
<style>. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
 </style>
<body>
 <div id="content">

<h2>Least-Squares Policy Iteration</h2>
<p>
<b>
<i>Michail G. Lagoudakis, Ronald Parr</i></b>; 4(Dec):1107-1149, 2003.</p>
</p>
<h3>Abstract</h3>We propose a new approach to reinforcement learning for control
problems which combines value-function approximation with linear
architectures and approximate policy iteration. This new approach is
motivated by the least-squares temporal-difference learning algorithm
(LSTD) for prediction problems, which is known for its efficient use
of sample experiences compared to pure temporal-difference
algorithms. Heretofore, LSTD has not had a straightforward application
to control problems mainly because LSTD learns the state value
function of a fixed policy which cannot be used for action selection
and control without a model of the underlying process.  Our new
algorithm, least-squares policy iteration (LSPI), learns the
state-action value function which allows for action selection without
a model and for incremental policy improvement within a
policy-iteration framework. LSPI is a model-free, off-policy method
which can use efficiently (and reuse in each iteration) sample
experiences collected in any manner. By separating the
sample collection method, the choice of the linear approximation
architecture, and the solution method, LSPI allows for focused
attention on the distinct elements that contribute to practical
reinforcement learning.  LSPI is tested on the simple task of
balancing an inverted pendulum and the harder task of balancing and
riding a bicycle to a target location. In both cases, LSPI learns to
control the pendulum or the bicycle by merely observing a relatively
small number of trials where actions are selected randomly. LSPI is
also compared against <i>Q</i>-learning (both with and without experience
replay) using the same value function architecture.  While LSPI
achieves good performance fairly consistently on the difficult bicycle
task, <i>Q</i>-learning variants were rarely able to balance for more than
a small fraction of the time needed to reach the target location.
<font color="gray"><p>[abs]</font>[<a href="http://www.jmlr.org/papers/volume4/lagoudakis03a/lagoudakis03a.pdf" target="_blank">pdf</a>][<a href="http://www.jmlr.org/papers/volume4/lagoudakis03a/lagoudakis03a.ps.gz" target="_blank">ps.gz</a>][<a href="http://www.jmlr.org/papers/volume4/lagoudakis03a/lagoudakis03a.ps" target="_blank">ps</a>]</div> 
 <!--#include virtual="/nav-bar.txt"--> 
  </body>
 </html>
