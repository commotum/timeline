                     Published as a conference paper at ICLR 2022
                                           2.8
                                                           Transformer
                                           2.6             Memorizing Transformer
                                           2.4
                                           2.2
                                          Perplexity (arXiv Math)2.0
                                           1.8200M        1B             8B
                                                         Model size
                        Figure 1: Adding a memory of 8K tokens improves perplexity across different model sizes.
                     our technique, we are easily able to scale external memory up to sequence lengths of 131k or 262k
                     tokens on a single TPU device, while maintaining a reasonable step time.
                     Weshowthatmodelperplexitysteadily improves with the size of external memory on a variety of
                     language modelling tasks, including C4 (long documents only), Github code repositories, PG-19
                     books, formal proofs in Isabelle, and arXiv math papers. We further show that models can generalize
                     to larger memory sizes than they were trained on: models trained with a small memory show gains
                     from using a much larger memory at inference time. Finally, we show that our models are actually
                     using memoryinthewaythatwehadhoped,e.g.bylookingupthedeﬁnitionsoflemmasinatheorem
                     proving corpus.
                     The simplicity of the changes to the Transformer architecture allows us to easily integrate this
                     approach into existing code bases, including extremely large language models. We further show that
                     the improvements to quality are maintained across models of increasing size, and that the model
                     improvements gained from adding memory are even larger than increasing the size of the model by
                     5XormoreasshowninFigure1.
                     2  RELATED WORK
                     Agreatdealofworkhasbeendoneonefﬁcientlong-rangeattentionmechanisms;seeTayetal.(2020;
                     2021) recent surveys. Sliding windows (Beltagy et al., 2020) use a long sequence, but attend within
                     a smaller window, thus reducing complexity to the window size, rather than total sequence length.
                     Approximate mechanisms such as Linformer (Wang et al., 2020b), and Performer (Choromanski
                     et al., 2021) refactor the attention matrix by using a different kernel than softmax to obtain O(N)
                     complexity. Poolingstrategies such as Hierarchical 1D attention (Zhu & Soricut, 2021), and Combiner
                     (Renetal., 2021) apply pooling or averaging over tokens at longer distances. Sparse strategies such as
                     Big Bird (Zaheer et al., 2020) select only a subset of tokens to attend to; Routing Transformers (Roy
                     et al., 2021) use clustering to select the subset, while Reformer (Kitaev et al., 2020) relies on hashing.
                     Hierarchical mechanisms (Ainslie et al., 2020) combine multiple tokens into phrases or sentences to
                     reduce sequence length. Expire-span (Sukhbaatar et al., 2021) prunes far-away tokens that it learns
                     are “unimportant”. (Zemlyanskiy et al., 2021) process long sequences in two passes with different
                     encoders. The second pass is given a lot of context by accessing summaries of the ﬁrst pass.
                     Feedback transformers (Fan et al., 2020) use a recurrent architecture in which each token attends to
                     the output of the ﬁnal layer instead of the previous layer. Recurrence does not increase the size of the
                     attention context itself, but it expands the receptive ﬁeld at the cost of parallelism and training speed.
                     Truncated backpropagation through time (Williams & Peng, 1990) was originally introduced as a
                     wayoftraining recurrent neural networks (RNN) over very long sequences, when the entire sequence
                     does not ﬁt in memory. The sequence is chopped into segments, and after each training step, the ﬁnal
                     RNNstateforthesegmentissavedinanon-differentiable cache, and used as the initial state on the
                     next training step. Neural caches (Grave et al., 2017) extend the cache to contain a record of many
                     prior hidden states, and attend over them. Transformer-XL (Dai et al., 2019) applies this technique to
                     transformers; it caches the (key,value) pairs computed from the previous training step, and uses them
                     as a preﬁx for the tokens on the next training step, which yields signiﬁcant gains on long documents.
                     Raeet al. (2020) improve over Transformer-XL by compressing the tokens before adding them to the
                                                          2
