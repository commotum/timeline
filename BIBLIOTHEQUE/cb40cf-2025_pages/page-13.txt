                                 A PerformanceofShortestMajorityVote                                                                                    R1-671b          R1-Distill-32b         R1-Distill-14b         QwQ          LIMO
                                                                                                                                             0.28      AIME Solution Distribution                           AIME Token Distribution
                                          onMoreModels.                                                                                      0.26                                                0.36
                                                                                                                                             0.24                                                0.33
                                                                                                                                             0.22                                                0.30
                                                                                                                                                                                                 0.27
                                 To demonstrate the applicability of shortest ma-                                                            0.20                                                0.24
                                                                                                                                             0.18                                               ercentage0.21
                                 jority voting across a wider range of models, we                                                            0.16                                               P0.18
                                                                                                                                             0.14                                                0.15
                                 have included additional evaluation results for both                                                        0.12                                                0.12
                                                                                                                                                   1        2         3        4         5             1        2        3         4        5
                                 o1-like models (s1 (Muennighoff et al., 2025)                                                                                      Group                                              Group
                                                                                                                                                       GPQA Solution Distribution                           GPQA Token Distribution
                                 and Open-Reasoner-Zero (Wang et al., 2024)) and                                                             0.28                                                0.33
                                                                                                                                             0.26                                                0.30
                                 instruction-based models (Deepseek-v3 (Liu et al.,                                                          0.24                                                0.27
                                                                                                                                             0.22                                                0.24
                                 2024) and Qwen-2.5-72b-instruct (Team, 2024a)),                                                             0.20                                                0.21
                                                                                                                                                                                                ercentage0.18
                                 as shown in Tables 5. We found that Shortest Ma-                                                            0.18                                               P0.15
                                                                                                                                             0.16                                                0.12
                                 jority Vote also performs excellently on these mod-                                                         0.14                                                0.09
                                                                                                                                                   1        2         3        4         5             1        2        3         4        5
                                 els, especially with s1 and Open-Reasoner-Zero.                                                                                    Group                                              Group
                                 Although Qwen-2.5-72b and Deepseek-v3 do not                                                               Figure 8: The number of correct solutions and tokens
                                 generate long chain-of-thought reasoning, Short-                                                           distributed across groups of different lengths.
                                 est Majority Vote still significantly improves their
                                 performance on AIME.
                                 B IsInvalidScaling PhenomenonConflict                                                                      C FurtheranalysisonSequentialScaling
                                          to Findings of R1 technique Report?                                                                         onR1-Distill-14b, R1-Distill-32b and
                                                                                                                                                      LIMO
                                 ThetrainingobjectiveofR1aimstoimprovemodel
                                 accuracy, yet we observe that correct solutions tend                                                       InSection4.2,weobservedthatR1-Distill-14b,R1-
                                 to be shorter than incorrect ones. This raises an                                                          Distill-32b and LIMO demonstrated some perfor-
                                 intriguing question: Why does R1â€™s reinforcement                                                           manceimprovements after multiple rounds of self-
                                 learning (RL) training consistently produce longer                                                         revision, followed by stabilization. Furthermore, in
                                 solutions?                                                                                                 Section4.1, wefoundthatthecorrectsolutionsgen-
                                      To investigate this phenomenon, we analyzed                                                           erated by R1-Distill-14b, R1-Distill-32b and LIMO
                                 five solutions per question, organizing them into                                                          were generally shorter than incorrect solutions. To
                                 groups by length in ascending order. Figure 8 il-                                                          reconcile these seemingly contradictory findings
                                 lustrates the distribution of correct solutions across                                                     and further analyze how R1-Distill-14b, R1-Distill-
                                 these groups.                                                                                              32bandLIMObenefitfromself-revision, we con-
                                      Ouranalysis revealed that correct solutions pre-                                                      ductedadetailedanalysisofself-revisionoutcomes
                                 dominantly appear in shorter-length groups, par-                                                           on both long and short solutions. Our methodol-
                                 ticularly in the AIME dataset. However, when                                                               ogyforcollecting long and short solutions involved
                                 examining the token distribution, we found that                                                            sampling five solutions for each question, ordering
                                 correct solution tokens are concentrated in longer-                                                        them by length, and then segregating the longest
                                 solution groups. This apparent contradiction arises                                                        and shortest solutions into separate groups. The
                                 because the total token count is determined by both                                                        results of self-revision on both short and long so-
                                 the number of solutions and the average tokens per                                                         lutions are presented in Figure 9. Our analysis
                                 solution. As shown in Figure 2a, solutions in the                                                          reveals that short solutions exhibited significant
                                 longest group contain nearly twice as many tokens                                                          performance improvements following self-revision,
                                 as those in the shortest group. This explains why,                                                         while this trend was less pronounced for long so-
                                 despite having fewer individual solutions, longer                                                          lutions. Therefore, the performance improvements
                                 solutions account for a greater share of the total                                                         we observed through self-revision in R1-Distill-
                                 tokens.                                                                                                    14b, R1-Distill-32b and LIMO primarily stem from
                                      Wehypothesize that this discrepancy explains                                                          the self-revision on short solutions. This suggests
                                 whyRLtrainingtends to produce longer solutions:                                                            that the relationship between accuracy and solution
                                 the training process may favor generating longer                                                           length for these models is complex, demonstrating
                                 solutions, even if they are less accurate, because                                                         neither a strictly positive nor negative correlation
                                 they contribute more tokens to the gradient.                                                               with length.
                                                                                                                                    4663
