                  found that the average length of correct solutions is    Vote substantially outperforms conventional Ma-
                  shorterthanthatofincorrectonesforthesameques-            jority Vote, significantly improving the test-time
                  tions, which is shown in Figure 1. This counterin-       scalability of both QwQ and R1 models.
                  tuitive finding underscores the need for a deeper           Ourcontributions are as follows:
                  understanding of the test-time scaling of o1-like
                  models.                                                    1) We systematically investigate the test-time
                    Tounderstand why the longer CoTs do not lead                scaling capabilities of o1-like models QwQ,
                  to the better performance, we compared the differ-            R1andLIMO,andfindthattheirperformance
                  ence between long CoTs and short CoTs, finding                can not be continuously improved through in-
                  that long CoTs contain more self-revisions (“Wait”,           creasing CoT length.
                 “Alternatively”) than the short CoTs, which is              2) Wereveal that insufficient self-revision capa-
                  shown in Appendix F. Inspired by that, we itera-              bility of o1-like models is the primary reason
                  tively prompted QwQ, R1 and LIMO for more self-               for their failure in sequential scaling.
                  revisions. Our observations revealed that QwQ and          3) We find that parallel scaling achieves better
                  R1-Distill-1.5b exhibited performance degradation             coverage and scalability than sequential revi-
                  as the length of reflection increased. In contrast,           sion for o1-like models.
                  R1-Distill-14b, R1-Distill-32b, and LIMO demon-            4) Based on our insights into sequential and
                  strated initial performance improvements during               parallel scaling, we propose Shortest Major-
                  early revisions, followed by oscillatory behavior             ity Vote, a test-time scaling method that en-
                  in subsequent iterations. To further understand               hances majority voting by considering solu-
                  the limitations of sequential scaling, we evaluated           tion length, significantly outperforming tradi-
                  the models’ capacity to revise incorrect answers.             tional methods.
                  Ourfindings indicate that QwQ, R1 and LIMO all           2 RelatedWork
                  demonstrated limited ability to convert incorrect
                  answers to correct ones during the revision pro-         The success of o1 has ushered in a new scaling
                  cess. Most revisions retained the original answers,      paradigm, test-time compute scaling, which en-
                  and more concerning, both QwQ and R1-Distill-            ables continuous improvements in model perfor-
                  1.5b showed a higher propensity to change correct        mance by increasing computational expenditure
                  answers to incorrect ones rather than vice versa.        during inference (OpenAI, 2024a,b). Currently,
                  These results reveal that self-revision ability is       scaling test-time compute can be approached in
                  a key factor in the effectiveness of sequential          two dimensions: parallel scaling and sequential
                  scaling for o1-like models.                              scaling (Snell et al., 2024; Zeng et al., 2024).
                    Giventhelimitedeffectivenessofsequentialscal-
                  ing, we explored an alternative test-time scaling        Parallel Scaling     Parallel scaling typicallly sam-
                  strategie, parallel scaling. Our comparative analy-      ples multiple solutions in parallel and pick one
                  sis of sequential and parallel scaling revealed that     according to some guidence signal like reward. No-
                  parallel scaling not only achieves the better cover-     table examples of parallel scaling include Best-of-
                  age (pass@k score) but also offers superior scala-       N Search (Cobbe et al., 2021; Sun et al., 2024;
                  bility compared to sequential scaling for QwQ and        Gui et al., 2024; Amini et al., 2024; Sessa et al.,
                  R1, which demonstrates that o1-like models have          2024), which is based on a reward model (Cobbe
                  limited sequential-scaling capability, but strong        et al., 2021; Lightman et al., 2024), and Majority
                  parallel-scaling capability.                             Vote (Wang et al., 2023), which exploits model un-
                    Building on these findings, we propose a novel         certainty. The primary distinction between these
                  test-time scaling method, Shortest Majority Vote,        approaches lies in the method used to select the
                  which incorporate parallel scaling approaches with       final solution or answer after sampling multiple
                  our insight on sequential scaling. In particular, this   candidates. Both Best-of-N Search and Majority
                  method leverages the observation that shorter solu-      Vote are parallel scaling techniques at the solution
                  tions tend to lead to better performance compared        level, while Tree-Search algorithms can be viewed
                  to longer ones. Shortest Majority Vote improves          as parallel scaling at the token or step level. Beam-
                  majority vote by prioritizing clusters that have both    Search (Qiu et al., 2024; Yu et al., 2024; Xie et al.,
                  moresolutions and shorter solution lengths. Exper-       2023; Kool et al., 2019) and MCTS (Hao et al.,
                  imental results demonstrate that Shortest Majority       2023; Wan et al., 2024; Chen et al., 2024a; Zhang
                                                                       4652
