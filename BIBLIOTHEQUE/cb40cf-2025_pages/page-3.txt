                  et al., 2023) are classic examples of Tree-Search        3 ExperimentSetting
                  algorithms. All parallel scaling methods rely on         Models Ourexperimentsinvolvedmodelsfrom
                  guidance signals to select the optimal token, step,      the QwQ (Team, 2024b), LIMO(Ye et al., 2025)
                  or solution from a set of candidates.                    and Deepseek-R1 series (DeepSeek-AI et al.,
                                                                           2025), including Deepseek-R1, Deepseek-R1-
                  Sequential Scaling     Sequential scaling enhances       Distill-Qwen-32b,Deepseek-R1-Distill-Qwen-14b,
                  test-time computation by generating progressively        and Deepseek-R1-Distill-Qwen-1.5b. For simpl-
                  longer solutions along the sequence dimension.           icy, we call these R1 models as R1-671b, R1-
                  The most prevalent method of sequential scaling          Distill-32b, R1-Distill-14b and R1-Distill-1.5b re-
                  is Self-Revision, where Madaan et al. (2023) first       spectively. The models were run using SGLang
                  generate an initial response and then iteratively        framework (Zheng et al., 2024), with the sampling
                  evaluate and refine it based on self-assessment. In      temperature set to 0.7 and the maximum generation
                  contrast, Chen et al. (2024b); Gou et al. (2024)         length set to 32k. We show the system prompt and
                  leverage external feedback—such as signals from          instructions used for evaluation in Appendix E.
                  a code execution environment—rather than self-
                  evaluation to enhance solutions.                         Benchmark Weconductedcomprehensiveevalu-
                    Theeffectiveness of sequential scaling with self-      ations across four benchmarks: MATH-500 (Light-
                  revision remains a contentious issue. Huang et al.       man et al., 2024), AIME (AIMO, 2018), Omini-
                  (2024a); Kamoi et al. (2024) argue that models           MATH(Gaoetal.,2024),andGPQA(Reinetal.,
                  cannot achieve effective self-refinement without         2023).   While MATH-500, AIME, and Omini-
                  external feedback. Conversely, some researchers          MATHfocusonmathematical reasoning, GPQA
                  posit that evaluating a solution’s correctness is in-    encompasses broader scientific domains.          For
                  herently easier than generating a correct solution      AIMEevaluation, we utilized the AIMO validation
                  (Leike, 2022), suggesting that LLMs have the ca-         set, comprising 90 questions from AIME 22, 23,
                  pacity for self-evaluation. Kumar et al. (2024);         and24(AIMO,2018). Giventhecomputationalde-
                  Zhangetal. (2024) show that it is possible to teach      mandsofevaluating the full Omini-MATH dataset
                  LLM to self-refine through reinforcement learn-          (4.4K questions), we randomly sampled 500 ques-
                  ing or supervised fine-tuning. Chen et al. (2024c)       tions to maintain efficiency. For GPQA,wefocused
                  compared various test-time scaling algorithms and        on the diamond subset containing 198 questions.
                  found that when feedback accuracy exceeds 90%,           Toensure robust evaluation of answer correctness,
                  Self-Revision outperforms Best-of-N Search.             weemployedboththeOpenCompass(Contributors,
                                                                           2023) and Qwen Math (Yang et al., 2024) evalua-
                                                                           tors, considering an answer correct if validated by
                  o1-like Models      The release of o1 (OpenAI,           either evaluator.
                  2024a,b) has further underscored the significance
                  of sequential scaling, as o1’s CoT length is substan-    4 TheFailureofSequentialScaling
                  tially greater than that of conventional models. The     4.1   Invalid Scaling of CoT Length: Longer
                  research community has made significant efforts to             CoTsDonotImprovePerformance
                  reproduce the capabilities of o1 (Qin et al., 2024;
                  Huangetal., 2024b; Jiang et al., 2024; Min et al.,       To investigate whether the accuracy of QwQ, R1
                  2024; Muennighoff et al., 2025), with QwQ (Team,         and LIMO genuinely improves with increasing
                  2024b) and R1 (DeepSeek-AI et al., 2025) and             CoT length, we sampled each model five times
                  LIMO(Yeetal.,2025)emergingasthemostsuc-                  onthe same question and sorted the five solutions
                  cessful attempts. However, Our findings reveal that      by length in ascending order. We grouped the so-
                  for R1 and QwQ, extending solution length does           lutions based on their rank in this sorted list, with
                  not necessarily yield better performance due to the      the i-th ranked solutions forming a distinct group.
                  models’ limited self-revision capabilities. Parallel     For instance, all the longest solutions (rank 5) from
                  findings by Wang et al. (2025) attribute this phe-       different questions formed one group, while all the
                  nomenon to model underthinking, where models             shortest solutions (rank 1) formed another, result-
                  initially reach correct intermediate solutions but       ing in 5 comprehensive solution groups for analy-
                  subsequently deviate toward incorrect conclusions        sis.
                  during extended reasoning.                                 Wepresenttheaveragelengthsofthefivegroups
                                                                      4653
