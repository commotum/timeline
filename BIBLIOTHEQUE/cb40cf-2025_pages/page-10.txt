                  References                                                 Lin Gui, Cristina Gârbacea, and Victor Veitch. 2024.
                  AIMO. 2018.         Dataset card for aimo valida-             Bonbon alignment for large language models and
                     tion aime. https://huggingface.co/datasets/                the sweetness of best-of-n sampling.         CoRR,
                     AI-MO/aimo-validation-aime.                                abs/2406.00832.
                  Afra Amini, Tim Vieira, and Ryan Cotterell. 2024. Vari-    ShiboHao,YiGu,HaodiMa,JoshuaJiahuaHong,Zhen
                     ational best-of-n alignment. CoRR, abs/2407.06057.         Wang,DaisyZheWang,andZhitingHu.2023. Rea-
                                                                                soning with language model is planning with world
                  Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan.           model. In Proceedings of the 2023 Conference on
                     2024a. Alphamath almost zero: process supervision          Empirical Methods in Natural Language Process-
                     without process. CoRR, abs/2405.03553.                     ing, EMNLP 2023, Singapore, December 6-10, 2023,
                                                                                pages 8154–8173. Association for Computational
                  Xinyun Chen, Maxwell Lin, Nathanael Schärli, and              Linguistics.
                     DennyZhou.2024b. Teaching large language mod-           Jie   Huang,    Xinyun    Chen,     Swaroop    Mishra,
                     els to self-debug. In The Twelfth International Con-       Huaixiu Steven Zheng, Adams Wei Yu, Xiny-
                     ference on Learning Representations, ICLR 2024,            ing Song, and Denny Zhou. 2024a. Large language
                     Vienna, Austria, May 7-11, 2024. OpenReview.net.           models cannot self-correct reasoning yet. In The
                  Ziru Chen, Michael White, Raymond J. Mooney, Ali              Twelfth International Conference on Learning
                     Payani, Yu Su, and Huan Sun. 2024c. When is                Representations, ICLR 2024, Vienna, Austria, May
                     tree search useful for LLM planning? it depends            7-11, 2024. OpenReview.net.
                     on the discriminator. In Proceedings of the 62nd        Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu,
                     Annual Meeting of the Association for Computa-             Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin,
                     tional Linguistics (Volume 1: Long Papers), ACL            Weizhe Yuan, and Pengfei Liu. 2024b. O1 replica-
                     2024, Bangkok, Thailand, August 11-16, 2024, pages         tion journey – part 2: Surpassing o1-preview through
                     13659–13678. Association for Computational Lin-            simple distillation, big progress or bitter lesson?
                     guistics.                                                  Preprint, arXiv:2411.16489.
                  Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
                     Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias          Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen,
                     Plappert, Jerry Tworek, Jacob Hilton, Reiichiro            Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haox-
                     Nakano, Christopher Hesse, and John Schulman.              iang Sun, Jia Deng, Wayne Xin Zhao, and 1 oth-
                     2021. Training verifiers to solve math word prob-          ers. 2024. Technical report: Enhancing llm reason-
                     lems. CoRR, abs/2110.14168.                                ing with reward-guided tree search. arXiv preprint
                                                                                arXiv:2411.11694.
                  OpenCompass Contributors. 2023.         Opencompass:
                     A universal evaluation platform for foundation          Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han,
                     models.     https://github.com/open-compass/               and Rui Zhang. 2024. When can llms actually cor-
                     opencompass.                                               rect their own mistakes? A critical survey of self-
                                                                                correction of llms. CoRR, abs/2406.01297.
                  DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,
                     Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,        Wouter Kool, Herke van Hoof, and Max Welling. 2019.
                     Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,           Stochastic beams and where to find them: The
                     Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi-              gumbel-top-k trick for sampling sequences without
                     hong Shao, Zhuoshu Li, Ziyi Gao, and 181 others.           replacement. In Proceedings of the 36th Interna-
                     2025. Deepseek-r1: Incentivizing reasoning capa-           tional Conference on Machine Learning, ICML 2019,
                     bility in llms via reinforcement learning. Preprint,       9-15 June 2019, Long Beach, California, USA, vol-
                     arXiv:2501.12948.                                          ume 97 of Proceedings of Machine Learning Re-
                                                                                search, pages 3499–3508. PMLR.
                  Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo
                     Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang          AviralKumar,VincentZhuang,RishabhAgarwal,YiSu,
                     Chen, Runxin Xu, Zhengyang Tang, Benyou Wang,              John D. Co-Reyes, Avi Singh, Kate Baumli, Shariq
                     Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei              Iqbal, Colton Bishop, Rebecca Roelofs, Lei M.
                     Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu,             Zhang, Kay McKinney, Disha Shrivastava, Cosmin
                     and Baobao Chang. 2024. Omni-math: A univer-               Paduraru, George Tucker, Doina Precup, Feryal M. P.
                     sal olympiad level mathematic benchmark for large          Behbahani, and Aleksandra Faust. 2024. Training
                     language models. CoRR, abs/2410.07985.                     language models to self-correct via reinforcement
                  Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,            learning. CoRR, abs/2409.12917.
                     Yujiu Yang, Nan Duan, and Weizhu Chen. 2024.            Jan Leike. 2022. Why i’m excited about ai-assisted
                     CRITIC:large language models can self-correct with         humanfeedback.
                     tool-interactive critiquing.  In The Twelfth Inter-
                     national Conference on Learning Representations,        Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harri-
                     ICLR2024,Vienna, Austria, May 7-11, 2024. Open-            son Edwards, Bowen Baker, Teddy Lee, Jan Leike,
                     Review.net.                                                John Schulman, Ilya Sutskever, and Karl Cobbe.
                                                                         4660
