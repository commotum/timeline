                          using 1 step of gradient-ascent latent optimization. Figure 6 shows the training loss and the model’s
                          top-2 accuracy on the 400 tasks from the training set, using mean latents for inference (no latent
                          optimization). We note that training was stopped before convergence due to computational limits.
                                    Inference    Training (400 tasks)  Evaluation (400 tasks)   Test (100 tasks)
                                    Mean                24.83                   3.25                   -
                                    GA1                 33.21                   3.50                   -
                                    GA5                 38.25                   6.12                   -
                                    GA25                42.58                   5.87                   -
                                    GA100               44.58                   8.38                   -
                                    GA200               43.88                   9.88                 3.00
                                    GA300               46.13                   9.38                   -
                                    GA400               46.00                   9.50                   -
                          Table 3: Performance after 220k training steps (5 days on a TPU v3-8). Top-2 accuracy on the
                          different sets, results are given in percentages. The Test results correspond to the leaderboard score,
                          i.e. performance on the private test set. GA N stands for gradient ascent with N gradient steps, used
                          for latent optimization.
                          Results   Weanalyze scaling test-time latent optimization in table 3. For gradient ascent latent
                          optimization we used the Adam [Kingma and Ba, 2017] optimizer with β = β = 0.9 and a cosine
                                                                                                1     2
                          decay learning rate schedule starting at a learning rate of 1.0. Here as well, we observe performance
                          increasing as we scale the number of gradient ascent steps performed in the latent space at test time,
                          with convergence occurring around 400 steps. On the training dataset, gradient ascent nearly doubles
                          the top-2 accuracy from 24.83% to 46.13% with 300 gradient steps, representing over 184 distinct
                          programs generated with pixel-perfect accuracy. We show in fig. 11 a T-SNE visualization of the
                          learned latent space and examples of generation on the training set using the trained checkpoint.
                          Weobserveasurprisingly high generalization to the evaluation dataset, even though it is known to
                          include significantly more complex programs than the training set. The generalization performance
                          sees similar increases when scaling gradient steps with performance increasing more than 3x between
                          Mean and GA 200 inference. Since the ARC test set is private, we evaluated only the highest-
                          performing inference method according to the evaluation set and scored 3.00% on the test dataset
                          (leaderboard score). Given that there cannot be any data leakage from the evaluation dataset, this
                          suggests that the generalization gap between training and testing could be slightly higher than that of
                          training and evaluation. Importantly we highlight that figure 10 clearly shows that training is far from
                          convergence, therefore we can expect both higher training accuracy and likely generalization with
                          higher compute.
                          6    Conclusion
                          In this work, we introduce Latent Program Network (LPN), a novel approach to inductive program
                          synthesis that leverages continuous latent space representations to enable efficient search and test-
                          time adaptation in the space of latent programs. Unlike previous methods, LPN directly integrates
                          the ability for test-time adaptation into the architecture, rather than relying on extensive sampling
                          from the model. Our results demonstrate that, for program synthesis tasks over grids, LPN can use
                          test-time adaptation to boost performance significantly by refining representations in the latent space.
                          Specifically, we find that gradient-based search is the most efficient adaptation method for LPN, with
                          step size and number of steps being important hyper-parameters for performance. Additionally, this
                          adaptive approach enables LPN to generalize beyond its training distribution, which is particularly
                          challenging for methods lacking test-time adaptation. On the ARC-AGI challenge, we show that
                          LPNcangeneralize from the training set to the evaluation set without additional synthetic datasets,
                          relying solely on test-time adaptation. We believe this to be a scalable approach as an alternative to
                          methods that improve neural architecture performance simply by expanding the training set to reduce
                          the generalization gap required. Our findings also indicate that LPN scales effectively with increased
                          compute, suggesting that, with additional resources, LPN would achieve higher performance on the
                                                                         15
