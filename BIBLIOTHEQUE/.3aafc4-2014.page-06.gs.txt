                   Unfortunately, this conceptual beneﬁt comes at a huge computational price. Just as K (x) is
                uncomputable, so one can show that the sophistication measures are uncomputable as well. But
                with K(x), at least we can get better and better upper bounds, by ﬁnding smaller and smaller
                compressed representations for x. By contrast, even to approximate sophistication requires solving
                two coupled optimization problems: ﬁrstly over possible models S, and secondly over possible ways
                to specify x given S.
                   A second disadvantage of sophistication is that, while there are highly-sophisticated strings,
                the only known way to produce such a string (even probabilistically) is via a somewhat-exotic
                diagonalization argument. (By contrast, for “reasonable” choices of smoothing function f, one can
                easily generate x for which the apparent complexity H (f (x)) is large.)  Furthermore, this is not
                an accident, but an unavoidable consequence of sophistication’s generality.   To see this, consider
                any short probabilistic program P: for example, the coﬀee automaton that we will study in this
                paper, which has a simple initial state and a simple probabilistic evolution rule. Then we claim
                that with overwhelming probability, P’s output x must have low sophistication. For as the model
                S, one can take the set of all possible outputs y of P such that Pr[y] ≈ Pr[x]. This S takes only
                O(logn) bits to describe (plus O(1) bits for P itself), and clearly K (x|S) ≥ log2|S|−c with high
                probability over x.
                   For this reason, sophistication as deﬁned above seems irrelevant to the coﬀee cup or other
                physical systems: it simply never becomes large for such systems!   On the other hand, note that
                the two drawbacks of sophistication might “cancel each other out” if we consider resource-bounded
                versions of sophistication: that is, versions where we impose constraints (possibly severe constraints)
                on both the program for generating S, and the program for generating x given S. Not only does
                the above argument fail for resource-bounded versions of sophistication, but those versions are the
                only ones we can hope to compute anyway! With Kolmogorov complexity, we’re forced to consider
                proxies (such as gzip ﬁle size) mostly just because K (x) itself is uncomputable. By contrast, even
                if we could compute soph (x) perfectly, it would never become large for the systems that interest
                                         c
                us here.
                2.3   Logical Depth
                Athird notion, introduced by Bennett [2], is logical depth. Roughly speaking, the logical depth of
                a string x is the amount of time taken by the shortest program that outputs x. (Actually, to avoid
                the problem of “brittleness,” one typically considers something like the minimum amount of time
                taken by any program that outputs x and whose length is at most K(x) + c, for some constant
                “fudge factor” c. This is closely analogous to what is done for sophistication.)
                   The basic idea here is that, both for simple strings and for random ones, the shortest program
                will also probably run in nearly linear time.  By contrast, one can show that there exist “deep”
                strings, which can be generated by short programs but only after large amounts of time.
                   Like sophistication, logical depth tries to probe the internal structure of a minimal program
                for x—and in particular, to distinguish between the “interesting code” in that program and the
                “boring data” on which the code acts. The diﬀerence is that, rather than trying to measure the
                size of the “interesting code,” one examines how long it takes to run.
                   Bennett [2] has advocated logical depth as a complexity measure, on the grounds that logical
                depth encodes the “amount of computational eﬀort” used to produce x, according to the “most
                probable” (i.e., lowest Kolmogorov complexity) hypothesis about how x was generated.       On the
                other hand, an obvious disadvantage of logical depth is that it’s even less clear how to estimate it
                                                                6
