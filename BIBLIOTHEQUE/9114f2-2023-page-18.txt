D_ Labelling Instructions

Labelers were tasked to look at steps in a solution and label each one as posi-

tive, negative, or neutral. A ste
context, reasonable, correct, and c
fied easily. A step is positive if i

p is considered neutral if it is appropriate in
ontains only computations that can be veri-
is neutral and also progresses towards the

solution. All other steps are considered negative. Labelers were not given ref-
erence solutions, but they were given the ground truth final answers. We chose

not to provide reference solutions
path to the solution. We chose to

information can sometimes help la
In phase 1, labelers were permit

o avoid biasing them towards one particular
provide ground truth final answers since this
delers resolve their own misunderstandings.

ted to enter their own steps in the case that

all candidate steps were negative. Then the solution would progress from a

randomly selected positive step (
This often resulted in trajectories t
steps that said reasonable things bt

or neutral if their were no positive ones).
1at got stuck in endless sequences of neutral
it made frustratingly slow progress towards a

solution or negative steps that needed constant human supervision. In phase 2,
we pre-generate whole solutions and end the task as soon as the first negative

step is encountered. The full ins
hi

S

ructions given to labelers can be found at

ps: //github.com/openai/prm800k/tree/main/prm800k/instructions.

E ORM Training Details

We train outcome-supervised reward models in the same manner as token-level
verifiers from Cobbe ct al. (2021), with a few subtle differences to hyperparam-

eters. In particular, we only train
samples and reward model labels,
ing a language modeling objective.

most other hyperparameters, within a reasonable range.

for a single epoch on each dataset of model
without dropout, and without jointly learn-
We find that performance is not sensitive to

To collect model samples, we simply sample uniformly from the generator at
a temperature of 1.0 without applying any rebalancing of positives or negatives.
At training time, the reward model makes predictions for every token in the

context.
the solution is labelled correct or

[he target for each token in a solution is the same, based on whether

incorrect. At test time, we simply use the

score of the final token in the completion as the overall score of the solution.
We note that this setup is identical to the way token-level verifiers were trained

in Cobbe et al. (2021).

19
