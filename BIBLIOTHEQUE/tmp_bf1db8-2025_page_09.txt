                                                                                                 25                                                                                25
                               l80        General vs Reasoner                 Larger vs Smaller                 l 80        General vs Reasoner                 Larger vs Smaller
                                                                                                 20             a                                                                  20   r
                               a                                                                       r        r                                                                       e
                               r                                                                                e                                                                       l
                               e                                                                       e        n 60                                                                    l
                                                                                                       l
                               n60                                                                     l        e                                                                  15   a
                               e                                                                 15    a        G                                                                       m
                               G                                                                       m                                                                                S
                                                                                                       S        s                                                                        
                               s                                                                                v                                                                       s
                               v                                                                       s          40                                                               10   v
                                40                                                               10    v        r                                                                        
                               r                                                                                e                                                                       r
                               e                                                                       r        n                                                                       e
                               n                                                                       e        o                                                                       g
                               o                                                                       g        s 20                                                               5    r
                               s20                                                               5     r        a                                                                       a
                               a                                                                       a        e                                                                       L
                               e                                                                       L        R                                                                        
                               R                                                                                                                                                   0
                                                                                                 0                 0
                                 0                                                                                                                                                  5
                                                                                                   5                     1000   2000   3000   4000    5000   6000   7000   8000
                                           2000      4000      6000       8000     10000                                    Average Output Length for the Task 
                                          Average Context Length for the Task                                       (Proxy for how much thinking the task may need)
                          Figure 5: Performance gains as a function of (left) context length and (right) output length (proxy for required
                          thinking). A scatter plot and line fit for the gains obtained by a reasoning-specialized model (o3-mini high) vs a
                          general-purpose model (GPT4o) and a larger model (Gemini 2.0 Flash) vs a smaller model (Gemini 2.0 Flash-Lite),
                          as a function of (left) the average context lengths and (right) the average output lengths, for the tasks in BBEH.
                          lengths of the tasks and average output length as                                   LLMsincomplex,real-world applications.
                                                                         5
                          a proxy for required thinking .. We observe that                                    Limitations
                          the gains of o3-mini tend to increase compared
                          to GPT4o both when context length increases and                                     As explained in the main text, BBEH has been
                          whentherequired thinking increases, showing how                                     constructed semi-adversarially with respect to two
                          reasoning models may have improved across both                                      reference models. This leads to two limitations: 1-
                          directions compared to general models. For Gem-                                     it will unavoidably bias the benchmark towards cer-
                          ini 2.0 Flash vs Gemini 2.0 Flash-Lite, we see a                                    tain types of failure modes, 2- since the benchmark
                          similar increase in gains when the context length                                   is created adversarially with respect to the refer-
                          increases, but the curve for the case of increased                                  ence models, a fair comparison of the reference
                          thinking remains mostly flat.                                                       and non-reference models may not be possible.
                          5 Conclusion                                                                        Acknowledgements
                          Recent advances in LLM reasoning has made these                                     We acknowledge the help from Vahab Mirrokni,
                          models reach near ceiling performance on exist-                                     Tania Bedrax-Weiss, Don Metzler, Javad Hosseini,
                          ing general reasoning benchmarks such as BIG-                                       Phoebe Kirk, and Katherine Tong.
                          Benchanditsharder variant BBH, and shifted fo-
                          cus toward other types of more focused reasoning.
                          However, substantial distance remains before we                                     References
                          can claim these models posses true mastery of di-                                   Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
                          verse reasoning skills. To rekindle the pursuit of                                      Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
                          truly robust and versatile LLM reasoners, we pre-                                       Diogo Almeida, Janko Altenschmidt, Sam Altman,
                          sented BIG-Bench Extra Hard (BBEH), a signifi-                                          ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
                          cantly more challenging successor to BBH. This                                          arXiv preprint arXiv:2303.08774.
                          new benchmark, meticulously crafted to amplify                                      RohanAnil, Andrew M. Dai, Orhan Firat, Melvin John-
                          the difficulty of existing tasks while preserving                                       son, Dmitry Lepikhin, Alexandre Passos, Siamak
                          their core diversity, reveals a stark reality: even the                                 Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
                          most advanced LLMs still grapple with fundamen-                                         Chen, Eric Chu, Jonathan H. Clark, Laurent El
                          tal aspects of general reasoning. BBEH provides                                         Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau-
                          a crucial stepping stone, reigniting the challenge                                      rav Mishra, Erica Moreira, Mark Omernick, Kevin
                                                                                                                  Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,
                          and offering a more rigorous platform for future                                        Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez
                          research aimed at unlocking the full potential of                                       Abrego, Junwhan Ahn, Jacob Austin, Paul Barham,
                                                                                                                  Jan Botha, James Bradbury, Siddhartha Brahma,
                              5for the latter case, we removed the Shuffled Objects task                          Kevin Brooks, Michele Catasta, Yong Cheng, Colin
                          as models ran out of effective tokens and started degenerating,                         Cherry, Christopher A. Choquette-Choo, Aakanksha
                          and this was adding noise to the analysis                                               Chowdhery, Cl√©ment Crepy, Shachi Dave, Mostafa
                                                                                                       26481
