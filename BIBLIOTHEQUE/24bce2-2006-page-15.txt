              AFastLearningAlgorithmforDeepBeliefNets                           1541
              synapses do not need to know which unit is competing with which other
              unit. The competition affects the probability of a unit turning on, but it is
              only this probability that affects the learning.
                 Afterthegreedylayer-by-layertraining,thenetworkwastrained,witha
              different learning rate and weightdecay,for300epochsusingtheup-down
              algorithmdescribedinsection5.Thelearningrate,momentum,andweight
                   7
              decay werechosenbytrainingthenetworkseveraltimesandobservingits
              performance on a separate validation set of 10,000 images that were taken
              from the remainder of the full training set. For the ﬁrst 100 epochs of the
              up-down algorithm, the up-pass was followed by three full iterations of
              alternating Gibbs sampling in the associative memory before performing
              thedown-pass.Forthesecond100epochs,6iterationswereperformed,and
              forthelast100epochs,10iterationswereperformed.Eachtimethenumber
              of iterations of Gibbs sampling was raised, the error on the validation set
              decreased noticeably.
                 The network that performed best on the validation set was tested and
              had an error rate of 1.39%. This network was then trained on all 60,000
                             8
              training images until its error rate on the full training set was as low as
              its ﬁnal error rate had been on the initial training set of 44,000 images. This
              took a further 59 epochs, making the total learning time about a week. The
              ﬁnalnetworkhadanerrorrateof1.25%.9 Theerrorsmadebythenetwork
              are shown in Figure 6. The 49 cases that the network gets correct but for
              which the second-best probability is within 0.3 of the best probability are
              showninFigure7.
                 The error rate of 1.25% compares very favorably with the error rates
              achievedbyfeedforwardneuralnetworksthathaveoneortwohiddenlay-
              ers and are trained to optimize discrimination using the backpropagation
              algorithm (see Table 1). When the detailed connectivity of these networks
              is not handcrafted for this particular task, the best reported error rate for
              stochastic online learning with a separate squared error on each of the 10
              output units is 2.95%. These error rates can be reduced to 1.53% in a net
              withonehiddenlayerof800unitsbyusingsmallinitialweights,aseparate
              cross-entropy error function on each output unit, and very gentle learning
                 7 No attempt was made to use different learning rates or weight decays for different
              layers,andthelearningrateandmomentumwerealwayssetquiteconservativelytoavoid
              oscillations. It is highly likely that the learning speed could be considerably improved by
              amorecarefulchoiceoflearningparameters,thoughitispossiblethatthiswouldleadto
              worsesolutions.
                 8 The training set has unequal numbers of each class, so images were assigned ran-
              domlytoeachofthe600mini-batches.
                 9 To check that further learning would not have signiﬁcantly improved the error rate,
              the network was then left running with a very small learning rate and with the test error
              beingdisplayedaftereveryepoch.Aftersixweeks,thetesterrorwasﬂuctuatingbetween
              1.12% and 1.31% and was 1.18% for the epoch on which number of training errors was
              smallest.
