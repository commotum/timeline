SUPPORT-VECTOR NETWORKS 283

The linearity of the dot-product implies, that the classification function f in (31) for an
unknown vector x only depends on the dot-products:

£
f(®) = G(%)-WHb = )_ yiarh(x) - CK) +b. (33)
i=1

The idea of constructing support-vector networks comes from considering general forms
of the dot-product in a Hilbert space (Anderson & Bahadur, 1966):

o(u) - (v) = Ku, y). (34)

According to the Hilbert-Schmidt Theory (Courant & Hilbert, 1953) any symmetric
function K (u, v), with K (u, v) € £2, can be expanded in the form

wo
K(u,v) = > Ai@i(u) - 6;(¥), (35)
i=1
where A; € % and ¢; are eigenvalues and eigenfunctions

/ K(u, v)¢;(u)du = 2:4,(¥).

of the integral operator defined by the kernel K (u, v). A sufficient condition to ensure that
(34) defines a dot-product in a feature space is that all the eigenvalues in the expansion (35)
are positive. To guarantee that these coefficients are positive, it is necessary and sufficient
(Mercer’s Theorem) that the condition

[/ K (a, v)g(w)g(v)dudv > 0

is satisfied for all g such that

[ewau < OO.

Functions that satisfy Mercer’s theorem can therefore be used as dot-products. Aizerman,
Braverman and Rozonoer (1964) consider a convolution of the dot-product in the feature
space given by function of the form

K(u, ¥) = exp(-2#=*) ; (36)

which they call Potential Functions.

However, the convolution of the dot-product in feature space can be given by any function
satisfying Mercer’s condition; in particular, to construct a polynomial classifier of degree
d in n-dimensional input space one can use the following function

Ku, vy) = u-v4 1). (37)
