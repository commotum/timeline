280 CORTES AND VAPNIK

If the training set (8) cannot be separated by a hyperplane, the margin between patterns
of the two classes becomes arbitrary small, resulting in the value of the functional W(A)
turning arbitrary large. Maximizing the functional (15) under constraints (16) and (17)
one therefore either reaches a maximum (in this case one has constructed the hyperplane
with the maximal margin fo), or one finds that the maximum exceeds some given (large)
constant Wp (in which case a separation of the training data with a margin Jarger then
»/2/ Wo is impossible).

The problem of maximizing functional (15) under constraints (16) and (17) can be soived
very efficiently using the following scheme. Divide the training data into a number of
portions with a reasonable small number of training vectors in each portion. Start out by
solving the quadratic programming problem determined by the first portion of training data.
For this problem there are two possible outcomes: either this portion of the data cannot be
separated by a hyperplane (in which case the full set of data as well cannot be separated),
or the optimal hyperplane for separating the first portion of the training data is found.

Let the vector that maximizes functional (15) in the case of separation of the first portion
be A,. Among the coordinates of vector A; some are equal to zero. They correspond to
non-support training vectors of this portion. Make a new set of training data containing
the support vectors from the first portion of training data and the vectors of the second
portion that do not satisfy constraint (10), where w is determined by A,. For this set a
new functional W2(A) is constructed and maximized at Ay. Continuing this process of
incrementally constructing a solution vector A, covering all the portions of the training
data one either finds that it is impossible to separate the training set without error, or one
constructs the optimal separating hyperplane for the full data set, A, = Ao. Note, that
during this process the value of the functional W(A) is monotonically increasing, since
more and more training vectors are considered in the optimization, leading to a smaller and
smaller separation between the two classes.

3. The Soft Margin Hyperplane

Consider the case where the training data cannot be separated without error. In this case
one may want to separate the training set with a minimal number of errors. To express this
formally let us introduce some non-negative variables &; > 0, i =1,..., 2.

We can now minimize the functional

£
OE) =) ef (21)
i=l
for small o > 0, subject to the constraints
yi(w-x +b) >1-&, i=1,...,2, (22)
& > 0, i=1,...,2. (23)

For sufficiently smal] o the functional (21) describes the number of the training errors”.
Minimizing (21) one finds some minimal subset of training errors:

(Yi: Xi,), wens (Vigs Xig)-
