288 CORTES AND VAPNIK

Table 2, Results obtained for dot products of polynomials of various degree. The number of “support vectors”
is a mean value per classifier.

Degree of Raw Support Dimensionality of
polynomial error, % vectors feature space

1 12.0 200 256

2 47 127 ~33000

3 44 148 ~1 x 106

4 43 165 ~1 x 10?

5 43 175 ~1 x 10”

6 4.2 185 ~1 x 10!

7 43 190 ~L x 1016

from publications and own experiments. The result of human performance was reported
by J. Bromley & E. Sackinger (Bromley & Sackinger, 1991). The result with CART was
carried out by Daryl Pregibon and Michael D. Riley at Bell Labs., Murray Hill, NJ. The
results of C4.5 and the best 2-layer neural network (with optimal number of hidden units)
were obtained specially for this paper by Corinna Cortes and Bernard Schoelkopf respec-
tively. The result with a special purpose neural network architecture with 5 layers, LeNet],
was obtained by Y. LeCun et al. (1990).

On the experiments with the US Postal Service Database we used pre-processing (cen-
tering, de-slanting and smoothing) to incorporate knowledge about the invariances of the
problem at hand. The effect of smoothing of this database as a pre-processing for support-
vector networks was investigated in (Boser, Guyon & Vapnik, 1992). For our experiments
we chose the smoothing kernel as a Gaussian with standard deviation 0 = 0.75 in agreement
with (Boser, Guyon & Vapnik, 1992).

In the experiments with this database we constructed polynomial indicator functions
based on dot-products of the form (39). The input dimensionality was 256, and the order
of the polynomial ranged from | to 7. Table 2 describes the results of the experiments. The
training data are not linearly separable.

Notice that the number of support vectors increases very slowly. The 7 degree polyno-
mial has only 30% more support vectors than the 3rd degree polynomial—and even less
than the first degree polynomial. The dimensionality of the feature space for a 7 degree
polynomial is however 10!° times larger than the dimensionality of the feature space for
a 3rd degree polynomial classifier. Note that performance almost does not change with
increasing dimensionality of the space—indicating no over-fitting problems.

The relatively high number of support vectors for the linear separator is due to non-
separability: the number 200 includes both support vectors and training vectors with a non-
zero &-value. If & > 1 the training vector is misclassified; the number of mis-classifications
on the training set averages to 34 per classifier for the linear case. For a 2nd degree classifier
the total number of mis-classifications on the training set is down to 4. These 4 patterns are
shown in Fig. 7.

It is remarkable that in all our experiments the bound for generalization ability (5) holds
when we consider the number of obtained support vectors instead of the expectation value
of this number. In all cases the upper bound on the error probability for the single classifier
does not exceed 3% (on the test data the actual error does not exceed 1.5% for the single
classifier).
