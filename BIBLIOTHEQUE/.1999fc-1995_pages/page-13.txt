286 CORTES AND VAPNIK

(the set of functions with bounded norm of the weights) can be less than the dimensionality
of the input space and will depend on Cy.

From this point of view the optimal margin classifier method executes an Occam-Razor
principle. It keeps the first term of (38) equal to zero (by satisfying the inequality (9))
and it minimizes the second term (by minimizing the functional w - w). This minimization
prevents an over-fitting problem.

However, even in the case where the training data are separable one may obtain better
generalization by minimizing the confidence term in (38) even further at the expense of
errors on the training set. In the soft margin classifier method this can be done by choosing
appropriate values of the parameter C. In the support-vector network algorithm one can
control the trade-off between complexity of decision rule and frequency of error by changing
the parameter C, even in the more general case where there exists no solution with zero
error on the training set. Therefore the support-vector network can control both factors for
generalization ability of the learning machine.

6. Experimental Analysis

To demonstrate the support-vector network method we conduct two types of experiments.
We construct artificial sets of patterns in the plane and experiment with 2nd degree poly-
nomial decision surfaces, and we conduct experiments with the real-life problem of digit
recognition.

6.1. Experiments in the Plane

Using dot-products of the form
Ku, v) = (u-v+ 1)? (39)

with d = 2 we construct decision rules for different sets of patterns in the plane. Results
of these experiments can be visualized and provide nice illustrations of the power of the
algorithm. Examples are shown in Fig. 5. The 2 classes are represented by black and white

Figure 5. Examples of the dot-product (39) with d = 2. Support patterns are indicated with double circles,
errors with a cross.
