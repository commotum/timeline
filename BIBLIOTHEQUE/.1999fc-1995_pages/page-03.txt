276 CORTES AND VAPNIK

Note that this bound does not explicitly contain the dimensionality of the space of separation.
It follows from this bound, that if the optimal hyperplane can be constructed from a small
number of support vectors relative to the training set size the generalization ability will be
highâ€”even in an infinite dimensional space. In Section 5 we will demonsirate that the ratio
(5) for a real life problems can be as low as 0.03 and the optimal hyperplane generalizes
well in a billion dimensional feature space.

Let

Wo: zZ + bo =0

be the optimal hyperplane in feature space. We will show, that the weights wo for the
optimal hyperplane in the feature space can be written as some linear combination of
support vectors

support vectors

The linear decision function /(z) in the feature space will accordingly be of the form:

| >: cometh), , (7)

support vectors

where 2; -z is the dot-product between support vectors z; and vector z in feature space. The
decision function can therefore be described as a two layer network (Fig. 3).

However, even if the optimal hyperplane generalizes well the technical problem of how
to treat the high dimensional feature space remains. In 1992 it was shown (Boser, Guyon,
& Vapnik, 1992), that the order of operations for constructing a decision function can
be interchanged: instead of making a non-linear transformation of the input vectors fol-
lowed by dot-products with support vectors in feature space, one can first compare two
vectors in input space (by e.g. taking their dot-product or some distance measure), and
then make a non-linear transformation of the value of the result (see Fig. 4). This en-
ables the construction of rich classes of decision surfaces, for example polynomial decision
surfaces of arbitrary degree. We will call this type of learning machine a support-vector
network?,

The technique of support-vector networks was first developed for the restricted case of
separating training data without errors. In this article we extend the approach of support-
vector networks to cover when separation without error on the training vectors is impossible.
With this extension we consider the support-vector networks as a new class of learning
machine, as powerful and universal as neural networks. In Section 5 we will demonstrate
how well it generalizes for high degree polynomial decision surfaces (up to order 7) in a
high dimensional space (dimension 256). The performance of the algorithm is compared
to that of classical learning machines e.g. linear classifiers, k-nearest neighbors classifiers,
and neural networks. Sections 2, 3, and 4 are devoted to the major points of the derivation
of the algorithm and a discussion of some of its properties. Details of the derivation are
relegated to an appendix.
