SUPPORT-VECTOR NETWORKS 285

In the next section we will consider support-vector network machines that use polynomial
decision surfaces. To specify polynomials of different order d one can use the following
functions for convolution of the dot-product

K(u, v) = (u-v+ 1%.

Radial Basis Function machines with decision functions of the form

_. ‘ Ix — x)/?
F)= vea( Sa on =")

can be implemented by using convolutions of the type

_ yl2
K(u, vy) = exp{ ~ ju — \

In this case the support-vector network machine will construct both the centers x, of the
approximating function and the weights o;.

One can also incorporate a priori knowledge of the problem at hand by constructing
special convolution functions. Support-vector networks are therefore a rather general class
of learning machines which changes its set of decision functions simply by changing the
form of the dot-product.

5.3. Support-Vector Networks and Control of Generalization Ability

To control the generalization ability of a learning machine one has to control two different
factors: the error-rate on the training data and the capacity of the learning machine as
measured by its VC-dimension (Vapnik, 1982). There exists a bound for the probability of
errors on the test set of the following form: with probability 1 — 7 the inequality

Pr(test error) < Frequency(training error) + Confidence Interval (38)

is valid. In the bound (38) the confidence interval depends on the VC-dimension of the
learning machine, the number of elements in the training set, and the value of 7.

The two factors in (38) form a trade-off: the smaller the VC-dimension of the set of
functions of the learning machine, the smaller the confidence interval, but the larger the
value of the error frequency.

A general way for resolving this trade-off was proposed as the principle of structural risk
minimization: for the given data set one has to find a solution that minimizes their sum.
A particular case of structural risk minimization principle is the Occam-Razor principle:
keep the first term equal to zero and minimize the second one.

It is known that the VC-dimension of the set of linear indicator functions

I(x) = sign(w-x+ 6), [xl < Cx

with fixed threshold b is equal to the dimensionality of the input space. However, the
VC-dimension of the subset

I(x) =sign(w-x+5), |[xl<C, lwl<Cw
