SUPPORT-VECTOR NETWORKS 277

classification

Si "alee t= a support vectors Z;

in feature space

_— input vector in feature space

non-linear transformation

input vector, x

Figure 3. Classification by a support-vector network of an unknown pattern is conceptually done by first trans-
forming the pattern into some high-dimensional feature space. An optimal hyperplane constructed in this feature
space determines the output. The similarity to a two-layer perceptron can be seen by comparison to Fig. 1.

2. Optimal Hyperplanes

In this section we review the method of optimal hyperplanes (Vapnik, 1982) for separation
of training data without errors. In the next section we introduce a notion of soft margins,
that will allow for an analytic treatment of learning with errors on the training set.

2.1. The Optimal Hyperplane Algorithm

The set of labeled training patterns
(yt, X1),--+ (Ye, Xe), yw €{-1, ]} (8)
is said to be linearly separable if there exists a vector w and ascalar b such that the inequalities

wx, tb>1 if y =1,
wextb<-1 if y = —1, (9)
