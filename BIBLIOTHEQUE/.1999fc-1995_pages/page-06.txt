SUPPORT-VECTOR NETWORKS 279

This means that the optimal hyperplane is the unique one that minimizes w - w under the
constraints (10). Constructing an optimal hyperplane is therefore a quadratic programming
problem.

Vectors x; for which y;(w-x; +b) = 1 will be termed support vectors. In Appendix A.1
we show that the vector wo that determines the optimal hyperplane can be written as a linear
combination of training vectors:

£
Wo = > iti, (14)
i=1

where a? > 0. Since a > 0 only for support vectors (see Appendix), the expression (14)
represents a compact form of writing wo. We also show that to find the vector of parameters
O;:

AS = (ap, ...,@?),

one has to solve the following quadratic programming problem:

1
W(A) =A‘1— zAiDA (15)
with respect to Al = (a1,..., @¢), subject to the constraints:
A > 0, (16)
ATY = 0, (17)
where 17 = (1,..., 1) is an €-dimensional unit vector, Y7 = (1, ..., yg) is the €-dimen-

sional vector of labels, and D is a symmetric @ x £-matrix with elements

The inequality (16) describes the nonnegative quadrant. We therefore have to maximize the
quadratic form (15) in the nonnegative quadrant, subject to the constraints (17).

When the training data (8) can be separated without errors we also show in Appendix A
the following relationship between the maximum of the functional (15), the pair (Ag, bo),
and the maximal margin po from (13):

2
W(Ao) = -- (19)

Po

If for some A, and large constant Wo the inequality
W(A,) > Wo (20)

is valid, one can accordingly assert that all hyperplanes that separate the training data (8)
have a margin
