296 CORTES AND VAPNIK

Finally, we can consider the hyperplane that minimizes the form

J £
i=1

subject to the constraints (49)-(50), where the second term minimizes the least square value
for the errors. This lead to the following quadratic programming problem: maximize the
functional

1
W(A) = A71— 5 [A’DA + Zara (67)

in the non-negative quadrant A > 0 subject to the constraint A7 Y = 0.

Notes

1. The optimal coefficient for t was found in the sixties (Anderson & Bahadur, 1966).
2. Recall Fisher’s concerns about small amounts of data and the quadratic discriminant function.

3. With this name we emphasize how crucial the idea of expanding the solution on support vectors is for these
learning machines. In the support-vectors learning algorithm the complexity of the construction does not
depend on the dimensionality of the feature space, but on the number of support vectors.

. Note that in the inequalities (9) and (10) the right-hand side, but not vector w, is normalized.
. A training error is here defined as a pattern where the inequality (22) holds with € > 0.

. The decision function is unique but not its expansion on support vectors.

sa nD A

. National Institute for Standards and Technology, Special Database 3.

References

Aizerman, M., Braverman, E., & Rozonoer, L. (1964). Theoretical foundations of the potential function method
in pattern recognition learning. Automation and Remote Control, 25:821-837.

Anderson, T.W., & Bahadur, R.R. (1966). Classification into two multivariate normal distributions with different
covariance matrices. Ann. Math. Stat., 33:420-431.

Boser, B.E., Guyon, I., & Vapnik, V.N. (1992). A training algorithm for optimal margin classifiers. In Proceedings
of the Fifth Annual Workshop of Computational Learning Theory, 5, 144-152, Pittsburgh, ACM.

Bottou, L., Cortes, C., Denker, J.S., Drucker, H., Guyon, I., Jackel, L.D., LeCun, Y., Sackinger, E., Simard, P.,
Vapnik, V., & Miller, U.A. (1994). Comparison of classifier methods: A case study in handwritten digit
recognition. Proceedings of 12th International Conference on Pattern Recognition and Neural Network.

Bromley, J., & Sackinger, E. (1991). Neural-network and k-nearest-neighbor classifiers. Technical Report 11359-
910819-16TM, AT&T.

Courant, R., & Hilbert, D. (1953). Methods of Mathematical Physics, Interscience, New York.

Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems. Ann. Eugenics, 7:111-132.

LeCun, Y. (1985). Une procedure d’apprentissage pour reseau a seuil assymetrique. Cognitiva 85: A la Frontiere
de IIntelligence Artificielle des Sciences de la Connaissance des Neurosciences, 599-604, Paris.

LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., & Jackel, L.D. (1990). Handwritten
digit recognition with a back-propagation network. Advances in Neural Information Processing Systems, 2, 396—
404, Morgan Kaufman.

Parker, D.B. (1985). Learning logic. Technical Report TR-47, Center for Computational Research in Economics
and Management Science, Massachusetts Institute of Technology, Cambridge, MA.

Rosenblatt, F. (1962). Principles of Neurodynamics, Spartan Books, New York.
