Machine Learning, 20, 273-297 (1995)
© 1995 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.

Support-Vector Networks

CORINNA CORTES corinna@neural.att.com
VLADIMIR VAPNIK vlad @neural.att.com
AT&T Bell Labs., Holmdel, NJ 07733, USA

Editor: Lorenza Saitta

Abstract. The support-vector network is a new learning machine for two-group classification problems. The
machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-
dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the
decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector
network was previously implemented for the restricted case where the training data can be separated without
errors. We here extend this result to non-separable training data.

High generalization ability of support-vector networks utilizing polynomial input transformations is demon-
strated. We also compare the performance of the support-vector network to various classical learning algorithms
that all took part in a benchmark study of Optical Character Recognition.

Keywords: pattern recognition, efficient learning algorithms, neural networks, radial basis function classifiers,
polynomial classifiers.

1. Introduction

More than 60 years ago R.A. Fisher (Fisher, 1936) suggested the first algorithm for pattern
recognition. He considered a model of two normal distributed populations, N(m;, ©)
and N(m,, %2) of n dimensional vectors x with mean vectors m,; and my and co-variance
matrices 33; and 329, and showed that the optimal (Bayesian) solution is a quadratic decision
function:

. 1 _ 1 _ p>
Fyq(x) = sgn) 5x —m,)’ 5) '(x—m) — 5% — mp)? SF '(x — mp) + In a . (1)
1
In the case where 33; = 42 = & the quadratic decision function (1) degenerates to a linear
function:

. ~ 1 _ _ '
Fin() = sgn] (om — m)' > KH 5 (mre Im) — m; 5 ‘mn . (2)

To estimate the quadratic decision function one has to determine Hort) free parameters. To
estimate the linear function only n free parameters have to be determined. In the case where
the number of observations is small (say less than 10 n”) estimating o(n”) parameters is not
reliable. Fisher therefore recommended, even in the case of ©}, 4 Uy, to use the linear
discriminator function (2) with © of the form:

B=, +1 — Sy, (3)

where t is some constant! Fisher also recommended a linear decision function for the
case where the two distributions are not normal. Algorithms for pattern recognition
