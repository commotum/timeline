SUPPORT- VECTOR NETWORKS 275

Figure 2, Anexample of a separable problem in a 2 dimensional space. The support vectors, marked with grey
squares, define the margin of largest separation between the two classes.

EXAMPLE. To obtain a decision surface corresponding to a polynomial of degree two, one

can create a feature space, Z, which has N = nets) coordinates of the form:
2] = X1,---52n = Xn, n coordinates,
Zatl = x?, 66+, 2 = x2, n coordinates,
n(n — 1) ;
Zant = X1X2,...,2N = XnXn~-1, — coordinates,
where x = (x1, ...,2X,). The hyperplane is then constructed in this space.

Two problems arise in the above approach: one conceptual and one technical. The con-
ceptual problem is how to find a separating hyperplane that will generalize well: the dimen-
sionality of the feature space will be huge, and not all hyperplanes that separate the training
data will necessarily generalize well”. The technical problem is how computationally to
treat such high-dimensional spaces: to construct polynomial of degree 4 or 5 in a 200
dimensional space it may be necessary to construct hyperplanes in a billion dimensional
feature space.

The conceptual part of this problem was solved in 1965 (Vapnik, 1982) for the case of
optimal hyperplanes for separable classes. An optimal hyperplane is here defined as the
linear decision function with maximal margin between the vectors of the two classes, see
Fig. 2. It was observed that to construct such optimal hyperplanes one only has to take into
account a small amount of the training data, the so called support vectors, which determine
this margin. It was shown that if the training vectors are separated without errors by an
optimal hyperplane the expectation value of the probability of committing an error on a test
example is bounded by the ratio between the expectation value of the number of support
vectors and the number of training vectors:

E[Pr(error)] < E{number of support vectors]

(S)

number of training vectors
