SUPPORT-VECTOR NETWORKS 293

Another observation, based on the Kuhn-Tucker Eqs. (44) and (45) for the optimal
solution, is the relationship between the maximal value W(Ag) and the separation distance

Po:

£ t i

0 0 0

Wo: Wo = > of yiXi «Wo = > a (1 — yibo) = > a;.
i=} i=l i=l

Substituting this equality into the expression (46) for W(Ao) we obtain

£
1 Wo: W
W(Ao) =) a? — 5Wo- Wo = “5 o

i=l
Taking into account the expression (13) from Section 2 we obtain
2
W(Ao) = 5,
Po

where fo is the margin for the optimal hyperplane.

A.2. Soft Margin Hyperplane Algorithm

Below we first consider the case of F(u) = u*. Then we describe the general result for a
monotonic convex function F(u).
To construct a soft margin separating hyperplane we maximize the functional

k
1 £
i=l
under the constraints

yi - wtb) >1—&, i=1,...,4, (49)
£, > 0, i=l,...,£. (50)

The Lagrange functional for this problem is

L(w, €,b, A, R)
1 £ k e £
=5W Ww +C (> :) — 2. ajly;(x;-w + b)-—1+&]—- dorsi, (51)

where the non-negative multipliers A! = (a, a,..., a) arise from the constraint (49),
and the multipliers R? = (1, r2,...,1) enforce the constraint (50).

We have to find the saddle point of this functional (the minimum with respect to the
variables w;, b, and &;, and the maximum with respect to the variables a; and r;).

Let us use the conditions for the minimum of this functional at the extremum point:

aL
7 =w—-
ow °

w=Wo i=]

£

