SUPPORT- VECTOR NETWORKS 281

If these data are excluded from the training set one can separate the remaining part of the
training set without errors. To separate the remaining part of the training data one can
construct an optimal separating hyperplane.

This idea can be expressed formally as: minimize the functional

£
Ww +CF (ds «] (24)

subject to constraints (22) and (23), where F(z) is a monotonic convex function and C is
a constant.

For sufficiently large C and sufficiently small o, the vector wo and constant bo, that
minimize the functional (24) under constraints (22) and (23), determine the hyperplane
that minimizes the number of errors on the training set and separate the rest of the elements
with maximal margin.

Note, however, that the problem of constructing a hyperplane which minimizes the
number of errors on the training set is in general NP-complete. To avoid NP-completeness
of our problem we will consider the case of o = 1 (the smallest value of o for which
the optimization problem (15) has a unique solution). In this case the functional (24)
describes (for sufficiently large C) the problem of constructing a separating hyperplane
which minimizes the sum of deviations, &, of training errors and maximizes the margin
for the correctly classified vectors. If the training data can be separated without errors the
constructed hyperplane coincides with the optimal margin hyperplane.

In contrast to the case with o < 1 there exists an efficient method for finding the solution
of (24) in the case of o = 1. Let us call this solution the soft margin hyperplane.

In Appendix A we consider the problem of minimizing the functional

1 £
5w +CF (> s] (25)

subject to the constraints (22) and (23), where F(z) is a monotonic convex function with
F (0) = 0. To simplify the formulas we only describe the case of F(u) = u? in this section.
For this function the optimization problem remains a quadratic programming problem.

In Appendix A we show that the vector w, as for the optimal hyperplane algorithm, can
be written as a linear combination of support vectors x;:

£
=) )
Wo = Ot; ViXi-
i=]

To find the vector A? = (a,..., 0) one has to solve the dual quadratic programming
problem of maximizing

82
W(A, 6) = AT1— 5[A7DA + =| (26)
subject to constraints
ATY =0, (27)
5 >0, (28)

O<A <6, (29)
