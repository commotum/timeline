SUPPORT- VECTOR NETWORKS 291

The algorithm has been tested and compared to the performance of other classical al-
gorithms. Despite the simplicity of the design in its decision surface the new algorithm
exhibits a very fine performance in the comparison study.

Other characteristics like capacity control and ease of changing the implemented decision
surface render the support-vector network an extremely powerful and universal learning
machine.

A. Constructing Separating Hyperplanes

In this appendix we derive both the method for constructing optimal hyperplanes and soft
margin hyperplanes.

A.l. Optimal Hyperplane Algorithm

It was shown in Section 2, that to construct the optimal hyperplane
which separates a set of training data

(y1, Xi), - +> (ve, Xe),

one has to minimize a functional

subject to the constraints
yay - wtb) > 1, i=l,...,2. (41)

To do this we use a standard optimization technique. We construct a Lagrangian

1 t
L(w, b, A) = swew—) oily -w+b)— 1, (42)
i=1
where A? = (a1, ..., &) is the vector of non-negative Lagrange multipliers corresponding

to the constraints (41).

It is known that the solution to the optimization problem is determined by the saddle point
of this Lagrangian in the 2 + 1-dimensional space of w, A, and b, where the minimum
should be taken with respect to the parameters w and b, and the maximum should be taken
with respect to the Lagrange multipliers A.

At the point of the minimum (with respect to w and b) one obtains:

aL(w, b, A £

SEPA = (wo — Draven | =0, (43)
dw W=Wo i=l

aL(w, b, A

GE PM ya, = 0. (44)
db b=bo ai
