282 CORTES AND VAPNIK

where 1, A, Y, and.D are the same elements as used in the optimization problem for
constructing an optimal hyperplane, 5 is a scalar, and (29) describes coordinate-wise in-
equalities.

Note that (29) implies that the smallest admissible value 6 in functional (26) is

& = Omax = Max(Q),..., Qg).

Therefore to find a soft margin classifier one has to find a vector A that maximizes

2

W(A) = AT1— 5|A7DA + al (30)
under the constraints A > Oand (27). This problem differs from the problem of constructing
an optimal margin classifier only by the additional term with om, in the functional (30).
Due to this term the solution to the problem of constructing the soft margin classifier is
unique and exists for any data set.

The functional (30) is not quadratic because of the term with a,x. Maximizing G0)
subject to the constraints A > 0 and (27) belongs to the group of so-called convex pro-
gramming problems. Therefore, to construct a soft margin classifier one can either solve
the convex programming problem in the £-dimensional space of the parameters A, or one
can solve the quadratic programming problem in the dual £ + 1 space of the parameters A
and 5. In our experiments we construct the soft margin hyperplanes by solving the dual
quadratic programming problem.

4. The Method of Convolution of the Dot-Product in Feature Space

The algorithms described in the previous sections construct hyperplanes in the input space.
To construct a hyperplane in a feature space one first has to transform the n-dimensional
input vector x into an N-dimensional feature vector through a choice of an N-dimensional
vector function ¢:

o: Ro RY.

An N dimensional linear separator w and a bias b is then constructed for the set of
transformed vectors

(x) = O1(%:), 2%), ..., Ow), i=1,...,4.

Classification of an unknown vector x is done by first transforming the vector to the sepa-
rating space (x > $(x)) and then taking the sign of the function

f@®) =w- d(x) +b. 31)

According to the properties of the soft margin classifier method the vector w can be
written as a linear combination of support vectors (in the feature space). That means

£
w = >) vio (Xi). (32)
i=1
