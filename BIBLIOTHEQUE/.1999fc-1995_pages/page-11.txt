284 CORTES AND VAPNIK.

Using different dot-products K (u, v) one can construct different learning machines with
arbitrary types of decision surfaces (Boser, Guyon & Vapnik, 1992). The decision surface
of these machines has a form

£
f® = Do ya K &, xi),
i=]

where x; is the image of a support vector in input space and a; is the weight of a support
vector in the feature space.

To find the vectors x; and weights a; one follows the same solution scheme as for the
original optimal margin classifier or soft margin classifier. The only difference is that
instead of matrix D (determined by (18)) one uses the matrix

Diz = yiy; K (i, Xj), i, j=1,...,1.

5. General Features of Support-Vector Networks
5.1. Constructing the Decision Rules by Support-Vector Networks is Efficient

To construct a support-vector network decision rule one has to solve a quadratic optimization
problem:

1 5
A)=A™T1— ~/A™TDA+ — J,
W(A) Al +5|

under the simple constraints:

0<A < 61,
ATY =0,

where matrix

is determined by the elements of the training set, and K (u, v) is the function determining
the convolution of the dot-products.

The solution to the optimization problem can be found efficiently by solving intermediate
optimization problems determined by the training data, that currently constitute the support
vectors. This technique is described in Section 3. The obtained optimal decision function
is unique®.

Each optimization problem can be solved using any standard techniques.

5.2. The Support-Vector Network is a Universal Machine

By changing the function K (u, v) for the convolution of the dot-product one can implement
different networks.
