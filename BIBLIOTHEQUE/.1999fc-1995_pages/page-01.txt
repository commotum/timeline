274 CORTES AND VAPNIK

| perceptron output
CET I weights of the output unit,

4 Cys Og

dot-product

output from the 5 hidden units: Zyeee ZB
na | i: see =e [] weights of the 5 hidden units
dot-products
» ~ al
Ld output from the 4 hidden units
Ei TT) CEC) CEC EES weights of the 4 hidden units

dot—products

input vector, x

Figure 1. A simple feed-forward perceptron with 8 input units, 2 layers of hidden units, and 1 output unit. The
gray-shading of the vector entries reflects their numeric value.

were therefore from the very beginning associated with the construction of linear deci-
sion surfaces.

In 1962 Rosenblatt (Rosenblatt, 1962) explored a different kind of learning machines:
perceptrons or neural networks. The perceptron consists of connected neurons, where each
neuron implements a separating hyperplane, so the perceptron as a whole implements a
piecewise linear separating surface. See Fig. 1.

No algorithm that minimizes the error on a set of vectors by adjusting all the weights of
the network was found in Rosenblatt’s time, and Rosenblatt suggested a scheme where only
the weights of the output unit were adaptive. According to the fixed setting of the other
weights the input vectors are non-linearly transformed into the feature space, Z, of the last
layer of units. In this space a linear decision function is constructed:

(x) = sign (x O42; «) (4)

by adjusting the weights «; from the ith hidden unit to the output unit so as to minimize some
error measure over the training data. As a result of Rosenblatt’s approach, construction of
decision rules was again associated with the construction of linear hyperplanes in some
space.

An algorithm that allows for all weights of the neural network to adapt in order locally to
minimize the error on a set of vectors belonging to a pattern recognition problem was found
in 1986 (Rumelhart, Hinton & Williams, 1986, 1987; Parker, 1985; LeCun, 1985) when the
back-propagation algorithm was discovered. The solution involves a slight modification
of the mathematical model of neurons. Therefore, neural networks implement “piece-wise
linear-type” decision functions.

in this article we construct a new type of learning machine, the so-called support-vector
network. The support-vector network implements the following idea: it maps the input
vectors into some high dimensional feature space Z through some non-linear mapping
chosen a priori. In this space a linear decision surface is constructed with special properties
that ensure high generalization ability of the network.
