292 CORTES AND VAPNIK

From equality (43) we derive

t
Wo = > oiyiXi, (45)
i=l

which expresses, that the optimal hyperplane solution can be written as a linear combina-
tion of training vectors. Note, that only training vectors x; with «; > 0 have an effective
contribution to the sum (45).

Substituting (45) and (44) into (42) we obtain

1
W(A) = di 5 Wo Wo (46)
£ 1 £ £
= 2a mi — 5 DD ier yi yjRi - (47)
f=1 i=l j=1

In vector notation this can be rewritten as
1
W(A) = Al1— zA'DA, (48)
where 1 is an /-dimensional unit vector, and D is a symmetric 2 x €-matrix with elements

Dij = iy j¥i - Xj-

To find the desired saddle point it remains to locate the maximum of (48) under the
constraints (43)

A’Y =0,
where Y? = (y,..., ye), and
A>0O.

The Kuhn-Tucker theorem plays an important part in the theory of optimization. Ac-
cording to this theorem, at our saddle point in wo, bp, Ao, any Lagrange multiplier a? and
its corresponding constraint are connected by an equality

oily; (x; - Wo + bo) — 1] = 90, i=1,...,€.
From this equality comes that non-zero values a; are only achieved in the cases where
yi (Xi - Wo + bo) —1 =0.

In other words: a; 4 0 only for cases were the inequality is met as an equality. We call
vectors x; for which

yiCX - Wo + bo) = 1

for support-vectors. Note, that in this terminology the Eq. (45) states that the solution vector
Wo can be expanded on support vectors.
