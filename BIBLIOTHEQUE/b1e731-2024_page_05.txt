                             or transduction [Li et al., 2024a]. If we do not limit the Kolmogorov complexity of programs [Kol-
                             mogorov, 1965, Solomonoff, 1964], we can find an explanation for any given specification whether
                             or not it corresponds to the true program that underlies the specification. Evaluating generalization
                             performance tests not only the ability to explain a specification but also to successfully infer the
                             underlying program in its generality and apply it to a new input. This problem bears a resemblance to
                             few-shot learning with the difference of having only one task. A recent problem posed in the Artificial
                             Intelligence research community that falls under this formulation is the ARC dataset [Chollet, 2019]
                             that contains programs on 2D grids of 30x30 cells that can take any 10 colors.
                             Variational Auto-Encoders (VAEs)            [Kingma, 2013] provide a framework for approximate
                             Bayesian inference. Given i.i.d. samples x from a dataset, the goal is to estimate the posterior
                             p(z|x), where z represents a latent variable. Since directly computing this posterior is generally
                             intractable, the evidence lower bound (ELBO) is introduced to approximate it. In this approach, a
                             parameterized encoder q (z|x) is used to approximate the true posterior distribution.
                                                       ϕ
                             TheELBOcanbeformulatedasfollows:
                                                 logp(x) ≥ E             [logp (x|z)] − D       (q (z|x)∥p(z)),                   (4)
                                                               z∼qϕ(z|x)       θ            KL ϕ
                             where the first term captures the likelihood of reconstructing x from the latent variable z, and
                             the second term is a Kullback-Leibler divergence that regularizes qϕ(z|x) to be close to the prior
                             distribution p(z).
                             4    Latent Program Network
                                                                 Latent Optimization
                                             Encoder
                                                                                                             Decoder
                                                                         Decoder
                             Figure 1: Inference of the Latent Program Network (LPN) model. (Left): the encoder maps I/O pairs
                             to a latent space of encoded programs. (Middle): the latent program is refined during an optimization
                             process to best explain the given I/O pairs. (Right): the decoder executes the latent program to
                             generate the desired output to a newly given input. The latent optimization figure in the middle comes
                             from the experiment described in the appendix, figure 16.
                             Wepropose the Latent Program Network (LPN), an algorithm that trains a neural network end to
                             end to take a specification of input-output pairs and generate the output of a newly given input. With
                             a focus on abstraction, search, and synthesis, LPN is designed with the ability to perform test-time
                             adaptation explicitly built into the architecture. LPN is composed of three components (see figure 1):
                             (1) a neural network encoder that takes in a specification X  andoutputsanabstractedlatentprogram,
                                                                                         m
                             (2) an optimization process that refines the latent to best explain the data, (3) a decoder neural network
                             that executes the latent program to generate an output.
                             Prior work has focused on directly training models to maximize the likelihood of decoding the correct
                             output given a specification [Kolev et al., 2020]. However, we diverge from such transduction-based
                             methods, which condition on all input-output pairs in the specification to predict an output, as these
                             approachesfacechallengeswhenscalingtospecificationsofdifferentsizesandgeneralizingtounseen
                             tasks. They also offer no inherent ability to adapt at test time. Instead, by explicitly factorizing our
                             system into abstraction generation, latent program synthesis, and output prediction, we aim to build
                             an induction machine that can utilize test-time computation to adapt to unseen instances.
                                                                                5
