                   Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi,             of language models with zero-init attention. ArXiv
                      Dingning Liu, Mukai Li, Xiaoshui Huang, Zhiyong             preprint, abs/2303.16199.
                     Wang, Lu Sheng, LEI BAI, Jing Shao, and Wanli             Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun
                      Ouyang. 2023b. Lamm: Language-assisted multi-               Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan
                      modal instruction-tuning dataset, framework, and            Lu, Kai-Wei Chang, Peng Gao, et al. 2024b. Math-
                      benchmark. In Advances in Neural Information Pro-           verse: Does your multi-modal llm truly see the di-
                      cessing Systems, volume 36, pages 26650–26685.              agrams in visual math problems?       arXiv preprint
                      Curran Associates, Inc.                                     arXiv:2403.14624.
                   Alex Young, Bei Chen, Chao Li, Chengen Huang,               Bo Zhao, Boya Wu, and Tiejun Huang. 2023. Svit:
                      Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng                Scaling up visual instruction tuning. ArXiv preprint,
                      Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi:             abs/2307.04087.
                      Open foundation models by 01. ai. ArXiv preprint,
                      abs/2403.04652.                                          Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma,
                   Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,           Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang,
                      Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan            WenjuanHan,andBaobaoChang.2024. Mmicl: Em-
                     Wang.2024. MM-vet: Evaluating large multimodal               powering vision-language model with multi-modal
                      models for integrated capabilities. In Proceedings of       in-context learning. The Twelfth International Con-
                      the 41st International Conference on Machine Learn-         ference on Learning Representations.
                      ing, volume235ofProceedingsofMachineLearning             BoyuanZheng,BoyuGou,JihyungKil,HuanSun,and
                     Research, pages 57730–57754. PMLR.                           YuSu.2024. Gpt-4v(ision) is a generalist web agent,
                   Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,              if grounded. In Forty-first International Conference
                      RuoqiLiu,GeZhang,SamuelStevens,DongfuJiang,                 onMachineLearning.
                     WeimingRen,YuxuanSun,etal.2024. Mmmu: A                   Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu,
                      massive multi-discipline multimodal understanding           Jason J. Corso, and Jianfeng Gao. 2020. Unified
                      and reasoning benchmark for expert agi. In Pro-             vision-language pre-training for image captioning
                      ceedings of the IEEE/CVF Conference on Computer             and VQA. In Proceedings of the AAAI Conference
                     Vision and Pattern Recognition, pages 9556–9567.             on Artificial Intelligence, 34, pages 13041–13049.
                   MertYuksekgonul, Federico Bianchi, Pratyusha Kalluri,          AAAIPress.
                      DanJurafsky, and James Zou. 2023. When and why           Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
                     vision-language models behave like bags-of-words,            MohamedElhoseiny. 2023. Minigpt-4: Enhancing
                      and what to do about it? In The Eleventh Interna-           vision-language understanding with advanced large
                      tional Conference on Learning Representations.              language models. ArXiv preprint, abs/2304.10592.
                   Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,
                      and Lucas Beyer. 2023. Sigmoid loss for language
                      image pre-training. In Proceedings of the IEEE/CVF
                     International Conference on Computer Vision, pages
                     11975–11986.
                   Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao,
                      Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan,
                      Bin Wang, Linke Ouyang, Songyang Zhang, Wen-
                     wei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue
                      Zhang,WeiLi,JingwenLi,WenhaiWang,HangYan,
                      Conghui He, Xingcheng Zhang, Kai Chen, Jifeng
                      Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2024a.
                      Internlm-xcomposer-2.5: A versatile large vision lan-
                      guage model supporting long-contextual input and
                      output. ArXiv preprint, abs/2407.03320.
                   Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei
                     Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-
                      feng Gao. 2021. Vinvl: Revisiting visual representa-
                      tions in vision-language models. In IEEE Conference
                      on Computer Vision and Pattern Recognition, CVPR
                     2021, virtual, June 19-25, 2021, pages 5579–5588.
                      Computer Vision Foundation / IEEE.
                   Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
                      Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and
                     YuQiao.2023. Llama-adapter: Efficient fine-tuning
                                                                          15146
