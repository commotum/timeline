         Information Processing Systems 2017, pages 5998–
         6008.
        Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
         Bosma,Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,
         and Denny Zhou. 2022. Chain-of-thought prompting
         elicits reasoning in large language models. In Ad-
         vances in Neural Information Processing Systems 35:
         Annual Conference on Neural Information Process-
         ing Systems 2022.
        Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,
         andChristian Szegedy. 2022. Memorizing transform-
         ers. In The Tenth International Conference on Learn-
         ing Representations.
        WenhanXiong,JingyuLiu,IgorMolybog,HejiaZhang,
         Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
         Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
         MadianKhabsa,HanFang,YasharMehdad,Sharan
         Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale,
         Sergey Edunov, Mike Lewis, Sinong Wang, and Hao
         Ma.2023. Effective long-context scaling of founda-
         tion models. CoRR, abs/2309.16039.
        LiangZhao,XiaochengFeng,XiachongFeng,BingQin,
         and Ting Liu. 2023. Length extrapolation of trans-
         formers: A survey from the perspective of position
         encoding. CoRR, abs/2312.17044.
        Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
         Nathan Scales, Xuezhi Wang, Dale Schuurmans,
         Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H.
         Chi. 2023. Least-to-most prompting enables com-
         plex reasoning in large language models. In The
         Eleventh International Conference on Learning Rep-
         resentations.
        DaweiZhu,NanYang,LiangWang,YifanSong,Wen-
         hao Wu, Furu Wei, and Sujian Li. 2024. PoSE: Ef-
         ficient context window extension of LLMs via po-
         sitional skip-wise training. In The Twelfth Interna-
         tional Conference on Learning Representations.
                              596
