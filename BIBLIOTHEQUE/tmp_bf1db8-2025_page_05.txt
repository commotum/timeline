                 which comprises 23 distinct tasks, each requir-      following sections. However, this approach also
                 ing careful design. To ensure our tasks challenge    has some notable limitations. Firstly, the choice
                 frontier models, we adopted a semi-adversarial ap-   of the reference model will unavoidably bias the
                 proach. We selected two strong reference models:     benchmark towards certain types of failure modes.
                 one general-purpose and one specialized in reason-   For instance, had our reference model not used
                 ing. We iteratively increased task difficulty while  code to solve the multi-hop Boolean expressions,
                 keeping in mind the extra skills that we wanted      we might have stopped there, resulting in a task
                 our benchmark to test for, evaluating the reference  too easy for models that appropriately trigger code.
                 models on each new iteration. If a task proved       Wetried to mitigate this as much as possible by
                 insufficiently challenging, we either replaced it    using strong reference models, and by avoiding
                 with another task or added extra types of difficulty over-engineering to the reference model failures.
                 and re-evaluated until the difficulty level was sat- Secondly, since the benchmark is created adver-
                 isfactory. We used Gemini 1.5 Flash (Team et al.,    sarially with respect to the reference models, a
                 2024a)asourgeneral-purposereferencemodeland          fair comparison of the reference and non-reference
                 the Gemini Thinking Experimental model as our        models may not be possible. We expect this limita-
                 reasoning-specialized reference model (initially the tion to be temporary and be resolved when newer
                 December 2024 version but later changed to the       versions of the reference models become available.
                 January 2025 version, known as Gemini-2.0-Flash-        BBEH Mini: Besides reporting results on
                 Thinking-Exp-01-21). These models were chosen        BBEH,wealsoreportresults on a smaller subset
                 for their performance and the speed of generating    called BBEH Mini which contains 460 examples
                 outputs, whichfacilitatedrapiditerationduringtask    overall (20 examples randomly selected from each
                 construction. We iterated on each task until both    task). This subset can be used for faster and cheaper
                 reference models achieved an accuracy below 70%.     experimentations.
                   In most cases, we tried to use the reference mod-
                 els only as a black box that provided feedback on    4 ResultsandAnalyses
                 the difficulty of our tasks. In some cases, how-     Westart by analyzing the BBEH dataset and com-
                 ever, making tasks more difficult required looking   paring it against its counterpart, BBH. We then
                 into the approach adopted by the model. As an        report results on BBEH for various models and
                 example, the original "Boolean Expression" task in   compare their performances. Then, we provide
                 BBHrequired models to evaluate the truth value       someextraanalysis of the results revealing interest-
                 of expressions such as (not True) or False. Our      ing insights about where reasoning-specialized and
                 initial attempt to increase difficulty involved cre- larger models gain more and where they gain less
                 ating longer expressions with significantly more     compared to general-purpose and smaller models
                 clauses. However, our reference model achieved       respectively. We also provide a large body of ob-
                 high accuracy regardless of the number of clauses.   servations and insights from task-specific results in
                 Whileinitially this seemed surprising, upon investi- Appendix A.
                 gating the modelâ€™s approach, we discovered it clev-
                 erly used Python to solve the problem by directly    4.1   BBEHAnalysis
                 evaluating the expression: result = <expression>;
                 print(result). Thus, adding more clauses did not     RequiredAmountofThinking: Manyoftheprob-
                 have much effect in increasing difficulty. Our next  lems in BBH only require few hops of reasoning,
                 step was to prevent the model from using Python.     sometimesnotrequiringagreatamountofthinking.
                 We achieved this by replacing some "True" and        Asaproxyformeasuringthe amount of thinking
                 "False" clauses with sentences that evaluated to the required by BBEH and compare it to BBH, we
                 sametruth value (e.g., replacing "True" with "The    compare the average length of the outputs gener-
                 capital of Canada is Ottawa.").                      ated by a fixed model (Gemini 2.0 Flash) for the
                   Given the similarity of the high-level approach    two datasets. The results are presented in Figure 3.
                 in creating LLM reasoners (architecture, train-      From the figure, we can observe that the average
                 ing phases, etc.), we believe our semi-adversarial   length of the output has significantly increased for
                 benchmark construction can lead to a benchmark       every single one of the tasks in BBEH compared to
                 that is also challenging for non-reference models.   their counterpart in BBH, thus providing evidence
                 This is confirmed by the experimental results in the that the problems in BBEHmayrequiremuchmore
                                                                  26477
