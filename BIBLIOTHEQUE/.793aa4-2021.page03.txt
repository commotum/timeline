                 for Transformers and it has been a popular choice
                 in the followup works (Radford et al., 2018; Devlin
                 et al., 2018). There are two common variations of
                 the absolute position encodings - ﬁxed and learned.
                 2.2.2  Relative Position Encodings
                 Onedrawbackofabsolutepositionencodingisthat
                 it requires ﬁxed length of input sequence and does
                 not directly capture relative positions to each word.
                 Tosolve these problems several relative positions
                 schemes have been proposed.
                   Shawetal. (2018) proposed using relative posi-      Figure 2: Rank of attention matrices: We present a
                 tion encodinginsteadofabsolutepositionencoding,       comparison of the rank of the attention score matrices
                 andaddposition embeddings to the key and option-      of a BERTBASE model with absolute position embed-
                 ally value projections instead of the input. They     dings at input v.s. absolute position embeddings per-
                 show that this new way of encoding position in-       head (DIET-ABS (1)). With additive positional embed-
                 formation leads to better performance on machine      ding at input, the attention matrices have much lower
                                                                       rank, limiting the representative power. This is allevi-
                 translation tasks. Yang et al. (2019) simpliﬁed this  ated by DIET-ABS.
                 by removing the position embeddings in value pro-
                 jections and showed better performance on the lan-    Theorem 1. Let P ∈ Rn×d be the input position
                 guage modeling tasks. Both these approaches use                               n×d
                                                                                        ˆ         p
                 a vector representation to encode position informa-   embedding and P ∈ R          be the layer-wise po-
                                                                       sition embeddings. Let W ,W          ∈ Rd×dh be
                 tion.                                                                             Q     K
                   Raffel et al. (2020) use scalars to encode rela-    the query and key projection matrices with head
                                                                       projection size d , and d     < d ,d and n ≥
                 tive position between query and key indices and                         h        h       p
                                                                                                            >           >
                                                                       d +dp. Let Aa = (X+P)W W (X+P)
                 add directly to the attention scores matrix. They      h                               Q   K
                                                                                             > >      ˆ ˆ>
                                                                       and Ar = XW W X +PP betheatten-
                 further use logarithmic binning of position infor-                     Q    K
                 mation into a ﬁxed number of buckets. All these       tion matrices computed using input and layer-wise
                 relative position methods further share the position  position embeddings respectively. Then for any
                                                                       X,P,W ,W
                 encoding parameters across layers.                             Q     K
                   Recently Ke et al. (2020) hypothesised that the                     rank(A ) ≤ d .
                 cross correlation between position and token em-                               a      h
                 beddings can result in weaker performance of ad-                                 ˆ
                                                                       There exists a choice of X,P,W ,W        such that
                 ditive absolute position embeddings and instead                                        Q     K
                 proposed to add both absolute and relative posi-                 rank(A ) = d +d >d .
                                                                                          r      p     h    h
                 tional information based attention directly in each   Remarks. Thistheoremshowsusthattherankof
                 head. However such cross terms are present in the
                 methodproposedbyShawetal.(2018),whichdoes             attention matrices is constrained with the absolute
                 competitively with other approaches. We instead       position encodings at the input and using per-head
                 hypothesise that position encodings at input limit    position encodings by adding position information
                 the rank of the position attention matrix leading to  to attention matrix directly results in allowing for
                 its poor performance.                                 higher rank attention. See § B for the proof.
                                                                          Adding the position encodings directly to the in-
                 2.3  Limitations of the Input Additive                put further places a constraint on training dynamics
                      Position Embedding                               byforcing gradients to be same for both the input
                 In this section we discuss some limitations of the    token and position embeddings (see § B). Relative
                 de facto way of adding absolute position encodings    position encodings discussed earlier, while address-
                 to the input token embeddings.                        ing some of these concerns, suffer from slower
                   We ﬁrst compare the representation power in         training/inference times (see Table 1) with com-
                 terms of the rank of attention matrices achievable    plex implementations (Shaw et al. (2018); Ke et al.
                 with different position encodings.                    (2020)). In the next section, we present simple posi-
                                                                       tion encoding methods that avoid these limitations.
                                                                   2976
