                             4D-Former: Multimodal4DPanopticSegmentation
                                                  1,3†∗          1,2∗                1,2                   1,2
                                         Ali Athar       EnxuLi         Sergio Casas       RaquelUrtasun
                                            1Waabi    2University of Toronto   3RWTHAachenUniversity
                                                  {aathar, tli, sergio, urtasun}@waabi.ai
                                    Abstract: 4D panoptic segmentation is a challenging but practically useful task
                                    that requires every point in a LiDAR point-cloud sequence to be assigned a se-
                                    mantic class label, and individual objects to be segmented and tracked over time.
                                    Existing approaches utilize only LiDAR inputs which convey limited information
                                    in regions with point sparsity. This problem can, however, be mitigated by uti-
                                    lizing RGB camera images which offer appearance-based information that can
                                    reinforce the geometry-based LiDAR features. Motivated by this, we propose
                                    4D-Former: a novel method for 4D panoptic segmentation which leverages both
                                    LiDARandimagemodalities, and predicts semantic masks as well as temporally
                                    consistent object masks for the input point-cloud sequence. We encode semantic
                                    classes and objects using a set of concise queries which absorb feature informa-
                                    tion from both data modalities. Additionally, we propose a learned mechanism
                                    to associate object tracks over time which reasons over both appearance and spa-
                                    tial location. We apply 4D-Former to the nuScenes and SemanticKITTI datasets
                                    where it achieves state-of-the-art results. For more information, visit the project
                                    website: https://waabi.ai/4dformer.
                                    Keywords: Panoptic Segmentation, Sensor Fusion, Temporal Reasoning, Au-
                                    tonomous Driving
                           1   Introduction
                           Perception systems employed in self-driving vehicles (SDVs) aim to understand the scene both spa-
                           tially and temporally. Recently, 4D panoptic segmentation has emerged as an important task which
                           involves assigning a semantic label to each observation, as well as an instance ID representing each
                           unique object consistently over time, thus combining semantic segmentation, instance segmentation
                           and object tracking into a single, comprehensive task. Potential applications of this task include
                           building semantic maps, auto-labelling object trajectories, and onboard perception. The task is,
                           however, challenging due to the sparsity of the point-cloud observations, and the computational
                           complexity of 4D spatio-temporal reasoning.
                           Traditionally, researchers have tackled the constituent tasks in isolation, i.e., segmenting classes
                           [1, 2, 3, 4], identifying individual objects [5, 6], and tracking them over time [7, 8]. However,
                           combining multiple networks into a single perception system makes it error-prone, potentially slow,
                           andcumbersometotrain. Recently,end-to-endapproaches[9,10,11]for4Dpanopticsegmentation
                           have emerged, but they utilize only LiDAR data which provides accurate 3D geometry, but is sparse
                           at range and lacks visual appearance information that might be important to disambiguate certain
                           classes (e.g., a pedestrian might look like a pole at range). Nonetheless, combining LiDAR and
                           camera data effectively and efficiently is non-trivial as the observations are very different in nature.
                           In this paper, we propose 4D-Former, a novel approach for 4D panoptic segmentation that effec-
                           tively fuses information from LiDAR and camera data to output high quality semantic segmentation
                           labels as well as temporally consistent object masks for the input point cloud sequence. To the
                           best of our knowledge, this is the first work that explores multi-sensor fusion for 4D panoptic point
                           cloud segmentation. Towards this goal, we propose a novel transformer-based architecture that
                              ∗Indicates equal contribution. †Work done while an intern at Waabi.
                           7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.
