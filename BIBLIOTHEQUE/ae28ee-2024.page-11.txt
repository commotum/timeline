           Published as a conference paper at ICLR 2024
           REFERENCES
           Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs
            with graphs. arXiv preprint arXiv:1711.00740, 2017.
           BenAthiwaratkun,SanjayKrishnaGouda,ZijianWang,XiaopengLi,andYuchenTianet.al. Multi-
            lingual evaluation of code generation models, 2023.
           JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,DavidDohan,
            EllenJiang,CarrieCai,MichaelTerry,QuocLe,andCharlesSutton. Programsynthesiswithlarge
            language models, 2021.
           Samuel R. Bowman and George E. Dahl. What will it take to fix benchmarking in natural language
            understanding?, 2021.
           Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald
            Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha,
            MichaelGreenberg,andAbhinavJangda.Multipl-e: Ascalableandextensibleapproachtobench-
            marking neural code generation, 2022.
           Saikat Chakraborty and Baishakhi Ray. On multi-modal learning of editing source code, 2021.
           Saikat Chakraborty, Yujian Li, Matt Irvine, Ripon Saha, and Baishakhi Ray. Entropy guided spec-
            trum based bug localization using statistical language model. arXiv preprint arXiv:1802.06947,
            2018.
           Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, and
            Jared Kaplan et. al. Evaluating large language models trained on code, 2021.
                                     ´
           Tri Dao, DanFu,StefanoErmon,AtriRudra,andChristopherRe. Flashattention: Fastandmemory-
            efficient exact attention with io-awareness. Advances in Neural Information Processing Systems,
            35:16344–16359, 2022.
           Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and
            YuSu. Mind2web: Towards a generalist agent for the web, 2023.
           Tuan Dinh, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, and George
            Karypis. Large language models of code fail at completing code with potential bugs. arXiv
            preprint arXiv:2306.03438, 2023.
           Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng,
            Chaofeng Sha, Xin Peng, and Yiling Lou. Classeval: A manually-crafted benchmark for evaluat-
            ing llms on class-level code generation, 2023.
           Sarah Fakhoury, Saikat Chakraborty, Madan Musuvathi, and Shuvendu K. Lahiri. Towards generat-
            ing functionally correct code edits from natural language issue descriptions, 2023.
           Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan. Automated
            repair of programs from large language models, 2023.
           Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,
            WentauYih,LukeZettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling
            and synthesis, 2023.
           Xiang Gao, Yannic Noller, and Abhik Roychoudhury. Program repair, 2022.
           Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. Automated program repair. Commu-
            nications of the ACM, 62(12):56–65, 2019.
           David Gros, Prem Devanbu, and Zhou Yu. Ai safety subproblems for software engineering re-
            searchers, 2023.
           Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepfix: Fixing common c lan-
            guage errors by deep learning. In Proceedings of the aaai conference on artificial intelligence,
            volume 31, 2017.
                               11
