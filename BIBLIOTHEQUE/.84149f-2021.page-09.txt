               References                                                               [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
                [1] Anurag Arnab, Chen Sun, and Cordelia Schmid. Uniﬁed                      Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
                    graph structured models for video understanding. In ICCV,                Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
                    2021. 1                                                                  vain Gelly, et al. An image is worth 16x16 words: Trans-
                [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.                   formers for image recognition at scale. In ICLR, 2021. 1, 2,
                    Layer normalization. In arXiv preprint arXiv:1607.06450,                 3, 5, 6
                    2016. 3                                                             [18] Quanfu Fan, Chun-Fu Chen, Hilde Kuehne, Marco Pistoia,
                [3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,               andDavidCox. Moreisless: Learningefﬁcient video repre-
                    and Quoc V Le. Attention augmented convolutional net-                    sentations by big-little network and depthwise temporal ag-
                    works. In ICCV, 2019. 1                                                  gregation. In NeurIPS, 2019. 8
                [4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is               [19] Christoph Feichtenhofer. X3d: Expanding architectures for
                    space-time attention all you need for video understanding?               efﬁcient video recognition. In CVPR, 2020. 1, 2, 6, 8
                    In arXiv preprint arXiv:2102.05095, 2021. 2, 3, 4, 8                [20] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
                [5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-                    Kaiming He. Slowfast networks for video recognition. In
                    biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-                  ICCV,2019. 1, 6, 7, 8
                    tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini           [21] Christoph Feichtenhofer, Axel Pinz, and Richard Wildes.
                    Agarwal, et al. Language models are few-shot learners. In                Spatiotemporal residual networks for video action recogni-
                    NeurIPS, 2020. 2                                                         tion. In NeurIPS, 2016. 2, 5, 6
                [6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han               [22] RohitGirdhar,JoaoCarreira,CarlDoersch,andAndrewZis-
                    Hu. Gcnet: Non-localnetworksmeetsqueeze-excitationnet-                   serman. Video action transformer network. In CVPR, 2019.
                    works and beyond. In CVPR Workshops, 2019. 2                             1
                [7] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas                [23] Rohit Girdhar and Deva Ramanan. Attentional pooling for
                    Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-               action recognition. In NeurIPS, 2017. 4
                    end object detection with transformers. In ECCV, 2020. 1,           [24] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-
                    2                                                                        culty of training deep feedforward neural networks. In AIS-
                [8] Joao Carreira and Andrew Zisserman.        Quo vadis, action             TATS, 2010. 6
                    recognition? a new model and the kinetics dataset. In CVPR,         [25] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-
                    2017. 1, 2, 5, 6, 8                                                      ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,
                [9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng                   Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz
                    Yan, and Jiashi Feng. A2-nets: Double attention networks.                Mueller-Freitag, et al.   The” something something” video
                    In NeurIPS, 2018. 2                                                      database for learning and evaluating visual common sense.
               [10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.               In ICCV, 2017. 1, 6
                    Generating long sequences with sparse transformers.        In       [26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
                    arXiv preprint arXiv:1904.10509, 2019. 2                                 Deep residual learning for image recognition.      In CVPR,
               [11] Krzysztof Choromanski, Valerii Likhosherstov, David Do-                  2016. 1, 2, 5
                    han, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter                [27] Dan Hendrycks and Kevin Gimpel. Gaussian error linear
                    Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,                    units (gelus). In arXiv preprint arXiv:1606.08415, 2016. 3
                    et al. Rethinking attention with performers. In ICLR, 2021.         [28] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim
                    2                                                                        Salimans. Axial attention in multidimensional transformers.
               [12] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V.                 In arXiv preprint arXiv:1912.12180, 2019. 4
                    Le. Randaugment: Practical automated data augmentation              [29] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
                    with a reduced search space. In NeurIPS, 2020. 7                         works. In CVPR, 2018. 2
               [13] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,                [30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian
                    Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide                     Weinberger. Deepnetworkswithstochasticdepth. InECCV,
                    Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and                2016. 7
                    Michael Wray.      Rescaling egocentric vision.     In arXiv        [31] Zilong Huang, Xinggang Wang, Lichao Huang, Chang
                    preprint arXiv:2006.13256, 2020. 1, 6                                    Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross
               [14] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob                    attention for semantic segmentation. In ICCV, 2019. 2
                    Uszkoreit, and Łukasz Kaiser. Universal transformers. In            [32] BoyuanJiang, MengmengWang,WeihaoGan,WeiWu,and
                    ICLR, 2019. 2                                                            Junjie Yan. Stm: Spatiotemporal and motion encoding for
               [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,                   action recognition. In ICCV, 2019. 8
                    and Li Fei-Fei. Imagenet: A large-scale hierarchical image          [33] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
                    database. In CVPR, 2009. 2, 5                                            Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video
               [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina                   classiﬁcation with convolutional neural networks. In CVPR,
                    Toutanova. Bert: Pre-training of deep bidirectional trans-               2014. 2, 4
                    formers for language understanding. In NAACL, 2019. 2, 3,           [34] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
                    5                                                                        Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
                                                                                    6844
