                                     Scaling Laws for Neural Language Models
                                                  Jared Kaplan ∗                            SamMcCandlish∗
                                        Johns Hopkins University, OpenAI                          OpenAI
                                                jaredk@jhu.edu                               sam@openai.com
                              TomHenighan                TomB.Brown             BenjaminChess               RewonChild
                                  OpenAI                    OpenAI                   OpenAI                    OpenAI
                          henighan@openai.com          tom@openai.com         bchess@openai.com          rewon@openai.com
                               Scott Gray             Alec Radford              Jeffrey Wu                Dario Amodei
                                OpenAI                   OpenAI                   OpenAI                      OpenAI
                          scott@openai.com         alec@openai.com         jeffwu@openai.com          damodei@openai.com
                                                                      Abstract
                               Westudyempiricalscalinglawsforlanguagemodelperformanceonthecross-entropyloss.
                               The loss scales as a power-law with model size, dataset size, and the amount of compute
                               used for training, with some trends spanning more than seven orders of magnitude. Other
                               architectural details such as network width or depth have minimal effects within a wide
                               range. Simpleequationsgovernthedependenceofoverﬁttingonmodel/datasetsizeandthe
                               dependence of training speed on model size. These relationships allow us to determine the
                               optimalallocationofaﬁxedcomputebudget. Largermodelsaresigniﬁcantlymoresample-
                               efﬁcient, such that optimally compute-efﬁcient training involves training very large models
                               onarelatively modest amount of data and stopping signiﬁcantly before convergence.
        arXiv:2001.08361v1  [cs.LG]  23 Jan 2020
                         ∗Equal contribution.
                       Contributions: Jared Kaplan and Sam McCandlish led the research.  Tom Henighan contributed the LSTM ex-
                       periments. Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer
                       implementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided
                       guidance throughout the project.
