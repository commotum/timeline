                                            TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                Modernversions of these transductive ideas for deep neural           a sequence of input-output pairs (x ,y ),...,(x ,y ) and
                                                                                                                            1   1          k   k
                networks have been widely referred to as test-time train-            a new input x       , an LM can generate the corresponding
                                                                                                     k+1
                ing. In TTT, a model is updated at inference time using              output yˆ     bysampling from:
                                                                                              k+1
                only the current test instance or a small batch of test in-                    yˆ    ∼LM(·|x ,y ,...,x ,y ,x               )
                stances, typically through explicit gradient steps. While test-                  k+1              1   1        k   k   k+1
                time adaptation has been explored for vision models (Sun             While the possibility of in-context learning (ICL) as im-
                et al., 2020) and sequence architectures (Gandelsman et al.,         plicit machine learning simulation is discussed in previous
                                                                                                 ¨
                2022; Sun et al., 2024; Behrouz et al., 2025), its interac-          work(Akyureketal., 2023), empirical evidence shows that
                tion with other techniques for few-shot learning—especially          in-context learning with language models does not always
                in-context learning—remains less understood.                         resemble standard machine learning algorithms (Zhao et al.,
                In this paper, we investigate how to leverage TTT on top             2024; Min et al., 2022b). Furthermore, ICL often strug-
                of standard in-context learning (ICL) to boost performance           gles with novel tasks “out-of-the-box.” For example, large
                on challenging tasks that require reasoning or rule-based            language models exhibit poor performance on datasets like
                generalization. In-context learning is a powerful means              ARC(Opiełkaetal., 2024; Bober-Irizar & Banerjee, 2024).
                of adaptation without parameter updates, guided by short,            2.2. Test-Time Training
                task-specific prompts. We show that combining ICL with
                explicit gradient-based updates on test data can significantly       Test-time training (TTT) enables parametric models to adapt
                improve performance on particularly difficult tasks. Specifi-        during inference through dynamic parameter updates in re-
                cally, our main contributions1 are:                                  sponse to each test input. This approach remains relatively
                                                                                     unexplored in the era of large language models. The gen-
                  1. A systematic analysis of the key components for effec-          eral TTT process is as follows: starting with initial model
                     tive test-time training, including strategies for selecting     parameters θ0, for each test input (or batch of inputs) d, we
                     training data at inference, training objectives, and how        generate a temporary training dataset DTTT.             Wethen
                     TTTinteracts with an LM’s pre-trained parameters and            optimize these parameters to minimize a loss function
                     in-context learning.                                                                      X
                                                                                                  argmin              L(LM(d       ; θ)),
                                                                                                                                TTT
                  2. An application of TTT to two challenging benchmark                               θ     d   ∈D
                     suites—The Abstraction and Reasoning Corpus                                             TTT   TTT
                     (ARC; Chollet, 2019) and BIG-Bench Hard (BBH;                   resulting in temporarily updated parameters θd, which are
                                                                                                                          2
                     Srivastava et al., 2023; Suzgun et al., 2023).                  subsequently used for prediction.
                                                                                     In previous work (e.g., Sun et al., 2020), D         is typically
                                                                                                                                    TTT
                On ARC, our TTT approach outperforms existing open-                  constructed by applying an unsupervised objective (e.g.,
                source neural methods, attaining 53.0% accuracy with an              masked autoencoding) to the input x alone. In this paper,
                8B model and 61.9% when ensembled with a program-                    weextendTTTtothefew-shotlearningsetting, treating it
                synthesis approach (comparable to human performance).                as a form of transductive learning by leveraging few-shot
                OnBBH,TTTyields a 7.3% absolute improvement over                     demonstration examples to improve predictions. Although
                few-shot prompting, achieving 57.8% accuracy. Gains are              TTTcanalsobeappliedtochainofthought(CoT;Weietal.,
                particularly large on tasks involving structural rules or dis-       2022), we focus on direct transduction, where demonstra-
                tribution shifts (e.g., Dyck languages, Ruin names), where           tions consist of input-output pairs (x,y) without intermedi-
                TTTyields20–50percentagepoints of improvement over                   ate reasoning steps or explicit function descriptions.
                standard in-context prompting.                                       The few-shot learning setting we consider provides
                Overall, ourfindingshighlightthatTTTdrasticallyimproves              richer context in the form of demonstration pairs
                LM’sfew-shot learning ability on out-of-distribution tasks.          (x ,y ),...,(x ,y ). One simple method for TTT is Di-
                                                                                        1   1          K K
                                                                                     rect I/O training, where we directly treat each input-output
                                                                                     (x ,y ) pair as training instances. Our key insight is that
                2. Preliminaries                                                        k   k
                                                                                     the few-shot examples can also be used to construct a more
                2.1. In-context Learning                                             robust and expansive D           of synthetic in-context learn-
                                                                                                                TTT
                Atacertain scale, many LMs exhibit the ability to adapt to           ing tasks, allowing for effective model adaptation during
                newtasks without updating their parameters by simply con-                2Note that this use of “test-time training” is related but dis-
                ditioning on input examples or instructions provided. Given          tinct from the one used in recent line of work wherein an RNN’s
                                                                                     hidden state is treated as parameters and the update equation is
                   1Codeanddataareavailable at https://github.com/ekina              interpreted as optimizing a recall-based regression objective (Ravi
                kyurek/marc (ARC) and https://github.com/adamzweiger                 &Larochelle, 2017; Sun et al., 2024; Behrouz et al., 2025; Wang
                /Fewshot-TTT(BBH).                                                   et al., 2025).
                                                                                  2
