                                                            Generative Pretraining from Pixels
               Radford, A., Metz, L., and Chintala, S. Unsupervised rep-       Trinh, T. H., Luong, M.-T., and Le, Q. V. Selﬁe: Self-
                 resentation learning with deep convolutional generative          supervised pretraining for image embedding.         arXiv
                 adversarial networks. arXiv preprint arXiv:1511.06434,           preprint arXiv:1906.02940, 2019.
                 2015.                                                         Uria, B., Murray, I., and Larochelle, H. Rnade: The real-
               Radford, A., Narasimhan, K., Salimans, T., and Sutskever,          valued neural autoregressive density-estimator. In Ad-
                 I. Improving language understanding by generative pre-           vances in Neural Information Processing Systems, pp.
                 training. 2018.                                                  2175–2183, 2013.
               Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and       Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
                 Sutskever, I. Languagemodelsareunsupervisedmultitask             L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
                 learners. 2019.                                                  tion is all you need. In Advances in neural information
                                                                                  processing systems, pp. 5998–6008, 2017.
               Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev-      Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A.
                 skaya, A., and Shlens, J. Stand-alone self-attention in          Extracting and composing robust features with denoising
                 vision models. arXiv preprint arXiv:1906.05909, 2019.            autoencoders. In Proceedings of the 25th international
               Ranzato, M., Szlam, A., Bruna, J., Mathieu, M., Collobert,         conference on Machine learning, pp. 1096–1103, 2008.
                 R., and Chopra, S. Video (language) modeling: a baseline      Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le,
                 for generative models of natural videos. arXiv preprint          Q. V. Unsupervised data augmentation. arXiv preprint
                 arXiv:1412.6604, 2014.                                           arXiv:1904.12848, 2019.
               Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick,     Zagoruyko, S. and Komodakis, N. Wide residual networks.
                 C.L.,Ma,J.,andFergus,R. Biologicalstructureandfunc-              arXiv preprint arXiv:1605.07146, 2016.
                 tion emerge from scaling unsupervised learning to 250         Zeiler, M. D. and Fergus, R. Visualizing and understand-
                 million protein sequences. bioRxiv, pp. 622803, 2019.            ing convolutional networks. In European conference on
               Sandler, M., Baccash, J., Zhmoginov, A., and Howard, A.            computer vision, pp. 818–833. Springer, 2014.
                 Non-discriminative data or weak model? on the relative
                 importance of data and model resolution. In Proceedings       A. Experimental details
                 of the IEEEInternationalConferenceonComputerVision
                 Workshops, pp. 0–0, 2019.                                     A.1. Hyperparameters
               Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N.,     In Table 5, we present the learning rates used to train each
                 Cubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C. Fix-     model in the paper. When using too high a learning rate,
                 match: Simplifying semi-supervised learning with consis-      weobserveanirrecoverable loss spike early on in training.
                 tency and conﬁdence. arXiv preprint arXiv:2001.07685,         Conversely, with too low a learning rate, training is stable
                 2020.                                                         but loss improves slowly and eventually underperforms. As
                                                                               weincrease model size, the irrecoverable loss spike occurs
               Tan, M. and Le, Q. V. Efﬁcientnet: Rethinking model             at even lower learning rates. This motivates our procedure
                 scaling for convolutional neural networks. arXiv preprint     of sequentially searching learning rates from large to small
                 arXiv:1905.11946, 2019.                                       and explains why larger models use lower learning rates
                                                                               than smaller models at ﬁxed input resolution.
               Tarvainen, A. and Valpola, H. Mean teachers are better role     WeusedanAdamβ of0.95instead of the default 0.999
                 models: Weight-averaged consistency targets improve                                 2
                 semi-supervised deep learning results. In Advances in         because the latter causes loss spikes during training. We
                 neural information processing systems, pp. 1195–1204,         did not use weight decay because applying a small weight
                 2017.                                                         decay of 0.01 did not change representation quality.
               Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview     OniGPT-S,wefoundsmallgainsinrepresentation quality
                 coding. arXiv preprint arXiv:1906.05849, 2019.                fromusingﬂoat32insteadofﬂoat16,fromuntyingthetoken
                                                                               embedding matrix and the matrix producing token logits,
               Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny    and from zero initializing the matrices producing token and
                 images: A large data set for nonparametric object and         class logits. We applied these settings to all models.
                 scene recognition. IEEE transactions on pattern analysis      WhentrainingBERTmodels,oneadditionalhyperparameter
                 andmachineintelligence, 30(11):1958–1970, 2008.               is the masking probability, set to 15% in Devlin et al. (2018).
