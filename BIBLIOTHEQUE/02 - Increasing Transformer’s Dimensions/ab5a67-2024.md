# POS-BERT: Point Cloud One-Stage BERT Pre-Training (2022)
Source: ab5a67-2024.pdf

## Core reasons
- The paper proposes POS-BERT as a one-stage BERT pre-training method for point clouds, extending Transformer-style pretraining to point cloud data.
- It processes point clouds by dividing them into patches and feeding them into a standard Transformer encoder, indicating a Transformer adaptation for a higher-dimensional domain.

## Evidence extracts
- "Inspired by BERT and MoCo, we propose POS-BERT, a one-stage BERT pre-training method for point clouds." (Abstract)
- "divide the point cloud into a series of patches, then randomly mask out some patches and feed them into an encoder based on standard transformer." (Section 1 Introduction)

## Classification
Class name: Increasing Transformer's Dimensions
Class code: 2

$$
\boxed{2}
$$
