             10
                             ...                                         ...                                         ...
                             act.
                                                                                    act.                                        act.
                   original
                  Residual             weight               asymmetric            weight                                      weight
                    Unit                                       output 
                                        act.                 activation             act.                                        act.
                                       weight                                     weight                                      weight
                           addition                                   addition                                    addition
                             act.
                                                                                                       pre-activation
                                                                                    act.                                        act.
                                                                                                       Residual Unit
                                       weight                                     weight                                      weight
                                        act.                                        act.                                        act.
                                       weight                                     weight                                      weight
                           addition                                   addition                                    addition
                             act.
                                                                         ...
                                                                                                                     ...
                                       adopt output activation
                                                                                        equivalent to
                             ...
                                         only to weight path
                            (a)                                         (b)                                         (c)
             Figure5. Using asymmetric after-addition activation is equivalent to constructing a
             pre-activation Residual Unit.
             Table 3. Classiﬁcation error (%) on the CIFAR-10/100 test set using the original
             Residual Units and our pre-activation Residual Units.
                             dataset        network                             baseline unit       pre-activation unit
                                            ResNet-110 (1layer skip)                  9.90                    8.91
                           CIFAR-10         ResNet-110                                6.61                    6.37
                                            ResNet-164                                5.93                    5.46
                                            ResNet-1001                               7.61                    4.92
                          CIFAR-100 ResNet-164                                       25.16                   24.33
                                            ResNet-1001                              27.82                   22.71
                  Thedistinction between post-activation/pre-activation is caused by the pres-
             ence of the element-wise addition. For a plain network that has N layers, there
             are N −1 activations (BN/ReLU), and it does not matter whether we think of
             them as post- or pre-activations. But for branched layers merged by addition,
             the position of activation matters.
                  Weexperimentwithtwosuchdesigns:(i)ReLU-onlypre-activation(Fig.4(d)),
             and (ii) full pre-activation (Fig. 4(e)) where BN and ReLU are both adopted be-
             fore weight layers. Table 2 shows that the ReLU-only pre-activation performs
             very similar to the baseline on ResNet-110/164. This ReLU layer is not used in
             conjunction with a BN layer, and may not enjoy the beneﬁts of BN [8].
                  Somehow surprisingly, when BN and ReLU are both used as pre-activation,
             the results are improved by healthy margins (Table 2 and Table 3). In Table 3 we
             report results using various architectures: (i) ResNet-110, (ii) ResNet-164, (iii)
             a 110-layer ResNet architecture in which each shortcut skips only 1 layer (i.e.,
