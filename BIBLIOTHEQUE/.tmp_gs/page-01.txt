        Identity Mappings in Deep Residual Networks
             Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun
                            Microsoft Research
           Abstract Deep residual networks [1] have emerged as a family of ex-
           tremely deep architectures showing compelling accuracy and nice con-
           vergence behaviors. In this paper, we analyze the propagation formu-
           lations behind the residual building blocks, which suggest that the for-
           ward and backward signals can be directly propagated from one block
           to any other block, when using identity mappings as the skip connec-
           tions and after-addition activation. A series of ablation experiments sup-
           port the importance of these identity mappings. This motivates us to
           propose a new residual unit, which makes training easier and improves
           generalization. We report improved results using a 1001-layer ResNet
           on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet
           on ImageNet. Code is available at: https://github.com/KaimingHe/
           resnet-1k-layers.
      1  Introduction
      Deep residual networks (ResNets) [1] consist of many stacked “Residual Units”.
      Each unit (Fig. 1 (a)) can be expressed in a general form:
                          y =h(x)+F(x,W),
                           l    l     l  l
      arXiv:1603.05027v3  [cs.CV]  25 Jul 2016xl+1 = f(yl),
      where xl and xl+1 are input and output of the l-th unit, and F is a residual
      function. In [1], h(xl) = xl is an identity mapping and f is a ReLU [2] function.
         ResNetsthatareover100-layerdeephaveshownstate-of-the-art accuracy for
      several challenging recognition tasks on ImageNet [3] and MS COCO [4] compe-
      titions. The central idea of ResNets is to learn the additive residual function F
      with respect to h(xl), with a key choice of using an identity mapping h(xl) = xl.
      This is realized by attaching an identity skip connection (“shortcut”).
         In this paper, we analyze deep residual networks by focusing on creating a
      “direct” path for propagating information — not only within a residual unit,
      but through the entire network. Our derivations reveal that if both h(xl) and
      f(yl) are identity mappings, the signal could be directly propagated from one
      unit to any other units, in both forward and backward passes. Our experiments
      empirically show that training in general becomes easier when the architecture
      is closer to the above two conditions.
         To understand the role of skip connections, we analyze and compare various
      types of h(xl). We ﬁnd that the identity mapping h(xl) = xl chosen in [1]
