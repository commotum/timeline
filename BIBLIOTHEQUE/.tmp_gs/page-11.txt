                                                                                                11
             2                                      20   2                                       20
                                                                           164, original
                                                                           164, proposed (pre−activation)
                                                   15                                          15
            0.2                                         0.2
           s                                        Test Error (%s                              Test Error (%
                                                   10                                          10
           Training Los                             )  Training Los                             )
           0.02                                         0.02
                                                   5                                           5
                   110, original
                   110, BN after add
           0.002                                   0   0.002                                   0
             0     1    2     3    4    5     6           0    1     2    3    4     5    6
                              Iterations        x 104                     Iterations        x 104
         Figure6. Training curves on CIFAR-10. Left: BN after addition (Fig. 4(b)) using
         ResNet-110. Right: pre-activation unit (Fig. 4(e)) on ResNet-164. Solid lines denote
         test error, and dashed lines denote training loss.
         a Residual Unit has only 1 layer), denoted as “ResNet-110(1layer)”, and (iv)
         a 1001-layer bottleneck architecture that has 333 Residual Units (111 on each
         feature map size), denoted as “ResNet-1001”. We also experiment on CIFAR-
         100. Table 3 shows that our “pre-activation” models are consistently better than
         the baseline counterparts. We analyze these results in the following.
         4.2    Analysis
         Weﬁndtheimpactofpre-activation is twofold. First, the optimization is further
         eased (comparing with the baseline ResNet) because f is an identity mapping.
         Second, using BN as pre-activation improves regularization of the models.
             Ease of optimization. This eﬀect is particularly obvious when training
         the 1001-layer ResNet. Fig. 1 shows the curves. Using the original design in
         [1], the training error is reduced very slowly at the beginning of training. For
         f = ReLU, the signal is impacted if it is negative, and when there are many
         Residual Units, this eﬀect becomes prominent and Eqn.(3) (so Eqn.(5)) is not
         a good approximation. On the other hand, when f is an identity mapping, the
         signal can be propagated directly between any two units. Our 1001-layer network
         reduces the training loss very quickly (Fig. 1). It also achieves the lowest loss
         among all models we investigated, suggesting the success of optimization.
             We also ﬁnd that the impact of f = ReLU is not severe when the ResNet
         has fewer layers (e.g., 164 in Fig. 6(right)). The training curve seems to suﬀer
         a little bit at the beginning of training, but goes into a healthy status soon. By
         monitoring the responses we observe that this is because after some training,
         the weights are adjusted into a status such that yl in Eqn.(1) is more frequently
         above zero and f does not truncate it (xl is always non-negative due to the pre-
         vious ReLU, so yl is below zero only when the magnitude of F is very negative).
         The truncation, however, is more frequent when there are 1000 layers.
