                                                                                                   3
         Units”. The original Residual Unit in [1] performs the following computation:
                                          y =h(x)+F(x,W),                                        (1)
                                           l        l        l    l
                                               xl+1 = f(yl).                                     (2)
         Here x is the input feature to the l-th Residual Unit. W = {W                |       } is a
                 l                                                         l       l,k 1≤k≤K
         set of weights (and biases) associated with the l-th Residual Unit, and K is the
         number of layers in a Residual Unit (K is 2 or 3 in [1]). F denotes the residual
         function, e.g., a stack of two 3×3 convolutional layers in [1]. The function f is
         the operation after element-wise addition, and in [1] f is ReLU. The function h
         is set as an identity mapping: h(xl) = xl.1
             If f is also an identity mapping: xl+1 ≡ yl, we can put Eqn.(2) into Eqn.(1)
         and obtain:
                                          x     =x +F(x,W).                                      (3)
                                            l+1     l        l   l
         Recursively (x       =x +F(x ,W )=x +F(x,W)+F(x ,W ), etc.) we
                          l+2     l+1       l+1   l+1      l       l   l       l+1    l+1
         will have:
                                                     L−1
                                         x =x +XF(x,W),                                          (4)
                                          L      l            i    i
                                                     i=l
         for any deeper unit L and any shallower unit l. Eqn.(4) exhibits some nice
         properties. (i) The feature xL of any deeper unit L can be represented as the
                                                                                          P
         feature xl of any shallower unit l plus a residual function in a form of            L−1F,
                                                                                             i=l
         indicating that the model is in a residual fashion between any units L and l. (ii)
                                     P
         The feature x = x +            L−1F(x ,W ), of any deep unit L, is the summation
                         L      0       i=0      i    i
         of the outputs of all preceding residual functions (plus x0). This is in contrast to
         a “plain network” where a feature xL is a series of matrix-vector products, say,
         QL−1Wix0 (ignoring BN and ReLU).
            i=0
             Eqn.(4) also leads to nice backward propagation properties. Denoting the
         loss function as E, from the chain rule of backpropagation [9] we have:
                                                                 L−1             !
                          ∂E = ∂E ∂xL = ∂E              1+ ∂ XF(x,W) .                           (5)
                          ∂x      ∂x ∂x         ∂x           ∂x            i   i
                             l       L     l       L            l i=l
         Eqn.(5) indicates that the gradient ∂E can be decomposed into two additive
                                                    ∂xl
         terms: a term of ∂E that propagates information directly without concern-
                               ∂xL                                    P        
         ing any weight layers, and another term of ∂E              ∂     L−1F that propagates
                                                             ∂xL    ∂x    i=l
                                                                      l
         through the weight layers. The additive term of ∂E ensures that information is
                                                                 ∂xL
         directly propagated back to any shallower unit l. Eqn.(5) also suggests that it
          1 It is noteworthy that there are Residual Units for increasing dimensions and reducing
            feature map sizes [1] in which h is not identity. In this case the following derivations
            do not hold strictly. But as there are only a very few such units (two on CIFAR and
            three on ImageNet, depending on image sizes [1]), we expect that they do not have
            the exponential impact as we present in Sec. 3. One may also think of our derivations
            as applied to all Residual Units within the same feature map size.
