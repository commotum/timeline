           12
           Table 4. Comparisons with state-of-the-art methods on CIFAR-10 and CIFAR-100
           using “moderate data augmentation” (ﬂip/translation), except for ELU [12] with no
           augmentation. Better results of [13,14] have been reported using stronger data augmen-
           tation and ensembling. For the ResNets we also report the number of parameters. Our
           results are the median of 5 runs with mean±std in the brackets. All ResNets results
           are obtained with a mini-batch size of 128 except † with a mini-batch size of 64 (code
           available at https://github.com/KaimingHe/resnet-1k-layers).
                  CIFAR-10                     error (%)         CIFAR-100                   error (%)
                  NIN [15]                     8.81              NIN [15]                    35.68
                  DSN[16]                      8.22              DSN[16]                     34.57
                  FitNet [17]                  8.39              FitNet [17]                 35.04
                  Highway [7]                  7.72              Highway [7]                 32.39
                  All-CNN [14]                 7.25              All-CNN [14]                33.71
                  ELU[12]                      6.55              ELU[12]                     24.28
                  FitResNet, LSUV [18]         5.84              FitNet, LSUV [18]           27.66
                  ResNet-110 [1] (1.7M)        6.61              ResNet-164 [1] (1.7M)       25.16
                  ResNet-1202 [1] (19.4M)      7.93              ResNet-1001 [1] (10.2M)     27.82
                  ResNet-164 [ours] (1.7M)     5.46              ResNet-164 [ours] (1.7M)    24.33
                  ResNet-1001 [ours] (10.2M)   4.92 (4.89±0.14)  ResNet-1001 [ours] (10.2M)  22.71 (22.68±0.22)
                                            †
                  ResNet-1001 [ours] (10.2M)   4.62 (4.69±0.20)
               Reducingoverﬁtting.Anotherimpactofusingtheproposedpre-activation
           unit is on regularization, as shown in Fig. 6 (right). The pre-activation ver-
           sion reaches slightly higher training loss at convergence, but produces lower test
           error. This phenomenon is observed on ResNet-110, ResNet-110(1-layer), and
           ResNet-164 on both CIFAR-10 and 100. This is presumably caused by BN’s reg-
           ularization eﬀect [8]. In the original Residual Unit (Fig. 4(a)), although the BN
           normalizes the signal, this is soon added to the shortcut and thus the merged
           signal is not normalized. This unnormalized signal is then used as the input of
           the next weight layer. On the contrary, in our pre-activation version, the inputs
           to all weight layers have been normalized.
           5     Results
           ComparisonsonCIFAR-10/100.Table4comparesthestate-of-the-artmeth-
           ods on CIFAR-10/100, where we achieve competitive results. We note that we
           do not specially tailor the network width or ﬁlter sizes, nor use regularization
           techniques (such as dropout) which are very eﬀective for these small datasets.
           Weobtaintheseresults via a simple but essential concept — going deeper. These
           results demonstrate the potential of pushing the limits of depth.
           ComparisonsonImageNet.Nextwereportexperimentalresultsonthe1000-
           class ImageNet dataset [3]. We have done preliminary experiments using the skip
           connections studied in Fig. 2 & 3 on ImageNet with ResNet-101 [1], and observed
           similar optimization diﬃculties. The training error of these non-identity shortcut
           networks is obviously higher than the original ResNet at the ﬁrst learning rate
