                                                                                                                                                                                                                                                5
                                                                             3x3 conv                                                                       3x3 conv
                                                                                    ReLU                                                                           ReLU
                                                                             3x3 conv                                                                       3x3 conv
                                                                                                                                   0.5                 0.5
                                                          addition                                                                      addition
                                                                 ReLU                                                                          ReLU
                                                                               (a) original                                                       (b) constant scaling
                                                                                             3x3 conv                                                                      3x3 conv
                                                                                                    ReLU                                                                           ReLU
                                                                       1x1 conv              3x3 conv                                                 1x1 conv             3x3 conv
                                                                         sigmoid                                                                           sigmoid
                                                                       1-                                                                                1-
                                                          addition                                                                      addition
                                                                 ReLU          (c) exclusive gating                                            ReLU            (d) shortcut-only gating
                                                                             3x3 conv                                                                       3x3 conv
                                                                                    ReLU                                                                           ReLU
                                                         1x1 conv            3x3 conv                                                    dropout            3x3 conv
                                                          addition                                                                      addition
                                                                 ReLU          (e) conv shortcut                                               ReLU            (f) dropout shortcut
                      Figure2. Various types of shortcut connections used in Table 1. The grey arrows
                      indicate the easiest paths for the information to propagate. The shortcut connections
                      in (b-f) are impeded by diﬀerent components. For simplifying illustrations we do not
                      display the BN layers, which are adopted right after the weight layers for all units here.
                      3.1              Experiments on Skip Connections
                      Weexperiment with the 110-layer ResNet as presented in [1] on CIFAR-10 [10].
                      This extremely deep ResNet-110 has 54 two-layer Residual Units (consisting of
                      3×3 convolutional layers) and is challenging for optimization. Our implementa-
                      tion details (see appendix) are the same as [1]. Throughout this paper we report
                      the median accuracy of 5 runs for each architecture on CIFAR, reducing the
                      impacts of random variations.
                                Though our above analysis is driven by identity f, the experiments in this
                      section are all based on f = ReLU as in [1]; we address identity f in the next sec-
                      tion. Our baseline ResNet-110 has 6.61% error on the test set. The comparisons
                      of other variants (Fig. 2 and Table 1) are summarized as follows:
                                Constant scaling. We set λ = 0.5 for all shortcuts (Fig. 2(b)). We further
                      study two cases of scaling F: (i) F is not scaled; or (ii) F is scaled by a constant
                      scalar of 1−λ = 0.5, which is similar to the highway gating [6,7] but with frozen
                      gates. The former case does not converge well; the latter is able to converge,
                      but the test error (Table 1, 12.35%) is substantially higher than the original
                      ResNet-110. Fig 3(a) shows that the training error is higher than that of the
                      original ResNet-110, suggesting that the optimization has diﬃculties when the
                      shortcut signal is scaled down.
