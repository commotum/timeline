# A Survey on Evaluation of Large Language Models (2024)
Source: 9c0189-2024.pdf

## Core reasons
- The paper frames evaluation of large language models as its central theme, reviewing what, where, and how to evaluate these systems across a broad set of tasks and protocols.
- Section 4 catalogs 46 benchmarks that tie datasets to specific evaluation criteria, split into general, downstream, and multi-modal categories, showing the paperâ€™s emphasis on measurement infrastructure.

## Evidence extracts
- "This paper serves as the first comprehensive survey on the evaluation of large language models. As depicted in Figure 1, we explore existing work in three dimensions: 1) What to evaluate, 2) Where to evaluate, and 3) How to evaluate." (p. 39:3)
- "LLMs evaluation datasets are used to test and compare the performance of different language models on various tasks, as depicted in Section 3. These datasets, such as GLUE [200] and SuperGLUE [199], aim to simulate real-world language processing scenarios and cover diverse tasks such as text classification, machine translation, reading comprehension, and dialogue generation. This section will not discuss any single dataset for language models but benchmarks for LLMs. Avarietyofbenchmarkshaveemergedtoevaluatetheirperformance.Inthisstudy,wecompile a selection of 46 popular benchmarks, as shown in Table 7.5 Each benchmark focuses on different aspects and evaluation criteria, providing valuable contributions to their respective domains. For a better summarization, we divide these benchmarks into three categories: benchmarks for general language tasks, benchmarks for specific downstream tasks, and benchmarks for multi-modal tasks." (p. 39:23)

## Classification
Class name: Data, Benchmarks & Measurement
Class code: 4

$$
\boxed{4}
$$
