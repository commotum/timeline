                                                                               Point Primitive Transformer       5
                                Fig.2. (a) The performance gain(MSR-Action3D [25]) with the increase of temporal
                                range. (b) The occupied memory with the increase of temporal range. We take the
                                2080Ti(11GB) GPU as an example. When the GPU memory cap is reached, the max-
                                imum number of frames that can be used is 15, which can only achieve 89% accuracy.
                                state-of-the-art performance on common tasks including 4D semantic segmen-
                                tation and 4D action recognition. Briefly speaking, instead of tracking points,
                                P4Transformer uses a point 4D convolution to encode the spatio-temporal local
                                structures in a point cloud video, and utilize the transformer to capture the
                                global appearance and motion information across the entire video. To motivate
                                the necessity of a new backbone, we conduct a pilot study to understand the
                                constraints of P4Transformer for long-term point cloud video understanding.
                                  – Wefirst experiment with the action recognition task on MSR-Action3D [25]
                                    dataset. We gradually increase the clip length until our GPU memory cap
                                    is reached and examine how well P4transformer performs. The results are
                                    shown in Figure 2.
                                  – We further conduct 4D semantic segmentation experiments on the synthia
                                    4Ddataset [34], to verify the effect of Transformer. Specifically, We removed
                                    the Transformer in P4Transformer and compared it with the full version.
                                     Wecandraw mainly two conclusions from the above experiments. First, as
                                showninFigure 2, P4Transformer achieves better performance as the clip length
                                increases but is soon restricted by the huge memory cost, and it is hard to ap-
                                ply P4Transformer to very long clips. When the GPU memory cap is reached,
                                the performance still keeps its trend of going up, indicating the huge potential
                                of exploring longer-term videos. Second, in synthia4D [34] semantic segmenta-
                                tion task, we find that P4transformer without Transformer can achieve mIoU
                                of 80.3%, which only drops 2.86% compared with original P4Transformer. This
                                result indicates global spatial-temporal context captured by P4Transformer be-
                                comes less useful in 4D dense prediction tasks. This is quite counter-intuitive as
                                the first conclusion indicates the benefit of modeling long-term information. We
                                conjecture that using a flat transformer as in P4Transformer is not effective for
                                long-term spatial-temporal context due to optimization issues.
