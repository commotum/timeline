                             of mean difference between solutions in the two manifolds. We use an L2-norm in the following
                             to quantify the deviations, i.e., L(s ,T r ) = ks ‚àí T r k . Our learning goal is to arrive at a
                                                                     t     t        t       t 2
                             correction operator C(s) such that a solution to which the correction is applied has a lower error
                             than an unmodiÔ¨Åed solution: L(P (C(T r )),T r ) < L(P (T r ),T r ). The correction function
                                                                 s       t        t          s     t      t
                                                                          0        1                0      1
                             C(s|Œ∏) is represented as a deep neural network with weights Œ∏ and receives the state s to infer an
                             additive correction Ô¨Åeld with the same dimension. To distinguish the original phase states s from
                                                                         Àú
                             corrected ones, we denote the latter with s, and we use an exponential notation to indicate a recursive
                             application of a function, i.e.,
                                                         s     =P(P (¬∑¬∑¬∑P (Tr )¬∑¬∑¬∑)) = Pn(Tr ).                                     (1)
                                                          t+n       s   s      s     t            s      t
                             Within this setting, any type of learn-
                             ing method naturally needs to com-
                                                                                                                          Ì†µÌ±†
                                                                                                                           0
                                                                                             Ì†µÌºè
                                                                             Ì†µÌ±°
                                                                                                        Ì†µÌ±°
                             pare states from the source domain               0
                                                                                                         0
                             with the reference domain in order to
                                                                                                                           Ì†µÌ±†
                                                                                                                     Ì†µÌ±†«Å
                                                                                                                            1
                                                                                                                      1
                                                                                                                Ì†µÌ±°
                             bridge the gap between the two solu-                                                Ì†µÌ±õ                   Time
                                                                                                                                Ì†µÌ±†
                             tion manifolds. How the evolution in                                                                2
                                                                                                                      Ì†µÌ±†«Å
                                                                                                                       2
                             the source manifold at training time                                          Ì†µÌ±°
                                                                                                            Ì†µÌ±õ
                                                                        R         Ì†µÌ±°           S
                                                                                                        Ì†µÌ±°
                                                                                   Ì†µÌ±õ
                                                                                                                                    Ì†µÌ±†
                                                                                                         Ì†µÌ±õ
                             is computed, i.e., if and how the cor-                                                      Ì†µÌ±†«Å
                                                                                                                                     3
                                                                                                                             Ì†µÌ∞∂
                                                                                                                          3
                             rector interacts with the PDE, has a       Figure 2: Transformed solutions of the reference sequence
                             profound impact on the learning pro- computed on R (blue) differ from solutions computed on
                             cess and the achievable Ô¨Ånal accuracy.     the source manifold S (orange). A correction function C
                             Wedistinguish three cases: no interac-     (green) updates the state after each iteration to more closely
                             tion, a pre-computed form of interac-      match the projected reference trajectory on S.
                             tion, and a tight coupling via a differ-
                             entiable solver in the training loop.
                               ‚Ä¢ Non-interacting (NON): The learning task purely uses the unaltered PDE trajectories, i.e.,
                                 s      =Pn(Tr )withnevaluationsofP . Thesetrajectories are fully contained in the source
                                   t+n      s     t                           s
                                 manifold S. Learning from these states means that a model will not see any states that deviate
                                 from the original solutions. As a consequence, models trained in this way can exhibit undesirably
                                 strong error accumulations over time. This corresponds to learning from the difference between
                                 the orange and blue trajectories in Fig. 2, and most commonly applied supervised approaches use
                                 this variant.
                               ‚Ä¢ Pre-computed interaction (PRE): To let an algorithm learn from states that are closer to those
                                 targeted by the correction, i.e., the reference states, a pre-computed or analytic correction is
                                 applied. Hence, the training process can make use of phase space states that deviate from those in
                                 S,asshowningreeninFig.2,toimproveinferenceaccuracyandstability. This approach can
                                 be formulated as s       =(P C )n(Tr )withapre-computedcorrectionfunctionC . Inthis
                                                      t+n       s pre       t                                                pre
                                 setting, the states s are corrected without employing a neural network, but they should ideally
                                 resemble the states achievable via the learned correction later on. As the modiÔ¨Åed states s are not
                                 inÔ¨Çuenced by the learning process, the training data can be pre-computed. A correction model
                                                        Àú
                                 C(s|Œ∏) is trained via s that replaces Cpre at inference time.
                               ‚Ä¢ Solver-in-the-loop (SOL): By integrating the learned function into a differentiable physics
                                 pipeline, the corrections can interact with the physical system, alter the states, and receive
                                 gradients about the future performance of these modiÔ¨Åcations. The learned function C now
                                 dependsonstates that are modiÔ¨Åed and evolved through P for one or more iterations. A trajectory
                                                                       Àú               n                Àú
                                 for n evaluations of P is given by s        =(P C) (Tr ),withC(s|Œ∏). The key difference with
                                                         s               t+n       s         t
                                                                        Àú
                                 this approach is that C is trained via s, i.e., states that were affected by previous evaluations of C,
                                                Àú
                                 and it affects s in the following iterations. As for (PRE), this learning setup results in a trajectory
                                 like the green one shown in Fig. 2, however, in contrast to before, the learned correction itself
                                 inÔ¨Çuences the evolution of the trajectory, preventing a gap for the data distribution of the inputs.
                             In addition to these three types of interaction, a second central parameter is the look-ahead tra-
                             jectory per iteration and mini-batch of the optimizer during learning. A subscript n denotes the
                             numberofsteps over which the future evolution is recursively evaluated, e.g., SOL . The objective
                                                                                                                      n
                             function, and hence the quality of the correction, is evaluated with the training goal to minimize
                             P
                                t+nL(s ,r ). Below, we will analyze a variety of learning methodologies that are categorized via
                                i=t      i  i
                             learning methodology (NON, PRE or SOL) and look-ahead horizon n.
                                                                                  4
