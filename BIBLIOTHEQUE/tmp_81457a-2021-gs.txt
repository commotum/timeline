=== Page 1 ===
                                        Point 4D Transformer Networks for Spatio-Temporal Modeling
                                                                                        in Point Cloud Videos
                                                                   HeheFan                                                                    Yi Yang
                                                        School of Computing                                                                    ReLER
                                              National University of Singapore                                         University of Technology Sydney
                                                                                               MohanKankanhalli
                                                                                             School of Computing
                                                                                   National University of Singapore
                                                        Abstract
                        Point cloud videos exhibit irregularities and lack of or-
                   deralongthespatialdimensionwherepointsemergeincon-
                   sistently across different frames. To capture the dynamics                                                coordinates            coordinates + features       features        max pooling
                   in point cloud videos, point tracking is usually employed.
                                                                                                                                                          Ì†µÌ∞∂
                   However, as points may Ô¨Çow in and out across frames,
                                                                                                                               Ì†µÌ±ë
                                                                                                                                       Ì†µÌ±í
                                                                                                                           Ì†µÌ±é
                                                                                                                                   Ì†µÌ±è
                                                                                                                               Ì†µÌ±é
                                                                                                                                       Ì†µÌ±ê
                                                                                                                           Ì†µÌ±è
                                                                                                                                   Ì†µÌ±ë                                                                high
                                                                                                                                                                               Ì†µÌ±é
                                                                                                                                                                                   Ì†µÌ±ë
                                                                                                                                                         Ì†µÌ±é
                   computing accurate point trajectories is extremely difÔ¨Åcult.                                                                             Ì†µÌ±ë
                                                                                                                      Ì†µÌ±Å
                                                                                                                                                      ‚Ä≤
                                                                                                                               Ì†µÌ±è
                                                                                                                                       Ì†µÌ±ì
                                                                                                                           Ì†µÌ±ê
                                                                                                                                   Ì†µÌ±ê                                                               throw
                                                                                                                                                                               Ì†µÌ±ê
                                                                                                                                                                                   Ì†µÌ±í
                                                                                                                                                         Ì†µÌ±ê
                                                                                                                                                            Ì†µÌ±í
                   Moreover, tracking usually relies on point colors and thus                                                                       Ì†µÌ±Å
                                                                                                                               Ì†µÌ±ê
                                                                                                                                       Ì†µÌ±ë
                                                                                                                           Ì†µÌ±ë
                                                                                                                                   Ì†µÌ±í
                                                                                                                                                                               Ì†µÌ±ë
                                                                                                                                                         Ì†µÌ±ë                        Ì†µÌ±ê
                                                                                                                                                            Ì†µÌ±ê
                   may fail to handle colorless point clouds. In this paper, to                                                                            ‚Ä≤                                      3D action
                                                                                                                                 Ì†µÌ∞ø
                                                                                                                                                          Ì†µÌ∞ø
                   avoid point tracking, we propose a novel Point 4D Trans-                                              point cloud video       point 4D convolution        transformer         recognition 
                   former (P4Transformer) network to model raw point cloud
                   videos. SpeciÔ¨Åcally, P4Transformer consists of (i) a point                                        Figure 1. Illustration of point cloud video modeling by our Point
                   4D convolution to embed the spatio-temporal local struc-                                          4DTransformer (P4Transformer) network. Color encodes depth.
                   tures presented in a point cloud video and (ii) a transformer                                     Apoint cloud video is a sequence of irregular and unordered 3D
                   to capture the appearance and motion information across                                           coordinate sets. Points in different frames are not consistent. Our
                   the entire video by performing self-attention on the embed-                                       P4Transformer consists of a point 4D convolution and a trans-
                   ded local features. In this fashion, related or similar local                                     former. Theconvolutionencodesapointcloudvideo(3√óL√óN),
                   areas are merged with attention weight rather than by ex-                                         where L and N denote the number of frames and the number of
                                                                                                                     points in each frame, to a coordinate tensor (3 √ó L‚Ä≤ √ó N‚Ä≤) and
                   plicit tracking. Extensive experiments, including 3D action                                       a feature tensor (C √ó L‚Ä≤ √ó N‚Ä≤). The transformer performs self-
                   recognition and 4D semantic segmentation, on four bench-                                          attention on the embedded tensors to capture the global spatio-
                   marks demonstrate the effectiveness of our P4Transformer                                          temporal structure across the entire point cloud video.
                   for point cloud video modeling.
                                                                                                                     Essentially, a point cloud video is a sequence of 3D coordi-
                   1. Introduction                                                                                   natesets. Whenpointcolorsareavailable,theyareoftenap-
                                                                                                                     pendedasadditionalfeatures. However, becausecoordinate
                        Point cloud videos are a rich source of visual informa-                                      sets are irregular and unordered, and points emerge incon-
                   tion and can be seen as a window into the dynamics of the                                         sistently across different sets/frames, modeling the spatio-
                   3D world we live in, showing how objects move against                                             temporal structure in point cloud videos is extremely chal-
                   backgrounds and what happens when we perform an ac-                                               lenging.
                   tion. Moreover, point cloud videos provide more Ô¨Çexibility                                             In order to capture the dynamics from point clouds, one
                   for action recognition in poor visibility environments, and                                       solution is to Ô¨Årst convert a point cloud video into a se-
                   covers more precise geometry dynamics than conventional                                           quence of regular and ordered voxels and then apply con-
                   videos. Therefore, understanding point cloud videos is im-                                        ventional grid based convolutions to these voxels. However,
                   portant for intelligent systems to interact with the world.                                       as points are usually sparse, directly performing convolu-
                                                                                                                14204

=== Page 2 ===
              tionsontheentirespacealongthetimedimensioniscompu-                  spatio-temporal modeling in RGB/RGBD videos. To cap-
              tationally inefÔ¨Åcient. Therefore, special engineering efforts,      ture the complementary information about appearance and
                                        6], are usually needed. Moreover,                                                               49, 56]
              e.g., sparse convolution [                                          motion, two-stream convolutional neural networks [
              voxelization requires additional computation [59], which            useaspatialstreamandanopticalÔ¨Çowstreamforvideoun-
              restricts applications that require real-time processing. An-       derstanding. As video is a kind of sequence, recurrent neu-
              other solution is to directly model raw point cloud videos          ral networks [19, 7, 65] are employed to capture the tempo-
              by grouping local points, in which point tracking is em-            ral dependencies [38, 13]. Similar to recurrent neural net-
              ployed to preserve the temporal structure [36]. However, as         works, 1D convolutional neural networks [24] can also be
              points may Ô¨Çow in and out across frames, accurately track-          used to model the temporal structure across frame features.
              ing points is extremely difÔ¨Åcult. In particular, when videos        Besides, pooling techniques [12] are also employed to se-
              become long, the tracking error increases. Moreover, point          lect and merge frames into a global video representation. In
              tracking usually requires point colors. It may fail to handle       addition, 3D convolutional neural networks [51, 4, 52] can
              colorless point clouds when point colors are not available.         directly learn spatio-temporal representations from videos
                 In this paper, to avoid tracking points, we propose a            by stacking 2D frames into 3D pixel tensors. Meanwhile,
              novel Point 4D Transformer Network (P4Transformer) to               interpretable video or action reasoning methods [66, 16]
              model the spatio-temporal structure in raw point cloud              are proposed by explicitly parsing changes in videos. For
              videos. First, we develop a point 4D convolution to en-             RGBDvideos, grid based methods are also widely used to
              code the spatio-temporal local structures in a point cloud          fuse RGB and depth information [30, 20].
              video. Our point 4D convolution is directly performed on            DeepLearningonStaticPointClouds. Deeplearninghas
              rawpointswithoutvoxelization and therefore saves compu-             been widely used in many point cloud problems, such as
              tation time. Moreover, by merging local points along the            classiÔ¨Åcation, object part segmentation, scene semantic seg-
              spatial and temporal dimensions, point 4D convolution re-           mentation[42,43,29,60,50],reconstruction[8,63,27]and
              duces the number of points to be processed by the subse-            object detection [5, 41]. Most recent works aim to process
              quent transformer. Second, instead of grouping these em-            rawpoint clouds without converting point clouds into regu-
              bedded local areas with tracking [36], we propose to uti-           lar voxels. However, these methods mainly focus on static
              lize the transformer to capture the global appearance and           point clouds and do not take the temporal dynamics of point
              motion information across the entire video. By performing           clouds into account.
              self-attention [53], related local areas are adaptively merged      Point Cloud Video Processing. Point cloud video mod-
              based on the attention weight.                                      eling is a fairly new task but very important for intelligent
                 Weevaluate our P4Transformer on a video-level classi-            agents to understand the dynamic 3D world we live in. Two
              Ô¨Åcation task, i.e., 3D action recognition, and a point-level        major categories of methods have been explored. The Ô¨Årst
              prediction task, i.e., 4D semantic segmentation. Experi-            one is based on voxelization. For example, Fast and Fu-
              ments on the MSR-Action3D [28], NTU RGB+D 60 [45],                  rious (FaF) [37] converts 3D point cloud frames into 2D
              NTURGB+D120[30]andSynthia4D[6]datasetsdemon-                        bird‚Äôs view voxels and then extracts features via 3D convo-
              strate the effectiveness of our method. The contributions of        lutions. MinkowskiNet [6] uses 4D Spatio-Temporal Con-
              this paper are threefold:                                           vNetstoextractappearanceandmotionfrom4Doccupancy
                ‚Ä¢ To avoid point tracking, we propose a transformer               voxel grids. 3DV [59] Ô¨Årst employs a temporal rank pool-
                   based network, named P4Transformer, for spatio-                ing to merge point motion into a voxel set and then applies
                   temporal modeling of raw point cloud videos. To the            PointNet++ [43] to extract the spatio-temporal representa-
                   best of our knowledge, we are the Ô¨Årst to apply trans-         tion from the set. The second category is directly performed
                   former in point cloud video modeling.                          on raw points. For example, Fan and Yang [14] proposed a
                ‚Ä¢ To embed spatio-temporal local structures and reduce            series of point recurrent neural networks (PointRNNs) for
                   the number of points to be processed by transformers,          moving point cloud prediction. MeteorNet [36] appends a
                   weproposeapoint4Dconvolution.                                  temporal dimension to PointNet++ to process 4D points,
                                                                                  in which a point tracking based chained-Ô¨Çow grouping is
                ‚Ä¢ Extensive experiments on four datasets show that the            used when merging points. PSTNet [15] constructs the
                   proposed P4Transformer effectively improves the ac-            spatio-temporal hierarchy to alleviate the requirement of
                   curacy of 3D action recognition and 4D semantic seg-           point tracking. Our P4Transformer belongs to the second
                   mentation.                                                     category, but aims to avoid point tracking when capturing
              2. Related Work                                                     spatio-temporalcorrelationacrossentirepointcloudvideos.
                                                                                  Transformer Networks.         Self-attention based architec-
                                                                                  tures, Transformers [
              Spatio-Temporal Modeling in Grid based Videos. Deep                                       53, 9] in particular, have substantially
              neural networks have achieved excellent performance on              helpedadvanceinnaturallanguageprocessing. Incomputer
                                                                              14205

=== Page 3 ===
                                                     a) 3D Action Recognition                                                                                                                                                                                                                                                                                                                                                          b) 4D Semantic Segmentation
                                                              Class                                                                                                                                 Ì†µÌ±ö √ó
                                                        drink water                                      MLP                                                         max 
                                                            pick up                                                                                            pooling 
                                                        high throw
                                                                                                                                                                                                                                                                                                                                                                                                                                                Ì†µÌ±Å
                                                                  ‚ãØ                                                                                                                                                                                                                         Transformer                                                                                                                                                                           MLP
                                                                                                                           4D Coordinate + Local Feature
                                                                                                                                                  Embedding
                                                                                                                                                                                                          Ì†µÌ±•                                    Ì†µÌ±•                                    Ì†µÌ±•                                     Ì†µÌ±•                                    Ì†µÌ±•
                                                                                                                                                                                                                                                                                                                                                                                                         Ì†µÌ±•
                                                                                                                                                                                                            Ì†µÌ±é                                    Ì†µÌ±è                                     Ì†µÌ±ê                                    Ì†µÌ±ë                                    Ì†µÌ±í
                                                                                                                                                                                                                                                                                                                                                                                                           Ì†µÌ±ì
                                                                                                                                                                                                                                                                                                                                                                                                                                                  Ì†µÌ±Å
                                                                                                               Ì†µÌ±†
                                                                                                                                                                                                          Ì†µÌ±¶                                    Ì†µÌ±¶                                     Ì†µÌ±¶                                    Ì†µÌ±¶                                    Ì†µÌ±¶
                                                                                                                 Ì†µÌ±°
                                                                                                                                                                                                                                                                                                                                                                                                         Ì†µÌ±¶
                                                                                                                                                                                                            Ì†µÌ±é                                    Ì†µÌ±è                                     Ì†µÌ±ê                                    Ì†µÌ±ë                                    Ì†µÌ±í
                                                                                                                                                                                                                 a                                     b                                      c                                     d                                      e                               Ì†µÌ±ì     f
                                                                         Ì†µÌ±ü                       Ì†µÌ±ü                       Ì†µÌ±ü                       Ì†µÌ±ü
                                                                                                                                                                                                          Ì†µÌ±ß                                    Ì†µÌ±ß                                     Ì†µÌ±ß                                    Ì†µÌ±ß                                    Ì†µÌ±ß
                                                                           Ì†µÌ±°                        Ì†µÌ±°                      Ì†µÌ±°                       Ì†µÌ±°
                                                                                                                                                                                                                                                                                                                                                                                                         Ì†µÌ±ß
                                                                                                                                                                                                            Ì†µÌ±é                                    Ì†µÌ±è                                     Ì†µÌ±ê                                    Ì†µÌ±ë                                    Ì†µÌ±í
                                                                                                                                                                                                                                                                                                                                                                                                           Ì†µÌ±ì                                        Feature Propagation
                                                                                                                                                                                                          Ì†µÌ±°                                    Ì†µÌ±°                                     Ì†µÌ±°                                     Ì†µÌ±°                                   Ì†µÌ±°
                                                                                                                                                                                                            2                                     2                                     2                                      4                                     4
                                                     Ì†µÌ±°                       Ì†µÌ±°                       Ì†µÌ±°                      Ì†µÌ±°                       Ì†µÌ±°
                                                                                                                                                                                                                                                                                                                                                                                                         Ì†µÌ±°
                                                                                       a                                                                                                                                                                                                                                                                                                                   4
                                                       1                        2                        3                        4           d 5
                                                                                                                                                                                                                                                                                                                                                                                                                                                       ‚Ä≤
                                                                                                                                                                                                                                                                                                                                                                                                                                                  Ì†µÌ±Å
                                                                                                                                                                                                                                                                                                                                      Ì†µÌ±§
                                                                                                                                                                                                                                                                                             Ì†µÌ±§
                                                                                                                                                                                                           1√ó
                                                                                                                                                                                                                                                                                                                                          7
                                                                                                                                             e                                                                                                                                                                       Ì†µÌ±§
                                                                             b                                                                                                                                                                                                                   1
                                                                                                                                                                                                                                                                                                                         4                                                         Point 4D                                                                         Transformer
                                                                                                                                                                                                                                                                                                                                          Ì†µÌ±§
                                                                                                                                                                                                                                                                                                                                              8
                                                                                                                                                                                                                                                                                         Ì†µÌ±§
                                                                                           c                                    f                                                                                                                                                            2
                                                                                                                                                                                                                                                                                                                        Ì†µÌ±§
                                                                                                                                                                                                                                                                                                                            5
                                                                                                                                                                                                                                                                                                                                        Ì†µÌ±§
                                                                                                                                                                                                                                                                                                                                                                                                                                                       ‚Ä≤
                                                                                                                                                                                                                                                                                                                                            9                                Convolution
                                                                                                                                                                                                                                                                                                                                                                                                                                                  Ì†µÌ±Å
                                                                                                                                                                                                                                                                                                        Ì†µÌ±§
                                                                                                                                                                                                                                                                                                            6
                                                                                                                                                                                                                                                                                  Ì†µÌ±§
                                                                                                                                                                                                                                                                                      3                                                                                                                                                            Point 4D Convolution
                                                                                       a                                                d
                                                                                    b                                                  e                                                                                                                                                                                                                                                                                                          Ì†µÌ±Å
                                                                                      c                                               f                                                                       a                                      b                                      c                                     d                                      e                                      f
                                                                                    Ì†µÌ±°                                               Ì†µÌ±°                                                                                                                  Spatio-Temporal Local Areas
                                                                                       2                                                4
                                                 Figure 2. Illustration of the proposed Point 4D Transformer (P4Transformer) networks. a) 3D action recognition. (1) Based on temporal
                                                 radius(rt), temporalstride(st), spatial radius (rs) and spatial subsampling rate (ss), we construct a few spatio-temporal local areas. (2) Our
                                                 point 4D convolution encodes each spatio-temporal local area to a feature vector. (3) 4D coordinates are integrated into the corresponding
                                                 local features by an embedding layer. (4) Our transformer performs self-attention on spatio-temporal local features and each local feature
                                                 is updated by adding more information from similar or related areas. (5) A max pooling merges local features to a global feature, which is
                                                 then mapped to action predictions by an MLP. b) 4D semantic segmentation. After the transformer, feature propagation layers recover the
                                                 subsampled N‚Ä≤ points to the original N points by interpolating features. Finally, an MLP maps interpolated features to point predictions.
                                                 vision, the community has used self-attention to capture                                                                                                                                                                                                  ferent point features (e.g., color, density and remission) may
                                                 non-local correlation [58, 3, 22, 10, 11] or leverage univer-                                                                                                                                                                                             be involved, we assume that Ft is available. Given a point
                                                 sal features [34]. Inspired by these methods, to avoid point                                                                                                                                                                                              cloud video  [P1;F1],[P2;F2],¬∑¬∑¬∑ ,[PL;FL], where L is
                                                 tracking, we employ a transformer to capture the spatio-                                                                                                                                                                                                  the numberofframes,weproposeapoint4Dconvolutionto
                                                 temporal structure of raw point cloud videos.                                                                                                                                                                                                             extract local structure and subsample points to be processed
                                                                                                                                                                                                                                                                                                           by the subsequent transformer, generating an encoded se-
                                                 3. Point 4D Transformer Networks                                                                                                                                                                                                                          quence  [P‚Ä≤;F‚Ä≤],[P‚Ä≤;F‚Ä≤],¬∑¬∑¬∑ ,[P‚Ä≤‚Ä≤;F‚Ä≤‚Ä≤], where P‚Ä≤ ‚àà
                                                                                                                                                                                                                                                                                                                                                        1              1                    2              2                                       L                  L                                                        t
                                                                                                                                                                                                                                                                                                                  3√óN‚Ä≤                                          ‚Ä≤                     C‚Ä≤√óN‚Ä≤                                                                    ‚Ä≤                                                ‚Ä≤
                                                                                                                                                                                                                                                                                                           R                            and Ft ‚àà R                                                            . Usually, N < N and L < L.
                                                            In this section, we describe the proposed Point 4D                                                                                                                                                                                                                                                                                                                                                               23, 18, 51, 4, 17]
                                                                                                                                                                                                                                                                                                                      Conventional grid based convolutions [
                                                 Transformer (P4Transformer) network in detail.                                                                                                                                                                       Our                                  have proven to be useful local structure modeling. The key
                                                 P4Transformer consists of a point 4D convolution and a                                                                                                                                                                                                    in convolution is to learning the kernel for all displacements
                                                 transformer.                                            In Section 3.1, we present how the point                                                                                                                                                          (including direction and magnitude) from a center grid to
                                                 4Dconvolution encodes spatio-temporal local structures in                                                                                                                                                                                                 its neighbor grids, which is then applied to grid features to
                                                 point cloud videos. In Section 3.2, we introduce the trans-                                                                                                                                                                                               capture the local structure. For example, a 3D convolution
                                                 former, which aims to capture the appearance and motion                                                                                                                                                                                                   with a kernel size (3,3,3) can capture the local structure
                                                 information across entire point cloud videos. Finally, we                                                                                                                                                                                                 from 3√ó3√ó3=27displacements. Inspiredbytraditional
                                                 show how to apply our P4Transformer to 3D action recog-                                                                                                                                                                                                   convolutions, our point 4D convolution is formulated as fol-
                                                 nition and 4D semantic segmentation in Section 3.3.                                                                                                                                                                                                       lows:
                                                                                                                                                                                                                                                                                                                  ‚Ä≤(x,y,z)                                               X                                                                                                       (x+Œ¥ ,y+Œ¥ ,z+Œ¥ )
                                                 3.1. Point 4D Convolution                                                                                                                                                                                                                                 F                                 =                                                             Œ∂ Œ¥ ,Œ¥ ,Œ¥ ,Œ¥ )¬∑F                                                                     x                  y                  z
                                                                                                                                                                                                                                                                                                                 t                                                                                                     x            y           z           t                  t+Œ¥
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           t
                                                                                                                                                                                                                                                                                                                                                      (Œ¥ ,Œ¥ ,Œ¥ ,Œ¥ )‚ààG
                                                                                                            3√óN                                                                 C√óN                                                                                                                                                                         x y z t
                                                            Let Pt ‚àà R                                                          and Ft ‚àà R                                                            denote the point co-
                                                                                                                                                                                                                                                                                                                               rt
                                                                                                                                                                                                                                                                                                                            X                                           X                                                                                                        (x+Œ¥ ,y+Œ¥ ,z+Œ¥ )
                                                 ordinates and features of the t-th frame in a point cloud                                                                                                                                                                                                   =                                                                                             Œ∂ Œ¥ ,Œ¥ ,Œ¥ ,Œ¥ )¬∑F                                                                     x                  y                  z ,
                                                 video, where N and C denote the number of points and                                                                                                                                                                                                                                                                                                                  x            y           z           t                  t+Œ¥t
                                                                                                                                                                                                                                                                                                                       Œ¥t=‚àírt ||(Œ¥ ,Œ¥ ,Œ¥ )||‚â§r
                                                                                                                                                                                                                                                                                                                                                              x y z                                   s
                                                 feature channels, respectively. Note that, because the MSR-                                                                                                                                                                                                                                                                                                                                                                                                                         (1)
                                                 Action3D [28], NTU RGB+D 60 [45] and NTU RGB+D
                                                 120 [30] datasets do not provide corresponding point col-                                                                                                                                                                                                 where (x,y,z) ‚àà Pt and (Œ¥x,Œ¥y,Œ¥z,Œ¥t) represents spatial-
                                                 ors, Ft is not provided. For Synthia 4D [6], the point colors                                                                                                                                                                                             temporal displacement and ¬∑ is matrix multiplication.
                                                 are provided and F is thus available. As we aim to develop                                                                                                                                                                                                F(x,y,z) ‚àà RC√ó1 denotes the feature of point at position
                                                                                                                          t                                                                                                                                                                                      t
                                                 a generic network for point cloud video processing and dif-                                                                                                                                                                                               (x,y,z,t) in the point cloud video. The P can be imple-
                                                                                                                                                                                                                                                                                             14206

=== Page 4 ===
              mented with different pooling methods, i.e., sum-pooling,            Because similar local regions share similar representations,
              max-pooling and average-pooling. G is the spatio-temporal            we can merge related areas based on their similarities in-
              local region around point (x,y,z,t). Note that, because              stead of explicit tracking. Moreover, because point posi-
              space and time are orthogonal and independent of each                tions also reÔ¨Çect the relationship among local regions, we
              other, we can split the region G into a sequence of spatial          can exploit them to enhance representations for performing
              areas, which is deÔ¨Åned by a spatial radius rs and a temporal         self-attention. Therefore, we combine anchor coordinates,
              radius rt.                                                           i.e., (x,y,z,t), and local area features as the input to our
                 Because displacements in grid data are discrete and reg-          transformer,
              ular, traditional convolutions can directly learn a kernel for                  I(x,y,z,t) = W ¬∑ (x,y,z,t)T + F‚Ä≤(x,y,z),          (3)
              all displacements within a region. However, point coordi-                                      i                  t
                                                                                                   C‚Ä≤√ó4
              natesarecontinuousandirregular,andthenumberofpoten-                  whereWi ‚ààR            is the weight to convert 4D coordinates
                                                                                               C‚Ä≤√óL‚Ä≤N‚Ä≤
              tial displacements is inÔ¨Ånite. Therefore, we propose to indi-        and I ‚àà R            is the self-attention input.
              rectly generate a kernel by a function h, instead of directly
              learning the kernel. SpeciÔ¨Åcally, Œ∂ : R1√ó4 ‚Üí RC‚Ä≤√óC is a
                                                                                   3.2.2   Self-Attention
              parameterizedfunctionof(Œ¥ ,Œ¥ ,Œ¥ ,Œ¥ )togeneratekernels
                                            x   y  z   t
              based on input displacements:                                        Given I, we aim to merge related local areas based on their
                                                                                   similarities so that each point has a larger receptive Ô¨Åeld to
                                                          T           
               Œ∂ Œ¥x,Œ¥y,Œ¥z,Œ¥t)¬∑f = Wd¬∑(Œ¥x,Œ¥y,Œ¥z,Œ¥t)           ‚äô Wf¬∑f , (2)          perceive what happens around it. To this end, we perform
                              (x+Œ¥ ,y+Œ¥ ,z+Œ¥ )                                     self-attention [53] on I. SpeciÔ¨Åcally, the self-attention can
              where f = F         x     y    z  is the point feature, W    ‚àà
                             t+Œ¥t                                        d         bedescribedasmappingaqueryandasetofkey-valuepairs
                C‚Ä≤√ó4                                                   C‚Ä≤√óC        to an output, where the queries, keys and values are gen-
              R       is to transform 4D displacements, W         ‚àà R
                                                                f
              aims to increase point feature dimension to improve the              erated by the input itself and the output is computed as a
              feature representation ability, and ‚äô is an element-wise             weighted sum of the values:
              operator, e.g., addition or product.         In this fashion,
                                                                                             Q=W ¬∑I, K=W ¬∑I, V =W ¬∑I,
              Œ∂ Œ¥ ,Œ¥ ,Œ¥ ,Œ¥ ) can generate kernels for all potential dis-                             q               k              v
                  x  y   z   t                                                                                                T
              placements. When Ft is not available, the function is im-                                                     Q ¬∑K                (4)
              plemented as Œ∂ Œ¥ ,Œ¥ ,Œ¥ ,Œ¥ ) = W ¬∑ (Œ¥ ,Œ¥ ,Œ¥ ,Œ¥ )T. We                           attention(Q,K) = softmax( ‚àöCk ),
                                x   y   z  t        d    x   y   z  t
              canalsoappendamultilayerperceptron(MLP)toEq.(2)to                              O=V¬∑attention(Q,K),
              enhance the modeling.                                                                         Ck√óC‚Ä≤                 Cv√óC‚Ä≤          k
                                                                                   where W ,W          ‚àà R         ,  W ‚àà R               and C
                 Grid based convolutions can be easily performed on reg-                      q    k                     v
                                                                                   and Cv are the dimension of key and value, respectively.
              ular conventional videos by sliding on grids. However, be-                                                   Ck√óL‚Ä≤N‚Ä≤
              cause point cloud videos are spatially irregular as well as          First, we generate queries Q ‚àà R                 , keys K ‚àà
                                                                                     Ck√óL‚Ä≤N‚Ä≤                          Cv√óL‚Ä≤N‚Ä≤
              unordered and points emerge inconsistently across different          R            and values V ‚àà R                based on the in-
              frames, it is challenging to perform convolution on them.            put I. Then, we compute the dot products of the query
              Moreover, in contrast to grid based convolutions, which are          with all keys and apply a softmax function to obtain the
                                                                                   weights attention(Q,K) ‚àà RL‚Ä≤N‚Ä≤√óL‚Ä≤N‚Ä≤. Given a lo-
              performed on all areas, our point based convolution should
              avoid empty regions. To this end, we use a method sim-               cal area, related or similar areas will have larger attention
                                                                                   weights than others. Finally, the output O ‚àà RCv√óL‚Ä≤N‚Ä≤
              ilar to [15] to generate spatio-temporal local areas before
              performing point 4D convolution. SpeciÔ¨Åcally, as shown               is computed as a weighted sum of the values V . Specif-
              in Fig. 2(a), we Ô¨Årst select some frames based on the tem-           ically, for a point (x,y,z,t), its new feature is computed
              poral stride st. Second, we use the farthest point sampling          as O(x,y,z,t) = Pattention(Q,K)(x,y,z,t),(x‚Ä≤,y‚Ä≤,z‚Ä≤,t‚Ä≤) √ó
                                                                                      (x‚Ä≤,y‚Ä≤,z‚Ä≤,t‚Ä≤)         ‚Ä≤  ‚Ä≤   ‚Ä≤  ‚Ä≤
              (FPS)[43]tosubsampleN‚Ä≤ = N/s pointsineachselected                    V             , where (x ,y ,z ,t ) belongs to the set of I‚Äôs
                                                    s
              frame, where ss is the spatial subsampling rate. These sub-          4Dcoordinates.
              sampledpointsarethentransferred to the rt nearest frames.               Note that, given a query, the softmax function is per-
              The original and transferred subsampled points form the              formed on the entire video, which is referred to as video-
              central axis of a spatio-temporal local area. Finally, spatial       level self-attention is this paper. Another option is the so-
              neighbors are searched based on spatial radius r for each            called frame-level self-attention, which applies softmax to
                                                                  s
              subsampled point in the selected or rt nearest frames.               each frame individually, so that the sum of weights in each
                                                                                   frame is 1. This option assumes that each query point ap-
              3.2. Transformer                                                     pears in all frames. However, points may Ô¨Çow in and out
              3.2.1   4DCoordinateandLocalFeatureEmbedding                         across frames, especially for long point cloud videos. Such
                                                                                   an assumption is not reasonable. Therefore, we employ the
              After point 4D convolution, the spatio-temporal local ar-            video-level self-attention, which performs softmax on all
              eas of the t-th frame are encoded to representations F‚Ä≤.             L‚Ä≤N‚Ä≤ points.
                                                                           t
                                                                               14207

=== Page 5 ===
                                             concatenation                                                                   Table1.Actionrecognitionaccuracy(%)onMSR-Action3D [28].
                                  Ì†µÌ±Ç                        Ì†µÌ±Ç                                     Ì†µÌ±Ç
                                                                                ‚ãØ
                                    1                          2                                     ‚Ñé
                                   attn 1                    attn 2                          attn ‚Ñé                              Method                                Input           #Frames             Accuracy
                                                                                ‚ãØ
                                                                                                                                 Vieira et al. [54]                   depth                 20                78.20
                            Ì†µÌ±Ñ     Ì†µÌ∞æ           Ì†µÌ±â    Ì†µÌ±Ñ      Ì†µÌ∞æ         Ì†µÌ±â          Ì†µÌ±Ñ      Ì†µÌ∞æ          Ì†µÌ±â
                              1       1           1     2       2          2            ‚Ñé      ‚Ñé           ‚Ñé
                                                                                ‚ãØ
                                                                                                                                    ¬®
                                                                                                                                 Klaser et al. [21]                   depth                 18                81.43
                                                                                                                                 Actionlet [55]                      skeleton               all               88.21
                                                                    Ì†µÌ±∞
                                                                                                                                 PointNet++ [43]                       point                1                 61.61
                                    Figure 3. Illustration of multi-head attention.                                                                                                         4                 78.11
                                                                                                                                                                                            8                 81.14
                                                                                                                                 MeteorNet [36]                        point                12                86.53
                         Instead of performing a single self-attention, we employ                                                                                                           16                88.21
                     multi-head attention [53], which performs Eq. (4) h times                                                                                                              24                88.50
                     withindependentWq,Wk andWv,toenhancethelearning                                                                                                                        4                 80.13
                     ability of the transformer. The Ô¨Ånal output of the multi-head                                                                                                          8                 83.17
                     attentionistheconcatenationofhindividualself-attentions‚Äô                                                    P4Transformer (ours)                  point                12                87.54
                     outputs. The multi-head structure is illustrated in Fig. 3.                                                                                                            16                89.56
                                                                                                                                                                                            20                90.24
                                                                                                               53,
                     Besidesthemulti-headattentionmechanism,wefollow[                                                                                                                       24                90.94
                     10] to equip the transformer with LayerNorms [1], linear
                     layers, ReLUs and residual connections [18]. Like most
                     deep neural networks, we stack multiple (m) transformers                                                4. Experiments
                     to improve the modeling ability.
                                                                                                                             4.1. 3D Action Recognition
                     3.3. P4TransformerNetworksfor3DActionRecog-
                               nition and 4D Semantic Segmentation                                                               To show the effectiveness in video-level classiÔ¨Åcation,
                                                                                                                             weapply P4Transformer to 3D action recognition. Follow-
                         To evaluate the ability to model point cloud videos, we                                             ing [36, 59], we sample 2,048 points for each frame. Point
                     apply our P4Transformer to 3D action recognition and 4D                                                 cloud videos are split into multiple clips (with a Ô¨Åxed num-
                     semantic segmentation. Action recognition is a fundamen-                                                ber of frames) as inputs. For training, video-level labels are
                     tal task for video modeling, which can be seen as a video-                                              used as clip-level labels. For evaluation, the mean of the
                     level classiÔ¨Åcation task. As shown in Fig. 2(a), given a point                                          clip-level predicted probabilities is used as the video-level
                     cloud video, we Ô¨Årst use a point 4D convolution layer to en-                                            prediction. Point colors are not used.
                     code spatio-temporally local areas. Second, m transformer                                                   Bydefault, the temporal radius rt is set to 1 so that point
                     layers (self-attention blocks) are stacked to capture appear-                                           4Dconvolution can capture temporal local correlation. The
                     ance and motion information across all encoded local fea-                                               temporal stride is set to 2 to subsample frames. The spa-
                     tures. Third, a max pooling merges the transformed local                                                tial subsampling rate ss is set to 32. The spatial radius rs
                     features to a single global one. Finally, an MLP layer con-                                             is speciÔ¨Åed in different datasets. The transformer contains
                     verts the global feature to action predictions.                                                         5 self-attention (m = 5) blocks, with 8 heads (h = 8) per
                         The 4D semantic segmentation can be seen as a point-                                                block. We train our models for 50 epochs with the SGD op-
                     level classiÔ¨Åcation task.                    The architecture is shown in                               timizer. Learning rate is set to 0.01, and decays with a rate
                     Fig. 2(b). Because point cloud frames for segmentation are                                              of 0.1 at the 20th epoch and the 30th epoch, respectively.
                     usually high-resolution, we stack multiple point 4D convo-                                                  We compare our P4Transformer with skeleton-based,
                     lution layers to exponentially reduce the number of points                                              depth-based and point-based methods on this task. Note
                     to be processed by the transformer. After the transformer,                                              that, skeleton-based methods rely on additional body key-
                     because the point 4D convolution layers subsample points,                                               point detection algorithms and cannot capture other objects‚Äô
                     we add feature propagation layers to interpolate point fea-                                             motion except for human. Moreover, only using body key-
                     tures. Inspired by [43], we use inverse distance weighted                                               points ignores scene information that may also provide rich
                     average based on k nearest neighbors:                                                                   and important cues for action recognition.                                Depth-based
                                             Pk                              (x+Œ¥ ,y+Œ¥ ,z+Œ¥ ,t)                              methods project 3D data to 2D depth frame and thus dis-
                                                       w(Œ¥x,Œ¥y,Œ¥z)O                 x       y       z
                          F‚Ä≤‚Ä≤(x,y,z) =           i=1                                                     ,     (5)
                            t                                P                                                               tort the real 3D shape [
                                                                 k    w(Œ¥ ,Œ¥ ,Œ¥ )                                                                                59].
                                                                 i=1        x    y    z
                     where w(Œ¥ ,Œ¥ ,Œ¥ ) =                            1          .  In this paper, we use
                                      x     y    z         k(Œ¥ ,Œ¥ ,Œ¥ )k2
                                                                x y z                                                        4.1.1       MSR-Action3D
                     k = 3. Skipconnectionsareaddedbetweenthecorrespond-
                     ingconvolutionlayersandpropagationlayers. Afterthelast                                                  The MSR-Action3D [28] dataset consists of 567 Kinect
                     feature interpolation layer, we add an MLP layer that con-                                              v1 depth videos, including 20 action categories and 23K
                     verts point features to point predictions.                                                              frames in total. We use the same training/test split as previ-
                                                                                                                       14208

=== Page 6 ===
             Table 2. Action recognition accuracy (%) on NTU RGB+D 60 [45] and NTU RGB+D 120 [30].   Figure 4. Visualization of transformer‚Äôs
                                                               NTURGB+D60         NTURGB+D120        attention. Input: color indicates depth.
               Method                             Input       Subject    View     Subject   Setup
                                                                                                      ut
               SkeleMotion [2]                   skeleton       69.6     80.1      67.7      66.9     inp
               GCA-LSTM[33]                      skeleton       74.4     82.8      58.3      59.3
                      31]                        skeleton        -         -       59.9      62.4
               FSNet[
               TwoStreamAttention LSTM[32]       skeleton       77.1     85.1      61.2      63.3
               BodyPoseEvolution Map[35]         skeleton        -         -       64.6      66.9     on
               AGC-LSTM[48]                      skeleton       89.2     95.0       -         -       ti
               AS-GCN[26]                        skeleton       86.8     94.2       -         -       tten
               VA-fusion [64]                    skeleton       89.4     95.0       -         -       a
               2s-AGCN[47]                       skeleton       88.5     95.1       -         -
               DGNN[46]                          skeleton       89.9     96.1       -         -
               HON4D[40]                          depth         30.6      7.3       -         -
               SNV[62]                            depth         31.8     13.6       -         -       ut
                    2
               HOG [39]                           depth         32.2     22.3       -         -       inp
               Li et al. [25]                     depth         68.1     83.4       -         -
               Wangetal.[57]                      depth         87.1     84.2       -         -
               MVDI[61]                           depth         84.6     87.3       -         -
               NTURGB+D120Baseline[30]            depth          -         -       48.7      40.1     on
               PointNet++ (appearance) [43]       point         80.1     85.1      72.1      79.4     enti
               3DV(motion) [59]                   voxel         84.5     95.4      76.9      92.5     att
               3DV-PointNet++ [59]             voxel + point    88.8     96.3      82.4      93.5
               P4Transformer (ours)               point         90.2     96.4      86.4      93.5
             ous works [55, 36]. We conduct experiments with 10 times        Table 3. Running time (ms) per video on NTU RGB+D 60 [45].
             and report the mean. As default, the spatial radius r is set
                                                                 s               Method                  CPU       GPU        Overall
             to 0.5 for this dataset.
                The performance comparisons are reported in Ta-                  3DV-PointNet++ [59]     2295       473        2768
             ble 1.   Our method outperforms all the state-of-the-art            P4Transformer (ours)     11        854        865
             methods, demonstrating the superiority of the proposed
             P4Transformer on feature extraction.
                We visualize a few transformer‚Äôs attention weights in        and 8M frames in total. The videos are also captured by
             Fig. 4.  For input, color indicates depth.   For attention,     Kinect v2, with 106 performers and 32 collection setups
             brighter color indicates higher weight. As expected, the        (locations and backgrounds). Besides cross-subject evalua-
             transformer is able to pay attention to the correct regions     tion, the dataset deÔ¨Ånes a new evaluation setting, i.e., cross-
             across the frames. This supports our intuition that the trans-  setup, where 16 setups are used for training, and the others
             former can take the place of explicit point tracking when       are used for testing. The spatial radius rs is set to 0.1 for
             capturing spatio-temporal structure of point cloud videos.      these two datasets.
                                                                                Comparison with state-of-the-art methods. As indi-
             4.1.2  NTURGB+D60andNTURGB+D120                                 cated in Table 2, P4Transformer outperforms all the other
                                                                             approaches in all evaluation settings. Particularly, as in-
             The NTU RGB+D60[45] is the second largest dataset for           dicated by the cross-setup evaluation on NTU RGB+D
             3D action recognition. It consists of 56K videos, with 60       120, P4Transformer outperforms the second best 3DV-
             action categories and 4M frames in total. The videos are        PointNet++[59]by4.0%. Moreover,comparedto3DVthat
             captured using Kinect v2, with 3 cameras and 40 subjects        extracts motion from voxels, P4Transformer directly mod-
             (performers). The dataset deÔ¨Ånes two types of evaluation,       els the dynamic information of raw point cloud sequences
             i.e., cross-subject and cross-view. The cross-subject evalu-    and thus is efÔ¨Åcient.
             ation splits the 40 performers into training and test groups.      Computational efÔ¨Åciency. We provide a running time
             Eachgroupconsistsof20performers. Thecross-vieweval-             comparison with the second best 3DV-PointNet++ [59].
             uation uses all the samples from camera 1 for testing and       The average running time per video is shown in Table 3.
             samples from cameras 2 and 3 for training.                      Experiments are conducted using 1 Nvidia RTX 2080Ti
                The NTU RGB+D 120 [30] dataset, the largest dataset          GPUonNTURGB+D60. Comparedto3DV-PointNet++,
             for 3D action recognition, is an extension of NTU RGB+D         P4Transformer reduces running time by 1903ms, demon-
             60. It consists of 114K videos, with 120 action categories      strating that P4Transformer is very efÔ¨Åcient.
                                                                          14209

=== Page 7 ===
                                                                               Table 4. 4D semantic segmentation results (mIoU %) on the Synthia 4D dataset [6].
                             Method                                  Input # Frames Track Bldn Road Sdwlk Fence Vegittn Pole                                                                       Car        T. Sign Pedstrn Bicycl Lane T. Light mIoU
                             3DMinkNet14[6]                          voxel              1               -        89.39 97.68 69.43 86.52                              98.11         97.26 93.50                79.45           92.27           0.00        44.61         66.69          76.24
                             4DMinkNet14[6]                          voxel              3               -        90.13 98.26 73.47 87.19                              99.10         97.50 94.01                79.04           92.62           0.00        50.01         68.14          77.46
                             PointNet++ [43]                         point              1               -        96.88 97.72 86.20 92.75                              97.12         97.09 90.85                66.87           78.64           0.00        72.93         75.17          79.35
                             MeteorNet-m[36]                         point              2              ‚úì 98.22 97.79 90.98 93.18 98.31 97.45 94.30 76.35                                                                       81.05           0.00        74.09         75.92          81.47
                             MeteorNet-m[36]                         point              2               ‚úó        97.65 97.83 90.03 94.06                              97.41         97.79 94.15                82.01           79.14           0.00        72.59         77.92          81.72
                             MeteorNet-l [36]                        point              3               ‚úó        98.10 97.72 88.65 94.00                              97.98         97.65 93.83                84.07           80.90           0.00        71.14         77.60          81.80
                             P4Transformer (ours) point                                 1               -        96.76 98.23 92.11 95.23                              98.62         97.77 95.46                80.75           85.48           0.00        74.28         74.22          82.41
                             P4Transformer (ours) point                                 3               ‚úó        96.73 98.35 94.03 95.23                              98.28         98.01 95.60                81.54           85.18           0.00        75.95         79.07          83.16
                                                                                                                                                                           92                    12      16      20       24                91.5                    12      16      20       24
                                                                                                                                                                           91
                                                                                                                                                                           90                                                               90.5
                                                                                                                                                                          )(%89                                                            (%)
                                                                                                                                                                           88                                                               89.5
                                                                                                                                                                          acy 87                                                           acy 
                                                                                                                                                                          accur86                                                          accur88.5
                                                                                                                                                                           85
                                                                                                                                                                           84                                                               87.5
                                                                                                                                                                           83
                                                                                                                                                                           82                                                               86.5
                                                                                                                                                                                    0         1        2         3         4                        0.1    0.5    0.9    1.3   1.7    2.1    2.5
                                                                                                                                                                                (a) Temporal radius rt                                              (b) Spatial radius rs
                                                                                                                                                                      Figure 6. InÔ¨Çuence of the temporal radius rt and spatial radius rs.
                                                                                                                                                                      TheMSR-Action3Ddatasetisused.
                                                                                                                                                                      framesoutperformsthestate-of-the-art methods. Moreover,
                                                                                                                                                                      our method achieves seven best accuracies among them,
                                                                                                                                                                      demonstratingtheeffectivenessofP4Transformer. Wevisu-
                           Figure 5. Visualization of 4D semantic segmentation. Top: inputs.                                                                          alize two segmentation results from the Synthia 4D dataset
                           Middle: ground truth. Bottom: P4Transformer predictions.                                                                                   in Fig. 5. Our method can accurately segment most objects.
                                                                                                                                                                            Wealso observe that tracking does not always improve
                                                                                                                                                                      accuracy.               We speculate that this phenomenon might be
                           4.2. 4D Semantic Segmentation                                                                                                              caused by unreliable point tracking results, which hinder
                                                                                                                                                                      temporal modeling.
                                  To demonstrate that our P4Transformer can be used for
                           point-level prediction tasks, we employ P4Transformer for                                                                                  4.3. Ablation Study
                           4D semantic segmentation. Following the works [6, 36],                                                                                           The point 4D convolution and transformer are two im-
                           we conduct experiments on video clips with length of 3                                                                                     portant components of our method. In this section, we in-
                           frames. Note that, although 4D semantic segmentation can                                                                                   vestigate the effects of the design choices in these two com-
                           be achieved from a single frame, exploring temporal cor-                                                                                   ponents on MSR-Action3D.
                           relation would help understanding the structure of scenes,
                           and thus improving segmentation accuracy and robustness
                           to noise. The mean Intersection over Union (mIoU) is used                                                                                  4.3.1          Point 4D Convolution: Temporal Radius and
                           as the evaluation metric.                                                                                                                                 Spatial Radius
                                  Synthia 4D [6] uses the Synthia dataset [44] to create                                                                              Kernel size is a basic attribute of convolutional operations,
                           3D videos, which includes 6 videos of driving scenarios,                                                                                   which controls local structure modeling.                                                       To effectively
                           where both objects and cameras are moving. Each video                                                                                      model the local structure, conventional CNNs usually use
                           consists of 4 stereo RGB-D images taken from the top of a                                                                                  small kernel sizes. In this paper, the kernel size of our point
                           moving car. Following [
                                                                               36], we reconstruct 3D point cloud                                                     4Dconvolutionconsistsofatemporalradiusr andaspatial
                                                                                                                                                                                                                                                                       t
                           videosfromRGBanddepthimages,andusethesametrain-                                                                                            radius r . We investigate the inÔ¨Çuence of the two radiuses
                                                                                                                                                                                       s
                           ing/validation/test split, with 19,888/815/1,886 frames, re-                                                                               onspatio-temporal modeling.
                           spectively.                                                                                                                                      The temporal radius r controls the temporal dynamics
                                                                                                                                                                                                                            t
                                  As seen in Table 4, the proposed P4Transformer with 3                                                                               modeling of point cloud videos. As shown in Fig. 6(a),
                                                                                                                                                              14210

=== Page 8 ===
                   92                                                                               Table 5. InÔ¨Çuence of frame-level and video-level self-attention on
                                  12   16   20    24       92            12   16   20   24          3Daction recognition. The MSR-Action3D dataset is used.
                   91                                      91
                   90                                     %)90
                   (%)89                                  (89                                            Self-attention         12           16            20            24
                                                           
                                                          y
                   acy 88                                 ac88
                                                          r                                              Frame-level          70.65         76.45        78.16         79.18
                   87                                     cu87                                           Video-level          87.54         89.56        90.24         90.94
                   accur86                                ac86
                   85                                      85
                                                           84
                   84
                        1   2  3   4   5  6   7   8              1     2     4    8     16          moreheadscaneffectively increase accuracy. However, us-
                 (a) Number of transformer layers       (b) Number of heads (h) of each             ing too many heads makes the feature dimension of each
                 or self-attention blocks (m)           self-attention block                        head too short.         The feature of each head cannot carry
                                                                                                    enough information for performing attention. The accuracy
                 Figure 7. InÔ¨Çuence of the number of transformer layers (m) and                     therefore decreases.
                 the numberofheads(h)on3Dactionrecognition. Notethat,when                               Finally, we evaluate the impact of the frame-level and the
                 wechangethenumberofheads,thetotalfeaturedimensionofeach                            video-level self-attention on point cloud video modeling.
                 transformer layer is Ô¨Åxed. The MSR-Action3D dataset is used.                       AsshowninTable5,thevideo-levelself-attention achieves
                                                                                                    much better accuracy than the frame-level one. This is be-
                 when rt is set to 0, point 4D convolution does not capture                         cause, based on the assumption that the query appears in
                 the temporal correlation. However, P4Transformer can still                         each frame, the frame-level attention performs the softmax
                 model spatio-temporal structure by the subsequent trans-                           function in each individual frame. However, this does not
                 former, and thus achieve satisfactory accuracy. When r is                          matchthefactthatpointsmayÔ¨Çowinandoutacrossframes,
                                                                                         t
                 set to 1, the temporal local structure of 3 frames is captured.                    especially for long videos. By contrast, the video-level self-
                 In this case, our convolution has the ability to model the                         attention performs the softmax function across the entire
                 spatio-temporal correlation. Compared to rt = 0, where                             video, in which Ô¨Çowing-out points can be ignored with low
                 only spatial structure is captured, the outputs of point 4D                        attention, thus facilitating temporal modeling and improv-
                 convolution with rt = 1 is more rich and informative, and                          ing action recognition accuracy.
                 therefore facilitates the subsequent transformer to perform
                 self-attention. Consequently, the 3D action recognition ac-                        4.3.3     Clip Length
                 curacy is improved. However, when rt > 1, the perfor-
                 mance gradually decreases. This is because, points Ô¨Çow in                          Because information is not equally distributed in point
                 and out in videos, especially for long videos. Noise is in-                        cloud videos along time, short video clips may miss key
                 evitably introduced when using long temporal radiuses.                             frames for action reasoning and confuse models as noise.
                     The spatial search radius r controls the region of the                         AsshowninTable1,increasingclip length effectively ben-
                                                        s                                           eÔ¨Åts models for 3D action recognition.
                 spatial structure to be modeled. As shown in Fig. 6(b), us-
                 ing a too small r cannot capture sufÔ¨Åcient spatial structure
                                      s                                                             5. Conclusion
                 information. However, when using large rs, it will decrease
                 the discriminativeness of local structure. Therefore, the 3D                           In this paper, we propose a Point 4D Transformer
                 action recognition accuracy decreases.                                             (P4Transformer) network to capture spatio-temporal corre-
                                                                                                    lation from raw point cloud videos. P4Transformer consists
                 4.3.2    Transformer: NumberofTransformers,Number                                  of a point 4D convolution and a transformer. The point 4D
                          of Heads and Frame-level Self-Attention                                   convolution embeds the spatio-temporal local structures to
                                                                                                    compact representations and subsamples frames and points
                 Like most deep neural networks, we can stack multiple                              for the subsequent transformer processing. The transformer
                 transformerlayerstoincreasethelearningabilityofthepro-                             captures the appearance and motion information across
                 posedP4Transformer. AsshownFig.7(a),withmoretrans-                                 the entire point cloud video by performing self-attention
                 former layers, P4Transformer can achieve better accuracy.                          on the embedded local representations. Extensive experi-
                 However, too many layers decrease performance. This is                             ments demonstrate the effectiveness of our P4Transformer
                 because, when networks become deeper, gradients may be                             onpoint cloud video modeling.
                 vanishing or exploding, making networks difÔ¨Åcult to train.
                     To investigate the inÔ¨Çuence of the number of heads on                          Acknowledgments
                 our transformer, we keep the total feature dimension Ô¨Åxed.
                 SpeciÔ¨Åcally, the total feature dimension is Ô¨Åxed to 1024.                          ThisresearchissupportedbytheAgencyforScience,Tech-
                 Suppose there are h heads, then the feature dimension of                           nology and Research (A*STAR) under its AME Program-
                 each self-attention is 1024/h. As shown Fig. 7(b), using                           matic Funding Scheme (#A18A2b0046).
                                                                                                14211

=== Page 9 ===
              References                                                          [16] Hehe Fan, Tao Zhuo, Xin Yu, Yi Yang, and Mohan Kankan-
                                                                                       halli. Understanding atomic hand-object interaction with hu-
               [1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.             man intention. IEEE Trans. Circuits Syst. Video Technol.,
                   Layer normalization. arXiv, 1607.06450, 2016.                       2021.
                                                                     ¬¥
               [2] Carlos Caetano, Jessica Sena de Souza, Franc¬∏ois Bremond,      [17] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can
                   Jefersson A. dos Santos, and William Robson Schwartz.               spatiotemporal 3d cnns retrace the history of 2d cnns and
                   Skelemotion: A new representation of skeleton joint se-             imagenet? In CVPR, 2018.
                   quences based on motion information for 3d action recogni-
                   tion. In IEEE International Conference on Advanced Video       [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
                   and Signal Based Surveillance, AVSS, 2019.                          Deep residual learning for image recognition.  In CVPR,
               [3] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas                2016.
                                                                                                            ¬®
                   Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-     [19] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term
                   end object detection with transformers. arXiv, 2005.12872,          memory. Neural Computation, 9(8):1735‚Äì1780, 1997.
                   2020.                                                          [20] Jianfang Hu, Wei-Shi Zheng, Jiahui Pan, Jianhuang Lai, and
                     Àú                                                                 Jianguo Zhang. Deep bilinear learning for RGB-D action
               [4] Joao Carreira and Andrew Zisserman.    Quo vadis, action
                   recognition? Anewmodelandthekineticsdataset. InCVPR,                recognition. In ECCV, 2018.
                                                                                                    ¬®
                   2017.                                                          [21] Alexander Klaser, Marcin Marszalek, and Cordelia Schmid.
               [5] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.               A spatio-temporal descriptor based on 3d-gradients.    In
                   Multi-view 3d object detection network for autonomous               BMVC,2008.
                   driving. In CVPR, 2017.                                        [22] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan
               [6] Christopher B. Choy, JunYoung Gwak, and Silvio Savarese.            Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.
                   4d spatio-temporal convnets: Minkowski convolutional neu-           Big transfer (bit): General visual representation learning.
                   ral networks. In CVPR, 2019.                                        arXiv, 1912.11370, 2019.
                                              ¬®
               [7] Junyoung Chung, C¬∏aglar Gulc¬∏ehre, KyungHyun Cho, and          [23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
                   YoshuaBengio. Empiricalevaluationofgatedrecurrentneu-               Imagenet classiÔ¨Åcation with deep convolutional neural net-
                   ral networksonsequencemodeling. arXiv,1412.3555,2014.               works. In NeurIPS, 2012.
               [8] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-                                              ¬¥
                                                                                  [24] Colin Lea, Michael D. Flynn, Rene Vidal, Austin Reiter, and
                   ber, Thomas A. Funkhouser, and Matthias Nie√üner. Scan-              Gregory D. Hager. Temporal convolutional networks for ac-
                   net: Richly-annotated 3d reconstructions of indoor scenes.          tion segmentation and detection. In CVPR, 2017.
                   In CVPR, 2017.                                                 [25] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S.
               [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina              Kankanhalli. Unsupervised learning of view-invariant action
                   Toutanova. BERT: pre-training of deep bidirectional trans-          representations. In NeurIPS, 2018.
                   formers for language understanding. In Proceedings of the      [26] Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng
                   2019 Conference of the North American Chapter of the As-            Wang, and Qi Tian. Actional-structural graph convolutional
                   sociation for Computational Linguistics: Human Language             networks for skeleton-based action recognition. In CVPR,
                   Technologies, NAACL-HLT, 2019.                                      2019.
              [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,         [27] Ruihui Li, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and
                   Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,                 Pheng-Ann Heng. PU-GAN: A point cloud upsampling ad-
                   Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-            versarial network. In ICCV, 2019.
                   vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
                   worth 16x16 words: Transformers for image recognition at       [28] Wanqing Li, Zhengyou Zhang, and Zicheng Liu. Action
                   scale. In ICLR, 2021.                                               recognition based on a bag of 3d points. In CVPR Work-
              [11] HemingDu,XinYu,andLiangZheng. VTNet: Visualtrans-                   shops, 2010.
                   former network for object goal navigation. In ICLR, 2021.      [29] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di,
              [12] Hehe Fan, Xiaojun Chang, De Cheng, Yi Yang, Dong Xu,                andBaoquanChen.Pointcnn: Convolutiononx-transformed
                   and Alexander G. Hauptmann. Complex event detection by              points. In NeurIPS, 2018.
                   identifying reliable shots from untrimmed videos. In ICCV,     [30] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,
                   2017.                                                               Ling-YuDuan,andAlexC.Kot.NTURGB+D120: Alarge-
              [13] Hehe Fan, Zhongwen Xu, Linchao Zhu, Chenggang Yan,                  scale benchmark for 3d human activity understanding. IEEE
                   Jianjun Ge, and Yi Yang. Watching a small portion could             Trans. Pattern Anal. Mach. Intell., 42(10):2684‚Äì2701, 2020.
                   be as good as watching all: Towards efÔ¨Åcient video classiÔ¨Å-    [31] Jun Liu, Amir Shahroudy, Gang Wang, Ling-Yu Duan, and
                   cation. In IJCAI, 2018.                                             Alex C. Kot. Skeleton-based online action prediction using
              [14] HeheFanandYiYang. Pointrnn: Point recurrent neural net-             scale selection network. IEEE Trans. Pattern Anal. Mach.
                   workformovingpointcloudprocessing. arXiv,1910.08287,                Intell., 42(6):1453‚Äì1467, 2020.
                   2019.                                                          [32] Jun Liu, Gang Wang, Ling-Yu Duan, Kamila Abdiyeva, and
              [15] Hehe Fan, Xin Yu, Yuhang Ding, Yi Yang, and Mohan                   Alex C. Kot. Skeleton-based human action recognition with
                   Kankanhalli. PSTNet: Point spatio-temporal convolution on           globalcontext-awareattentionLSTMnetworks. IEEETrans.
                   point cloud sequences. In ICLR, 2021.                               Image Processing, 27(4):1586‚Äì1599, 2018.
                                                                              14212

=== Page 10 ===
              [33] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, and Alex C.          [51] Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torre-
                   Kot. Global context-aware attention LSTM networks for 3d             sani, and Manohar Paluri. Learning spatiotemporal features
                   action recognition. In CVPR, 2017.                                   with 3d convolutional networks. In ICCV, 2015.
              [34] Lu Liu, William L. Hamilton, Guodong Long, Jing Jiang,          [52] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
                   andHugoLarochelle. Auniversalrepresentationtransformer               LeCun,andManoharPaluri. Acloserlookatspatiotemporal
                   layer for few-shot image classiÔ¨Åcation. In ICLR, 2021.               convolutions for action recognition. In CVPR, 2018.
              [35] Mengyuan Liu and Junsong Yuan. Recognizing human ac-            [53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
                   tions as the evolution of pose estimation maps. In CVPR,             reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
                   pages 1159‚Äì1168, 2018.                                               Polosukhin. Attention is all you need. In NeurIPS, 2017.
              [36] Xingyu Liu, Mengyuan Yan, and Jeannette Bohg. Meteor-                    ÀÜ
                                                                                   [54] Antonio Wilson Vieira, Erickson R. Nascimento, Gabriel L.
                   net: Deep learning on dynamic 3d point cloud sequences. In           Oliveira, Zicheng Liu, and Mario Fernando Montenegro
                   ICCV,2019.                                                           Campos. STOP: space-time occupancy patterns for 3d ac-
              [37] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furi-             tion recognition from depth map sequences. In Progress in
                   ous: Real time end-to-end 3d detection, tracking and motion          Pattern Recognition, Image Analysis, Computer Vision, and
                   forecasting with a single convolutional net. In CVPR, 2018.          Applications - 17th Iberoamerican Congress, CIARP, 2012.
              [38] Joe Yue-Hei Ng, Matthew J. Hausknecht, Sudheendra Vi-           [55] Jiang Wang, Zicheng Liu, Ying Wu, and Junsong Yuan.
                   jayanarasimhan, Oriol Vinyals, Rajat Monga, and George               Mining actionlet ensemble for action recognition with depth
                   Toderici. Beyond short snippets: Deep networks for video             cameras. In CVPR, 2012.
                   classiÔ¨Åcation. In CVPR, 2015.                                   [56] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
              [39] Eshed Ohn-Bar and Mohan M. Trivedi. Joint angles similar-            Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment
                   ities and HOG2 for action recognition. In CVPR Workshops,            networks: Towards good practices for deep action recogni-
                   2013.                                                                tion. In ECCV, 2016.
              [40] Omar Oreifej and Zicheng Liu. HON4D: histogram of ori-          [57] Pichao Wang, Wanqing Li, Zhimin Gao, Chang Tang, and
                   ented 4d normals for activity recognition from depth se-             Philip O. Ogunbona. Depth pooling based large-scale 3-d
                   quences. In CVPR, 2013.                                              action recognition with convolutional neural networks. IEEE
              [41] Charles Ruizhongtai Qi, Or Litany, Kaiming He, and                   Trans. Multimedia, 20(5):1051‚Äì1061, 2018.
                   Leonidas J. Guibas. Deep hough voting for 3d object de-         [58] Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and
                   tection in point clouds. In ICCV, 2019.                              Kaiming He. Non-local neural networks. In CVPR, 2018.
              [42] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and                 [59] Yancheng Wang, Yang Xiao, Fu Xiong, Wenxiang Jiang,
                   Leonidas J. Guibas. Pointnet: Deep learning on point sets            Zhiguo Cao, Joey Tianyi Zhou, and Junsong Yuan. 3dv:
                   for 3d classiÔ¨Åcation and segmentation. In CVPR, 2017.                3d dynamic voxel for action recognition in depth video. In
              [43] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J.               CVPR,2020.
                   Guibas. Pointnet++: Deep hierarchical feature learning on
                   point sets in a metric space. In NeurIPS, 2017.                 [60] WenxuanWu,ZhongangQi,andFuxinLi. Pointconv: Deep
                         ¬¥                                                              convolutional networks on 3d point clouds. In CVPR, 2019.
              [44] German Ros, Laura Sellart, Joanna Materzynska, David
                     ¬¥                         ¬¥                                   [61] Yang Xiao, Jun Chen, Yancheng Wang, Zhiguo Cao,
                   Vazquez, and Antonio M. Lopez. The SYNTHIA dataset:
                   Alarge collection of synthetic images for semantic segmen-           Joey Tianyi Zhou, and Xiang Bai. Action recognition for
                   tation of urban scenes. In CVPR, 2016.                               depth video using multi-view dynamic images.     Inf. Sci.,
              [45] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.               480:287‚Äì304, 2019.
                   NTU RGB+D: A large scale dataset for 3d human activity          [62] Xiaodong Yang and Yingli Tian. Super normal vector for
                   analysis. In CVPR, 2016.                                             activity recognition using depth sequences. In CVPR, 2014.
              [46] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.               [63] LequanYu,XianzhiLi,Chi-WingFu,DanielCohen-Or,and
                   Skeleton-based action recognition with directed graph neu-           Pheng-Ann Heng. Pu-net: Point cloud upsampling network.
                   ral networks. In CVPR, 2019.                                         In CVPR, 2018.
              [47] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Two-          [64] Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng,
                   stream adaptive graph convolutional networks for skeleton-           Jianru Xue, and Nanning Zheng.      View adaptive neural
                   based action recognition. In CVPR, 2019.                             networks for high performance skeleton-based human ac-
              [48] Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, and                  tion recognition. IEEE Trans. Pattern Anal. Mach. Intell.,
                   Tieniu Tan.   An attention enhanced graph convolutional              41(8):1963‚Äì1978, 2019.
                   LSTM network for skeleton-based action recognition.     In      [65] Xiaohan Zhang, Lu Liu, Guodong Long, Jing Jiang, and
                   CVPR,2019.                                                           Shenquan Liu.    Episodic memory governs choices: An
              [49] Karen Simonyan and Andrew Zisserman. Two-stream con-                 rnn-basedreinforcementlearningmodelfordecision-making
                   volutional networks for action recognition in videos.   In           task. Neural Networks, 134:1‚Äì10, 2021.
                   NeurIPS, 2014.                                                  [66] Tao Zhuo, Zhiyong Cheng, Peng Zhang, Yongkang Wong,
              [50] Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud,                and Mohan S. Kankanhalli. Explainable video action rea-
                   Beatriz Marcotegui, Franc¬∏ois Goulette, and Leonidas J.              soning via prior knowledge and state transitions. In ACM
                   Guibas. Kpconv: Flexible and deformable convolution for              Multimedia, 2019.
                   point clouds. In ICCV, 2019.
                                                                               14213

