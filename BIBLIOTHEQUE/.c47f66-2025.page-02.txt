                                     LLM Filtering                   2 Option Augmentation                   3 Photos/Screenshots
                                         MMMU                               Filtered MMMU                        •  Manually take photos 
                                    LLMs                               Augment up     Human 
                                 w/ text-only                          to 10 options validation                  •  Synthetic artifacts
                                    input
                                      Highly image-                   Option augmented questions                 •  Different font styles
                                  dependent questions
                                               Figure 1: An overview of the construction process of MMMU-Pro.
                    this approach aligns with how users naturally inter-              carefully curated multimodal questions from col-
                    act with AI systems, often sharing screenshots or                 lege exams, quizzes, and textbooks, covering six
                    photos rather than separating text and images.                    core disciplines across 30 subjects and 183 sub-
                       Ourexperimental results demonstrate the effec-                 fields. Each question in MMMU is a multimodal
                    tiveness of MMMU-Pro in providing a more rigor-                   image-text pair with 4 multiple-choice options, fea-
                    ous evaluation of multimodal models. We observe                   turing 30 diverse image types such as charts, di-
                    significant performancedropsacrossalltestedmod-                   agrams, maps, and chemical structures. MMMU
                    els when compared to the original MMMU bench-                     has rapidly established itself as a standard evalua-
                    mark,withdecreasesrangingfrom16.8%to26.9%.                        tion framework for testing prominent multimodal
                    These results highlight the limitations of current                models upon their release. (OpenAI, 2024b,a; An-
                    state-of-the-art models in true multimodal under-                 thropic, 2024; Reid et al., 2024; Li et al., 2024a).
                    standing and reasoning. Furthermore, our analysis                    However, we find that text-only LLMs can ac-
                    reveals that while CoT (Wei et al., 2022) prompting               curately answer some questions without requiring
                    generally improves performance, the benefits vary                 any visual input. We take a closer look at these
                    across models and settings.                                       questions and identify two main issues: 1) Text-
                       Interestingly, we find that explicit OCR prompts               OnlyDependency: Certainquestionsarerelatively
                    do not significantly impact performance for most                  independent or irrelevant to the corresponding im-
                    models, suggesting that advanced multimodal mod-                  ages. 2) Shortcut Exploitation: Even when ques-
                    els have already developed robust text extraction                 tions require images for humans to answer cor-
                    capabilities from images. However, this result also               rectly, models often find shortcuts or correlations
                    underscores that simple OCR is insufficient for the               within the candidate options, leveraging their pre-
                    challenges presented by MMMU-Pro’s vision-only                    existing knowledge (from pre-training) to arrive
                    input setting. Our further qualitative analysis indi-             at the correct answer. Two examples that are an-
                    cates that when text is embedded within images, it                swered correctly by Llama-3-70B Instruct (Dubey
                    significantly increases the overall complexity of the             et al., 2024) are shown in Figure 2.
                    visual input, requiring models to not only recognize              2.2    Methods
                    text but also understand its context, relationship
                    to visual elements, and relevance to the question.                To address these issues and build a more robust
                    These findings not only provide a more accurate                   benchmark, we implemented a three-step approach.
                    assessment of current multimodal AI capabilities                  Filtering Questions: We begin by filtering out
                    but also highlight the need for more sophisticated                questions that can be answered by text-only
                    multimodal reasoning abilities.                                   LLMs. Weselect four strong open-source LLMs:
                                                                                      Llama3-70B-Instruct (Dubey et al., 2024), Qwen2-
                    2 MMMU-Pro:AMoreRobustVersion                                     72B-Instruct (Yang et al., 2024), Yi-1.5-34B-
                         of MMMU                                                      Chat (Young et al., 2024), and Mixtral-8×22B-
                    2.1    Revisiting the MMMUBenchmark                               Instruct (gpt-4o)—and task them with answering
                                                                                      the MMMU questions without access to images.
                    TheMassiveMulti-discipline Multimodal Under-                      Themodelsarerequired to provide answers even
                    standingandReasoning(MMMU)benchmark(Yue                           when they indicate that visual input is necessary.
                    et al., 2024) is a comprehensive dataset designed                 Werepeat this process ten times for each model,
                    to evaluate multimodal AI models on college-level                 considering a question as “answerable” if a model
                    tasks that require subject-specific knowledge and                 correctly answers it more than five times. We then
                    deliberate reasoning. MMMU consists of 11.5K                      exclude any question where at least three out of the
                                                                                 15135
