                       Published as a conference paper at ICLR 2025
                       Onthe CIFAR-100 dataset (Krizhevsky, 2009), which consists of 100 classes with 600 images each, the
                       MINDmodelattainedatop-1accuracyof85.53%andatop-5accuracyof92.6%. Thisshowcasesthemodel’s
                       proficiency in handling smaller, more focused datasets.
                       Themodel’s performance on other datasets further underscores its versatility:
                              • CIFAR-10: The model achieved a high accuracy of 96.4% on this 10-class dataset, which is
                                comparable to state-of-the-art results (Krizhevsky, 2009).
                              • MNIST:Onthisclassichandwrittendigitrecognitiondataset,themodelreachedanimpressive99.7%
                                accuracy, demonstrating its effectiveness in handling grayscale images and simple classification
                                tasks (LeCun et al., 1998).
                              • SVHN: The Street View House Numbers dataset posed a more challenging real-world scenario,
                               where the model achieved 98.2% accuracy, highlighting its robustness in recognizing digits in
                                complex environments (Netzer et al., 2011).
                              • Pascal VOC2012: Withatop-1accuracyof89.8%andatop-5accuracyof95.3%,themodelshowed
                                strong performance on this dataset, which includes various object detection and segmentation tasks
                                (Everingham et al., 2010).
                       Themodelalsoperformedwellonspecialized datasets such as Oxford-IIIT Pets (95.9% top-1, 96.5% top-5)
                       (Parkhi et al., 2012), Stanford Cars (94.8% top-1, 96.3% top-5) (Krause et al., 2013), and CUB-200-2011
                       (92.8% top-1, 95.7% top-5) (Wah et al., 2011). These results demonstrate the MIND model’s effectiveness in
                       fine-grained classification tasks.
                       These comprehensive experiments across diverse datasets underscore the MIND model’s adaptability and
                       strong performance across a wide range of image classification tasks, from simple digit recognition to complex
                       scene understanding and fine-grained classification.
                          Table 8: MIND performance with Top-1 and Top-5 accuracy scores for various vision-based datasets
                                                  Dataset           Top-1 Accuracy   Top-5 Accuracy
                                                  ImageNet               88.3%           96.62%
                                                  CIFAR-100             85.53%            92.6%
                                                  CIFAR-10               96.4%             —
                                                  MNIST                  99.7%             —
                                                  SVHN                   98.2%             —
                                                  Pascal VOC 2012        89.8%            95.3%
                                                  MSCOCO                 80.5%            94.6%
                                                  Places365              73.3%            92.4%
                                                  Oxford-IIIT Pets       95.9%            96.5%
                                                  Stanford Cars          94.8%            96.3%
                                                  CUB-200-2011           92.8%            95.7%
                                                  Food-101               93.0%            95.7%
                       E.6   TEXT-BASED EXPERIMENTS WITH LSTMS
                       Tofurther validate the effectiveness and adaptability of MIND model, we extend our experiments to text-based
                       tasks employing Long Short-Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997).
                                                                      26
