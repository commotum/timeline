                    Method                   cat. mIoU    ins. mIoU              Number of neighbors. We first investigate the setting of
                    PointNet [25]               80.4         83.7                the number of neighbors k, which is used in determining
                    A-SCN[48]                    –           84.6                the local neighborhood around each point. The results are
                    PCNN[42]                    81.8         85.1                shown in Table 5. The best performance is achieved when
                    PointNet++ [27]             81.9         85.1                k is set to 16. When the neighborhood is smaller (k = 4
                    DGCNN[44]                   82.3         85.1                or k = 8), the model may not have sufficient context for its
                    Point2Sequence [21]          –           85.2                predictions. When the neighborhood is larger (k = 32 or
                    SpiderCNN[49]               81.7         85.3                k = 64), each self-attention layer is provided with a large
                    SPLATNet[33]                83.7         85.4                number of datapoints, many of which may be farther and
                    PointConv [46]              82.8         85.7                less relevant. This may introduce excessive noise into the
                    SGPN[43]                    82.8         85.8                processing, lowering the model’s accuracy.
                    PointCNN[20]                84.6         86.1                Softmax regularization. We conduct an ablation study
                    InterpCNN [22]              84.0         86.3                on the normalization function ρ in Eq. 3.        The perfor-
                    KPConv[37]                  85.1         86.4                mance without softmax regularization on S3DIS Area5 is
                    PointTransformer            83.7         86.6                66.5%/72.8%/89.3%, in terms of mIoU/mAcc/OA. It is
              Table 4. Object part segmentation results on the ShapeNetPart      muchlower than the performance with softmax regulariza-
              dataset.                                                           tion (70.4%/76.5%90.8%). This suggests that the normal-
                              k     mIoU     mAcc     OA                         ization is essential in this setting.
                              4     59.6      66.0    86.0                       Position encoding. We now study the choice of the posi-
                              8     67.7      73.8    89.9                       tion encoding δ. The results are shown in Table 6. We can
                              16    70.4      76.5    90.8                       see that without position encoding, the performance drops
                              32    68.3      75.0    89.8                       significantly. With absolute position encoding, the perfor-
                              64    67.7      74.1    89.9                       mance is higher than without. Relative position encoding
              Table 5. Ablation study: number of neighbors k in the definition   yields the highest performance. When relative position en-
              of local neighborhoods.                                            codingisaddedonlytotheattentiongenerationbranch(first
                                                                                 term in Eq. 3) or only to the feature transformation branch
                        Pos. encoding        mIoU     mAcc      OA               (second term in Eq. 3), the performance drops again, in-
                             none             64.6     71.9    88.2              dicating that adding the relative position encoding to both
                           absolute           66.5     73.2    88.9              branches is important.
                           relative           70.4     76.5    90.8              Attention type. Finally, we investigate the type of self-
                     relative for attention   67.0     73.0    89.3              attention used in the point transformer layer. The results are
                      relative for feature    68.7     74.4    90.4              shown in Table 7. We examine four conditions. ‘MLP’ is
                        Table 6. Ablation study: position encoding.              a no-attention baseline that replaces the point transformer
                                                                                 layer in the point transformer block with a pointwise MLP.
                           Operator       mIoU      mAcc     OA                  ‘MLP+pooling’ is a more advanced no-attention baseline
                            MLP            61.7     68.6     87.1                that replaces the point transformer layer with a pointwise
                        MLP+pooling        63.7     71.0     87.8                MLPfollowedbymaxpoolingwithineach kNNneighbor-
                       scalar attention    64.6     71.9     88.4                hood: this performs feature transformation at each point
                       vector attention    70.4     76.5     90.8                and enables each point to exchange information with its lo-
                  Table 7. Ablation study: form of self-attention operator.      cal neighborhood, but does not leverage attention mecha-
                                                                                 nisms. ‘scalar attention’ replaces the vector attention used
              Visualization. Object part segmentation results on a num-          in Eq. 3 by scalar attention, as in Eq. 1 and in the original
                                                                                 transformer design [39]. ‘vector attention’ is the formula-
              ber of models are shown in Figure 7. The Point Trans-              tion we use, presented in Eq. 3. We can see that scalar at-
              former’s part segmentation predictions are clean and close         tention is more expressive than the no-attention baselines,
              to the ground truth.                                               but is in turn outperformed by vector attention. The per-
              4.4. Ablation Study                                                formance gap between vector and scalar attention is signif-
                                                                                 icant: 70.4% vs. 64.6%, an improvement of 5.8 absolute
                 Wenowconductanumberofcontrolledexperimentsthat                  percentage points. Vector attention is more expressive since
              examinespecific decisions in the Point Transformer design.         it supports adaptive modulation of individual feature chan-
              These studies are performed on the semantic segmentation           nels, not just whole feature vectors. This expressivity ap-
              task on the S3DIS dataset, tested on Area 5.                       pears to be very beneficial in 3D data processing.
                                                                             16265
