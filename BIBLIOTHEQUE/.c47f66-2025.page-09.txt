                                            MMMU MMMU-Pro                  Lin et al., 2024; Zhang et al., 2024a). Proprietary
                     Method                   (Val)      (Vision)
                                                                           models such as GPT-4V (OpenAI, 2023), GPT-4o
                     DINOv2ViT-G-14           37.1         17.4            (OpenAI, 2024b), Gemini (Team et al., 2023), and
                     Siglip ViT-SO400M-14     37.9         16.7            Claude-3.5 (Anthropic, 2024) have demonstrated
                  Table 3: Performance of an MLLM with different vision    strong performance across various vision-language
                  encoders on MMMUandMMMU-Pro.                             tasks. However, a significant challenge remains
                  domains within MMMU-Pro, as reflected in Fig-            in accurately evaluating the capabilities of these
                  ure 5 and Table 6. While domains like Tech and En-       advanced LMMs, highlighting the need for more
                  gineering and Business see notable improvements,         robust and comprehensive benchmarks.
                  CoTperformanceremainsweakorevendetrimen-                 MLLMBenchmarks. Theriseofmoreadvanced
                  tal in areas such as Art and Design. To address          multimodal pre-training and instruction tuning has
                  these gaps, future efforts focus on synthesizing         exposed the limitations of earlier benchmarks like
                  morediverse reasoning-intensive CoT data and tai-        VQA(Antoletal.,2015;Goyaletal., 2017), OK-
                  loring strategies for domains where CoT impact           VQA(Marino et al., 2019), and MSCOCO (Lin
                  is minimal. Leveraging inference-compute con-            et al., 2014), which no longer suffice to evaluate
                  cepts (Welleck et al., 2024) further enhances CoT        the full spectrum of LMMs capabilities. To address
                  capabilities, enabling models to generalize more         this, recent benchmarks such as LAMM (Yin et al.,
                  effectively across varied reasoning tasks.               2023b), LVLM-eHub(Xuetal., 2023), SEED (Li
                  Text-Rich Image Generation in Reasoning Sce-             et al., 2024b), MMBench (Liu et al., 2023d),CV-
                  narios. Our analysis shows that strong OCR ac-           Bench (Tong et al., 2024a), MM-Vet (Yu et al.,
                  curacy and reasoning performance on traditional          2024), Mantis (Jiang et al., 2024), and BLINK (Fu
                  benchmarks do not always translate to success on         et al., 2024) have emerged, covering aspects from
                  MMMU-ProVision. Apotential reason is the lack            basic perception to hallucination detection (Cui
                  of training data with text-rich images in reasoning-     et al., 2023; Liu et al., 2023a).      However, ex-
                  intensive contexts. To address this, we developed        isting benchmarks often fall short in evaluating
                  a tool leveraging the MMMU-Pro Vision human              expert-level domain knowledge and complex rea-
                  annotation process. This tool processes a JSON file      soning (Lu et al., 2023a; Zhang et al., 2024b).
                  with questions and images and outputs screenshots        WhileMMMU(Yueetal.,2024)madestridesbyin-
                  embedding both. Such tools can further generate          corporating multimodal, college-level questions, it
                  similar datasets at scale, enhancing models’ abil-       still permits text-only models to find shortcuts (Lu
                  ity to integrate visual and textual information in       et al., 2023b; Zhang et al., 2024b). To address these
                  real-world scenarios.                                    limitations, we introduce MMMU-Pro, a more ro-
                    Byfocusing on these directions, future model-          bust benchmark that removes text-only answerable
                  ing efforts can address limitations highlighted by       questions, expands candidate options, and includes
                  MMMU-Proandpushmultimodalunderstanding                   a vision-only input setting to better reflect real-
                  and reasoning boundaries.                                world multimodal scenarios.
                  5 RelatedWork                                            6 Conclusion
                  Multimodal Large Language Models. Recent                 MMMU-Prooffers a stronger multimodal under-
                  progress in multimodal AI has been marked by             standing and reasoning benchmark than its prede-
                  innovative training approaches (Lu et al., 2019;         cessor MMMU. Our results show MMMU-Pro’s
                  Chenet al., 2020; Zhou et al., 2020; Zhang et al.,       effectiveness in exposing current state-of-the-art
                  2021;Lietal.,2020;Alayracetal.,2022;Awadalla             model limitations, with significant performance
                  et al., 2023). Inspired by the success of large lan-     drops across all tested systems. MMMU-Pro high-
                  guage models, researchers have developed vari-           lights critical research directions: 1) Developing
                  ous models with improved instruction-following           modelswithconsistentperformanceacrosssettings,
                  capabilities (Liu et al., 2023c,b, 2024a; Li et al.,     particularly bridgingstandardandvision-onlyinput
                  2024a; Dai et al., 2023; Zhu et al., 2023; Zhang         gaps. 2) Enhancing vision-text integration for com-
                  et al., 2023; Gao et al., 2023; Ye et al., 2023a,b;      plex mixed-format inputs. 3) Advancing reasoning
                  Zhao et al., 2023; Li et al., 2023; Monajatipoor         techniques to address MMMU-Pro’s heightened
                  et al., 2023; Zhao et al., 2024; Li et al., 2024c;       question complexity.
                                                                      15142
