408 «© Unified Theories of Cognition

assumption that a system uses annotated models? It is certainly
possible to build problem spaces whose states are propositional
representations and whose operators are logical inference rules.
Are these simply banned in an organism that uses only mental mod-
els? The answer is a rather surprising one. No, these representa-
tions can be used. When they are, what the organism has is a
mental model of the proposition (this is essentially the nature of the
utterance model in syllogistic reasoning). When the organism ma-
nipulates that model by means of rules of inference, it is going from
a model of one set of propositions to a model of a different set. But,
of course, a model of a set of propositions is not a model of what
those propositions are about. For a model-representing system to
use propositional representations directly is for it to work indirectly
with respect to the world those propositions represent. When the
human does use propositions, the considerations always seem one
off.'§

Second, the central reason syllogisms are difficult is the inade-
quacy of the representation, namely, the use of (annotated) models.
Models are adequate for many purposes, in particular, for dealing
with the world as perceived. So there are good reasons why humans
would use models routinely. But categorical syllogisms, with their
quantifiers, are too complex to be represented by a single model
with the sorts of annotations available to humans.

Representational inadequacy is not the sole reason for difficulty.
Problems of the interpretation of the language used are possible,
especially of some and ail. Most important, the task situation is not
one in which any corrective feedback occurs so that humans can
discover how to reason correctly. Thus, a primary feature of human
cognition, the ability to learn, is basically excluded. There seem to
be no published studies on what learning occurs over the 64 syllo-
gisms in an experiment, but some examination of the raw data from
the study discussed here shows very little change in performance.
As corroboration, Soar does learn in these syllogistic tasks, but
only in terms of the speed, not of correctness. The effects of the

16. The use of propositional representations for the state is different from the use of
annotations on a model, even though annotations are propositions, The annotations are
processed as part of the match-like processing of the model itself; the propositions
are processed by inference operators. Consequently, two forms of proposition-tike oper-
ations can go on in the human, so that the identification of a proposition-like representa-
tion requires some care.

