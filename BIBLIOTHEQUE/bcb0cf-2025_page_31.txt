                     Published as a conference paper at ICLR 2025
                        Table 12: Performance comparison of Standard Softmax and Adaptive Softmax in the MIND model.
                             Metric                     StandardSoftmax     Adaptive Softmax    Improvement
                             Avg. Inference Time (ms)          45.2               32.5             28.1%
                             Accuracy (%)                      76.3               76.8              +0.5
                             Layer Utilization Efficiency      0.72               0.89             23.6%
                             MemoryUsage(MB)                   256                 198             22.7%
                       Table 13: Relationship between Softmax Entropy, FLOPs, Accuracy, Layers Activated, and Iterations.
                            SoftmaxEntropy     FLOPs(G)     Accuracy(%)     Layers Activated   Avg. Iterations
                            Low(0.0–0.4)         0.40G          90.5%           2.1 ±0.5         2.5 ±0.7
                            Medium(0.4–0.8)      0.84G          87.7%           2.9 ±0.3         3.9 ±0.9
                            High(0.8–1.0)        1.20G          86.5%           3.0 ±0.0         4.9 ±0.7
                     F.5  ABALATION OF THE MIND MODEL VARIANTS
                     Given the interconnected nature of the introspection network (which drives adaptive dynamics) and the
                     Fixed-Point Iteration (FPI) components, we propose the following ablations to gain insights into their relative
                     contributions:
                           • MIND-Reduced: AversionoftheMINDmodelwheretheintrospectionnetworkconsidersonlya
                             reduced set of activations of the prediction model’s initial run. In this case, only the activations of
                             the first and the second layer are considered.
                           • MIND-Fixed: In this version, the introspection network is not active during inference. Instead,
                             decisions about which layers to FPI are based on the input complexity, measured as H(softmax(x)).
                             If H < 0.4 then FPI(layer ) → layer → layer ; if 0.4 ≤ H < 0.8 then FPI(layer → layer ) →
                                                     1        2        3                               1       2
                             layer ; if H ≥ 0.8 then FPI(layer → layer → layer ). This procedure removes a significant part
                                 3                         1        2        3
                             of reflective computation at inference but keeps the FPI structure.
                           • MIND-Uniform: Aversionwherealllayers are always used in the FPI iteration. Specifically, the
                             FPI loop iterates the layer → layer → layer block until convergence. This approach removes
                                                    1        2        3
                             adaptive selection keeping the weight-tying benefits.
                     Table 14: Ablation study of MIND variants on CIFAR-100 and ImageNet datasets, evaluating accuracy and
                     computational cost (FLOPs).
                             ModelVariant     CIFAR-100Accuracy      ImageNetTop-1Accuracy       FLOPs(G)
                             MIND(Full)               91.3%                    88.0%                1.2G
                             MIND-Reduced             89.5%                    86.5%                0.9G
                             MIND-Fixed               90.8%                    85.8%                2.8G
                             MIND-Uniform             90.2%                    86.2%                1.5G
                     AsshowninTable14,theresults highlight the following:
                           • MIND-Reducedshowstheimpactoflimitingbothadaptive capacity and weight tying.
                           • MIND-Fixedshowstheimportanceofreal-time adaptivity during inference.
                           • MIND-Uniformshowsthevalueofselective computation versus always using all layers.
                                                                31
