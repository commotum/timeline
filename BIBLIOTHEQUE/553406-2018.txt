=== Page 1 ===
                                         HOTPOTQA:ADatasetforDiverse,Explainable
                                                        Multi-hop Question Answering
                                                                   ♠                 ♥                             ♣
                                                 Zhilin Yang*           PengQi*            Saizheng Zhang*
                                                                           ♣♦                                †
                                                       YoshuaBengio                William W. Cohen
                                                                             ♠                                       ♥
                                              RuslanSalakhutdinov                  Christopher D. Manning
                                                                                                                 ´          ´
                                      ♠CarnegieMellonUniversity        ♥Stanford University     ♣Mila,Universite de Montreal
                                                               ♦CIFARSeniorFellow         † Google AI
                                   {zhiliny, rsalakhu}@cs.cmu.edu, {pengqi, manning}@cs.stanford.edu
                              saizheng.zhang@umontreal.ca, yoshua.bengio@gmail.com, wcohen@google.com
                                            Abstract                                  ParagraphA,ReturntoOlympus:
                                                                                      [1] Return to Olympus is the only album by the alterna-
                         Existing question answering (QA) datasets fail               tive rock band Malfunkshun. [2] It was released after
                         to train QA systems to perform complex rea-                  the band had broken up and after lead singer Andrew
                                                                                      Wood (later of Mother Love Bone) had died of a drug
                         soning and provide explanations for answers.                 overdose in 1990. [3] Stone Gossard, of Pearl Jam, had
                         Weintroduce HOTPOTQA,anewdatasetwith                         compiled the songs and released the album on his label,
                         113k Wikipedia-based question-answer pairs                   Loosegroove Records.
                         with four key features: (1) the questions re-                ParagraphB,MotherLoveBone:
                         quire ﬁnding and reasoning over multiple sup-                [4] Mother Love Bone was an American rock band that
                         porting documents to answer; (2) the ques-                   formed in Seattle, Washington in 1987. [5] The band
                                                                                      was active from 1987 to 1990. [6] Frontman Andrew
                         tions are diverse and not constrained to any                 Wood’s personality and compositions helped to catapult
                         pre-existing knowledge bases or knowledge                    the group to the top of the burgeoning late 1980s/early
                         schemas; (3) we provide sentence-level sup-                  1990s Seattle music scene. [7] Wood died only days be-
                         porting facts required for reasoning, allowing               fore the scheduled release of the band’s debut album,
                         QAsystemstoreason with strong supervision                    “Apple”, thus ending the group’s hopes of success. [8]
                                                                                      Thealbumwasﬁnallyreleased a few months later.
                         andexplain the predictions; (4) we offer a new               Q: What was the former band of the member of Mother
                         type of factoid comparison questions to test                 LoveBonewhodiedjustbeforetherelease of “Apple”?
                         QA systems’ ability to extract relevant facts                A:Malfunkshun
                         and perform necessary comparison. We show                    Supporting facts: 1, 2, 4, 6, 7
                         that HOTPOTQA is challenging for the latest
                         QA systems, and the supporting facts enable                Figure 1: An example of the multi-hop questions in
                         models to improve performance and make ex-                 HOTPOTQA.Wealsohighlight the supporting facts in
                         plainable predictions.                                     blue italics, which are also part of the dataset.
                    1    Introduction                                                  First, some datasets mainly focus on testing the
                    The ability to perform reasoning and inference                  ability of reasoning within a single paragraph or
                    over natural language is an important aspect of in-             document, or single-hop reasoning. For example,
                    telligence. The task of question answering (QA)                 in SQuAD (Rajpurkar et al., 2016) questions are
                    provides a quantiﬁable and objective way to test                designed to be answered given a single paragraph
                    the reasoning ability of intelligent systems. To this           as the context, and most of the questions can in
                    end, a few large-scale QA datasets have been pro-               fact be answered by matching the question with
                    posed, which sparked signiﬁcant progress in this                a single sentence in that paragraph. As a result, it
                    direction. However, existing datasets have limita-              hasfallenshortattestingsystems’abilitytoreason
                    tions that hinder further advancements of machine               over a larger context. TriviaQA (Joshi et al., 2017)
                    reasoningovernaturallanguage,especiallyintest-                  and SearchQA (Dunn et al., 2017) create a more
                    ing QAsystems’ ability to perform multi-hop rea-                challenging setting by using information retrieval
                    soning, where the system has to reason with in-                 to collect multiple documents to form the con-
                    formation taken from more than one document to                  text given existing question-answer pairs. Nev-
                    arrive at the answer.                                           ertheless, most of the questions can be answered
                       ∗These authors contributed equally. The order of author-     by matching the question with a few nearby sen-
                    ship is decided through dice rolling.                           tences in one single paragraph, which is limited as
                       †WorkdonewhenWWCwasatCMU.                                    it does not require more complex reasoning (e.g.,
                                                                               2369
                           Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380
                                                                                   c
                               Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics

=== Page 2 ===
                  over multiple paragraphs).                               2   DataCollection
                     Second, existing datasets that target multi-hop       The main goal of our work is to collect a diverse
                  reasoning, such as QAngaroo (Welbl et al., 2018)         and explainable question answering dataset that
                  and COMPLEXWEBQUESTIONS(TalmorandBe-                     requires multi-hop reasoning. One way to do so
                  rant, 2018), are constructed using existing knowl-       is to deﬁne reasoning chains based on a knowl-
                  edge bases (KBs). As a result, these datasets are        edge base (Welbl et al., 2018; Talmor and Berant,
                  constrained by the schema of the KBs they use,           2018). However, the resulting datasets are limited
                  and therefore the diversity of questions and an-         by the incompleteness of entity relations and the
                  swers is inherently limited.                             lack of diversity in the question types. Instead,
                     Third, all of the above datasets only provide dis-    in this work, we focus on text-based question an-
                  tant supervision; i.e., the systems only know what       swering in order to diversify the questions and an-
                  the answer is, but do not know what supporting           swers. The overall setting is that given some con-
                  facts lead to it. This makes it difﬁcult for models      text paragraphs (e.g., a few paragraphs, or the en-
                  to learn about the underlying reasoning process, as      tire Web) and a question, a QA system answers
                  well as to make explainable predictions.                 the question by extracting a span of text from the
                     Toaddress the above challenges, we aim at cre-        context, similar to Rajpurkar et al. (2016). We
                  ating a QA dataset that requires reasoning over          additionally ensure that it is necessary to perform
                  multiple documents, and does so in natural lan-          multi-hop reasoning to correctly answer the ques-
                  guage, without constraining itself to an existing        tion.
                  knowledge base or knowledge schema. We also                 It is non-trivial to collect text-based multi-hop
                  want it to provide the system with strong supervi-       questions. In our pilot studies, we found that sim-
                  sion about what text the answer is actually derived      ply giving an arbitrary set of paragraphs to crowd
                  from, to help guide systems to perform meaning-          workers is counterproductive, because for most
                  ful and explainable reasoning.                           paragraph sets, it is difﬁcult to ask a meaning-
                     Wepresent HOTPOTQA1, a large-scale dataset            ful multi-hop question. To address this challenge,
                  that satisﬁes these desiderata. HOTPOTQA is col-         wecarefullydesignapipelinetocollecttext-based
                  lected by crowdsourcing based on Wikipedia ar-           multi-hop questions. Below, we will highlight the
                  ticles, where crowd workers are shown multiple           key design choices in our pipeline.
                  supporting context documents and asked explic-
                  itly to come up with questions requiring reason-         Building a Wikipedia Hyperlink Graph.            We
                  ing about all of the documents. This ensures it          use the entire English Wikipedia dump as our cor-
                  covers multi-hop questions that are more natural,            2
                                                                           pus.   In this corpus, we make two observations:
                  and are not designed with any pre-existing knowl-        (1) hyper-links in the Wikipedia articles often nat-
                  edge base schema in mind. Moreover, we also              urally entail a relation between two (already dis-
                  ask the crowd workers to provide the supporting          ambiguated) entities in the context, which could
                  facts they use to answer the question, which we          potentially be used to facilitate multi-hop reason-
                  also provide as part of the dataset (see Figure 1 for    ing; (2) the ﬁrst paragraph of each article often
                  an example). We have carefully designed a data           contains much information that could be queried
                  collection pipeline for HOTPOTQA, since the col-         in a meaningful way. Based on these observations,
                  lection of high-quality multi-hop questions is non-      we extract all the hyperlinks from the ﬁrst para-
                  trivial. We hope that this pipeline also sheds light     graphs of all Wikipedia articles. With these hy-
                  on future work in this direction. Finally, we also       perlinks, we build a directed graph G, where each
                  collected a novel type of questions—comparison           edge (a,b) indicates there is a hyperlink from the
                  questions—as part of HOTPOTQA, in which we               ﬁrst paragraph of article a to article b.
                  require systems to compare two entities on some
                  shared properties to test their understanding of         Generating Candidate Paragraph Pairs.             To
                  both language and common concepts such as nu-            generate meaningful pairs of paragraphs for multi-
                  merical magnitude. We make HOTPOTQA pub-                 hop question answering with G, we start by
                  licly available at https://HotpotQA.github.io.           considering an example question “when was the
                                                                           singer and songwriter of Radiohead born?” To
                     1The name comes from the ﬁrst three authors’ arriving at
                  the main idea during a discussion at a hot pot restaurant.  2https://dumps.wikimedia.org/
                                                                      2370

=== Page 3 ===
                  answer this question, one would need to ﬁrst rea-          Algorithm 1 Overall data collection procedure
                  son that the “singer and songwriter of Radiohead”            Input: question type ratio r1 = 0.75, yes/no ratio r2 =
                  is “Thom Yorke”, and then ﬁgure out his birth-               0.5
                  day in the text. We call “Thom Yorke” a bridge               while not ﬁnished do
                                                                                  if random() < r1 then
                  entity in this example.     Given an edge (a,b) in                 Uniformly sample an entity b ∈ B
                  the hyperlink graph G, the entity of b can usually                 Uniformly sample an edge (a,b)
                  be viewed as a bridge entity that connects a and                   Workers ask a question about paragraphs a and b
                                                                                  else
                  b. As we observe articles b usually determine the                  Samplealist from L, with probabilities weighted by
                  theme of the shared context between a and b, but                   list sizes
                                                                                     Uniformly sample two entities (a,b) from the list
                  not all articles b are suitable for collecting multi-              if random() < r2 then
                  hop questions. For example, entities like coun-                      Workers ask a yes/no question to compare a and
                  tries are frequently referred to in Wikipedia, but                   b
                                                                                     else
                  don’t necessarily have much in common with all                       Workers ask a question with a span answer to
                  incoming links. It is also difﬁcult, for instance,                   compare a and b
                  for the crowd workers to ask meaningful multi-                     endif
                                                                                  endif
                  hop questions about highly technical entities like              Workers provide the supporting facts
                  the IPv4 protocol. To alleviate this issue, we con-          endwhile
                  strain the bridge entities to a set of manually cu-
                  rated pages in Wikipedia (see Appendix A). Af-             quires reasoning over both paragraphs.
                  ter curating a set of pages B, we create candidate            To the best of our knowledge, text-based com-
                  paragraph pairs by sampling edges (a,b) from the           parisonquestionsareanoveltypeofquestionsthat
                  hyperlink graph such that b ∈ B.                           have not been considered by previous datasets.
                  Comparison Questions.          In addition to ques-        Moreimportantly, answering these questions usu-
                  tions collected using bridge entities, we also             ally requires arithmetic comparison, such as com-
                  collect another type of multi-hop questions—               paring ages given birth dates, which presents a
                  comparison questions. The main idea is that com-           newchallenge for future model development.
                  paring two entities from the same category usu-            Collecting Supporting Facts.         To enhance the
                  ally results in interesting multi-hop questions, e.g.,     explainability of question answering systems, we
                  “Who has played for more NBA teams, Michael                want them to output a set of supporting facts nec-
                  Jordan or Kobe Bryant?” To facilitate collecting           essary to arrive at the answer, when the answer
                  this type of question, we manually curate 42 lists         is generated.    To this end, we also collect the
                                                                        3
                  ofsimilarentities (denoted as L) from Wikipedia.           sentences that determine the answers from crowd
                  To generate candidate paragraph pairs, we ran-             workers.    These supporting facts can serve as
                  domly sample two paragraphs from the same list             strong supervision for what sentences to pay at-
                  and present them to the crowd worker.                      tention to. Moreover, we can now test the explain-
                     Toincreasethediversityofmulti-hopquestions,             ability of a model by comparing the predicted sup-
                  we also introduce a subset of yes/no questions             porting facts to the ground truth ones.
                  in comparison questions. This complements the                 Theoverallprocedureofdatacollectionisillus-
                  original scope of comparison questions by offer-           trated in Algorithm 1.
                  ing new ways to require systems to reason over
                  both paragraphs. For example, consider the en-             3   Processing and Benchmark Settings
                  tities Iron Maiden (from the UK) and AC/DC                 We collected 112,779 valid examples in total on
                  (from Australia). Questions like “Is Iron Maiden                                        4
                  or AC/DC from the UK?” are not ideal, because              Amazon Mechanical Turk using the ParlAI in-
                  one would deduce the answer is “Iron Maiden”               terface (Miller et al., 2017) (see Appendix A).To
                  even if one only had access to that article. With          isolate potential single-hop questions from the de-
                  yes/no questions, one may ask “Are Iron Maiden             sired multi-hop ones, we ﬁrst split out a sub-
                  and AC/DC from the same country?”, which re-               set of data called train-easy.      Speciﬁcally, we
                      3                                                      randomly sampled questions (∼3–10 per Turker)
                      This is achieved by manually curating lists from the   from top-contributing turkers, and categorized all
                  Wikipedia “List of lists of lists” (https://wiki.sh/
                  y8qv). One example is “Highest Mountains on Earth”.           4https://www.mturk.com/
                                                                        2371

=== Page 4 ===
                        Name               Desc.               Usage       # Examples         to retrieve 8 paragraphs from Wikipedia as dis-
                        train-easy         single-hop          training          18,089       tractors, using the question as the query. We mix
                        train-medium       multi-hop           training          56,814       them with the 2 gold paragraphs (the ones used
                        train-hard         hard multi-hop      training          15,661       to collect the question and answer) to construct
                        dev                hard multi-hop      dev                7,405       the distractor setting.            The 2 gold paragraphs
                        test-distractor    hard multi-hop      test               7,405
                        test-fullwiki      hard multi-hop      test               7,405       and the 8 distractors are shufﬂed before they are
                        Total                                                  112,779        fed to the model. In the second setting, we fully
                       Table 1:     Data split.      The splits train-easy, train-            test the model’s ability to locate relevant facts as
                       medium,andtrain-hard arecombinedfortraining. The                       well as reasoning about them by requiring it to
                       distractor and full wiki settings use different test sets so           answer the question given the ﬁrst paragraphs of
                       that the gold paragraphs in the full wiki test set remain              all Wikipedia articles without the gold paragraphs
                       unknowntoanymodels.                                                    speciﬁed. This full wiki setting truly tests the per-
                                                                                              formance of the systems’ ability at multi-hop rea-
                                                                                                                        5
                       their questions into the train-easy set if an over-                    soning in the wild.          The two settings present dif-
                       whelming percentage in the sample only required                        ferent levels of difﬁculty, and would require tech-
                       reasoning over one of the paragraphs. We sam-                          niques ranging from reading comprehension to in-
                       pled these turkers because they contributed more                       formation retrieval. As shown in Table 1, we use
                       than 70% of our data. This train-easy set contains                     separate test sets for the two settings to avoid leak-
                       18,089 mostly single-hop examples.                                     ing information, because the gold paragraphs are
                                                                                              available to a model in the distractor setting, but
                          We implemented a question answering model                           should not be accessible in the full wiki setting.
                       based on the current state-of-the-art architectures,                       We also try to understand the model’s good
                       which we discuss in detail in Section 5.1. Based                       performance on the train-medium split. Manual
                       on this model, we performed a three-fold cross                         analysis shows that the ratio of multi-hop ques-
                       validation on the remaining multi-hop examples.                        tions in train-medium is similar to that of the hard
                       Among these examples, the models were able to                          examples (93.3% in train-medium vs. 92.0% in
                       correctly answer 60% of the questions with high                        dev), but one of the question types appears more
                       conﬁdence(determinedbythresholdingthemodel                             frequently in train-medium compared to the hard
                       loss). These correctly-answered questions (56,814                      splits (Type II: 32.0% in train-medium vs. 15.0%
                       in total, 60% of the multi-hop examples) are split                     in dev, see Section 4 for the deﬁnition of Type II
                       out and marked as the train-medium subset, which                       questions). These observations demonstrate that
                       will also be used as part of our training set.                         given enough training data, existing neural archi-
                          After splitting out train-easy and train-medium,                    tectures can be trained to answer certain types and
                       we are left with hard examples. As our ultimate                        certain subsets of the multi-hop questions. How-
                       goal is to solve multi-hop question answering, we                      ever, train-medium remains challenging when not
                       focus on questions that the latest modeling tech-                      just the gold paragraphs are present—we show in
                       niques are not able to answer. Thus we constrain                       AppendixCthattheretrievalproblemontheseex-
                       our dev and test sets to be hard examples. Specif-                     amplesareasdifﬁcultasthatontheirhardcousins.
                       ically, we randomly divide the hard examples into
                       four subsets, train-hard, dev, test-distractor, and                    4     Dataset Analysis
                       test-fullwiki. Statistics about the data split can be                  In this section, we analyze the types of questions,
                       found in Table 1. In Section 5, we will show that                      typesofanswers,andtypesofmulti-hopreasoning
                       combining train-easy, train-medium, and train-                         covered in the dataset.
                       hard to train models yields the best performance,
                       so we use the combined set as our default train-                       Question Types.             We heuristically identiﬁed
                       ing set. The two test sets test-distractor and test-                   question types for each collected question.                     To
                       fullwiki are used in two different benchmark set-                      identify the question type, we ﬁrst locate the cen-
                       tings, which we introduce next.                                        tral question word (CQW) in the question. Since
                          Wecreate two benchmark settings. In the ﬁrst                         HOTPOTQA contains comparison questions and
                       setting, to challenge the model to ﬁnd the true sup-                       5As we required the crowd workers to use complete en-
                       porting facts in the presence of noise, for each ex-                   tity names in the question, the majority of the questions are
                       ampleweemploybigramtf-idf(Chenetal.,2017)                              unambiguous in the full wiki setting.
                                                                                         2372

=== Page 5 ===
                                                                               AnswerType        % Example(s)
                                                                               Person            30   King Edward II, Rihanna
                                                                               Group/Org         13   Cartoonito, Apalachee
                                                                               Location          10   Fort Richardson, California
                                                                               Date               9   10th or even 13th century
                                                                               Number             8   79.92 million, 17
                                                                               Artwork            8   Die schweigsame Frau
                                                                               Yes/No             6   -
                                                                               Adjective          4   conservative
                                                                               Event              1   Prix Benois de la Danse
                                                                               Other    proper    6   Cold War, Laban Movement
                                                                               noun                   Analysis
                                                                               Commonnoun         5   comedy, both men and women
                                                                                    Table 2: Types of answers in HOTPOTQA.
                                                                              Multi-hop Reasoning Types.          Wealso sampled
                                                                              100 examples from the dev and test sets and man-
                   Figure 2: Types of questions covered in HOTPOTQA.          ually classiﬁed the types of reasoning required to
                   Question types are extracted heuristically, starting at    answer each question. Besides comparing two en-
                   question words or prepositions preceding them. Empty       tities, there are three main types of multi-hop rea-
                   colored blocks indicate sufﬁxes that are too rare to       soning required to answer these questions, which
                   showindividually. See main text for more details.          weshowinTable3accompaniedwithexamples.
                                                                                 Most of the questions require at least one sup-
                   yes/no questions, we consider as question words            porting fact from each paragraph to answer. A ma-
                   WH-words, copulas (“is”, “are”), and auxiliary             jority of sampled questions (42%) require chain
                   verbs (“does”, “did”). Because questions often in-         reasoning (Type I in the table), where the reader
                   volve relative clauses beginning with WH-words,            mustﬁrstidentifyabridgeentitybeforethesecond
                   we deﬁne the CQW as the ﬁrst question word in              hop can be answered by ﬁlling in the bridge. One
                   the question if it can be found in the ﬁrst three to-      strategy to answer these questions would be to de-
                   kens, or the last question word otherwise. Then,           compose them into consecutive single-hop ques-
                   we determine question type by extracting words             tions. The bridge entity could also be used im-
                   upto2tokensawaytotherightoftheCQW,along                    plicitly to help infer properties of other entities re-
                   with the token to the left if it is one of a few com-      lated to it. In some questions (Type III), the entity
                   monprepositions (e.g., in the cases of “in which”          in question shares certain properties with a bridge
                   and “by whom”).                                            entity (e.g., they are collocated), and we can in-
                     We visualize the distribution of question types          fer its properties through the bridge entity. An-
                   in Figure 2, and label the ones shared among more          othertypeofquestioninvolveslocatingtheanswer
                   than 250 questions. As is shown, our dataset cov-          entity by satisfying multiple properties simultane-
                   ers a diverse variety of questions centered around         ously (Type II). Here, to answer the question, one
                   entities, locations, events, dates, and numbers, as        could ﬁnd the set of all entities that satisfy each of
                   wellasyes/noquestionsdirectedatcomparingtwo                the properties mentioned, and take an intersection
                   entities (“Are both A and B ...?”), to name a few.         to arrive at the ﬁnal answer. Questions comparing
                                                                              two entities (Comparison) also require the system
                   Answer Types.       We further sample 100 exam-            to understand the properties in question about the
                   ples from the dataset, and present the types of an-        two entities (e.g., nationality), and sometimes re-
                   swers in Table 2. As can be seen, HOTPOTQA                 quire arithmetic such as counting (as seen in the
                   covers a broad range of answer types, which                table) or comparing numerical values (“Who is
                   matches our initial analysis of question types. We         older, A or B?”). Finally, we ﬁnd that sometimes
                   ﬁnd that a majority of the questions are about en-         the questions require more than two supporting
                   tities in the articles (68%), and a non-negligible         facts to answer (Other). In our analysis, we also
                   amountofquestionsalsoaskaboutvariousproper-                ﬁnd that for all of the examples shown in the ta-
                   ties like date (9%) and other descriptive properties       ble, the supporting facts provided by the Turkers
                   such as numbers (8%) and adjectives (4%).                  matchexactlywiththelimitedcontextshownhere,
                                                                         2373

=== Page 6 ===
                       Reasoning Type             % Example(s)
                       Inferring  the  bridge    42    Paragraph A: The 2015 Diamond Head Classic was a college basketball tournament ...
                       entity   to   complete          BuddyHield wasnamedthetournament’s MVP.
                       the 2nd-hop question            Paragraph B: Chavano Rainier ”Buddy” Hield is a Bahamian professional basketball
                       (Type I)                        player for the Sacramento Kings of the NBA...
                                                       Q:Whichteamdoestheplayernamed2015DiamondHeadClassic’sMVPplayfor?
                       Comparing two enti-       27    Paragraph A: LostAlone were a British rock band ... consisted of Steven Battelle, Alan
                       ties (Comparison)               Williamson, and Mark Gibson...
                                                       Paragraph B: Guster is an American alternative rock band ... Founding members Adam
                                                       Gardner, Ryan Miller, and Brian Rosenworcel began...
                                                       Q:DidLostAloneandGusterhavethesamenumberofmembers? (yes)
                       Locating the answer       15    Paragraph A: Several current and former members of the Pittsburgh Pirates – ... John
                       entity   by   checking          Milner, Dave Parker, and Rod Scurry...
                       multiple     properties         ParagraphB:DavidGeneParker,nicknamed”TheCobra”,isanAmericanformerplayer
                       (Type II)                       in Major League Baseball...
                                                       Q:WhichformermemberofthePittsburghPirates was nicknamed ”The Cobra”?
                       Inferring  about    the     6   Paragraph A: Marine Tactical Air Command Squadron 28 is a United States Marine Corps
                       property of an entity           aviation command and control unit based at Marine Corps Air Station Cherry Point...
                       in  question   through          Paragraph B: Marine Corps Air Station Cherry Point ... is a United States Marine Corps
                       a bridge entity (Type           airﬁeld located in Havelock, North Carolina, USA ...
                       III)                            Q:WhatcityistheMarineAirControlGroup28locatedin?
                       Other types of reason-      2   Paragraph A: ... the towns of Yodobashi, Okubo, Totsuka, and Ochiai town were merged
                       ing that require more           into Yodobashi ward. ... Yodobashi Camera is a store with its name taken from the town and
                       than two supporting             ward.
                       facts (Other)                   Paragraph B: Yodobashi Camera Co., Ltd. is a major Japanese retail chain specializing in
                                                       electronics, PCs, cameras and photographic equipment.
                                                       Q:AsidefromYodobashi,whatothertownsweremergedintothewardwhichgavethemajor
                                                       Japanese retail chain specializing in electronics, PCs, cameras, and photographic equipment
                                                       it’s name?
                     Table 3: Types of multi-hop reasoning required to answer questions in the HOTPOTQA dev and test sets. We show
                     in orange bold italics bridge entities if applicable, blue italics supporting facts from the paragraphs that connect
                     directly to the question, and green bold the answer in the paragraph or following the question. The remaining 8%
                     are single-hop (6%) or unanswerable questions (2%) by our judgement.
                     showing that the supporting facts collected are of                 cal advances on question answering, including
                     high quality.                                                      character-level models, self-attention (Wang et al.,
                        Aside from the reasoning types mentioned                        2017),andbi-attention(Seoetal.,2017). Combin-
                     above, we also estimate that about 6% of the sam-                  ing these three key components is becoming stan-
                     pled questions can be answered with one of the                     dard practice, and various state-of-the-art or com-
                     two paragraphs, and 2% of them unanswerable.                       petitive architectures (Liu et al., 2018; Clark and
                     We also randomly sampled 100 examples from                         Gardner,2017;Wangetal.,2017;Seoetal.,2017;
                     train-medium and train-hard combined, and the                      Pan et al., 2017; Salant and Berant, 2018; Xiong
                     proportions of reasoning types are: Type I 38%,                    et al., 2018) on SQuAD can be viewed as simi-
                     Type II 29%, Comparison 20%, Other 7%, Type                        lar to our implemented model. To accommodate
                     III 2%, single-hop 2%, and unanswerable 2%.                        yes/no questions, we also add a 3-way classiﬁer
                                                                                        after the last recurrent layer to produce the prob-
                     5    Experiments                                                   abilities of “yes”, “no”, and span-based answers.
                     5.1    ModelArchitecture and Training                              During decoding, we ﬁrst use the 3-way output to
                                                                                        determine whether the answer is “yes”, “no”, or a
                     To test the performance of leading QA systems                      text span. If it is a text span, we further search for
                     on our data, we reimplemented the architecture                     the most probable span.
                     described in Clark and Gardner (2017) as our
                     baseline model.        We note that our implementa-                Supporting Facts as Strong Supervision.                    To
                     tion without weight averaging achieves perfor-                     evaluate the baseline model’s performance in pre-
                     mance very close to what the authors reported                      dicting explainable supporting facts, as well as
                     on SQuAD (about 1 point worse in F1).                    Our       how much they improve QA performance, we
                     implemented model subsumes the latest techni-                      additionally design a component to incorporate
                                                                                  2374

=== Page 7 ===
                                             Linear        Yes/no/span                 Table 5. After retrieving these 10 paragraphs, we
                                  RNN                                                  then use the model trained in the distractor setting
                                                                                       to evaluate its performance on these ﬁnal candi-
                                             Linear        End token                   date paragraphs.
                                     concat                                               Following previous work (Rajpurkar et al.,
                                  RNN                                                  2016), we use exact match (EM) and F as two
                                                                                                                                         1
                                             Linear        Start token                 evaluation metrics. To assess the explainability of
                                     concat                                            the models, we further introduce two sets of met-
                                  RNN                                                  rics involving the supporting facts. The ﬁrst set fo-
                             concat                                                    cuses on evaluating the supporting facts directly,
                                             RNN                0/1                    namely EM and F1 on the set of supporting fact
                              Self-Attention             (is supporting facts?)
                                                            Strong supervision         sentences as compared to the gold set. The second
                                  RNN         residual                                 set features joint metrics that combine the evalu-
                                                                                       ation of answer spans and supporting facts as fol-
                              Bi-Attention                                             lows. For each example, given its precision and
                                                                                       recall on the answer span (P(ans),R(ans)) and the
                                  RNN                           RNN                    supporting facts (P(sup),R(sup)), respectively, we
                                                                                       calculate joint F as
                        Char RNN       Word emb       Char RNN       Word emb                             1
                                paragraphs                     question                  P(joint) = P(ans)P(sup), R(joint) = R(ans)R(sup),
                     Figure 3: Our model architecture. Strong supervision                                          2P(joint)R(joint)
                     over supporting facts is used in a multi-task setting.                         Joint F1 =      (joint)     (joint) .
                                                                                                                  P        +R
                     such strong supervision into our model. For each                  Joint EM is 1 only if both tasks achieve an ex-
                     sentence, we concatenate the output of the self-                  act match and otherwise 0. Intuitively, these met-
                     attention layer at the ﬁrst and last positions, and               rics penalize systems that perform poorly on ei-
                     use a binary linear classiﬁer to predict the prob-                ther task. All metrics are evaluated example-by-
                     ability that the current sentence is a supporting                 example, and then averaged over examples in the
                     fact. We minimize a binary cross entropy loss for                 evaluation set.
                     this classiﬁer. This objective is jointly optimized                  The performance of our model on the bench-
                     with the normal question answering objective in                   mark settings is reported in Table 4, where all
                     a multi-task learning setting, and they share the                 numbersareobtainedwithstrongsupervisionover
                     same low-level representations. With this classi-                 supporting facts. From the distractor setting to the
                     ﬁer, the model can also be evaluated on the task of               full wiki setting, expanding the scope of the con-
                     supporting fact prediction to gauge its explainabil-              text increases the difﬁculty of question answering.
                     ity. Our overall architecture is illustrated in Figure            The performance in the full wiki setting is sub-
                     3. Thoughit is possible to build a pipeline system,               stantially lower, which poses a challenge to exist-
                     in this work we focus on an end-to-end one, which                 ingtechniquesonretrieval-basedquestionanswer-
                     is easier to tune and faster to train.                            ing.   Overall, model performance in all settings
                                                                                       is signiﬁcantly lower than human performance as
                     5.2    Results                                                    shown in Section 5.3, which indicates that more
                     Weevaluate our model in the two benchmark set-                    technical advancementsareneededinfuturework.
                     tings. In the full wiki setting, to enable efﬁcient tf-              We also investigate the explainability of our
                     idf retrieval among 5,000,000+ wiki paragraphs,                   model by measuring supporting fact prediction
                     given a question we ﬁrst return a candidate pool of               performance. Our model achieves 60+ support-
                                                                                       ing fact prediction F and ∼40 joint F , which in-
                     at most 5,000 paragraphs using an inverted-index-                                          1                    1
                                                 6                                     dicates there is room for further improvement in
                     based ﬁltering strategy and then select the top 10                terms of explainability.
                     paragraphsinthepoolastheﬁnalcandidatesusing                          In Table 6, we break down the performance
                                     7
                     bigram tf-idf.     Retrieval performance is shown in              on different question types. In the distractor set-
                        6See Appendix C for details.                                   ting, comparison questions have lower F scores
                        7We choose the number of ﬁnal candidates as 10 to stay                                                            1
                     consistent with the distractor setting where candidates are 2     gold paragraphs plus 8 distractors.
                                                                                 2375

=== Page 8 ===
                                               Setting      Split      Answer           SupFact             Joint
                                                                     EM       F        EM       F       EM        F
                                                                                1                1                 1
                                               distractor   dev     44.44    58.28    21.95   66.66    11.56    40.86
                                               distractor   test    45.46    58.99    22.24   66.62    12.04    41.37
                                               full wiki    dev     24.68    34.36     5.28   40.98      2.54   17.73
                                               full wiki    test    25.23    34.40     5.07   40.69      2.63   17.85
                    Table 4: Main results: the performance of question answering and supporting fact prediction in the two benchmark
                    settings. We encourage researchers to report these metrics when evaluating their methods.
                         Set    MAP MeanRank Hits@2               Hits@10                Setting                                EM       F
                                                                                                                                           1
                         dev    43.93      314.71       39.43       56.06                our model                             44.44    58.28
                         test   43.21      314.05       38.67       55.88                – sup fact                            42.79    56.19
                    Table 5: Retrieval performance in the full wiki setting.             – sup fact, self attention            41.59    55.19
                    MeanRankisaveragedovertheranksoftwogoldpara-                         – sup fact, char model                41.66    55.25
                    graphs.                                                              – sup fact, train-easy                41.61    55.12
                                                                                         – sup fact, train-easy, train-medium  31.07    43.61
                          Setting      BrEM BrF           CpEM CpF                       gold only                             48.38    63.58
                                                     1                   1               sup fact only                         51.95    66.98
                          distractor   43.41     59.09     48.55    55.05
                          full wiki    19.76     30.42     43.87    50.70            Table 7: Ablation study of question answering perfor-
                    Table 6: Performance breakdown over different ques-              mance on the dev set in the distractor setting. “– sup
                    tion types on the dev set in the distractor setting. “Br”        fact” means removing strong supervision over support-
                    denotes questions collected using bridge entities, and           ing facts from our model. “– train-easy” and “– train-
                    “Cp”denotes comparison questions.                                medium” means discarding the according data splits
                                                                                     from training. “gold only” and “sup fact only” refer
                                                                                     to using the gold paragraphs or the supporting facts as
                    thanquestionsinvolvingbridgeentities(asdeﬁned                    the only context input to the model.
                    in Section 2), which indicates that better mod-
                    eling this novel question type might need better                 model,whichachievesa10+F improvementover
                                                                                                                          1
                    neural architectures. In the full wiki setting, the              not using the supporting facts. Compared with the
                    performance of bridge entity questions drops sig-                gainofstrongsupervisioninourmodel(∼2points
                    niﬁcantly while that of comparison questions de-                 in F1), our proposed method of incorporating sup-
                    creases only marginally. This is because both en-                porting facts supervision is most likely subopti-
                    tities usually appear in the comparison questions,               mal, and we leave the challenge of better model-
                    and thus reduces the difﬁculty of retrieval. Com-                ing to future work. At last, we show that combin-
                    bined with the retrieval performance in Table 5,                 ing all data splits (train-easy, train-medium, and
                    we believe that the deterioration in the full wiki               train-hard) yields the best performance, which is
                    setting in Table 4 is largely due to the difﬁculty of            adopted as the default setting.
                    retrieving both entities.
                       We perform an ablation study in the distractor                5.3    Establishing Human Performance
                    setting, and report the results in Table 7. Both self-           To establish human performance on our dataset,
                    attention and character-level models contribute                  we randomly sampled 1,000 examples from the
                    notably to the ﬁnal performance, which is consis-                dev and test sets, and had at least three additional
                    tent with prior work. This means that techniques                 Turkers provide answers and supporting facts for
                    targeted at single-hop QA are still somewhat ef-                 these examples. As a baseline, we treat the orig-
                    fective in our setting. Moreover, removing strong                inal Turker during data collection as the predic-
                    supervision over supporting facts decreases per-                 tion, and the newly collected answers and support-
                    formance,whichdemonstratestheeffectivenessof                     ing facts as references, to evaluate human perfor-
                    our approach and the usefulness of the supporting                mance. For each example, we choose the answer
                    facts. We establish an estimate of the upper bound               andsupportingfactreferencethatmaximizetheF
                                                                                                                                                1
                    of strong supervision by only considering the sup-               scoretoreporttheﬁnalmetricstoreducetheeffect
                    porting facts as the oracle context input to our                 of ambiguity (Rajpurkar et al., 2016).
                                                                                2376

=== Page 9 ===
                     Setting          Answer         SpFact           Joint          supporting documents are collected after the ques-
                                    EM      F1     EM       F1     EM      F1        tion answer pairs with information retrieval, the
                     gold only     65.87 74.67 59.76 90.41 41.54 68.15               questions are not guaranteed to involve interesting
                     distractor    60.88 68.99 30.99 74.67 20.06 52.37               reasoning between multiple documents.
                     Human         83.60 91.40 61.50 90.04 52.30 82.55               KB-based multi-hop datasets.              Recent datasets
                     HumanUB 96.80 98.77 87.40 97.56 84.60 96.37                     like QAngaroo (Welbl et al., 2018) and COM-
                    Table 8: Comparing baseline model performance with               PLEXWEBQUESTIONS(TalmorandBerant,2018)
                    human performance on 1,000 random samples. “Hu-                  explore different approaches of using pre-existing
                    manUB”standsfortheupperboundonannotator per-                     knowledgebases(KB)withpre-deﬁnedlogicrules
                    formance on HOTPOTQA. For details please refer to                to generate valid QA pairs, to test QA models’ ca-
                    the main body.                                                   pability of performing multi-hop reasoning. The
                                                                                     diversity of questions and answers is largely lim-
                       As can be seen in Table 8, the original crowd                 ited by the ﬁxed KB schemas or logical forms.
                    worker achieves very high performance in both                    Furthermore, some of the questions might be an-
                    ﬁnding supporting facts, and answering the ques-                 swerable by one text sentence due to the incom-
                    tion correctly. If the baseline model were provided              pleteness of KBs.
                    with the correct supporting paragraphs to begin                  Free-form answer-generation datasets.                   MS
                    with, it achieves parity with the crowd worker                   MARCO(Nguyenetal.,2016)contains100kuser
                    in ﬁnding supporting facts, but still falls short at             queries from Bing Search with human generated
                    ﬁnding the actual answer. When distractor para-                  answers.      Systems generate free-form answers
                    graphs are present, the performance gap between                  and are evaluated by automatic metrics such as
                    the baseline model and the crowd worker on both                  ROUGE-L and BLEU-1. However, the reliabil-
                    tasks is enlarged to ∼30% for both EM and F .
                                                                           1         ity of these metrics is questionable because they
                       Wefurther establish the upper bound of human                  have been shown to correlate poorly with human
                    performance in HOTPOTQA, by taking the maxi-                     judgement (Novikova et al., 2017).
                    mumEMandF1foreachexample. Here, we use
                    eachTurker’s answer in turn as the prediction, and               7    Conclusions
                    evaluate it against all other workers’ answers. As               We present HOTPOTQA, a large-scale question
                    can be seen in Table 8, most of the metrics are                  answering dataset aimed at facilitating the devel-
                    close to 100%, illustrating that on most examples,               opment of QA systems capable of performing ex-
                    at least a subset of Turkers agree with each other,              plainable, multi-hop reasoning over diverse nat-
                    showing high inter-annotator agreement. We also                  ural language. We also offer a new type of fac-
                    note that crowd workers agree less on supporting                 toid comparison questions to test systems’ ability
                    facts, which could reﬂect that this task is inher-               to extract and compare various entity properties in
                    ently moresubjectivethanansweringthequestion.                    text.
                    6    Related Work                                                Acknowledgements
                    Variousrecently-proposedlarge-scaleQAdatasets                    ThisworkispartlyfundedbytheFacebookParlAI
                    can be categorized in four categories.                           Research Award. ZY, WWC, and RS are sup-
                    Single-document datasets.            SQuAD(Rajpurkar             ported by a Google grant, the DARPA grant
                    et al., 2016, 2018) questions that are relatively                D17AP00001, the ONR grants N000141512791,
                    simple because they usually require no more than                 N000141812861, and the Nvidia NVAIL Award.
                                                                                                                                            ´
                    one sentence in the paragraph to answer.                         SZ and YB are supported by Mila, Universite de
                                                                                             ´
                                                                                     Montreal. PQ and CDM are supported by the Na-
                    Multi-document          datasets.      TriviaQA      (Joshi      tional Science Foundation under Grant No. IIS-
                    et al., 2017) and SearchQA (Dunn et al., 2017)                   1514268. Anyopinions, ﬁndings, and conclusions
                    contain question answer pairs that are accompa-                  or recommendations expressed in this material are
                    nied with more than one document as the context.                 those of the authors and do not necessarily reﬂect
                    This further challenges QA systems’ ability to                   the views of the National Science Foundation.
                    accommodatelongercontexts. However, since the
                                                                                2377

=== Page 10 ===
                   References                                                 PranavRajpurkar,JianZhang,KonstantinLopyrev,and
                   Danqi Chen, Adam Fisch, Jason Weston, and Antoine             PercyLiang.2016. SQuAD:100,000+questionsfor
                     Bordes. 2017. Reading Wikipedia to answer open-             machine comprehension of text. In Proceedings of
                     domain questions.     In Association for Computa-           the 2016 Conference on Empirical Methods in Nat-
                     tional Linguistics (ACL).                                   ural Language Processing (EMNLP).
                   Christopher Clark and Matt Gardner. 2017.       Simple     Shimi Salant and Jonathan Berant. 2018. Contextu-
                     and effective multi-paragraph reading comprehen-            alized word representations for reading comprehen-
                     sion.  In Proceedings of the 55th Annual Meeting            sion. In Proceedings of the 16th Annual Conference
                     of the Association of Computational Linguistics.            of the North American Chapter of the Association
                                                                                 for Computational Linguistics.
                   Matthew Dunn, Levent Sagun, Mike Higgins, Ugur             Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
                     Guney, Volkan Cirik, and Kyunghyun Cho. 2017.               Hannaneh Hajishirzi. 2017. Bidirectional attention
                     SearchQA: A new Q&A dataset augmented with                  ﬂowformachinecomprehension. In Proceedings of
                     context from a search engine.        arXiv preprint         the International Conference on Learning Represen-
                     arXiv:1704.05179.                                           tations.
                   Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke        Alon Talmor and Jonathan Berant. 2018. The web as
                     Zettlemoyer. 2017.    TriviaQA: A large scale dis-          a knowledge-base for answering complex questions.
                     tantly supervised challenge dataset for reading com-        In Proceedings of the 16th Annual Conference of
                     prehension.    In Proceedings of the 55th Annual            the North American Chapter of the Association for
                     Meeting of the Association for Computational Lin-           Computational Linguistics.
                     guistics.
                   Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng         Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
                     Gao. 2018.     Stochastic answer networks for ma-           and Ming Zhou. 2017.      Gated self-matching net-
                     chine reading comprehension. In Proceedings of the          works for reading comprehension and question an-
                     56th Annual Meeting of the Association for Compu-           swering. In Proceedings of the 55th Annual Meet-
                     tational Linguistics.                                       ing of the Association for Computational Linguistics
                                                                                 (Volume1: LongPapers),volume1,pages189–198.
                   Christopher D. Manning, Mihai Surdeanu, John Bauer,        Johannes Welbl, Pontus Stenetorp, and Sebastian
                     Jenny Finkel, Steven J. Bethard, and David Mc-              Riedel. 2018. Constructing datasets for multi-hop
                     Closky. 2014. The Stanford CoreNLP natural lan-             reading comprehension across documents. Transac-
                     guageprocessingtoolkit. InAssociationforCompu-              tions of the Association of Computational Linguis-
                     tational Linguistics (ACL) System Demonstrations,           tics.
                     pages 55–60.
                   AlexanderHMiller,WillFeng,AdamFisch,JiasenLu,              Caiming Xiong, Victor Zhong, and Richard Socher.
                     Dhruv Batra, Antoine Bordes, Devi Parikh, and Ja-           2018. DCN+: Mixed objective and deep residual
                     son Weston. 2017. ParlAI: A dialog research soft-           coattention for question answering. In Proceedings
                     ware platform. arXiv preprint arXiv:1705.06476.             of the International Conference on Learning Repre-
                                                                                 sentations.
                   Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,         Zhilin Yang, Saizheng Zhang, Jack Urbanek, Will
                     Saurabh Tiwary, Rangan Majumder, and Li Deng.               Feng, Alexander H Miller, Arthur Szlam, Douwe
                     2016. MS MARCO: A human generated machine                   Kiela, and Jason Weston. 2018. Mastering the dun-
                     reading comprehension dataset. In Proceedings of            geon: Grounded language learning by mechanical
                     the 30th Annual Conference on Neural Information            turker descent. In Proceedings of the International
                     Processing Systems (NIPS).                                  Conference on Learning Representations.
                                              ˇ     ˇ
                   Jekaterina Novikova, Ondrej Dusek, Amanda Cercas
                     Curry, and Verena Rieser. 2017. Why we need new
                     evaluation metrics for NLG. In Proceedings of the
                     Conference on Empirical Methods in Natural Lan-
                     guage Processing.
                   Boyuan Pan, Hao Li, Zhou Zhao, Bin Cao, Deng Cai,
                     and Xiaofei He. 2017. Memen: Multi-layer embed-
                     ding with memory networks for machine compre-
                     hension. arXiv preprint arXiv:1707.09098.
                   Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
                     Know what you don’t know: Unanswerable ques-
                     tions for SQuAD. In Proceedings of the 56th An-
                     nual Meeting of the Association for Computational
                     Linguistics.
                                                                         2378

=== Page 11 ===
                  A DataCollectionDetails                                                                        Supporting Paragraphs
                  A.1    DataPreprocessing
                  WedownloadedthedumpofEnglishWikipediaof
                  October1,2017,andextractedtextandhyperlinks
                                       8
                  with WikiExtractor.     We use Stanford CoreNLP
                  3.8.0 (Manning et al., 2014) for word and sen-
                  tence tokenization. We use the resulting sentence                                                    Friendly Hints
                  boundaries for collection of supporting facts, and
                  usetokenboundariestocheckwhetherTurkersare
                  providing answers that cover spans of entire to-                                                     Worker Input
                  kens to avoid nonsensical partial-word answers.
                  A.2    Further Data Collection Details
                  Details on Curating Wikipedia Pages.              To     Figure 4: Screenshot of our worker interface on Ama-
                  make sure the sampled candidate paragraph pairs          zon Mechanical Turk.
                  are intuitive for crowd workers to ask high-quality                   4
                  multi-hop questions about, we manually curate                      ·10
                  591 categories from the lists of popular pages by               4
                               9
                  WikiProject. For each category, we sample (a,b)
                  pairs fromthegraphGwherebisintheconsidered                      3
                  category, and manually check whether a multi-hop              Examples
                  question can be asked given the pair (a,b). Those             of2
                  categories with a high probability of permitting
                  multi-hop questions are selected.                             Number
                                                                                  1
                  Bonus Structures.      To incentivize crowd work-
                  ers to produce higher-quality data more efﬁciently,
                  we follow Yang et al. (2018), and employ bonus                      10    30    50    70   90   110   130
                  structures. We mix two settings in our data collec-                        Question Length (tokens)
                  tion process. In the ﬁrst setting, we reward the top
                  (in terms of numbers of examples) workers every          Figure 5: Distribution of lengths of questions in HOT-
                  200 examples. In the second setting, the workers         POTQA.
                  get bonuses based on their productivity (measured
                  as the number of examples per hour).                     B FurtherDataAnalysis
                  A.3    CrowdWorkerInterface                              To further look into the diversity of the data in
                  Our crowd worker interface is based on ParlAI            HOTPOTQA, we further visualized the distribu-
                  (Miller et al., 2017), an open-source project that       tion of question lengths in the dataset in Figure
                  facilitates the development of dialog systems and        5. Besides being diverse in terms of types as is
                  data collection with a dialog interface. We adapt        show in the main text, questions also vary greatly
                  ParlAI for collecting question answer pairs by           in length, indicating different levels of complexity
                  converting the collection workﬂow into a system-         and details covered.
                  oriented dialog. This allows us to have more con-
                  trol over the turkers input, as well as provide turk-    C FullWikiSettingDetails
                  ers with in-the-loop feedbacks or helpful hints to       C.1    TheInvertedIndexFiltering Strategy
                  help Turkers ﬁnish the task, and therefore speed
                  up the collection process.                                  In the full wiki setting, we adopt an efﬁcient
                     PleaseseeFigure4foranexampleoftheworker               inverted-index-based ﬁltering strategy for prelim-
                  interface during data collection.                        inary candidate paragraph retrieval. We provide
                     8https://github.com/attardi/                          details in Algorithm 2, where we set the control
                  wikiextractor                                            thresholdN = 5000inourexperiments. Forsome
                     9https://wiki.sh/y8qu                                 of the question q, its corresponding gold para-
                                                                       2379

=== Page 12 ===
                 Algorithm 2 Inverted Index Filtering Strategy
                   Input: question text q, control threshold N, ngram-to-
                   Wikidoc inverted index D
                   Inintialize:
                   Extract unigram + bigram set rq from q
                   Ncand = +∞
                   Cgram = 0
                   while Ncands > N do
                      Cgram = Cgram +1
                      Set S      to be an empty dictionary
                          overlap
                      for w ∈ r do
                              q
                        for d ∈ D[w] do
                           if d not in S   then
                                     overlap
                             S      [d] = 1
                              overlap
                           else
                             S      [d] = S      [d] + 1
                              overlap      overlap
                           endif
                        endfor
                      endfor
                      S    =∅
                       cand
                      for d in S    do
                              overlap
                        if S     [d] ≥ Cgram then
                           overlap
                           S    =S      ∪{d}
                            cand    cand
                        endif
                      endfor
                      N      =|S    |
                       cands     cand
                   endwhile
                   return S
                           cand
                 graphsmaynotbeincludedintheoutputcandidate
                 pool Scand, we set such missing gold paragraph’s
                 rank as |Scand|+1 during the evaluation, so MAP
                 and Mean Rank reported in this paper are upper
                 bounds of their true values.
                 C.2   Comparetrain-mediumSplittoHard
                       Ones
                 Table 9 shows the comparison between train-
                 medium split and hard examples like dev and test
                 under retrieval metrics in full wiki setting.   As
                 we can see, the performance gap between train-
                 medium split and its dev/test is close, which im-
                 plies that train-medium split has a similar level of
                 difﬁculty as hard examples under the full wiki set-
                 ting in which a retrieval model is necessary as the
                 ﬁrst processing step.
                   Set            MAP MeanRank CorAnsRank
                   train-medium  41.89     288.19       82.76
                   dev           42.79     304.30       97.93
                   test          45.92     286.20       74.85
                 Table9: Retrievalperformancecomparisononfullwiki
                 setting for train-medium, dev and test with 1,000 ran-
                 dom samples each. MAP and are in %. Mean Rank
                 averages over retrieval ranks of two gold paragraphs.
                 CorAns Rank refers to the rank of the gold paragraph
                 containing the answer.
                                                                   2380

