                             Under review as a conference paper at ICLR 2025
                    378
                    379
                    380                                                                     9    9    9    9    3    9    3
                    381                                                                     9    9    9    9    9    3    9
                    382
                    383                                                                     9    9    9    9    9    9    3
                    384
                    385
                    386
                    387
                    388
                    389
                    390
                    391
                    392
                    393
                    394
                    395      Figure 6: VITARC-VT failure analysis for ARC task (#1cf80156). Cross-attention heatmap
                    396      across all attention heads in the final layer at the step predicting the color-3 pixel within the dark
                    397      blue box. The task requires finding the maximum rectangular subgrid in the input. The attention,
                    398      visualized in a thermal heatmap, shows that none of the heads successfully distinguish the subgrid
                    399      (orange bounding box) from its surroundings that motivates the PEMixer and OPE, nor do they
                    400      differentiate the color-3 pixel inside the cyan box (within the subgrid) from the pixel in the yellow
                    401      box(outside the subgrid) that motivates the 2D-RPE directional bias.
                    402
                    403
                                                                                                        0
                             To better understand this behavior, we refer back to Equation (1): h         = E +E . In this
                    404                                                                                 i       pi     posi
                             setup, the absolute positional encoding, E      , is directly added to the input embedding, E , so
                                                                          pos                                                 p
                    405                                                     i                                                  i
                    406      that it adjusts the token’s representation without overwhelming its semantic content. This works
                    407      effectively in NLP tasks, where the semantic meaning of tokens generally takes precedence over
                    408      their position. However, in vision tasks, especially those requiring detailed visual reasoning, spatial
                    409      relationships often carry as much importance as, if not more than, the content of the tokens. For
                             tasks in the ARC that involve complex multi-colored objects, such as subgrids, accurately encoding
                    410      positional information becomes crucial. Figure 6 illustrates a specific case where the model fails
                    411      to group pixels within a multi-colored subgrid correctly. The cross-attention map reveals that the
                    412      model overly relies on color similarity, resulting in confusion between similarly colored pixels in
                    413      different positions. This indicates a lack of sufficient attention to spatial relationships, which is
                    414      essential for such tasks and guides us to develop further enhancements in the next section.
                    415
                    416      5    RECENTERING POSITIONS & OBJECTS FOR SPATIAL REASONING IN VIT
                    417
                    418      Our observations on the failure cases of ViTARC-VT lead us to implement further enhancements
                    419      to tackle tasks with complex visual structures by better encapsulating the positional information of
                    420      pixels and objects.
                    421
                    422      Positional Encoding Mixer (PEmixer).         To better balance the importance of positional informa-
                    423      tion and tokens, we modify Equation (1) by learning weight vectors for the encodings, i.e.,
                    424                                           h0 = α⊙E +β⊙E ,                                               (10)
                                                                   i           p           pos
                    425                                                         i             i
                    426      whereαandβarelearnablevectorsofthesamedimensionastheencodingvectors,and⊙denotes
                    427      element-wise multiplication. This effectively allows the model to learn the optimal balance between
                    428      input tokens and positional encoding.
                    429      Furthermore, our implementation of 2D APE as described in Section 4, where E                is the con-
                    430                                                                                           pos(x,y)
                             catenation of E     andE      , allows the vector-based mixing coefficients to focus on specific coor-
                    431                      posx       posy
                             dinates, which further improves the model’s reasoning capability over specific pixels.
                                                                                8
