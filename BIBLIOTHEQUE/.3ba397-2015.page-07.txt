              34-layer net with this 3-layer bottleneck block, resulting in                        method                    error (%)
              a50-layerResNet(Table1). WeuseoptionBforincreasing                                Maxout[10]                   9.38
              dimensions. This model has 3.8 billion FLOPs.                                       NIN[25]                    8.81
                 101-layer and 152-layer ResNets: We construct 101-                               DSN[24]                    8.22
              layer and 152-layer ResNets by using more 3-layer blocks                                 #layers   #params
              (Table 1). Remarkably, although the depth is signiﬁcantly               FitNet [35]        19        2.5M      8.39
              increased, the 152-layer ResNet (11.3 billion FLOPs) still           Highway[42, 43]       19        2.3M      7.54 (7.72±0.16)
              has lower complexity than VGG-16/19 nets (15.3/19.6 bil-             Highway[42, 43]       32       1.25M      8.80
              lion FLOPs).                                                              ResNet           20       0.27M      8.75
                 The 50/101/152-layer ResNets are more accurate than                    ResNet           32       0.46M      7.51
              the 34-layer ones by considerable margins (Table 3 and 4).                ResNet           44       0.66M      7.17
              We do not observe the degradation problem and thus en-                    ResNet           56       0.85M      6.97
              joy signiﬁcant accuracy gains from considerably increased                 ResNet           110       1.7M      6.43 (6.61±0.16)
              depth. The beneﬁts of depth are witnessed for all evaluation              ResNet          1202      19.4M      7.93
              metrics (Table 3 and 4).
              Comparisons with State-of-the-art Methods. In Table 4              Table 6. Classiﬁcation error on the CIFAR-10 test set. All meth-
                                                                                 odsarewithdataaugmentation. ForResNet-110,werunit5times
              we compare with the previous best single-model results.            and show “best (mean±std)” as in [43].
              Ourbaseline 34-layer ResNets have achieved very compet-
              itive accuracy. Our 152-layer ResNet has a single-model            so our residual models have exactly the same depth, width,
              top-5 validation error of 4.49%. This single-model result          and number of parameters as the plain counterparts.
              outperforms all previous ensemble results (Table 5). We               Weuseaweightdecayof0.0001andmomentumof0.9,
              combine six models of different depth to form an ensemble          and adopt the weight initialization in [13] and BN [16] but
              (only with two 152-layer ones at the time of submitting).          with no dropout. These models are trained with a mini-
              This leads to 3.57% top-5 error on the test set (Table 5).         batch size of 128 on two GPUs. We start with a learning
              This entry won the 1st place in ILSVRC 2015.                       rate of 0.1, divide it by 10 at 32k and 48k iterations, and
              4.2. CIFAR-10 and Analysis                                         terminate training at 64k iterations, which is determined on
                 We conducted more studies on the CIFAR-10 dataset               a 45k/5k train/val split. We follow the simple data augmen-
              [20], which consists of 50k training images and 10k test-          tation in [24] for training: 4 pixels are padded on each side,
              ing images in 10 classes. We present experiments trained           and a 32×32 crop is randomly sampled from the padded
              on the training set and evaluated on the test set. Our focus       image or its horizontal ﬂip. For testing, we only evaluate
              is on the behaviors of extremely deep networks, but not on         the single view of the original 32×32 image.
              pushing the state-of-the-art results, so we intentionally use         Wecomparen = {3,5,7,9}, leading to 20, 32, 44, and
              simple architectures as follows.                                   56-layer networks. Fig. 6 (left) shows the behaviors of the
                 Theplain/residual architectures follow the form in Fig. 3       plain nets. The deep plain nets suffer from increased depth,
              (middle/right). The network inputs are 32×32 images, with          and exhibit higher training error when going deeper. This
              the per-pixel mean subtracted. The ﬁrst layer is 3×3 convo-        phenomenonissimilartothatonImageNet(Fig.4,left)and
              lutions. Then we use a stack of 6n layers with 3×3 convo-          on MNIST(see [42]), suggesting that such an optimization
              lutions on the feature maps of sizes {32,16,8} respectively,       difﬁculty is a fundamental problem.
              with 2n layers for each feature map size. The numbers of              Fig. 6 (middle) shows the behaviors of ResNets. Also
              ﬁlters are {16,32,64}respectively. Thesubsamplingisper-            similar to the ImageNet cases (Fig. 4, right), our ResNets
              formedbyconvolutionswithastrideof2. Thenetworkends                 managetoovercometheoptimizationdifﬁcultyanddemon-
              with a global average pooling, a 10-way fully-connected            strate accuracy gains when the depth increases.
              layer, and softmax. Therearetotally6n+2stackedweighted                We further explore n = 18 that leads to a 110-layer
              layers. The following table summarizes the architecture:           ResNet. In this case, we ﬁnd that the initial learning rate
                                                                                                                                5
                                                                                 of 0.1 is slightly too large to start converging . So we use
                      output map size    32×32      16×16     8×8                0.01towarmupthetraininguntilthetrainingerrorisbelow
                          # layers        1+2n        2n       2n                80%(about400iterations),andthengobackto0.1andcon-
                          # ﬁlters         16         32       64                tinue training. The rest of the learning schedule is as done
                                                                                 previously. This 110-layer network converges well (Fig. 6,
              When shortcut connections are used, they are connected             middle). It has fewer parameters than other deep and thin
              to the pairs of 3×3 layers (totally 3n shortcuts). On this            5With an initial learning rate of 0.1, it starts converging (<90% error)
              dataset we use identity shortcuts in all cases (i.e., option A),   after several epochs, but still reaches similar accuracy.
                                                                             7
