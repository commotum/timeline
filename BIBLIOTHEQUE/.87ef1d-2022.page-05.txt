                         Published as a conference paper at ICLR 2022
                         Toreducetheeffects of staleness, we normalize keys and queries (Henry et al., 2020). Normalization
                         does not eliminate staleness, but it at least ensures that older keys and newer keys do not differ in
                         magnitude. We also found that normalization helps stabilize training with the Transformer-XL cache.
                         In some of our experiments, we observed that training models from scratch with a large memory
                         sometimes resulted in worse performance than pretraining the model with a small memory of size
                         8192, and then ﬁnetuning it on a larger memory. This training instability could be due to staleness.
                         However, models seem to be able to cope with a limited degree of staleness (with the small memory)
                         byadjusting their queries accordingly.
                         3.3  APPROXIMATEkNN
                         WeemployapproximatekNNsearchratherthanexactkNNsearchbecauseitsigniﬁcantly improves
                         the computational speed of our model. We use a simple approximation of kNN for TPUs, which has
                         a recall of about 90%, i.e. 90% of the true top k are returned in the approximate top k. There are
                         various other efﬁcient approximate kNN algorithms available for CPU and GPU/TPU, for example
                         through Faiss (Johnson et al., 2021) or ScaNN (Guo et al., 2020), which can scale into the billions.
                         4   EXPERIMENTS
                         Weevaluate the effect of adding external memory on ﬁve language modeling tasks, all of which
                         involve long-form text: English language books (PG-19), long web articles (C4), technical math
                         papers (arXiv Math), source code (Github), and formal theorems (Isabelle). The results show
                         signiﬁcant improvements in the perplexity of the model with the addition of external memory. We
                         experimented with various sizes of external memory, from 1536 to as high as 262K. On most of the
                         datasets, there was an initial sharp gain from adding a small external memory, followed by smaller
                         but steadily increasing gains as the size of the memory was increased.
                         4.1  DATASETS
                         arXiv Math For the arXiv dataset, we collected a corpus of papers by downloading them via
                         the arXiv Bulk Data Access1. We ﬁltered papers to include only articles labeled as “Mathematics”
                                    A
                         and whose LT X source was available. The number of tokens per paper in this dataset is roughly
                                     E
                                                                                      A
                         comparable to the number of tokens per book in PG19, because LT X source has many special
                         characters and the tokenizer tends to output small subwords.  E
                         Github WeusedBigQuery2 toobtainalargecorpus of Github repositories that are published with
                         open-source licenses. We used ﬁle endings to ﬁlter for ﬁles in the languages C, C++, Java, Python
                         (including Jupyter notebooks), Go, and TypeScript. Individual source code ﬁles are often fairly short,
                         and there are many dependencies and cross-references between ﬁles in the repository. To capture
                         these dependencies, we created one long document for each Github repository by traversing the
                         directory tree, and concatenating all of the ﬁles within it. The order in which ﬁles are traversed within
                         the repository is random, but each subdirectory is processed as a unit, so that all the ﬁles within the
                         subdirectory are close to each other in the resulting document. Source code is usually structured
                         so that related ﬁles are all grouped together in the same subdirectory; this traversal preserves that
                         structure, while still shufﬂing ﬁles and subdirectories in random order.
                         FormalMath–Isabelle TheIsabellecorpusconsists of formal mathematical proofs of theories.
                         Wecollected all 627 theories available on The Archive of Formal Proofs3 (as of October 6, 2021) and
                                                                              4
                         an additional 57 theories from the Isabelle standard library to create a corpus of 684 theories. All
                         theories have open-source licenses. Each theory is a self-contained mathematical object, on topics
                         such as foundational logic, advanced analysis, algebra, or cryptography, and consists of multiple
                         ﬁles containing proofs. As with the Github corpus, all ﬁles that make up a theory are concatenated
                            1https://arxiv.com/help/bulk_data
                            2https://console.cloud.google.com/marketplace/product/github/github-repos
                            3https://www.isa-afp.org/topics.html
                            4https://isabelle.in.tum.de/
                                                                    5
