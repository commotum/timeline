                 way networks have not demonstrated accuracy gains with                            ReLU [29] and the biases are omitted for simplifying no-
                 extremely increased depth (e.g., over 100 layers).                                tations. The operation F + x is performed by a shortcut
                                                                                                   connection and element-wise addition. We adopt the sec-
                 3. Deep Residual Learning                                                         ondnonlinearity after the addition (i.e., σ(y), see Fig. 2).
                 3.1. Residual Learning                                                                TheshortcutconnectionsinEqn.(1)introduceneitherex-
                                                                                                   tra parameter nor computation complexity. This is not only
                    Let us consider H(x) as an underlying mapping to be                            attractive in practice but also important in our comparisons
                 ﬁt by a few stacked layers (not necessarily the entire net),                      between plain and residual networks. We can fairly com-
                 with x denoting the inputs to the ﬁrst of these layers. If one                    pare plain/residual networks that simultaneously have the
                 hypothesizes that multiple nonlinear layers can asymptoti-                        same number of parameters, depth, width, and computa-
                 cally approximate complicated functions2, then it is equiv-                       tionalcost(exceptforthenegligibleelement-wiseaddition).
                 alent to hypothesize that they can asymptotically approxi-                            The dimensions of x and F must be equal in Eqn.(1).
                 mate the residual functions, i.e., H(x) − x (assuming that                        If this is not the case (e.g., when changing the input/output
                 the input and output are of the same dimensions).                       So        channels), we can perform a linear projection Ws by the
                 rather than expect stacked layers to approximate H(x), we                         shortcut connections to match the dimensions:
                 explicitly let these layers approximate a residual function
                 F(x) := H(x) − x. The original function thus becomes                                                    y=F(x,{Wi})+Wsx.                                  (2)
                 F(x)+x. Althoughbothformsshouldbeabletoasymptot-                                  WecanalsouseasquarematrixWs inEqn.(1). Butwewill
                 ically approximate the desired functions (as hypothesized),                       show by experiments that the identity mapping is sufﬁcient
                 the ease of learning might be different.                                          for addressing the degradation problem and is economical,
                    This reformulation is motivated by the counterintuitive                        and thus W is only used when matching dimensions.
                 phenomenaaboutthedegradationproblem(Fig.1,left). As                                              s
                 we discussed in the introduction, if the added layers can                             The form of the residual function F is ﬂexible. Exper-
                 beconstructed as identity mappings, a deeper model should                         iments in this paper involve a function F that has two or
                 have training error no greater than its shallower counter-                        three layers (Fig. 5), while more layers are possible. But if
                 part.   The degradation problem suggests that the solvers                         Fhasonlyasinglelayer,Eqn.(1)issimilartoalinearlayer:
                 might have difﬁculties in approximating identity mappings                         y=W1x+x,forwhichwehavenotobservedadvantages.
                 by multiple nonlinear layers. With the residual learning re-                          Wealsonotethatalthough the above notations are about
                 formulation, if identity mappings are optimal, the solvers                        fully-connected layers for simplicity, they are applicable to
                                                                                                   convolutional layers. The function F(x,{W }) can repre-
                 maysimplydrivetheweightsofthemultiple nonlinear lay-                                                                                        i
                 ers toward zero to approach identity mappings.                                    sent multiple convolutional layers. The element-wise addi-
                    In real cases, it is unlikely that identity mappings are op-                   tion is performed on two feature maps, channel by channel.
                 timal, but our reformulation may help to precondition the                         3.3. Network Architectures
                 problem. If the optimal function is closer to an identity
                 mapping than to a zero mapping, it should be easier for the                           Wehavetested various plain/residual nets, and have ob-
                 solver to ﬁnd the perturbations with reference to an identity                     served consistent phenomena. To provide instances for dis-
                 mapping, than to learn the function as a new one. We show                         cussion, we describe two models for ImageNet as follows.
                 byexperiments(Fig.7)thatthelearnedresidualfunctionsin                             Plain Network. Our plain baselines (Fig. 3, middle) are
                 general have small responses, suggesting that identity map-                       mainlyinspiredbythephilosophyofVGGnets[41](Fig.3,
                 pings provide reasonable preconditioning.                                         left). The convolutional layers mostly have 3×3 ﬁlters and
                 3.2. Identity Mapping by Shortcuts                                                follow two simple design rules: (i) for the same output
                    Weadopt residual learning to every few stacked layers.                         feature map size, the layers have the same number of ﬁl-
                 Abuilding block is shown in Fig. 2. Formally, in this paper                       ters; and (ii) if the feature map size is halved, the num-
                 weconsider a building block deﬁned as:                                            ber of ﬁlters is doubled so as to preserve the time com-
                                                                                                   plexity per layer. We perform downsampling directly by
                                        y=F(x,{W})+x.                                   (1)        convolutional layers that have a stride of 2. The network
                                                          i                                        ends with a global average pooling layer and a 1000-way
                 Here x and y are the input and output vectors of the lay-                         fully-connected layer with softmax. The total number of
                 ers considered. The function F(x,{W }) represents the                             weighted layers is 34 in Fig. 3 (middle).
                                                                     i                                 It is worth noticing that our model has fewer ﬁlters and
                 residual mapping to be learned. For the example in Fig. 2                         lower complexity than VGGnets[41](Fig.3, left). Our 34-
                 that has two layers, F = W σ(W x) in which σ denotes
                                                      2      1                                     layer baseline has 3.6 billion FLOPs (multiply-adds), which
                    2This hypothesis, however, is still an open question. See [28].                is only 18% of VGG-19 (19.6 billion FLOPs).
                                                                                               3
